id,number,title,user,state,created_at,updated_at,body
589802137,33120,.gt() add support for single column dataframes,mariolovric,closed,2020-03-29T14:16:45Z,2020-08-17T22:03:33Z,"It would be great if when comparing df to series like in pandas .gt() or .lt() one could compare also to single column dataframes instead of series.
Maybe also extend it to other functions?

Thanks



This works:
```
df1 = pd.DataFrame(np.array([[2,2],[1,1], [0,0]]), columns = ['a1','a2'])
df2 = pd.Series([2,2,2], name = 'b')
df1.lt(df2, axis=0)
```

This doesnt
```
df1 = pd.DataFrame(np.array([[2,2],[1,1], [0,0]]), columns = ['a1','a2'])
df2 = pd.DataFrame([2,2,2], columns = ['b'])
df1.lt(df2, axis=0)
```

"
659240714,35323,Remove deprecated warn kwarg for matplotlib use,saulshanabrook,closed,2020-07-17T13:04:46Z,2020-08-17T22:04:24Z,"The warn param has been deprecated in 3.2.1: https://matplotlib.org/3.2.1/api/prev_api_changes/api_changes_3.2.0.html#matplotlib-use

On the most recent matplotlib, this is failing for me with 

```
TypeError: use() got an unexpected keyword argument 'warn'
```
"
672431117,35533,Check if NPY_NAT is NA for int64 in rank() (#32859),gabrielNT,closed,2020-08-04T01:04:37Z,2020-08-18T01:03:40Z,"- [x] closes #32859
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Used  `missing.isnaobj()` to try to handle the suggestion from @jbrockmendel. Lmk if this might cause other issues.
"
680473938,35774,REGR: follow-up to return copy with df.interpolate on empty DataFrame,simonjayhawkins,closed,2020-08-17T19:27:18Z,2020-08-18T13:06:35Z,follow-up to #35543
656124107,35266,REGR: setting column with setitem should not modify existing array inplace,jorisvandenbossche,closed,2020-07-13T20:39:20Z,2020-08-18T13:37:46Z,"Closes #33457

This changes `ExtensionBlock.setitem` to how it was before #32831: updating the Block itself inplace, but not the array. 

@jbrockmendel I know you won't like that this update the Block inplace (it's also not exactly what you suggested at https://github.com/pandas-dev/pandas/issues/33457#issuecomment-654367183). But this was the smallest change I could find to fix the issue. 

The alternative is probably changing `BlockManager.iset` to not use Block.set in case of an existing ExtensionBlock, but `BlockManager.iset` is quite complex ..
"
656412210,35271,REGR: revert ExtensionBlock.set to be in-place,jorisvandenbossche,closed,2020-07-14T08:01:29Z,2020-08-18T13:40:30Z,Alternative for https://github.com/pandas-dev/pandas/pull/35266 and closes #35369
681027141,35789,Backport PR #35774 on branch 1.1.x (REGR: follow-up to return copy with df.interpolate on empty DataFrame),meeseeksmachine,closed,2020-08-18T13:06:29Z,2020-08-18T14:12:20Z,Backport PR #35774: REGR: follow-up to return copy with df.interpolate on empty DataFrame
680432555,35772,CI: close sockets in SQL tests,jbrockmendel,closed,2020-08-17T18:12:28Z,2020-08-18T15:46:19Z,"closes #35660
closes #29514
 
Broken off from #35711"
680673365,35783,CLN: remove unused variable,jbrockmendel,closed,2020-08-18T04:08:03Z,2020-08-18T15:49:35Z,
593711939,33281,ENH: Optimize nrows in read_excel,mproszewska,closed,2020-04-04T01:10:43Z,2020-08-18T19:44:29Z,"- [x] closes #32727
- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] If header, skiprows and nrows are integers, rows that will be skipped are not loaded in get_sheet_data function. 
"
597548000,33443,BUG: Low performance of DataFrame operations,rkashapov,closed,2020-04-09T20:56:23Z,2020-08-18T22:26:48Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import timeit

import pandas as pd
import numpy as np


def div_simple(df):
    return df.div(df.sum(axis=0), axis=1)


def div_transposed(df):
    df = df.T
    return df.div(df.sum(axis=1), axis=0).T


def div_python(df):
    data = df.values.tolist()
    for i in range(COLS):
        s = sum(data[j][i] for j in range(ROWS))
        for j in range(ROWS):
            data[j][i] /= s
    return data


COLS = 1000
ROWS = 10


df = pd.DataFrame(
    np.random.randint(0,100,size=(ROWS, COLS)),
    columns=list(range(COLS)),
)

for func in (div_simple, div_transposed, div_python):
    print(f'{timeit.timeit(lambda: func(df.copy()), number=100):08.4f}s\t{func.__name__}')
```

#### Problem description

I've faced a strange performance issue. The pandas' version works much longer than a bare-python version.

```
100 x 100
001.6424s	div_simple
000.0596s	div_transposed
000.1856s	div_python
```
```
10 x 1000
000.2322s	div_simple
000.0576s	div_transposed
000.1780s	div_python
```
```
1000 x 10
015.3119s	div_simple
000.0560s	div_transposed
000.2327s	div_python
```


<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.4.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : ru_RU.UTF-8
LOCALE           : ru_RU.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
680992287,35787,DOC: clean v1.1.1 release notes,simonjayhawkins,closed,2020-08-18T12:12:49Z,2020-08-19T09:52:45Z,
681704118,35800,Backport PR #35787 on branch 1.1.x (DOC: clean v1.1.1 release notes),meeseeksmachine,closed,2020-08-19T09:49:44Z,2020-08-19T11:34:40Z,Backport PR #35787: DOC: clean v1.1.1 release notes
673423086,35564,BUG:df.sort_values() disturb the order of values among columns. ,yangyxt,closed,2020-08-05T10:13:33Z,2020-08-19T12:19:38Z,"- [ ] I have checked that this issue has not already been reported.
check
- [ ] I have confirmed this bug exists on the latest version of pandas.
check
- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.
check
---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
anno_table.sort_values(by=['Denovo_status', 'CLNSIG', patient_gt_col, 'ExonicFunc.refGene', 'DANN_score'], ascending=False, inplace=True)
    
```

#### Problem description
I have two columns named ""Gene.refGene"" and ""AAChange.refGene"" the later one is the detailed description of the amino acid change caused by a variant on the gene. So it should strictly correspond to the former column I mentioned here. 

However, after the running the code pasted above, their correspondence become completely messed. 
[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output
Here I prepare the sample code for you to re-generate the issue.
[Archive.zip](https://github.com/pandas-dev/pandas/files/5027662/Archive.zip)
Pls de-compress the package and run the sh file. Keep them in the same folder.


#### Output of ``pd.show_versions()``

<details> 

![image](https://user-images.githubusercontent.com/40780228/89400761-eb162100-d746-11ea-8c55-85910b690273.png)

</details>
"
505328202,28896,Failing multiplication if both dataframes contain the (seemingly) identical categorical column,harmbuisman,closed,2019-10-10T14:57:25Z,2020-08-19T12:30:10Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

pop = pd.read_pickle('pop.pcl')
a = pd.read_pickle('rates_all.pcl')
b = pd.read_pickle('rates_main.pcl')

print('####### Dataframes #######')
print(pop)
print(a)
print(b)

print('\n####### Indices #######')
print(b.index.get_level_values(1))
print(a.index.get_level_values(1))
print(pop.index.get_level_values(0))

print('\n####### Output #######')
# the following works
print(pop.mul(b))

# the following fails
pop.mul(a)
```

Output:
```python
####### Dataframes #######
Age group 5-yrs to 90  PROVINCIE 
 0-4                   Drenthe        801402
                       Flevoland      740024
                       Friesland     1118421
                       Gelderland    3443051
                       Groningen      907168
Name: Population, dtype: int32
All tumors  Age group 5-yrs to 90
-1           0-4                     0.000198
             5-9                     0.000112
            10-14                    0.000123
            15-19                    0.000211
            20-24                    0.000329
dtype: float64
Main tumor group  Age group 5-yrs to 90
0                  0-4                     3.727047e-07
                   5-9                     2.410106e-07
                  10-14                    3.438238e-07
                  15-19                    6.399917e-08
                  20-24                    1.843772e-07
dtype: float64

####### Indices #######
b: Index([' 0-4', ' 5-9', '10-14', '15-19', '20-24'], dtype='object', name='Age group 5-yrs to 90')
a: CategoricalIndex([' 0-4', ' 5-9', '10-14', '15-19', '20-24'], categories=[' 0-4', ' 5-9', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39', ...], ordered=True, name='Age group 5-yrs to 90', dtype='category')
pop: CategoricalIndex([' 0-4', ' 0-4', ' 0-4', ' 0-4', ' 0-4'], categories=[' 0-4', ' 5-9', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39', ...], ordered=True, name='Age group 5-yrs to 90', dtype='category')

####### Output #######
Age group 5-yrs to 90  PROVINCIE   Main tumor group
 0-4                   Drenthe     0                   0.298686
                       Flevoland   0                   0.275810
                       Friesland   0                   0.416841
                       Gelderland  0                   1.283241
                       Groningen   0                   0.338106
 5-9                   NaN         0                        NaN
10-14                  NaN         0                        NaN
15-19                  NaN         0                        NaN
20-24                  NaN         0                        NaN
dtype: float64
```

#### Problem description
The above code needs the following files:
[Analysis.zip](https://github.com/pandas-dev/pandas/files/3712866/Analysis.zip)

Running it leads to an error ""TypeError: No matching signature found"" in the last call.

The pop dataframe has a category column 'Age group 5-yrs to 90' in the multi-index. In dataframe b the index column 'Age group 5-yrs to 90' has type object. Multiplying both dataframes succeeds. Dataframe a also has a category column 'Age group 5-yrs to 90', but multiplying it with pop fails because it cannot find a matching signature.

#### Expected Output
I would expect the output of pop.mul(a) to be similar to the pop.mul(b) call, without an error.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.1
numpy            : 1.16.4
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.2
setuptools       : 41.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.7.6.1 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.7.0
pandas_datareader: None
bs4              : None
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.7
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
597041943,33417,DOC: lreshape and wide_to_long references,tpanza,closed,2020-04-09T05:59:41Z,2020-08-19T13:05:41Z,"#### Location of the documentation

* [General functions](https://pandas.pydata.org/pandas-docs/version/1.0.3/reference/general_functions.html): Does not list `lreshape`
* [DataFrame reshaping, sorting, transposing](https://pandas.pydata.org/pandas-docs/version/1.0.3/reference/frame.html#reshaping-sorting-transposing): Does not list `lreshape` or `wide_to_long`
* [pd.melt](https://pandas.pydata.org/pandas-docs/version/1.0.3/reference/api/pandas.melt.html#pandas.melt): Does not list `lreshape` or `wide_to_long` in the ""See Also"" section
* [pd.pivot](https://pandas.pydata.org/pandas-docs/version/1.0.3/reference/api/pandas.pivot.html): Include more related functions in the ""See Also"" section
* [pd.pivot_table](): Include more related functions in the ""See Also"" section
* [pd.wide_to_long](https://pandas.pydata.org/pandas-docs/version/1.0.3/reference/api/pandas.wide_to_long.html#pandas.wide_to_long): Does not have a ""See Also"" section
* [pd.lreshape](https://pandas.pydata.org/pandas-docs/version/1.0.3/reference/api/pandas.lreshape.html): _Does not exist_

#### Documentation problem

There remains an outstanding question as to whether `lreshape` and/or `wide_to_long` should be deprecated/removed (#15003). Until that question is resolved, the pandas documentation should include `lreshape` and `wide_to_long`. 

More links to and between the related pivoting/melting functions should be included in their respective ""See Also"" sections of their docstrings.

#### Suggested fix for documentation

Fairly self-explanatory. PR will contain the suggested additions.
"
597056991,33418,lreshape and wide_to_long documentation (Closes #33417),tpanza,closed,2020-04-09T06:39:07Z,2020-08-19T13:05:48Z,"- [ ] closes #33417
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
680009701,35760,"DOC: downcast argument for pd.to_numeric should be integer, not int",languitar,closed,2020-08-17T07:21:08Z,2020-08-19T13:16:42Z,"#### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.to_numeric.html?highlight=to_numeric#pandas.to_numeric

#### Documentation problem

The documentation declares that the `downcast` argument of `pd.to_numeric` can use the value `int`. However, only `integer` is working in the real code:

```
In [8]: pd.to_numeric([""42""], downcast=""int"", errors=""coerce"")
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-8-3b178e77dcd5> in <module>
----> 1 pd.to_numeric([""42""], downcast=""int"", errors=""coerce"")

/usr/lib/python3.8/site-packages/pandas/core/tools/numeric.py in to_numeric(arg, errors, downcast)
    110     """"""
    111     if downcast not in (None, ""integer"", ""signed"", ""unsigned"", ""float""):
--> 112         raise ValueError(""invalid downcasting method provided"")
    113 
    114     if errors not in (""ignore"", ""raise"", ""coerce""):

ValueError: invalid downcasting method provided

In [9]: pd.to_numeric([""42""], downcast=""integer"", errors=""coerce"")
Out[9]: array([42], dtype=int8)
```

#### Suggested fix for documentation

Replace `int` with `integer`"
680568806,35776,Changed 'int' type to 'integer' in to_numeric docstring,edwardkong,closed,2020-08-17T22:44:38Z,2020-08-19T13:16:56Z,"- [x] closes #35760 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
681832440,35806,Backport PR #35776 on branch 1.1.x (Changed 'int' type to 'integer' in to_numeric docstring),meeseeksmachine,closed,2020-08-19T13:16:52Z,2020-08-19T14:25:28Z,Backport PR #35776: Changed 'int' type to 'integer' in to_numeric docstring
677015595,35672,CI/TST: change skip to xfail #35660,fangchenli,closed,2020-08-11T16:22:57Z,2020-08-19T17:37:41Z,Followup of #35660 
675491372,35620,CI: Linux py36_locale failures with pytest DeprecationWarning,simonjayhawkins,closed,2020-08-08T09:01:27Z,2020-08-19T17:42:48Z,"tests using `async_mark` are failing cc @alimcmaster1 
```
=========================== short test summary info ============================
ERROR pandas/tests/arrays/categorical/test_warnings.py::TestCategoricalWarnings
ERROR pandas/tests/arrays/categorical/test_warnings.py::TestCategoricalWarnings
ERROR pandas/tests/frame/test_api.py::TestDataFrameMisc - pytest.PytestDeprec...
ERROR pandas/tests/frame/test_api.py::TestDataFrameMisc - pytest.PytestDeprec...
ERROR pandas/tests/indexes/test_base.py::TestIndex - pytest.PytestDeprecation...
ERROR pandas/tests/indexes/test_base.py::TestIndex - pytest.PytestDeprecation...
ERROR pandas/tests/resample/test_resampler_grouper.py - pytest.PytestDeprecat...
ERROR pandas/tests/resample/test_resampler_grouper.py - pytest.PytestDeprecat...
ERROR pandas/tests/series/test_api.py::TestSeriesMisc - pytest.PytestDeprecat...
ERROR pandas/tests/series/test_api.py::TestSeriesMisc - pytest.PytestDeprecat...
= 71242 passed, 1936 skipped, 1024 xfailed, 156 warnings, 10 errors in 655.85s (0:10:55) =
```"
675212832,35611,ENH: Make top-level Pandas functions serializable,mrocklin,closed,2020-08-07T18:56:19Z,2020-08-19T18:02:56Z,"```python
import pandas as pd
import pickle
pickle.dumps(pd.read_csv)
```

```python-traceback
AttributeError: Can't pickle local object '_make_parser_function.<locals>.parser_f'
```"
677838401,35692,ENH: GH-35611 Tests for top-level Pandas functions serializable,ikedaosushi,closed,2020-08-12T16:52:24Z,2020-08-19T18:03:00Z,"- [x] closes #35611
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
659307145,35325,BUG: Combination of groupby.resample.interpolate() fails,UnderTheCarpet,closed,2020-07-17T14:35:14Z,2020-08-19T20:22:59Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
d = {'price': [10, 11, 9, 13, 14, 18, 17, 19],
     'volume': [50, 60, 40, 100, 50, 100, 40, 50]}

df = pd.DataFrame(d)

df['week_starting'] = pd.date_range('01/01/2018', periods=8, freq='W')

df \
    .set_index(""week_starting"") \
    .groupby(""volume"") \
    .resample(""1D"") \
    .interpolate(method=""linear"")

```

Error:
```
TypeError                                 Traceback (most recent call last)
~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/groupby/groupby.py in apply(self, func, *args, **kwargs)
    735             try:
--> 736                 result = self._python_apply_general(f)
    737             except TypeError:

~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/groupby/groupby.py in _python_apply_general(self, f)
    751     def _python_apply_general(self, f):
--> 752         keys, values, mutated = self.grouper.apply(f, self._selected_obj, self.axis)
    753 

~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/groupby/ops.py in apply(self, f, data, axis)
    205             group_axes = group.axes
--> 206             res = f(group)
    207             if not _is_indexed_like(res, group_axes):

~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/resample.py in func(x)
    988 
--> 989             return x.apply(f, *args, **kwargs)
    990 

~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/resample.py in aggregate(self, func, *args, **kwargs)
    284             grouper = None
--> 285             result = self._groupby_and_aggregate(how, grouper, *args, **kwargs)
    286 

~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/resample.py in _groupby_and_aggregate(self, how, grouper, *args, **kwargs)
    360             else:
--> 361                 result = grouped.aggregate(how, *args, **kwargs)
    362         except DataError:

~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/groupby/generic.py in aggregate(self, func, *args, **kwargs)
    923             # nicer error message
--> 924             raise TypeError(""Must provide 'func' or tuples of '(column, aggfunc)."")
    925 

TypeError: Must provide 'func' or tuples of '(column, aggfunc).

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-15-5d08bff36e4d> in <module>
----> 1 df = df \
      2     .set_index(""week_starting"") \
      3         .groupby(""volume"") \
      4             .resample(""1D"") \
      5                 .interpolate(method=""linear"")

~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/resample.py in interpolate(self, method, axis, limit, inplace, limit_direction, limit_area, downcast, **kwargs)
    797         Interpolate values according to different methods.
    798         """"""
--> 799         result = self._upsample(None)
    800         return result.interpolate(
    801             method=method,

~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/resample.py in _apply(self, f, grouper, *args, **kwargs)
    989             return x.apply(f, *args, **kwargs)
    990 
--> 991         result = self._groupby.apply(func)
    992         return self._wrap_result(result)
    993 

~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/groupby/groupby.py in apply(self, func, *args, **kwargs)
    745 
    746                 with _group_selection_context(self):
--> 747                     return self._python_apply_general(f)
    748 
    749         return result

~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/groupby/groupby.py in _python_apply_general(self, f)
    750 
    751     def _python_apply_general(self, f):
--> 752         keys, values, mutated = self.grouper.apply(f, self._selected_obj, self.axis)
    753 
    754         return self._wrap_applied_output(

~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/groupby/ops.py in apply(self, f, data, axis)
    204             # group might be modified
    205             group_axes = group.axes
--> 206             res = f(group)
    207             if not _is_indexed_like(res, group_axes):
    208                 mutated = True

~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/resample.py in func(x)
    987                 return getattr(x, f)(**kwargs)
    988 
--> 989             return x.apply(f, *args, **kwargs)
    990 
    991         result = self._groupby.apply(func)

~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/resample.py in aggregate(self, func, *args, **kwargs)
    283             how = func
    284             grouper = None
--> 285             result = self._groupby_and_aggregate(how, grouper, *args, **kwargs)
    286 
    287         result = self._apply_loffset(result)

~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/resample.py in _groupby_and_aggregate(self, how, grouper, *args, **kwargs)
    359                 result = grouped._aggregate_item_by_item(how, *args, **kwargs)
    360             else:
--> 361                 result = grouped.aggregate(how, *args, **kwargs)
    362         except DataError:
    363             # we have a non-reducing function; try to evaluate

~/.cache/pypoetry/virtualenvs/lagps-6BuXYM4Y-py3.8/lib/python3.8/site-packages/pandas/core/groupby/generic.py in aggregate(self, func, *args, **kwargs)
    922         elif func is None:
    923             # nicer error message
--> 924             raise TypeError(""Must provide 'func' or tuples of '(column, aggfunc)."")
    925 
    926         func = _maybe_mangle_lambdas(func)

TypeError: Must provide 'func' or tuples of '(column, aggfunc).
```

On master the error is raised in line 86 of the same file.

#### Problem description

The combination of `groupby`, `resample`, and `interpolate` leads to an `TypeError: Must provide 'func' or tuples of '(column, aggfunc).` [0].

Other functions like `ffill`, or `bfill` work without issues.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-18362-Microsoft
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 44.0.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>

[0], btw, there is a quote missing in the error message.
"
680615545,35779,CI: unpin numpy and matplotlib,fangchenli,closed,2020-08-18T01:07:53Z,2020-08-19T20:40:05Z,revert #35312 and #35358
680616359,35780,CI: unpin numpy and matplotlib #35779,fangchenli,closed,2020-08-18T01:10:26Z,2020-08-19T21:38:01Z,"- [x] closes #35779

"
651789761,35150,"BUG: pd.crosstab fails when passed multiple columns, margins True and normalize True",CloseChoice,closed,2020-07-06T20:09:58Z,2020-08-20T00:09:02Z,"- [x] closes #35144
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
662312812,35360,fix bug when combining groupby with resample and interpolate with dat…,CloseChoice,closed,2020-07-20T22:06:37Z,2020-08-20T00:09:38Z,"…etime-index (GH 35325)

- [x] closes #35325
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
651562770,35138,CI/DOC: autodoc failures for DataFrame.sparse methods with latest sphinx,jorisvandenbossche,closed,2020-07-06T13:58:52Z,2020-08-20T07:24:56Z,"On master and on PRs, there is a CI failure:

```
WARNING: autodoc: failed to import method 'sparse.from_spmatrix' from module 'DataFrame'; the following exception was raised:
No module named 'DataFrame'
WARNING: autodoc: failed to import method 'sparse.to_coo' from module 'DataFrame'; the following exception was raised:
No module named 'DataFrame'
WARNING: autodoc: failed to import method 'sparse.to_dense' from module 'DataFrame'; the following exception was raised:
No module named 'DataFrame'
WARNING: autodoc: failed to import method 'sparse.from_coo' from module 'Series'; the following exception was raised:
No module named 'Series'
WARNING: autodoc: failed to import method 'sparse.to_coo' from module 'Series'; the following exception was raised:
No module named 'Series'
```

Looking at the env of the failing ones and the last one that passed on master: sphinx upgraded from 3.1.1 to 3.1.2. 
"
682481162,35817,BUG: error in reduction.pyx file while building pandas from source with setup.py file,akshaymanu007,closed,2020-08-20T07:15:40Z,2020-08-20T07:46:02Z,"I am compiling pandas1.10 from source with the help of setup.py

It is showing me following error in reduction.pyx file 

        self.orig_data = self.buf.data
        self.orig_len = self.buf.shape[0]
        self.orig_stride = self.buf.strides[0]

        self.buf.data = self.values.data
               ^
------------------------------------------------------------

pandas/_libs/reduction.pyx:313:16: Assignment to a read-only property

Error compiling Cython file:


enviornment :  Linux opensuze
python version : 3.7.9"
682033875,35809,CI: more xfail failing 32-bit tests,simonjayhawkins,closed,2020-08-19T18:03:26Z,2020-08-20T12:05:50Z,xref #35294 and https://github.com/pandas-dev/pandas/issues/35489#issuecomment-676550757
682660387,35821,Backport PR #35809 on branch 1.1.x (CI: more xfail failing 32-bit tests),meeseeksmachine,closed,2020-08-20T12:05:33Z,2020-08-20T13:32:05Z,Backport PR #35809: CI: more xfail failing 32-bit tests
682019488,35808,Backport PR #35672 on branch 1.1.x CI: test_chunks_have_consistent_numerical_type periodically fails on 1.1.x,simonjayhawkins,closed,2020-08-19T17:37:14Z,2020-08-20T13:41:55Z,"PR directly against 1.1.x

effectively backport of #35672"
681800035,35801,DOC: another pass of v1.1.1 release notes,simonjayhawkins,closed,2020-08-19T12:27:32Z,2020-08-20T15:07:07Z,https://pandas.pydata.org/docs/dev/whatsnew/v1.1.1.html
654651790,35208,ROADMAP: add consistent missing values for all dtypes to the roadmap,jorisvandenbossche,closed,2020-07-10T09:34:16Z,2020-08-20T15:50:22Z,"The more detailed motivation is described in https://hackmd.io/@jorisvandenbossche/Sk0wMeAmB, and many aspects have already been discussed in https://github.com/pandas-dev/pandas/issues/28095 and linked issues.  
(and there are probably still other details / practical aspects that can be further discussed in dedicated issues)

Last year when I made the `pd.NA` proposal (and which resulted in using that for the nullable integer, boolean and string dtypes), which described it as ""can be used consistently across all data types"", the implicit / aspirational end goal of this proposal for me always was to *actually* have this for all dtypes (and as the default, at some point). 
I tried to discuss this goal more explicitly on the mailing list earlier this year (in the thread about pandas 2.0: https://mail.python.org/pipermail/pandas-dev/2020-February/001180.html). But we never really ""officially"" adopted this as a goal / roadmap item, or discussed about doing that. 
Proposing to add a section about it to the roadmap is an attempt to do this (as that is actually how it's described to do it in our [roadmap](https://pandas.pydata.org/docs/dev/development/roadmap.html#roadmap-evolution)).

The aforementioned mailing list thread mostly resulted in a discussion about how to integrate the new semantics in the datetime-like dtypes (pd.NaT vs pd.NA, keep pd.NaT but change its behaviour, etc). This is still a technical discussion we further need to resolve, but note that I kept the text on that in the PR somewhat vague on purpose for this reason: *""..: all data types should support missing values and with the same behaviour""*

And the general disclaimer that is also in our roadmap: *An item being on the roadmap does not mean that it will* necessarily *happen, even with unlimited funding. During the implementation period we may discover issues preventing the adoption of the feature.*

cc @pandas-dev/pandas-core @pandas-dev/pandas-triage "
682226962,35812,WEB: Add new Maintainers to Team Page,alimcmaster1,closed,2020-08-19T22:37:48Z,2020-08-20T16:26:36Z,"Add new maintainers.

cc:
@bashtage
@charlesdong1991
@Dr-Irv
@dsaxton
@MarcoGorelli
@rhshadrach

In case anyone doesn't want to appear here: https://pandas.pydata.org/about/team.html"
682792602,35822,Backport PR #35801 on branch 1.1.x (DOC: another pass of v1.1.1 release notes),meeseeksmachine,closed,2020-08-20T15:06:03Z,2020-08-20T16:36:03Z,Backport PR #35801: DOC: another pass of v1.1.1 release notes
679650280,35746,REF: insert self.on column _after_ concat,jbrockmendel,closed,2020-08-15T22:17:21Z,2020-08-20T17:53:01Z,"The idea here is to push towards #34714 by making _wrap_results do things in the same order as other other similar methods do.  cc @mroeschke LMK if there is a simpler way to accomplish this.

Orthogonal to #35470, #35730, #35696."
682247454,35813,CI: Failures due to pyarrow 0.11 in Travis 37 Locale Build ,alimcmaster1,closed,2020-08-19T23:30:50Z,2020-08-20T21:26:05Z,"Looks like recently the Travis 37 Locale Build has started picking up pyarrow 0.11.1

https://travis-ci.org/github/pandas-dev/pandas/jobs/719430999
pyarrow                   0.11.1           py37he6710b0_0 

This causes the pyarrow tests to run which seem to be failing.

Previously no pyarrow in env: https://travis-ci.org/github/pandas-dev/pandas/jobs/719138849 (passing build)

Feels like one of our deps has started to depend to pyarrow."
683154790,35835,MAINT: Manual Backport PR #35825 on branch 1.1.x,alimcmaster1,closed,2020-08-20T23:28:05Z,2020-08-21T08:24:10Z,"Manual backport of below

Ref: https://github.com/pandas-dev/pandas/pull/35828#issuecomment-677917854

This is because we dropped 3.6 support on master (https://github.com/pandas-dev/pandas/pull/35214)

cc @simonjayhawkins 
"
682996506,35828,CI: Pyarrow Min Version on Travis 3.7 Locale Build,alimcmaster1,closed,2020-08-20T18:18:33Z,2020-08-21T08:28:06Z,"- [x] closes #35813

Conda seems to be installing pyarrow 0.11 for this env. 

Note pyarrow tests will now run on  Travis 3.7 Locale Build"
682819999,35825,DOC: Start 1.1.2,simonjayhawkins,closed,2020-08-20T15:35:24Z,2020-08-21T08:38:08Z,
683130320,35834,Backport PR #35825 on branch 1.1.x (DOC: Start 1.1.2),meeseeksmachine,closed,2020-08-20T22:19:11Z,2020-08-21T09:07:34Z,Backport PR #35825: DOC: Start 1.1.2
683116135,35833,Backport PR #35757 on branch 1.1.x (CI: Unpin Pytest + Pytest Asyncio Min Version),meeseeksmachine,closed,2020-08-20T21:47:34Z,2020-08-21T09:08:08Z,Backport PR #35757: CI: Unpin Pytest + Pytest Asyncio Min Version
640205930,34842,Infer filesystem from path when writing a partitioned DataFrame to remote file systems using pyarrow,kylase,closed,2020-06-17T07:25:11Z,2020-08-21T15:58:14Z,"Infer the file system to be passed to pyarrow based on the path provided.

- [x] closes #34841
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
640186990,34841,BUG: to_parquet write partitioned DataFrame to local filesystem for other filesystems (e.g. S3),kylase,closed,2020-06-17T06:49:42Z,2020-08-21T15:59:13Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

remote = ""s3://some-pandas-test-bucket/table""

pd.DataFrame([[1, 'a'], [1, 'b']], columns=['col_one', 'col_two']).to_parquet(remote, partition_cols=['col_two'])
```

#### Problem description

When a DataFrame is to be saved in object file system such as S3 as partitioned Parquet files, it saves the files in local file system instead, only a 0 byte object of is created, whereas the files are stored in the working directory as `s3:`

```
(pandas-dev) ➜  pandas git:(fix/parquet-write-dataset-s3) ✗ ls -R s3:
some-pandas-test-bucket

s3:/some-pandas-test-bucket:
table

s3:/some-pandas-test-bucket/table:
col_two=a col_two=b

s3:/some-pandas-test-bucket/table/col_two=a:
2a386111fd44400b8de1901254a7c485.parquet

s3:/some-pandas-test-bucket/table/col_two=b:
aefb0d5b907a415d943b6bcf59f2521d.parquet
```
On S3
```
(pandas-dev) ➜  pandas git:(fix/parquet-write-dataset-s3) ✗ aws s3 ls s3://some-pandas-test-bucket
2020-06-17 14:44:00          0 table
```

#### Expected Output

```
(pandas-dev) ➜  pandas git:(fix/parquet-write-dataset-s3) ✗ aws s3 ls s3://some-pandas-test-bucket --recursive --human-readable --summarize
2020-06-17 14:46:30    0 Bytes table
2020-06-17 14:46:30    1.3 KiB table/col_two=a/291b3f0d1d1f4eab9d6358e39ac47631.parquet
2020-06-17 14:46:30    1.3 KiB table/col_two=b/834e872441224885a5089554524d3b12.parquet
```

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : 5fdd6f50ac5ac2de939ef5b849e4c41e27e623a3
python           : 3.8.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.5.0
Version          : Darwin Kernel Version 19.5.0: Tue May 26 20:41:44 PDT 2020; root:xnu-6153.121.2~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : None.UTF-8

pandas           : 1.1.0.dev0+1887.g5fdd6f50a.dirty
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1.post20200616
Cython           : 0.29.20
pytest           : 5.4.3
hypothesis       : 5.16.1
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fastparquet      : 0.4.0
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0


</details>
"
682300300,35815,"CI/DOC: unpin sphinx, fix autodoc usage",fangchenli,closed,2020-08-20T02:14:50Z,2020-08-21T16:06:02Z,"- [x] closes #35138
"
657331930,35285,Incorrect behavior in `pandas.merge` function,YarShev,closed,2020-07-15T13:01:29Z,2020-08-21T17:04:36Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas
import numpy as np

test_data = np.random.uniform(0, 100, size=(2 ** 7, 2 ** 6))
test_data2 = np.random.uniform(0, 100, size=(2 ** 6, 2 ** 6))

pandas_df = pandas.DataFrame(
   test_data,
   columns=[""col{}"".format(i) for i in range(test_data.shape[1])],
   index=pandas.Index(
   [i for i in range(1, test_data.shape[0] + 1)], name=""key""
   ),
)
pandas_df2 = pandas.DataFrame(
   test_data2,
   columns=[""col{}"".format(i) for i in range(test_data2.shape[1])],
   index=pandas.Index(
   [i for i in range(1, test_data2.shape[0] + 1)], name=""key""
   ),
)

pandas_result = pandas_df.merge(pandas_df2,how=""left"",right_on=""key"",left_index=True,sort=False)
pandas_result2 = pandas_df.merge(pandas_df2,how=""inner"",right_on=""key"",left_index=True,sort=False)

pandas_result
     key     col0_x     col1_x     col2_x     col3_x     col4_x     col5_x     col6_x     col7_x  ...    col55_y    col56_y    col57_y    col58_y    col59_y    col60_y    col61_y    col62_y    col63_y
1.0    1  22.268792  91.710456  35.425585   5.998272   7.616223  50.855316  70.778027  39.345351  ...  79.470662   1.139462  82.394800  49.279405  71.492669  19.118798   8.338031  15.240684  96.935519
2.0    2  24.359239  24.789241  83.329200  24.369174  92.494368  48.520427  73.401821  47.736986  ...  77.504507  83.228024  16.743863  28.598973  89.282114  24.783087  94.681264  76.430020  49.957027
3.0    3  66.973326  85.911902  85.839220  39.444853  43.164051   0.711455   1.728169  67.356380  ...  82.361473  69.188814  19.254839  80.721210  97.038337  50.344042  47.969856  33.863619  22.711405
4.0    4  64.578630  64.185421  97.307670  10.697612  38.090190  63.198376  91.842608  58.068671  ...  10.917623  98.375547  69.298407  69.486654  26.956081  14.512972  38.663965  44.573867  67.612123
5.0    5  73.212801  41.605993  79.149803   0.545511  61.767295  80.562112  78.970619  89.752911  ...  36.965341  99.078379  72.797682  28.273150  50.379084  14.131667  33.817043  64.267731  15.687369
..   ...        ...        ...        ...        ...        ...        ...        ...        ...  ...        ...        ...        ...        ...        ...        ...        ...        ...        ...
NaN  124  13.955523  91.797658  44.563825  60.746861  28.904216  71.772330  28.841887  89.951210  ...        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN
NaN  125  49.694130  56.604940  92.628512  14.191864  31.197524  82.710209  95.428222  35.449014  ...        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN
NaN  126  58.950112  76.638796  89.459637  86.865600  93.866105   3.612650  88.072591  20.871411  ...        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN
NaN  127  76.472780  72.939919  24.263068  71.021804  89.470422  29.811838  14.484325  51.755485  ...        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN
NaN  128  97.720457  31.118757  85.992609  85.823341  69.280047  71.261342  62.747467  23.256187  ...        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN

[128 rows x 129 columns]

pandas_result2
        col0_x     col1_x     col2_x     col3_x     col4_x     col5_x     col6_x     col7_x     col8_x  ...    col55_y    col56_y    col57_y    col58_y    col59_y    col60_y    col61_y    col62_y    col63_y
key                                                                                                     ...
1    22.268792  91.710456  35.425585   5.998272   7.616223  50.855316  70.778027  39.345351  25.069931  ...  79.470662   1.139462  82.394800  49.279405  71.492669  19.118798   8.338031  15.240684  96.935519
2    24.359239  24.789241  83.329200  24.369174  92.494368  48.520427  73.401821  47.736986  32.137170  ...  77.504507  83.228024  16.743863  28.598973  89.282114  24.783087  94.681264  76.430020  49.957027
3    66.973326  85.911902  85.839220  39.444853  43.164051   0.711455   1.728169  67.356380  83.307993  ...  82.361473  69.188814  19.254839  80.721210  97.038337  50.344042  47.969856  33.863619  22.711405
4    64.578630  64.185421  97.307670  10.697612  38.090190  63.198376  91.842608  58.068671  17.910149  ...  10.917623  98.375547  69.298407  69.486654  26.956081  14.512972  38.663965  44.573867  67.612123
5    73.212801  41.605993  79.149803   0.545511  61.767295  80.562112  78.970619  89.752911  44.926515  ...  36.965341  99.078379  72.797682  28.273150  50.379084  14.131667  33.817043  64.267731  15.687369
..         ...        ...        ...        ...        ...        ...        ...        ...        ...  ...        ...        ...        ...        ...        ...        ...        ...        ...        ...
60   40.822611  50.875277  84.380647  26.325878  46.883415   0.048349  94.366177  10.860537  95.910254  ...  16.139270  16.442644  38.910856  52.139794  75.130737  79.407901   9.574825  43.741220  71.341946
61   79.372509  92.123416  60.440763  24.429257  58.681905  27.281294  24.816561  53.338771  37.340585  ...  17.580590  49.181221  23.445983  46.408847   1.805793  72.093458  46.068460  86.171202  67.535472
62   73.059258  86.432887  52.869162  26.113074  21.653439  18.971489  86.998840  36.259379  55.659895  ...  34.785796   2.524984  63.754044  21.315063  26.431136   9.647946   9.740197  55.826159  81.770851
63   73.177085  48.520687   6.165317  75.703392  32.593617  15.452280   0.227452   1.036644  90.657810  ...  30.676622   2.000542   6.701021  78.782964  50.210517  71.748162  43.102752  89.775608  51.033208
64   30.236389  10.706612  31.498764   0.872496  88.008391  65.306584  92.053531  98.306667  28.063389  ...  90.833733  35.615374  12.593278   4.371464  66.767380  11.140186  79.031274  23.489189  56.079695

[64 rows x 128 columns]
```

#### Problem description

My expectation was that both `pandas_result` and `pandas_result2` will preserve index with name `key` after performing the operations mentioned above. It turned out that `index` will be preserved if `how=""inner""`, whereas when `how=""left""` index will be moved into column.

#### Expected Output

```python
        col0_x     col1_x     col2_x     col3_x     col4_x     col5_x     col6_x     col7_x     col8_x  ...    col55_y    col56_y    col57_y    col58_y    col59_y    col60_y    col61_y    col62_y    col63_y
key                                                                                                     ...
1    22.268792  91.710456  35.425585   5.998272   7.616223  50.855316  70.778027  39.345351  25.069931  ...  79.470662   1.139462  82.394800  49.279405  71.492669  19.118798   8.338031  15.240684  96.935519
2    24.359239  24.789241  83.329200  24.369174  92.494368  48.520427  73.401821  47.736986  32.137170  ...  77.504507  83.228024  16.743863  28.598973  89.282114  24.783087  94.681264  76.430020  49.957027
3    66.973326  85.911902  85.839220  39.444853  43.164051   0.711455   1.728169  67.356380  83.307993  ...  82.361473  69.188814  19.254839  80.721210  97.038337  50.344042  47.969856  33.863619  22.711405
4    64.578630  64.185421  97.307670  10.697612  38.090190  63.198376  91.842608  58.068671  17.910149  ...  10.917623  98.375547  69.298407  69.486654  26.956081  14.512972  38.663965  44.573867  67.612123
5    73.212801  41.605993  79.149803   0.545511  61.767295  80.562112  78.970619  89.752911  44.926515  ...  36.965341  99.078379  72.797682  28.273150  50.379084  14.131667  33.817043  64.267731  15.687369
..         ...        ...        ...        ...        ...        ...        ...        ...        ...  ...        ...        ...        ...        ...        ...        ...        ...        ...        ...
124  13.955523  91.797658  44.563825  60.746861  28.904216  71.772330  28.841887  89.951210  69.801427  ...        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN
125  49.694130  56.604940  92.628512  14.191864  31.197524  82.710209  95.428222  35.449014  84.495449  ...        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN
126  58.950112  76.638796  89.459637  86.865600  93.866105   3.612650  88.072591  20.871411   9.433378  ...        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN
127  76.472780  72.939919  24.263068  71.021804  89.470422  29.811838  14.484325  51.755485  41.570147  ...        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN
128  97.720457  31.118757  85.992609  85.823341  69.280047  71.261342  62.747467  23.256187  87.411083  ...        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN

[128 rows x 128 columns]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.5
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 41.2.0
Cython           : None
pytest           : 5.4.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : 0.4.1
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.14.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.4.2
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : 3.6.1
tabulate         : None
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
657127275,35281,ENH: to_latex positional argument,SylvainLan,closed,2020-07-15T07:26:40Z,2020-08-21T17:04:36Z,"#### Is your feature request related to a problem?

I would like to have the possibility to specify the location of the table I'm saving, or instead of having 
```
\begin{table}
...
\end{table}
```

be able to have 
```
\begin{table}[h]
...
\end{table}
```

#### Describe the solution you'd like

I guess the solution would be to add another argument in the `to_latex` API which could take values in `(None, 'h', 'H', 'h!'...)`

#### Describe alternatives you've considered

I'm saving my tables to `.tex` files and then use the `\input{path/to/tex}` command, I did not find the possibility to give the [h] argument with input...

"
657546310,35293,BUG: upsampling returns less rows than expected,rwijtvliet,closed,2020-07-15T18:01:11Z,2020-08-21T17:04:37Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

I have a pandas Series with a (tz-localized) DateTimeIndex with one value per day, which I want to upsample to hourly values, so that each value is repeated 24 times (or 23 or 25, depending on summer/wintertime changeover).

```python
s = pd.Series([1,2,3,4], index=pd.date_range('2020-03-28 00:00:00', periods=4, freq='D', tz='Europe/Berlin'))
r = s.resample('H').ffill() #misses the last 23 hours of March
```
```
s:
2020-03-28 00:00:00+01:00    1
2020-03-29 00:00:00+01:00    2
2020-03-30 00:00:00+02:00    3
2020-03-31 00:00:00+02:00    4
Freq: D, dtype: int64

r:
2020-03-28 00:00:00+01:00    1
2020-03-28 01:00:00+01:00    1
2020-03-28 02:00:00+01:00    1
                            ..
2020-03-30 22:00:00+02:00    3
2020-03-30 23:00:00+02:00    3
2020-03-31 00:00:00+02:00    4  #<-- !
Freq: H, Length: 72, dtype: int64
```

#### Problem description

Each row in series `s` has the correct number of corresponding rows in series `r` - expect for the final row. The final row in `s` has only 1 corresponding row in `r`, where 24 would be expected.

#### Additional information / observations

a)

I've tried various combinations of having the original and/or the resampled timeseries be open or closed on the right side, but no setting gave me the wanted result.

b)

I assume the reason for the observed behaviour in `r`, above, is the following: I suppose that `freq == D` on a `DatetimeIndex` means, that the values are spaced one calendar day apart, and not, that they are valid for the entire calender day. Using a `PeriodIndex` instead does indeed solve this, see below.

However, using a `PeriodIndex` instead of a `DateTimeIndex` is not an option. It solves the problem with the missing hours, but loses the timezoneawareness that I need:
```python
s2 = pd.Series(s.values, pd.PeriodIndex(s.index, freq='D')) #misses timezoneawareness.
r2 = s2.resample('H').ffill() #does not miss last 23 hours of March, but is still incorrect.
```
```
s2:
2020-03-28    1
2020-03-29    2
2020-03-30    3
2020-03-31    4
Freq: D, dtype: int64

r2:
2020-03-28 00:00    1
2020-03-28 01:00    1
2020-03-28 02:00    1
                   ..
2020-03-31 21:00    4
2020-03-31 22:00    4
2020-03-31 23:00    4
Freq: H, Length: 96, dtype: int64
```

c)

Full disclosure: I've asked this questions before, on [stackoverflow](https://stackoverflow.com/questions/61761237/keep-24h-for-each-day-when-resampling-pandas-series-from-daily-to-hourly), but did not receive a satisfactory answer - just a workaround.

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en
LOCALE           : de_DE.cp1252

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
657456225,35290,BUG: Inversion of True is -2,gitPrinz,closed,2020-07-15T15:45:13Z,2020-08-21T17:04:37Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame()

df.equals(df)
Out: True

~df.equals(df)
Out: -2

```

#### Problem description

The inversion of a boolean result from df1.equals(df2) is an integer which will be recognized as True.

#### Expected Output

False

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-61-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 9.0.1
setuptools       : 41.6.0
Cython           : None
pytest           : 5.3.2
hypothesis       : None
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 0.999999999
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.2
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
658970534,35320,BUG:New epoch in matplotlib 3.3 causes plotting issues,selasley,closed,2020-07-17T07:39:40Z,2020-08-21T17:04:38Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import matplotlib.pyplot as plt

ndx = pd.to_datetime(['2020-07-1 04:01:34', '2020-07-2 04:03:33', '2020-07-3 04:05:33',
                      '2020-07-4 04:07:35', '2020-07-5 04:09:35']) 
df = pd.DataFrame(index=ndx, data={'Vsw': [389.4, 388.5, 384.5, 384.7, 386.6]})
print(df.info())
ax1 = df['Vsw'].plot(xlim=['2020-7-1', '2020-7-6'])
print(ax1.get_xlim(), '\n\n\n')
plt.show()

# plot without specifying x limits
ax2 = df['Vsw'].plot()
print(ax2.get_xlim(), '\n\n\n')
plt.show()

# plot from a DataFrame with a pd.date_range index
df = pd.DataFrame(index=pd.date_range('2020-7-1', '2020-7-5', freq='D'),
                  data={'Vsw': [389.4, 388.5, 384.5, 384.7, 386.6]})
print(df.info())
ax3 = df['Vsw'].plot(xlim=['2020-7-1', '2020-7-6'])
print(ax3.get_xlim())
plt.show()

```

#### Problem description

See https://matplotlib.org/3.3.0/users/whats_new.html#dates-use-a-modern-epoch

The first plot is blank. get_xlim returns (18444.0, 18449.0)  The second plot contains the values being plotted and get_xlim returns (737606.9674762732, 737611.3736001157).  The third plot shows the plotted values and get_xlim returns (18444.0, 18449.0).

If the epoch is set to the old epoch value with 
import matplotlib.dates as mpld
mpld.set_epoch('0000-12-31T00:00:00')
all three plots show the plotted values.  get_xlim returns (737607.0, 737612.0) for the first plot, (737606.9674762732, 737611.3736001157) for the second and (18444.0, 18449.0) for the third.

#### Expected Output

The three plots should show the values being plotted whether or not xlim is specified in the plot() command or the DatetimeIndex has a Freq value

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 65090286608602ba6d67ca4a29cf0535156cd509
python           : 3.8.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Sun Jul  5 00:43:10 PDT 2020; root:xnu-6153.141.1~9/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0.dev0+2131.g650902866
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1
Cython           : 0.29.20
pytest           : None
hypothesis       : None
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.50.1

</details>
"
658156909,35305,QST:shape mismatch: objects cannot be broadcast to a single shape,pooja565,closed,2020-07-16T12:33:11Z,2020-08-21T17:04:38Z,"Hi,

Im having a problem using bar chart in python is  ""shape mismatch: objects cannot be broadcast to a single shape"".
and the code is:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


df=pd.read_csv('2017Jaipur_month accidents.csv')

data1=pd.DataFrame(df,columns=['acc_2017','acc_2016','Months'])
data1

index=np.arange(2,13)
bar_width=0.25


acc_2017=plt.bar(index,data1['acc_2017'],color='blue',width=0.25,label='Death in 2017')
acc_2016=plt.bar(index+0.25,data1['2016'],color='red',width=0.25, label='Death in 2016')

plt.title('2016-17 Death in Accident')
plt.xticks(index, data1.Months, rotation=90)

plt.xlabel('Time accidents')
plt.ylabel('no of accidents')
plt.show()


and the error is:
ValueError                                Traceback (most recent call last)
<ipython-input-107-3c2ff406e777> in <module>
     13 
     14 
---> 15 acc_2017=plt.bar(index,data1['acc_2017'],color='blue',width=0.25,label='Death in 2017')
     16 acc_2016=plt.bar(index+0.25,data1['2016'],color='red',width=0.25, label='Death in 2016')
     17 

C:\ProgramData\Anaconda3\lib\site-packages\matplotlib\pyplot.py in bar(x, height, width, bottom, align, data, **kwargs)
   2439     return gca().bar(
   2440         x, height, width=width, bottom=bottom, align=align,
-> 2441         **({""data"": data} if data is not None else {}), **kwargs)
   2442 
   2443 

C:\ProgramData\Anaconda3\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs)
   1597     def inner(ax, *args, data=None, **kwargs):
   1598         if data is None:
-> 1599             return func(ax, *map(sanitize_sequence, args), **kwargs)
   1600 
   1601         bound = new_sig.bind(ax, *args, **kwargs)

C:\ProgramData\Anaconda3\lib\site-packages\matplotlib\axes\_axes.py in bar(self, x, height, width, bottom, align, **kwargs)
   2372         x, height, width, y, linewidth = np.broadcast_arrays(
   2373             # Make args iterable too.
-> 2374             np.atleast_1d(x), height, width, y, linewidth)
   2375 
   2376         # Now that units have been converted, set the tick locations.

<__array_function__ internals> in broadcast_arrays(*args, **kwargs)

C:\ProgramData\Anaconda3\lib\site-packages\numpy\lib\stride_tricks.py in broadcast_arrays(*args, **kwargs)
    262     args = [np.array(_m, copy=False, subok=subok) for _m in args]
    263 
--> 264     shape = _broadcast_shape(*args)
    265 
    266     if all(array.shape == shape for array in args):

C:\ProgramData\Anaconda3\lib\site-packages\numpy\lib\stride_tricks.py in _broadcast_shape(*args)
    189     # use the old-iterator because np.nditer does not handle size 0 arrays
    190     # consistently
--> 191     b = np.broadcast(*args[:32])
    192     # unfortunately, it cannot handle 32 or more arguments directly
    193     for pos in range(32, len(args), 31):

ValueError: shape mismatch: objects cannot be broadcast to a single shape
  plz help to find out the issue."
662521267,35362,"Pandas is the most difficult data processing tool in Python, which is very inhumane and difficult to use",wanghuqiang123,closed,2020-07-21T03:13:55Z,2020-08-21T17:04:39Z,"Pandas is the most difficult data processing tool in Python, which is very inhumane and difficult to use"
664617406,35395,Random Sample Not Working,TanushGoel,closed,2020-07-23T16:36:29Z,2020-08-21T17:04:40Z,"I think there's an issue with the sample function. It displays the randomized dataframe with a different heading, but when you actually get the value of the dataframe at a certain index, it displays the same value as before the random sample.

<img width=""651"" alt=""Screen Shot 2020-07-23 at 9 30 38 AM"" src=""https://user-images.githubusercontent.com/47517229/88312776-68fa1700-ccc7-11ea-94fc-00087cc743e7.png"">

<img width=""693"" alt=""Screen Shot 2020-07-23 at 9 31 49 AM"" src=""https://user-images.githubusercontent.com/47517229/88312788-6dbecb00-ccc7-11ea-9a6a-353951e56fca.png"">

How can I fix this? 
"
664104280,35387,Display dataframe name or title when using display(),spizwhiz,closed,2020-07-22T23:12:58Z,2020-08-21T17:04:40Z,"Requesting ability to display name of dataframe above dataframe when using display(df)

I have tried df.name = ""Name""

and display(df, display_id = ""Name"")

Neither results in any kind of identifier for the dataframe when using the display() function in Jupyter Notebook.
"
663871050,35383,Drop similar rows of dataframe except for one column,paulthemagno,closed,2020-07-22T15:56:12Z,2020-08-21T17:04:40Z,"It seems a trivial question, but I don't find anything on the web. I have a dataframe with **inconsistent rows**. In the example below `primary_key` is the primary key so it has to be once in dataset, while `info` is an information of `primary_key`. While for **exactly repeated rows** I want to keep only one of them using a function like `df.drop_duplicates(keep = 'first')`, I don't know which keep in the case in which primary keys (`primary_key`) are the same so refer to the same item, but `info` is different, because they are inconsistent information. In this case I'd like to remove all of these rows with a function like `df.drop_duplicates(subset = ""primary_key"" , keep = False)`. I don't know how to do this kind of filter because I want to keep 1 row if one case and 0 rows in the other, depending on the values of a specific column within groups of _similar rows_.
 
### Desired behaviour
In:
```bash
     primary_key    info
0    a              1 
1    a              1 #duplicated row of index 0
2    a              2 #similar row of indexes 0 and 1 but inconsistent with info field
3    b              1 
4    b              1 #duplicated row of index 3
```

Out:
```bash
     primary_key    info
1    b              1  #the only primary key with the same value on info field
```

Thank you"
665666953,35413,BUG: When plotting a pandas dataframe year is wrong,smeznaric,closed,2020-07-25T21:55:56Z,2020-08-21T17:04:41Z,"- [ x] I have checked that this issue has not already been reported.

- [x ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.


---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import matplotlib.pyplot as plt

# Save the attached file somewhere and open it as follows
file_name = 'your_path'
df = pd.read_csv(file_name, index_col = 'date', parse_dates = True)
df.plot()

plt.show()

```

#### Problem description

The year is displayed as in 3900s. See picture:

![image](https://user-images.githubusercontent.com/28095296/88466980-e68d7500-cec9-11ea-9155-3400112e7c01.png)

I have noticed a few things:

- Generating random data and keeping the index the same does not resolve the issue
- Trying to recreate the index from scratch but using the same date does not resolve the issue
- Plotting using plotly backend resolves the issue
- Doing df.to_period('D').plot() also seems to resolve the issue


#### Expected Output

Year should not be in 3900s.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.104-microsoft-standard
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 44.0.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.16.1
pandas_datareader: 0.9.0
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.2
matplotlib       : 3.3.0
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : 0.16.0
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
664955775,35400,BUG: Groupby Is Changing Date Format,marekre90,closed,2020-07-24T06:49:08Z,2020-08-21T17:04:41Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```
df = pd.read_csv(""Uk-Orders.csv"",parse_dates=['WO Created'])
df=df.rename(columns={""WO Created"":""WO-Created"",""Sale Price"":""Sale-Price""})
df['Sale-Price']=df['Sale-Price'].str.replace(',', '').astype('float64')
df['WO-Created'] =  pd.to_datetime(df['WO-Created']).dt.normalize()
df=df.groupby('WO-Created',sort=False)['Sale-Price'].sum().reset_index()
```

#### Problem description

I have a dataset that has two columns. WO created as starting on 04-01-2016 and are populated until 06-29-2020. 
After running df=df.groupby('WO-Created',sort=False)['Sale-Price'].sum().reset_index(), the last row is giving me dates like 2020-12-06, which is incorrect, because there is no such date range in my report (no December in my dataset). I suppose it has something to do with date formatting, but I am not sure how to fix it.

#### Expected Output

I would expect dates to be summarized correctly.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
669684061,35495,DOC: pd.read_sql() connection disposal for str type connections ,AdamSpannbauer,closed,2020-07-31T10:47:13Z,2020-08-21T17:04:42Z,"This is in between a documentation issue and a general question.

#### Location of the documentation

[`pd.read_sql()`](https://github.com/pandas-dev/pandas/blob/v1.1.0/pandas/io/sql.py#L441-L443)

#### Documentation problem

The documentation notes that if the `con` is a `SQLAlchemy connectable` that the user is responsible for disposing/closing.  The documentation doesn't note whether the connection is closed if the user provides a `str` to the `con` parameter.  In my quick search, I couldn't find a reference to `.dispose()`/`.close()` in the source code when the user provides a `str` type `con`.  Is this connection being closed? Is it being left up to garbage collection as noted in [SQLAlchemy docs](https://docs.sqlalchemy.org/en/13/core/connections.html#engine-disposal)?

#### Suggested fix for documentation

I think a note about connection closure for `str` type `con`s would be a nice addition.  Right now, if I'm understanding correctly, the connection is being garbage collected and the user needs to infer from the docs that they don't need to worry about an open connection.

Current wording:

> The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable.

Potential wording:

> The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable; str connections are closed automatically."
668210353,35472,ENH: Method on pd.Period to find the nth weekday in a period. ,smithto1,closed,2020-07-29T22:28:44Z,2020-08-21T17:04:42Z,"#### Is your feature request related to a problem?

It should be easy to find the nth weekday of period (usually a month). This kind of operation comes up all the time in financial contracts (i.e. futures and options expiring on the 3rd Wed of the month) and other use-cases (i.e. US election dates).

#### Describe the solution you'd like

Add an `nth_weekday` method to the `pd.Period` object to handle this operation. It could then be extended to the `PeriodArray`. 

`nth_weekday` should work forward and backwards with Pythonic-indexing (n=0 gives you date of first weekday, n=-1 gives you date of last weekday) 

`nth_weekday` should have a flag to to enforce that the returned date has to be inside the period, or if you let it go outside the period bounds. It should default to enforcing the period bounds. 

#### API breaking implications

This will add a new method to the `pd.Period`  and the `PeriodArray`. Proposed name is `nth_weekday` but that can be changed. It shouldn't have any other impact on the API. 

#### Describe alternatives you've considered

A general search of the internet did not find any convenient solution for this in all of Python. These looked like the best solutions available, and they're not great:
https://stackoverflow.com/questions/11883058/nth-weekday-calculation-in-python-whats-wrong-with-this-code
https://stackoverflow.com/questions/61106105/python-how-to-find-the-nth-weekday-of-the-year

#### Additional context

Below is a quick mock-up of the method (unoptimised, undocumented, but illustrates the functionality). 

```python

import pandas as pd

class Period_(pd.Period):
    def nth_weekday(self, n:int, weekday:int, within_period:bool=True) -> pd.Timestamp :
        
        assert weekday >= 0
        assert weekday <= 6
        
        if n >= 0:
            diff = weekday - self.start_time.weekday()
            diff += 7 if diff < 0 else 0            
            diff = diff + n * 7
            
            res = self.start_time + pd.Timedelta(days=diff)
            
            if within_period:
                if res > self.end_time:
                    res = pd.NaT
            return res
        
        if n < 0:
            diff = weekday - self.end_time.weekday()
            diff -= 7 if diff > 0 else 0
            diff = diff + (n+1) * 7
            
            res = self.end_time.normalize() + pd.Timedelta(days=diff)
            
            if within_period:
                if res < self.start_time:
                    res = pd.NaT
            return res
```

Some examples of use: 
```
# third Wednesday
Period_('2020-09', 'M').nth_weekday(n=2, weekday=2)

# first Friday
Period_('2020-09', 'M').nth_weekday(n=0, weekday=4)

# last Monday
Period_('2020-09', 'M').nth_weekday(n=-1, weekday=0)

# sixth Tuesday --> returns NaT when within_period=True 
Period_('2020-09', 'M').nth_weekday(n=6, weekday=1)

# sixth Tuesday --> returns a date in the next month if user explicitly 
# specifies within_period=False
Period_('2020-09', 'M').nth_weekday(n=6, weekday=1, within_period=False)

```

Some details would need to be ironed out on how to handle frequencies less than one day (i.e. an Hour). The mock example has some protection because `within_period` defaults to `True`, so any op that goes to a different day (not the day the hour occurrs on), it will return `NaT`. 

```
# Trying to shift from a smaller freq can return NaT
Period_('2020-01-01 15:00', 'H').nth_weekday(n=0, weekday=3)

Period_('2020-01-01 15:00', 'H').nth_weekday(n=1, weekday=2)
```

The mock still needs handling of corner cases: if you ask for the `n=0` instance of the day that the hour occurs on, you get the start_time back (rather than `NaT`). Will need to put in some thought for handling these corner cases. 

```
# This hour period occurs on a Wed. If I ask for the n=0 Wed of that Period, it gives me the start_time back. 
Period_('2020-01-01 15:00', 'H').nth_weekday(n=0, weekday=2)
```

I'm happy to work on this but wanted to raise an issue for discussion in case there already exists such a function somewhere (couldn't find one in the pandas docs) or if it has already been proposed and decided against. "
673153438,35557,BUG: 1.1.0 breaks custom indexer support in groupby().rolling(),WhistleWhileYouWork,closed,2020-08-04T23:52:11Z,2020-08-21T17:04:43Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
class SimpleIndexer(pd.api.indexers.BaseIndexer):
    ''' Custom `Indexer` duplicating basic fixed length windowing '''
    def get_window_bounds(self, num_values=0, min_periods=None, center=None, closed=None):
        min_periods = self.window_size if min_periods is None else 0
        end = np.arange(num_values, dtype=np.int64) + 1
        start = end.copy() - self.window_size
        #---- Clip to `min_periods`
        start[start < 0] = min_periods
        return (start, end)

x = pd.DataFrame({'a': [1.0,2.0,3.0,4.0,5.0] * 3}, index=[0]*5+[1]*5+[2]*5)
x

Out: 
     a
0  1.0
0  2.0
0  3.0
0  4.0
0  5.0
1  1.0
1  2.0
1  3.0
1  4.0
1  5.0
2  1.0
2  2.0
2  3.0
2  4.0
2  5.0

x.groupby(x.index).rolling(SimpleIndexer(window_size=3), min_periods=1).sum()

Out: 
       a
0 0  1.0
  0  2.0
  0  3.0
  0  4.0
  0  5.0
1 1  1.0
  1  2.0
  1  3.0
  1  4.0
  1  5.0
2 2  1.0
  2  2.0
  2  3.0
  2  4.0
  2  5.0
```

#### Problem description

`groupby().rolling()` does not use custom indexer supplied in `window=` likely as a result of #34052 where a `GroupbyRollingIndexer` is used instead.

The output data is always the same as the input regardless of the windowing function.

1.0.5 does not have this behavior.

#### Expected Output

```python
x.groupby(x.index).rolling(window=3, min_periods=1).sum()

Out: 
        a
0 0   1.0
  0   3.0
  0   6.0
  0   9.0
  0  12.0
1 1   1.0
  1   3.0
  1   6.0
  1   9.0
  1  12.0
2 2   1.0
  2   3.0
  2   6.0
  2   9.0
  2  12.0
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-59-generic
Version          : #53~18.04.1-Ubuntu SMP Thu Jun 4 14:58:26 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2
setuptools       : 49.2.1.post20200802
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.14.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.50.1
</details>
"
672306607,35531,QST:Adding NOT NULL Columns using to_sql to MSSQL database using Pandas SqlAlchemy,minhajsultana1,closed,2020-08-03T19:55:03Z,2020-08-21T17:04:43Z,"
I am trying to read table from SQL and writing to ORACLE using pandas.
Everything works except the columns are created as NULL in Oracle.
Any help is greatly appreciated."
671867372,35523,ENH: Allow pd.Series to fill NA with a callable,ligz08,closed,2020-08-03T07:33:01Z,2020-08-21T17:04:43Z,"#### Is your feature request related to a problem?

I wanted to do Series additions/subtractions where two Series objects' index may not align. I was using the [`pd.Series.add()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.add.html) function, which first aligns two Series' by their indices, and then do the adding at each index -- this is great. However, when dealing with NaNs generated at misaligned indices, I want to use a callable (e.g. a function or a class constructor) with a Series item's index and/or value as arguments to fill the NaNs. Right now Pandas.Series doesn't support such functionality, the `fill_value` argument for the  `pd.Series.add()` function can be a float number at best.

Example:
```python
idx1 = [1,2,3]
idx2 = [2,3,4]
s1 = pd.Series((_**2 for _ in idx1), index=idx1, name='square')
s2 = pd.Series((_**2 for _ in idx2), index=idx2, name='square')
```
Hopefully with
```python
result = s1.add(s2, fill_value=lambda idx, val: idx**2)
```
the holes introduced when aligning `s1` and `s2` can be filled by calling the `lambda` function on the NA items in the Series, so eventually the `result` is effectively `pd.Series([1,4,9,16]) + pd.Series([1,4,9,16])`, and looks like 
```python
result
>>>
1     2
2     8
3    18
4    32
```

#### Describe the solution you'd like

Whenever a Series needs to fill NAs, e.g. `pd.Series.fillna()`, `pd.Series.align()`, `pd.Series.add()` etc, the `fill_value` argument should be able to accept a callable that takes two arguments `idx` and `val`, and call it with the Series' (index, value) pair as arguments to fill NAs.

#### API breaking implications

This may break the API behavior in (imo a rare) case a user intentionally wants to put callable objects in a Series to fill NAs. For example:
```python
s = pd.Series([min, max, sum, np.nan])
result = s.fillna(max)
```
Current behavior
```python
result
>>>
0    <built-in function min>
1    <built-in function max>
2    <built-in function sum>
3    <built-in function max>
```

New behavior after the proposed change:
```python
result
>>>
0    <built-in function min>
1    <built-in function max>
2    <built-in function sum>
3                          3
```
because at item 3, calling `max(3, np.nan)` results in `3`.

#### Describe alternatives you've considered

A workaround for `pd.Series.fillna()` but can use a callable to generate fill values is to create a `backup` Series separately with that callable, and use `pd.Series.mask()` instead. Suppose `func = lambda idx, val: idx**2`
```python
backup = pd.Series((func(idx, val) for idx, val in s.items()), index=s.index)
result = s.mask(pd.isna, other=backup)
result
>>>
0    <built-in function min>
1    <built-in function max>
2    <built-in function sum>
3                          9
```
Above is just for the `pd.Series.fillna()` function, and `.add()` and `.align()` functions can use similar approaches, but obviously it gets quite tedious."
675540135,35631,skiprows function adding extra column in pandas,ajinkya976,closed,2020-08-08T15:12:20Z,2020-08-21T17:04:44Z,"I am new to python. I am trying to read excel files and merge into master excel.

I want to skip first 3 rows in my excel which contains heading and just take rest of rows.

i am using skiprows=3 in pd.read_excel() method, it is skipping first 3 rows but the problem is, it is including one extra column with name 2 in my excel.

if i mention skiprows=2 then that extra column (2) is not coming. but empty row in excel is getting added with ""Unnamed"". if i mention skiprows=3 it is not adding ""Unamed"" row but adding extra column with name 2.

can someone please help me out in this. below is my code.

```
import os
import pandas as pd

if os.path.isfile('total_sales.xlsx'):
    os.remove('total_sales.xlsx')

cwd = os.path.abspath('')
files = os.listdir(cwd)

df = pd.DataFrame() #create an empty dataframe df for storing the data for master spreadsheet
cols = [0,1,2,3,4,6,7,8,9,10,11]
for file in files:
    if file.endswith('.xlsx'):
        df=df.append(pd.read_excel(file,skiprows=3,usecols=cols))


print(cwd)
print(df.head())
print(df.shape)

df.to_excel('total_sales.xlsx')


```

please find below master excel with output.

[![enter image description here][1]][1]


  [1]: https://i.stack.imgur.com/Y3MNB.png"
674879859,35599,"BUG: After sort_values() is executed, a large block of memory is not freed",wjsi,closed,2020-08-07T09:12:50Z,2020-08-21T17:04:44Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python                                                                                                                                               
import psutil
import pandas as pd

print(psutil.Process().memory_info().rss)  # outputs 77533184

# first execution
arrow_ser = pd.Series(['abcdefg'] * (10 * 1024 ** 2))
arrow_ser = arrow_ser.sort_values()  # same behavior as sort_values(inplace=True)
del arrow_ser
print(psutil.Process().memory_info().rss)  # outputs 686264320

# second execution
arrow_ser = pd.Series(['abcdefg'] * (10 * 1024 ** 2))
arrow_ser = arrow_ser.sort_values()
del arrow_ser
print(psutil.Process().memory_info().rss)  # outputs 686456832
```

#### Problem description

When calling ``sort_values()`` on dataframes or series, we discover that a large block of memory is allocated and not freed even if all related objects are deleted. When sort_values() is executed again, the size of allocated memory does not increase significantly. When the line of ``sort_values()`` is commented out, memory sizes are ok. We guess if there is a cache or memory pool when calling ``sort_values()``, but we cannot spot it.

We test ``sort_values()`` under 1.1.0, 1.0.5 and 0.24.1, and they share the same behavior. The test environment is an independent conda env without MKL, thus MKL buffers is not a problem.

#### Expected Output

The memory shall be freed once the related object is deleted, or a method cleaning the caches is needed.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Sun Jul  5 00:43:10 PDT 2020; root:xnu-6153.141.1~9/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : zh_CN.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
673798042,35576,ENH: way to opt-in to StringDtype instead of object dtype in read_csv(),chrish42,closed,2020-08-05T19:38:07Z,2020-08-21T17:04:44Z,"I got bit recently again by the mixed types DtypeWarning while processing a CSV file. I assume that at some point, when StringDtype is not experimental anymore, `read_csv()` will use that and won't need object dtype anymore, and so this potential problem source will go away.

In the meantime though, would it be possible to have an option in `read_csv()` to use StringDtype instead of the object dtype? Both for early adopters and people who want to try it out... and it would also be a nice migration path for when StringDtype is ready. Then, it would only be a matter of flipping the default for this switch. And for people who need to revert to object dtype for some reason, that would provide them a way to do that too at that time. Thoughts? Or is it still too soon for even experimental usage of StringDtype in `read_csv()`?

I'd be willing to create a pull request for this (at last for the Python version of the CSV parser)."
680325158,35771,BUG: df.replace() ,wjziv,closed,2020-08-17T15:31:36Z,2020-08-21T17:04:45Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame.from_dict(
    {
        'a_original': ['w', 'x', 'y', 'z', 'w', 'x', 'y', 'z'],
        'a_expected': ['w', 'x', 'y', 'z', 'w', 'x', 'y', 'z'],
        'a_result': ['w', 'x', 'y', 'z', 'w', 'x', 'y', 'z'],
        'b_original': ['w', 'x', 'y', 'z', 'z', 'y', 'x', 'w'],
        'b_expected': ['w', 'x', 'y', 'z', 'z', 'y', 'x', 'w'],
        'b_result': ['w', 'x', 'y', 'z', 'z', 'y', 'x', 'w']
    }
)

# Produces expectation.
df.replace(
    {column: {replace: None for replace in ['x', 'z']} for column in ['a_expected', 'b_expected']},
    inplace=True
)

# Does not produce expectation.
df.replace(
    {column: ['x', 'z'] for column in ['a_result', 'b_result']},
    {column: None for column in ['a_result', 'b_result']},
    inplace=True
)

>>
  a_original a_expected a_result b_original b_expected b_result
0          w          w        w          w          w        w
1          x       None        w          x       None        w
2          y          y        y          y          y        y
3          z       None        y          z       None        y
4          w          w        w          z       None        y
5          x       None        w          y          y        y
6          y          y        y          x       None        y
7          z       None        y          w          w        w
```

#### Problem description
`df.replace()` has multiple ways to define which values in which columns ought to be replaced. I expected the two uses of `df.replace()` above to be equivalent, but the second does not replace the values ""x"" and ""z"" in the declared columns with `None`. Instead, some form of filling/padding is used.

#### Expected Output
The second instance of `df.replace()` should be the same as the first; all instances of ""x"" and ""z"" should be replaced with `None`, but there seems to be a default usage of `method` underneath; this result remains the case, no matter what is entered into `method`.

#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.112+
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2018.9
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 49.2.0
Cython           : 0.29.21
pytest           : 3.6.4
hypothesis       : None
sphinx           : 1.8.5
blosc            : None
feather          : 0.4.1
xlsxwriter       : 1.3.3
lxml.etree       : 4.2.6
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.6.1 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 5.5.0
pandas_datareader: 0.8.1
bs4              : 4.6.3
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.6
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 2.5.9
pandas_gbq       : 0.11.0
pyarrow          : 1.0.0
pytables         : None
pytest           : 3.6.4
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.18
tables           : 3.4.4
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.1.0
xlwt             : 1.3.0
xlsxwriter       : 1.3.3
numba            : 0.48.0

</details>
"
676624209,35663,BUG: Styler `cells_ids` fails on second render,attack68,closed,2020-08-11T06:39:47Z,2020-08-21T17:04:45Z,"issue #35588 addressed the issue of `cell_ids` failing.

This bug reports a failure on second render(), due to `ctx` object, as a `defaultdict`, having a key inserted when it is queried, which then impacts the programming logic the second time it is run.

The solution is to move the query within conditional a few lines above.
"
682937600,35827,BUG: Same Directory copy.py incompatibility.,Tazarus,closed,2020-08-20T17:22:48Z,2020-08-21T17:04:46Z,"#### Problem description
If you are importing pandas into a script which shares a folder with another python script named copy.py pandas fails to import and the script cannot execute.

I would recommend changing the duplicate name in the pandas code to something less accidentally callable (perhaps simply add a few numbers or symbols afterward), but overall this is a very minor issue that I suspect very few people will run into.

- [Yes] I have checked that this issue has not already been reported.

- [Yes] I have confirmed this bug exists on the latest version of pandas.

- [No] (optional) I have confirmed this bug exists on the master branch of pandas.
Apologies unsure how.
---

#### Code Sample

```python
import pandas as pd
print(yes)
```

#### Expected Output
```python
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""C:\Coding\JetBrains\PyCharm 2020.2\plugins\python\helpers\pydev\_pydev_bundle\pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""C:\Coding\JetBrains\PyCharm 2020.2\plugins\python\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""C:/Users/Christian/PycharmProjects/pythonProject/Workspace.py"", line 1, in <module>
    import pandas as pd
  File ""C:\Coding\JetBrains\PyCharm 2020.2\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda3\envs\pythonProject\lib\site-packages\pandas\__init__.py"", line 52, in <module>
    from pandas.core.api import (
  File ""C:\Coding\JetBrains\PyCharm 2020.2\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda3\envs\pythonProject\lib\site-packages\pandas\core\api.py"", line 15, in <module>
    from pandas.core.arrays import Categorical
  File ""C:\Coding\JetBrains\PyCharm 2020.2\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda3\envs\pythonProject\lib\site-packages\pandas\core\arrays\__init__.py"", line 10, in <module>
    from pandas.core.arrays.interval import IntervalArray
  File ""C:\Coding\JetBrains\PyCharm 2020.2\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda3\envs\pythonProject\lib\site-packages\pandas\core\arrays\interval.py"", line 43, in <module>
    from pandas.core.indexes.base import ensure_index
  File ""C:\Coding\JetBrains\PyCharm 2020.2\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda3\envs\pythonProject\lib\site-packages\pandas\core\indexes\base.py"", line 1, in <module>
    from copy import copy as copy_func
ImportError: cannot import name 'copy' from 'copy' (C:\Users\Christian\PycharmProjects\pythonProject\copy.py)

```
#### Output of ``pd.show_versions()``

<details>
Note: This is my first attempt at bug reporting, and am very new to this, and I don't have as much time as I would have liked to ensure I'm posting this in the correct place. If this should be reported to JetBrains instead or simply somewhere else on the site please let me know and I'll do my best to update/fix it promptly. 

I am using Jetbrains Pycharm Professional IDE and found that having a python file named copy.py in the same project folder (windows folder) causes some kind of check/name confusion when pandas is importing. This happened by accident as I have been lately using copy.py as a dummy backup of working versions of code while working on improvements. I discovered the issue by accident.

A screenshot of the issue
![image](https://user-images.githubusercontent.com/69982590/90804338-7ae3cf80-e2f0-11ea-92a3-ca06ec65bc2c.png)

A screenshot of the issue not occurring under identical circumstances but when copy.py is renamed to copy2.py
![image](https://user-images.githubusercontent.com/69982590/90804431-96e77100-e2f0-11ea-8a6b-24b4de70b128.png)


[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]
INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : AMD64 Family 23 Model 8 Stepping 2, AuthenticAMD
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_Canada.1252
pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200814
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
682157908,35810,ENH: dataframe to markdown doesn't have option to hide index,bgmello,closed,2020-08-19T20:27:31Z,2020-08-21T17:04:46Z,"Suppose I have the following `pd.DataFrame`:

```
df = pd.DataFrame({'a': [1,2,3], 'b': [4,5,6]})
```

If I want to print it without the index, I could do:

To string:
```
df.to_string(index=False)
```

To html:
```
df.to_html(index=False)
```

To json:
```
df.to_json(index=False, orient='table')
```

But it fails in `to_markdown`:

```
df.to_markdown(index=False)

TypeError: tabulate() got an unexpected keyword argument 'index'
``` 

Shouldn't `to_markdown` method have the option to hide the index too?"
632579990,34620,BUG: blosc:snappy doesn't seem to be supported with blosc=1.19.0,twoertwein,closed,2020-06-06T16:42:56Z,2020-08-21T17:05:14Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example
The following example works with blosc=1.18.1 but fails with the latest blosc (1.19.0)

conda install pandas pytables blosc=1.18.1 # works
conda install pandas pytables blosc=1.19.0 # leads to failure

```python
import numpy as np
import pandas as pd

pd.DataFrame(np.random.rand(10, 3)).to_hdf(""test.hdf"", ""df"", complib=""blosc:snappy"")
```

#### Problem description
The error message is

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/twoertwe/miniforge3/envs/test/lib/python3.8/site-packages/pandas/core/generic.py"", line 2490, in to_hdf
    pytables.to_hdf(
  File ""/home/twoertwe/miniforge3/envs/test/lib/python3.8/site-packages/pandas/io/pytables.py"", line 279, in to_hdf
    with HDFStore(
  File ""/home/twoertwe/miniforge3/envs/test/lib/python3.8/site-packages/pandas/io/pytables.py"", line 521, in __init__
    raise ValueError(
ValueError: complib only supports ['zlib', 'lzo', 'bzip2', 'blosc', 'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:zlib', 'blosc:zstd'] compression.
```

The change log of blosc doesn't mentioned snappy: https://github.com/Blosc/c-blosc/releases

#### Expected Output

The compression should either work or `to_hdf` should mention that `blosc:snappy` doesn't work with `blosc=1.19.0`.

#### Output of ``pd.show_versions()``

<details>

I installed the latest blosc as above but for some reason  ``pd.show_versions()`` doesn't find it.

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-693.5.2.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.4
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.1.1.post20200529
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None


</details>
"
630781224,34571,BUG: problem with index set_names when names is a list,zsinnema,closed,2020-06-04T12:32:01Z,2020-08-21T17:05:14Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Problem description

I have a question: 

why raise a ValueError here https://github.com/pandas-dev/pandas/blob/1ce1c3c1ef9894bf1ba79805f37514291f52a9da/pandas/core/indexes/base.py#L1239 when values can be a list and thus len of the list can be > 1

and please also check this: https://github.com/pandas-dev/pandas/blob/1ce1c3c1ef9894bf1ba79805f37514291f52a9da/pandas/core/indexes/base.py#L1246 if it's a list why select the first element?"
638260251,34759,BUG: problem converting between new dtypes ('string' and 'Int8' / 'Int64'),SultanOrazbayev,closed,2020-06-14T00:48:14Z,2020-08-21T17:05:15Z,"- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of pandas.
---

Attempts to convert between the new nullable integer and string dtypes result in an error. This might be a different manifestation of the same underlying problem as in:  https://github.com/pandas-dev/pandas/issues/32073 and https://github.com/pandas-dev/pandas/issues/32234

```python
import pandas as pd
import numpy as np

test = {
    'a': [0, 1, np.nan],
    'b': ['0', '1', np.nan]
}

df = pd.DataFrame(test).astype({'a': 'Int8', 'b': 'string'})
df.b.astype('Int8')
# TypeError: data type not understood
```

Here's the traceback:
<details>
---------------------------------------------------------
TypeError               Traceback (most recent call last)
<ipython-input-13-fcd2b8190e95> in <module>
----> 1 df.b.astype('Int8')

~/myenv/lib/python3.7/site-packages/pandas/core/generic.py in astype(self, dtype, copy, errors)
   5696         else:
   5697             # else, only a single dtype is given
-> 5698             new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors)
   5699             return self._constructor(new_data).__finalize__(self)
   5700 

~/myenv/lib/python3.7/site-packages/pandas/core/internals/managers.py in astype(self, dtype, copy, errors)
    580 
    581     def astype(self, dtype, copy: bool = False, errors: str = ""raise""):
--> 582         return self.apply(""astype"", dtype=dtype, copy=copy, errors=errors)
    583 
    584     def convert(self, **kwargs):

~/myenv/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, filter, **kwargs)
    440                 applied = b.apply(f, **kwargs)
    441             else:
--> 442                 applied = getattr(b, f)(**kwargs)
    443             result_blocks = _extend_blocks(applied, result_blocks)
    444 

~/myenv/lib/python3.7/site-packages/pandas/core/internals/blocks.py in astype(self, dtype, copy, errors)
    605         if self.is_extension:
    606             # TODO: Should we try/except this astype?
--> 607             values = self.values.astype(dtype)
    608         else:
    609             if issubclass(dtype.type, str):

~/myenv/lib/python3.7/site-packages/pandas/core/arrays/string_.py in astype(self, dtype, copy)
    258                 return self.copy()
    259             return self
--> 260         return super().astype(dtype, copy)
    261 
    262     def _reduce(self, name, skipna=True, **kwargs):

~/myenv/lib/python3.7/site-packages/pandas/core/arrays/base.py in astype(self, dtype, copy)
    448             NumPy ndarray with 'dtype' for its dtype.
    449         """"""
--> 450         return np.array(self, dtype=dtype, copy=copy)
    451 
    452     def isna(self) -> ArrayLike:

TypeError: data type not understood
</details>

#### Expected Output

It should be possible to convert between different dtypes.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-957.12.1.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.4
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.1.1.post20200529
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fastparquet      : 0.4.0
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : 0.15.1
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
636415917,34702,BUG: DataFrame.loc[] returns a Series instead of a value,kuraga,closed,2020-06-10T17:13:56Z,2020-08-21T17:05:15Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame([ [ 17 ] ], index=pd.MultiIndex.from_tuples([('x', pd.Timestamp('2020-06-10')) ]), columns=[ 'A' ])

print(type(df.loc[ ('x', '2020-06-10'), 'A' ]))
print(type(df.loc[ ('x', pd.Timestamp('2020-06-10')), 'A' ]))
```

#### Problem description

```
<class 'pandas.core.series.Series'>
<class 'numpy.int64'>
```

#### Expected Output

```
<class 'numpy.int64'>
<class 'numpy.int64'>
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.32-calculate
machine          : x86_64
processor        : Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz
byteorder        : little
LC_ALL           : None
LANG             : ru_RU.utf8
LOCALE           : ru_RU.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : 0.29.15
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : 1.2.8
numba            : None
</details>
"
639666569,34824,DOC: to_sql accepting Engine vs. Connectable,gordthompson,closed,2020-06-16T13:30:06Z,2020-08-21T17:05:16Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html

#### Documentation problem

[Documentation](https://github.com/pandas-dev/pandas/blob/1ce1c3c1ef9894bf1ba79805f37514291f52a9da/pandas/core/generic.py#L2530) says that `con` is to be a ""sqlalchemy.engine.Engine or ..."" while [source](https://github.com/pandas-dev/pandas/blob/f34fd588cd9facfa82a792fb570fb8a56f11f6c7/pandas/io/sql.py#L539) says that `con` can be a ""SQLAlchemy connectable(engine/connection) or ...""

#### Suggested fix for documentation

Documentation should mention that a SQLAlchemy `Connection` is acceptable because some operations like [an upsert via a temporary table in SQL Server](https://stackoverflow.com/q/62388767/2144390) need to ensure that the same DBAPI connection is used for the entire operation. (SQL Server local temp tables are only visible to the connection/session that created them).
"
638986514,34805,BUG: Inconsistent behavior for isin() over columns with different datatypes,dmarx,closed,2020-06-15T16:32:20Z,2020-08-21T17:05:16Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample

```python
s_obj = pd.Series(['1', '2', '3'])
s_int = pd.Series([1, 2, 3])

s_obj.isin(s_int).sum()  # 0 
s_int.isin(s_obj).sum() # 3 
s_obj.astype(np.int).isin(s_int).sum() # 3
```

#### Problem description

It is not uncommon for numeric fields to be read into a dataframe column as text. The behavior of the `isin` function is inconsistent in these cases, working as expected in the `s_int.isin(s_obj)` case, but failing in the `s_obj.isin(s_int)` case unless the text field is coerced to numeric first by the user. 

#### Expected Output

**Behavior should be consistent in both directions.** Either `isin()` should always fail when there is a type mismatch, or it should perform necessary coersions automatically. It would be helpful to throw a warning in either case to inform the user either that they should standardize their schemas or that a potentially costly coersion is going to happen automatically. At the very least, we could keep the current behavior and show the user a warning explaining that an empty result set could be due to a type mismatch. 

Middle ground solution: add something like a `coerce_if_necessary` option to `isin()`. Defaulting to False would return the current behavior (which preserves backwards compatibility and ensures we don't surprise users with expensive coersions). We could augment this behavior with a new warning, alerting users to the type mismatch and recommending solutions. Users who aren't comfortable with datatypes (or are just feeling lazy) could then set `coerce=True` to have necessary coersions attempted for them behind the scenes.

Interested to hear other's thoughts regarding what the preferred behavior should be.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 47.1.1.post20200604
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
640095325,34838,"BUG: cut() ignores labels, include_lowest and other parameters when bins are given by IntervalIndex",alexey-kry1,closed,2020-06-17T02:38:54Z,2020-08-21T17:05:17Z,"#### Code Sample, a copy-pastable example

```python
import pandas as pd
df = pd.DataFrame({'A': range(1, 10, 1)})

bins_s = [1, 5, 8]
bins_i = pd.IntervalIndex.from_tuples([(1, 5), (5, 8)])
labels = ['low', 'high']

df['A_s'] = pd.cut(df['A'], bins_s, labels=labels, include_lowest=True, right=False)
df['A_i'] = pd.cut(df['A'], bins_i, labels=labels, include_lowest=True, right=False)

print(df)
```

actual output
```
   A   A_s         A_i
0  1   low         NaN
1  2   low  (1.0, 5.0]
2  3   low  (1.0, 5.0]
3  4   low  (1.0, 5.0]
4  5  high  (1.0, 5.0]
5  6  high  (5.0, 8.0]
6  7  high  (5.0, 8.0]
7  8   NaN  (5.0, 8.0]
8  9   NaN         NaN

```
#### Problem description

The cut() implementation redirects to `Categorical.from_codes` when the bins are given by an IntervalIndex ignoring labels and producing results inconsistent with an equivalent call to cut with bins given by a sequence. 

#### Expected Output

One would expect `A_s` anb `A_i` to be identical given the equivalent bin specifications.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 47.1.1.post20200604
Cython           : None
</details>
"
639822183,34826,BUG: pandas.read_parquet no longer accepting a file-like object,DavidFarago,closed,2020-06-16T16:47:12Z,2020-08-21T17:05:17Z,"#### Code Sample, a copy-pastable example

```python
from io import BytesIO
import numpy as np
import pandas as pd
import pytest

def test_read_parquet_on_stream():
    stream = BytesIO()
    df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
    df.to_parquet(stream)
    result = pd.read_parquet(stream)
    assert np.array_equal(result.values, df.values)
```

#### Problem description

The documentation of `pandas.read_parquet()` (see [1]) says 

> Parameters: path: str, path object or file-like object

It is quite handy to be able to use a stream as parameter. It used to work with pandas version 1.0.3, but causes the error below for pandas version 1.0.4.

#### Expected Output

I expect the test to pass silently, which it does when using `pandas == 1.0.3`. 

#### Output 

But for `pandas == 1.0.4`, the test case fails due to `stream` not being a path-like object:

> 
> tests/util/test_parquet_util.py:388 (test_read_parquet_on_stream)
> def test_read_parquet_on_stream():
>         stream = BytesIO()
>         df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
>         df.to_parquet(stream)
> >       result = pd.read_parquet(stream)
> 
> tests/util/test_parquet_util.py:393: 
> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
> /usr/local/lib/python3.7/dist-packages/pandas/io/parquet.py:315: in read_parquet
>     return impl.read(path, columns=columns, **kwargs)
> /usr/local/lib/python3.7/dist-packages/pandas/io/parquet.py:131: in read
>     path, filesystem=get_fs_for_path(path), **kwargs
> /usr/local/lib/python3.7/dist-packages/pyarrow/parquet.py:1019: in __init__
>     self.paths = _parse_uri(path_or_paths)
> /usr/local/lib/python3.7/dist-packages/pyarrow/parquet.py:49: in _parse_uri
>     path = _stringify_path(path)
> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
> 
> path = <_io.BytesIO object at 0x7f07d0e77530>
> 
>     def _stringify_path(path):
>         """"""
>         Convert *path* to a string or unicode path if possible.
>         """"""
>         if isinstance(path, six.string_types):
>             return path
>     
>         # checking whether path implements the filesystem protocol
>         try:
>             return path.__fspath__()  # new in python 3.6
>         except AttributeError:
>             # fallback pathlib ckeck for earlier python versions than 3.6
>             if _has_pathlib and isinstance(path, pathlib.Path):
>                 return str(path)
>     
> >       raise TypeError(""not a path-like object"")
> E       TypeError: not a path-like object
> 
> /usr/local/lib/python3.7/dist-packages/pyarrow/util.py:84: TypeError
> 

#### Details

Besides pandas, my `requirements.txt` contains the following relevant dependencies:
```
pyarrow ~= 0.15.1
pytest ~= 5.3.5
pyyaml ~= 5.1.2
```

My Python versions are 3.6.10 and 3.7.7.

[1]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html"
642663582,34925,ENH: Use binary right shift as pipe operator,JulianWgs,closed,2020-06-21T23:44:18Z,2020-08-21T17:05:18Z,"#### Is your feature request related to a problem?

In R there is the pipe operator ""%>%"" which applies the function after the operator on the data before the operator. Pandas also follows this philosophy of method chaining. There is also a [pipe function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pipe.html). Here is a good comparison of R vs pandas: https://stmorse.github.io/journal/tidyverse-style-pandas.html

Pandas is missing a stylish pipe operator.

#### Describe the solution you'd like

```python
In [1]: import pandas as pd
   ...: import numpy as np
   ...:
   ...: def pipe(self, func):
   ...:     return func(self)
   ...:
   ...: pd.DataFrame.__rshift__ = pipe
   ...: pd.Series.__rshift__ = pipe

In [2]: df = pd.DataFrame({""A"": [0, 1], ""B"": [2, 3]})
   ...: df
Out[2]:
   A  B
0  0  2
1  1  3
```
The old style. Execution from right to left or with pipe function.
```python
In [3]: np.mean(np.sum(df))
Out[3]: 3.0

In [4]: df.pipe(np.sum).pipe(np.mean)
Out[4]: 3.0
```
With the pipe operator. Execution from left to right with pipe operator.
```python
In [5]: df >> np.sum >> np.mean
Out[5]: 3.0

In [6]: df >> print
   A  B
0  0  2
1  1  3
```
I think the pipe operator makes it much more minimal.

#### API breaking implications

See above.

#### Describe alternatives you've considered

I picked the binary shift operator, because it is not currently used within pandas. I think it resembles a pipe the best.

#### Additional context

This does not work out of the box if the function takes additional argumements. One could use a lambda function, but that does not look clean. I think this is also the bigget downfall of this proposal. 
"
641108314,34861,BUG: read_parquet doesn't accept anymore a file route starting with 'file://',jorloplaz,closed,2020-06-18T10:50:58Z,2020-08-21T17:05:18Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

```python
# Your code here
import pandas as pd
pd.read_parquet('file://<route_to_parquet_file>')
```

#### Problem description

When passed a path with the file protocol (e.g., `file:///home/Desktop/my_parquet.pq`) it throws the following error:

OSError: Passed non-file path: file:///home/Desktop/my_parquet.pq

#### Expected Output

Should read things fine. 

With Pandas 1.0.3 it does work perfectly, so there's something wrong when moving from 1.0.3 to 1.0.4.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-101-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : es_ES.UTF-8
LOCALE           : es_ES.UTF-8

pandas           : 1.0.4
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.0.post20200616
Cython           : 0.29.20
pytest           : 5.4.2
hypothesis       : None
sphinx           : None
blosc            : 1.7.0
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.4.2
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.49.1

</details>
"
640157637,34840,ENH: Use of NamedTuple instead of Tuple for MultiIndex,yohplala,closed,2020-06-17T05:41:22Z,2020-08-21T17:05:18Z,"#### Is your feature request related to a problem?

Simply from an ergonomic point of view, when using MultiIndex, it would be worth using a list of NamedTuple instead of Tuple, so that level name can be directly used to call index values.

#### Describe the solution you'd like

Currently,

```python
import pandas as pd
midx = pd.MultiIndex.from_product([['value1','value2'],[1]], names=['level1','level2'])
df = pd.DataFrame(0,index=[1,2],columns=midx)
df.columns[0]['level1']
```
returns:
```python
TypeError: tuple indices must be integers or slices, not str
```

Would this be a NamedTuple, this would return the correct result: 'value1'
Bests,

PS: and thanks for your awesome work!"
644797952,34975,BUG: Unexpected behaviour comparison dataframes with None values,rosekoopman,closed,2020-06-24T17:47:40Z,2020-08-21T17:05:19Z,"- [x ] I have checked that this issue has not already been reported.

- [ x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
np.nan == np.nan
# False

pd.Series(index=[0], data=[np.nan]) == pd.Series(index=[0], data=[np.nan])
# False

None == None
# True

pd.Series(index=[0], data=[None]) == pd.Series(index=[0], data=[None])
# False

```

#### Problem description

If None==None equals True, I had expected that pd.Series(index=[0], data=[None]) == pd.Series(index=[0], data=[None]) would also equal True.

I have pandas version 1.0.5

#### Expected Output

pd.Series(index=[0], data=[None]) == pd.Series(index=[0], data=[None])
# True

#### Output of ``pd.show_versions()``


</details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.10.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.171-105.231.amzn1.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.16.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : 2.4.3
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : None
tabulate         : 0.8.6
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None"
644566263,34969,BUG: NUMPY_IMPORT_ARRAY_RETVAL undeclared when installing pandas 0.18.1,pkaleta,closed,2020-06-24T12:12:49Z,2020-08-21T17:05:19Z,"I've suddenly started getting the following error when installing pandas 0.18.1 via pip on Python3.6:

```
building 'pandas.json' extension
  creating build/temp.linux-x86_64-3.6/pandas/src/ujson
  creating build/temp.linux-x86_64-3.6/pandas/src/ujson/python
  creating build/temp.linux-x86_64-3.6/pandas/src/ujson/lib
  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2 -fPIC -Ipandas/src/ujson/python -Ipandas/src/ujson/lib -Ipandas/src/datetime -Ipandas/src/klib -Ipandas/src -I/tmp/pip-install-8vres3tf/pandas/.eggs/numpy-1.19.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/include/python3.6m -I/var/lib/go-agent/pipelines/make-ci-01/code/.venv/include/python3.6m -c pandas/src/ujson/python/ujson.c -o build/temp.linux-x86_64-3.6/pandas/src/ujson/python/ujson.o -D_GNU_SOURCE -Wno-unused-function
  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2 -fPIC -Ipandas/src/ujson/python -Ipandas/src/ujson/lib -Ipandas/src/datetime -Ipandas/src/klib -Ipandas/src -I/tmp/pip-install-8vres3tf/pandas/.eggs/numpy-1.19.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/include/python3.6m -I/var/lib/go-agent/pipelines/make-ci-01/code/.venv/include/python3.6m -c pandas/src/ujson/python/objToJSON.c -o build/temp.linux-x86_64-3.6/pandas/src/ujson/python/objToJSON.o -D_GNU_SOURCE -Wno-unused-function
  In file included from /tmp/pip-install-8vres3tf/pandas/.eggs/numpy-1.19.0-py3.6-linux-x86_64.egg/numpy/core/include/numpy/ndarraytypes.h:1822:0,
                   from /tmp/pip-install-8vres3tf/pandas/.eggs/numpy-1.19.0-py3.6-linux-x86_64.egg/numpy/core/include/numpy/ndarrayobject.h:12,
                   from /tmp/pip-install-8vres3tf/pandas/.eggs/numpy-1.19.0-py3.6-linux-x86_64.egg/numpy/core/include/numpy/arrayobject.h:4,
                   from pandas/src/ujson/python/objToJSON.c:40:
  /tmp/pip-install-8vres3tf/pandas/.eggs/numpy-1.19.0-py3.6-linux-x86_64.egg/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning ""Using deprecated NumPy API, disable it with "" ""#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-Wcpp]
   #warning ""Using deprecated NumPy API, disable it with "" \
    ^
  pandas/src/ujson/python/objToJSON.c: In function ‘initObjToJSON’:
  pandas/src/ujson/python/objToJSON.c:201:10: error: ‘NUMPY_IMPORT_ARRAY_RETVAL’ undeclared (first use in this function)
     return NUMPY_IMPORT_ARRAY_RETVAL;
            ^
  pandas/src/ujson/python/objToJSON.c:201:10: note: each undeclared identifier is reported only once for each function it appears in
  pandas/src/ujson/python/objToJSON.c:202:1: warning: control reaches end of non-void function [-Wreturn-type]
   }
   ^
  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
  ----------------------------------------
  ERROR: Failed building wheel for pandas
```

This has been working fine for quite some time already and I suspect this is related to the recent release of numpy-1.19.0.

I've seen there was a similar issue here: https://github.com/pandas-dev/pandas/issues/5326, but there was no mention of undeclared variable`NUMPY_IMPORT_ARRAY_RETVAL` in there.

What's strange to me is the fact that pandas is attempting to build numpy-1.19.0, whereas I have numpy-1.17.4 in requirements. Is it using most recent numpy to build the ujson extension and doesn't look at requirements for that purpose at all?"
652007304,35155,BUG: Dollar symbol appears in dataframe rows which have numerical values,Sibicoder,closed,2020-07-07T05:16:20Z,2020-08-21T17:05:20Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

pd.options.display.max_rows = 1000                                                                  
pd.options.display.float_format = '${:,.2f}'.format    

df = pd.DataFrame(columns=['topic', 'key', 'min', 'avg', 'max', 'current'])

list1 = [
['aggregator1', 'NA', 628, '14.26 K', '15.39 K', 14014.27],
['aggregator2', 'changelog', 0, '4 k', '8 k', 21.2],
['aggregator3', 'CacheData', 0, '2 k', '3 k', 3432.2],
['aggregator4', 'EntityData', 0, 200, 600, 456.21],
['aggregator5', 'internalData', 482, '10.79 K', '11.65 K', 10646.77],
['aggregator6', 'externalData', 145, '3.47 K', '3.74 K', 3366.96]
]

i = 0
for el in list1:
    df.loc[i] = el
    i = i + 1

print(df)

print(df.to_html())

print(df.to_markdown())
```

#### Problem description


Sometimes when a dataframe is converted to markdown or HTML with to_markdown() or to_html(), some numeric values automatically get converted to currency, like $40 instead of 40. There is no mention of currency in the code I Used

#### Expected Output
$ sign should be avoided if user doesn't choose it

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-1048-aws
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
649271977,35089,QST: Zero copy access to the mask of a nullable array,brandon-b-miller,closed,2020-07-01T19:46:40Z,2020-08-21T17:05:20Z,"- [ ] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

Hello Pandas devs,
I was wondering what's the best zero-copy way to access the mask of a nullable array, say a StringArray for example. I noticed these all inherit from BaseMaskedArray which provides `isna()` wrapping `_mask` however I want to make sure this is the recommended method, if I can always be sure this doesn't incur a copy, etc. Should I be using `isna()` or something else, if it exists?


"
646723357,35031,DOC: text in pandas.Series.dt.day refers to month instead of day,danielolsen,closed,2020-06-27T17:43:48Z,2020-08-21T17:05:20Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.day.html

#### Documentation problem

The text here is a duplicate of https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.month.html (`The month as January=1, December=12.`), when it should probably be something else. "
652874268,35172,QST: I get an error when use to_sql func to save a dataframe to mysql for BLOB column.how to fix it ?,945226956,closed,2020-07-08T03:04:53Z,2020-08-21T17:05:21Z,"my package list is:
Package                           Version            Note: you may need to restart the kernel to use updated packages.

--------------------------------- -------------------
-cikit-learn                      0.22.2.post1       
asn1crypto                        0.24.0             
attrs                             19.3.0             
automan                           1.0.0              
autopep8                          1.5.2              
backcall                          0.1.0              
bcrypt                            3.1.4              
bitarray                          1.2.1              
bleach                            3.1.4              
cacheout                          0.11.1             
certifi                           2018.11.29         
cffi                              1.12.2             
chardet                           3.0.4              
Click                             7.0                
cloudpickle                       1.4.1              
colorama                          0.4.3              
configobj                         5.0.6              
cryptography                      2.3.1              
cycler                            0.10.0             
Cython                            0.29.19            
DBUtils                           1.3                
decorator                         4.4.2              
defusedxml                        0.6.0              
entrypoints                       0.3                
et-xmlfile                        1.0.1              
Flask                             1.0.2              
future                            0.18.2             
hyperopt                          0.2.3              
idna                              2.6                
imbalanced-learn                  0.6.2              
imblearn                          0.0                
importlib-metadata                1.5.0              
impyla                            0.14.0             
ipykernel                         5.1.4              
ipython                           7.13.0             
ipython-genutils                  0.2.0              
itsdangerous                      1.1.0              
jdcal                             1.4.1              
jedi                              0.16.0             
Jinja2                            2.10               
joblib                            0.14.1             
jsonschema                        3.2.0              
jupyter-client                    6.1.2              
jupyter-contrib-core              0.3.3              
jupyter-contrib-nbextensions      0.5.1              
jupyter-core                      4.6.3              
jupyter-highlight-selected-word   0.2.0              
jupyter-latex-envs                1.4.6              
jupyter-nbextensions-configurator 0.4.1              
kafka-python                      1.3.5              
kiwisolver                        1.2.0              
lxml                              4.5.0              
MarkupSafe                        1.1.1              
matplotlib                        3.2.1              
mistune                           0.8.4              
mysqlclient                       1.4.6              
nbconvert                         5.6.1              
nbformat                          5.0.4              
networkx                          2.2                
notebook                          6.0.3              
numpy                             1.18.2             
opencv-python                     4.2.0.34           
openpyxl                          3.0.3              
pandas                            0.25.3             
pandocfilters                     1.4.2              
paramiko                          2.4.2              
parso                             0.6.2              
patsy                             0.5.1              
pickleshare                       0.7.5              
Pillow                            7.1.2              
pip                               20.0.2             
ply                               3.11               
prometheus-client                 0.7.1              
prompt-toolkit                    3.0.4              
pyasn1                            0.4.5              
pycodestyle                       2.5.0              
pycparser                         2.19               
pydotplus                         2.0.2              
Pygments                          2.6.1              
PyHDFS                            0.2.1              
PyMySQL                           0.9.2              
PyNaCl                            1.3.0              
pyparsing                         2.4.7              
pyrsistent                        0.16.0             
python-dateutil                   2.8.1              
pytz                              2018.9             
pywin32                           227                
pywinpty                          0.5.7              
PyYAML                            5.3.1              
pyzmq                             18.1.1             
redis                             3.2.1              
requests                          2.18.4             
sasl                              0.2.1              
scikit-learn                      0.19.1             
scipy                             1.4.1              
seaborn                           0.10.0             
Send2Trash                        1.5.0              
setuptools                        46.1.3.post20200330
simplejson                        3.16.0             
six                               1.12.0             
sklearn-pandas                    1.8.0              
sklearn2pmml                      0.49.0             
SQLAlchemy                        1.3.15             
statsmodels                       0.11.1             
terminado                         0.8.3              
testpath                          0.4.4              
thrift                            0.10.0             
thrift-sasl                       0.2.1              
thriftpy                          0.3.9              
tornado                           6.0.4              
tqdm                              4.46.1             
traitlets                         4.3.3              
urllib3                           1.22               
wcwidth                           0.1.9              
webencodings                      0.5.1              
Werkzeug                          0.14.1             
wheel                             0.34.2             
wincertstore                      0.2                
xgboost                           0.6                
xlrd                              1.2.0              
XlsxWriter                        1.2.9              
zipp                              2.2.0

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here

from sqlalchemy.types import VARCHAR, Float, Integer, Date, Numeric,BLOB,CLOB
from sqlalchemy import create_engine
# db_conn_str = 'mysql+pymysql://user:pswd@localhost:3307/xxx?charset=utf8'
con=create_engine(db_conn_str)  
# print(db_conn_str)
a = pd.DataFrame({'a':[1,2,3,],'b':['dddddd','3','5']})

a.to_sql(name='test_blob', con=con, if_exists='append', dtype={'b':BLOB},index=False)

```

#### Problem description
<details>
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
D:\Anaconda3\envs\py3-back\lib\site-packages\sqlalchemy\engine\base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)
   1174 
-> 1175             context = constructor(dialect, self, conn, *args)
   1176         except BaseException as e:

D:\Anaconda3\envs\py3-back\lib\site-packages\sqlalchemy\engine\default.py in _init_compiled(cls, dialect, connection, dbapi_connection, compiled, parameters)
    833                         )
--> 834                         for key in compiled_params
    835                     )

D:\Anaconda3\envs\py3-back\lib\site-packages\sqlalchemy\engine\default.py in <genexpr>(.0)
    833                         )
--> 834                         for key in compiled_params
    835                     )

D:\Anaconda3\envs\py3-back\lib\site-packages\sqlalchemy\sql\sqltypes.py in process(value)
    912             if value is not None:
--> 913                 return DBAPIBinary(value)
    914             else:

D:\Anaconda3\envs\py3-back\lib\site-packages\pymysql\__init__.py in Binary(x)
     84     else:
---> 85         return bytes(x)
     86 

TypeError: string argument without an encoding

The above exception was the direct cause of the following exception:

StatementError                            Traceback (most recent call last)
<ipython-input-66-e1bff449d290> in <module>
      6 a = pd.DataFrame({'a':[1,2,3,],'b':['dddddd','3','5']})
      7 
----> 8 a.to_sql(name='test_blob', con=con, if_exists='append', dtype={'b':BLOB},index=False)

~\AppData\Roaming\Python\Python36\site-packages\pandas\core\generic.py in to_sql(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)
   2710             chunksize=chunksize,
   2711             dtype=dtype,
-> 2712             method=method,
   2713         )
   2714 

~\AppData\Roaming\Python\Python36\site-packages\pandas\io\sql.py in to_sql(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)
    516         chunksize=chunksize,
    517         dtype=dtype,
--> 518         method=method,
    519     )
    520 

~\AppData\Roaming\Python\Python36\site-packages\pandas\io\sql.py in to_sql(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method)
   1318         )
   1319         table.create()
-> 1320         table.insert(chunksize, method=method)
   1321         if not name.isdigit() and not name.islower():
   1322             # check for potentially case sensitivity issues (GH7815)

~\AppData\Roaming\Python\Python36\site-packages\pandas\io\sql.py in insert(self, chunksize, method)
    754 
    755                 chunk_iter = zip(*[arr[start_i:end_i] for arr in data_list])
--> 756                 exec_insert(conn, keys, chunk_iter)
    757 
    758     def _query_iterator(

~\AppData\Roaming\Python\Python36\site-packages\pandas\io\sql.py in _execute_insert(self, conn, keys, data_iter)
    668         """"""
    669         data = [dict(zip(keys, row)) for row in data_iter]
--> 670         conn.execute(self.table.insert(), data)
    671 
    672     def _execute_insert_multi(self, conn, keys, data_iter):

D:\Anaconda3\envs\py3-back\lib\site-packages\sqlalchemy\engine\base.py in execute(self, object_, *multiparams, **params)
    982             )
    983         else:
--> 984             return meth(self, multiparams, params)
    985 
    986     def _execute_function(self, func, multiparams, params):

D:\Anaconda3\envs\py3-back\lib\site-packages\sqlalchemy\sql\elements.py in _execute_on_connection(self, connection, multiparams, params)
    291     def _execute_on_connection(self, connection, multiparams, params):
    292         if self.supports_execution:
--> 293             return connection._execute_clauseelement(self, multiparams, params)
    294         else:
    295             raise exc.ObjectNotExecutableError(self)

D:\Anaconda3\envs\py3-back\lib\site-packages\sqlalchemy\engine\base.py in _execute_clauseelement(self, elem, multiparams, params)
   1101             distilled_params,
   1102             compiled_sql,
-> 1103             distilled_params,
   1104         )
   1105         if self._has_events or self.engine._has_events:

D:\Anaconda3\envs\py3-back\lib\site-packages\sqlalchemy\engine\base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)
   1176         except BaseException as e:
   1177             self._handle_dbapi_exception(
-> 1178                 e, util.text_type(statement), parameters, None, None
   1179             )
   1180 

D:\Anaconda3\envs\py3-back\lib\site-packages\sqlalchemy\engine\base.py in _handle_dbapi_exception(self, e, statement, parameters, cursor, context)
   1480             elif should_wrap:
   1481                 util.raise_(
-> 1482                     sqlalchemy_exception, with_traceback=exc_info[2], from_=e
   1483                 )
   1484             else:

D:\Anaconda3\envs\py3-back\lib\site-packages\sqlalchemy\util\compat.py in raise_(***failed resolving arguments***)
    176 
    177         try:
--> 178             raise exception
    179         finally:
    180             # credit to

D:\Anaconda3\envs\py3-back\lib\site-packages\sqlalchemy\engine\base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)
   1173                 conn = self._revalidate_connection()
   1174 
-> 1175             context = constructor(dialect, self, conn, *args)
   1176         except BaseException as e:
   1177             self._handle_dbapi_exception(

D:\Anaconda3\envs\py3-back\lib\site-packages\sqlalchemy\engine\default.py in _init_compiled(cls, dialect, connection, dbapi_connection, compiled, parameters)
    832                             else compiled_params[key],
    833                         )
--> 834                         for key in compiled_params
    835                     )
    836 

D:\Anaconda3\envs\py3-back\lib\site-packages\sqlalchemy\engine\default.py in <genexpr>(.0)
    832                             else compiled_params[key],
    833                         )
--> 834                         for key in compiled_params
    835                     )
    836 

D:\Anaconda3\envs\py3-back\lib\site-packages\sqlalchemy\sql\sqltypes.py in process(value)
    911         def process(value):
    912             if value is not None:
--> 913                 return DBAPIBinary(value)
    914             else:
    915                 return None

D:\Anaconda3\envs\py3-back\lib\site-packages\pymysql\__init__.py in Binary(x)
     83         return bytearray(x)
     84     else:
---> 85         return bytes(x)
     86 
     87 

StatementError: (builtins.TypeError) string argument without an encoding
[SQL: INSERT INTO test_blob (a, b) VALUES (%(a)s, %(b)s)]
[parameters: [{'a': 1, 'b': 'dddddd'}, {'a': 2, 'b': '3'}, {'a': 3, 'b': '5'}]]

</details>
"
652290394,35159,BUG: subtract `pd.offsets.BDay()` from `Timestamp` gives different results when same are expected,tobiaslocker,closed,2020-07-07T12:41:17Z,2020-08-21T17:05:21Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
def close_prices_start_date(date, period):
    start_date = date - period * pd.offsets.BDay()
    print(
            'date.tzinfo =', date.tzinfo,
            'date.freqstr =', date.freqstr,
            'date.tzname() =', date.tzname(),
            'date =', date,
            'period =', period,
            'result =', start_date)
    return start_date

```

#### Problem description

The above function gives different results when called with the same parameters. I can't see any reason for that so my guess is that this behavior is unwanted. The parameter `date` comes from different `DataFrames` and is read with the following function:
```python
def last_date_of_column(df, column_name):
    return df[column_name].index[-1]
```

I mention this, because the same dataframes give the same results, so it is reproducible. I won't provide the dataframes here, since any relevant information about the parameters should be in the function's print out.

#### Expected Output

When calling the above code with the same parameters `date` and `period` i expect to get the same result



#### Output of ``pip freeze``

```console 
$ pip freeze
numpy==1.19.0
pandas==1.0.5
python-dateutil==2.8.1
pytz==2020.1
quantitative==0.1
schedule==0.6.0
six==1.15.0
```

The print out in the function (result is on the very right and differs for any output for `AAPL US EQUITY`)

```
AAPL US EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-26 00:00:00 period = 20 result = 2020-05-29 00:00:00
9984 JT EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
7203 JT EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
6963 JP EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
6861 JP EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
AAPL US EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-26 00:00:00 period = 20 result = 2020-05-29 00:00:00
9984 JT EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
7203 JT EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
6963 JP EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
6861 JP EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
AAPL US EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-26 00:00:00 period = 20 result = 2020-05-29 00:00:00
9984 JT EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
7203 JT EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
6963 JP EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
6861 JP EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
AAPL US EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-26 00:00:00 period = 20 result = 2020-05-29 00:00:00
9984 JT EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
7203 JT EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
6963 JP EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
6861 JP EQUITY
date.tzinfo = None date.freqstr = None date.tzname() = None date = 2020-06-29 00:00:00 period = 20 result = 2020-06-01 00:00:00
```

"
653845853,35189,BUG: sorting a MultiIndex with pd.Period does not sort correctly when assigning new values inplace,rhsmits91,closed,2020-07-09T07:52:26Z,2020-08-21T17:05:22Z,"This is a problem on pandas 1.0.5:

```python
import pandas as pd

s = pd.Series(
    {
        (""A"", pd.Period(""2021"")): 20,
        (""A"", pd.Period(""2020"")): 10,
    }
)

s.sort_index(level=1, sort_remaining=False)
# gives:
# A  2020    10
#    2021    20
# dtype: int64
# --> works!

s[(""B"", pd.Period(""2019""))] = 15

s.sort_index(level=1, sort_remaining=False)
# gives:
# A  2020    10
#    2021    20
# B  2019    15
# dtype: int64
# --> wrong?

s.sort_index(level=1, sort_remaining=False, ascending=False)
# gives:
# B  2019    15
# A  2021    20
#    2020    10
# dtype: int64
# --> wrong?
```

#### Problem description

The problem seems to be that adding new periods is giving a wrong ordering of the periods in MultiIndex levels, so that sorting takes the wrong order to sort them.

If I do `s.index.levels[-1]` I can see the levels are ordered wrong - and when I do `s.index.levels.sort_values()` they are ordered correctly. So, it seems like internally the wrong ordering is somehow used?

Is this indeed a bug - are there ways I can help to fix it? If not, is there a work around for me to achieve what I need to achieve?

Thanks a lot,


Ron"
653253292,35177,BUG: ImportError: cannot import name 'Collection' from 'typing',niju1,closed,2020-07-08T12:22:15Z,2020-08-21T17:05:22Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python

import pandas as pd 

data = pd.read_csv('Simulations Ãœbersicht_Labled_intrusion.csv')
print(data.head(5))

```

#### Problem description
base) ballal@i6.emi.fhg.de:/home-24/ballal/Skripte/Rule Extraction
$ python3 decisiontree.py
Traceback (most recent call last):
  File ""decisiontree.py"", line 1, in <module>
    import pandas as pd 
  File ""/home/ballal/anaconda3/lib/python3.7/site-packages/pandas/__init__.py"", line 54, in <module>
    from pandas.core.api import (
  File ""/home/ballal/anaconda3/lib/python3.7/site-packages/pandas/core/api.py"", line 6, in <module>
    from pandas.core.dtypes.dtypes import (
  File ""/home/ballal/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/dtypes.py"", line 10, in <module>
    from pandas._typing import Ordered
  File ""/home/ballal/anaconda3/lib/python3.7/site-packages/pandas/_typing.py"", line 2, in <module>
    from typing import (
ImportError: cannot import name 'Collection' from 'typing' (/usr/lib/python3.4/site-packages/typing.py)


I cannot import pandas. I installed the latest version of pandas too. 

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
652987486,35174,"ENH: Inconsistency: .astype(""string"") should convert int-Series to string",FlorianWetschoreck,closed,2020-07-08T05:49:26Z,2020-08-21T17:05:22Z,"```python
pd.Series(['1', '2', '3'], dtype=""string"").astype(""int"")
```
Turns the String Series into an Integer Series and converts the values.


However, the inverse operation raises an Error:

```python
pd.Series([1, 2, 3], dtype=""int"").astype(""string"")
```

In order to be consistent, the inverse operation should also work.
Currently, I think that the best alternative solutions are something like this:

```python
pd.Series(pd.Series([1, 2, 3], dtype=""int"").apply(str), dtype=""string"")
```
or
```python
pd.Series([1, 2, 3], dtype=""int"").apply(str).convert_dtypes()
```
But those solutions are too convoluted and not consistent with the above API.
"
654230405,35193,Python3.6 FreeBSD pip3 install pandas fails with error 1,andibb,closed,2020-07-09T17:31:14Z,2020-08-21T17:05:23Z,"- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

**Note**: If you'd still like to submit a question, please read [this guide](
https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your question.

```python
# Your code here, if applicable

```
"
654071402,35191,ENH: Print and manipulate rows,osayanhu,closed,2020-07-09T13:42:34Z,2020-08-21T17:05:23Z,"#### Is your feature request related to a problem?

[this should provide a description of what the problem is, e.g. ""I wish I could use pandas to do [...]""]

#### Describe the solution you'd like

[this should provide a description of the feature request, e.g. ""`DataFrame.foo` should get a new parameter `bar` that [...]"", try to write a docstring for the desired feature]

#### API breaking implications

[this should provide a description of how this feature will affect the API]

#### Describe alternatives you've considered

[this should provide a description of any alternative solutions or features you've considered]

#### Additional context

[add any other context, code examples, or references to existing implementations about the feature request here]

```python
# Your code here, if applicable

```
"
656024438,35265,ENH: Add `into` argument for `to_xarray` method? ,jolespin,closed,2020-07-13T17:38:28Z,2020-08-21T17:05:24Z,"#### Is your feature request related to a problem?

I would like to able to specify the object used for creating `xarray` objects.  

#### Describe the solution you'd like
Right now, the only option is `xr.Dataset` but I tend to prefer `xr.DataArray` objects.

#### API breaking implications

It shouldn't affect the API in any complicated ways. 

#### Describe alternatives you've considered

Create a `xr.Dataset` and then convert that to `xr.DataArray`.  However, this step could be obviated by usage similar to [`to_dict`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_dict.html) method.
#### Additional context


```python
import pandas as pd
import xarray as xr
df = pd.read_csv(""https://pastebin.com/raw/dR59vTD4"", sep=""\t"", index_col=0)
da = df.to_xarray(into=xr.DataArray)
```
"
609366059,33879,DOC: Fix Dataframe.apply documentation to include note regarding calling function twice on first element,alonme,closed,2020-04-29T21:17:40Z,2020-08-21T17:05:35Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html

#### Documentation problem

In commit f625730eb the documentation of `df.apply` was changed, and a note regarding `df.apply` behavior to call the applied function twice on the first column/row.

As far as i can tell from this [issue](https://github.com/pandas-dev/pandas/pull/24748#issuecomment-532148732) and from my own testing (See below) this behavior still exists.

```python
In [1]:  import pandas as pd

In [2]: df = pd.DataFrame({'a': [1,2,3]})

In [3]: def mul2(x):
    ...:     print('Hello')
    ...:     return x*2
    ...:

In [4]: df.apply(mul2)
Hello
Hello
Out[4]:
   a
0  2
1  4
2  6

In [5]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.3.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.2.0
Cython           : None
pytest           : 5.3.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.3.3
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : 0.13.1
pyarrow          : None
pytables         : None
pytest           : 5.3.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.12
tables           : None
tabulate         : None
xarray           : 0.15.1
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.47.0

```


#### Suggested fix for documentation
Revert commit f625730eb in order to reflect the actual behavior of the apply function

"
610750965,33916,ENH: Add nrows parameter to pd.read_json,sp1thas,closed,2020-05-01T13:29:07Z,2020-08-21T17:05:36Z,"#### Is your feature request related to a problem?

Let's say I have a huge `jsonlines` file and I want to read only the first `n` lines of the file.

#### Describe the solution you'd like

This problem could be fixed by adding `nrows` parameter in method `pd.read_json`. This parameter should be applicable if and only if `lines=True`

#### API breaking implications

Simply add and implement `nrows` parameter in `pd.read_json` (like `pd.read_csv`).

#### Additional context

How this enhancement could work:

```python
import pandas as pd

df = pd.read_json('/path/to/file.jsonlines', lines=True, nrows=1000)
```
"
615964780,34117,BUG: my_DataFrame.groupby().min()/-max() gives Assertion error message,d-kleine,closed,2020-05-11T15:14:57Z,2020-08-21T17:05:37Z,"With pandas 1.0.0. (and newer) min/max methods for groups to not work as before:
```python
my_DataFrame.groupby().min()
my_DataFrame.groupby().max()
```
are not working anymore - it will give you an Assertion error message with no special location in the code.

You currently new to use aggregation to get this information:
```python
my_DataFrame.groupby().agg('min')
my_DataFrame.groupby().agg('max')
```

I could verify that
```python
my_DataFrame.groupby().mean()
```
still works in newest pandas, but somehow -min()/-max() no more.

Would it be possible to implement these methods back again to pandas?
I can confirm that my_DataFrame.groupby().max()/-min() worked properly in pandas 0.3.0.

Kind regards,
DK"
614919010,34078,BUG: SettingWithCopyWarning:  A value is trying to be set on a copy of a slice from a DataFrame,DSKaarthick,closed,2020-05-08T18:52:17Z,2020-08-21T17:05:37Z,"
#### Code Sample, a copy-pastable example

```python
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import gensim 
from gensim.models import FastText
from sklearn.feature_extraction.text import TfidfVectorizer
from difflib import SequenceMatcher

#Loading Query Text for Corpus Building
qt=pd.read_csv('C:/Demo/Query_Text.csv')
qt.shape

#Loading QueryText for comparing

convid =pd.read_csv('C:/Demo/ConvId_May4th.csv')

convid.shape

sentences = qt


convid = convid.sort_values(['User_PUID','EventInfo_Time'], ascending=[True,True])
convid = convid.reset_index()
convid['FastTextResult'] =float()
convid['Tfidf'] = float()
convid['TfidfWc'] = float()

model_ted = FastText(qt,  window=1, min_count=1, sg=0)

for i in range(len(convid['Query_Text'])):
    print('i',i)
    if(i == len(convid)-1):
            break
    
    #print(""The FastText Output"")
    
    ft=model_ted.wv.similarity(str(convid['Query_Text'][i]).lower(),str(convid['Query_Text'][i+1]).lower())
    ft=round(ft,3)
    #print(ft)
    convid['FastTextResult'][i]=ft;
    
    #print(""vector output "")
    
    vectorizer = TfidfVectorizer(decode_error='ignore',strip_accents='unicode',stop_words='english',min_df=1,analyzer='word',vocabulary=qt) 
    tfidf= vectorizer.fit_transform([str(convid['Query_Text'][i]).lower(),str(convid['Query_Text'][i+1]).lower()])
    product=(tfidf *tfidf.T).A
    pro = product[0,1]
    pro = round(pro,3)
    #print(pro)
    convid['Tfidf'][i]=pro;
    
    #print(""widlchar"")
    wildchar_value = SequenceMatcher(str(convid['Query_Text'][i]).lower(),str(convid['Query_Text'][i+1]).lower()).ratio()
    
    wildchar_value = round(wildchar_value,3)

    convid['TfidfWc'][i]=wildchar_value;
    #print(wildchar_value)

convid.to_csv('C:/Demo/ConvIdOutput_May4th.csv')





```

#### Problem description

when i am trying to execute the above python script , getting the following error:

C:\Anaconda3\lib\site-packages\ipykernel_launcher.py:11: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

#### Expected Output

The above code should 
#### Output of ``pd.show_versions()``


[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
"
683227415,35838,"Fix Series construction from Sparse[""datetime64[ns]""]",dsaxton,closed,2020-08-21T03:15:17Z,2020-08-27T02:37:46Z,"- [x] closes #35762
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
622643120,34298,sum() after groupby returns different value compared to regular sum(),Denisolt,closed,2020-05-21T17:00:59Z,2020-08-27T02:57:54Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
np.random.seed(0)
df = pd.DataFrame(np.random.rand(253, 2) * 254, columns=['a', 'b'])
df['type'] = 'test'

sum_mult_v1 = df.assign(mult=(lambda x: x.a * x.b)).groupby('type')['mult'].sum()[0]
sum_mult_v2 = (df['a'] * df['b']).sum()
print(sum_mult_v1)
print(sum_mult_v2)
print(sum_mult_v1 == sum_mult_v2)
```

#### Problem description
The output of the code is:
```
4010049.3807103755
4010049.3807103736
False
```

For some reason, the summation of values after groupby is different from the same operation done without a groupby. I understand that there is no point in grouping by a column that only has one value, but I wonder if there is something off with the summation function after groupby? 

#### Expected Output
The expected output would be having the same numbers. 
```
4010049.3807103736
4010049.3807103736
True
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.4.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : 0.29.17
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.14.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.8
numba            : 0.49.1

</details>
"
402258669,24883,Pandas dataframe eval ,poeticcode01,closed,2019-01-23T14:13:59Z,2020-08-27T09:02:57Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
Scenario 1:

df2=pd.DataFrame({'a1':[10,20]})

df2.eval(""c=((a1>10) & True )"")
Out[8]: 
   a1      c
0  10  False
1  20   True
Scenario 2:
df2=pd.DataFrame({'a1':['Y','N']})
df2.eval(""c=((a1 == 'Y') & True )"")
Traceback (most recent call last):

TypeError: issubclass() arg 1 must be a class






```
#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]
By changing datatype of a column from int to str, pd.eval throws ""TypeError: issubclass() arg 1 must be a class"" 


**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output
In Scenario 2

 a1      c
  10  True
 20   False

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.13.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en
LOCALE: None.None

pandas: 0.21.1
pytest: 3.0.7
pip: 9.0.1
setuptools: 27.2.0
Cython: 0.25.2
numpy: 1.14.2
scipy: 0.19.0
pyarrow: None
xarray: None
IPython: 5.3.0
sphinx: 1.5.6
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.2.2
numexpr: 2.6.2
feather: None
matplotlib: 2.0.2
openpyxl: 2.4.7
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: 0.9.6
lxml: 3.7.3
bs4: 4.6.0
html5lib: 0.999
sqlalchemy: 1.1.9
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None


</details>
"
681391468,35794,"BUG: issubclass check with dtype instead of type, closes GH#24883",jbrockmendel,closed,2020-08-18T23:00:56Z,2020-08-27T09:18:49Z,"- [x] closes #24883
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
687083052,35919,"Backport PR #35794: BUG: issubclass check with dtype instead of type,…",simonjayhawkins,closed,2020-08-27T09:18:35Z,2020-08-27T10:37:16Z,xref #35794
684018431,35853,DEPR: deprecate dtype param in Index.copy,topper-123,closed,2020-08-22T15:47:59Z,2020-08-27T13:17:26Z,"Deprecate ``dtype`` param in ``Index.copy`` and child methods. If users want to change dtype, they should use ``Index.astype``."
685146131,35885,DOC: Fix documentation for pandas.Series.transform #35870,onshek,closed,2020-08-25T03:46:11Z,2020-08-27T13:41:27Z,"- [x] closes #35870
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
687362943,35926,remove unnecessary trailing commas,MarcoGorelli,closed,2020-08-27T16:04:04Z,2020-08-27T16:05:37Z,xref #35925 
686574314,35910,CI: Attempt to unpin pytest-xdist,alimcmaster1,closed,2020-08-26T19:48:33Z,2020-08-27T16:06:38Z,"- [x] closes #35756

2.1.0 was released yesterday: https://pypi.org/project/pytest-xdist/#history"
679848073,35756,CI: Unpin pytest-xdist,alimcmaster1,closed,2020-08-16T23:26:26Z,2020-08-27T16:06:48Z,"Ref: https://github.com/pandas-dev/pandas/issues/35737

Ref: https://github.com/scipy/scipy/pull/12730

Restricting version for now."
687364726,35927,CLN remove unnecessary trailing commas,MarcoGorelli,closed,2020-08-27T16:06:48Z,2020-08-27T16:18:53Z,xref #35925
686891525,35915,"""Backport PR #35838 on branch 1.1.x""",dsaxton,closed,2020-08-27T02:36:25Z,2020-08-27T17:11:02Z,xref #35838
686881338,35913,TYP: annotate tseries.holiday,jbrockmendel,closed,2020-08-27T02:08:26Z,2020-08-27T18:50:01Z,cc @simonjayhawkins 
567255831,32086,REF: remove fast_apply,jbrockmendel,closed,2020-02-19T01:19:21Z,2020-08-27T20:18:08Z,"xref #32083, one more coming that i think finishes off libreduction"
567256299,32087,REF: remove compute_reduction,jbrockmendel,closed,2020-02-19T01:20:55Z,2020-08-27T20:18:21Z,"Along with #32083, #32086 this gets rid of _libs.reduction.

Now, time to run some asvs"
567154756,32083,"POC/REF: remove SeriesBinGrouper, SeriesGrouper",jbrockmendel,closed,2020-02-18T21:00:12Z,2020-08-27T20:18:36Z,"Maintenance of libreduction is a PITA, so I want to see how big a performance hit we take if we rip it out.

I get a test failure locally because apparently something in resample behaves differently if we go through the python path, which itself should be considered a bug.  cc @mroeschke any ideas how to fix the failing test/"
561164986,31752,REF: _cython_agg_blocks,jbrockmendel,closed,2020-02-06T17:37:16Z,2020-08-27T20:18:58Z,"This is effectively an alternative approach to #31616, trying to use a 1-pass approach. 

This does _not_ address the uniqueness problem discussed in #31735.

cc @TomAugspurger @WillAyd "
686901351,35916,BUG: Dataframe slicing incosistent behavior when using & operator,henrypinkard,closed,2020-08-27T03:05:40Z,2020-08-28T01:08:42Z,"- [ ] I have checked that this issue has not already been reported.

Not familiar enough with pandas to know what to search for to verify this

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Problem description

There seems to be a bug with indexing into DataFrames using logical and. When I do the operations in series, like this, it works, giving two rows:

```
slidescan_db = pd.read_csv('slidescan_db.csv')
db = slidescan_db[slidescan_db['Stain'].str.contains('unstained').fillna(False)]
db[db['Batch'] == batch_index]
```

But when I do it with a one-liner, it gives a different result--no rows

```
slidescan_db = pd.read_csv('slidescan_db.csv')
slidescan_db[slidescan_db['Batch'] == batch_index & slidescan_db['Stain'].str.contains(ch_name).fillna(False)]
```

slidescan_db.csv is attached, but renamed to slidescan.txt because GitHub wouldn't let me upload a csv

[slidescan_db.txt](https://github.com/pandas-dev/pandas/files/5133799/slidescan_db.txt)



#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.7.0.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.2.0.post20200511
Cython           : 0.29.17
pytest           : 5.4.2
hypothesis       : 5.11.0
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : 1.3.2
fsspec           : 0.7.1
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.16
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.49.0
</details>
"
239085223,16785,ERR: validate partial string indexing with tz-aware end-points,1kastner,closed,2017-06-28T08:11:44Z,2020-08-28T07:23:18Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import dateutil.parser

df = pd.DataFrame(index=pd.date_range('2016-01-01T00:00', '2016-12-31T23:59', freq='T'))

df[
    ""2016-01-01T00:00-02:00""
    :
    ""2016-01-01T02:03""
]  # returns 124 entries

df[
    dateutil.parser.parse(""2016-01-01T00:00-02:00"")
    :
    dateutil.parser.parse(""2016-01-01T02:03"")
]  # returns 4 entries

df[
    pd.Timestamp(""2016-01-01T00:00-02:00"")
    :
    pd.Timestamp(""2016-01-01T02:03"")
]  # returns 4 entries
```
#### Problem description

The current behavior is that the time zone information is ignored without any warning when providing strings.

#### Expected Output

Either use the time zone information and only return the four desired entries OR warn the user about the fact that the time zone information is not used

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.0.final.0
python-bits: 32
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.20.1
pytest: None
pip: 9.0.1
setuptools: 28.8.0
Cython: None
numpy: 1.12.1
scipy: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
pandas_gbq: None
pandas_datareader: None
</details>
"
688019998,35952,"BUG: Series.fillna(..., inplace=True) causes subsequent df.sort_values() to crash for categorical dtype",DanielFEvans,closed,2020-08-28T11:50:26Z,2020-08-28T12:40:30Z,"- [ x ] I have checked that this issue has not already been reported.

- [ x ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

# Fillna without inplace - everything works OK.
df = pd.DataFrame({""a"": [1, 5, 7, 3, 7], ""b"": [""a"", ""b"", ""c"", ""d"", np.nan]})
df[""b""] = df[""b""].astype(""category"")
df[""b""] = df[""b""].fillna(""a"")
df = df.sort_values(by=""a"")
print(df)

# As above, but the fillna() is done inplace
df = pd.DataFrame({""a"": [1, 5, 7, 3, 7], ""b"": [""a"", ""b"", ""c"", ""d"", np.nan]})
df[""b""] = df[""b""].astype(""category"")
df[""b""].fillna(""a"", inplace=True)
# df looks normal here - the np.nan has been replaced
print(df)

df = df.sort_values(by=""a"")
# Crash
```

Traceback:

<details>

```
Traceback (most recent call last):
  File ""test.py"", line 17, in <module>
    df = df.sort_values(by=""a"")
  File ""/home/jbanorthwest.co.uk/danielevans/venvs/farmcat3/lib64/python3.6/site-packages/pandas/core/frame.py"", line 5301, in sort_values
    indexer, axis=self._get_block_manager_axis(axis), verify=False
  File ""/home/jbanorthwest.co.uk/danielevans/venvs/farmcat3/lib64/python3.6/site-packages/pandas/core/internals/managers.py"", line 1415, in take
    new_axis=new_labels, indexer=indexer, axis=axis, allow_dups=True
  File ""/home/jbanorthwest.co.uk/danielevans/venvs/farmcat3/lib64/python3.6/site-packages/pandas/core/internals/managers.py"", line 1259, in reindex_indexer
    for blk in self.blocks
  File ""/home/jbanorthwest.co.uk/danielevans/venvs/farmcat3/lib64/python3.6/site-packages/pandas/core/internals/managers.py"", line 1259, in <listcomp>
    for blk in self.blocks
  File ""/home/jbanorthwest.co.uk/danielevans/venvs/farmcat3/lib64/python3.6/site-packages/pandas/core/internals/blocks.py"", line 1720, in take_nd
    new_values = self.values.take(indexer, fill_value=fill_value, allow_fill=True)
  File ""/home/jbanorthwest.co.uk/danielevans/venvs/farmcat3/lib64/python3.6/site-packages/pandas/core/series.py"", line 829, in take
    nv.validate_take(tuple(), kwargs)
  File ""/home/jbanorthwest.co.uk/danielevans/venvs/farmcat3/lib64/python3.6/site-packages/pandas/compat/numpy/function.py"", line 68, in __call__
    validate_kwargs(fname, kwargs, self.defaults)
  File ""/home/jbanorthwest.co.uk/danielevans/venvs/farmcat3/lib64/python3.6/site-packages/pandas/util/_validators.py"", line 148, in validate_kwargs
    _check_for_invalid_keys(fname, kwargs, compat_args)
  File ""/home/jbanorthwest.co.uk/danielevans/venvs/farmcat3/lib64/python3.6/site-packages/pandas/util/_validators.py"", line 122, in _check_for_invalid_keys
    raise TypeError(f""{fname}() got an unexpected keyword argument '{bad_arg}'"")
TypeError: take() got an unexpected keyword argument 'allow_fill'
```

</details>

#### Problem description

If a dataframe contains a categorical column, `df[""col""].fillna(..., inplace=True)` is called on that column, and the dataframe is subsequently sorted by a different column, Pandas will crash due to an internal error.

This error only occurs in Pandas 1.1.0/1.1.1 (latest at time of writing); Pandas 1.0.5 and below behaves correctly.

Briefly poking around the internals of Pandas, the difference seems to be that `self.values.take` in `take_nd()` is operating on a `pandas.core.series.Series` object in the latest Pandas (i.e. `self.values` is a Series), but was formerly working with a `pandas.core.arrays.categorical.Categorical` object in 1.0.5 and below. As the traceback states, `Series` does not support `allow_fill` in its `take()` function, while most other implementations of `take()` in Pandas do allow it.

#### Expected Output

The two versions of the code both run, producting the sorted, na-filled output dataframe:

```
   a  b
0  1  a
3  3  d
1  5  b
2  7  c
4  7  a
```

#### Output of ``pd.show_versions()``

<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1127.13.1.el7.x86_64
Version          : #1 SMP Tue Jun 23 10:32:27 CDT 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 45.2.0
Cython           : 0.29.14
pytest           : 5.4.3
hypothesis       : 5.16.0
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext)
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.6.2
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : None
tables           : 3.5.2
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.51.0


</details>
"
687375207,35930,CLN remove unnecessary trailing commas from aggregation,MarcoGorelli,closed,2020-08-27T16:22:26Z,2020-08-28T16:50:33Z,"xref #35925
"
687844124,35949,CLN remove unnecessary trailing commas to get ready for new version of black: _testing -> generic,MarcoGorelli,closed,2020-08-28T06:59:47Z,2020-08-28T16:50:38Z,"xref #35925
"
687888511,35950,CLN remove unnecessary trailing commas to get ready for new version of black: generic -> blocks,MarcoGorelli,closed,2020-08-28T08:00:48Z,2020-08-28T16:50:43Z,xref #35925
687438530,35935,TYP: annotations in pandas.plotting,jbrockmendel,closed,2020-08-27T18:06:46Z,2020-08-28T17:09:57Z,
688163423,35955,TYP: misc cleanup in core\groupby\generic.py,simonjayhawkins,closed,2020-08-28T15:40:41Z,2020-08-28T17:58:18Z,"pandas\core\groupby\generic.py:610: error: Too many arguments  [call-arg]
pandas\core\groupby\generic.py:1212: error: ""create_series_with_explicit_dtype"" gets multiple values for keyword argument ""dtype_if_empty""  [misc]"
688208628,35956,Issue35925 remove trailing commas,jpribyl,closed,2020-08-28T16:42:01Z,2020-08-28T18:06:28Z,xref #35925
688254078,35959,Issue35925 remove more trailing commas,jpribyl,closed,2020-08-28T17:55:58Z,2020-08-28T18:40:00Z,"xref #35925
"
688442417,35970,Comma cleanup for Issue #35925,JonathanShrek,closed,2020-08-29T02:40:36Z,2020-08-29T03:31:50Z,"- [x] contributes to #35925
- [x] tests added / passed

Tested with:
`./ci/code_checks.sh`

Files edited:
pandas/tests/frame/test_analytics.py,
pandas/tests/frame/test_constructors.py,
pandas/tests/frame/test_reshape.py,
pandas/tests/generic/test_finalize.py,
pandas/tests/generic/test_to_xarray.py,
pandas/tests/groupby/aggregate/test_numba.py
"
688461964,35972,BUG: rolling count does not respect the default behavior of min_periods,mroeschke,closed,2020-08-29T05:11:31Z,2020-08-29T05:43:24Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

Per the documentation:

> For a window that is specified by an offset, min_periods will default to 1. Otherwise, min_periods will default to the size of the window.

Therefore these two statements should be equal

```
In [4]: pd.__version__
Out[4]: '1.2.0.dev0+160.gd90b73bde'

In [5]: pd.Series([np.nan] * 4).rolling(2).count()
Out[5]:
0    0.0
1    0.0
2    0.0
3    0.0
dtype: float64

In [6]: pd.Series([np.nan] * 4).rolling(2, min_periods=2).count()
Out[6]:
0    NaN
1    0.0
2    0.0
3    0.0
dtype: float64
```"
433774853,26107,TypeError on while retrieving values from a Series,rsaim,closed,2019-04-16T13:17:13Z,2020-08-29T11:47:48Z,"```python
In [37]: import pandas as pd
    ...: import numpy as np
    ...: from pandas.core.indexes.datetimes import DatetimeIndex
    ...:
    ...: data = np.ma.array(data=DatetimeIndex(['2011-01-01']))
    ...: s = pd.Series(data=data)
    ...: s.values # FAILS
OUTPUT:
TypeError                                 Traceback (most recent call last)
<ipython-input-37-b6f50d56e315> in <module>
      5 data = np.ma.array(data=DatetimeIndex(['2011-01-01']))
      6 s = pd.Series(data=data)
----> 7 s.values # FAILS

/home/user1/python3.7/site-packages/pandas/core/series.py in values(self)
    472                '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')
    473         """"""
--> 474         return self._data.external_values()
    475
    476     @property

/home/user1/python3.7/site-packages/pandas/core/internals/managers.py in external_values(self)
   1545
   1546     def external_values(self):
-> 1547         return self._block.external_values()
   1548
   1549     def internal_values(self):

/home/user1/python3.7/site-packages/pandas/core/internals/blocks.py in external_values(self)
   2234
   2235     def external_values(self):
-> 2236         return np.asarray(self.values.astype('datetime64[ns]', copy=False))
   2237
   2238

TypeError: astype() got an unexpected keyword argument 'copy'
```
#### Problem description

This breaks with **0.24.2** but not with **0.23.4**.

#### Expected Output
masked_array(data=['2011-01-01T00:00:00.000000000'],
             mask=False,
       fill_value=numpy.datetime64('NaT'),
            dtype='datetime64[ns]')

OR

['2011-01-01T00:00:00.000000000']

#### Output of ``pd.show_versions()``
<details>
In [38]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Linux
OS-release: 3.10.rsaim.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 4.4.0
pip: 10.0.1
setuptools: 39.0.1
Cython: 0.29.5
numpy: 1.14.3
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 7.3.0
sphinx: 2.0.0
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2017.3
blosc: 1.8.1
bottleneck: None
tables: 3.4.4
numexpr: 2.6.9
feather: None
matplotlib: 2.2.3
openpyxl: 2.6.2
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.5
lxml.etree: 3.6.0
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.2.5+4.rsaim
pymysql: None
psycopg2: 2.7.7 (dt dec pq3 ext)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
688544252,35976,BUG:,yunkypunky,closed,2020-08-29T14:33:42Z,2020-08-29T14:45:45Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here

```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
688154740,35954,TYP: misc typing cleanups for #32911,simonjayhawkins,closed,2020-08-28T15:26:28Z,2020-08-29T18:23:23Z,"pandas\io\excel\_odswriter.py:39:5: error: Argument 5 of ""write_cells"" is incompatible with supertype ""ExcelWriter""; supertype defines the argument type as ""Optional[Tuple[int, int]]""  [override]
pandas\io\excel\_odswriter.py:62:35: error: Argument 1 to ""_validate_freeze_panes"" has incompatible type ""Optional[List[Any]]""; expected ""Optional[Tuple[int, int]]""  [arg-type]"
687721447,35944,Version Number correction in to_json table,iudeen,closed,2020-08-28T05:21:36Z,2020-08-29T18:45:42Z,"Added a line of code to obtain version of Pandas and display it in Schema of a table json when to_json(orient='table') is used.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
688275609,35961,"BUG: Unable to add np.timedelta64 + pd.Interval(pd.Timestamp, pd.Timestamp)",dsaxton,closed,2020-08-28T18:26:08Z,2020-08-30T02:41:13Z,"Note: this is currently _not_ a bug on master but will be after https://github.com/pandas-dev/pandas/pull/35938, which reverts a change that caused other regressions.

```python
import numpy as np
import pandas as pd

delta = np.timedelta64(0)
interval = pd.Interval(pd.Timestamp(""2020-01-01""), pd.Timestamp(""2020-02-01""))
delta + interval
```

```python
UFuncTypeError                            Traceback (most recent call last)
<ipython-input-1-2c3b3a0b4fb8> in <module>
      4 delta = np.timedelta64(0)
      5 interval = pd.Interval(pd.Timestamp(""2020-01-01""), pd.Timestamp(""2020-02-01""))
----> 6 delta + interval

UFuncTypeError: ufunc 'add' cannot use operands with types dtype('<m8') and dtype('O')
```

Flipping the operands however works

```python
interval + delta
```

```python
Interval('2020-01-01', '2020-02-01', closed='right')
```"
688317510,35963,TYP: misc cleanup in core\generic.py,simonjayhawkins,closed,2020-08-28T19:43:38Z,2020-08-30T11:17:04Z,"pandas\core\generic.py:4707: error: Unsupported operand types for in (""Optional[str]"" and ""str"")  [operator]
pandas\core\generic.py:6559: error: 'builtins.object' object is not iterable  [misc]
"
688224091,35957,TYP: Remove NDFrame._add_series_or_dataframe_operations,topper-123,closed,2020-08-28T17:04:32Z,2020-08-30T12:54:09Z,Refactoring ``NDFrame._add series or dataframe`` class method helps with typing.
688733840,35988,TYP: mypy [attr-defined] error in `pandas/core/indexes/numeric.py`,fangchenli,closed,2020-08-30T14:59:20Z,2020-08-30T15:04:28Z,"This is one of the errors after unpin mypy. 

```
pandas/core/indexes/numeric.py:68: error: ""Type[NumericIndex]"" has no attribute ""_default_dtype""  [attr-defined]
pandas/core/indexes/numeric.py:69: error: ""Type[NumericIndex]"" has no attribute ""_default_dtype""  [attr-defined]

```"
688578331,35978,TYP: annotate plotting._matplotlib.converter,jbrockmendel,closed,2020-08-29T18:01:24Z,2020-08-30T15:06:17Z,
688272816,35960,TYP: annotate plotting based on _get_axe_freq,jbrockmendel,closed,2020-08-28T18:20:48Z,2020-08-30T15:06:52Z,"In some places in plotting `ax` is an Axes object and in other its an Axis object.  Current goal is to pin these down.

in timeseries._get_ax_freq we call `ax.get_shared_x_axes()`, which is an Axes method that does not exist on Axis.  This annotates that usage and annotates all the other places where we can infer Axes from that."
687420430,35933,TYP: Annotations,jbrockmendel,closed,2020-08-27T17:34:52Z,2020-08-30T15:07:35Z,
688751204,35993,Updating fork,souris-dev,closed,2020-08-30T16:44:16Z,2020-08-30T16:47:58Z,"Updating fork (this PR was opened in the wrong repo by mistake, apologies for the same.)"
688151143,35953,TYP: misc typing cleanups for #29116,simonjayhawkins,closed,2020-08-28T15:20:55Z,2020-08-30T18:41:14Z,"pandas\core\frame.py:7429:59: error: Argument 3 to ""relabel_result"" has incompatible type ""Optional[List[str]]""; expected ""Tuple[Any, ...]""  [arg-type]
pandas\core\frame.py:7429:68: error: Argument 4 to ""relabel_result"" has incompatible type ""Optional[List[int]]""; expected ""List[int]""  [arg-type]"
688739994,35992,TYP: check_untyped_defs core.dtypes.cast,simonjayhawkins,closed,2020-08-30T15:35:14Z,2020-08-31T08:07:07Z,
688762630,35996,Issue35925 Remove trailing commas,metehankutlu,closed,2020-08-30T17:56:06Z,2020-08-31T09:59:26Z,"#35925 
Files edited: 
- pandas/tests/test_multilevel.py
- pandas/tests/test_nanops.py
- pandas/tests/window/moments/test_moments_consistency_rolling.py
- pandas/tests/window/moments/test_moments_ewm.py
- pandas/tests/window/moments/test_moments_rolling.py
- pandas/tests/window/test_base_indexer.py
- pandas/tests/window/test_pairwise.py
- pandas/tests/window/test_rolling.py
- pandas/tseries/frequencies.py
- pandas/util/_test_decorators.py"
679421423,35731,"BUG: Series has no attribute ""reshape"" after adding a new category in df",chen-bowen,closed,2020-08-14T21:55:57Z,2020-08-31T12:36:12Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd
from pandas.api.types import CategoricalDtype

# create dataframe (note: every single column is a category)
df =   pd.DataFrame(
               {
                    ""a"": pd.Series([np.nan, 2.0, 3.0, 1.0]).astype(""category""),
                    ""b"": pd.Series([""A"", ""A"", ""B"", ""C""]).astype(""category""),
                    ""c"": pd.Series([""D"", ""E"", ""E"", np.nan]).astype(""category""),
                }
      )

# get all categories from column ""a""
cats =df[""a""].cat.categories.tolist()
# append new category 
cats.append(0.0)
df[""a""] = df[""a""].astype(CategoricalDtype(categories=cats, ordered=False))
# fillna with that new category
df[""a""].fillna(0, inplace=True)
# run df.isnull()
df.isnull().sum()
```

#### Problem description

We are trying to add the new fillna as a new category in the dataframe, but it fails when we are trying to use `df.isnull()`  In this case we are pretty much blocked from using df.isull().sum() functionality. Running the above snippet will get us the attribute error
`AttributeError: 'Series' object has no attribute 'reshape'`

Full stack trace is shown below.

```
AttributeError                            Traceback (most recent call last)
<ipython-input-29-cc81fd27034f> in <module>
----> 1 df.isnull()

~/.pyenv/versions/3.7.0/envs/fair_ml/lib/python3.7/site-packages/pandas/core/frame.py in isnull(self)
   4865     @doc(NDFrame.isna, klass=_shared_doc_kwargs[""klass""])
   4866     def isnull(self) -> ""DataFrame"":
-> 4867         return self.isna()
   4868 
   4869     @doc(NDFrame.notna, klass=_shared_doc_kwargs[""klass""])

~/.pyenv/versions/3.7.0/envs/fair_ml/lib/python3.7/site-packages/pandas/core/frame.py in isna(self)
   4860     @doc(NDFrame.isna, klass=_shared_doc_kwargs[""klass""])
   4861     def isna(self) -> ""DataFrame"":
-> 4862         result = self._constructor(self._data.isna(func=isna))
   4863         return result.__finalize__(self, method=""isna"")
   4864 

~/.pyenv/versions/3.7.0/envs/fair_ml/lib/python3.7/site-packages/pandas/core/internals/managers.py in isna(self, func)
    500 
    501     def isna(self, func) -> ""BlockManager"":
--> 502         return self.apply(""apply"", func=func)
    503 
    504     def where(

~/.pyenv/versions/3.7.0/envs/fair_ml/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, align_keys, **kwargs)
    394                 applied = b.apply(f, **kwargs)
    395             else:
--> 396                 applied = getattr(b, f)(**kwargs)
    397             result_blocks = _extend_blocks(applied, result_blocks)
    398 

~/.pyenv/versions/3.7.0/envs/fair_ml/lib/python3.7/site-packages/pandas/core/internals/blocks.py in apply(self, func, **kwargs)
    346             result = func(self.values, **kwargs)
    347 
--> 348         return self._split_op_result(result)
    349 
    350     def _split_op_result(self, result) -> List[""Block""]:

~/.pyenv/versions/3.7.0/envs/fair_ml/lib/python3.7/site-packages/pandas/core/internals/blocks.py in _split_op_result(self, result)
    361 
    362         if not isinstance(result, Block):
--> 363             result = self.make_block(result)
    364 
    365         return [result]

~/.pyenv/versions/3.7.0/envs/fair_ml/lib/python3.7/site-packages/pandas/core/internals/blocks.py in make_block(self, values, placement)
    250             placement = self.mgr_locs
    251         if self.is_extension:
--> 252             values = _block_shape(values, ndim=self.ndim)
    253 
    254         return make_block(values, placement=placement, ndim=self.ndim)

~/.pyenv/versions/3.7.0/envs/fair_ml/lib/python3.7/site-packages/pandas/core/internals/blocks.py in _block_shape(values, ndim)
   2745             # block.shape is incorrect for ""2D"" ExtensionArrays
   2746             # We can't, and don't need to, reshape.
-> 2747             values = values.reshape(tuple((1,) + shape))  # type: ignore
   2748     return values
   2749 

~/.pyenv/versions/3.7.0/envs/fair_ml/lib/python3.7/site-packages/pandas/core/generic.py in __getattr__(self, name)
   5128             if self._info_axis._can_hold_identifiers_and_holds_name(name):
   5129                 return self[name]
-> 5130             return object.__getattribute__(self, name)
   5131 
   5132     def __setattr__(self, name: str, value) -> None:

AttributeError: 'Series' object has no attribute 'reshape'

```
#### Expected Output
a 0
b 0
c 1

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.0.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Sun Jul  5 00:43:10 PDT 2020; root:xnu-6153.141.1~9/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 18.1
setuptools       : 47.3.1
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.13
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
687510191,35936,REGR: Fix inplace updates on column to set correct values,jorisvandenbossche,closed,2020-08-27T20:11:50Z,2020-08-31T13:09:24Z,Closes #35731
688534112,35975,"BUG: When the value in the data set has underscore i.e ""_"" and forward slash ""/"", the select query with where clause does not fetch actual result set.",santhosh92c,closed,2020-08-29T13:34:58Z,2020-08-31T13:31:13Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here

```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
687646188,35939,TYP: annotations in core.groupby,jbrockmendel,closed,2020-08-28T01:34:17Z,2020-08-31T14:44:24Z,"I'm still seeing a couple of mypy complaints, suggestions @simonjayhawkins ?"
688388623,35968,TYP: annotate plotting._matplotlib.tools,jbrockmendel,closed,2020-08-28T22:30:05Z,2020-08-31T14:45:45Z,"Same idea as #35960, focused on clarifying Axis vs Axes"
688739164,35990,TYP: misc typing fixes for pandas\core\frame.py,simonjayhawkins,closed,2020-08-30T15:29:57Z,2020-08-31T16:23:51Z,"pandas\core\frame.py:1091: error: namedtuple() expects a string literal as the first argument  [misc]
pandas\core\frame.py:4594: error: Need type annotation for 'names' (hint: ""names: List[<type>] = ..."")  [var-annotated]"
689270222,36011,CI: suppress another setuptools warning,jbrockmendel,closed,2020-08-31T14:53:54Z,2020-08-31T16:57:40Z,Seeing this on some new PRs.  xref #35252
688613920,35981,DOC clean up doc/source/getting_started/overview.rst,0xpranjal,closed,2020-08-29T22:26:24Z,2020-08-31T18:24:15Z,"- [x] closes #35980

"
688756272,35994,TYP: type error in `pandas/io/excel/_xlsxwriter.py`,fangchenli,closed,2020-08-30T17:16:05Z,2020-08-31T18:28:17Z,"Those are some of the typing errors generated by mypy after unpining it.

```
pandas/io/excel/_xlsxwriter.py:105: error: ""object"" has no attribute ""__iter__""; maybe ""__str__"" or ""__dir__""? (not iterable)  [attr-defined]
pandas/io/excel/_xlsxwriter.py:196: error: ""None"" has no attribute ""close""  [attr-defined]
pandas/io/excel/_xlsxwriter.py:207: error: ""None"" has no attribute ""add_worksheet""  [attr-defined]
pandas/io/excel/_xlsxwriter.py:225: error: ""None"" has no attribute ""add_format""  [attr-defined]
```"
688613441,35980,DOC: doc/source/getting_started/overview.rst,0xpranjal,closed,2020-08-29T22:21:55Z,2020-08-31T18:31:36Z,"#### Location of the documentation

""https://pandas.pydata.org/docs/getting_started/overview.html""

#### Documentation problem

Unnecessary words and hyphens were missing.

#### Suggested fix for documentation

E.g. real world can be replaced by real-world
"
687695490,35941,DOC: complement the documentation for pandas.DataFrame.agg #35912,onshek,closed,2020-08-28T04:06:05Z,2020-08-31T18:40:16Z,"- [x] closes #35912
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
<img width=""676"" alt=""Screenshot 2020-08-28 at 11 59 23"" src=""https://user-images.githubusercontent.com/21543236/91519967-a71dd280-e926-11ea-87f5-e647fe650168.png"">
"
686611587,35912,DOC: Aggregate and rename is undocumented,rhshadrach,closed,2020-08-26T20:57:51Z,2020-08-31T18:43:09Z,"The function `DataFrame.agg` allows for kwargs to aggregate and rename the results.

    df = pd.DataFrame({'a': [1, 2, 3]})
    print(df.agg(a_new=('a', 'sum')))

results in

           a
    a_new  6

Perhaps I'm missing it, but I can't find it in both the API reference and the User Guide."
683178499,35837,Update SparseDtype user guide doc,dsaxton,closed,2020-08-21T00:40:57Z,2020-08-31T18:47:20Z,Tiny doc nit. The doc says you can pass both dtype and fill_value to SparseDtype then only passes one in the example.
675666043,35640,DOC: Add specific Visual Studio Installer instructions,bdforbes,closed,2020-08-09T09:12:42Z,2020-08-31T18:47:35Z,"- [ ] further improves #28316 - Currently the 'Contributing to pandas', 'Installing a C compiler' section advises to install Build Tools for Visual Studio 2017 but doesn't specify which components to install.

Notes:

- The first time I installed the Build Tools it didn't seem to get everything needed, and I had to search around for a while before I could figure out what I'd missed.
- The advice at [python.org](https://wiki.python.org/moin/WindowsCompilers#Microsoft_Visual_C.2B-.2B-_14.2_standalone:_Build_Tools_for_Visual_Studio_2019_.28x86.2C_x64.2C_ARM.2C_ARM64.29) is more specific, so I have added that in here. I'm fairly sure that's the minimum required, correct?
"
685102098,35883,REF: use BlockManager.apply for Rolling.count,jbrockmendel,closed,2020-08-25T02:12:19Z,2020-08-31T19:21:05Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
688756701,35995,TYP: typing errors in _xlsxwriter.py #35994,fangchenli,closed,2020-08-30T17:18:37Z,2020-08-31T20:03:42Z,"- [x] closes #35994
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
688644830,35982,CLN: window/rolling.py,mroeschke,closed,2020-08-30T03:24:03Z,2020-08-31T22:11:38Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
69714194,9959,columns selection after groupby reset group_keys to True,ruoyu0088,closed,2015-04-21T00:56:18Z,2020-08-31T22:31:08Z,"```
df = pd.DataFrame({""g"":[1, 2, 1, 2, 1], ""a"":range(5), ""b"":range(1, 6), ""c"":range(2, 7)})
g = df.groupby(""g"", group_keys=False)
print g.group_keys  #False
print g[[""a"", ""b"", ""c""]].group_keys  #True
```

So the `apply()` results are different for these two case:

```
print g.apply(lambda x:x[:2])
```

ouput:

```
   a  b  c  g
0  0  1  2  1
2  2  3  4  1
1  1  2  3  2
3  3  4  5  2
```

but 

```
print g[[""a"", ""b"", ""c""]].apply(lambda x:x[:2])
```

output:

```
     a  b  c
g           
1 0  0  1  2
  2  2  3  4
2 1  1  2  3
  3  3  4  5
```
"
667468615,35444,BUG: Attributes are lost when subsetting columns in groupby,rhshadrach,closed,2020-07-29T00:46:49Z,2020-08-31T22:37:50Z,"- [x] closes #9959
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Avoiding the behavior of Series here, e.g. `df.groupby('a')['b']` as I think that will involve some API changes. Will follow up: ref #35443"
681018225,35788,BUG: Series.groupby returns an error if index is float and size is >= 1000000,MarcosCarreira,closed,2020-08-18T12:53:17Z,2020-09-01T01:20:41Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```
# Imports
import numpy as np
import pandas as pd

# Values
nsteps = 5*10**6
sv = np.random.normal(loc=100.0, scale=1.0, size=nsteps+1)

# Series with int index
se = pd.Series(sv)

# Series with float index
flind = np.arange(nsteps+1)/nsteps
sef = pd.Series(sv, index=flind)

# Group with int index works
seg = se.groupby(level=0).last()

# Group with float index and size<1000000 works
sefg = sef.iloc[:999999].groupby(level=0).last()

# Group with float index and size>=1000000 doesn't work
sefg2 = sef.iloc[:1000000].groupby(level=0).last()
```

#### Problem description

Series.groupby returns an error if index is float and size is >= 1000000 (no problem with this size if index is int):

File ""pandas/_libs/index.pyx"", line 345, in pandas._libs.index._bin_search
TypeError: '<' not supported between instances of 'float' and 'NoneType'
![Screenshot 2020-08-18 at 13 51 08](https://user-images.githubusercontent.com/14150204/90515006-e5aed280-e159-11ea-8b05-4a56aa568aa6.png)


#### Expected Output

sefg2 should be similar to sefg (with one additional row)
![Screenshot 2020-08-18 at 13 39 59](https://user-images.githubusercontent.com/14150204/90515138-1abb2500-e15a-11ea-85f4-60b72d389ebe.png)
![Screenshot 2020-08-18 at 13 39 23](https://user-images.githubusercontent.com/14150204/90515165-26a6e700-e15a-11ea-96fb-6ab7c749b674.png)


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.1.0.post20200704
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.50.1

</details>
"
596560918,33393,CI: Check private function not used across modules,ShaharNaveh,closed,2020-04-08T13:02:40Z,2020-09-01T01:57:44Z,"- [x] xref https://github.com/pandas-dev/pandas/pull/32942#issuecomment-609893282
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
434223869,26118,Should assert_frame_equal allow for a 'None' diff when there is actually a difference?,lovemeblender,closed,2019-04-17T11:07:25Z,2020-09-01T02:03:52Z,"#### Code Sample, a copy-pastable example if possible

From this module used in dataframes' comparison [here](https://github.com/pandas-dev/pandas/blob/master/pandas/util/testing.py#L829).

#### Problem description

First of all, great job, I am excited to be using Pandas and thanks for everything! :)

While asserting for dataframe equality in PyTest using PyCharm I get a message that some values are different (100.0%) but the diff is None (diff = None). This is rather confusing and since there is no obvious difference between these values I struggle to find it. 

#### Expected Output
If diff is none, I would expect for `assert_frame_equal` method to not throw an exception and return true. If the assertion is false and an exception is being raised I would expect it to always have some diff to show. If for any reason, having a ""diff"" is not feasible, I would expect this to be injected in the output message.

#### Output of ``pd.show_versions()``

<details>
Latest
</details>
"
688457737,35971,Comma cleanup,JonathanShrek,closed,2020-08-29T04:38:16Z,2020-09-01T02:47:23Z,Comma cleanup for #35925 
689172023,36008,TYP: check_untyped_defs core.internals.concat,simonjayhawkins,closed,2020-08-31T12:33:43Z,2020-09-01T09:16:11Z,"pandas\core\internals\concat.py:106: error: Incompatible types in assignment (expression has type ""Tuple[Any, ...]"", variable has type ""List[Any]"")  [assignment]
pandas\core\internals\concat.py:131: error: Incompatible types in assignment (expression has type ""Tuple[Any, ...]"", variable has type ""List[Any]"")  [assignment]
pandas\core\internals\concat.py:377: error: Need type annotation for 'upcast_classes'  [var-annotated]
pandas\core\internals\concat.py:378: error: Need type annotation for 'null_upcast_classes'  [var-annotated]"
689119799,36005,TYP: misc typing cleanup for core/computation/expressions.py,simonjayhawkins,closed,2020-08-31T11:07:47Z,2020-09-01T09:19:38Z,"pandas\core\computation\expressions.py:78: error: Need type annotation for 'dtypes' (hint: ""dtypes: Set[<type>] = ..."")  [var-annotated]
pandas\core\computation\expressions.py:258: error: Need type annotation for '_TEST_RESULT' (hint: ""_TEST_RESULT: List[<type>] = ..."")  [var-annotated]"
642377200,34902,DOC: Contributor guideline should guide contributors for expected local mypy failures,oguzhanogreden,closed,2020-06-20T13:36:54Z,2020-09-01T11:42:04Z,"#### Location of the documentation

[Location](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#style-guidelines).

#### Documentation problem

See conversation [here](https://gitter.im/pydata/pandas?at=5eee0e2ba813c72dcffccbb5).

#### Suggested fix for documentation

This 'failing' of mypy should be noted so that inexperienced contributors don't have to Google around and experienced ones don't have to explain repeatedly.

I came across this in [this context](https://github.com/pandas-dev/pandas/pull/32542#discussion_r442959117)."
648425793,35066,DOC: Remove mypy from pre commit,erfannariman,closed,2020-06-30T19:08:56Z,2020-09-01T11:42:05Z,"- [x] closes #34902
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
689334294,36013,Backport PR #36011 on branch 1.1.x (CI: suppress another setuptools warning),meeseeksmachine,closed,2020-08-31T16:28:59Z,2020-09-01T14:14:38Z,Backport PR #36011: CI: suppress another setuptools warning
689193900,36009,Backport PR #35936: (REGR: Fix inplace updates on column to set correct values),simonjayhawkins,closed,2020-08-31T13:08:37Z,2020-09-01T14:16:51Z,xref #35936
687642416,35938,REGR: Fix comparison broadcasting over array of Intervals,dsaxton,closed,2020-08-28T01:21:44Z,2020-09-01T14:55:45Z,"- [x] closes https://github.com/pandas-dev/pandas/issues/35931
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
687389797,35931,BUG: breaking change in df.replace() from 1.0.5 to 1.1.0,agnesbao,closed,2020-08-27T16:45:23Z,2020-09-01T14:56:22Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
df = pd.DataFrame({""age"": range(1,100)})
df[""age""] = pd.cut(df[""age""], [0, 50, 65, 99], right=False)
df.replace(to_replace={
    ""age"":{
        pd.Interval(0, 50, closed='left'): ""Ages < 50"",
        pd.Interval(50, 65, closed='left'): ""Ages 50-64"",
        pd.Interval(65, 99, closed='left'): ""Ages 65+""
    }
})
```

#### Problem description

The above sample code works in 1.0.5 but error out in 1.1.0 and above with error msg:
```
TypeError: Cannot compare types 'ndarray(dtype=object)' and 'Interval'
```
It only happens with pd.Interval dtype, nested dict of string to string mapping works fine. 

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : e3bcf8d1949833793ff50cca393240dab0f0d461
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+165.ge3bcf8d19
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200814
Cython           : 0.29.21
pytest           : 6.0.1
hypothesis       : 5.29.0
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : 1.3.3
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.8.0
fastparquet      : 0.4.1
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pyxlsb           : None
s3fs             : 0.2.0
scipy            : 1.5.2
sqlalchemy       : 1.3.19
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.0
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.1

</details>
"
688811296,35999,"BUG: None in Float64Index raising TypeError, should return False",jbrockmendel,closed,2020-08-30T23:24:59Z,2020-09-01T15:03:03Z,"- [x] closes #35788
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
690162222,36035,CI: pin s3fs for Windows py37_np18 on 1.1.x,simonjayhawkins,closed,2020-09-01T13:52:17Z,2020-09-01T15:26:59Z,
689823017,36027,TYP: add type annotation to `expr.py`,fangchenli,closed,2020-09-01T05:25:46Z,2020-09-01T15:45:47Z,
689819853,36026,DOC: Update documentation for pd.Interval if string endpoints are not allowed anymore,souris-dev,closed,2020-09-01T05:19:17Z,2020-09-01T16:15:33Z,"- [X] closes #36002 
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Removed outdated example in documentation for pd.Interval (string endpoints are not allowed anymore)."
689147378,36007,TYP: misc typing cleanup in core/indexes/multi.py,simonjayhawkins,closed,2020-08-31T11:52:25Z,2020-09-01T16:33:31Z,"pandas\core\indexes\multi.py:496: error: Need type annotation for 'arrays'  [var-annotated]
pandas\core\indexes\multi.py:722: error: Incompatible types in assignment (expression has type ""List[Any]"", variable has type ""FrozenList"")  [assignment]
pandas\core\indexes\multi.py:893: error: Incompatible types in assignment (expression has type ""List[Any]"", variable has type ""FrozenList"")  [assignment]
pandas\core\indexes\multi.py:2438: error: List item 0 has incompatible type ""slice""; expected ""str""  [list-item]
pandas\core\indexes\multi.py:3095: error: Unsupported left operand type for | (""None"")  [operator]"
689564340,36017,TYP: annotate plotting._matplotlib.misc,jbrockmendel,closed,2020-08-31T22:46:43Z,2020-09-01T16:43:16Z,
689887892,36029,remove trailing commas for #35925,Anshoo-Rajput,closed,2020-09-01T07:15:29Z,2020-09-01T16:46:34Z,"- [x] pandas/io/parquet.py
- [x] pandas/io/parsers.py
- [x] pandas/io/pytables.py
"
690219984,36041,"Backport PR #35999 on branch 1.1.x (BUG: None in Float64Index raising TypeError, should return False)",meeseeksmachine,closed,2020-09-01T15:03:13Z,2020-09-01T16:57:17Z,"Backport PR #35999: BUG: None in Float64Index raising TypeError, should return False"
688875214,36002,DOC: Update documentation for pd.Interval if string endpoints are not allowed anymore,souris-dev,closed,2020-08-31T03:46:07Z,2020-09-01T17:02:04Z,"#### Location of the documentation

[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Interval.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Interval.html)

On `master`:
[https://pandas.pydata.org/docs/dev/reference/api/pandas.Interval.html](https://pandas.pydata.org/docs/dev/reference/api/pandas.Interval.html)

#### Documentation problem

The examples in the documentation for `pd.Interval` show in the end that Intervals with string endpoints can also be created. However, when the example is run on `master`, a `ValueError` is raised, saying:

""Only numeric, Timestamp and Timedelta endpoints are allowed when constructing an Interval.""

#### Suggested fix for documentation

If Intervals with strings as endpoints are not allowed anymore, this example should be removed from the documentation.

I've confirmed that the docstring still contains this example, so I'd be happy to make a PR for this if we agree.
"
689810289,36024,TYP: add type annotation to `_xlwt.py`,fangchenli,closed,2020-09-01T05:00:16Z,2020-09-01T17:02:37Z,
689554588,36016,TYP: Annotate plotting stacker,jbrockmendel,closed,2020-08-31T22:23:53Z,2020-09-01T17:41:26Z,"In AreaPlot._plot we call `ax.fill_between`, which means `ax` must be an `Axes` object (and in particular, not an `Axis` object).  This chases down all the other places we can infer `Axes` from that.

Then some edits in an `__init__` to get mypy passing, and revert annotations of `_plot` in a few places because mypy complained about signature mismatch."
689772204,36023,Comma cleanup for #35925,JonathanShrek,closed,2020-09-01T03:16:46Z,2020-09-01T18:22:18Z,Comma cleanup for #35925
690215032,36039,Backport PR #35938 on branch 1.1.x (REGR: Fix comparison broadcasting over array of Intervals),meeseeksmachine,closed,2020-09-01T14:56:53Z,2020-09-01T18:29:08Z,Backport PR #35938: REGR: Fix comparison broadcasting over array of Intervals
690233170,36042,BUG: `apply` behaviour changed when using function that returns tuple,martijnvanattekum,closed,2020-09-01T15:20:42Z,2020-09-01T19:34:46Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import seaborn as sns
import pandas as pd
import numpy as np

def get_mean_sd(column):
    return np.mean(column), np.std(column)

iris = sns.load_dataset(""iris"")[[""sepal_length"", ""sepal_width"", ""petal_length"", ""petal_width""]]
iris.apply(get_mean_sd)
```

#### Problem description
In version 1.0, the result was a pandas series of tuples, whereas in 1.1 the result is a data frame, thus breaking existing code that depends on apply.

The result from 1.1 is
```
   sepal_length  sepal_width  petal_length  petal_width
0      5.843333     3.057333      3.758000     1.199333
1      0.825301     0.434411      1.759404     0.759693
```

#### Expected Output
The output from version 1.0 was:
```
sepal_length     (5.843333333333334, 0.8253012917851409)
sepal_width     (3.0573333333333337, 0.4344109677354946)
petal_length     (3.7580000000000005, 1.759404065775303)
petal_width     (1.1993333333333336, 0.7596926279021594)
dtype: object
```

#### Output of ``pd.show_versions()``

<details>

For 1.0.3:
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1127.18.2.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.utf-8
LANG             : en_US.utf-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.4.0.post20200518
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

For 1.1.1:
INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1127.18.2.el7.x86_64
Version          : #1 SMP Sun Jul 26 15:27:06 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.utf-8
LANG             : en_US.utf-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.4.0.post20200518
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
689764565,36021,"TYP/CLN: cleanup `_openpyxl.py`, add type annotation",fangchenli,closed,2020-09-01T02:56:11Z,2020-09-01T20:01:44Z,
642231537,34876,Cln 31942/replace appender with doc 6,smartvinnetou,closed,2020-06-19T21:42:56Z,2020-09-01T21:41:39Z,"- [x] ref #31942
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- fixes issues with the introduction of the doc decorator
"
585861455,32917,A strange bug in version 1.0.3,geeeekDing,closed,2020-03-23T01:06:45Z,2020-09-01T22:25:29Z,"

```python
# Your code here

import pandas as pd

```
#### Problem description
When I import pandas 1.03  in pycharm, an error message was returned. This error did not occur when I lowered the version of the package to 0.23.4. In addition, this error no occur in spyder .

Traceback (most recent call last):
  File ""E:/coherenceestimation/code/venv/ROC_classifier.py"", line 8, in <module>
    import pandas as pd
  File ""D:\Users\ThinkPad\Anaconda3\envs\MeachineLing\lib\site-packages\pandas\__init__.py"", line 124, in <module>
    from pandas.core.computation.api import eval
  File ""D:\Users\ThinkPad\Anaconda3\envs\MeachineLing\lib\site-packages\pandas\core\computation\api.py"", line 3, in <module>
    from pandas.core.computation.eval import eval
  File ""D:\Users\ThinkPad\Anaconda3\envs\MeachineLing\lib\site-packages\pandas\core\computation\eval.py"", line 14, in <module>
    from pandas.core.computation.engines import _engines
  File ""D:\Users\ThinkPad\Anaconda3\envs\MeachineLing\lib\site-packages\pandas\core\computation\engines.py"", line 9, in <module>
    from pandas.core.computation.ops import _mathops, _reductions
  File ""D:\Users\ThinkPad\Anaconda3\envs\MeachineLing\lib\site-packages\pandas\core\computation\ops.py"", line 17, in <module>
    from pandas.core.computation.scope import _DEFAULT_GLOBALS
  File ""D:\Users\ThinkPad\Anaconda3\envs\MeachineLing\lib\site-packages\pandas\core\computation\scope.py"", line 17, in <module>
    from pandas.compat.chainmap import DeepChainMap
  File ""D:\Users\ThinkPad\Anaconda3\envs\MeachineLing\lib\site-packages\pandas\compat\chainmap.py"", line 1, in <module>
    from typing import ChainMap, MutableMapping, TypeVar, cast
ImportError: cannot import name 'ChainMap'




"
579879301,32655,Error in py_convert_pandas_df(x),norapap,closed,2020-03-12T11:39:47Z,2020-09-01T22:28:07Z,"#### Problem description
Hello!

I have a fresh Rstudio where i want to work with some data which come from python scripts. I try to use reticulate to source the following python script:

```python
# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np
import functools as ft


pd.set_option('display.float_format', lambda x: '%.3f' % x)
pd.set_option('display.max_columns', 10)

def get_table(df, table_ref_no = all, col_ref_no = all, row_ref_no = all, \
              col_name = """", cols = [""Year"", ""Client"", ""Amount_thuf""], add_cols = []):
    
    sub_table = df
    
    sub_table = sub_table.reset_index().drop(columns = [""index""])
    
     # Selection
    if (cols != all):
        sub_table = sub_table[cols + add_cols]
    
    # Aggregation
    if (len(row_ref_no) > 1):
        sub_table = pd.DataFrame(sub_table.groupby([""Year"", ""Client""]).sum()).reset_index()
    
    # Renaming
    if (col_name != """"):
        sub_table = sub_table.rename(columns = {""Amount_thuf"" : col_name})

    return(sub_table)


excel_file_path = ""C:/Users/nori/Projects/data/""
excel_file_name = ""Data2016-2018_final.xlsx""
excel_sheet_name = ""Total""


orig_df = pd.read_excel(excel_file_path + excel_file_name, sheet_name = excel_sheet_name)
dfo = orig_df
dfo[""Amount_thuf""] = pd.to_numeric(dfo[""Amount_thuf""], errors = ""coerce"")
df = dfo

tmp5001 = get_table(df, ""Stat"", [""C0001""], [""R0001""], ""Eredmenytartalek"")
tmp5002 = get_table(df, ""Stat"", [""C0001""], [""R0002""], ""Karingadozasi_tartalek"")
tmp5003 = get_table(df, ""Stat"", [""C0001""], [""R0003""], ""Nagykarok_tartaleka"")
tmp5004 = get_table(df, ""Stat"", [""C0001""], [""R0004""], ""Adozas_utani_eredmeny"")
tmp3002 = get_table(dfo, ""S05"", [""C0200"", ""C0300""], [""R0210"", ""R1510""], ""Brutto_MSZD_direkt"")
tmp3021 = get_table(dfo, ""S05"", [""C0200""], [""R0220""], ""Nem-elet_MSZD_aranyos_VB"")

dfs500               = [tmp5001, tmp5002, tmp5003, tmp5004, tmp3002, tmp3021]
print(dfs500)

```

But when I call the following R code in Rstudio, I always got an error: 
Browse[1]> reticulate::source_python(paste0(project_folder,""data.py""))
Error in py_convert_pandas_df(x) :
SystemError: <built-in method item of numpy.ndarray object at 0x0000012709211440> returned a result with an error set

Detailed traceback:
File ""C:\Users\nori\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py"", line 791, in iteritems
for k in self.columns:
Browse[1]>

Everything (pandas, numpy, python, r, rstudio) is up to date

Thanks in advance,
Nora"
576906123,32491,"DOC indexing: Google search for 'pandas greater than' returns `pandas.DataFrame.ge` as #1, and `pandas.DataFrame.gt`/`pandas.Series.gt` aren't even found",smcinerney,closed,2020-03-06T12:39:19Z,2020-09-01T22:50:31Z,"#### Problem description

(I'm aware that Google SEO is not under pandas' control. However this one's quite important, and nowhere in the pandas doc does it actually tell us where/how to report SEO fails, if at all. So please tell us how and where to report things like this. Are all pandas doc pages automatically submitted to be crawled by Google?)

The indexing of pandas DataFrame/Series operators seems to have some holes:

A Google search for *'pandas greater than'* returns ['pandas.DataFrame.ge'](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ge.html) as #1, and `pandas.DataFrame.gt`/`pandas.Series.gt` aren't even found, let alone in the top 10.

1. The #1 hit should be `pandas.DataFrame.gt`
1. `pandas.Series.gt` should also be found, #2, but they're not. Similarly #3 `pandas.Series.between`, `pandas.Series.ge` etc.

A Google search for *'pandas less than'* is slightly better:
1. #1 hit is `pandas.DataFrame.le` (pandas 1.0.1 doc)
1. #2 hit is `pandas.Series.le` (pandas 1.0.1 doc)
1. #3 hit is `pandas.Series.between` (pandas 0.23.1 doc(?), not 1.0.x)
1. The #1 and #2 hits should be `pandas.DataFrame.lt`/`pandas.Series.lt`

#### Expected Output

as above"
552945993,31177,Setting multiindex with columns that contain list as element raises TypeError,matthewgson,closed,2020-01-21T15:22:20Z,2020-09-01T22:59:48Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame({'A': ['x1','x2','x3', 'x4'], 'B':[['v1','v2'],['v3','v4'],['v5','v6'],['v7','v8']], 'C':[['c1','c2'],['c3','c4','cc'],['c5','c6'],['c7','c8']],'D':[['d1','d2'],['d3','d4'],['d5','d6'],['d7','d8']], 'E':[['e1','e2'],['e3','e4'],['e5','e6'],['e7','e8']]})
df
  | A | B | C | D | E
-- | -- | -- | -- | -- | --
x1 | [v1, v2] | [c1, c2] | [d1, d2] | [e1, e2]
x2 | [v3, v4] | [c3, c4, cc] | [d3, d4] | [e3, e4]
x3 | [v5, v6] | [c5, c6] | [d5, d6] | [e5, e6]
x4 | [v7, v8] | [c7, c8] | [d7, d8] | [e7, e8]

```
#### Problem description
I was trying to set index with column A and column C (multiindex) in order to explode columns B,D,E only. (Column C should be excluded for explosion for one of its elements has different size)
Below code should do the job:
```
df.set_index(['A','C']).apply(pd.Series.explode).reset_index()
```
But pandas refuse to set multiindex with columns that contains list as their element, raising TypeError.

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\core\arrays\categorical.py in __init__(self, values, categories, ordered, dtype, fastpath)
    383             try:
--> 384                 codes, categories = factorize(values, sort=True)
    385             except TypeError:

~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\util\_decorators.py in wrapper(*args, **kwargs)
    207                     kwargs[new_arg_name] = new_arg_value
--> 208             return func(*args, **kwargs)
    209 

~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\core\algorithms.py in factorize(values, sort, order, na_sentinel, size_hint)
    671         labels, uniques = _factorize_array(
--> 672             values, na_sentinel=na_sentinel, size_hint=size_hint, na_value=na_value
    673         )

~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\core\algorithms.py in _factorize_array(values, na_sentinel, size_hint, na_value)
    507     uniques, labels = table.factorize(
--> 508         values, na_sentinel=na_sentinel, na_value=na_value
    509     )

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.factorize()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable._unique()

TypeError: unhashable type: 'list'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-32-c3b81bee4c46> in <module>
      1 # setting index with columns that contains list values : it causes error
----> 2 df.set_index(['A','C']) # error because C has list values in it.

~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\core\frame.py in set_index(self, keys, drop, append, inplace, verify_integrity)
   4459                 )
   4460 
-> 4461         index = ensure_index_from_sequences(arrays, names)
   4462 
   4463         if verify_integrity and not index.is_unique:

~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\core\indexes\base.py in ensure_index_from_sequences(sequences, names)
   5702         return Index(sequences[0], name=names)
   5703     else:
-> 5704         return MultiIndex.from_arrays(sequences, names=names)
   5705 
   5706 

~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\core\indexes\multi.py in from_arrays(cls, arrays, sortorder, names)
    418         from pandas.core.arrays.categorical import _factorize_from_iterables
    419 
--> 420         codes, levels = _factorize_from_iterables(arrays)
    421         if names is None:
    422             names = [getattr(arr, ""name"", None) for arr in arrays]

~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\core\arrays\categorical.py in _factorize_from_iterables(iterables)
   2814         # For consistency, it should return a list of 2 lists.
   2815         return [[], []]
-> 2816     return map(list, zip(*(_factorize_from_iterable(it) for it in iterables)))

~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\core\arrays\categorical.py in <genexpr>(.0)
   2814         # For consistency, it should return a list of 2 lists.
   2815         return [[], []]
-> 2816     return map(list, zip(*(_factorize_from_iterable(it) for it in iterables)))

~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\core\arrays\categorical.py in _factorize_from_iterable(values)
   2786         # but only the resulting categories, the order of which is independent
   2787         # from ordered. Set ordered to False as default. See GH #15457
-> 2788         cat = Categorical(values, ordered=False)
   2789         categories = cat.categories
   2790         codes = cat.codes

~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\core\arrays\categorical.py in __init__(self, values, categories, ordered, dtype, fastpath)
    384                 codes, categories = factorize(values, sort=True)
    385             except TypeError:
--> 386                 codes, categories = factorize(values, sort=False)
    387                 if dtype._ordered:
    388                     # raise, as we don't have a sortable data structure and so

~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\util\_decorators.py in wrapper(*args, **kwargs)
    206                 else:
    207                     kwargs[new_arg_name] = new_arg_value
--> 208             return func(*args, **kwargs)
    209 
    210         return wrapper

~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\core\algorithms.py in factorize(values, sort, order, na_sentinel, size_hint)
    670 
    671         labels, uniques = _factorize_array(
--> 672             values, na_sentinel=na_sentinel, size_hint=size_hint, na_value=na_value
    673         )
    674 

~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\core\algorithms.py in _factorize_array(values, na_sentinel, size_hint, na_value)
    506     table = hash_klass(size_hint or len(values))
    507     uniques, labels = table.factorize(
--> 508         values, na_sentinel=na_sentinel, na_value=na_value
    509     )
    510 

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.factorize()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable._unique()

TypeError: unhashable type: 'list'
```
####

Setting index with a single column with lists works fine.
```
df.set_index('C')
	A	B	D	E
C				
[c1, c2]	x1	[v1, v2]	[d1, d2]	[e1, e2]
[c3, c4, cc]	x2	[v3, v4]	[d3, d4]	[e3, e4]
[c5, c6]	x3	[v5, v6]	[d5, d6]	[e5, e6]
[c7, c8]	x4	[v7, v8]	[d7, d8]	[e7, e8]
```
#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 44.0.0.post20200106
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

</details>
"
535410179,30172,groupby .sum() includes non-pertinent combinations of grouping criteria in result with a 'NaN' value,RS574,closed,2019-12-10T00:04:26Z,2020-09-01T23:04:37Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

d1 = ['foo', 'bar'] * 50
d2 = list('ABCD' * 25)
d3 = pd.date_range('2019-01-01', periods=100, freq='D')
d4 = list(range(1, 101, 1))
d5 = list(range(1000, 100001, 1000))

df = pd.DataFrame(dict(col1=d1, col2=d2, col3=d3, col4=d4, col5=d5))

bins = [0, 30, float('inf')]

grouped = df.groupby(['col1', 'col2', pd.Grouper(key='col3', freq='MS'), 
                      pd.cut(df.col4, bins)])
summary = grouped['col5'].sum()

```
#### Problem description

When using pandas version 0.25.3, **_grouped['col5'].sum()_**  in the code above generates a result that has every possible combination of the grouping criteria, and shows 'NaN' for non-significant combinations (see screenshot of the results below). This behavior was not observed when **_.count()_** was used instead of **_.sum()_**. This behavior was also not observed in pandas version 0.24.2.

#### Output

![image](https://user-images.githubusercontent.com/58710879/70833873-74df3b80-1dbe-11ea-82e1-5789696a8d36.png)


#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.2.post20191203
Cython           : 0.29.14
pytest           : 5.3.1
hypothesis       : None
sphinx           : 2.2.2
blosc            : None
feather          : None
xlsxwriter       : 1.2.6
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.10.1
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.11
tables           : 3.6.1
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.6

</details>
"
568459157,32132,pandas import error,JASHWANTHREDDYTUPILI,closed,2020-02-20T17:29:57Z,2020-09-01T23:23:16Z,"
Description
What steps will reproduce the problem?
import pandas
Traceback (most recent call last):

File """", line 1, in
import pandas

File ""C:\Users\tjash\Anaconda3\lib\site-packages\pandas_init_.py"", line 54, in
from pandas.core.api import (

File ""C:\Users\tjash\Anaconda3\lib\site-packages\pandas\core\api.py"", line 29, in
from pandas.core.groupby import Grouper, NamedAgg

File ""C:\Users\tjash\Anaconda3\lib\site-packages\pandas\core\groupby_init_.py"", line 1, in
from pandas.core.groupby.generic import DataFrameGroupBy, NamedAgg, SeriesGroupBy

File ""C:\Users\tjash\Anaconda3\lib\site-packages\pandas\core\groupby\generic.py"", line 63, in
from pandas.core.groupby.groupby import (

File ""C:\Users\tjash\Anaconda3\lib\site-packages\pandas\core\groupby\groupby.py"", line 61, in
from pandas.core.groupby import base, ops

File ""C:\Users\tjash\Anaconda3\lib\site-packages\pandas\core\groupby\ops.py"", line 16, in
import pandas._libs.reduction as libreduction

File ""pandas_libs\reduction.pyx"", line 30, in init pandas._libs.reduction

AttributeError: type object 'pandas._libs.reduction.Reducer' has no attribute 'reduce_cython'

Versions
Spyder version: 4.0.1
Python version: 3.7.3
Qt version: 5.9.6
PyQt5 version: 5.9.2
Operating System: Windows 10
Dependencies
atomicwrites >=1.2.0         :  1.3.0 (OK)
chardet >=2.0.0              :  3.0.4 (OK)
cloudpickle >=0.5.0          :  1.3.0 (OK)
diff_match_patch >=20181111  :  20181111 (OK)
intervaltree                 :  None (OK)
IPython >=4.0                :  7.12.0 (OK)
jedi =0.14.1                 :  0.14.1 (OK)
nbconvert >=4.0              :  5.6.1 (OK)
numpydoc >=0.6.0             :  0.9.2 (OK)
pexpect >=4.4.0              :  4.8.0 (OK)
pickleshare >=0.4            :  0.7.5 (OK)
psutil >=0.3                 :  5.6.7 (OK)
pygments >=2.0               :  2.5.2 (OK)
pylint >=0.25                :  2.4.4 (OK)
pyls >=0.31.2;<0.32.0        :  0.31.7 (OK)
zmq >=17                     :  18.1.1 (OK)
qdarkstyle >=2.7             :  2.8 (OK)
qtawesome >=0.5.7            :  0.6.1 (OK)
qtconsole >=4.6.0            :  4.6.0 (OK)
qtpy >=1.5.0                 :  1.9.0 (OK)
rtree >=0.8.3                :  0.9.3 (OK)
sphinx >=0.6.6               :  2.4.0 (OK)
spyder_kernels >=1.8.1;<2.0.0:  1.8.1 (OK)
watchdog                     :  None (OK)
cython >=0.21                :  0.29.15 (OK)
matplotlib >=2.0.0           :  3.1.3 (OK)
numpy >=1.7                  :  1.18.1 (OK)
pandas >=0.13.1              :  1.0.1 (OK)
scipy >=0.17.0               :  1.4.1 (OK)
sympy >=0.7.3                :  1.5.1 (OK)"
620498539,34240,BUG: PeriodIndex.get_loc should raise KeyError,TomAugspurger,closed,2020-05-18T20:51:53Z,2020-09-01T23:32:01Z,"Currently PeriodIndex.get_loc raises a ValueError rather than a KeyError.

```python
In [11]: s = pd.Series([1, 2, 3], index=pd.period_range('2000', periods=3, name='A'))

In [12]: s.index.get_loc('A')
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-12-3a472f211c46> in <module>
----> 1 s.index.get_loc('A')

~/sandbox/pandas/pandas/core/indexes/period.py in get_loc(self, key, method, tolerance)
    501
    502             try:
--> 503                 asdt, reso = parse_time_string(key, self.freq)
    504             except DateParseError as err:
    505                 # A string with invalid format

~/sandbox/pandas/pandas/_libs/tslibs/parsing.pyx in pandas._libs.tslibs.parsing.parse_time_string()

~/sandbox/pandas/pandas/_libs/tslibs/parsing.pyx in pandas._libs.tslibs.parsing.parse_datetime_string_with_reso()

ValueError: Given date string not likely a datetime.
```

Compare that with DatetimeIndex

```python
In [13]: t = pd.Series([1, 2, 3], index=pd.date_range('2000', periods=3, name='A'))

In [14]: t.index.get_loc(""A"")
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/sandbox/pandas/pandas/_libs/tslibs/conversion.pyx in pandas._libs.tslibs.conversion.convert_str_to_tsobject()

~/sandbox/pandas/pandas/_libs/tslibs/parsing.pyx in pandas._libs.tslibs.parsing.parse_datetime_string()

ValueError: Given date string not likely a datetime.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
~/sandbox/pandas/pandas/core/indexes/datetimes.py in get_loc(self, key, method, tolerance)
    566             try:
--> 567                 key = self._maybe_cast_for_get_loc(key)
    568             except ValueError as err:

~/sandbox/pandas/pandas/core/indexes/datetimes.py in _maybe_cast_for_get_loc(self, key)
    594         # needed to localize naive datetimes
--> 595         key = Timestamp(key)
    596         if key.tzinfo is None:

~/sandbox/pandas/pandas/_libs/tslibs/timestamps.pyx in pandas._libs.tslibs.timestamps.Timestamp.__new__()

~/sandbox/pandas/pandas/_libs/tslibs/conversion.pyx in pandas._libs.tslibs.conversion.convert_to_tsobject()

~/sandbox/pandas/pandas/_libs/tslibs/conversion.pyx in pandas._libs.tslibs.conversion.convert_str_to_tsobject()

ValueError: could not convert string to Timestamp

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
<ipython-input-14-436c3fe5a8ed> in <module>
----> 1 t.index.get_loc(""A"")

~/sandbox/pandas/pandas/core/indexes/datetimes.py in get_loc(self, key, method, tolerance)
    567                 key = self._maybe_cast_for_get_loc(key)
    568             except ValueError as err:
--> 569                 raise KeyError(key) from err
    570
    571         elif isinstance(key, timedelta):

KeyError: 'A'
```

This leads to other issues like `Serie.__getitem__` and `Series.__contains__`:

```python
In [16]: ""A"" in t
Out[16]: False

In [17]: ""A"" in s
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-17-6445c229fb58> in <module>
----> 1 ""A"" in s

~/sandbox/pandas/pandas/core/generic.py in __contains__(self, key)
   1739     def __contains__(self, key) -> bool_t:
   1740         """"""True if the key is in the info axis""""""
-> 1741         return key in self._info_axis
   1742
   1743     @property

~/sandbox/pandas/pandas/util/_decorators.py in wrapper(*args, **kwargs)
    353         @wraps(func)
    354         def wrapper(*args, **kwargs) -> Callable:
--> 355             return func(*args, **kwargs)
    356
    357         # collecting docstring and docstring templates

~/sandbox/pandas/pandas/core/indexes/period.py in __contains__(self, key)
    332             hash(key)
    333             try:
--> 334                 self.get_loc(key)
    335                 return True
    336             except KeyError:

~/sandbox/pandas/pandas/core/indexes/period.py in get_loc(self, key, method, tolerance)
    501
    502             try:
--> 503                 asdt, reso = parse_time_string(key, self.freq)
    504             except DateParseError as err:
    505                 # A string with invalid format

~/sandbox/pandas/pandas/_libs/tslibs/parsing.pyx in pandas._libs.tslibs.parsing.parse_time_string()

~/sandbox/pandas/pandas/_libs/tslibs/parsing.pyx in pandas._libs.tslibs.parsing.parse_datetime_string_with_reso()

ValueError: Given date string not likely a datetime.
```

As part of fixing this, we should revert the change to `pandas/core/groupby/grouper.py` in https://github.com/pandas-dev/pandas/pull/34049."
641647080,34869,ENH: Vendor typing_extensions,WillAyd,closed,2020-06-19T01:48:57Z,2020-09-01T23:37:09Z,"I’ve discussed this with @simonjayhawkins here and there but never had a formal issue opened, hence this :-)

I think we need to vendor typing_extensions to realistically be able to unpin mypy from where it is now.  The main thing I think we need is to unlock Protocols; these aren’t added until Python 3.8 but as of mypy 0.75 seem to be the documented solution to a lot of the Mixin problems that we face:

https://mypy-lang.blogspot.com/2019/11/mypy-0.html

There are some other useful things like Literal, Final and TypedDict that this would unlock as well, to at least hold us over until 3.8 is the minimum supported version"
688858558,36000,ENH: vendor typing_extensions,jbrockmendel,closed,2020-08-31T02:47:39Z,2020-09-01T23:47:58Z,"- [x] closes #34869
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

haven't figured out how to make isort and black ignore this file"
662158342,35356,REF: Avoid post-processing in blockwise op,jbrockmendel,closed,2020-07-20T19:04:37Z,2020-08-08T17:30:51Z,"Small step towards #34714

cc @TomAugspurger IIRC you implemented split_frames/split_items, which this removes"
675528763,35628,TYP: update setup.cfg,simonjayhawkins,closed,2020-08-08T13:52:21Z,2020-08-08T18:03:00Z,
452051892,26647,json_normalize Support for Generators,msteijaert,closed,2019-06-04T15:06:31Z,2020-08-08T21:15:17Z,"The data argument in json_normalize requires be an dictionary or list of dictionaries. If an iterator is passed, unexpected behavior can occur. In particular, this will result in the loss of the first row. See https://stackoverflow.com/questions/56362810/missing-first-document-when-loading-multi-document-yaml-file-in-pandas-dataframe.

I think this should be catched, either by throwing an error (as proposed in https://github.com/pandas-dev/pandas/pull/26646) or by ensuring that the output for the iterator is the same as for the list created by ""materializing"" the iterator.
"
661940296,35352,to_sql functionality not working on python,cdu2620,closed,2020-07-20T15:15:36Z,2020-08-09T11:09:39Z,"Whenever I try to insert a pandas DF into my database, I get this error:
<img width=""844"" alt=""Screen Shot 2020-07-20 at 11 13 28 AM"" src=""https://user-images.githubusercontent.com/48495622/87954278-255e9d80-ca7a-11ea-90aa-12316ef8b105.png"">

It happens with any dataframe I try, even dataframes I had successfully inserted before. The specific test dataframe I was trying to insert is this:
<img width=""208"" alt=""Screen Shot 2020-07-20 at 11 14 49 AM"" src=""https://user-images.githubusercontent.com/48495622/87954346-41623f00-ca7a-11ea-9ba6-04b1e3709c1e.png"">

"
631114220,34581,BUG: read_csv with quoting=csv.QUOTE_NONNUMERIC error trying to convert header row,scimecac,closed,2020-06-04T20:10:56Z,2020-08-09T11:11:15Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here

```

#### Problem description
attempt to read_csv file with first row header. Using option quoting=csv.QUOTE_NONNUMERIC and header=0.  causes:

""ValueError: could not convert string to float: 'frid'""
   **(frid is my column name defined in the header row)

My text type fields are quoted and numeric are not. Looks like the quoting=csv.QUOTE_NONNUMERIC tries to convert non-quoted values to numeric, however it is also doing this to the header row.

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]
Most methods of dumping to csv do not quote the header row names. read_csv() function should not attempt to convert unquoted header row names.

#### Expected Output
Header row should not be subject to data conversion, as header names are always text.

#### Output of ``pd.show_versions()``
0.25.1


[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]
0.25.1
"
323105170,21047,Dataframe filtering duplicates columns,geriasd2,closed,2018-05-15T07:54:51Z,2020-08-09T16:23:23Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas
df = pandas.DataFrame(columns=['A', 'B'])
print(df.columns)
df = df.filter(items=['A', 'A', 'A'])
print(df.columns)
```
#### Problem description

Filtering a dataframe with a list which contains multiple times a column that is already inside the original dataframe results in a new dataframe, where the mentioned column is present multiple times. This problem might be fixed easily by converting the input list into a set.

#### Expected Output
A filtered dataframe which have only one column: 'A'.
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.3.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.22.0
pytest: None
pip: 9.0.1
setuptools: 28.8.0
Cython: None
numpy: 1.14.3
scipy: None
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0b10
sqlalchemy: 1.2.3
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
657293269,35284,To latex position,SylvainLan,closed,2020-07-15T11:57:25Z,2020-08-10T06:20:08Z,"- [ ] closes #35281 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
675918658,35648,BUG: Reading files fails when option use_inf_as_na is set to True,Khris777,closed,2020-08-10T07:05:20Z,2020-08-10T08:44:01Z,"- [ x] I have checked that this issue has not already been reported.

- [ x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
#### Code Sample, a copy-pastable example

```python
import pandas as pd
pd.set_option(""use_inf_as_na"", True)
pd.read_csv(some_csv_file)
```

#### Problem description

When the option `use_inf_as_na` is set to `True` reading any csv-file fails with this error:

```
  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\io\parsers.py"", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\io\parsers.py"", line 458, in _read
    data = parser.read(nrows)

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\io\parsers.py"", line 1201, in read
    df = DataFrame(col_dict, columns=columns, index=index)

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\frame.py"", line 467, in __init__
    mgr = init_dict(data, index, columns, dtype=dtype)

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\internals\construction.py"", line 250, in init_dict
    missing = arrays.isna()

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\series.py"", line 4795, in isna
    return super().isna()

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\generic.py"", line 7109, in isna
    return isna(self).__finalize__(self, method=""isna"")

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\dtypes\missing.py"", line 124, in isna
    return _isna(obj)

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\dtypes\missing.py"", line 157, in _isna
    return _isna_ndarraylike(obj, inf_as_na=inf_as_na)

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\dtypes\missing.py"", line 218, in _isna_ndarraylike
    result = _isna_string_dtype(values, dtype, inf_as_na=inf_as_na)

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\dtypes\missing.py"", line 246, in _isna_string_dtype
    vec = libmissing.isnaobj_old(values.ravel())

  File ""pandas\_libs\missing.pyx"", line 160, in pandas._libs.missing.isnaobj_old

ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
```

EDIT 1: Error also happens with `read_table()` and when feeding a `StringIO` object to `read_csv()`.

EDIT 2: `read_excel()` fails as well.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.8.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 79 Stepping 1, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en
LOCALE           : None.None

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 49.2.1.post20200802
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.3.1
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : 0.8.0
fastparquet      : 0.4.1
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 1.0.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.50.1

</details>
"
672952531,35549,BUG: Closed parameter in rolling function producing ValueError,nrcjea001,closed,2020-08-04T17:20:50Z,2020-08-10T13:13:31Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
df = pd.DataFrame({""column1"": range(6), 
                   ""column2"": range(6), 
                   'group': 3*['A','B'], 
                   'date':pd.date_range(""20190101"", periods=6)})
df.loc[:,'date']=df.loc[0,'date']
df
### Output
   column1  column2 group       date
0        0        0     A 2019-01-01
1        1        1     B 2019-01-01
2        2        2     A 2019-01-01
3        3        3     B 2019-01-01
4        4        4     A 2019-01-01
5        5        5     B 2019-01-01

df.groupby('group').rolling('1D',on='date',closed='left')['column1'].sum()
### Output
ValueError: closed only implemented for datetimelike and offset based windows
```

#### Problem description

Closed parameter not producing the result as it should. Only closed='both' is working.

Current workaround as follows (but considerably slower):
```
df.groupby('group').apply(lambda x: x.rolling('1D',on='date',closed='left')['column1'].sum())
```

#### Expected Output

```
group  date      
A      2019-01-01    NaN
       2019-01-01    0.0
       2019-01-01    2.0
B      2019-01-01    NaN
       2019-01-01    1.0
       2019-01-01    4.0
Name: column1, dtype: float64
```
Name: column1, dtype: float64


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 11, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.1.0
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 49.2.1.post20200802
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.48.0

</details>
"
675565195,35634,BUG: Pandas 1.1.0 apply function with axis=1 seems to mishandle the rows,arshad171,closed,2020-08-08T18:08:39Z,2020-08-10T13:21:13Z,"### The functionality of the **apply(func, axis=1)** in the newly released pandas==1.1.0 is not working as expected.

Pandas seems to be overwriting all the rows in the data frame with the 1st row present. This is happening specifically when introducing a new column in the data frame when running **func** method on each of the rows.

This working in pandas==1.0.5, but seems to be a bug in pandas=1.10.

I am attaching a sample script and the logs captured for pandas==1.0.5 and pandas==1.10.

### Attachments:
1. sample script to reproduce the issue (rename to .py before running) --> [script.txt](https://github.com/pandas-dev/pandas/files/5045977/script.txt)

2. output 1 (pandas==1.0.5) - working as expected --> [out_pandas_1.0.5.log](https://github.com/pandas-dev/pandas/files/5045978/out_pandas_1.0.5.log)


3. output 2 (pandas==1.1.0) - **buggy** --> [out_pandas_1.1.0.log](https://github.com/pandas-dev/pandas/files/5045980/out_pandas_1.1.0.log)


As you can see in the *out_pandas_1.1.0.log* log, after preprocessing the data frame using ``` df = df.apply(process_text, axis=1) ``` all the rows in the data frame have been overwritten with the 1st row.

This was not the case with pandas==1.0.5, check the **out_pandas_1.0.5.log** log.


### Environment
- OS: Ubuntu 20.04
- Python: 3.7.7 (anaconda env)

"
675425653,35618,tp_print is deprecated error,shyamsantoki,closed,2020-08-08T04:23:52Z,2020-08-10T13:22:25Z,"# tp_print is deprecated
I'm getting ```tp_print is deprecated``` whenever I try to install Pandas on Termux.
Here's few lines of error I'm getting
```
/data/data/com.termux/files/usr/include/python3.8/cpython/object.h:260:5: note: 'tp_print' has been explicitly marked deprecated he
      Py_DEPRECATED(3.8) int (*tp_print)(PyObject *, FILE *, int);
      ^
  /data/data/com.termux/files/usr/include/python3.8/pyport.h:515:54: note: expanded from macro 'Py_DEPRECATED'
  #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                       ^
  pandas/io/sas/sas.c:20655:3: error: 'tp_print' is deprecated [-Werror,-Wdeprecated-declarations]
    0, /*tp_print*/
    ^
  /data/data/com.termux/files/usr/include/python3.8/cpython/object.h:260:5: note: 'tp_print' has been explicitly marked deprecated he
      Py_DEPRECATED(3.8) int (*tp_print)(PyObject *, FILE *, int);
      ^
  /data/data/com.termux/files/usr/include/python3.8/pyport.h:515:54: note: expanded from macro 'Py_DEPRECATED'
  #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                       ^
  5 errors generated.
  building 'pandas._libs.json' extension
  creating build/temp.linux-aarch64-3.8/pandas/_libs/src/ujson
  creating build/temp.linux-aarch64-3.8/pandas/_libs/src/ujson/python
  creating build/temp.linux-aarch64-3.8/pandas/_libs/src/ujson/lib
  aarch64-linux-android-clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -fstack-protector-st
  aarch64-linux-android-clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -fstack-protector-st
  aarch64-linux-android-clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -fstack-protector-st
  aarch64-linux-android-clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -fstack-protector-st
  aarch64-linux-android-clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -fstack-protector-st
  aarch64-linux-android-clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -fstack-protector-st
  aarch64-linux-android-clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -fstack-protector-st
  aarch64-linux-android-clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -fstack-protector-st
  aarch64-linux-android-clang -shared -L/data/data/com.termux/files/usr/lib -Wl,-rpath=/data/data/com.termux/files/usr/lib -Wl,--enab
  error: command 'aarch64-linux-android-clang' failed with exit status 1
  ----------------------------------------
  ERROR: Failed building wheel for pandas
Failed to build pandas
```

However rolling back cython didn't work for me, I was still getting error. So I tried to rollback Pandas to 1.0.5 and it worked. So I think there's some bug in Pandas package from your side which results above error.

```
pip list
Package            Version
------------------ ---------
appdirs            1.4.4
argon2-cffi        20.1.0
asv                0.4.2
attrs              19.3.0
backcall           0.2.0
black              19.10b0
bleach             3.1.5
certifi            2020.6.20
cffi               1.14.1
chardet            3.0.4
click              7.1.2
Cython             0.29.21
decorator          4.4.2
defusedxml         0.6.0
entrypoints        0.3
idna               2.10
ipykernel          5.3.4
ipython            7.17.0
ipython-genutils   0.2.0
ipywidgets         7.5.1
jedi               0.17.2
Jinja2             2.11.2
jsonschema         3.2.0
jupyter            1.0.0
jupyter-client     6.1.6
jupyter-console    6.1.0
jupyter-core       4.6.3
lxml               4.5.2
MarkupSafe         1.1.1
mistune            0.8.4
nbconvert          5.6.1
nbformat           5.0.7
notebook           6.1.1
numpy              1.19.1
packaging          20.4
pandas             1.0.5
pandocfilters      1.4.2
parso              0.7.1
pathspec           0.8.0
pexpect            4.8.0
pickleshare        0.7.5
pip                20.2.1
prometheus-client  0.8.0
prompt-toolkit     3.0.5
ptyprocess         0.6.0
pycparser          2.20
Pygments           2.6.1
pymongo            3.11.0
pyparsing          2.4.7
pyrsistent         0.16.0
python-dateutil    2.8.1
pytz               2020.1
pyzmq              19.0.2
qtconsole          4.7.5
QtPy               1.9.0
regex              2020.7.14
requests           2.24.0
Send2Trash         1.5.0
setuptools         49.2.1
six                1.15.0
terminado          0.8.3
testpath           0.4.4
toml               0.10.1
tornado            6.0.4
traitlets          4.3.3
typed-ast          1.4.1
urllib3            1.25.10
wcwidth            0.2.5
webencodings       0.5.1
wheel              0.34.2
widgetsnbextension 3.5.1
```"
671859430,35522,BUG: Fix assert_equal when check_exact=True for non-numeric dtypes #3…,ivirshup,closed,2020-08-03T07:20:45Z,2020-08-10T13:31:13Z,"`assert_series_equal(..., check_exact=True)` no longer raises an error when series has a non-numeric dtype. This fixes a bug/ regression introduced in `1.1.0`.

- [x] closes #35446
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I've added a very simple test, but could add more. For instance, there could be a check on `assert_frame_equal`, but I'm not sure what the desired strategy is for that.

I've left off the `whatsnew` entry for now since I figure that's the most likely to generate conflicts while this gets reviewed."
676139806,35651,Backport PR #35639 on branch 1.1.x (BUG: RollingGroupby with closed and column selection no longer raises ValueError),meeseeksmachine,closed,2020-08-10T13:32:34Z,2020-08-10T14:48:34Z,Backport PR #35639: BUG: RollingGroupby with closed and column selection no longer raises ValueError
676140006,35652,Backport PR #35522 on branch 1.1.x (BUG: Fix assert_equal when check_exact=True for non-numeric dtypes #3…),meeseeksmachine,closed,2020-08-10T13:32:51Z,2020-08-10T14:54:15Z,Backport PR #35522: BUG: Fix assert_equal when check_exact=True for non-numeric dtypes #3…
675106043,35609,BUG: Pandas merge DFs on index and column,carlosg-m,closed,2020-08-07T15:55:06Z,2020-08-10T15:15:34Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
# Your code here
df = pd.DataFrame([0.5,0.6,6,6,7,12],index=[0,0,1,1,1,2], columns=['distance'])

print(df)

   distance
0       0.5
0       0.6
1       6.0
1       6.0
1       7.0
2      12.0

result = df.merge(df.groupby(df.index)['distance'].min(), left_index=True, right_index=True, on='distance')

print(result)

   distance
0       0.5
0       0.6
1       6.0
1       6.0
1       7.0
2      12.0

```

#### Problem description

Merging two DataFrames using left and right index and a column does not seem to be possible.

I only have two options, reset the index on both DFs and merge on both columns which is slower, or create a multiindex which is unnecessary.

#### Expected Output

```
result = df.reset_index().merge(df.groupby(df.index)['distance'].min().reset_index(), on=['index','distance'])

print(result)

   index  distance
0      0       0.5
1      1       6.0
2      1       6.0
3      2      12.0
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.112+
Version          : #1 SMP Thu Jul 23 08:00:38 PDT 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.18.5
pytz             : 2018.9
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 49.2.0
Cython           : 0.29.21
pytest           : 3.6.4
hypothesis       : None
sphinx           : 1.8.5
blosc            : None
feather          : 0.4.1
xlsxwriter       : None
lxml.etree       : 4.2.6
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.6.1 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 5.5.0
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : 1.3.2
fsspec           : 0.8.0
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 2.5.9
pandas_gbq       : 0.11.0
pyarrow          : 0.14.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.18
tables           : 3.4.4
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.1.0
xlwt             : 1.3.0
numba            : 0.48.0

</details>
"
663864841,35381,Storage options,martindurant,closed,2020-07-22T15:47:31Z,2020-08-10T15:19:27Z,"- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
675645541,35639,BUG: RollingGroupby with closed and column selection no longer raises ValueError,mroeschke,closed,2020-08-09T06:21:50Z,2020-08-10T15:26:24Z,"- [x] closes #35549
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
675516353,35626,DEPR: Deprecate inplace param in MultiIndex.set_codes and MultiIndex.set_levels,topper-123,closed,2020-08-08T12:13:30Z,2020-08-10T17:52:24Z,"Deprecates ``inplace`` parameter in ``MultiIndex.set_codes`` and ``MultiIndex.set_levels``.

Allowing inplace ops in indexes makes them more complicated wtr. caching etc. After ``inplace`` is removed, ``MultiIndex`` will be immutable, except the ``names`` attribute, similarly to other index classes."
674608965,35592,REF: Simplify Index.copy,topper-123,closed,2020-08-06T21:22:05Z,2020-08-10T17:52:34Z,"Avoid the use of the ``Index.set_names`` in ``Index.copy``, which is a slightly awkward method to use here (it calls ``_shallow_copy`` + is longwinded).

This is archieved by tightening up the checks in ``_validate_names``."
675668665,35641,REF/PERF: Move MultiIndex._tuples to MultiIndex._cache,topper-123,closed,2020-08-09T09:33:24Z,2020-08-10T23:00:12Z,"Currently, the heavy-to-calculate ``MultiIndex.values`` attribute is cached in ``MultiIndex._tuples``. It would be more dogmatic to store it in ``MultiIndex._cache`` IMO, which is what this PR does.

This has the added benefit of ``._values`` getting copied over to new copies of the MultiIndex, so also gives a performance boost in cases where copying is needed:

```python
>>> n = 100_000;
>>> df = pd.DataFrame({'a': ['a', 'b'] * int(n / 2), 'b': range(n), 'c': range(20, n + 20)})
>>> mi = pd.MultiIndex.from_frame(df)
>>> mi.values # also caches mi._values in mi._cache
array([('a', 0, 20), ('b', 1, 21), ('a', 2, 22), ...,
       ('b', 99997, 100017), ('a', 99998, 100018), ('b', 99999, 100019)],
      dtype=object)
>>> %timeit mi._shallow_copy().values
22.8 ms ± 3.43 ms per loop  # master
34.6 µs ± 997 ns per loop  # this PR
```
"
675040411,35608,Doc notes for core team members,TomAugspurger,closed,2020-08-07T14:07:24Z,2020-08-10T23:02:17Z,
667930415,35462,BUG: dataframe.apply() loops on first row when applied method attempts to modify the row,nicolasrozain,closed,2020-07-29T15:21:58Z,2020-08-11T00:01:58Z,"With pandas 1.1.0 on Python 3.6.8:

```
>>> import pandas as pd
>>> df = pd.DataFrame({'a': list(range(0,100)), 'b': list(range(100,200))})
>>> def func(row):
...     row.loc['a'] += 1
...     return row
... 
>>> df
     a    b
0    0  100
1    1  101
2    2  102
3    3  103
4    4  104
..  ..  ...
95  95  195
96  96  196
97  97  197
98  98  198
99  99  199
[100 rows x 2 columns]
>>> df.apply(func, axis=1)
      a    b
0   100  100
1   100  100
2   100  100
3   100  100
4   100  100
..  ...  ...
95  100  100
96  100  100
97  100  100
98  100  100
99  100  100
[100 rows x 2 columns]
>>> df
      a    b
0   100  100
1     1  101
2     2  102
3     3  103
4     4  104
..  ...  ...
95   95  195
96   96  196
97   97  197
98   98  198
99   99  199
[100 rows x 2 columns]
>>> pd.show_versions()
INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.6.8.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.17763
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.1.0
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 42.0.2
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 2.5.14
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.3.3
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : None
```

With pandas 1.0.5:
```
>>> import pandas as pd
>>> df = pd.DataFrame({'a': list(range(0,100)), 'b': list(range(100,200))})
>>> def func(row):
...     row.loc['a'] += 1
...     return row
... 
>>> df
     a    b
0    0  100
1    1  101
2    2  102
3    3  103
4    4  104
..  ..  ...
95  95  195
96  96  196
97  97  197
98  98  198
99  99  199
[100 rows x 2 columns]
>>> df.apply(func, axis=1)
      a    b
0     1  100
1     2  101
2     3  102
3     4  103
4     5  104
..  ...  ...
95   96  195
96   97  196
97   98  197
98   99  198
99  100  199
[100 rows x 2 columns]
>>> df
      a    b
0     1  100
1     2  101
2     3  102
3     4  103
4     5  104
..  ...  ...
95   96  195
96   97  196
97   98  197
98   99  198
99  100  199
[100 rows x 2 columns]
>>> pd.show_versions()
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.0.5
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 42.0.2
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 2.5.14
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.3.3
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : None
numba            : None
```

I expected the behavior of 1.0.5 in 1.1.0, did I misunderstood the apply method?
Thank you for your help."
662907258,35365,"BUG: read_csv fails with float_precision=""round_trip"" and decimal="",""",ales-erjavec,closed,2020-07-21T11:27:45Z,2020-08-11T00:36:27Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

---

#### Code Sample, a copy-pastable example

```python
import io
import pandas as pd

content = """"""\
6,5;3,3
3,4;1,0
""""""

pd.read_csv(io.StringIO(content), delimiter="";"", header=None,
            dtype=float, decimal="","", float_precision=""round_trip"")

```

#### Problem description

read_csv fails if using `float_precision=""round_trip""` and `decimal="",""`  (or any other decimal and thousands separator).

It raises an 
```
Traceback (most recent call last):
  File ""pandas/_libs/parsers.pyx"", line 1152, in pandas._libs.parsers.TextReader._convert_tokens
TypeError: Cannot cast array from dtype('O') to dtype('float64') according to the rule 'safe'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 10, in <module>
  File ""/Users/aleserjavec/virtual/py37/lib/python3.7/site-packages/pandas/io/parsers.py"", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""/Users/aleserjavec/virtual/py37/lib/python3.7/site-packages/pandas/io/parsers.py"", line 454, in _read
    data = parser.read(nrows)
  File ""/Users/aleserjavec/virtual/py37/lib/python3.7/site-packages/pandas/io/parsers.py"", line 1133, in read
    ret = self._engine.read(nrows)
  File ""/Users/aleserjavec/virtual/py37/lib/python3.7/site-packages/pandas/io/parsers.py"", line 2037, in read
    data = self._reader.read(nrows)
  File ""pandas/_libs/parsers.pyx"", line 860, in pandas._libs.parsers.TextReader.read
  File ""pandas/_libs/parsers.pyx"", line 875, in pandas._libs.parsers.TextReader._read_low_memory
  File ""pandas/_libs/parsers.pyx"", line 952, in pandas._libs.parsers.TextReader._read_rows
  File ""pandas/_libs/parsers.pyx"", line 1084, in pandas._libs.parsers.TextReader._convert_column_data
  File ""pandas/_libs/parsers.pyx"", line 1158, in pandas._libs.parsers.TextReader._convert_tokens
ValueError: could not convert string to float: '6,5'
```

#### Expected Output

```
     0    1
0  6.5  3.3
1  3.4  1.0
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.16.2
pytz             : 2018.3
dateutil         : 2.6.1
pip              : 20.1.1
setuptools       : 39.0.1
Cython           : 0.29.13
pytest           : 3.7.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.1.5
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 6.2.1
pandas_datareader: None
bs4              : 4.6.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.3.0
numexpr          : 2.6.6
odfpy            : None
openpyxl         : 3.0.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 3.7.1
pyxlsb           : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.1.0
xlwt             : None
xlsxwriter       : 1.1.5
numba            : 0.41.0

</details>
"
663691722,35377,[FIX] Handle decimal and thousand separator in 'round_trip' converer,ales-erjavec,closed,2020-07-22T11:32:33Z,2020-08-11T00:36:31Z,"- [x] closes #35365
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

In case of non c-locale decimal and tsep, copy and fixup the source string before passing it to PyOS_string_to_double"
627600039,34472,DEPS: drop 3.6,jreback,closed,2020-05-29T23:45:57Z,2020-08-11T01:17:48Z,"according to schedule we should 3.6 in june 2020

https://numpy.org/neps/nep-0029-deprecation_policy.html

prob ok to keep for the 1.1 release then should drop for 1.2. we will likely be adding. 3.9 support so this makes sense."
675563401,35633,BUG: DataFrame.apply with func altering row in-place,jbrockmendel,closed,2020-08-08T17:55:50Z,2020-08-11T08:40:19Z,"closes #35462
closes #35634 

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
676693065,35666,Backport PR #35633 on branch 1.1.x (BUG: DataFrame.apply with func altering row in-place),meeseeksmachine,closed,2020-08-11T08:40:57Z,2020-08-11T09:41:03Z,Backport PR #35633: BUG: DataFrame.apply with func altering row in-place
675686341,35642,BUG:Resample with groupby & agg(),BassKot,closed,2020-08-09T11:50:34Z,2020-08-11T11:08:40Z,"- [ X] I have checked that this issue has not already been reported.

- [ all versions] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

data = pd.DataFrame({
    'cat': ['cat_1', 'cat_1', 'cat_2', 'cat_1', 'cat_2', 'cat_1', 'cat_2', 'cat_1'],
    'num': [5,20,22,3,4,30,10,50],
    'date': ['2019-2-1', '2018-02-03','2020-3-11','2019-2-2', '2019-2-2', '2018-12-4','2020-3-11', '2020-12-12']
})
data['date'] = pd.to_datetime(data['date'])
aggreg = data.groupby('cat').resample('Y', on='date')
summ_ = aggreg.sum()
agg_summ_ = aggreg.agg({'num': 'sum'})
summ_
agg_summ_

```

#### Problem description

When I want aggregate all columns by sum, example with summ_ calculates normal, but if I do example with agg it calculates incorrect.

#### Expected Output
summ_
#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Sun Jul  5 00:43:10 PDT 2020; root:xnu-6153.141.1~9/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : ru_RU.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.18.4
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 20.0.2
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.2.1
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.1
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fsspec           : 0.5.2
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.9
tables           : 3.5.2
tabulate         : 0.8.7
xarray           : 0.16.0
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.45.1

</details>
"
675301107,35613,REF: use consistent pattern in tslibs.vectorized,jbrockmendel,closed,2020-08-07T21:31:30Z,2020-08-11T15:30:43Z,"DO NOT MERGE - Performance takes a hit

We have edited some of the functions in `tslibs.vectorized to do some pre-processing in order to de-duplicate some code.  This new pattern is conducive to refactoring out helper functions for further de-duplication.

So far, moving to the new pattern has been performance-neutral or better.  But for the functions which this branch moves to the new pattern, performance takes a hit, particularly for dt64arr_to_periodarray.  I am at a loss as to why this would be, hoping more eyeballs will help.  cc @WillAyd

```
asv continuous -E virtualenv -f 1.01 master HEAD -b tslibs
[...]
       before           after         ratio
     [d82e5403]       [9f5d9ef6]
     <master~4>       <ref-vectorized>
+      21.4±0.3ms         53.7±6ms     2.51  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 1011, datetime.timezone(datetime.timedelta(seconds=3600)))
+         248±4μs         613±50μs     2.48  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(10000, 3000, datetime.timezone(datetime.timedelta(seconds=3600)))
+      21.9±0.3ms         53.8±6ms     2.46  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 1000, datetime.timezone(datetime.timedelta(seconds=3600)))
+         238±8μs         574±60μs     2.41  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(10000, 1011, datetime.timezone(datetime.timedelta(seconds=3600)))
+         268±3μs         617±70μs     2.30  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(10000, 7000, datetime.timezone(datetime.timedelta(seconds=3600)))
+         246±5μs         562±70μs     2.28  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(10000, 2011, datetime.timezone(datetime.timedelta(seconds=3600)))
+        282±10μs         636±70μs     2.26  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(10000, 12000, datetime.timezone(datetime.timedelta(seconds=3600)))
+        274±20μs         616±70μs     2.24  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(10000, 11000, datetime.timezone(datetime.timedelta(seconds=3600)))
+         270±6μs         599±60μs     2.22  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(10000, 6000, datetime.timezone(datetime.timedelta(seconds=3600)))
+         248±4μs         544±40μs     2.20  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(10000, 2000, datetime.timezone(datetime.timedelta(seconds=3600)))
+      23.4±0.4ms         51.3±2ms     2.19  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 2000, datetime.timezone(datetime.timedelta(seconds=3600)))
+         278±8μs         605±70μs     2.18  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(10000, 8000, datetime.timezone(datetime.timedelta(seconds=3600)))
+         290±9μs        620±100μs     2.14  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(10000, 9000, datetime.timezone(datetime.timedelta(seconds=3600)))
+        286±40μs         611±60μs     2.14  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(10000, 10000, datetime.timezone(datetime.timedelta(seconds=3600)))
+        259±20μs         538±60μs     2.08  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(10000, 1000, datetime.timezone(datetime.timedelta(seconds=3600)))
+       35.5±10ms        73.5±20ms     2.07  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 9000, datetime.timezone(datetime.timedelta(seconds=3600)))
+         300±5μs         610±60μs     2.03  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(10000, 4006, datetime.timezone(datetime.timedelta(seconds=3600)))
+         289±8μs         586±60μs     2.03  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(10000, 4000, datetime.timezone(datetime.timedelta(seconds=3600)))
+       38.3±10ms        77.3±20ms     2.02  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 10000, datetime.timezone(datetime.timedelta(seconds=3600)))
+       36.6±10ms        70.9±20ms     1.94  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 11000, datetime.timezone(datetime.timedelta(seconds=3600)))
+        26.1±1ms         49.9±1ms     1.91  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 2011, datetime.timezone(datetime.timedelta(seconds=3600)))
+       38.8±10ms        72.4±20ms     1.87  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 12000, datetime.timezone(datetime.timedelta(seconds=3600)))
+        27.0±3ms       50.2±0.5ms     1.86  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 3000, datetime.timezone(datetime.timedelta(seconds=3600)))
+       35.3±10ms        64.6±10ms     1.83  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 7000, datetime.timezone(datetime.timedelta(seconds=3600)))
+        35.2±9ms        64.3±10ms     1.83  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 6000, datetime.timezone(datetime.timedelta(seconds=3600)))
+        38.0±1ms        67.7±10ms     1.78  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 4006, datetime.timezone(datetime.timedelta(seconds=3600)))
+         358±7μs         637±70μs     1.78  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(10000, 5000, datetime.timezone(datetime.timedelta(seconds=3600)))
+      9.43±0.2μs         16.7±2μs     1.77  tslibs.resolution.TimeResolution.time_get_resolution('s', 100, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
+     3.10±0.04μs         5.49±1μs     1.77  tslibs.fields.TimeGetStartEndField.time_get_start_end_field(1, 'start', 'month', None, 5)
+       37.7±10ms        66.5±10ms     1.76  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 8000, datetime.timezone(datetime.timedelta(seconds=3600)))
+        33.5±5ms         57.0±2ms     1.70  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 4000, datetime.timezone(datetime.timedelta(seconds=3600)))
+      26.5±0.1μs        42.8±10μs     1.61  tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc(100, datetime.timezone(datetime.timedelta(seconds=3600)))
+        45.3±8ms        70.5±10ms     1.56  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 5000, datetime.timezone(datetime.timedelta(seconds=3600)))
+      24.7±0.2μs        38.2±10μs     1.54  tslibs.tz_convert.TimeTZConvert.time_tz_localize_to_utc(100, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
+     1.55±0.08ms       2.36±0.2ms     1.52  tslibs.normalize.Normalize.time_is_date_array_normalized(1000000, datetime.timezone(datetime.timedelta(seconds=3600)))
+         327±4ms        491±100ms     1.50  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('time', 1000000, None)
+      24.7±0.9ms        36.6±10ms     1.48  tslibs.fields.TimeGetDateField.time_get_date_field(1000000, 'dow')
+     3.29±0.06μs         4.87±1μs     1.48  tslibs.fields.TimeGetStartEndField.time_get_start_end_field(1, 'start', 'month', None, 12)
+         338±2ms        497±100ms     1.47  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('time', 1000000, datetime.timezone(datetime.timedelta(seconds=3600)))
+      22.7±0.3μs         32.0±3μs     1.41  tslibs.normalize.Normalize.time_is_date_array_normalized(10000, datetime.timezone(datetime.timedelta(seconds=3600)))
+      12.2±0.5μs         17.1±4μs     1.41  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(100, 2000, datetime.timezone(datetime.timedelta(seconds=3600)))
+      12.0±0.4μs         16.5±4μs     1.38  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(100, 2011, datetime.timezone(datetime.timedelta(seconds=3600)))
+      12.1±0.3μs         16.8±2μs     1.38  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(100, 12000, datetime.timezone(datetime.timedelta(seconds=3600)))
+      12.2±0.2μs         16.6±2μs     1.36  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(100, 11000, datetime.timezone(datetime.timedelta(seconds=3600)))
+      11.7±0.3μs         15.9±1μs     1.35  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(100, 3000, datetime.timezone(datetime.timedelta(seconds=3600)))
+      12.6±0.3μs         16.5±2μs     1.31  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(100, 4000, datetime.timezone(datetime.timedelta(seconds=3600)))
+      8.89±0.6μs         11.3±1μs     1.27  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(0, 2000, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
+         115±2μs         143±20μs     1.25  tslibs.normalize.Normalize.time_is_date_array_normalized(10000, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
+      12.8±0.3μs         15.9±2μs     1.24  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(100, 6000, datetime.timezone(datetime.timedelta(seconds=3600)))
+        61.7±2μs         75.0±7μs     1.22  tslibs.normalize.Normalize.time_is_date_array_normalized(10000, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
+     7.58±0.08ms       8.61±0.1ms     1.14  tslibs.normalize.Normalize.time_is_date_array_normalized(1000000, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
+     8.80±0.07ms       9.55±0.4ms     1.09  tslibs.normalize.Normalize.time_normalize_i8_timestamps(1000000, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
-     3.46±0.05μs       3.06±0.1μs     0.89  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1, 3000, tzlocal())
-      25.2±0.9ms       21.2±0.5ms     0.84  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 2011, datetime.timezone.utc)
-      2.39±0.1ms       2.01±0.3ms     0.84  tslibs.normalize.Normalize.time_normalize_i8_timestamps(1000000, datetime.timezone(datetime.timedelta(seconds=3600)))
-      47.0±0.5ms       38.7±0.8ms     0.82  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 2011, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
-        38.5±3ms       31.2±0.4ms     0.81  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 3000, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
-        36.4±1ms       29.3±0.7ms     0.81  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 2011, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
-        25.7±3ms       20.7±0.2ms     0.80  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 3000, datetime.timezone.utc)
-        25.7±3ms       20.5±0.2ms     0.80  tslibs.period.TimeDT64ArrToPeriodArr.time_dt64arr_to_periodarr(1000000, 3000, None)
-     1.88±0.02ms        607±200μs     0.32  tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc(1000000, datetime.timezone.utc)
```"
654845210,35214,Drop Python 3.6 support,fangchenli,closed,2020-07-10T15:04:33Z,2020-08-11T16:10:21Z,"- [x] closes #34472 
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
676563272,35662,CI: troubleshoot ResourceWarnings,jbrockmendel,closed,2020-08-11T04:06:37Z,2020-08-12T18:01:36Z,"There's a file being left open somewhere, trying to track it down."
677208170,35679,BUG: DataFrame.to_pickle(bytes_io_buffer) is automatically closed internally ,le1nux,closed,2020-08-11T21:25:40Z,2020-08-12T22:23:26Z,"

#### Code Sample

```python
import pandas as pd
import io

# create example DataFrame
d = {'col1': [1, 2], 'col2': [3, 4]}
df = pd.DataFrame(data=d)

# load dataframe into bytesIO
buffer = io.BytesIO()
df.to_pickle(path=buffer)

# make sure buffer is still open
assert not buffer.closed
```

#### Problem description
Instead of dumping the binarized DataFrame on disk as e.g., by passing a file path or a file handle, I want to store its byte stream into an in memory bytesIO buffer, as shown above. Unfortunately, inside of `to_pickle` the bytesIO stream is already closed, thus rendering it useless. As far as I know the Python io API does not let you reopen a stream once it was closed. 

#### Expected Output
In my opinion it makes more sense to leave it to the user, when to close the buffer, e.g., by using a context manager:

```python
with io.BytesIO() as f:
    df.to_pickle(path=buffer)
   # do something with f 
```"
677655489,35686,BUG: to_pickle/read_pickle do not close user-provided file objects,twoertwein,closed,2020-08-12T12:30:50Z,2020-08-12T22:30:28Z,"- [x] closes #35679
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Do not close user-provided file-objects in `to_pickle` and `read_pickle`."
675562407,35632,REF: _cython_agg_blocks follow patterns similar to _apply_blockwise,jbrockmendel,closed,2020-08-08T17:48:52Z,2020-08-12T22:49:04Z,"Follows #35535 

This isn't yet identical to apply_blockwise, because both this and apply_blockwise have some odd behaviors that need to get standardized before they get combined into a BlockManager method, but this is a move in that direction."
675916613,35647,BUG: Support custom BaseIndexers in groupby.rolling,mroeschke,closed,2020-08-10T07:01:01Z,2020-08-13T06:14:05Z,"- [x] closes #35557
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
587654958,33011,pandas.errors.EmptyDataError: No columns to parse from file despite file being populated,Rob-murphys,closed,2020-03-25T12:17:02Z,2020-08-13T07:26:31Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.read_csv(filename, sep = r'\s{1,}')

```
#### Problem description
I am trying to read in a txt as a pandas dataframe that I previous edited a column header in using `.replace()` and I get the error:

```
Traceback (most recent call last):
  File ""dbcan_to_EC.py"", line 26, in <module>
    EC_generator(directory)
  File ""dbcan_to_EC.py"", line 20, in EC_generator
    df = pd.read_csv(directory, sep = "" "")
  File ""/home/lamma/.local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 685, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""/home/lamma/.local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 457, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File ""/home/lamma/.local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 895, in __init__
    self._make_engine(self.engine)
  File ""/home/lamma/.local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 1135, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File ""/home/lamma/.local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 1917, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File ""pandas/_libs/parsers.pyx"", line 545, in pandas._libs.parsers.TextReader.__cinit__
pandas.errors.EmptyDataError: No columns to parse from file

```

#### Sample of the inut file

```
Gene_ID	HMMER	Hotpep	DIAMOND	#ofTools
M32_00003	CBM16(54-167)+GH18(293-521)	GH18(11)+CBM16(1)	CBM16+GH18	3
M32_00048	GH87(65-567)	GH87(2)	GH87	3
M32_00053	GH13_30(36-384)	GH13(4)	GH13_30	3
M32_00083	CE4(60-180)	CE4(33)	CE4	3
M32_00093	GH16(86-322)	GH16(2)+CBM11(2)	GH16	3
M32_00101	GH15(232-611)	GH15(2)	GH15	3
M32_00106	GH114(62-231)	GH114(2)	GH114	3
M32_00164	GH3(61-286)	GH3(15)	GH3	3
M32_00173	GH1(30-487)	GH1(2)	GH1	3
M32_00174	GH2(358-783)	GH2(6)	GH2	3
```

#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1062.1.1.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.utf-8
LANG             : en_US.utf-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.4.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

"
678182252,35699,Backport PR #35647 on branch 1.1.x (BUG: Support custom BaseIndexers in groupby.rolling),meeseeksmachine,closed,2020-08-13T06:14:44Z,2020-08-13T10:14:52Z,Backport PR #35647: BUG: Support custom BaseIndexers in groupby.rolling
676208659,35654,BUG: GH-35558 merge_asof tolerance error,ikedaosushi,closed,2020-08-10T15:07:53Z,2020-08-13T10:17:17Z,"- [x] closes #35558
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
678323727,35702,Backport PR #35654 on branch 1.1.x (BUG: GH-35558 merge_asof tolerance error),meeseeksmachine,closed,2020-08-13T10:17:28Z,2020-08-13T11:04:51Z,Backport PR #35654: BUG: GH-35558 merge_asof tolerance error
678506048,35708,Reorganize imports to be compliant with isort (and conventional),el-iot,closed,2020-08-13T14:59:01Z,2020-08-13T16:21:47Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
678553227,35709,add web/ directory to isort checks,el-iot,closed,2020-08-13T16:04:03Z,2020-08-13T17:52:37Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
558413130,31522,AssertionError when grouping with max/min as aggregation functions (pandas-1.0.0),marcevrard,closed,2020-01-31T22:44:03Z,2020-08-13T18:02:48Z,"#### Code Sample

```python
import pandas as pd
import numpy as np

df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],
                   'key2' : ['one', 'two', 'one', 'two', 'one'],
                   'key3' : ['three', 'three', 'three', 'six', 'six'],
                   'data1' : np.random.randn(5),
                   'data2' : np.random.randn(5)})
df.groupby('key1').min()
```
#### Problem description

Since `pandas-1.0.0`, an `AssertionError` is thrown when grouping a `DataFrame` by a key and using `max`/`min` as aggregation functions. It works fine if only 1 key (other than the grouping key) is of the type `object` in the DataFrame, but it doesn't when the number of keys of type `object` is bigger than 1 (as shown in the example). This configuration worked fine on previous versions of `pandas` (e.g., `pandas-0.25.3`).

#### Expected Output

| key1   | key2   | key3   |    data1 |     data2 |
|:-------|:-------|:-------|---------:|----------:|
| a      | one    | six    | -0.67246 | -1.6302   |
| b      | one    | six    | -1.72628 | -0.907298 |

#### Output of ``pd.show_versions()``

commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.2.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : 0.29.14
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
tabulate         : 0.8.3
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0"
677103674,35676,PERF: make RangeIndex iterate over ._range,topper-123,closed,2020-08-11T18:35:41Z,2020-08-13T18:27:18Z,"Minor performance issue.

By adding a custom ``__iter__`` method to ``RangeIndex``, we partly avoid needing to create/cache the expensive ``_data`` attribute and partly it's just faster to iterate over a ``range`` than a ``ndarray``:

```python
>>> idx = pd.RangeIndex(100_000)
>>> %%timeit
... for _ in idx:
...     pass
10.9 ms ± 74.7 µs per loop  # master
6.11 ms ± 48.8 µs per loop  # this PR
>>> ""_data"" in idx._cache
True  # master
False  # this PR
```

xref #35432, #26565."
675736122,35645,BUG/ENH: consistent gzip compression arguments,twoertwein,closed,2020-08-09T17:11:31Z,2020-08-13T22:04:56Z,"- [x] closes #28103
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

`to_csv` let's the user set all keyword arguments for gzip. Depending on whether the user provides a filename or a file object different keyword arguments can be set (`gzip.open` vs `gzip.GzipFile`).

This PR always uses `gzip.GzipFile`. The additional keyword arguments valid for `gzip.open` but not valid for `gzip.GzipFile` (`encoding`, `errors`, and ~~`newline`~~) are still accessible:
https://github.com/pandas-dev/pandas/blob/aefae55e1960a718561ae0369e83605e3038f292/pandas/io/common.py#L512

Using `gzip.GzipFile`, also allows us to set `mtime` to create reproducible gzip archives.

"
677851782,35693,CI: avoid file leaks in sas_xport tests,jbrockmendel,closed,2020-08-12T17:15:52Z,2020-08-13T23:09:59Z,Introduces a contextmanager version of td.check_file_leaks so we can do more targeted versions of those checks for debugging
676346737,35657,BUG: ValueError: cannot convert float NaN to integer - on dataframe.reset_index(),capelastegui,closed,2020-08-10T18:58:43Z,2020-08-14T01:50:52Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---



#### Code Sample

```python
import pandas as pd
df = pd.DataFrame(
    dict(c1=[10.], c2=['a'], c3=pd.to_datetime('2020-01-01')))
# Triggering conditions: multiindex with date, empty dataframe

# Multiindex without date works
df.set_index(['c1', 'c2']).head(0).reset_index()

# Regular index with date also works
df.set_index(['c3']).head(0).reset_index()

# Multiindex with date crashes...
df.set_index(['c2', 'c3']).head(0).reset_index()
# >> ValueError: cannot convert float NaN to integer
# This used to work on pandas 1.0.3, but breaks on pandas 1.1.0

# Though the error doesn't trigger if the dataframe is empty before
# calling set_index()
df.head(0).set_index(['c2', 'c3']).reset_index()

# I originally observed the bug in a groupby call
df.head(0).groupby(['c2', 'c3'])[['c1']].sum().reset_index()
# >> ValueError: cannot convert float NaN to integer
# This used to work on pandas 1.0.3, but breaks on pandas 1.1.0
```

#### Problem description

On pandas 1.1.0, I'm getting a ValueError exception when calling dataframe.reset_index() under the following conditions:
- Input dataframe is empty
- Multiindex from multiple columns, at least one of which is a datetime

The exception message is `ValueError: cannot convert float NaN to integer`.

Error trace:
```
Error
Traceback (most recent call last):
    df_out.reset_index()
  File ""/Users/pec21/PycharmProjects/anp_voice_report/virtual/lib/python3.6/site-packages/pandas/core/frame.py"", line 4848, in reset_index
    level_values = _maybe_casted_values(lev, lab)
  File ""/Users/pec21/PycharmProjects/anp_voice_report/virtual/lib/python3.6/site-packages/pandas/core/frame.py"", line 4782, in _maybe_casted_values
    fill_value, len(mask), dtype
  File ""/Users/pec21/PycharmProjects/anp_voice_report/virtual/lib/python3.6/site-packages/pandas/core/dtypes/cast.py"", line 1554, in construct_1d_arraylike_from_scalar
    subarr.fill(value)
ValueError: cannot convert float NaN to integer
```

This error didn't happen on pandas 1.0.3 and earlier. I haven't tested any intermediate releases, nor the master branch.


#### Expected Output

No exception is raised, returns an empty dataframe.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.6.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.6.0
Version          : Darwin Kernel Version 18.6.0: Thu Apr 25 23:16:27 PDT 2019; root:xnu-4903.261.4~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_GB.UTF-8
pandas           : 1.1.0
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 49.1.0
Cython           : None
pytest           : 5.3.4
hypothesis       : None
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : None
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.2
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.3.3
sqlalchemy       : 1.3.11
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
674999864,35606,REGR: ValueError: cannot convert float NaN to integer - on dataframe.reset_index() in pandas 1.1.0,ndhansen,closed,2020-08-07T12:59:58Z,2020-08-14T01:50:52Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
from pandas.core.dtypes.cast import construct_1d_arraylike_from_scalar
from pandas.core.dtypes.missing import na_value_for_dtype
import numpy as np

dtype = np.datetime64().dtype
length = 10
works_value = np.NaN
fails_value = na_value_for_dtype(dtype)

# works
construct_1d_arraylike_from_scalar(works_value, length, dtype)

# fails
construct_1d_arraylike_from_scalar(fails_value, length, dtype)
```
Results in:
```
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
    construct_1d_arraylike_from_scalar(value, length, dtype)
  File ""/usr/lib/python3.8/site-packages/pandas/core/dtypes/cast.py"", line 1453, in construct_1d_arraylike_from_scalar
    subarr.fill(value)
ValueError: cannot convert float NaN to integer
```

#### Problem description

This is a problem was found when calling `reset_index` on a MultiIndex Dataframe with an index containing datetime information. The expected behaviour would be that it would return an array of NaTs, which ironically only happens if you pass it `np.NaN`.
I'm not familiar with the source code of numpy, so I can only speculate on how to fix it, but I would assume we would need special handling for time values, so we fill time arrays with `np.NaN`s instead of `pandas._libs.tslibs.nattype.NaTType`s.

#### Expected Output

`array(['NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT'], dtype=datetime64)`

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.7.9-1-MANJARO
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.5
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: 0.10.0dev0
bs4              : 4.9.1
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.2
matplotlib       : 3.3.0
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.5.1
sqlalchemy       : 1.3.17
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
675925933,35649,Refactor tables latex,SylvainLan,closed,2020-08-10T07:21:07Z,2020-08-14T06:13:01Z,"- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

As suggested [here](https://github.com/pandas-dev/pandas/pull/35284#issuecomment-665273834), utils fonctions which begin or end tables / tabulars / longtables environments could be merged. "
677178440,35678,August 2020 Developer Meeting (August 12th),jorisvandenbossche,closed,2020-08-11T20:33:00Z,2020-08-14T07:30:17Z,"The monthly dev meeting is Wednesday August 12th, at 18:00 UTC. Our calendar is at https://pandas.pydata.org/docs/development/meeting.html#calendar to check your local time.

Video Call: https://zoom.us/j/942410248
Minutes: https://docs.google.com/document/u/1/d/1tGbTiYORHiSPgVMXawiweGJlBw5dOkVJLY-licoBmBU/edit?ouid=102771015311436394588&usp=docs_home&ths=true

Please add items you'd like to see discussed to the agenda.

All are welcome to attend.

cc @pandas-dev/pandas-core @pandas-dev/pandas-triage "
677016637,35673,REGR: Dataframe.reset_index() on empty DataFrame with MI and datatime level,simonjayhawkins,closed,2020-08-11T16:24:25Z,2020-08-14T10:23:32Z,"- [ ] closes #35606
- [ ] closes #35657
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
678363383,35705,CI: pin isort < 5.4.0,simonjayhawkins,closed,2020-08-13T11:27:00Z,2020-08-14T10:29:10Z,xref #35703
675576122,35636,CLN: consistent signatures for equals methods,simonjayhawkins,closed,2020-08-08T19:28:27Z,2020-08-14T11:13:43Z,
679057459,35716,Backport PR #35673 on branch 1.1.x (REGR: Dataframe.reset_index() on empty DataFrame with MI and datatime level),meeseeksmachine,closed,2020-08-14T10:17:29Z,2020-08-14T11:15:48Z,Backport PR #35673: REGR: Dataframe.reset_index() on empty DataFrame with MI and datatime level
679092575,35718,CI: doctest failure for read_hdf on 1.1.x ,simonjayhawkins,closed,2020-08-14T11:24:39Z,2020-08-14T12:12:48Z,"NOTE: PR against 1.1.x branch

(fixed in #35214 on master)

xref https://github.com/pandas-dev/pandas/pull/35699#issuecomment-673392019"
673588480,35571,Change check_freq default to False,MaxWinterstein,closed,2020-08-05T14:34:01Z,2020-08-14T14:29:33Z,"- [x] closes #35570
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
673189882,35559,BUG: A read-only DataFrame cannot be .diff()'ed,dycw,closed,2020-08-05T01:46:19Z,2020-08-14T14:35:21Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import numpy as np
import pandas as pd

data = np.ones(2, dtype=int)
data.flags.writeable = False
df = pd.DataFrame(data)
df.diff()

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-2-df60a870b020> in <module>
      2 data.flags.writeable = False
      3 df = pd.DataFrame(data)
----> 4 df.diff()

~/miniconda3/envs/dts/lib/python3.8/site-packages/pandas/core/frame.py in diff(self, periods, axis)
   7247             return self.T.diff(periods, axis=0).T
   7248 
-> 7249         new_data = self._mgr.diff(n=periods, axis=bm_axis)
   7250         return self._constructor(new_data)
   7251 

~/miniconda3/envs/dts/lib/python3.8/site-packages/pandas/core/internals/managers.py in diff(self, n, axis)
    546 
    547     def diff(self, n: int, axis: int) -> ""BlockManager"":
--> 548         return self.apply(""diff"", n=n, axis=axis)
    549 
    550     def interpolate(self, **kwargs) -> ""BlockManager"":

~/miniconda3/envs/dts/lib/python3.8/site-packages/pandas/core/internals/managers.py in apply(self, f, align_keys, **kwargs)
    394                 applied = b.apply(f, **kwargs)
    395             else:
--> 396                 applied = getattr(b, f)(**kwargs)
    397             result_blocks = _extend_blocks(applied, result_blocks)
    398 

~/miniconda3/envs/dts/lib/python3.8/site-packages/pandas/core/internals/blocks.py in diff(self, n, axis)
   1265     def diff(self, n: int, axis: int = 1) -> List[""Block""]:
   1266         """""" return block for the diff of the values """"""
-> 1267         new_values = algos.diff(self.values, n, axis=axis, stacklevel=7)
   1268         return [self.make_block(values=new_values)]
   1269 

~/miniconda3/envs/dts/lib/python3.8/site-packages/pandas/core/algorithms.py in diff(arr, n, axis, stacklevel)
   1914         # TODO: can diff_2d dtype specialization troubles be fixed by defining
   1915         #  out_arr inside diff_2d?
-> 1916         algos.diff_2d(arr, out_arr, n, axis)
   1917     else:
   1918         # To keep mypy happy, _res_indexer is a list while res_indexer is

pandas/_libs/algos.pyx in pandas._libs.algos.diff_2d()

~/miniconda3/envs/dts/lib/python3.8/site-packages/pandas/_libs/algos.cpython-38-x86_64-linux-gnu.so in View.MemoryView.memoryview_cwrapper()

~/miniconda3/envs/dts/lib/python3.8/site-packages/pandas/_libs/algos.cpython-38-x86_64-linux-gnu.so in View.MemoryView.memoryview.__cinit__()

ValueError: buffer source array is read-only
```

#### Problem description

`df.diff()` does not seem like a data-mutating operation, at least not to me. My read is that `df.iloc[]` was given the same assessment in 2015 (#10043).

#### Expected Output

```
        0
0     nan
1 0.00000
```

#### Output of ``pd.show_versions()``

<details>
```
INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-42-generic
Version          : #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_HK.UTF-8
LOCALE           : en_HK.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : None
pytest           : 6.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : None
tabulate         : 0.8.3
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
```
</details>
"
678453086,35707,REGR: fix DataFrame.diff with read-only data,jorisvandenbossche,closed,2020-08-13T13:49:48Z,2020-08-14T14:42:58Z,Closes #35559
679200971,35721,Backport PR #35707 on branch 1.1.x (REGR: fix DataFrame.diff with read-only data),meeseeksmachine,closed,2020-08-14T14:37:31Z,2020-08-14T15:38:21Z,Backport PR #35707: REGR: fix DataFrame.diff with read-only data
679201252,35722,Backport PR #35664 on branch 1.1.x (BUG: Styler cell_ids fails on multiple renders),meeseeksmachine,closed,2020-08-14T14:37:59Z,2020-08-14T15:39:01Z,Backport PR #35664: BUG: Styler cell_ids fails on multiple renders
628394075,34511,BUG: DateOffset pickle bug when months=12,jjbarton,closed,2020-06-01T12:17:34Z,2020-08-14T16:17:38Z,"- [ x] I have checked that this issue has not already been reported.

- [ x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```
import pickle
import pandas as pd

offset = pd.DateOffset(months=12)

pickle.dump(offset, open(""/tmp/o.p"", ""wb""))
loaded = pickle.load(open(""/tmp/o.p"", ""rb""))
print(loaded)

# <DateOffset: months=12, years=1>

```

#### Problem description

If you pickle a pandas DateOffset instance with months=12, and then unpickle it, you have a DateOffset(months=12, years=1) instance (effectively a 2 year date offset object.

Arguably one shouldn't create a 12 month date offset in preference to a 1 year offset, but this is still bad behaviour imho.

#### Expected Output

print(loaded)
# <DateOffset: months=12>

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.4.0.post20200518
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
669618716,35490,BUG: DataFrame.agg with multiple cum functions creates wrong result,qinxuye,closed,2020-07-31T09:33:45Z,2020-08-14T20:59:10Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
In [1]: import pandas as pd                                                     

In [2]: df1 = pd.DataFrame({ 
   ...:     'a': [3, 4, 5, 3, 5, 4, 1, 2, 3], 
   ...:     'b': [1, 3, 4, 5, 6, 5, 4, 4, 4], 
   ...:     'c': list('aabaaddce'), 
   ...:     'd': [3, 4, 5, 3, 5, 4, 1, 2, 3], 
   ...:     'e': [1, 3, 4, 5, 6, 5, 4, 4, 4], 
   ...:     'f': list('aabaaddce'), 
   ...: })                                                                      

In [3]: df1.groupby('b').agg(['cummax', 'cumsum'])                              
Out[3]: 
       a             d             e       
  cummax cumsum cummax cumsum cummax cumsum
b                                          
1      4      4      4      4      3      3
3      3      3      3      3      5      5
4      5      5      5      5      6      6
5      4      7      4      7      5     10
6      5      6      5      6      4      8
```

Cumulative functions should generate the DataFrame with the same length.

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

In pandas 1.0.5, the result is

```python
In [1]: import pandas as pd                                                     

In [2]: df1 = pd.DataFrame({ 
   ...:     'a': [3, 4, 5, 3, 5, 4, 1, 2, 3], 
   ...:     'b': [1, 3, 4, 5, 6, 5, 4, 4, 4], 
   ...:     'c': list('aabaaddce'), 
   ...:     'd': [3, 4, 5, 3, 5, 4, 1, 2, 3], 
   ...:     'e': [1, 3, 4, 5, 6, 5, 4, 4, 4], 
   ...:     'f': list('aabaaddce'), 
   ...: })                                                                      

In [3]: df1.groupby('b').agg(['cummax', 'cumsum'])                              
Out[3]: 
       a             d             e       
  cummax cumsum cummax cumsum cummax cumsum
0      3      3      3      3      1      1
1      4      4      4      4      3      3
2      5      5      5      5      4      4
3      3      3      3      3      5      5
4      5      5      5      5      6      6
5      4      7      4      7      5     10
6      5      6      5      6      4      8
7      5      8      5      8      4     12
8      5     11      5     11      4     16
```

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

In [5]: pd.show_versions()                                                      
/Users/qinxuye/miniconda3/envs/test_pandas_1.1/lib/python3.7/site-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.
  ""Distutils was imported before Setuptools. This usage is discouraged ""
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-5-3d232a07e144> in <module>
----> 1 pd.show_versions()

~/miniconda3/envs/test_pandas_1.1/lib/python3.7/site-packages/pandas/util/_print_versions.py in show_versions(as_json)
    104     """"""
    105     sys_info = _get_sys_info()
--> 106     deps = _get_dependency_info()
    107 
    108     if as_json:

~/miniconda3/envs/test_pandas_1.1/lib/python3.7/site-packages/pandas/util/_print_versions.py in _get_dependency_info()
     82     for modname in deps:
     83         mod = import_optional_dependency(
---> 84             modname, raise_on_missing=False, on_version=""ignore""
     85         )
     86         result[modname] = _get_version(mod) if mod else None

~/miniconda3/envs/test_pandas_1.1/lib/python3.7/site-packages/pandas/compat/_optional.py in import_optional_dependency(name, extra, raise_on_missing, on_version)
     97     minimum_version = VERSIONS.get(name)
     98     if minimum_version:
---> 99         version = _get_version(module)
    100         if distutils.version.LooseVersion(version) < minimum_version:
    101             assert on_version in {""warn"", ""raise"", ""ignore""}

~/miniconda3/envs/test_pandas_1.1/lib/python3.7/site-packages/pandas/compat/_optional.py in _get_version(module)
     42 
     43     if version is None:
---> 44         raise ImportError(f""Can't determine version for {module.__name__}"")
     45     return version
     46 

ImportError: Can't determine version for numba
</details>
"
679267409,35726,CLN: remove unused variable,jbrockmendel,closed,2020-08-14T16:26:34Z,2020-08-14T21:38:46Z,Tiny step towards #34714.
679332203,35729,DOC: Fix broken link in cookbook.rst,estasney,closed,2020-08-14T18:29:27Z,2020-08-14T22:06:03Z,"The original link [Aggregation and plotting time series](http://nipunbatra.github.io/2015/06/timeseries/) found in the [Pandas Cookbook](https://pandas.pydata.org/pandas-docs/stable/user_guide/cookbook.html?highlight=get_group#timeseries) is broken.

This appears to have been moved to the authors [Blog](https://nipunbatra.github.io/blog/visualisation/2013/05/01/aggregation-timeseries.html)

While the date does not match ( 2013/05 vs 2015/06 ) the contents appear identical. I was able to determine this after viewing the archive at [WayBack Machine](https://web.archive.org/web/20161202094122/http://nipunbatra.github.io/2015/06/timeseries/)


- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry


"
679461098,35734,CI: See if socket check is responsible for CI failures,jbrockmendel,closed,2020-08-15T00:14:18Z,2020-08-15T01:28:39Z,Reverts part of #35693
678354568,35703,CI / Checks (pull_request) on Github Actions failing with isort 5.3.2 -> 5.4.0,simonjayhawkins,closed,2020-08-13T11:11:39Z,2020-08-15T03:42:53Z,"
```
2020-08-13T10:30:51.9470350Z isort --version-number
2020-08-13T10:30:52.2249999Z 5.4.0
2020-08-13T10:30:52.2443133Z Check import format using isort
2020-08-13T10:30:56.4886273Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/_testing.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4892079Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/_typing.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4892907Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/tests/frame/test_analytics.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4893331Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/tests/extension/base/__init__.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4893737Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/tseries/frequencies.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4894183Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/tseries/offsets.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4894928Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/tseries/holiday.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4895326Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/plotting/_matplotlib/timeseries.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4895747Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/core/index.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4896139Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/core/common.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4896552Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/core/ops/__init__.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4897157Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/core/window/__init__.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4897613Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/core/dtypes/dtypes.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4898032Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/core/dtypes/common.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4898422Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/core/reshape/melt.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4898832Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/core/reshape/merge.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4899228Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/core/internals/__init__.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4899642Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/core/tools/datetimes.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4900046Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/api/__init__.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4900461Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/util/__init__.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4900859Z ##[error]ERROR: /home/runner/work/pandas/pandas/pandas/io/pytables.py Imports are incorrectly sorted and/or formatted.
2020-08-13T10:30:56.4901145Z Check import format using isort DONE
```"
679198797,35720,update io documentation,usneil,closed,2020-08-14T14:34:07Z,2020-08-15T11:13:23Z,"The HDFS dropna=True parameter does not drop the NaN values. Perhaps this is a bug to the core Pandas, or really a typo in the documentation

- [X] closes #35719
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
679281025,35728,BLD: bump xlrd min version to 1.2.0,jbrockmendel,closed,2020-08-14T16:53:02Z,2020-08-15T16:02:08Z,"Warnings about time.clock are filling up the py37 min_version build.  xlrd 1.2.0 was released Dec 15, 2018, so I think we're safe to bump it."
676626541,35664,BUG: Styler cell_ids fails on multiple renders,attack68,closed,2020-08-11T06:44:45Z,2020-08-15T16:28:19Z,"- [x] closes #35663 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
673727494,35573,BUG: pd.read_sql_query with chunksize = 0 should be treated like chunksize = None and it should return a DataFrame,paolobellomo,open,2020-08-05T17:42:06Z,2020-08-15T16:39:51Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# the details of the query are immaterial
from sqlalchemy import create_engine
_ENGINE = create_engine(
    ""mssql+pymssql://{user}:{password}@{server}/{database}"".format(
        user=""my_username"",
        password=""my_password"",
        server=""serve.my_company.com"",
        database=""database_name"",
    )
)
query = ""SELECT SomeCol FROM SomeTable"" 

# chunksize=None, it returns a DataFrame. GOOD
test_df = pd.read_sql_query(small_query, _ENGINE, chunksize=None)
print()
print(""chunksize = None"")
print(type(test_df))
print(test_df.shape)

# chunksize=100, it returns a Generator. GOOD
test_df = pd.read_sql_query(small_query, _ENGINE, chunksize=100)
print()
print(""chunksize = 100"")
print(type(test_df))
test_df = pd.concat(test_df, axis=0)
print(test_df.shape)

# chunksize=0, it returns a Generator. BAD
# pd.concat fails because the generator fails to return anything
test_df = pd.read_sql_query(small_query, _ENGINE, chunksize=0)
print()
print(""chunksize = 0"")
print(type(test_df))
test_df = pd.concat(test_df, axis=0)
print(test_df.shape)
```

#### Problem description

read_sql_query:
chunksize can be either None or int. 
The case chunksize = 0 should be treated like chunksize = None and it should return a DataFrame.
At the moment it returns a generator which fails to return any object when pd.concat is used
Incidentally: the DocString of read_sql_query says it will return an Iterator. Not so.

#### Expected Output

#### Output

<details>

chunksize = None
<class 'pandas.core.frame.DataFrame'>
(1854, 8)

chunksize = 100
<class 'generator'>
(1854, 8)

chunksize = 0
<class 'generator'>
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-36-16c97e2c25e0> in <module>
     16 print(""chunksize = 0"")
     17 print(type(test_df))
---> 18 test_df = pd.concat(test_df, axis=0)
     19 print(test_df.shape)

/usr/local/venvs/algovenv9/lib/python3.6/site-packages/pandas/core/reshape/concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    226                        keys=keys, levels=levels, names=names,
    227                        verify_integrity=verify_integrity,
--> 228                        copy=copy, sort=sort)
    229     return op.get_result()
    230 

/usr/local/venvs/algovenv9/lib/python3.6/site-packages/pandas/core/reshape/concat.py in __init__(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)
    260 
    261         if len(objs) == 0:
--> 262             raise ValueError('No objects to concatenate')
    263 
    264         if keys is None:

ValueError: No objects to concatenate

</details>
"
679599544,35738,Backport PR #35723 on branch 1.1.x (agg with list of non-aggregating functions),meeseeksmachine,closed,2020-08-15T15:55:41Z,2020-08-15T16:59:37Z,Backport PR #35723: agg with list of non-aggregating functions
679659070,35749,BUG: AttributeError: 'Cell' object has no attribute 'font' when using to_excel() with openpyxl engine,mcs2017,closed,2020-08-15T23:35:42Z,2020-08-16T00:01:00Z,"- [x]  I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of pandas.
- [x] I have confirmed this bug exists on the latest version of openpyxl.
- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.


I am using to_excel() with openpyxl engine to export a pandas dataframe to a sheet of an .xlsx file. My code works just fine two days ago, but it does not work now with Error:
```
with pd.ExcelWriter(PATH+ '/' + FILENAME, engine='openpyxl', mode='a') as writer:
      df.head().to_excel(writer, sheet_name='test')
      writer.save()



---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-69-1e1ecfd20f72> in <module>
      1 with pd.ExcelWriter(PATH+ '/' + FILENAME, engine='openpyxl', mode='a') as writer:
----> 2     df.head().to_excel(writer, sheet_name='test')
      3     writer.save()
      4 
      5 writer.close()

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/generic.py in to_excel(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes)
   2027             startcol=startcol,
   2028             freeze_panes=freeze_panes,
-> 2029             engine=engine,
   2030         )
   2031 

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/formats/excel.py in write(self, writer, sheet_name, startrow, startcol, freeze_panes, engine)
    737             startrow=startrow,
    738             startcol=startcol,
--> 739             freeze_panes=freeze_panes,
    740         )
    741         if need_save:

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/excel/_openpyxl.py in write_cells(self, cells, sheet_name, startrow, startcol, freeze_panes)
    437             if style_kwargs:
    438                 for k, v in style_kwargs.items():
--> 439                     setattr(xcell, k, v)
    440 
    441             if cell.mergestart is not None and cell.mergeend is not None:

AttributeError: 'Cell' object has no attribute 'font'









```"
673162151,35558,BUG: Incompatible tolerance error in merge_asof with comaptible types.,jkibele,closed,2020-08-05T00:16:06Z,2020-08-16T10:08:05Z,"- [X ] I have checked that this issue has not already been reported.

- [X ] I have confirmed this bug exists on the latest version of pandas.
Crashes in 1.1.0, but works in 1.0.5

- [X ] (optional) I have confirmed this bug exists on the master branch of pandas.

---



#### Code Sample, a copy-pastable example

```python
import pandas as pd

dr1 = pd.date_range(start='1/1/2020', end='2/1/2020')
dr2 = pd.date_range(start='1/1/2020', end='1/20/2020', freq='2D') + pd.Timedelta(seconds=0.4)

df1 = pd.DataFrame({'val1': 'foo'}, index=pd.DatetimeIndex(dr1))
df2 = pd.DataFrame({'val2': 'bar'}, index=pd.DatetimeIndex(dr2))

td = pd.Timedelta(seconds=0.5)
pd.merge_asof(df2, df1, left_index=True, right_index=True, tolerance=td)

```

#### Problem description

When trying to use `merge_asof` in 1.1.0 (and in master), I get message about ""incompatible tolerance"" even though I'm using `pandas.DatetimeIndex` and `pandas.Timedelta` for the tolerance.

Well, technically, I actually get `UnbondLocalError` because a local variable is referenced before assignment while trying to generate the error message, but it looks like the underlying problem is something to do with an incompatible tolerance error.

Here's the traceback:

<details>

```python
---------------------------------------------------------------------------
UnboundLocalError                         Traceback (most recent call last)
<ipython-input-4-c18851904e91> in <module>
      1 td = pd.Timedelta(seconds=0.5)
----> 2 pd.merge_asof(df2, df1, left_index=True, right_index=True, tolerance=td)

~/miniconda3/envs/pandas_problem/lib/python3.8/site-packages/pandas/core/reshape/merge.py in merge_asof(left, right, on, left_on, right_on, left_index, right_index, by, left_by, right_by, suffixes, tolerance, allow_exact_matches, direction)
    544     4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN
    545     """"""
--> 546     op = _AsOfMerge(
    547         left,
    548         right,

~/miniconda3/envs/pandas_problem/lib/python3.8/site-packages/pandas/core/reshape/merge.py in __init__(self, left, right, on, left_on, right_on, left_index, right_index, by, left_by, right_by, axis, suffixes, copy, fill_method, how, tolerance, allow_exact_matches, direction)
   1571         self.direction = direction
   1572 
-> 1573         _OrderedMerge.__init__(
   1574             self,
   1575             left,

~/miniconda3/envs/pandas_problem/lib/python3.8/site-packages/pandas/core/reshape/merge.py in __init__(self, left, right, on, left_on, right_on, left_index, right_index, axis, suffixes, copy, fill_method, how)
   1465 
   1466         self.fill_method = fill_method
-> 1467         _MergeOperation.__init__(
   1468             self,
   1469             left,

~/miniconda3/envs/pandas_problem/lib/python3.8/site-packages/pandas/core/reshape/merge.py in __init__(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)
    650             self.right_join_keys,
    651             self.join_names,
--> 652         ) = self._get_merge_keys()
    653 
    654         # validate the merge keys dtypes. We may need to coerce

~/miniconda3/envs/pandas_problem/lib/python3.8/site-packages/pandas/core/reshape/merge.py in _get_merge_keys(self)
   1667 
   1668             msg = (
-> 1669                 f""incompatible tolerance {self.tolerance}, must be compat ""
   1670                 f""with type {repr(lk.dtype)}""
   1671             )

UnboundLocalError: local variable 'lk' referenced before assignment
```
</details>

#### Expected Output

Here's what I get in version 1.0.4, and what I think I should get.

```
                         val2 val1
2020-01-01 00:00:00.400  bar  foo
2020-01-03 00:00:00.400  bar  foo
2020-01-05 00:00:00.400  bar  foo
2020-01-07 00:00:00.400  bar  foo
2020-01-09 00:00:00.400  bar  foo
2020-01-11 00:00:00.400  bar  foo
2020-01-13 00:00:00.400  bar  foo
2020-01-15 00:00:00.400  bar  foo
2020-01-17 00:00:00.400  bar  foo
2020-01-19 00:00:00.400  bar  foo
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : a4203cf8d440f33d50076166fad3b0577a3ef8fa
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-7634-generic
Version          : #38~1595345317~20.04~a8480ad-Ubuntu SMP Wed Jul 22 15:13:45 UTC 
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+29.ga4203cf8d
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>

I hope I did this right. Just trying to help. Thanks."
679770852,35752,BUG: rolling count on string Series and TimeIndex raises an error,ezerkar,closed,2020-08-16T15:08:48Z,2020-08-16T15:44:38Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

#### Code Sample, a copy-pastable example

```python
d = {'vv': {pd.Timestamp('2020-01-13 08:22:00', freq='T'): 'aa',
  pd.Timestamp('2020-01-13 08:23:00', freq='T'): 'bb',
  pd.Timestamp('2020-01-13 08:24:00', freq='T'): 'cc',
  pd.Timestamp('2020-01-13 08:25:00', freq='T'): np.nan,
  pd.Timestamp('2020-01-13 08:26:00', freq='T'): 'dd'}}

df = pd.DataFrame(d)

print(df['vv'].rolling('72s').count())

```

#### Problem description

this raises DataError: No numeric types to aggregate insted if the expected output:

    2020-01-13 08:22:00    1.0
    2020-01-13 08:23:00    2.0
    2020-01-13 08:24:00    2.0
    2020-01-13 08:25:00    1.0
    2020-01-13 08:26:00    1.0
    Name: vv, dtype: float64

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.16299
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 49.2.1.post20200807
Cython           : 0.29.21
pytest           : 6.0.1
hypothesis       : None
sphinx           : 3.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.1

</details>
"
679599400,35737,CI: Travis failing with INTERNALERROR,jbrockmendel,closed,2020-08-15T15:54:42Z,2020-08-17T08:50:01Z,"I'm seeing this on every recent Travis 37-coverage build:

```
============================= test session starts ==============================
platform linux -- Python 3.7.7, pytest-6.0.1, py-1.9.0, pluggy-0.13.1
rootdir: /home/travis/build/pandas-dev/pandas, configfile: setup.cfg, testpaths: pandas
plugins: xdist-2.0.0, cov-2.10.0, forked-1.2.0, hypothesis-5.24.0
gw0 I
INTERNALERROR> Traceback (most recent call last):
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/_pytest/main.py"", line 238, in wrap_session
INTERNALERROR>     config.hook.pytest_sessionstart(session=session)
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pluggy/hooks.py"", line 286, in __call__
INTERNALERROR>     return self._hookexec(self, self.get_hookimpls(), kwargs)
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pluggy/manager.py"", line 93, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pluggy/manager.py"", line 87, in <lambda>
INTERNALERROR>     firstresult=hook.spec.opts.get(""firstresult"") if hook.spec else False,
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pluggy/callers.py"", line 208, in _multicall
INTERNALERROR>     return outcome.get_result()
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pluggy/callers.py"", line 80, in get_result
INTERNALERROR>     raise ex[1].with_traceback(ex[2])
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pluggy/callers.py"", line 187, in _multicall
INTERNALERROR>     res = hook_impl.function(*args)
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/xdist/dsession.py"", line 78, in pytest_sessionstart
INTERNALERROR>     nodes = self.nodemanager.setup_nodes(putevent=self.queue.put)
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/xdist/workermanage.py"", line 65, in setup_nodes
INTERNALERROR>     return [self.setup_node(spec, putevent) for spec in self.specs]
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/xdist/workermanage.py"", line 65, in <listcomp>
INTERNALERROR>     return [self.setup_node(spec, putevent) for spec in self.specs]
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/xdist/workermanage.py"", line 73, in setup_node
INTERNALERROR>     node.setup()
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/xdist/workermanage.py"", line 260, in setup
INTERNALERROR>     self.config.hook.pytest_configure_node(node=self)
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pluggy/hooks.py"", line 286, in __call__
INTERNALERROR>     return self._hookexec(self, self.get_hookimpls(), kwargs)
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pluggy/manager.py"", line 93, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pluggy/manager.py"", line 87, in <lambda>
INTERNALERROR>     firstresult=hook.spec.opts.get(""firstresult"") if hook.spec else False,
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pluggy/callers.py"", line 208, in _multicall
INTERNALERROR>     return outcome.get_result()
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pluggy/callers.py"", line 80, in get_result
INTERNALERROR>     raise ex[1].with_traceback(ex[2])
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pluggy/callers.py"", line 187, in _multicall
INTERNALERROR>     res = hook_impl.function(*args)
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pytest_cov/plugin.py"", line 239, in pytest_configure_node
INTERNALERROR>     self.cov_controller.configure_node(node)
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pytest_cov/engine.py"", line 274, in configure_node
INTERNALERROR>     workerinput(node).update({
INTERNALERROR>   File ""/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pytest_cov/compat.py"", line 42, in fn
INTERNALERROR>     return getattr(obj, attr, *args)
INTERNALERROR> AttributeError: 'WorkerController' object has no attribute 'slaveinput'
The command ""ci/run_tests.sh"" exited with 3.

```"
679810450,35754,CI: Min Pytest Cov Version/Restrict xdist version,alimcmaster1,closed,2020-08-16T19:21:29Z,2020-08-17T08:51:47Z,- [x] closes #35737
680062759,35761,Backport PR #35754 on branch 1.1.x (CI: Min Pytest Cov Version/Restrict xdist version),meeseeksmachine,closed,2020-08-17T08:52:00Z,2020-08-17T09:39:03Z,Backport PR #35754: CI: Min Pytest Cov Version/Restrict xdist version
677260009,35680,BUG: Different results from Series.replace with compiled regex,pLeBlanc93,closed,2020-08-11T23:18:28Z,2020-08-17T11:23:54Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import re
import pandas as pd

print(pd.__version__)

s = pd.Series(['a', 'aa', 'b'])
regex = {'^a$': 1, '^b$': 2, '^aa$': 3}
print(s.replace(regex, regex=True)) 
print(s.replace({re.compile(k): v for k,v in regex.items()}, regex=True))


```

#### Problem description
These two `.replace` calls are equivalent on pandas `1.0.5`. They are different on `1.1.0` and `master`.

```
1.2.0.dev0+79.g3c87b019b
0    1
1    3
2    2
dtype: int64
0     a
1    aa
2     b
dtype: object


1.0.5
0    1
1    3
2    2
dtype: int64
0    1
1    3
2    2
dtype: int64
```

#### Expected Output


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 63 Stepping 2, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.5
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 49.3.1.post20200810
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
680145801,35765,Backport PR #35697 on branch 1.1.x (REGR: Don't ignore compiled patterns in replace),meeseeksmachine,closed,2020-08-17T11:00:57Z,2020-08-17T12:21:40Z,Backport PR #35697: REGR: Don't ignore compiled patterns in replace
678097562,35697,REGR: Don't ignore compiled patterns in replace,dsaxton,closed,2020-08-13T02:19:55Z,2020-08-17T13:03:21Z,"- [x] closes #35680
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
679434758,35732,BLD: minimum versions need update in `pandas/compat/_optional.py`,fangchenli,closed,2020-08-14T22:34:13Z,2020-08-17T13:11:54Z,The minimum versions in `pandas/compat/_optional.py` were not updated in #35214.
670293102,35499,"BUG: pandas 1.1.0 MemoryError using .astype(""string"") which worked using pandas 1.0.5",ldacey,closed,2020-07-31T22:24:41Z,2020-08-17T14:35:00Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

I tried to pinpoint the specific row which causes the error. The column has HTML data like this:

`
'''<div class=""comment"" dir=""auto""><p dir=""auto"">Request <a href=""/agent/tickets/test"" target=""_blank"" rel=""ticket"">#test</a> ""RE: **  M ..."" Last comment in request </a>:</p> <p dir=""auto""></p> <p dir=""auto"">Thank you</p> <p dir=""auto"">Stuff.</p> <p dir=""auto"">We will keep you posted .</p> <p dir=""auto"">Regards,</p> <p dir=""auto"">Name</p></div>'''`
 

```python

#This fails (memory error below):
df['event_html_body'].astype(""string"")

#Filtering the dataframe to only convert rows which are not null for this field **works**
x = df[~df.event_html_body.isnull()][['event_html_body']]
x['event_html_body'].astype(""string"")


#Filling NAs with another value fails:
df['event_html_body'].fillna('-').astype(""string"")
```

#### Problem description
I have code which has been converting columns to the ""string"" dtype and this has worked up until pandas 1.1.0

For example, I tried to process a file which I successfully processed in April and it works when I use .astype(str), but it fails when I use .astype(""string"") event though this worked in pandas 1.0.5.

The column does not **need** to be the new ""string"" type, but I wanted to raise this issue anyways.

Rows: 201368
Empty/null rows for the column in question: 189014 / 201368

So this column is quite sparse, and as I mentioned below if I filtered the nulls and then do .astype(""string"") then it will run fine. I am not sure why this worked before (same server, 64 GB of RAM), and this file was previous processed as a ""string"" before the update.

Error: 
```

MemoryError                               Traceback (most recent call last)
<ipython-input-38-939f88862e64> in <module>
----> 1 df['event_html_body'].astype(""string"")

/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py in astype(self, dtype, copy, errors)
   5535         else:
   5536             # else, only a single dtype is given
-> 5537             new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors,)
   5538             return self._constructor(new_data).__finalize__(self, method=""astype"")
   5539 

/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py in astype(self, dtype, copy, errors)
    565         self, dtype, copy: bool = False, errors: str = ""raise""
    566     ) -> ""BlockManager"":
--> 567         return self.apply(""astype"", dtype=dtype, copy=copy, errors=errors)
    568 
    569     def convert(

/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, align_keys, **kwargs)
    394                 applied = b.apply(f, **kwargs)
    395             else:
--> 396                 applied = getattr(b, f)(**kwargs)
    397             result_blocks = _extend_blocks(applied, result_blocks)
    398 

/opt/conda/lib/python3.7/site-packages/pandas/core/internals/blocks.py in astype(self, dtype, copy, errors)
    588             vals1d = values.ravel()
    589             try:
--> 590                 values = astype_nansafe(vals1d, dtype, copy=True)
    591             except (ValueError, TypeError):
    592                 # e.g. astype_nansafe can fail on object-dtype of strings

/opt/conda/lib/python3.7/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna)
    911     # dispatch on extension dtype if needed
    912     if is_extension_array_dtype(dtype):
--> 913         return dtype.construct_array_type()._from_sequence(arr, dtype=dtype, copy=copy)
    914 
    915     if not isinstance(dtype, np.dtype):

/opt/conda/lib/python3.7/site-packages/pandas/core/arrays/string_.py in _from_sequence(cls, scalars, dtype, copy)
    215 
    216         # convert to str, then to object to avoid dtype like '<U3', then insert na_value
--> 217         result = np.asarray(result, dtype=str)
    218         result = np.asarray(result, dtype=""object"")
    219         if has_nans:

/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py in asarray(a, dtype, order)
     83 
     84     """"""
---> 85     return array(a, dtype, copy=False, order=order)
     86 
     87 

MemoryError: Unable to allocate array with shape (201368,) and data type <U131880
```

#### Expected Output

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-1028-azure
Version          : #29~18.04.1-Ubuntu SMP Fri Jun 5 14:32:34 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.17.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : 0.29.14
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fsspec           : 0.6.2
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 1.0.0
pytables         : None
pyxlsb           : 1.0.6
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : None
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.48.0


</details>

"
679834915,35755,BUG: custom window not working in groupby,MaxHalford,closed,2020-08-16T22:03:08Z,2020-08-17T14:51:27Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
readings = pd.DataFrame(
    [
        ('A', 'Saturday', 101),
        ('A', 'Sunday', 88),
        ('A', 'Saturday', 103),
        ('A', 'Sunday', 82),
        ('A', 'Saturday', 100),
        ('B', 'Saturday', 27),
        ('B', 'Sunday', 13),
        ('B', 'Saturday', 21),
        ('B', 'Sunday', 17),
        ('B', 'Saturday', 25)
    ],
    columns=['building', 'day', 'reading']
)

class ShiftedWindow(pd.api.indexers.BaseIndexer):

    def __init__(self, window_size):
        self.window_size = window_size

    def get_window_bounds(self, num_values=0, min_periods=None, center=None, closed=None):

        starts = np.arange(-self.window_size, num_values - self.window_size)
        ends = starts + self.window_size
        starts[:self.window_size] = 0
        
        return starts, ends

readings.groupby('building')['reading'].rolling(window=ShiftedWindow(2), min_periods=1).mean()

```

#### Problem description

I've defined a custom window that uses the previous values, and therefore ignores the current value. It's very useful for, say, target encoding on time series.

#### Expected Output

I would be expecting the following output:

```py
>>> readings.groupby('building')['reading'].apply(lambda x: x.shift(1).rolling(2, min_periods=1).mean())
0      NaN
1    101.0
2     94.5
3     95.5
4     92.5
5      NaN
6     27.0
7     20.0
8     17.0
9     19.0
Name: reading, dtype: float64
```

Instead, I'm getting:

```py
>>> readings.groupby('building')['reading'].rolling(window=ShiftedWindow(2), min_periods=1).mean()
building   
A         0    101.0
          1     88.0
          2    103.0
          3     82.0
          4    100.0
B         5     27.0
          6     13.0
          7     21.0
          8     17.0
          9     25.0
Name: reading, dtype: float64
```

I've checked and my custom window works as expected without using `groupby`. I've checked to see if `get_window_bounds` gets called when a `groupby` is used, and the answer is no. Basically, it seems that my custom window is being ignored entirely.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.5.0
Version          : Darwin Kernel Version 19.5.0: Tue May 26 20:41:44 PDT 2020; root:xnu-6153.121.2~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.2.1
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.1
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fsspec           : 0.5.2
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.2
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.9
tables           : 3.5.2
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.45.1

</details>
"
679650219,35745,BUG: slicing DataFrameGroupBy to SeriesGroupBy doesn't propagate dropna,arw2019,closed,2020-08-15T22:16:52Z,2020-08-17T14:52:57Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

xref  #9959, #35444

Slicing a `DataFrameGroupBy` object to `SeriesGroupBy` doesn't correctly propagate `dropna`. 

The `DataFrameGroupBy` &#8594; `DataFrameGroupBy` case is being handled in #35444. Opening this because as far as I can tell the `DataFrameGroupBy` &#8594; `SeriesGroupBy` variant is an independent bug.

#### Code Sample, a copy-pastable example
```
In [11]: df = pd.DataFrame({""a"": [1], ""b"": [2], ""c"": [3]}) 
    ...: gb = df.groupby('a', dropna=False)                                                                                                                                                                       

In [12]: gb.dropna                                                                                                                                                                                                
Out[12]: False

In [13]: gb['b'].dropna                                                                                                                                                                                   
Out[13]: True
```

#### Expected Output
I'd like to see:
```
In [22]: gb.dropna == gb['b'].dropna                                                                                                                                                                              
Out[22]: True
```

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : a3f5c6a5a3f05edc1c56ab8051f28c4cf322c45c
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-42-generic
Version          : #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+99.ga3f5c6a5a.dirty
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.1.0.post20200704
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : 5.19.0
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : 0.4.0
gcsfs            : 0.6.2
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.1
</details>
"
677363347,35682,QST: Such a behavior of DataFrameGroupBy.cumcount() is intended ??,itholic,closed,2020-08-12T03:58:20Z,2020-08-17T14:54:06Z,"#### Question about pandas

Let's say we have a `DataFrame` named by `pdf` like the below.

```python
>>> pdf = pd.DataFrame([['a'], ['a'], ['a'], ['b'], ['b'], ['a']], columns=list('A'))
>>> pdf
   A
0  a
1  a
2  a
3  b
4  b
5  a
```

For above `pdf`, `GroupBy.cum~` functions seems not work, but raise `DataError`

```python
# GroupBy.cummax
>>> pdf.groupby(""A"").cummax()
Traceback (most recent call last):
...
pandas.core.base.DataError: No numeric types to aggregate

# GroupBy.cummin
>>> pdf.groupby(""A"").cummin()
Traceback (most recent call last):
...
pandas.core.base.DataError: No numeric types to aggregate

# GroupBy.cumprod
>>> pdf.groupby(""A"").cumprod() 
Traceback (most recent call last):
...
pandas.core.base.DataError: No numeric types to aggregate

# GroupBy.cumsum
>>> pdf.groupby(""A"").cumsum()
Traceback (most recent call last):
...
pandas.core.base.DataError: No numeric types to aggregate
```

And I thought that `GroupBy.cumcount()` also wouldn't work, but It's actually working like the below.

```python
>>> pdf.groupby(""A"").cumcount()
0    0
1    1
2    2
3    0
4    1
5    3
dtype: int64
```

Would someone let me know if Is it intended or unexpected behavior ?

Thanks."
678966849,35714,BUG: Negative RangeIndex can have positive step,topper-123,closed,2020-08-14T07:39:16Z,2020-08-17T14:54:30Z,"Python allows positve steps in ranges, e.g.

```python
>>> rng = range(2, -2)
>>> rng.step
1  # positive step in a negative range
list(rng)
[]  # empty
```

which surprises me and surely must be a bug in Python. This affects pandas' RangeIndex:

```python
>>> idx = pd.RangeIndex(2, -2)
>>> df = pd.DataFrame(idx, index=idx)
>>> df.shape
(0, 1)  # 0 length
>>> df.index
RangeIndex(start=2, stop=-2, step=1)  # positive step!
>>> df  # diplays as empty, even though isn't empty
Empty DataFrame
Columns: [0]
Index: []
>>> df.sum()  # result in master, wrong
0    0
dtype: int64
>>> df.sum()  # Correct result, not in master
0    2
dtype: int64
```

We see that the dataframe displays as empty, even though it is not actually empty and aggregations give wrong results.

Do people agree that in cases of negative ranges, we should supply a default step of -1?"
680286951,35770,"Backport PR #35519 on branch 1.1.x (REF: StringArray._from_sequence, use less memory)",meeseeksmachine,closed,2020-08-17T14:38:44Z,2020-08-17T15:32:36Z,"Backport PR #35519: REF: StringArray._from_sequence, use less memory"
679437691,35733,BLD: update min versions #35732,fangchenli,closed,2020-08-14T22:45:03Z,2020-08-17T15:39:20Z,"- [x] closes #35732
"
678101293,35698,CLN: replace PyObject_Str with str #34213,fangchenli,closed,2020-08-13T02:31:02Z,2020-08-17T15:40:06Z,"Part of #34213
"
679047117,35715,"Attribute ""dtype"" are different error with testing.assert_frame_equal",tejasj654,closed,2020-08-14T09:59:34Z,2020-08-17T18:20:20Z,"Hi,
When trying testing.assert_frame_equal, I get an assert error.
E           Attribute ""dtype"" are different
E           [left]:  Int64
E           [right]: Int16

However, for assert_frame_equal, I do set the argument `check_dtype=False`.

From traceback, I can see that `assert_extension_array_equal` is being called with `check_dtype=True` which is causing the issue.

I do not have a sample at hand and I will update when I can.

Affected version: 1.1.0
The test cases were / are running fine with 1.0.5"
680099857,35763,MAINT: Initialize year to silence warning,bashtage,closed,2020-08-17T09:47:13Z,2020-08-17T18:27:49Z,"Initialize year to silence warning due to subtracting from
value that compiler cannot reason must be either initialized
or never reached

closes #35622

- [X] closes #35622
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
679684712,35750,Pass check_dtype to assert_extension_array_equal,dsaxton,closed,2020-08-16T04:14:10Z,2020-08-17T18:31:25Z,"- [x] closes #35715
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
90588756,10424,csv_read() fails on properly decoding  latin-1(i.e. non utf8) encoded file from URL,BotoKopo,closed,2015-06-24T07:05:36Z,2020-08-17T18:59:43Z,"### Problem

Here is a problem that we had with a colleague, working on data available on a ftp (or http) server (internal network, we're sorry we can't have a proper example file to point to).

Reading a csv file (with csv_read) encoded with non utf8 (like latin-1), with special character in header, fails to properly unicode the header when file is accessed through an URL (http or ftp), but **not** when file is local, nor when it's utf-8 (local or distant) file.
The result looks like the file was decoded twice.

An example shoud be clearer.

Let's say we have 2 CSV files (on a distant server), _data.latin1.csv_ and _data.utf8.csv_, encoded in latin-1 and utf-8, and both containing :

```
a,b°
1.1,2.2
```

Then following code :

``` python
import sys
import os.path as op
import pandas as pd

path = ""ftp://sorry/I/cant/supply/such/a/path/for/the/example/data.encoding.csv""

for enc in ('latin1', 'utf8') :
    f = path.replace('encoding', enc)
    data = pd.read_csv(f, encoding=enc)
    print(""encoding {0} : non-ascii={1} , length={2}"".format(enc, data.columns[1].encode('utf8'), len(data.columns[1])))
```

will give :

```
encoding latin1 : non-ascii=bÂ° , length=3
encoding utf8 : non-ascii=b° , length=2
```

This was tested with _Python 2.7.6 +  Pandas 0.13.1_ and _Python 3.4.0 + Pandas 0.15.2_ with same result.

Same action on local files will give appropriate result, i.e. like previous 'utf8' encoding output (this REALLY IS  a matter of URL+latin1 or anything but utf-8). It looks like data was decoded twice, as we can see in output length as latin1 escape code for '°' is considered as a ""normal"" character being converted to utf-8.

This test will raise an error (""UnicodeEncodeError: 'ascii' codec can't encode character u'\xb0' in position 3: ordinal not in range(128)"") when python engine is used for read_csv() .
### in pandas code

Now, having a look at Pandas' code, I would focus on 2 points in pandas.io.parsers : 
- when file is an url, data is opened through urllib (or urllib2), then read, decoded (according to requested encoding) and result is fed into a StringIO stream  (Cf. _pandas.io.common.maybe_read_encoded_stream()_ ) , 
- as far as I could trace it, file seems to be decoded later, especially for 'c'-engine in _pandas.io.parsers.CParserWrapper.read()_ method (in fact by __parser.read()_ at the end, which is C-parser)

This would explain the twice decoding scheme when file is url, and normal decoding when file is local.

Furthermore, in  pandas.io.common, when replacing (in _maybe_read_encoded_stream()_ function) :

``` python
from pandas.compat import StringIO
...
reader = StringIO(reader.read().decode(encoding, errors))
```

by :

``` python
from pandas.compat import StringIO, BytesIO
...
reader = BytesIO(reader.read())
```

this problem seems to be solved (which is logical when we look at which StringIO/ByteIO functions are pointing to (depending on Python version) and which data they're handling).

So it seems to me that the problem is located at that point, and it would then be a bug.
However, it could be a feature ;-) as I don't know whether there could be side-effects for other cases than the one discussed here, especially if StringIO was intentionally used for a purpose I can't figure out.
"
674861801,35598,BUG: interpolate gives a TypeError on empty dataframes,sanderland,closed,2020-08-07T08:42:21Z,2020-08-17T19:32:54Z,"Already fixed in #35543, making an issue for linking/discussion.

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
from pandas import DataFrame
DataFrame().interpolate()
```
    ""Cannot interpolate with all object-dtype columns ""
TypeError: Cannot interpolate with all object-dtype columns in the DataFrame. Try setting at least one column to a numeric dtype.

#### Problem description
This is a regression, 1.0.5 handles it fine.

The problem appeared due to checking `np.all(obj.dtypes == np.dtype(object))` since the all of an empty set is True

#### Expected Output

#### Output of ``pd.show_versions()``

pandas           : 1.1.0
"
679656238,35748,BUG: close file handles in mmap,jbrockmendel,closed,2020-08-15T23:04:48Z,2020-08-17T19:44:57Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Broken off from #35711"
678082836,35696,REF: implement reset_dropped_locs,jbrockmendel,closed,2020-08-13T01:32:19Z,2020-08-17T19:45:25Z,"We get to get rid of the comment `# really should be done in internals :<`

Making the window.rolling usage use this helper method is the next step."
679634685,35742,TST: encoding for URLs in read_csv,twoertwein,closed,2020-08-15T20:08:03Z,2020-08-17T19:53:42Z,"- [x] closes #10424
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Reading CSVs from URLs with a non UTF-8 encoding should already work."
680443310,35773,Backport PR #35750 on branch 1.1.x (Pass check_dtype to assert_extension_array_equal),meeseeksmachine,closed,2020-08-17T18:31:38Z,2020-08-17T20:37:01Z,Backport PR #35750: Pass check_dtype to assert_extension_array_equal
680123853,35764,Backport PR #35543 on branch 1.1.x (REGR: Fix interpolation on empty dataframe),meeseeksmachine,closed,2020-08-17T10:24:15Z,2020-08-17T20:37:25Z,Backport PR #35543: REGR: Fix interpolation on empty dataframe
676521002,35660,BUG/CI: unreliable tests in `io\parser\test_common.py`,fangchenli,closed,2020-08-11T01:54:53Z,2020-08-17T20:50:14Z,"Those two tests
```bash
pandas\tests\io\parser\test_common.py:test_chunks_have_consistent_numerical_type
pandas\tests\io\parser\test_common.py:test_warn_if_chunks_have_mismatched_type
```
have been producing ResourceWarning occasionally on Windows-py37-np16 and travis-37. I think they might happen on other builds as well. I just haven't seen them yet.

```bash
all_parsers = <pandas.tests.io.parser.conftest.PythonParser object at 0x0000016ADEAFB808>

    def test_chunks_have_consistent_numerical_type(all_parsers):
        parser = all_parsers
        integers = [str(i) for i in range(499999)]
        data = ""a\n"" + ""\n"".join(integers + [""1.0"", ""2.0""] + integers)
    
        # Coercions should work without warnings.
        with tm.assert_produces_warning(None):
>           result = parser.read_csv(StringIO(data))

pandas\tests\io\parser\test_common.py:1148: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <contextlib._GeneratorContextManager object at 0x0000016AA37A05C8>
type = None, value = None, traceback = None

    def __exit__(self, type, value, traceback):
        if type is None:
            try:
>               next(self.gen)
E               AssertionError: Caused unexpected warning(s): [('ResourceWarning', ResourceWarning(""unclosed <socket.socket fd=2972, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('10.1.0.4', 1650), raddr=('169.254.169.254', 80)>""), 'D:\\a\\1\\s\\pandas\\io\\parsers.py', 3007)]

C:\Miniconda\envs\pandas-dev\lib\contextlib.py:119: AssertionError
```

```bash
all_parsers = <pandas.tests.io.parser.conftest.PythonParser object at 0x000001F32E4A2748>

    def test_warn_if_chunks_have_mismatched_type(all_parsers):
        warning_type = None
        parser = all_parsers
        integers = [str(i) for i in range(499999)]
        data = ""a\n"" + ""\n"".join(integers + [""a"", ""b""] + integers)
    
        # see gh-3866: if chunks are different types and can't
        # be coerced using numerical types, then issue warning.
        if parser.engine == ""c"" and parser.low_memory:
            warning_type = DtypeWarning
    
        with tm.assert_produces_warning(warning_type):
>           df = parser.read_csv(StringIO(data))

pandas\tests\io\parser\test_common.py:1167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <contextlib._GeneratorContextManager object at 0x000001F3076E7948>
type = None, value = None, traceback = None

    def __exit__(self, type, value, traceback):
        if type is None:
            try:
>               next(self.gen)
E               AssertionError: Caused unexpected warning(s): [('ResourceWarning', ResourceWarning(""unclosed <socket.socket fd=6620, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('10.1.0.4', 1656), raddr=('169.254.169.254', 80)>""), 'D:\\a\\1\\s\\pandas\\io\\parsers.py', 3007)]

C:\Miniconda\envs\pandas-dev\lib\contextlib.py:119: AssertionError
```

Those two tests don't fail everytime. And they are skipped for now.
"
520491825,29514,Flakey Test in CI - Unclosed File Handle,alimcmaster1,closed,2019-11-09T18:03:49Z,2020-08-17T20:50:14Z,"Seen this in a few pipelines today:

https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=20521
https://github.com/pandas-dev/pandas/runs/295247988

```
______________ test_chunks_have_consistent_numerical_type[python] ______________
[gw0] linux -- Python 3.6.1 /home/vsts/miniconda3/envs/pandas-dev/bin/python

all_parsers = <pandas.tests.io.parser.conftest.PythonParser object at 0x7f635d65e470>

    def test_chunks_have_consistent_numerical_type(all_parsers):
        parser = all_parsers
        integers = [str(i) for i in range(499999)]
        data = ""a\n"" + ""\n"".join(integers + [""1.0"", ""2.0""] + integers)
    
        # Coercions should work without warnings.
        with tm.assert_produces_warning(None):
>           result = parser.read_csv(StringIO(data))

pandas/tests/io/parser/test_common.py:1204: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7f631f91c8d0>
type = None, value = None, traceback = None

    def __exit__(self, type, value, traceback):
        if type is None:
            try:
>               next(self.gen)
E               AssertionError: Caused unexpected warning(s): [('ResourceWarning', ResourceWarning(""unclosed file <_io.BufferedReader name='test1.xlsx'>"",), 
```"
617554718,34157,QST: Select some data based on a condition,BlueskyFR,closed,2020-05-13T15:41:26Z,2020-08-21T17:05:38Z,"- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [x] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

```python
def condition(index):
    # Complex operations here, involving several external arrays queries
    # And multiple for loops
    return [ True / False ] # Returns a boolean
```

Hi!
I have a condition (like above) and a DataFrame.
How may I filter my DataFrame to only keep the rows for which the index returns True when passed to the above `condition()`?
I cannot find a way, using pandas, to pass a function to `.where()` or to `.filter()`.

Please do not hesitate if you have questions.

Thanks in advance!"
617385971,34155,BUG: str.split fails on a multiple character match,pblankley,closed,2020-05-13T11:55:33Z,2020-08-21T17:05:38Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
test_df = pd.DataFrame({
    'fails': ['let me||test', 'the||splitting']
})

test_df['fails'].str.split('||')

```

#### Problem description

I expect pandas to retain the same behavior as `str.split` and return the split column.

However, it doesn't handle multiple characters, and throws this instead (I didn't include the whole traceback to keep the issue clean).

```python
ValueError: split() requires a non-empty pattern match.
```

#### Expected Output

```python
0      [let me, test]
1    [the, splitting]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 9f69ccadffbc67de0c946e4fd9f22d41d30392f3
python           : 3.6.8.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.4.0
Version          : Darwin Kernel Version 19.4.0: Wed Mar  4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0.dev0+1563.g9f69ccadf
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 18.1
setuptools       : 40.6.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
617308612,34153,ENH: Allow for `loc` to specify numeric and/or name slicing,lazarillo,closed,2020-05-13T09:50:56Z,2020-08-21T17:05:38Z,"#### Is your feature request related to a problem?

Yes. The problem is that many use cases require specifying a numeric row range (say, the 10th - 20th rows) and a name-based column range (columns ""timestamp"" and ""value"").

As of now, the most consistent way to solve it is:

```python
df.loc[df.index[start_val:(end_val + 1)],cols_of_interest]
```

This is not ideal because

1. It is long and ugly to need to use `df.index` at all.
1. Notice the `(end_val + 1)`:  While most of Python uses exclusive ranges (eg, `range(1,3)` does not include `3`), `pandas`' `loc` is inclusive.  So, developers get used to `df.loc[1:3,'a']` giving all three rows.

#### Describe the solution you'd like

[this should provide a description of the feature request, e.g. ""`DataFrame.foo` should get a new parameter `bar` that [...]"", try to write a docstring for the desired feature]

`DataFrame.loc` should have a new argument `num_idxs` which is a boolean list of 2 values, specifying whether the row or column indices should be numeric or names.  Eg, `[True, False]` means that the rows should be treated as numeric and the columns as names.

This argument should have a default value of `None`, since default lists are dangerous in Python, but that `None` should resolve to `[True, False]`, since this is the most common situation that most of us want (I think).

The behavior of numeric should be exactly identical to the current implementation of `iloc`.  The behavior of names should be identical to `loc`, currently.


#### API breaking implications

[this should provide a description of how this feature will affect the API]

The only API breaking implication I am aware of is that using the default I suggested (rows being treated as numeric) will create ""off-by-one"" issues, since `iloc` is exclusive and `loc` is inclusive.

This could potentially be handled by first making the default resolve to `[False, False]` upon initial introduction, but it should eventually be deprecated to `[True, False]` as I say above, since this is the most common case.


#### Describe alternatives you've considered

[this should provide a description of any alternative solutions or features you've considered]

As mentioned above, I sprinkle my code with the ugly:

```python
df.loc[df.index[start_val:(end_val + 1)],cols_of_interest]
```

The reason this is necessary is because (a) row *names* rarely mean anything to me in my work, and (b) I try to make code as generalizable as possible.

#### Additional context

I feel the description above covers it, but I am happy to add some examples if there is confusion."
617853545,34170,BUG: pandas.concat() got an unexpected keyword argument 'join_axes',LynnXtreme,closed,2020-05-14T01:16:49Z,2020-08-21T17:05:39Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here

```

#### Problem description

pandas.concat() got an unexpected keyword argument 'join_axes'

#### Expected Output

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 17.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : zh_CN.UTF-8

pandas           : 1.0.3
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None


</details>
"
617784521,34165,BUG: Inconsistent behaviour for assert_index_equal on empty Indices,matthewgilbert,closed,2020-05-13T22:04:31Z,2020-08-21T17:05:39Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas

pandas.testing.assert_index_equal(
    pandas.RangeIndex(0, 0),
    pandas.RangeIndex(5, 5)
)

pandas.testing.assert_index_equal(
    pandas.RangeIndex(0, 0),
    pandas.Index([])
)

pandas.testing.assert_index_equal(
    pandas.RangeIndex(0, 1),
    pandas.Index([0])
)

```

#### Problem description

The first and third assertions pass, while the second fails. This seems like inconsistent behaviour, particularly with respect to the 2nd and 3rd cases.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-51-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.2.0.post20200511
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
621863413,34276,"QST: How ""safe"" is it to pickle a DataFrame with v1.x for long(er) term storage? ",jolespin,closed,2020-05-20T15:38:11Z,2020-08-21T17:05:40Z,"
#### Question about pandas

How ""safe"" is pickling `DataFrame` objects with v1.x for long term?  Pickling in past versions has been a sort of gamble, as expected, but I'm wondering how the long term storability is for `pandas` objects (in particular, `DataFrame`) now that the API has solidified in v1.x. 
"
623829962,34350,retrieve unwanted things later,ramimohammad,closed,2020-05-24T09:40:51Z,2020-08-21T17:05:41Z,"If I'm trying to make some data processing and I want to hide unwanted symbols or html codes during processing stage and I want to popup those unwanted things again on their origin place within the text after finishing.

My question is : is that doable ? if yes .. how to do that ?

Thanks
"
623682576,34341,BUG: Failing clipboard tests,matteosantama,closed,2020-05-23T15:25:42Z,2020-08-21T17:05:41Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
# Your code here
>>> pd.__version__
'1.1.0.dev0+1655.g1132aba3e'

pytest pandas/tests/io/test_clipboard.py
```

#### Problem description

Final three tests fail. Looks like the test is designed for Windows, I'm running a Mac. The test references [PR#25040](https://github.com/pandas-dev/pandas/pull/25040)

**EDIT:**  Just realized I'm actually running it in a Docker container. From Pyperclip documentation, looks like I need either gtk or PyQt4 (for Linux) or pbcopy and pbpaste (for Mac) installed. I suppose that should be included as a dependency in the Dockerfile?"
623591559,34332,ENH: A keep_fields type argument for merge operation,MuditJ,closed,2020-05-23T06:12:49Z,2020-08-21T17:05:41Z,"#### Is your feature request related to a problem?

It would increase user convenience if there were an argument where the user could specify the fields from the two data frames to be be merged which should be kept(or alternatively, a discard_fields column to specify which ones should be discarded)

This would make the merge operation more analogous to its SQL counterpart

#### Describe the solution you'd like

For two data frames A and B where A has fields 'c' and 'd' and B has fields 'e' and 'f' respectively,
it would simply result in the merge API looking like:

`pd.merge(A,B,left_on = 'c', right_on = 'e', keep_fields = ['c','d','e'])
`
"
625590272,34404,BUG: Memory leak when calling an apply with datetime.fromtimestamp,frndrs,closed,2020-05-27T10:52:35Z,2020-08-21T17:05:42Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np
import psutil
import os
from datetime import datetime

    
def process_samples(df):
    df['time'] = df['time'].apply(lambda x: datetime.fromtimestamp(x//10**3))
    
    df = df.rename(columns={'time':f'time_timestamp' })

df = pd.DataFrame({'time':np.random.randint(0,1000, size=30)})

process = psutil.Process(os.getpid())
beg = process.memory_info().rss/1024/1024

for i in range(1000000):
    
    process_samples(df.copy())
    
    end = process.memory_info().rss/1024/1024
    if i % 500 ==0:
        print(f'Memory after {i} iterations ', end, ', difference w.r.t. beggining',end-beg)

```

#### Problem description

We have identified a memory leak in our code when executing a combination of an apply and a rename. If we do any of these two steps without the other, there is no leak.

The apply simply uses datetime.fromtimestamp to convert POSIX timestamp to a date. We understand that we achieve similar results with the apply using time pd.to_datetime(), but still believe there could be a memory leak with the apply method in general that should be investigated

If we use datetime.fromtimestamp to convert the POSIX timestamps to dates without pandas there is no leak.

#### Expected Output

Memory consumption should not increase and stay constant

#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.9.184-linuxkit
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.16.0
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.1
setuptools       : 45.1.0
Cython           : None
pytest           : 5.4.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : 0.9.3
psycopg2         : 2.8.2 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.2.16
tables           : None
tabulate         : 0.8.3
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.49.1

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
624626306,34380,BUG:,jesrael,closed,2020-05-26T06:14:40Z,2020-08-21T17:05:42Z,"From [SO](https://stackoverflow.com/q/62015410/2901002):
---

After specified column after groupby output is incorrect.


    df = pd.DataFrame({'token': [12345.0, 12345.0, 12345.0, 12345.0, 12345.0, 12345.0, 6789.0, 6789.0, 6789.0, 6789.0, 6789.0, 6789.0], 'name': ['abc', 'abc', 'abc', 'abc', 'abc', 'abc', 'xyz', 'xyz', 'xyz', 'xyz', 'xyz', 'xyz'], 'ltp': [2.0, 5.0, 3.0, 9.0, 5.0, 16.0, 1.0, 5.0, 3.0, 13.0, 9.0, 20.0], 'change': [np.nan, 1.5, -0.4, 2.0, -0.44444399999999995, 2.2, np.nan, 4.0, -0.4, 3.333333, -0.307692, 1.222222]})
    print (df)
          token name   ltp    change
    0   12345.0  abc   2.0       NaN
    1   12345.0  abc   5.0  1.500000
    2   12345.0  abc   3.0 -0.400000
    3   12345.0  abc   9.0  2.000000
    4   12345.0  abc   5.0 -0.444444
    5   12345.0  abc  16.0  2.200000
    6    6789.0  xyz   1.0       NaN
    7    6789.0  xyz   5.0  4.000000
    8    6789.0  xyz   3.0 -0.400000
    9    6789.0  xyz  13.0  3.333333
    10   6789.0  xyz   9.0 -0.307692
    11   6789.0  xyz  20.0  1.222222

---

Correct using named aggregation:
    
    df0 = df.groupby('name').agg(pos=pd.NamedAgg(column='change',aggfunc=lambda x: x.gt(0).sum()),\
                                neg = pd.NamedAgg(column='change',aggfunc=lambda x:x.lt(0).sum()))
    print (df0)
          pos  neg
    name          
    abc   3.0  2.0
    xyz   3.0  2.0

Wrong output:
    
    df1 = df.groupby('name')['change'].agg(pos = pd.NamedAgg(column='change',aggfunc=lambda x:x.gt(0).sum()),\
                                     neg = pd.NamedAgg(column='change',aggfunc=lambda x:x.lt(0).sum()))
        
    print (df1)
          pos  neg
    name          
    abc   2.0  2.0
    xyz   2.0  2.0

Correct using column after groupby:
    
    df2 = df.groupby('name')['change'].agg(pos = lambda x:x.gt(0).sum(),\
                                          neg = lambda x:x.lt(0).sum())
    print (df2)
          pos  neg
    name          
    abc   3.0  2.0
    xyz   3.0  2.0

In my opinion instead wrong output it should raise error/warning.

Version of pandas:


    INSTALLED VERSIONS
    ------------------
    commit           : None
    python           : 3.7.6.final.0
    python-bits      : 64
    OS               : Windows
    OS-release       : 7
    machine          : AMD64
    processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
    byteorder        : little
    LC_ALL           : None
    LANG             : en
    LOCALE           : None.None
    
    pandas           : 1.0.1
    numpy            : 1.18.1
    pytz             : 2019.3
    dateutil         : 2.8.1
    pip              : 20.0.2
    setuptools       : 45.2.0.post20200210
    Cython           : 0.29.15
    pytest           : 5.3.5
    hypothesis       : 5.5.4
    sphinx           : 2.4.0
    blosc            : None
    feather          : None
    xlsxwriter       : 1.2.7
    lxml.etree       : 4.5.0
    html5lib         : 1.0.1
    pymysql          : None
    psycopg2         : None
    jinja2           : 2.11.1
    IPython          : 7.12.0
    pandas_datareader: None
    bs4              : 4.8.2
    bottleneck       : 1.3.2
    fastparquet      : None
    gcsfs            : None
    lxml.etree       : 4.5.0
    matplotlib       : 3.1.3
    numexpr          : 2.7.1
    odfpy            : None
    openpyxl         : 3.0.3
    pandas_gbq       : None
    pyarrow          : None
    pytables         : None
    pytest           : 5.3.5
    pyxlsb           : None
    s3fs             : None
    scipy            : 1.4.1
    sqlalchemy       : 1.3.13
    tables           : 3.6.1
    tabulate         : None
    xarray           : None
    xlrd             : 1.2.0
    xlwt             : 1.3.0
    xlsxwriter       : 1.2.7
    numba            : 0.48.0
    None

"
628008797,34501,ENH: add logfmt support,link2xt,closed,2020-05-31T17:40:06Z,2020-08-21T17:05:43Z,"[logfmt](https://www.brandur.org/logfmt) is a structured log format similar to JSON lines, but much simpler. Each line has a format `column1=value1 column2=value2 ...`. Pandas already has support for JSON lines, which can be read with `pd.read_json(path, lines=True)`, but to read logfmt logs a conversion is needed.

I have been using [agrind](https://github.com/rcoh/angle-grinder) to convert logs to JSON lines, but it is an additional step that I would like to eliminate from my workflow. You can see an example at https://gitlab.com/nsnam/ns-3-dev/-/merge_requests/303

Besides, I would like to use logfmt as an output format for simple computational experiments (think C/Fortran without dependencies), because it is easier to maintain one line of code that outputs keys and values interleaved, in contrast to maintaining the code to output headers and code to output values separately. And logfmt becomes easier to read than CSV as the number of columns grows. Compare
```
node=1 throughput=63.0 delay=52.0 x=0.0 y=5.0
node=2 throughput=100.0 delay=35.0 x=5.0 y=5.0
```
to
```
node,throughput,delay,x,y
1,63.0,52.0,0.0,5.0
2,100.0,35.0,5.0,5.0
```

To read logfmt directly with pandas, I want a function `pd.read_logfmt()` that can be used in place of `pd.read_csv()` and `pd.read_json()` in described above cases.
"
626740582,34436,ENH: Implement min_max() function that returns both,matteosantama,closed,2020-05-28T19:02:13Z,2020-08-21T17:05:43Z,"#### Describe the solution you'd like

It might be a common enough situation for someone to want to know the min AND max of an index or series. I assume the min and max are not stored as attributes so the function has to scan all the data. It would be an improvement to scan once, and return both values as a tuple.

#### API breaking implications

None

```python
my_min, my_max = df.index.min_max()

```
"
625957061,34415,BUG: String series cat produces NaNs when left operand has a custom index,brandon-b-miller,closed,2020-05-27T19:06:38Z,2020-08-21T17:05:43Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
print(pd.__version__)

x = pd.Series(['a','b','c','d','e'])
x.index = ['1', '2', '3', '4', '5']
pd_others = pd.Series(['f','g','h','i','j'])

print(x.str.cat(others=pd_others))

1.1.0.dev0+1690.g70d7c04ff
1    NaN
2    NaN
3    NaN
4    NaN
5    NaN
dtype: object
```

#### Problem description
Possibly related to https://github.com/pandas-dev/pandas/issues/33425. The trigger for this bug seems to be the custom index. I get the correct answer if the line where `x.index` is set is removed.  

#### Expected Output

```
1.1.0.dev0+1690.g70d7c04ff
0    af
1    bg
2    ch
3    di
4    ej
dtype: object
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 70d7c04ff585de361622e4fe1788480a7a4526b5
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-76-generic
Version          : #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0.dev0+1690.g70d7c04ff
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.4.0.post20200518
Cython           : 0.29.17
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
</details>
"
628913902,34525,BUG: Replacing rows with iloc changes dtype,brcharron,closed,2020-06-02T05:33:30Z,2020-08-21T17:05:44Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df1 = pd.DataFrame([(1, pd.Timestamp.utcfromtimestamp(1))] * 5, columns=['nb', 'ts'])
df2 = pd.DataFrame([(2, pd.Timestamp.utcfromtimestamp(2))] * 3, columns=['nb', 'ts'])

print(""df2:\n"", df2)
print(""\ndf1 before:\n"", df1)

df1.iloc[[1, 4]] = df2.iloc[[0, 2]].values

print(""\ndf1 after replacing some rows by some of df2's rows:\n"", df1)
```

Full output is
```
df2:
    nb                  ts
0   2 1970-01-01 00:00:02
1   2 1970-01-01 00:00:02
2   2 1970-01-01 00:00:02

df1 before:
    nb                  ts
0   1 1970-01-01 00:00:01
1   1 1970-01-01 00:00:01
2   1 1970-01-01 00:00:01
3   1 1970-01-01 00:00:01
4   1 1970-01-01 00:00:01

df1 after replacing some rows by some of df2's rows:
    nb                   ts
0   1           1000000000
1   2  1970-01-01 00:00:02
2   1           1000000000
3   1           1000000000
4   2  1970-01-01 00:00:02
```

#### Problem description

The rows which are _not_ overwritten have their dtype changed (cast from Timestamp to int) although all dtypes align.

#### Expected Output

```
df1 after replacing some rows by some of df2's rows:
  nb                  ts
0  1 1970-01-01 00:00:01
1  2 1970-01-01 00:00:02
2  1 1970-01-01 00:00:01
3  1 1970-01-01 00:00:01
4  2 1970-01-01 00:00:02
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-862.14.4.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.14.5
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.1
setuptools       : 39.1.0
Cython           : 0.29.13
pytest           : 5.0.1
hypothesis       : None
sphinx           : 2.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.4.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.0
matplotlib       : 3.1.1
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.0.1
pyxlsb           : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : 1.3.6
tables           : 3.5.2
tabulate         : 0.8.3
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8
numba            : 0.45.1

</details>
"
628465400,34513,Release notes still missing for 1.0.4,impredicative,closed,2020-06-01T14:04:55Z,2020-08-21T17:05:44Z,"What's up with the release notes still missing for 1.0.4? It has been four days since the release. This is unprofessional.

https://github.com/pandas-dev/pandas/releases/tag/v1.0.4 links to https://pandas.pydata.org/docs/whatsnew/v1.0.4.html but the whatsnew page has been missing. Shouldn't it be made available no later than two hours after a release? People look for it and can't find it. A release shouldn't be published if its notes are not ready to go.

@simonjayhawkins you seem responsible."
597978313,33460,"BUG: When using apply on a DataFrame with SparseDtype, passing raw=True doesn't have any effect.",victor-ab,closed,2020-04-10T16:33:45Z,2020-08-21T17:05:58Z,"- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of pandas.
---


#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

pd.DataFrame(
    [[0, 1, 0, 2, 0, 3, 0, 4]],
    dtype=pd.SparseDtype(np.dtype(""int"")),
).apply(lambda x: type(x), raw=True, axis=1)
```
Output: 

 `0    <class 'pandas.core.series.Series'>`

#### Problem description

When using apply on a DataFrame with SparseDtype, passing raw=True doesn't have any effect.

#32425 is related but did not solve it.

#### Expected Output

 `0    <class 'numpy.ndarray'>`

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
597481280,33438, BUG: Inconsistent behavior of pd.DataFrame.drop ,FSpanhel,closed,2020-04-09T18:46:41Z,2020-08-21T17:05:58Z,"**Problem description**
Until an hour ago I thought that I can safely omit the use of inplace = True (which is not recommended, e.g., #30484) and instead use inplace = False and directly assign the result. 
For example, I thought that
```python
df.drop(columns = ['a'], inplace = True)
```
can be replaced by
```python
df = df.drop(columns = ['a']) 
```
where df is a pd.DataFrame.

However, this does not seem to be case when I use these operations within a function.
```python
# 1) direct assignment, .drop
df = pd.DataFrame([1])
def tfun(df):
    df['a'] = 2
    df = df.drop(columns = ['a'])
tfun(df)
print(df.columns)
>>> Index([0, 'a'], dtype='object')
``` 
Compare this with
```python
# 2) direct assignment, .drop with inplace (or del)
df = pd.DataFrame([1])
def tfun(df):
    df['a'] = 2
    df.drop(columns = ['a'], inplace = True) # using del df['a'] leads to the same result
tfun(df)
print(df.columns)
>>> Index([0], dtype='object')
```
The result of 2) is as expected (we add column 'a' and immediately remove it). However, I am very confused about the result of 1). The removal of column 'a' which is done inside tfun is not reflected in df outside after tfun is applied.

It gets even stranger when we use .assign to add column 'a' to df inside tfun:
```python
# 3) .assign, .drop
df= pd.DataFrame([1])
def tfun(df):
    df= df.assign(a = 2)
    df= df.drop(columns = ['a'])
tfun(df)
print(df.columns)
>>> RangeIndex(start=0, stop=1, step=1)
```
Now, column 'a' is removed, although the type of the remaining column is now a RangeIndex.
I definitely would expect that the result of 3) is equal to the result of 1). What is going on here?

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : de_DE.UTF-8
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : 0.29.15
pytest           : 5.3.5
hypothesis       : 5.5.4
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.48.0

</details>
"
597060535,33419,pandas 1.0.0 conflict with Anaconda,ronyarmon,closed,2020-04-09T06:47:34Z,2020-08-21T17:05:58Z,"Trying to install or update from pandas 0.25.1 to 1.0.0 I'm getting into a long process of examining conflicts with existing packages: 
```
conda install pandas=1.0.0
```
Installation log:

> Collecting package metadata (current_repodata.json): done
> Solving environment: failed with initial frozen solve. Retrying with flexible solve.
> Collecting package metadata (repodata.json): done
> Solving environment: failed with initial frozen solve. Retrying with flexible solve.
> Solving environment: | 
> Found conflicts! Looking for incompatible packages.
> This can take several minutes.  Press CTRL-C to abort

Followed by:
> Examining conflict for psutil parso spyder-kernels sphinxcontrib-websupport tblib urllib3  jupyter_core glob2 asn1crypto docutils sphinxcontrib boto sortedcollections matplotlib entrypoints markupsafe astropy... 
And a conflict check against individual packages which takes a long time. 

Working with pandas standalone or on virtual env is not an option as I need to run my function using a jupyter notebook.  Do I need to write it with version 0.25.1 or is there a work around that I may be missing here? 
"
598818419,33518,BUG: Illegal instruction: 4 on mid2010 Mac,lc3t35,closed,2020-04-13T10:46:14Z,2020-08-21T17:05:59Z,"#### Code Sample, a copy-pastable example

```python
Python 3.7.7 (default, Mar 26 2020, 10:32:53) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas as pd
Illegal instruction: 4
(Pandas) iMac:~ laurent$ sysctl -a | grep machdep.cpu.features
machdep.cpu.features: FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 DTES64 MON DSCPL VMX SMX EST TM2 SSSE3 CX16 TPR PDCM SSE4.1 SSE4.2 POPCNT
(Pandas) iMac:~ laurent$ conda list numpy
# packages in environment at /Users/laurent/opt/anaconda3/envs/Pandas:
#
# Name                    Version                   Build  Channel
numpy                     1.18.1           py37h7241aed_0  
numpy-base                1.18.1           py37h6575580_1  
```

#### Problem description

With a mid 2010 Mac, OSX 10.13.6 (High Sierra)

  ```
Identifiant du modèle :	iMac11,3
  Nom du processeur :	Intel Core i5
  Vitesse du processeur :	2,8 GHz
  Nombre de processeurs :	1
  Nombre total de cœurs :	4
  Cache de niveau 2 (par cœur) :	256 Ko
  Cache de niveau 3 :	8 Mo
  Mémoire :	16 Go
  Version de la ROM de démarrage :	99.0.0.0.0
  Version SMC (système) :	1.59f2
```

`import pandas as pd`, crashes with `Illegal instruction: 4`

Old Mac doesn't support VFX, as it is not displayed in sysctl -a | grep machdep.cpu.features output.

This problem is also mentioned here : 

https://github.com/conda/conda/issues/9678
https://github.com/das-developers/condaCDF/issues/1

and as it was detected when trying to run inside Anaconda/jupyter, the work in progress to solve is described here : 

https://discourse.jupyter.org/t/osx-10-13-6-kernelrestarter-restarting-kernel/3965/8
"
598185969,33475,BUG: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.   import pandas.util.testing as tm,fescobar96,closed,2020-04-11T04:12:16Z,2020-08-21T17:05:59Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

```

#### Problem description

I'm getting this future warning when I import pandas as pd. It does not affect pandas' performance, but I think this is something that should be looked at. I have already looked into reports for similar issues, but the circumstances are different. It is also important to note that I am working in Google Colab.

#### Expected Output
There should be no warning.
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]
pandas           : 1.0.3
</details>
"
598154375,33472,DOC: unwanted line of text,benji1123,closed,2020-04-11T00:38:12Z,2020-08-21T17:05:59Z,"#### Location of the documentation
[./doc/source/development/contributing.rst](https://github.com/pandas-dev/pandas/blob/master/doc/source/development/contributing.rst#bug-reports-and-enhancement-requests)

#### Documentation problem
There seems to be an [unwanted line of text](https://github.com/pandas-dev/pandas/blob/master/doc/source/development/contributing.rst) at the top of the document: ""{{ header }}""

#### Suggested fix for documentation
[I fixed it in my fork](https://github.com/benji1123/pandas/blob/master/doc/source/development/contributing.rst) and can make a PR (into master?)."
600611134,33574,appropriate converting without '1900-01-01',BenbrahimMouad,closed,2020-04-15T21:54:34Z,2020-08-21T17:06:00Z,"hello everybody, i wish that you are in good healthy, so i have one little problem, i wanna to convert one column from my tabular csv (a lot of columns of time HH:MM:SS) so i see that a lot of my columns are object so i wanna to convert them to datetime64 with this method include in pandas library, 

`data['DT']=data['DT].apply(pd.to_datetime)` 
the problem that he showed me the actual date so i wanna just HH:MM:SS without date and preserving that their type format in datetime.
any kind of help if possible in my case."
599346577,33541,DownSampling Time Series Data using pandas,Solly7,closed,2020-04-14T07:08:21Z,2020-08-21T17:06:00Z,"Hi all, i am still new to machine learning , I have time series data as per below and i need tips/help on how to downsample it using scikit-learn. see my dummy data below ,Payment status is my label(0=missed, 1=paid), i need to downsample using time series strategy and keeping temporal order

```
Policy_no  Deduction_date    Payment_status
1               01-Jan-2019                      1
2               01-Jan-2019                      1
3               01-Jan-2019                      1
4               01-Jan-2019                      1
5               01-Jan-2019                      1
1               01-feb-2019                      1
2               01-feb-2019                      1
3               01-feb-2019                      0
4               01-feb-2019                      0
5               01-feb-2019                      0
1               01-mar-2019                     1
2               01-mar-2019                     1
3               01-mar-2019                     1
```
"
600917119,33587,QST: pd.pivot_table() generates wrong sums,zybex86,closed,2020-04-16T10:11:07Z,2020-08-21T17:06:01Z,"- [ X ] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ X ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

I have stumbled upon a strange phenomenon...

The same question asked on [StackOverflow](https://stackoverflow.com/questions/60883087/pandas-pivot-table-generates-wrong-sums)

I have a working code to generate pivoted tables from data uploaded from excels. I have stumbled on a strange case where after applying pivot_table to the data, I get different summed values than before pivot...

I generate data from xlsx and xls files, do some stuff with it and then want to generate a pivoted table. Before the pivot I do df.VAL1.sum() and df.VAL2.sum() and the values are the same ones as the ones summed in excels using =sum(), but after I run my pd.pivot_table() code I get a different result when I do pivot.sum()

The data has the following columns:
```
COL1 COL2 COL3 time_period VAL1 VAL2 UNIT_VAL1 COL4 COL5 COL6 UNIT_VAL2 COL7 COL8 COL9
```
where time_period is a string - YYYY-MM

and here is the pivot_table attributes:

```python
pivot = pd.pivot_table(
    df[[
        'COL1', 'COL2', 'COL3', 'COL9', 'time_period',
        'VAL1','COL8', 'COL4', 'COL5', 'COL6', 'COL7', 'UNIT_VAL1'
    ]],
    values='VAL1',
    columns='time_period',
    index=[
        'COL7', 'COL8', 'COL3', 'COL1',
        'COL5', 'COL2', 'COL9',
        'COL4', 'COL6', 'UNIT_VAL1'
    ],
    aggfunc=np.sum
)
```
Can the string `time_period` be the problem here or the order of the passed column names? Or maybe I am doing something wrong here? I want to note that this happens only with 2 dataframes, as other dataframes work well.

I did observe that this pivot code produces different sums after 4 decimal spaces, but that shouldn't change the sums IMHO... Maybe I should use `df.picot_table` instead of `pd.pivot_table` ? The problem is I need two tables from the data frame - one in KG and one in local CUR...
"
600643908,33576,BUG:,GeyseR,closed,2020-04-15T23:18:32Z,2020-08-21T17:06:01Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame([
    {'id': 1},
    {'id': 2}
])
df2 = pd.DataFrame([
    {'id': 2, 'col1': 'text', 'col2': None}
])
df = pd.merge(df, df2, how='left', on=['id'])
print(df)

#      id  col1    col2
# 0   1   NaN   NaN
# 1   2   text    None

df = pd.DataFrame([
    {'id': 1},
    {'id': 2}
])
df2 = pd.DataFrame([
    {'id': 2, 'col2': None}
])
df = pd.merge(df, df2, how='left', on=['id'])
print(df)
#     id  col2
# 0   1  None
# 1   2  None
```

#### Problem description
Not sure is it a ""real "" bug or just an expected behavior of pandas. From the code fragment below you can see that with different input pandas fills empty columns with different empty values. 

#### Expected Output
```
#      id  col1    col2
# 0   1   NaN   None
# 1   2   text    None
```
For the first print statement

#### Output of ``pd.show_versions()``

<details>

import pandas as pd
pd.show_versions()
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.76-linuxkit
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_US.UTF-8
pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.8
numba            : None

</details>
"
603878355,33695,ENH: Enable rolling.apply on custom function that requires multiple columns of data frame,bbkaran,closed,2020-04-21T10:13:27Z,2020-08-21T17:06:02Z,"#### Is your feature request related to a problem?

Yes.

I wish I could use pandas to:

create an additional column ` 'newc' ` of my dataframe `df` as `df['newc']` through rolling.apply on `df['cond']` with a custom function. The custom function requires two columns of `df`. 

#### Describe the solution you'd like

I prefer the solution to look something like:

`df['newc'] = df['cond'].rolling(4).apply(T_correction, 
args = (df['temp'].rolling(4))) `

where

```
>>> df.head()
                       temp   cond
ts
2018-06-01 00:00:00  51.908  27.83
2018-06-01 00:05:00  52.144  27.83
2018-06-01 00:10:00  51.880  27.83
2018-06-01 00:15:00  52.001  27.83
2018-06-01 00:20:00  51.835  27.83

def T_correction(df, d):
    df = pd.DataFrame(data = df)
    df.columns = ['cond']
    df['temp'] = d
    X = df.drop(['cond'], axis = 1)    # X features: temp

    X = sm.add_constant(X)             # add intercept
    lmodel = sm.OLS(df.cond, X)        # fit cond = a + b*temp
    results = lmodel.fit()             #
    Op = results.predict(X)            # derive 'cond' as explained by temp
    Tc1 = df.cond - Op                 # remove the linear influence

#---conditional correction --------------------------------------
    Tc = np.where(df.temp > (np.mean(df.temp) + 0.5*np.std(df.temp)), df.cond, Tc1)
    return Tc[-1]     # returning the last value
```
I understand it may be related to the issue:

```
raise NotImplementedError('See issue #11704 {url}'.format(url=url))
NotImplementedError: See issue #11704 https://github.com/pandas-dev/pandas/issues/11704
```

This may not have reached high enough importance for someone to look into it.  Or there may be other ways we could achieve this. I am not experienced enough with pandas code to start looking into myself but I am happy try if some guidance is available.

#### API breaking implications

No changes in the way we call the API

#### Describe alternatives you've considered

Ended up using loop - much slower

"
601052736,33590,BUG: replace in Series not working when pd.NA is present,BayerSe,closed,2020-04-16T13:26:35Z,2020-08-21T17:06:02Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

# works
pd.Series(['A', None]).replace({'A': 0})

# raises TypeError: Cannot compare types 'ndarray(dtype=object)' and 'str'
pd.Series(['A', pd.NA]).replace({'A': 0})
```

#### Problem description

The `replace` method does not work when `pd.NA` instead of `None`  is contained in the series.

#### Expected Output

Both statements should replace 'A' with 0 without touching the missing number.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-96-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.16
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None


</details>
"
604137951,33704,BUG: min/max of empty datetime dataframe raises,adbull,closed,2020-04-21T17:02:58Z,2020-08-21T17:06:03Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
df = pd.DataFrame(dict(x=pd.to_datetime([])))
df.max()
```

```python-traceback
Traceback (most recent call last):
  File ""<ipython-input-17-be9940feb663>"", line 1, in <module>
    df.max()
  File ""pandas/core/generic.py"", line 11215, in stat_func
    f, name, axis=axis, skipna=skipna, numeric_only=numeric_only
  File ""pandas/core/frame.py"", line 7907, in _reduce
    result = f(values)
  File ""pandas/core/frame.py"", line 7865, in f
    return op(x, axis=axis, skipna=skipna, **kwds)
  File ""pandas/core/nanops.py"", line 109, in f
    return _na_for_min_count(values, axis)
  File ""pandas/core/nanops.py"", line 392, in _na_for_min_count
    result.fill(fill_value)
ValueError: cannot convert float NaN to integer
```

#### Problem description

When taking the min/max of an empty datetime dataframe, a ValueError is raised. This is surprising, and inconsistent with the case of an empty datetime series, where min/max return NaT.

#### Expected Output

```python
x   NaT
dtype: datetime64[ns]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.20.11-100.fc28.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : 0.29.15
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : 1.3.16
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
603969199,33698,BUG:  Incorrect dtypes after resetting a multi-index for a data frame with no rows,pkarol,closed,2020-04-21T12:45:22Z,2020-08-21T17:06:03Z,"- [X] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
>>> import pandas as pd
>>> df = pd.DataFrame({'a': [1], 'b': [pd.Timestamp('2020-04-21')]})
>>> df = df[df['a'] == 0]
>>> df
Empty DataFrame
Columns: [a, b]
Index: []

>>> df['time'] = pd.Timestamp.now()
>>> df['index'] = 0
>>> df
Empty DataFrame
Columns: [a, b, time, index]
Index: []
>>> df.dtypes
a                 int64
b        datetime64[ns]
time     datetime64[ns]
index             int64
dtype: object

>>> df = df.set_index(['time', 'index'])
>>> df.dtypes
a             int64
b    datetime64[ns]
dtype: object
>>> df.index.get_level_values(0)
DatetimeIndex([], dtype='datetime64[ns]', name='time', freq=None)
>>> df.index.get_level_values(1)
Int64Index([], dtype='int64', name='index')

>>> df.reset_index().dtypes
time            float64
index           float64
a                 int64
b        datetime64[ns]
dtype: object
```

#### Problem description

The dtypes on a data frame multi-index with no rows are lost when the index is reset. As you can see from the above sample, setting the index preserves the correct dtypes in the multi-index levels. When the index is reset, the dtypes become `float64`. Instead, I would expect the dtypes for `time` and `index` to be `datetime64[ns]` and `int64` when the index is reset.

Note: The behavior is correct when there is at least one row in the data frame and this bug only occurs when the data frame has no rows.

#### Expected Output

```python
time     datetime64[ns]
index             int64
a                 int64
b        datetime64[ns]
dtype: object
```

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.10.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.0.0-29-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.0.3
numpy            : 1.16.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : None
setuptools       : 46.1.3.post20200325
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : 1.3.15
tables           : None
tabulate         : None
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0
```

</details>
"
605361693,33744,pandas statistics,nschloe,closed,2020-04-23T08:58:35Z,2020-08-21T17:06:04Z,"Just a small heads-up of how well pandas is doing. I wrote [hotware](https://github.com/nschloe/hotware) to check the popularity of software on GitHub and StackOverflow, and in the scientific Python department, pandas is clearly number 1:

![pd](https://user-images.githubusercontent.com/181628/80079616-cfe52400-8550-11ea-95a6-cdcab06530c1.png)
https://github.com/nschloe/hotware#scientific-python

You might want to use this in your next funding proposal. :smile_cat: 

Congrats on a great piece of software!"
605202515,33738,ENH: rejected support for Python data structures containing Pandas DataFrames/Series,xuancong84,closed,2020-04-23T03:01:14Z,2020-08-21T17:06:04Z,"It is great that you have implemented to_csv and read_csv() to export and import tables in CSV format. It would be even better to be able to serialize/deserialize Python data structures such as list and dict containing DataFrames/Series. For example, something like [this](https://github.com/xuancong84/pandas-serializer).

Would that be possible? Thanks!"
605098023,33732,BUG: groupby ignores sort=False when observed=True breaking transform,dhorkel,closed,2020-04-22T21:51:41Z,2020-08-21T17:06:04Z,"- [x] I have checked that this issue has not already been reported.
Maybe related to but distinct from https://github.com/pandas-dev/pandas/issues/27369

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample

```python
import pandas as pd
df = pd.DataFrame({'cat_col':pd.Categorical(['a','c','a','b']),'other_col':['q','w','w','q'],'val':[1,2,3,4]})
df.groupby(['cat_col','other_col'],observed=False,sort=False)['val'].transform(max)
```
returns
```
0    1.0
1    3.0
2    NaN
3    2.0
Name: val, dtype: float64
```
whereas
```python
df.groupby(['cat_col','other_col'],observed=True,sort=False)['val'].transform(max)
```
returns
```
0    1
1    2
2    3
3    4
Name: val, dtype: int64
```
What is going on is more clear when using `.agg()`

```python
df.groupby(['cat_col','other_col'],observed=False,sort=False)['val'].agg(max)
```
returns
```
a        q            1.0
         w            3.0
c        q            NaN
         w            2.0
b        q            4.0
         w            NaN
```
whereas 
```python
df.groupby(['cat_col','other_col'],observed=True,sort=False)['val'].agg(max)
```
returns
```
a        q            1
c        w            2
a        w            3
b        q            4
Name: val, dtype: int64
```

#### Problem description
The `sort=False` is not respected when `observed=False`. It appears `transform()` assumes the original order still holds and maps the results to the shape of the original dataframe.

This is an issue when using `.transform()` to create a new column in the original (unsorted) dataframe as the rows will not be correctly associated.


#### Expected Output
`transform()` should give the same result whether `observed=True` or `observed=False` in the `groupby()`.


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : 0.9.3
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.3.1
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
606833001,33794,BUG: can't import pandas in directory containing file named inspect.py,ekulno,closed,2020-04-25T18:55:05Z,2020-08-21T17:06:05Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```bash
$ python3 -c ""import pandas;print(pandas.__version__)"" && touch inspect.py && python3 -c ""import pandas""
1.0.3
Traceback (most recent call last): <error>
```
The error message: 
<details>

  File ""<string>"", line 1, in <module>
  File ""/home/f/.local/lib/python3.6/site-packages/pandas/__init__.py"", line 55, in <module>
    from pandas.core.api import (
  File ""/home/f/.local/lib/python3.6/site-packages/pandas/core/api.py"", line 29, in <module>
    from pandas.core.groupby import Grouper, NamedAgg
  File ""/home/f/.local/lib/python3.6/site-packages/pandas/core/groupby/__init__.py"", line 1, in <module>
    from pandas.core.groupby.generic import DataFrameGroupBy, NamedAgg, SeriesGroupBy
  File ""/home/f/.local/lib/python3.6/site-packages/pandas/core/groupby/generic.py"", line 60, in <module>
    from pandas.core.frame import DataFrame
  File ""/home/f/.local/lib/python3.6/site-packages/pandas/core/frame.py"", line 104, in <module>
    from pandas.core.generic import NDFrame, _shared_docs
  File ""/home/f/.local/lib/python3.6/site-packages/pandas/core/generic.py"", line 151, in <module>
    class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):
  File ""/home/f/.local/lib/python3.6/site-packages/pandas/core/generic.py"", line 1111, in NDFrame
    def rename_axis(self, mapper=lib.no_default, **kwargs):
  File ""/home/f/.local/lib/python3.6/site-packages/pandas/util/_decorators.py"", line 229, in decorate
    kind = inspect.Parameter.POSITIONAL_OR_KEYWORD
AttributeError: module 'inspect' has no attribute 'Parameter'
</details>

#### Problem description

When using pandas in a project, I must take care not to use the filename `inspect.py`. 

#### Expected Output

No error

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-96-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 9.0.1
setuptools       : 41.5.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 0.999999999
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : None
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 2.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : 0.8.3
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
605909539,33754,ENH: .to_rpy2 method for DataFrame and Series,jolespin,closed,2020-04-23T22:16:29Z,2020-08-21T17:06:05Z,"#### Is your feature request related to a problem?

https://rpy2.github.io/doc/v3.0.x/html/generated_rst/pandas.html
I've been using rpy2 a lot and considering that pandas was inspired from R it would be nice to have a .to_rpy2 method for seemless conversion. 

#### Describe the solution you'd like
```
df = pd.DataFrame(data)
r_df = df.to_rpy2()
```
#### API breaking implications

It shouldn't affect the API at all.  If `rpy2` isn't installed then throw an error just like writing excel files needs another package. 

#### Describe alternatives you've considered

I made wrappers that I need to use. 

#### Additional context

Current method with `rpy2` requires some boilerplate and is less seamless:


![image](https://user-images.githubusercontent.com/9061708/80154923-64617d80-8575-11ea-98bf-1fb88d5a689b.png)


Here's my current hack: 

![image](https://user-images.githubusercontent.com/9061708/80155790-4f85e980-8577-11ea-9498-8a8edbd2bebe.png)
"
606895170,33800,BUG: get_dummies does not create dummy columns for values passed to CategoricalDtype,queirozfcom,closed,2020-04-26T01:48:02Z,2020-08-21T17:06:06Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

from pandas.api.types import CategoricalDtype

df = pd.DataFrame({'country': ['russia', 'germany', 'russia', 'germany', 'germany']})

df[""country""] = df[""country""].astype('category',CategoricalDtype(categories=[""germany"",""russia"",""japan""]))

# there should be a column called `japan`, but it's missing
pd.get_dummies(df[""country""])
```

#### Problem description

Calling `pd.get_dummies()` on a DataFrame column of type `category` having extra `categories` passed does **not** generate columns for **extra categories** other than those that exist in the DataFrame.

#### Expected Output

The output should be a dataframe with one-hot encoded columns for values `germany`, `russia` **and** for `japan`. 

(This was the output in Pandas 0.24 when we could still use `.astype('category', categories=[...])` without needing to define a `CategoricalDtype`)


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-42-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : pt_BR.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
615430654,34100,DOC: timezone warning for dates beyond TODAY,joooeey,closed,2020-05-10T16:47:28Z,2020-08-21T17:16:40Z,"introducing a suggestion discussed in PR #33863 :
Added a warning in the user guide that timezone conversion on future dates is inherently unreliable.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
682818473,35823,"CI/DOC: unpin gitdb, use gitdb instead of gitdb2",fangchenli,closed,2020-08-20T15:33:52Z,2020-08-21T21:12:59Z,The developers of gitdb have regained their original PyPi account. And they have moved back to the package name gitdb from gitdb2. See [gitpython-developers/gitdb/issues/59](https://github.com/gitpython-developers/gitdb/issues/59). 
683177299,35836,CI: avoid file leak from ipython tests,jbrockmendel,closed,2020-08-21T00:37:00Z,2020-08-21T21:16:45Z,Broken off from #35711
683625921,35841,REF: simplify _cython_agg_blocks,jbrockmendel,closed,2020-08-21T15:02:36Z,2020-08-21T21:22:26Z,"Orthogonal to #35839, though a rebase will be needed.

cc @TomAugspurger did you already try this in #31616?  If so, we need to identify a test case in which this doesnt work"
682819654,35824,CI/DOC: unpin gitdb #35823,fangchenli,closed,2020-08-20T15:35:04Z,2020-08-21T21:52:46Z,"- [x] closes #35823

"
679628270,35740,REF: _apply_blockwise define exclude in terms of skipped,jbrockmendel,closed,2020-08-15T19:16:36Z,2020-08-21T22:06:35Z,orthogonal to #35730
477921074,27800,"BUG: aggregation on ordered categorical column drops grouping index or crashes, depending on context",kpflugshaupt,closed,2019-08-07T13:13:15Z,2020-08-21T22:34:52Z,"### Code Sample
Build the model data frame:
```python
df = pd.DataFrame({
    'nr': [1,2,3,4,5,6,7,8], 
    'cat_ord': list('aabbccdd'), 
    'cat':list('aaaabbbb')
})
df = df.astype({'cat': 'category', 'cat_ord': 'category'})
df['cat_ord'] = df['cat_ord'].cat.as_ordered()
```
When grouping, single aggregations on a _numeric column_ work:
```python
df.groupby('cat').agg({'nr': 'min'})
```
```
	nr
cat	
a	1
b	5
```
**Single aggregations** on an _ordered categorical column_ work, but drop the grouping index:
```python
df.groupby('cat').agg({'cat_ord': 'min'})
```
```
   cat_ord
0        a
1        c
```
**Combined single aggregations** on a _numeric_ and an _ordered categorical_ column work:
```python
df.groupby('cat').agg({'nr': 'min', 'cat_ord': 'min'})
```
```
       nr cat_ord
cat		
a	1	a
b	5	c
```
**Multiple aggregations** on an _ordered categorical_ column work, but drop the grouping index:
```python
df.groupby('cat').agg({'cat_ord': ['min', 'max']})
```
```
          cat_ord
      min     max
0	a	b
1	c	d
```
**Combined aggregations** on a _numeric_ (single) and an _ordered categorical_ column (multiple) fail with a **TypeError**:
```python
df.groupby('cat').agg({'nr': 'min', 'cat_ord': ['min', 'max']})
```
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-148-b446de510106> in <module>
----> 1 df.groupby('cat').agg({'nr': 'min', 'cat_ord': ['min', 'max']})

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\groupby\generic.py in aggregate(self, arg, *args, **kwargs)
   1453     @Appender(_shared_docs[""aggregate""])
   1454     def aggregate(self, arg=None, *args, **kwargs):
-> 1455         return super().aggregate(arg, *args, **kwargs)
   1456 
   1457     agg = aggregate

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\groupby\generic.py in aggregate(self, func, *args, **kwargs)
    227         func = _maybe_mangle_lambdas(func)
    228 
--> 229         result, how = self._aggregate(func, _level=_level, *args, **kwargs)
    230         if how is None:
    231             return result

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\base.py in _aggregate(self, arg, *args, **kwargs)
    528                 # return a MI DataFrame
    529 
--> 530                 return concat([result[k] for k in keys], keys=keys, axis=1), True
    531 
    532             elif isinstance(self, ABCSeries) and is_any_series():

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\reshape\concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    256     )
    257 
--> 258     return op.get_result()
    259 
    260 

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\reshape\concat.py in get_result(self)
    466                     obj_labels = mgr.axes[ax]
    467                     if not new_labels.equals(obj_labels):
--> 468                         indexers[ax] = obj_labels.reindex(new_labels)[1]
    469 
    470                 mgrs_indexers.append((obj._data, indexers))

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexes\category.py in reindex(self, target, method, level, limit, tolerance)
    616                 # coerce to a regular index here!
    617                 result = Index(np.array(self), name=self.name)
--> 618                 new_target, indexer, _ = result._reindex_non_unique(np.array(target))
    619             else:
    620 

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexes\base.py in _reindex_non_unique(self, target)
   3434 
   3435         target = ensure_index(target)
-> 3436         indexer, missing = self.get_indexer_non_unique(target)
   3437         check = indexer != -1
   3438         new_labels = self.take(indexer[check])

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexes\base.py in get_indexer_non_unique(self, target)
   4792             tgt_values = target._ndarray_values
   4793 
-> 4794         indexer, missing = self._engine.get_indexer_non_unique(tgt_values)
   4795         return ensure_platform_int(indexer), missing
   4796 

pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_indexer_non_unique()

TypeError: '<' not supported between instances of 'str' and 'int'
```
**Combined aggregations** on a _numeric_ (multiple) and an _ordered categorical_ column (single) also fail with the same **TypeError**:
```python
df.groupby('cat').agg({'nr': ['min', 'max'], 'cat_ord': 'min'})
```
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-164-b1d70184bd81> in <module>
----> 1 df.groupby('cat').agg({'nr': ['min', 'max'], 'cat_ord': 'min'})

...

pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_indexer_non_unique()

TypeError: '<' not supported between instances of 'str' and 'int'
```

### Problem description

Aggregations on ordered categoricals drop the grouping index, or crash, as shown above.

This makes it hard to calculate combined aggregations over big data sets correctly and efficiently.

### Expected Output

Aggregations on ordered categoricals should work as on non-categorical columns.

### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.0
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : 0.29.12
pytest           : 5.0.1
hypothesis       : None
sphinx           : 2.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.3.4
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.7.0
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.1.1
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : 0.11.1
pytables         : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : 1.3.5
tables           : 3.5.2
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8
</details>
"
675536103,35630,TST: add test for agg on ordered categorical cols,mathurk1,closed,2020-08-08T14:44:51Z,2020-08-21T22:34:57Z,"- [x] closes #27800
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
95477094,10603,Resample yields empty groups,JonasAbernot,closed,2015-07-16T16:48:05Z,2020-08-21T22:42:51Z,"With some parameters, the last group yield by resample is empty. Example : 

``` python
import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.normal(size=(10000,4)))
df.index = pd.timedelta_range(start='0s', periods=10000, freq='3906250n')

df.loc['1s':,:].resample('3s',how=lambda x : len(x))
```

Depending of the 'how' function used, this can lead to surprising bugs.

```
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.8.final.0
python-bits: 64
OS: Linux
OS-release: 3.13.0-46-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: fr_FR.UTF-8

pandas: 0.16.0-294-g45f69cd
nose: 1.3.6
Cython: 0.20.2
numpy: 1.9.2
scipy: 0.14.0
statsmodels: 0.5.0
IPython: 3.0.0-dev
sphinx: 1.2.2
patsy: 0.3.0
dateutil: 2.4.2
pytz: 2015.4
bottleneck: None
tables: 3.1.1
numexpr: 2.4
matplotlib: 1.4.3
openpyxl: 1.7.0
xlrd: 0.9.2
xlwt: 0.7.5
xlsxwriter: None
lxml: None
bs4: 4.3.2
html5lib: 0.999
httplib2: None
apiclient: None
sqlalchemy: 0.8.2
pymysql: None
psycopg2: 2.5.3 (dt dec mx pq3 ext)
```
"
681644333,35799,TST: resample does not yield empty groups (#10603),tkmz-n,closed,2020-08-19T08:19:44Z,2020-08-21T22:42:56Z,"- [x] closes #10603
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This adds a test to verify that resample does not yield empty groups.

I'm new to contributing to pandas, and I'm not sure where to add this test.
Let me know if this place is inappropriate."
677017594,35674,Avoid redirect,chankeypathak,closed,2020-08-11T16:25:43Z,2020-08-22T03:08:29Z,A minor change to avoid 301 redirect on link.
683248743,35839,REF: remove unnecesary try/except,jbrockmendel,closed,2020-08-21T04:19:57Z,2020-08-22T03:17:42Z,and make cast_result_block into cast_agg_result operate on values instead of blocks
679960787,35759,PERF: Allow jitting of groupby agg loop,mroeschke,closed,2020-08-17T06:01:31Z,2020-08-22T03:31:34Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

New performance comparison for 10,000 groups

```
In [1]: In [1]: df_g = pd.DataFrame({'a': range(10**4), 'b': range(10**4), 'c': range(10**4)})

In [2]: In [2]: def f(x):
   ...:    ...:     return np.sum(x) + 1
   ...:

In [3]: df_g.groupby('a').agg(f)
Out[3]:
          b      c
a
0         1      1
1         2      2
2         3      3
3         4      4
4         5      5
...     ...    ...
9995   9996   9996
9996   9997   9997
9997   9998   9998
9998   9999   9999
9999  10000  10000

[10000 rows x 2 columns]

In [4]: %timeit df_g.groupby('a').agg(f)
1.2 s ± 70.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [5]: def f(values, index):
   ...:     return np.sum(values) + 1
   ...:

In [6]: df_g.groupby('a').agg(f, engine='numba', engine_kwargs={'parallel': True})
Out[6]:
            b        c
a
0         1.0      1.0
1         2.0      2.0
2         3.0      3.0
3         4.0      4.0
4         5.0      5.0
...       ...      ...
9995   9996.0   9996.0
9996   9997.0   9997.0
9997   9998.0   9998.0
9998   9999.0   9999.0
9999  10000.0  10000.0

In [8]: %timeit df_g.groupby('a').agg(f, engine='numba', engine_kwargs={'parallel': True})
2.07 ms ± 64.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```"
651505256,35135,BUG:  .corr() values higher than 1,PanPip,closed,2020-07-06T12:39:29Z,2020-08-22T16:49:47Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

_Sorry for including an external dataset. I couldn't reproduce this bug with a smaller one._

```python
url = 'https://raw.githubusercontent.com/MislavSag/trademl/master/trademl/modeling/random_forest/X_TEST.csv'
df = pd.read_csv(url, sep=',')
df = X_TEST.loc[:,['RSI30','CMO30']]

df.corr() > 1
```

#### Problem description

When applying .corr() on the given dataset the output Pearson's correlation is slightly >1 (6.661338e-16). I'd assume it should be equal to 1. 

![picture](https://user-images.githubusercontent.com/16906989/86593669-ed455e00-bf95-11ea-84fe-2a790f894d5e.png)

#### Expected Output

The expected result would be correlation values <= 1.

![picture](https://user-images.githubusercontent.com/16906989/86593787-2d0c4580-bf96-11ea-8358-60b6f5aec462.png)


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.0.post20200616
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.49.1

</details>
"
676232008,35655,Moto server,martindurant,closed,2020-08-10T15:40:01Z,2020-08-22T19:26:28Z,"Changes moto for s3 tests from monkeypatched/mocking to server mode. This allows aiobotocore exceptions to raise correctly, needed for tests to pass; the change is required by the upcoming async release of s3fs, but also works for old sync (botocore) version. A release of s3fs without this change would break pandas tests.

Note: since I now need to pass storage_options to the various s3 calls in the tests to make them pass (giving the endpoint of the moto s3 server), I needed to also plumb storage_options through the excel IO, which was previously missing, and update the excel tests.

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
679416640,35730,REF: move towards making _apply_blockwise actually block-wise,jbrockmendel,closed,2020-08-14T21:42:51Z,2020-08-23T01:26:18Z,"A step towards #34714, orthogonal to the other outstanding PR in this vein #35696."
684043233,35857,DOC: Fix code of conduct link,ghost,closed,2020-08-22T18:35:39Z,2020-08-23T07:40:30Z,"closes #35855
"
683716626,35844,BUG: UserWarning: Could not import the lzma module. Your installed Python is incomplete. ,andreamoro,closed,2020-08-21T17:47:05Z,2020-08-23T10:28:49Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

file = 'financial.csv'
df = pd.read_csv(file)

df.head(10)
```

#### Problem description

A simple csv import like the one above produce a user warning which arose since I had to move the HomeBrew directory into a custom location due to IT settings not letting me write anymore on the /usr/local.

I have since then reinstalled a fresh version of all the packages via pip as well as the zx module via homebrew. 

Accordingly to this [user](https://superuser.com/a/1277306/229674), he was able to resolve by adding the following command into the .zshrc

export LDFLAGS=""-L$HOME/homebrew/opt/xz/lib $LDFLAGS""
export CPPFLAGS=""-I$HOME/homebrew/opt/xz/include $CPPFLAGS""
export PKG_CONFIG_PATH=""$HOME/homebrew/opt/xz/lib/pkgconfig $PKG_CONFIG_PATH""

#### Expected Output

No User Warning

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.1
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 47.1.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.3.3
lxml.etree       : None
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : 1.0.6
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
681454183,35796,ENH:dataframe columns dropping with column names case-insensitive,rmsmani,closed,2020-08-19T00:25:03Z,2020-08-23T13:50:19Z,"#### Is your feature request related to a problem?
NO

#### Describe the solution you'd like
Currently the data frame columns are case-sensitive, when we try to drop the column with different case, the error is thrown.
My suggestion is to have one more option like ignorecase as boolean, if the flag is true, then drop the column if the name satisifies

#### API breaking implications

[this should provide a description of how this feature will affect the API]

#### Describe alternatives you've considered

[this should provide a description of any alternative solutions or features you've considered]
Currently, changing the both the dataframe column and the drop column to lower case and delete it.

#### Additional context
Its good to have feature..

```python
# Your code here, if applicable

import io
import pandas as pd

source = io.StringIO(""""""col1, col2,col3
1,2,3
"""""")

df = pd.read_csv(source)

print (df)
df = df.drop(""COL3"", axis=1)
print(df)

   col1   col2  col3
0     1      2     3

Error
  File ""test/test.py"", line 16, in <module>
    df = df.drop(""COL3"", axis=1)

KeyError: ""['COL3'] not found in axis""

```
"
683972508,35851,How can i read the a non UTF-8 CSV file in pandas,Yashdew,closed,2020-08-22T10:24:10Z,2020-08-23T15:14:25Z,"hey guys, Can anyone tell me how to read a CSV which is not a UTF-8 CSV file format. I am doing a project for my school. help me.!"
684031371,35855,DOC: code of conduct link doesn't work,MarcoGorelli,closed,2020-08-22T17:14:00Z,2020-08-24T08:01:59Z,"#### Location of the documentation

https://pandas.pydata.org/about/team.html

**Note**: You can check the latest versions of the docs on `master` [here](https://pandas.pydata.org/docs/dev/).

#### Documentation problem

> We have a code of conduct to ensure a friendly and welcoming environment. 

""code of conduct"" links to https://pandas.pydata.org/about/%7Bbase_url%7D/community/coc.html , which doesn't exist

#### Suggested fix for documentation

Link to correct page
"
675500779,35624,DOC: Fixed docstring for mode(),MarianD,closed,2020-08-08T10:14:59Z,2020-08-24T08:07:17Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
684475495,35868,CI: TestParquetFastParquet::test_s3_roundtrip on Windows py37_np16/Windows py38_np18,simonjayhawkins,closed,2020-08-24T08:31:34Z,2020-08-24T09:06:29Z,"https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=40833&view=logs&j=404760ec-14d3-5d48-e580-13034792878f&t=f81e4cc8-d61a-5fb8-36be-36768e5c561a&l=23

https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=40833&view=logs&j=4b4d380c-92b5-58cf-5b80-2c6c9b1d5805&t=8eab1181-1335-58ed-7ed5-a271d9d86005&l=23"
349932298,22305,pd.to_datetime() throws if caching is on with Null-like arguments,realead,closed,2018-08-13T08:25:40Z,2020-08-24T11:56:23Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
result = pd.to_datetime([pd.NaT, None], cache=True)
```
#### Problem description

It results in error:

> ...
> ~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance)
>    3242 
>    3243         if not self.is_unique:
> -> 3244             raise InvalidIndexError('Reindexing only valid with uniquely'
>    3245                                     ' valued Index objects')
>    3246 
> 
> InvalidIndexError: Reindexing only valid with uniquely valued Index objects
> 

#### Expected Output

The same as `result = pd.to_datetime([pd.NaT, None],cache=False)`:

`DatetimeIndex(['NaT', 'NaT'], dtype='datetime64[ns]', freq=None)`

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.2.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-53-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: 3.2.1
pip: 10.0.1
setuptools: 36.5.0.post20170921
Cython: 0.28.3
numpy: 1.13.1
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.1.0
sphinx: 1.6.3
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.2
feather: None
matplotlib: 2.0.2
openpyxl: 2.4.8
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 0.9.8
lxml: 3.8.0
bs4: 4.6.0
html5lib: 0.9999999
sqlalchemy: 1.1.13
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: 0.1.3
fastparquet: None
pandas_gbq: None
pandas_datareader: None
[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
675576953,35638,DOC: Mention NA for missing data in README,dsaxton,closed,2020-08-08T19:35:27Z,2020-08-24T12:52:01Z,
672768529,35543,REGR: Fix interpolation on empty dataframe,sanderland,closed,2020-08-04T12:51:33Z,2020-08-24T14:08:52Z,"Interpolation on an empty dataframe broke in 1.1 due to a change in how 'all columns are objects' is checked (specifically all(empty set) is True, while before dtype count object = None was checked against size = 0).
This is a complex function and I'm not sure what the proper fix is, suggesting to keep the empty check out of the rest of the logic.

Example code that broke:

```python
import pandas as pd
df = pd.DataFrame([1,2])
df[[]].interpolate(limit_area='inside')
```

- [x] closes #35598
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
684286520,35866,CLN/PERF: delay evaluation of get_day_of_month,jbrockmendel,closed,2020-08-24T00:27:50Z,2020-08-24T14:59:00Z,"I don't expect a major perf improvement, but in some corner cases we can avoid evaluating get_day_of_month"
646937182,35042,DOC: Updated aggregate docstring,gurukiran07,closed,2020-06-28T14:44:20Z,2020-08-24T15:36:18Z,"Link to current `Series.agg`: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.agg.html

Original question asked in gitter:
Does `pd.Series.agg` with `func` parameter set to `'median'` uses `np.nanmedian`(Not only `median` including `mean`, `mode`)?
```python
s= pd.Series([np.nan, np.nan,1,1,1])
s.agg('median') 
# 1
s.agg(np.median)
# 1
np.median(s.to_numpy())
# nan
np.nanmedian(s.to_numpy())
# 1
```
Whenever `NaN` is present does it fallback to using `np.nanmedian`?

---

Reply from @MarcoGorelli 
if you look at `pandas/core/base.py` you'll see
```
    np.median: ""median"",
    np.nanmedian: ""median"",
```
in `_cython_table`. So, both resolve to the same internal cython function.

---

IMO it's better to mention this in the docs under `Note:` section.

Under note section saying:
*Some NumPy functions such as `np.mean`, `np.nanmean`, `np.median` etc. resolve to their corresponding internal cython function.*


#### Output of `python validate_docstrings.py pandas.Series.agg`
<details>

```
################################################################################
######################## Docstring (pandas.Series.agg)  ########################
################################################################################

Aggregate using one or more operations over the specified axis.

.. versionadded:: 0.20.0

Parameters
----------
func : function, str, list or dict
    Function to use for aggregating the data. If a function, must either
    work when passed a Series or when passed to Series.apply.

    Accepted combinations are:

    - function
    - string function name
    - list of functions and/or function names, e.g. ``[np.sum, 'mean']``
    - dict of axis labels -> functions, function names or list of such.
axis : {0 or 'index'}
        Parameter needed for compatibility with DataFrame.
*args
    Positional arguments to pass to `func`.
**kwargs
    Keyword arguments to pass to `func`.

Returns
-------
scalar, Series or DataFrame

    The return can be:

    * scalar : when Series.agg is called with single function
    * Series : when DataFrame.agg is called with a single function
    * DataFrame : when DataFrame.agg is called with several functions

    Return scalar, Series or DataFrame.

See Also
--------
Series.apply : Invoke function on a Series.
Series.transform : Transform function producing a Series with like indexes.

Notes
-----
`agg` is an alias for `aggregate`. Use the alias.
Some NumPy functions such as `np.mean`, `np.nanmean`, `np.median` etc.
resolve to their corresponding internal cython function.

A passed user-defined-function will be passed a Series for evaluation.

Examples
--------
>>> s = pd.Series([1, 2, 3, 4])
>>> s
0    1
1    2
2    3
3    4
dtype: int64

>>> s.agg('min')
1

>>> s.agg(['min', 'max'])
min   1
max   4
dtype: int64

################################################################################
################################## Validation ##################################
################################################################################

Docstring for ""pandas.Series.agg"" correct. :)
```
</details>"
657597800,35294,Various 32-bit failures at MacPython/pandas-wheels,TomAugspurger,closed,2020-07-15T19:30:42Z,2020-08-24T23:42:02Z,"https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=39280&view=logs&j=a846d25a-e32c-5640-1b53-e815fab94407&t=8fc25549-8157-5c9c-f998-0884bd2e8ccb&l=3099

```
FAILED test_venv/lib/site-packages/pandas/tests/window/test_api.py::test_multiple_agg_funcs[rolling-2-expected_vals0]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_apply.py::test_rolling_apply_args_kwargs[args_kwargs0]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_apply.py::test_rolling_apply_args_kwargs[args_kwargs1]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_grouper.py::TestGrouperGrouping::test_groupby_rolling[1.0-True]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_grouper.py::TestGrouperGrouping::test_groupby_rolling[0.0-False]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_grouper.py::TestGrouperGrouping::test_getitem
FAILED test_venv/lib/site-packages/pandas/tests/window/test_grouper.py::TestGrouperGrouping::test_rolling
FAILED test_venv/lib/site-packages/pandas/tests/window/test_grouper.py::TestGrouperGrouping::test_rolling_quantile[linear]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_grouper.py::TestGrouperGrouping::test_rolling_quantile[lower]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_grouper.py::TestGrouperGrouping::test_rolling_quantile[higher]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_grouper.py::TestGrouperGrouping::test_rolling_quantile[midpoint]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_grouper.py::TestGrouperGrouping::test_rolling_quantile[nearest]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_grouper.py::TestGrouperGrouping::test_rolling_apply[True]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_grouper.py::TestGrouperGrouping::test_rolling_apply[False]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_grouper.py::TestGrouperGrouping::test_rolling_apply_mutability
FAILED test_venv/lib/site-packages/pandas/tests/window/test_rolling.py::test_closed_one_entry_groupby[min]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_rolling.py::test_closed_one_entry_groupby[max]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_rolling.py::test_rolling_positional_argument[True-grouping0-_index0]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_rolling.py::test_rolling_positional_argument[True-grouping1-_index1]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_rolling.py::test_rolling_positional_argument[False-grouping0-_index0]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_rolling.py::test_rolling_positional_argument[False-grouping1-_index1]
FAILED test_venv/lib/site-packages/pandas/tests/window/test_timeseries_window.py::TestRollingTS::test_groupby_monotonic
FAILED test_venv/lib/site-packages/pandas/tests/window/test_timeseries_window.py::TestRollingTS::test_non_monotonic
```

Just going to skip these for now. Maybe (probably?) some overlap with #35148 @mroeschke. Not sure."
651727893,35148,CI: MacPython failing test_consistency_with_window,TomAugspurger,closed,2020-07-06T18:09:41Z,2020-08-24T23:42:02Z,"https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=38672&view=logs&j=c0130b29-789d-5a3c-6978-10796a508a7f&t=e120bc6c-1f5e-5a41-8f0a-1d992cd2fbfb&l=1286

```python
    def test_consistency_with_window():
    
        # consistent return values with window
        df = test_frame
                index_array = self.index_array
            indexer = self.rolling_indexer(
                index_array=index_array, window_size=self.window_size,
            )
            start, end = indexer.get_window_bounds(
                len(indicies), min_periods, center, closed
            )
            start = start.astype(np.int64)
            end = end.astype(np.int64)
            # Cannot use groupby_indicies as they might not be monotonic with the object
            # we're rolling over
            window_indicies = np.arange(
                window_indicies_start, window_indicies_start + len(indicies),
            )
            window_indicies_start += len(indicies)
            # Extend as we'll be slicing window like [start, end)
            window_indicies = np.append(
                window_indicies, [window_indicies[-1] + 1]
            ).astype(np.int64)
>           start_arrays.append(window_indicies.take(start))
E           TypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'

test_venv\lib\site-packages\pandas\core\window\indexers.py:318: TypeError
```

cc @mroeschke if you know where to look / who to ping. I haven't looked yet."
684877349,35876,groupby AssertionError with datetime column name,hliatrussellinvestments,closed,2020-08-24T18:29:56Z,2020-08-24T23:45:46Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd
import datetime

column_names = ['dimension_1', 'dt', 'value']
data = [
        ('D1', datetime.date(2020, 1, 1), 2.1),
        ('D1', datetime.date(2020, 1, 2), 4.1),
        ('D1', datetime.date(2020, 1, 3), 1.7),
    ]
df_stack = pd.DataFrame(data=data, columns=column_names)
df_stack['dt'] = pd.to_datetime(df_stack['dt'])
df_stack.set_index(['dimension_1', 'dt'], inplace=True)

df_pivot = df_stack.unstack(level=['dt']).transpose().droplevel(None).transpose()

agg_map = {c: 'sum' for c in df_pivot.columns}
df_pivot.groupby(level=['dimension_1']).agg(agg_map)
```

#### Problem description

When executing groupby (on index) for dataframe whose column-names are of type datetime ('datetime64[ns]'), the groupby fails with AssertionError (result.name == res_name). 

<details>
AssertionError                            Traceback (most recent call last)
<ipython-input-3-2655c5698d24> in <module>
      1 # do groupby on index
      2 agg_map = {c: 'sum' for c in df_pivot.columns}
----> 3 df_pivot.groupby(level=['dimension_1']).agg(agg_map)

c:\users\hli\appdata\local\programs\python\python38-32\lib\site-packages\pandas\core\groupby\generic.py in aggregate(self, func, engine, engine_kwargs, *args, **kwargs)
    947             )
    948 
--> 949         result, how = self._aggregate(func, *args, **kwargs)
    950         if how is None:
    951             return result

c:\users\hli\appdata\local\programs\python\python38-32\lib\site-packages\pandas\core\base.py in _aggregate(self, arg, *args, **kwargs)
    349                 keys = list(arg.keys())
    350                 if isinstance(obj, ABCDataFrame) and len(
--> 351                     obj.columns.intersection(keys)
    352                 ) != len(keys):
    353                     cols = sorted(set(keys) - set(obj.columns.intersection(keys)))

c:\users\hli\appdata\local\programs\python\python38-32\lib\site-packages\pandas\core\indexes\datetimelike.py in intersection(self, other, sort)
    701                     # TODO: no tests rely on this; needed?
    702                     result = result._with_freq(""infer"")
--> 703             assert result.name == res_name
    704             return result
    705 

</details>

#### Expected Output

The expected result in this example should be the dataframe itself, since it only has one row. This can be seen by changing the column data type to string:
```python
df_pivot.columns = df_pivot.columns.astype(str)
agg_map = {c: 'sum' for c in df_pivot.columns}
df_pivot.groupby(level=['dimension_1']).agg(agg_map)
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.5.final.0
python-bits      : 32
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : English_United States.1252

pandas           : 1.1.1
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 47.1.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
684982945,35877,REGR: DatetimeIndex.intersection incorrectly raising AssertionError,jbrockmendel,closed,2020-08-24T21:39:04Z,2020-08-25T00:06:31Z,"- [x] closes #35876
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

@dsaxton this PR's test and whatsnew note reference the underlying issue you identified in #35876, but not the OP issue.  Do you think we should add a test/note for that directly?"
684323566,35867,REF: implement Block.reduce for DataFrame._reduce,jbrockmendel,closed,2020-08-24T02:57:40Z,2020-08-25T00:08:39Z,"This lets us avoid reconstructing results from a dict, makes it feasible to use the same block-skipping code for DataFrame._reduce that we use for cython_agg_blocks and apply_blockwise."
684024526,35854,REF: use Block.apply in cython_agg_blocks,jbrockmendel,closed,2020-08-22T16:26:36Z,2020-08-25T00:17:08Z,Getting closer to making this use a BlockManager method
684108765,35861,REF: make window _apply_blockwise actually blockwise,jbrockmendel,closed,2020-08-23T03:43:55Z,2020-08-25T00:23:27Z,"There will be one more clean-up pass after this, kept separate to maintain targeted diff."
685055188,35880,QST: Date Overlapping Ranges,ghost,closed,2020-08-24T23:47:24Z,2020-08-25T01:36:31Z,"I've been trying to learn how to use Pandas, but I'm thoroughly confused about where in the API to find methods that can aggregate data conditionally based on sign across date ranges. I have a data frame like so:

```
Date        Change 
2010-08-25    0.08
2010-08-26   -0.22
2010-08-27    0.04
2010-08-30   -0.08
2010-08-31   -0.11
...            ...
2020-08-18    0.96
2020-08-19   -1.79
2020-08-20    5.04
2020-08-21   -0.84
2020-08-24   -1.10
```
The Date column is an index of course. What I want to do is basically partition this data by year. Once partitioned by year, I want to group consecutive rows by the sign of the change column such that consecutive negatives and consecutive positives are grouped together. Once that is done, I want to get the overlap of date ranges with matching sign for all years. For example, if change is positive from 2010-08-25 to 2010-08-27 and from 2011-08-26 to 2011-08-29 the common overlap would be 08-26 to 08-27, obviously accounting for all years not just 2. At that point, once I have the common date ranges and their values I want to average all of the numbers in that range such that at the end I have ranges of dates for which change is consistently positive or negative and the average change for each range. How can I achieve this?
"
685054802,35879,Backport PR #35877 on branch 1.1.x (REGR: DatetimeIndex.intersection incorrectly raising AssertionError),meeseeksmachine,closed,2020-08-24T23:46:22Z,2020-08-25T06:27:56Z,Backport PR #35877: REGR: DatetimeIndex.intersection incorrectly raising AssertionError
330265753,21353,Memory leak in pd.read_csv or DataFrame,kuraga,closed,2018-06-07T13:11:43Z,2020-08-25T07:01:56Z,"#### Code Sample, a copy-pastable example if possible

```python
import sys

m = int(sys.argv[1])
n = int(sys.argv[2])

with open('df.csv', 'wt') as f:
    for i in range(n-1):
        f.write('c' + str(i) + ',')
    f.write('c' + str(n-1) + '\n')
    for j in range(m):
        for i in range(n-1):
            f.write('1,')
        f.write('1\n')


import psutil

print(psutil.Process().memory_info().rss / 1024**2)

import pandas as pd
df = pd.read_csv('df.csv')

print(df.shape)
print(psutil.Process().memory_info().rss / 1024**2)

import gc
del df
gc.collect()

print(psutil.Process().memory_info().rss / 1024**2)
```

#### Problem description

```
$ ~/miniconda3/bin/python3 g.py 1 1
11.60546875
(1, 1)
64.02734375
64.02734375

$ ~/miniconda3/bin/python3 g.py 5000000 15
11.58203125
(5000000, 15)
640.45703125
68.25

$ ~/miniconda3/bin/python3 g.py 5000000 20
11.84375
(5000000, 20)
1586.65625
823.71875 - !!!

$ ~/miniconda3/bin/python3 g.py 10000000 10
11.83984375
(10000000, 10)
830.92578125
67.984375

$ ~/miniconda3/bin/python3 g.py 10000000 15
11.89453125
(10000000, 15)
2344.3046875
1199.89453125 - !!!
```

Two issues:
1) There is a ""standard"" leak after reading any CSV OR just creating by `pd.DataFrame()` - ~53Mb.
2) We see a large leak in some other cases.

cc  @gfyoung 

#### Output of ``pd.show_versions()``

(same for 0.21, 0.22, 0.23)

<details>
pandas: 0.23.0
pytest: None
pip: 9.0.3
setuptools: 39.0.1
Cython: None
numpy: 1.14.3
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.4.0
sphinx: None
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.2.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.9999999
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>"
682283449,35814,TST: Fix test_parquet failures for pyarrow 1.0,alimcmaster1,closed,2020-08-20T01:31:11Z,2020-08-25T07:08:14Z,"- [x] closes https://github.com/pandas-dev/pandas/issues/35791
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

cc @martindurant -> looks like this is a pyarrow 1.0.0 compat issue (read_table uses the new API) - https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html

I noticed the partition cols are casted from int64 -> int32 is that expected pyarrow behaviour? From the write_table docs looking at version 1.0/2.0 it suggests it is https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html#pyarrow.parquet.write_table 
"
681261019,35791,BUG: failing test for pyarrow/s3fs on windows,martindurant,closed,2020-08-18T18:56:39Z,2020-08-25T09:47:48Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```
FAILED pandas/tests/io/test_parquet.py::TestParquetPyArrow::test_s3_roundtrip_for_dir[partition_col0]
```

#### Problem description

See build https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=40576&view=logs&jobId=404760ec-14d3-5d48-e580-13034792878f&j=404760ec-14d3-5d48-e580-13034792878f&t=f81e4cc8-d61a-5fb8-36be-36768e5c561a

env:
```
pyarrow                   1.0.0
botocore                  1.17.44 
fsspec                    0.8.0 
moto                      1.3.14 
s3fs                      0.4.2 
```

I do not know the cause of this problem, but it appears in https://github.com/pandas-dev/pandas/pull/35655 and also for the original code, so skipping it on win for now."
684540209,35870,DOC: pandas.Series.transform,onshek,closed,2020-08-24T10:08:04Z,2020-08-25T10:36:09Z,"#### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.Series.transform.html?highlight=pandas%20series%20transform#pandas.Series.transform

**Note**: You can check the latest versions of the docs on `master` [here](https://pandas.pydata.org/docs/dev/).

#### Documentation problem

> list of functions and/or function names, e.g. [np.exp. 'sqrt']


#### Suggested fix for documentation

list of functions and/or function names, e.g. [np.exp, 'sqrt']"
669616546,35489,RLS: 1.1.1,simonjayhawkins,closed,2020-07-31T09:31:19Z,2020-08-25T11:21:57Z,"Tracking issue for the 1.1.1 release.

https://github.com/pandas-dev/pandas/milestone/75

Please do not remove/change milestones from these issues without a note explaining the reasoning (changing milestones doesn't trigger notification) 

All issues raised that are regressions from 1.0.5 should be milestoned 1.1.1. Post 1.1.1 release, unclosed issues from this list can be changed to 1.1.2 to maintain the tracking or reviewed."
586238634,32930,pd.read_csv()/dtype and index_col combo,sbwiecko,closed,2020-03-23T14:10:16Z,2020-08-25T12:43:48Z,"#### Code Sample, a copy-pastable example if possible

```python
data=pd.read_csv(file, dtype={'donor':str}, index_col='donor') # dtype of the index IS NOT str
# to fix:
data= pd.read_csv(file, dtype={'donor': str})
data.set_index('donor', drop=True, inplace=True)
```

#### Problem description

during pd.read_csv(), when I set a dtype to one column and set the same column as index, the index dtype is lost

#### Expected Output
When both dtype and index_col are used with the same column, I want the index in dtype specificied in the pd.read_csv() call, i.e. in my example str

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 69 Stepping 1, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : None
pytest           : 5.0.1
hypothesis       : 4.27.0
sphinx           : 2.1.2
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.0.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : None
tabulate         : None
xarray           : 0.12.3
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
682521052,35818,"BUG: Dataframe.apply(func, axis=1) when func returns a Series, apply replaces all the rows in the Dataframe for the result of func(first_row) ",0sewa0,closed,2020-08-20T08:20:37Z,2020-08-25T12:54:42Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
import pandas as pd
df = pd.DataFrame([[4, 9], [1, 2], [3, -1]], columns=['A', 'B'])
def add_columns(series):
    series[""C""] = series.A * series.B
    series[""D""] = series.A + series.B
    return series
df.apply(add_columns, axis=1)
```

#### Problem description
In pandas '1.1.0' I encountered an issue when using `Dataframe.apply(func, axis=1)` to dynamically insert new columns into a dataframe. After calling `Dataframe.apply(func, axis=1)`, where `func` returns a `Series` (as shown in the Code Sample), every row in the will be a copy of the first row with the new columns added.

So the output of the Sample Code:
   A  B   C   D
0  4  9  36  13
1  4  9  36  13
2  4  9  36  13

#### Expected Output
Each row shouldn't be the same as the first row.
(Code Sample using pandas-1.0.5)
   A  B   C   D
0  4  9  36  13
1  1  2   2   3
2  3 -1  -3   2

#### Output of ``pd.show_versions()``

<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.7.10-201.fc32.x86_64
Version          : #1 SMP Thu Jul 23 00:58:39 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.6.0
Cython           : None
pytest           : 6.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.0
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
684794881,35874,BUG: to_dict_of_blocks failing to invalidate item_cache,jbrockmendel,closed,2020-08-24T16:15:02Z,2020-08-25T15:13:21Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
685144252,35884,REF: reuse _combine instead of reset_dropped_locs,jbrockmendel,closed,2020-08-25T03:40:01Z,2020-08-25T15:14:10Z,
685234908,35887,Backport PR #35814: TST: Fix test_parquet failures for pyarrow 1.0,jorisvandenbossche,closed,2020-08-25T07:07:52Z,2020-08-25T15:18:11Z,Backport https://github.com/pandas-dev/pandas/pull/35814
685537102,35892,DOC: pandas.Series.filter talks about dataframe not series,notestaff,closed,2020-08-25T14:32:51Z,2020-08-25T16:22:46Z,"#### Location of the documentation
https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.Series.filter.html

[this should provide the location of the documentation, e.g. ""pandas.read_csv"" or the URL of the documentation, e.g. ""https://dev.pandas.io/docs/reference/api/pandas.read_csv.html""]

**Note**: You can check the latest versions of the docs on `master` [here](https://pandas.pydata.org/docs/dev/).

#### Documentation problem

Documentation talks about dataframe, not series
[this should provide a description of what documentation you believe needs to be fixed/improved]

#### Suggested fix for documentation

[this should explain the suggested fix and **why** it's better than the existing documentation]
Replace with the correct documentation for Series.  "
685592940,35896,BUG: DataFrame([]).squeeze() returns DataFrame instead of Series,jbrockmendel,closed,2020-08-25T15:42:50Z,2020-08-25T18:24:19Z,
685545941,35894,DOC: avoid StorageOptions type alias in docstrings,jorisvandenbossche,closed,2020-08-25T14:42:06Z,2020-08-25T20:12:35Z,"Small follow-up on https://github.com/pandas-dev/pandas/pull/35655, replacing the ""StorageOptions"" with a plain ""dict"" in the docstrings (""StorageOptions"" is not something known to users, in the type annotations it will expand but not in the docstrings)

(cc @martindurant)"
684038427,35856,CI: specified bucket does not exist in TestParquetPyArrow.test_s3_roundtrip_explicit_fs,charlesdong1991,closed,2020-08-22T18:01:57Z,2020-08-26T02:21:41Z,"might be related to #35655

```
================================== FAILURES ===================================
______________ TestParquetPyArrow.test_s3_roundtrip_explicit_fs _______________
[gw2] win32 -- Python 3.8.5 C:\Miniconda\envs\pandas-dev\python.exe

self = <pandas.tests.io.test_parquet.TestParquetPyArrow object at 0x000002B24268BF10>
df_compat =    A    B
0  1  foo
1  2  foo
2  3  foo
s3_resource = s3.ServiceResource(), pa = 'pyarrow'
s3so = {'client_kwargs': {'endpoint_url': 'http://127.0.0.1:5555/'}}

    def test_s3_roundtrip_explicit_fs(self, df_compat, s3_resource, pa, s3so):
        s3fs = pytest.importorskip(""s3fs"")
        if LooseVersion(pyarrow.__version__) <= LooseVersion(""0.17.0""):
            pytest.skip()
        s3 = s3fs.S3FileSystem(**s3so)
        kw = dict(filesystem=s3)
>       check_round_trip(
            df_compat,
            pa,
            path=""pandas-test/pyarrow.parquet"",
            read_kwargs=kw,
            write_kwargs=kw,
        )

        service_id = self._service_model.service_id.hyphenize()
        handler, event_response = self.meta.events.emit_until_response(
            'before-call.{service_id}.{operation_name}'.format(
                service_id=service_id,
                operation_name=operation_name),
            model=operation_model, params=request_dict,
            request_signer=self._request_signer, context=request_context)
    
        if event_response is not None:
            http, parsed_response = event_response
        else:
            http, parsed_response = self._make_request(
                operation_model, request_dict, request_context)
    
        self.meta.events.emit(
            'after-call.{service_id}.{operation_name}'.format(
                service_id=service_id,
                operation_name=operation_name),
            http_response=http, parsed=parsed_response,
            model=operation_model, context=request_context
        )
    
        if http.status_code >= 300:
            error_code = parsed_response.get(""Error"", {}).get(""Code"")
            error_class = self.exceptions.from_code(error_code)
>           raise error_class(parsed_response, operation_name)
E           botocore.errorfactory.NoSuchBucket: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist
```
"
685333577,35890,"BUG: ValueError: Of the four parameters: start, end, periods, and freq, exactly three must be specified",malapradej,closed,2020-08-25T09:39:50Z,2020-08-26T08:23:10Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here

```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
686085156,35903,Datetime error,ayushsom96,closed,2020-08-26T07:34:02Z,2020-08-26T12:27:12Z,"### Can you please let me know how I can correct the following error
**CODE:**
tx_data['InvoiceDate'] = pd.to_datetime(tx_data['InvoiceDate'])
tx_uk = tx_data.query(""Country=='United Kingdom'"").reset_index(drop=True)

#create 3m and 6m dataframes
tx_3m = tx_uk[(tx_uk.InvoiceDate < date(2011,6,1)) & (tx_uk.InvoiceDate >= date(2011,3,1))].reset_index(drop=True)
tx_6m = tx_uk[(tx_uk.InvoiceDate >= date(2011,6,1)) & (tx_uk.InvoiceDate < date(2011,12,1))].reset_index(drop=True)

**ISSUE:**
 32     else:
     33         typ = type(right).__name__
---> 34         raise TypeError(f""Invalid comparison between dtype={left.dtype} and {typ}"")
     35     return res_values
     36

**TypeError: Invalid comparison between dtype=datetime64[ns] and date**"
675987834,35650,BUG: pd.factorize with read-only datetime64 numpy array raises ValueError,ruchirgarg05,closed,2020-08-10T09:14:21Z,2020-08-26T12:44:25Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python

In [1]: pandas.__version__
[PYFLYBY] import pandas
Out[1]: u'0.24.2'

In [2]: arr = numpy.array([numpy.datetime64('2015-11-20T15:06:58.000')])

In [3]: arr.dtype
Out[3]: dtype('<M8[ms]')

In [4]: arr.flags.writeable = False

[PYFLYBY] import pandas as pd
In [5]: pd.factorize(arr)
```

#### Problem description

[Construction with non-mutable datetime64 strings]

#### Expected Output
(array([0]), array(['2015-11-20T15:06:58.000000000'], dtype='datetime64[ns]'))

#### Output of ``pd.show_versions()``

<details>
pandas/_libs/tslibs/conversion.pyx in pandas._libs.tslibs.conversion.ensure_datetime64ns()

/usr/local/python/python-2.7/std/lib/python2.7/site-packages/pandas/_libs/tslibs/conversion.so in View.MemoryView.memoryview_cwrapper()

/usr/local/python/python-2.7/std/lib/python2.7/site-packages/pandas/_libs/tslibs/conversion.so in View.MemoryView.memoryview.__cinit__()

ValueError: buffer source array is read-only
</details>
"
680560180,35775,TST: Verify whether read-only datetime64 array can be factorized (35650),avinashpancham,closed,2020-08-17T22:21:28Z,2020-08-26T12:44:51Z,"- [x] closes #35650
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
581340794,32708,skip 32 bit linux,WillAyd,closed,2020-03-14T19:32:57Z,2020-08-26T13:19:15Z,follow up to #32706
678655211,35712,PERF: RangeIndex.format performance,topper-123,closed,2020-08-13T18:39:42Z,2020-08-26T13:29:51Z,"#35440 dropped ``RangeIndex._format_with_header``, which was functionally not needed, but needed to avoid creating an internal ndarray.

This rectifies that + gives some perf. improvements.

```python
>>> idx = pd.RangeIndex(1_000_000)
>>> %timeit idx.format()
4.6 s ± 102 ms per loop  # pandas v1.1.0
1.67 s ± 19.6 ms per loop  # master
595 ms ± 2.35 ms per loop  # this PR
```

Also, now the ``_data`` attribute isn't called, so this PR gives a perf. & memory improvement in some use cases compared to master:

```python
>>> idx = pd.RangeIndex(1_000_000)
>>> idx.format()
>>> ""_data"" in idx._cache
False # pandas v.1.1.0
True  # master
False  # this PR
```
"
686249006,35905,Backport PR #35777: BUG: DataFrame.apply with result_type=reduce incorrect index,simonjayhawkins,closed,2020-08-26T11:39:27Z,2020-08-26T13:32:36Z,#35777
635693308,34677,BUG: ASV Benchmarks failing to build,matteosantama,closed,2020-06-09T19:13:53Z,2020-08-26T13:57:41Z,"ASV benchmarks fail to build on master. Could this be related to #34666? 

`error: command 'gcc' failed with exit status 1`

Anyone else having this issue?"
686244872,35904,Backport PR #35712: PERF: RangeIndex.format performance,simonjayhawkins,closed,2020-08-26T11:32:06Z,2020-08-26T14:40:27Z,#35712
686437660,35909,BUG: sorting dataframe increases memory usage,tacitvenom,closed,2020-08-26T16:00:39Z,2020-08-26T16:29:07Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python

import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randint(0,4000,size=(1000, 4)), columns=list('ABCD'))
print(df.memory_usage(index=False).sum())
df = df.sort_values(['A', 'B']).reset_index(drop=True)
print(df.memory_usage(index=False).sum())

```

Output:
32000
32000

#### Problem description

The dataframe is essentially same rows and same dtypes, just sorted. Why should the sorted df be bigger?
I understand that during sorting, a copy is created so the memory consumption increases but once sorting is finished, that should be back to where it was. Here, the memory usage of the dataframe increased from 31KB to 40 KB for this toy example.

#### Expected Output
The memory usage of sorted pandas dataframe should be similar to what it was before sorting.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.6.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.76-linuxkit
Version          : #1 SMP Tue May 26 11:42:35 UTC 2020
machine          : x86_64
processor        :
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0
Cython           : None
pytest           : 6.0.0
hypothesis       : 5.16.1
sphinx           : 1.6.7
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.7.4
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.18
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
681504203,35798,Bump asv Python version,dsaxton,closed,2020-08-19T02:45:00Z,2020-08-26T17:06:12Z,"Got this error trying to run the asvs locally and bumping the Python version in asv.conf.json fixed it:

```
   STDOUT -------->
   Processing /Users/danielsaxton/pandas/asv_bench/env/a36d556a9d1611aba4092dc48036d261/project
       Preparing wheel metadata: started
       Preparing wheel metadata: finished with status 'done'
   STDERR -------->
   ERROR: Package 'pandas' requires a different Python: 3.6.10 not in '>=3.7.1'
```

Let me know if this is needed / correct in general or if it was just a thing with my machine.

cc @TomAugspurger 

Closes https://github.com/pandas-dev/pandas/issues/34677"
319952558,20944,Drop rows based on condition,sursu,closed,2018-05-03T14:46:18Z,2020-08-26T19:41:13Z,"Here are 2 ways to drop rows from a pandas data-frame based on a condition:

1. `df = df[condition]`

2. `df.drop(df[condition].index, axis=0, inplace=True)`

The first one does not do it *inplace*, right?

The second one does not work as expected when the index is not unique, so the user would need to `reset_index()` then `set_index()` back. 


**Question**
Would it be possible to have column dropping based directly on the condition?
e.g.
`df.drop(condition, axis=0, inplace=True)`
"
680074629,35762,BUG: Sparse[datetime64[ns]] TypeError: data type not understood,sbrugman,closed,2020-08-17T09:09:57Z,2020-08-27T02:22:02Z,"- [X] I have checked that this issue has not already been reported ([related, but different](https://github.com/pandas-dev/pandas/issues/26407)).

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import numpy as np
import pandas as pd
# Taken from the example here: https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#sparsedtype
dtype = pd.SparseDtype(np.dtype('datetime64[ns]'))

# Some value
values = [np.datetime64('2012-05-01T01:00:00.000000'), np.datetime64('2016-05-01T01:00:00.000000')]

# Create the series
series = pd.Series(values, dtype=dtype)
```
Alternatively:
```
series = pd.Series(values, dtype=""Sparse[datetime64[ns]]"")
```

#### Problem description

As a user I would expect  that `datetime64[ns]` is supported as SparseDtype for the SparseArray based on the [_Sparse data structures_ page](https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#sparsedtype) in the documentation.  This is desireable for the same rationale as supporting other sparse types, since date(time)s can also contain mostly NaT values that the users wants to store efficiently.

Running the code above yields the following TypeError:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-52-9fc6a02721e3> in <module>
----> 1 series = pd.Series(values, dtype=dtype)

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\series.py in __init__(self, data, index, dtype, name, copy, fastpath)
    325                     data = data.copy()
    326             else:
--> 327                 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True)
    328 
    329                 data = SingleBlockManager.from_array(data, index)

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure)
    439     elif isinstance(data, (list, tuple)) and len(data) > 0:
    440         if dtype is not None:
--> 441             subarr = _try_cast(data, dtype, copy, raise_cast_failure)
    442         else:
    443             subarr = maybe_convert_platform(data)

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\construction.py in _try_cast(arr, dtype, copy, raise_cast_failure)
    551             subarr = arr
    552         else:
--> 553             subarr = maybe_cast_to_datetime(arr, dtype)
    554 
    555         # Take care in creating object arrays (but iterators are not

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\dtypes\cast.py in maybe_cast_to_datetime(value, dtype, errors)
   1327                 # pandas supports dtype whose granularity is less than [ns]
   1328                 # e.g., [ps], [fs], [as]
-> 1329                 if dtype <= np.dtype(""M8[ns]""):
   1330                     if dtype.name == ""datetime64"":
   1331                         raise ValueError(msg)

TypeError: data type not understood
```

#### Expected Output

The expected output is for `Sparse[datetime[ns]]` similar to what one would expect from other dtypes:

```
>>> series.values
[2012-05-01 01:00:00, 2018-05-01 01:00:00]
 Fill: NaT
 IntIndex
 Indices: array([0, 1])

>>> series.dtype
Sparse[datetime64[ns], NaT]
```

#### Workaround

Note that the following workaround partially gives the expected results:
```python
series = pd.Series(pd.arrays.SparseArray(values))
```

For which these operations work as expected:

```
>>> series.values
[2012-05-01 01:00:00, 2018-05-01 01:00:00]
 Fill: NaT
 IntIndex
 Indices: array([0, 1])

>>> series.dtype
Sparse[datetime64[ns], NaT]
```

However `series.head()` will throw an error:

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\formatters.py in __call__(self, obj)
    700                 type_pprinters=self.type_printers,
    701                 deferred_pprinters=self.deferred_printers)
--> 702             printer.pretty(obj)
    703             printer.flush()
    704             return stream.getvalue()

C:\ProgramData\Anaconda3\lib\site-packages\IPython\lib\pretty.py in pretty(self, obj)
    400                         if cls is not object \
    401                                 and callable(cls.__dict__.get('__repr__')):
--> 402                             return _repr_pprint(obj, self, cycle)
    403 
    404             return _default_pprint(obj, self, cycle)

C:\ProgramData\Anaconda3\lib\site-packages\IPython\lib\pretty.py in _repr_pprint(obj, p, cycle)
    695     """"""A pprint that just redirects to the normal repr function.""""""
    696     # Find newlines and replace them with p.break_()
--> 697     output = repr(obj)
    698     for idx,output_line in enumerate(output.splitlines()):
    699         if idx:

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\series.py in __repr__(self)
   1319             min_rows=min_rows,
   1320             max_rows=max_rows,
-> 1321             length=show_dimensions,
   1322         )
   1323         result = buf.getvalue()

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\series.py in to_string(self, buf, na_rep, float_format, header, index, length, dtype, name, max_rows, min_rows)
   1384             max_rows=max_rows,
   1385         )
-> 1386         result = formatter.to_string()
   1387 
   1388         # catch contract violations

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\formats\format.py in to_string(self)
    356 
    357         fmt_index, have_header = self._get_formatted_index()
--> 358         fmt_values = self._get_formatted_values()
    359 
    360         if self.truncate_v:

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\formats\format.py in _get_formatted_values(self)
    345             None,
    346             float_format=self.float_format,
--> 347             na_rep=self.na_rep,
    348         )
    349 

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\formats\format.py in format_array(values, formatter, float_format, na_rep, digits, space, justify, decimal, leading_space, quoting)
   1177     )
   1178 
-> 1179     return fmt_obj.get_result()
   1180 
   1181 

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\formats\format.py in get_result(self)
   1208 
   1209     def get_result(self) -> List[str]:
-> 1210         fmt_values = self._format_strings()
   1211         return _make_fixed_width(fmt_values, self.justify)
   1212 

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\formats\format.py in _format_strings(self)
   1465 
   1466         if not isinstance(values, DatetimeIndex):
-> 1467             values = DatetimeIndex(values)
   1468 
   1469         if self.formatter is not None and callable(self.formatter):

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexes\datetimes.py in __new__(cls, data, freq, tz, normalize, closed, ambiguous, dayfirst, yearfirst, dtype, copy, name)
    277             dayfirst=dayfirst,
    278             yearfirst=yearfirst,
--> 279             ambiguous=ambiguous,
    280         )
    281 

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\arrays\datetimes.py in _from_sequence(cls, data, dtype, copy, tz, freq, dayfirst, yearfirst, ambiguous)
    321             dayfirst=dayfirst,
    322             yearfirst=yearfirst,
--> 323             ambiguous=ambiguous,
    324         )
    325 

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\arrays\datetimes.py in sequence_to_dt64ns(data, dtype, copy, tz, dayfirst, yearfirst, ambiguous)
   1957     if is_datetime64tz_dtype(data_dtype):
   1958         # DatetimeArray -> ndarray
-> 1959         tz = _maybe_infer_tz(tz, data.tz)
   1960         result = data._data
   1961 

AttributeError: 'SparseArray' object has no attribute 'tz'
```

Expected:

```
0   2012-05-01 01:00:00
1   2018-05-01 01:00:00
dtype: Sparse[datetime64[ns]]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.4.1
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.2
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : 1.2.1
fsspec           : 0.5.2
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : 0.14.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.10
tables           : 3.6.0
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.45.1

</details>"
667422468,35440,BUG: CategoricalIndex.format,topper-123,closed,2020-07-28T22:28:20Z,2020-08-04T09:55:01Z,"- [x] closes #35439
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I've temporarily put the whatsnewentry in the v.1.1.0 release note, because there isn't a v.1.1.1 version yet. I'll move it, before this is merged."
671232722,35510,REGR: Check for float in isnaobj_old,dsaxton,closed,2020-08-01T21:55:55Z,2020-08-04T09:55:53Z,"- [x] closes #35493
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Looks like we lost an isinstance check that caused this regression."
666804755,35432,CLN/PERF: move RangeIndex._cached_data to RangeIndex._cache,topper-123,closed,2020-07-28T06:27:36Z,2020-08-04T10:20:54Z,"The ``._cached_data`` attribute is not necessary. It was originally added to allow a check to see if the ``._data`` ndarray had been created, but that's also possible to do by a ``""_data"" in _cache`` check in the new implemention, which IMO would be more idiomatic.

The new implementation has the benefit that the ``_data`` will be available to new copies of a RangeIndex, saving the need to create a new ndarray for each new copy of the RangeIndex.

```python
>>> idx = pd.RangeIndex(1_000_000)
>>> idx[[1, 4]]  # this accesses ._data and saves it in cached_data (master) or _cache[""_data""](this PR)
>> %timeit idx._shallow_copy()[[1, 4]]
2.55 ms ± 69.3 µs per loop  # master
17.7 µs ± 405 ns per loop  # this PR
```

xref #26565.
"
672664455,35539,Backport PR #35440 on branch 1.1.x (BUG: CategoricalIndex.format),meeseeksmachine,closed,2020-08-04T09:54:49Z,2020-08-04T10:38:00Z,Backport PR #35440: BUG: CategoricalIndex.format
672664977,35540,Backport PR #35510 on branch 1.1.x (REGR: Check for float in isnaobj_old),meeseeksmachine,closed,2020-08-04T09:55:42Z,2020-08-04T10:56:33Z,Backport PR #35510: REGR: Check for float in isnaobj_old
306187291,20395,PERF: df.loc is 100x slower for CategoricalIndex than for normal Index,topper-123,closed,2018-03-17T20:19:37Z,2020-08-04T10:58:42Z,"ORIGINAL: **13.8 ms**
EDIT: After #21369 was merged the result of ``%timeit df2.loc['b']`` has improved to  **3.8 ms**.
EDIT: After #21618 was merged the result of ``%timeit df2.loc['b']`` has improved to  **3.3 ms**.
EDIT: After #21659 was merged the result of ``%timeit df2.loc['b']`` has improved to  **1.6 ms**.
EDIT: After #23235 was merged the result of ``%timeit df2.loc['b']`` has improved to  **159 µs**. Issue resolved.

#### Code Sample

```python
>>> n = 100_000
>>> df1 = pd.DataFrame(dict(A=range(n*3)), index=list('a'*n + 'b'*n + 'c'*n))
>>> df1.index.is_monotonic_increasing
True
>>> df2 = df1.copy()
>>> df2.index = pd.CategoricalIndex(df2.index)
>>> df2.index.is_monotonic_increasing
True
>>> %timeit df1.loc['b']
125 µs ± 2.95 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
>>> %timeit df2.loc['b']
13.8 ms ± 193 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```
#### Problem description

Selecting on a ``CategoricalIndex`` is 100x slower than selecting on a normal ``Index``.

I've tested this on master ( a few days old)  and on v0.22, with same result for both versions. The speed is even worse than the speed for a full columns scan:

```python
>>> df3 = df2.reset_index()
>>> %timeit df3[df3['index'] == 'b']
6.58 ms ± 25.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

A guess is that the binary search is bypassed and a full index scan is being done + some extra stuff  so it's even slower than a normal full columns scan.

#### Expected Output

The output is as expected, but the speed is very slow for ``CategoricalIndex``.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: a7a7f8c1101aed1a9d37abbbcd80f77da414f0a8
python: 3.6.3.final.0
python-bits: 32
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.22.0.dev0+870.ga7a7f8c
pytest: 3.3.1
pip: 9.0.1
setuptools: 38.2.5
Cython: 0.26.1
numpy: 1.13.3
scipy: 1.0.0
pyarrow: None
xarray: 0.10.0
IPython: 6.2.1
sphinx: 1.6.3
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.3
blosc: None
bottleneck: None
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 2.1.0
openpyxl: 2.4.9
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.2
lxml: None
bs4: None
html5lib: 1.0b10
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>"
672664228,35538,Can't access multi-index names when iterating over rows,konstantinmiller,closed,2020-08-04T09:54:26Z,2020-08-04T12:32:57Z,"When iterating over rows with either `iterrows()` or `itertuples()`, it is not possible to access index names if you have a multi-index.

So, I would expect something like that to work
```
import pandas as pd

df = pd.DataFrame(
    index=pd.MultiIndex(
        names=['ind1'],
        levels=[['a']],
        codes=[[0]]
    ),
    data={'C': [42]})

ind, row = next(df.iterrows())
row.C
ind.ind1
```
However, since `ind` is not a named tuple, it throws an `AttributeError: 'tuple' object has no attribute 'ind1'`

Same with `itertuples()`:
```
import pandas as pd

df = pd.DataFrame(
    index=pd.MultiIndex(
        names=['ind1'],
        levels=[['a']],
        codes=[[0]]
    ),
    data={'C': [42]})

row = next(df.itertuples())
ind = row.Index
row.C
ind.ind1
```
throws `AttributeError: 'tuple' object has no attribute 'ind1'`"
667761552,35451,WEB: Fixing whatsnew link in the home page (version was hardcoded),datapythonista,closed,2020-07-29T11:14:06Z,2020-08-04T15:21:03Z,
652238110,35157,July 2020 Developer Meeting,TomAugspurger,closed,2020-07-07T11:16:37Z,2020-08-04T16:15:52Z,"The monthly dev meeting is Wednesday July 8th, at 18:00 UTC. Our calendar is at https://pandas.pydata.org/docs/development/meeting.html#calendar to check your local time.

Video Call: https://zoom.us/j/942410248
Minutes: https://docs.google.com/document/u/1/d/1tGbTiYORHiSPgVMXawiweGJlBw5dOkVJLY-licoBmBU/edit?ouid=102771015311436394588&usp=docs_home&ths=true

Please add items you'd like to see discussed to the agenda.

All are welcome to attend.

FYI, I might not be able to attend, or I might just be on the phone. I think the Zoom call is structured to start automatically without me. @jreback and @jorisvandenbossche should also have Zoom accounts through NumFOCUS that can be used."
614256415,34052,PERF: Use Indexers to implement groupby rolling,mroeschke,closed,2020-05-07T18:26:48Z,2020-08-04T16:40:50Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Currently, `grouby.rolling` is implemented essentially as `groupby.apply(lambda x: x.rolling())` which can be potentially slow.

This PR implements `groupby.rolling` by calculating bounds with a `GroupbyRollingIndxer` and using the rolling aggregations in cython to compute the results. 
"
667330187,35438,DOC: Can't find mention of DataFrame accepting namedtuples as input,sjvrijn,closed,2020-07-28T19:54:08Z,2020-08-05T02:27:23Z,"#### Location of the documentation

User guide ['Intro to data structures'](https://pandas.pydata.org/docs/dev/user_guide/dsintro.html#dataframe)
(alternatively [`DataFrame` API docs](https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.html?highlight=dataframe#pandas.DataFrame))

#### Documentation problem

I recently only found out through a [StackOverflow answer](https://stackoverflow.com/a/51173794/) that the `DataFrame` constructor can take a list of namedtuples and automatically infers column names from it. Until then I had only found the [`DataFrame.from_records`](https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.from_records.html#pandas.DataFrame.from_records) constructor, which manually requires specifying `columns=Namedtuple._fields`.

After searching the documentation for [namedtuple](https://pandas.pydata.org/docs/dev/search.html?q=namedtuple), I haven't been able to find an explicit mention of constructors accepting namedtuples, while there is apparently explicit code in the constructor to deal with them nicely. Currently it only finds mentions where internal functions return rows as namedtuples.

In contrast, searching for e.g. ['dataclass'](https://pandas.pydata.org/docs/dev/search.html?q=dataclass) finds [an explicit paragraph](https://pandas.pydata.org/docs/dev/user_guide/dsintro.html?highlight=dataclass#from-a-list-of-dataclasses) listing the option.

#### Suggested fix for documentation

I would propose adding an explicit mention and/or example somewhere that these kinds of inputs are accepted/encouraged, such that they can be easily found when searching for the 'namedtuple' keyword.

I can write a similar paragraph as for dataclasses in the intro to data structures, if that is indeed the right place for this.
"
671106252,35507,Added paragraph on creating DataFrame from list of namedtuples,sjvrijn,closed,2020-08-01T18:53:17Z,2020-08-05T09:38:25Z,"- [x] closes #35438 

Is a whatsnew entry necessary for this PR?"
673198502,35560,BUG: series.where(boolean) works but df.where(boolean) does not,dycw,closed,2020-08-05T02:12:32Z,2020-08-05T11:34:46Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

x = pd.Series(range(3), dtype=float)
mask = pd.Series([True, False, nan], dtype=pd.BooleanDtype())
print(f""x=\n{x}\n\nmask=\n{mask}\n\nx.where(mask)=\n{x.where(mask)}"")

x=
0   0.00000
1   1.00000
2   2.00000
dtype: float64

mask=
0     True
1    False
2     <NA>
dtype: boolean

x.where(mask)=
0   0.00000
1       nan
2       nan
dtype: float64
```

```python
x_df = x.to_frame()
mask_df = mask.to_frame()
print(f""x_df=\n{x_df}\n\nmask_df=\n{mask_df}"")
x_df.where(mask_df)

x_df=
        0
0 0.00000
1 1.00000
2 2.00000

mask_df=
       0
0   True
1  False
2   <NA>

---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-9-7e122bb223a4> in <module>
      2 mask_df = mask.to_frame()
      3 print(f""x_df=\n{x_df}\n\nmask_df=\n{mask_df}"")
----> 4 x_df.where(mask_df)

~/miniconda3/envs/dts/lib/python3.8/site-packages/pandas/core/generic.py in where(self, cond, other, inplace, axis, level, errors, try_cast)
   8990         """"""
   8991         other = com.apply_if_callable(other, self)
-> 8992         return self._where(
   8993             cond, other, inplace, axis, level, errors=errors, try_cast=try_cast
   8994         )

~/miniconda3/envs/dts/lib/python3.8/site-packages/pandas/core/generic.py in _where(self, cond, other, inplace, axis, level, errors, try_cast)
   8847 
   8848         else:
-> 8849             new_data = self._mgr.where(
   8850                 other=other,
   8851                 cond=cond,

~/miniconda3/envs/dts/lib/python3.8/site-packages/pandas/core/internals/managers.py in where(self, other, cond, align, errors, try_cast, axis)
    511             other = extract_array(other, extract_numpy=True)
    512 
--> 513         return self.apply(
    514             ""where"",
    515             align_keys=align_keys,

~/miniconda3/envs/dts/lib/python3.8/site-packages/pandas/core/internals/managers.py in apply(self, f, align_keys, **kwargs)
    394                 applied = b.apply(f, **kwargs)
    395             else:
--> 396                 applied = getattr(b, f)(**kwargs)
    397             result_blocks = _extend_blocks(applied, result_blocks)
    398 

~/miniconda3/envs/dts/lib/python3.8/site-packages/pandas/core/internals/blocks.py in where(self, other, cond, errors, try_cast, axis)
   1299         import pandas.core.computation.expressions as expressions
   1300 
-> 1301         cond = _extract_bool_array(cond)
   1302         assert not isinstance(other, (ABCIndexClass, ABCSeries, ABCDataFrame))
   1303 

~/miniconda3/envs/dts/lib/python3.8/site-packages/pandas/core/internals/blocks.py in _extract_bool_array(mask)
   2862 
   2863     assert isinstance(mask, np.ndarray), type(mask)
-> 2864     assert mask.dtype == bool, mask.dtype
   2865     return mask

AssertionError: object
```

#### Problem description

If the truthiness of `boolean` Series is defined, then it certainly should be for DataFrames.

#### Expected Output

Equivalent to doing this columnwise, i.e.:
```python
expected = pd.concat([x.where(m) for (_, x), (_, m) in zip(x_df.items(), mask_df.items())], axis=1)
print(f""expected=\n{expected}"")

expected=
        0
0 0.00000
1     nan
2     nan
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-42-generic
Version          : #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_HK.UTF-8
LOCALE           : en_HK.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : None
pytest           : 6.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : None
tabulate         : 0.8.3
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
614193278,34050,CI: check_freq=False,jbrockmendel,closed,2020-05-07T16:36:37Z,2020-08-05T14:25:51Z,Aimed at a test that has been periodically failing in the CI for a week or so.
580126749,32665,BUG: Fix to_json when converting Period column,colonesej,closed,2020-03-12T18:10:32Z,2020-08-06T11:19:32Z,"AttributeError  when trying to access frequency string ``freqstr`` directly on Series of ``Period`` type. 

Modified to get it through ``dtype`` of Series, which is known to be ``Period``.

- [X] closes #31917
- [X] fix #31917
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

```python
import pandas as pd
test = pd.Series(pd.period_range('1/1/2011', freq='B', periods=3))
test.to_json(None, orient='table')

>>> '{""schema"":{""fields"":[{""name"":""index"",""type"":""integer""},{""name"":""values"",""type"":""datetime"",""freq"":""B""}],""primaryKey"":[""index""],""pandas_version"":""0.20.0""},""data"":[{""index"":0,""values"":{""day"":3,""dayofyear"":3,""daysinmonth"":31,""freqstr"":""B"",""is_leap_year"":false,""month"":1,""ordinal"":10697,""qyear"":2011,""start_time"":""2011-01-03T00:00:00.000Z"",""week"":1,""weekofyear"":1}},{""index"":1,""values"":{""day"":4,""dayofyear"":4,""daysinmonth"":31,""freqstr"":""B"",""is_leap_year"":false,""month"":1,""ordinal"":10698,""qyear"":2011,""start_time"":""2011-01-04T00:00:00.000Z"",""week"":1,""weekofyear"":1}},{""index"":2,""values"":{""day"":5,""dayofyear"":5,""daysinmonth"":31,""freqstr"":""B"",""is_leap_year"":false,""month"":1,""ordinal"":10699,""qyear"":2011,""start_time"":""2011-01-05T00:00:00.000Z"",""week"":1,""weekofyear"":1}}]}'
```"
666199957,35427,MAINT: Fix issue in StataReader due to upstream changes,bashtage,closed,2020-07-27T10:57:26Z,2020-08-06T12:43:13Z,"Avoid creating an array of dtypes to workaround NumPy future change

closes #35426

- [X] closes #35426
- [ ] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
673861051,35577,ENH: write support for google cloud storage ,zyxue,closed,2020-08-05T21:33:12Z,2020-08-06T14:20:44Z,"#### Is your feature request related to a problem?

Pandas can read from google cloud storage directly. I wonder if it's a good idea to support writing data frame to google cloud storage directly.

#### Describe the solution you'd like

pd.to_parquet('gs://some-bucket/mydata.parquet')

#### API breaking implications

unaware 

#### Describe alternatives you've considered

I've been writing custom functions like `write_df_to_gcs(df, gcs_uri, gcs_credentials)`, but I feel the better abstraction is to be inside `pd.to_parquet`, e.g. if to_parquet detects the path is a GCS URL, then write to GCS instead.

"
672290983,35530,"BUG: ""buffer source array is read-only"" with tz_convert_from_utc/DatetimeArray.date",lidavidm,closed,2020-08-03T19:25:16Z,2020-08-06T15:11:22Z,"- [X] I have checked that this issue has not already been reported.
There are similar issues with the same symptom
- [X] I have confirmed this bug exists on the latest version of pandas.
Tested with Pandas 1.1.0
- [X] (optional) I have confirmed this bug exists on the master branch of pandas.
Tested with bdcc5bffaadb7488474b65554c4e8e96a00aa4af
---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pyarrow as pa
import pandas as pd
import pytz
print(""PyArrow:"", pa.__version__)
print(""Pandas:"", pd.__version__)
table = pa.table([
    pa.array([
        pd.Timestamp('2014-01-01'),
    ], type=pa.timestamp(""ns""))
], names=[""time""])
df = table.to_pandas(self_destruct=True, split_blocks=True)
df.set_index(""time"", inplace=True)
df.index = df.index.tz_localize(pytz.utc)
# These are OK
print(df.index.date)
print(df.index.tz_convert(""America/New_York""))
# But not this
print(df.index.tz_convert(""America/New_York"").date)
```

#### Problem description

The reproduction results in this exception:

```python
Traceback (most recent call last):
  File ""repro.py"", line 18, in <module>
    print(df.index.tz_convert(""America/New_York"").date)
  File ""/home/lidavidm/Code/twosigma/pandas/temp/venv/lib/python3.8/site-packages/pandas/core/indexes/extension.py"", line 54, in fget
    result = getattr(self._data, name)
  File ""/home/lidavidm/Code/twosigma/pandas/temp/venv/lib/python3.8/site-packages/pandas/core/arrays/datetimes.py"", line 1246, in date
    timestamps = self._local_timestamps()
  File ""/home/lidavidm/Code/twosigma/pandas/temp/venv/lib/python3.8/site-packages/pandas/core/arrays/datetimes.py"", line 731, in _local_timestamps
    return tzconversion.tz_convert_from_utc(self.asi8, self.tz)
  File ""pandas/_libs/tslibs/tzconversion.pyx"", line 407, in pandas._libs.tslibs.tzconversion.tz_convert_from_utc
  File ""stringsource"", line 658, in View.MemoryView.memoryview_cwrapper
  File ""stringsource"", line 349, in View.MemoryView.memoryview.__cinit__
ValueError: buffer source array is read-only
```
The data is not being modified in place, so it should work with an immutable source array.

This is because `tz_convert_from_utc` and `_tz_convert_from_utc` are missing some `const` specifiers in Cython.

#### Expected Output

The `.date` accessor should work as expected:
```python
[datetime.date(2013, 12, 31)]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.7.9-arch1-1
Version          : #1 SMP PREEMPT Thu, 16 Jul 2020 19:34:49 +0000
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.1.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
673592169,35572,Ensure _group_selection_context is always reset,eric-wieser,closed,2020-08-05T14:39:09Z,2020-08-06T15:14:51Z,"Context managers will resume with an exception if the with block calling them fails.
This happening is not an excuse to not clean up.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Found by inspecting the code, not by actually finding a failing example.
"
511631455,29197,MyPy Error Codes,WillAyd,closed,2019-10-24T00:02:05Z,2020-08-06T15:35:16Z,"mypy 0.73 came with support for error codes:

http://mypy-lang.blogspot.com/2019/09/mypy-730-released.html

I think we should add these to CI and update docs to require these going forward for any `type: ignore` statements

@simonjayhawkins "
674374346,35589,BUG: `DataFrame.reset_index()` discards `MultiIndex` dtypes if `DataFrame` is empty,batterseapower,closed,2020-08-06T14:50:31Z,2020-08-06T15:48:20Z,"- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of pandas. (tried on v1.0.5 -- the latest in Conda -- and checked the release notes for the new version to make sure they don't mention anything related)
- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
x = pd.DataFrame.from_dict({
    'A': pd.Categorical.from_codes([1], categories=['G', 'H']),
    'B': np.array([0], dtype='i8'),
})

x.dtypes # category and int64

# This is the bug:
x.iloc[:0].set_index(['A', 'B']).reset_index().dtypes # float64 and float64

# These cases all work correctly i.e. the dtypes are category and int64
x.set_index(['A', 'B']).reset_index().dtypes
x.set_index('A').reset_index().dtypes
x.iloc[:0].set_index('A').reset_index().dtypes

# This is CategoricalDtype as expected, so the MultiIndex construction is correct: it's just the `reset_index()` which is throwing away the info
x.iloc[:0].set_index(['A', 'B']).index.get_level_values('A').dtype
```

#### Problem description

`reset_index` shouldn't throw away dtype information in the special case where the `DataFrame` has zero rows. Sometimes it's hard to avoid throwing away info when you manipulate zero-element DataFrames, but in this case there is no reason to do it.

#### Expected Output

category and int64 dtypes from `reset_index` in all cases

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 63 Stepping 2, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United Kingdom.936

pandas           : 1.0.5
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
617650653,34160,"ENH: enable mul, div on Index by dispatching to Series",jbrockmendel,closed,2020-05-13T18:08:12Z,2020-08-06T15:52:14Z,"One of the last remaining inconsistencies between Index and Series arithmetic is that division and multiplication is entirely disabled for object-dtype Index, while for Series we operate pointwise.  This PR makes Index behave the same way, and simplifies the code by using existing Series-op-wrapping code to do so.

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I'm pretty sure there is at least one issue for this, will track it down."
658567029,35311, TYP: Add MyPy Error Codes,simonjayhawkins,closed,2020-07-16T21:01:31Z,2020-08-06T15:57:50Z,"- [ ] closes #29197
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
672228184,35529,BUG: ValueError during floordiv of a series with timedelta type,galipremsagar,closed,2020-08-03T17:26:52Z,2020-08-06T16:20:29Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
>>> import pandas as pd
>>> sr = pd.Series([10, 20, 30], dtype='timedelta64[ns]')
>>> sr
0   00:00:00.000000
1   00:00:00.000000
2   00:00:00.000000
dtype: timedelta64[ns]
>>> sr = pd.Series([1000, 20, 30], dtype='timedelta64[ns]')
>>> sr
0   00:00:00.000001
1   00:00:00.000000
2   00:00:00.000000
dtype: timedelta64[ns]
>>> sr = pd.Series([1000, 222330, 30], dtype='timedelta64[ns]')
>>> sr
0   00:00:00.000001
1   00:00:00.000222
2   00:00:00.000000
dtype: timedelta64[ns]
>>> sr1 = pd.Series([1000, 222330, None], dtype='timedelta64[ns]')
>>> sr1
0   00:00:00.000001
1   00:00:00.000222
2               NaT
dtype: timedelta64[ns]
>>> sr / sr1
0    1.0
1    1.0
2    NaN
dtype: float64
>>> sr // sr1
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/nvme/0/pgali/envs/cudfdev1/lib/python3.7/site-packages/pandas/core/ops/common.py"", line 64, in new_method
    return method(self, other)
  File ""/nvme/0/pgali/envs/cudfdev1/lib/python3.7/site-packages/pandas/core/ops/__init__.py"", line 503, in wrapper
    result = arithmetic_op(lvalues, rvalues, op, str_rep)
  File ""/nvme/0/pgali/envs/cudfdev1/lib/python3.7/site-packages/pandas/core/ops/array_ops.py"", line 193, in arithmetic_op
    res_values = dispatch_to_extension_op(op, lvalues, rvalues)
  File ""/nvme/0/pgali/envs/cudfdev1/lib/python3.7/site-packages/pandas/core/ops/dispatch.py"", line 125, in dispatch_to_extension_op
    res_values = op(left, right)
  File ""/nvme/0/pgali/envs/cudfdev1/lib/python3.7/site-packages/pandas/core/arrays/timedeltas.py"", line 637, in __floordiv__
    result[mask] = np.nan
ValueError: cannot convert float NaN to integer

```

#### Problem description

When there is a `NaT` in either of the series, the result should ideally be of type `nullable` integer (`Int64`) to avoid this kind of `ValueError`

#### Expected Output

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-76-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.1.0.post20200704
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : 5.19.0
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.50.1

</details>
"
673952323,35583,BUG: TDA.__floordiv__ with NaT,jbrockmendel,closed,2020-08-06T01:42:45Z,2020-08-06T17:33:26Z,"- [x] closes #35529
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
669391946,35486,QST: is the new behavior of GroupByRolling in v1.1.0 intended?,itholic,closed,2020-07-31T04:13:37Z,2020-08-06T17:45:20Z,"#### Question about pandas

Let's say we have a `DataFrame` like the below.

```python
>>> pdf = pd.DataFrame({""a"": [1, 2, 3, 2], ""b"": [4.0, 2.0, 3.0, 1.0], ""c"": [10, 20, 30, 20]})
>>> pdf
   a    b   c
0  1  4.0  10
1  2  2.0  20
2  3  3.0  30
3  2  1.0  20
```

Then, when I use `GroupByRolling`, In the version of pandas <= 1.0.5 shows result as below.

```python
>>> pdf.groupby('a')[['b']].rolling(2).max()
       b
a
1 0  NaN
3 2  NaN
2 1  NaN
  3  2.0
```

However, In the pandas 1.1.0, the result seems different from the previous version as below.

```python
>>> pdf.groupby('a')[['b']].rolling(2).max()
       a    b     c
a
1 0  NaN  NaN   NaN
2 1  NaN  NaN   NaN
  3  2.0  2.0  20.0
3 2  NaN  NaN   NaN
```

Could someone let me know Is it intended? or unexpected behavior (maybe kind of bug) ?

Thanks :)"
633971639,34635,BUG: test_eval.TestMathNumExprPandas.test_result_complex128 failing on OSX,jbrockmendel,closed,2020-06-08T01:41:58Z,2020-08-06T17:55:42Z,"intermittently

OSX 10.15.5
python 3.7.6
numpy 1.18.5
numexpr 2.7.1

```
________________________________________________________________________________ TestMathNumExprPandas.test_result_complex128 _________________________________________________________________________________

self = <pandas.tests.computation.test_eval.TestMathNumExprPandas object at 0x7fc0ec87de50>

    @td.skip_if_windows
    def test_result_complex128(self):
        # xref https://github.com/pandas-dev/pandas/issues/12293
        #  this fails on Windows, apparently a floating point precision issue
    
        # Did not test complex64 because DataFrame is converting it to
        # complex128. Due to https://github.com/pandas-dev/pandas/issues/10952
>       self.check_result_type(np.complex128, np.complex128)

pandas/tests/computation/test_eval.py:1832: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pandas/tests/computation/test_eval.py:1817: in check_result_type
    tm.assert_series_equal(got, expect, check_names=False)
pandas/_libs/testing.pyx:68: in pandas._libs.testing.assert_almost_equal
    cpdef assert_almost_equal(a, b,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise_assert_detail(obj, msg, lobj, robj, index_values=index_values)
E   AssertionError: Series are different
E   
E   Series values are different (20.0 %)
E   [index]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
E   [left]:  [(0.771519184640353+0j), (-0.7269031850139311-0j), (0.995153744613331+0j), (-0.08814548225306257+0j), (0.9413908236833275+0j), (0.9482003101223208+0j), (-0.9535450271705115+0j), (-0.27032459595133673+0j), (0.011064344419221385+0j), (-0.8916020886333388+0j)]
E   [right]: [(0.771519184640353+0j), (-0.7269031850139313+0j), (0.995153744613331+0j), (-0.08814548225306257+0j), (0.9413908236833275+0j), (0.9482003101223209+0j), (-0.9535450271705115+0j), (-0.27032459595133673+0j), (0.011064344419221385+0j), (-0.8916020886333388+0j)]

pandas/_libs/testing.pyx:183: AssertionError
```"
672387439,35532,BUG: handle immutable arrays in tz_convert_from_utc (#35530),lidavidm,closed,2020-08-03T22:49:18Z,2020-08-06T18:17:25Z,"- [x] closes #35530 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
671410467,35513,BUG: RollingGroupby respects __getitem__,mroeschke,closed,2020-08-02T02:23:26Z,2020-08-06T19:18:11Z,"- [x] closes #35486
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
650104593,35097,QST: Does the following method have circular dependencies? hash_pandas_object,alessiosavi,closed,2020-07-02T17:44:34Z,2020-08-06T19:31:27Z,"- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

Hi Team, does the method `hash_pandas_object` have circular dependencies?

I'm not able to serialize class that contains the method in my code with `dill`.

```python
# Your code here, if applicable
import numpy as np
import pandas as pd
from pandas.core.util.hashing import hash_pandas_object
# Same result using the following library
# from pandas.util import hash_pandas_object


class T:
	def __init__(self):
		self.dataset_hash = """"

	def main(self):
		df = pd.DataFrame(np.random.randint(0, 100, size=(100, 4)), columns=list('ABCD'))
		print(df)
		self.dataset_hash = hash_pandas_object(df).sum()


import sys
import dill

sys.setrecursionlimit(10 ** 4)

t = T()
with open(""test.dill"", ""wb"") as f:
	dill.dump(t, f, recurse=True)
t.main()
```

NOTE:
I've opened an issue to the `dill` repository to [https://github.com/uqfoundation/dill/issues/374]"
673795373,35575,CLN: remove kwargs from RangeIndex.copy,topper-123,closed,2020-08-05T19:33:14Z,2020-08-06T20:23:31Z,"xref #31669.
"
674544494,35591,Backport PR #35513 on branch 1.1.x (BUG: RollingGroupby respects __getitem__),meeseeksmachine,closed,2020-08-06T19:18:42Z,2020-08-06T21:50:03Z,Backport PR #35513: BUG: RollingGroupby respects __getitem__
673054979,35554,DOC: Document that read_hdf can use pickle,TomAugspurger,closed,2020-08-04T20:16:58Z,2020-08-06T22:00:31Z,"This documents that `read_hdf`, which uses PyTables, might invoke pickle to deserialize arrays that were serialize with pickle.

PyTables clearly documents this at http://www.pytables.org/usersguide/libref/declarative_classes.html#tables.ObjectAtom, but pandas users may not realize that if they aren't aware that pandas uses pytables under the hood.

I've tried to include the warning in all the methods that eventually read data, but it's a bit hard to say if I've gotten them all.

Ideally we would also like to provide an `allow_pickle=False` keyword, but that would be best implemented in PyTables, and used by us. I opened https://github.com/PyTables/PyTables/issues/813 for discussion."
647040921,35046,"BUG/API Should pd.NaT < ""foo"" raise?",jbrockmendel,closed,2020-06-29T00:43:57Z,2020-08-06T22:06:47Z,"If we treat NaT as a datetime, we would expect pd.NaT == ""foo"" to return False, != to return True, and for the inequalities to raise:

```
>>> np.datetime64(""NaT"", ""ns"") < ""foo""
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: '<' not supported between instances of 'numpy.ndarray' and 'str'

>>> np.nan < ""foo""
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: '<' not supported between instances of 'float' and 'str'

>>> pd.NaT < ""foo""
False
```

It isn't obvious to me that the existing behavior is intentional.  If we change this to raise, we can simplify `NaT.__richcmp__` a bit."
671566545,35515,CLN: get_flattened_iterator,mroeschke,closed,2020-08-02T07:07:45Z,2020-08-06T23:00:09Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Refactor to just a single function
"
674625709,35593,Backport PR #35554 on branch 1.1.x (DOC: Document that read_hdf can use pickle),meeseeksmachine,closed,2020-08-06T22:00:39Z,2020-08-06T23:19:31Z,Backport PR #35554: DOC: Document that read_hdf can use pickle
673990696,35585,BUG: NaT.__cmp__(invalid) should raise TypeError,jbrockmendel,closed,2020-08-06T03:42:48Z,2020-08-06T23:28:01Z,"- [x] closes #35046
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
665272652,35405,DOC: Add compose ecosystem docs,jeff-hernandez,closed,2020-07-24T16:14:25Z,2020-08-06T23:37:30Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
662288253,35359,REF: implement _apply_blockwise,jbrockmendel,closed,2020-07-20T21:35:19Z,2020-08-06T23:54:44Z,Preliminary pushing towards #34714
597469450,33437,BUG: DataFrame constructor doesn't validate index/data lengths with EA data,jbrockmendel,closed,2020-04-09T18:24:35Z,2020-08-07T02:20:03Z,"```
dti = pd.date_range(""2016-01-01"", periods=3, tz=""US/Pacific"")

>>> pd.DataFrame(dti, index=range(4))
                          0
0 2016-01-01 00:00:00-08:00
1 2016-01-02 00:00:00-08:00
2 2016-01-03 00:00:00-08:00
3                          

>>> pd.DataFrame(dti.tz_localize(None), index=range(4))
ValueError: Shape of passed values is (3, 1), indices imply (4, 1)
```"
339139864,21786,.rolling().std() only returns NaN in Python3.7,tweakimp,closed,2018-07-07T11:43:16Z,2020-08-07T05:48:41Z,"```python
import pandas as pd
d = {""col"": [1, 23, 231, 231, 4, 353, 62, 3, 56, 43, 354, 43, 231, 21, 7]}
df = pd.DataFrame(data=d)
std = df[""col""].std()
df[""mean5""] = df[""col""].rolling(5).mean()
df[""std5""] = df[""col""].rolling(5).std()

print(std)
print(df[[""mean5"", ""std5""]])

# OUTPUT
130.20855066648528
    mean5  std5 
0     NaN   NaN
1     NaN   NaN
2     NaN   NaN
3     NaN   NaN
4    98.0   NaN
5   168.4   NaN
6   176.2   NaN
7   130.6   NaN
8    95.6   NaN
9   103.4   NaN
10  103.6   NaN
11   99.8   NaN
12  145.4   NaN
13  138.4   NaN
14  131.2   NaN
```

#### Problem description
`.std()` and `.rolling().mean()` work as intended, but `.rolling().std()` only returns NaN
I just upgraded from Python 3.6.5 where the same code did work perfectly.
I am now on Python 3.7, pandas 0.23.2

#### Expected Output
```python
130.20855066648528
    mean5        std5
0     NaN         NaN
1     NaN         NaN
2     NaN         NaN
3     NaN         NaN
4    98.0  108.855868
5   168.4  134.226078
6   176.2  126.458531
7   130.6  138.965607
8    95.6  131.085621
9   103.4  126.482568
10  103.6  126.877264
11   99.8  128.342355
12  145.4  126.337010
13  138.4  131.941805
14  131.2  137.803338
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.0.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.23.2
pytest: None
pip: 10.0.1
setuptools: 39.0.1
Cython: None
numpy: 1.14.5
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: 4.6.0
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
None

</details>
"
673511553,35567,Ensure file is closed promptly in case of error,rxxg,closed,2020-08-05T12:46:05Z,2020-08-07T07:33:59Z,Fixes #35566
674532326,35590,BUG: validate index/data length match in DataFrame construction,jbrockmendel,closed,2020-08-06T18:56:01Z,2020-08-07T07:45:04Z,"- [x] closes #33437
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
674830478,35597,Backport PR #35590 on branch 1.1.x (BUG: validate index/data length match in DataFrame construction),meeseeksmachine,closed,2020-08-07T07:45:37Z,2020-08-07T08:32:55Z,Backport PR #35590: BUG: validate index/data length match in DataFrame construction
663284364,35368,DOC: Fix heading capitalization in doc/source/whatsnew - part6 (#32550) (open PR again due to branch removed),cleconte987,closed,2020-07-21T20:22:21Z,2020-08-07T09:35:56Z,"- [ ] Modify files v0.22.0.rst, v0.23.1.rst, v0.19.0.rst, v0.24.0.rst, v0.24.2.rst
-[ ] Add exceptions in 'validate_rst_title_capitalization.py'"
672780990,35544,BUG: series.truncate doesn't return the correct value in pandas 1.1.0,gabicca,closed,2020-08-04T13:10:48Z,2020-08-07T11:39:32Z,"- [Y] I have checked that this issue has not already been reported.

- [Y] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
def test_truncate():
    series = pd.Series([0.1], index=pd.DatetimeIndex(['2020-08-04']))
    before = pd.Timestamp('2020-08-02')
    after = pd.Timestamp('2020-08-04')

    assert series.truncate(before=before, after=after).equals(series)

```

#### Problem description

The above test passes in pandas 1.0.5 while fails in pandas 1.1.0. 

After diving into the code I found that *truncate* in pandas/core/generic.py changed as follows:
```
if ax.is_monotonic_decreasing:
     before, after = after, before
```
the above check was added to the method. ```ax.is_monotonic_decreasing``` returns true for 1-element series, (```ax.is_monotonic_increasing``` also returns true). When *before*, and *after* are given in the correct, increasing order (like in the test), the following check reverses them and the result of  truncate will be an empty series, instead of returning the input series which is the expected behaviour."
669526998,35488,BUG: Shift on DataFrame which has more than 1 block creates wrong result,qinxuye,closed,2020-07-31T07:49:59Z,2020-08-07T11:45:49Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
In [1]: import pandas as pd                                                     

In [2]: import numpy as np                                                      

In [3]: df1 = pd.DataFrame(np.random.randint(1000, size=(5, 3)))                

In [4]: df2 = pd.DataFrame(np.random.randint(1000, size=(5, 2)))                

In [5]: df3 = pd.concat([df1, df2], axis=1)                                     

In [6]: df3                                                                     
Out[6]: 
     0    1    2    0    1
0   61  536  154  766  179
1  484   18   15  787  766
2  391  171  715  836  654
3  914  969  765  824  950
4  169  414  759   16  666

In [7]: len(df3._data.blocks)                                                   
Out[7]: 2

In [9]: df3.shift(2, axis=1)                                                    
Out[9]: 
    0   1      2   0   1
0 NaN NaN   61.0 NaN NaN
1 NaN NaN  484.0 NaN NaN
2 NaN NaN  391.0 NaN NaN
3 NaN NaN  914.0 NaN NaN
4 NaN NaN  169.0 NaN NaN

```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

I guess `shift` is applied to both of the internal blocks.

#### Expected Output

I forced consolidate, the result is right.

```
In [12]: df3._data._consolidate_inplace()                                       

In [13]: df3.shift(2, axis=1)                                                   
Out[13]: 
    0   1      2      0      1
0 NaN NaN   61.0  536.0  154.0
1 NaN NaN  484.0   18.0   15.0
2 NaN NaN  391.0  171.0  715.0
3 NaN NaN  914.0  969.0  765.0
4 NaN NaN  169.0  414.0  759.0
```

#### Output of ``pd.show_versions()``

<details>

In [14]: pd.show_versions()                                                     
/Users/qinxuye/miniconda3/envs/test_pandas_1.1/lib/python3.7/site-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.
  ""Distutils was imported before Setuptools. This usage is discouraged ""
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-14-3d232a07e144> in <module>
----> 1 pd.show_versions()

~/miniconda3/envs/test_pandas_1.1/lib/python3.7/site-packages/pandas/util/_print_versions.py in show_versions(as_json)
    104     """"""
    105     sys_info = _get_sys_info()
--> 106     deps = _get_dependency_info()
    107 
    108     if as_json:

~/miniconda3/envs/test_pandas_1.1/lib/python3.7/site-packages/pandas/util/_print_versions.py in _get_dependency_info()
     82     for modname in deps:
     83         mod = import_optional_dependency(
---> 84             modname, raise_on_missing=False, on_version=""ignore""
     85         )
     86         result[modname] = _get_version(mod) if mod else None

~/miniconda3/envs/test_pandas_1.1/lib/python3.7/site-packages/pandas/compat/_optional.py in import_optional_dependency(name, extra, raise_on_missing, on_version)
     97     minimum_version = VERSIONS.get(name)
     98     if minimum_version:
---> 99         version = _get_version(module)
    100         if distutils.version.LooseVersion(version) < minimum_version:
    101             assert on_version in {""warn"", ""raise"", ""ignore""}

~/miniconda3/envs/test_pandas_1.1/lib/python3.7/site-packages/pandas/compat/_optional.py in _get_version(module)
     42 
     43     if version is None:
---> 44         raise ImportError(f""Can't determine version for {module.__name__}"")
     45     return version
     46 

ImportError: Can't determine version for numba
</details>
"
673769628,35574,BUG: pd.to_timedelta() fails when arg is a Series with Int64 dtype,egoddard,closed,2020-08-05T18:50:10Z,2020-08-07T11:49:52Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd


series = pd.Series([pd.NA], dtype=""Int64"")

pd.to_timedelta(series, errors=""coerce"")
```

#### Problem description

When converting a `Series` with an `Int64` dtype to a timedelta, `to_timedelta` fails with
 
```python
ValueError: cannot convert to 'int64'-dtype NumPy array with missing values. Specify an appropriate 'na_value' for this dtype.
```

Passing a Python list containing a NA value works, as does passing an `Int64` Series that does not contain NA values. The functionality between these cases should be equivalent.


#### Expected Output

`pd.to_timedelta()` output returns a `timedelta64` Series containing NA values when the input is an `Int64` Series containg NA values:

```python
0   <NA>
dtype: timedelta64[ns]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3b6915ef9435b13ac5d8c19634270ae231f8e47c
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-7634-generic
Version          : #38~1595345317~20.04~a8480ad-Ubuntu SMP Wed Jul 22 15:13:45 UTC 
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+28.g3b6915ef9
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 45.2.0.post20200210
Cython           : 0.29.21
pytest           : 6.0.1
hypothesis       : 5.23.11
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.1
lxml.etree       : 4.4.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.8.0
fastparquet      : 0.4.1
gcsfs            : 0.6.2
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.2
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.0
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.1

</details>
"
673952127,35582,BUG: to_timedelta fails on Int64 Series with null values,egoddard,closed,2020-08-06T01:42:06Z,2020-08-07T11:50:28Z,"- [X] closes #35574 
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
647799653,35058,BUG: to_csv doesn't support file handles from ZipFiles,twoertwein,closed,2020-06-30T02:21:10Z,2020-08-07T11:53:10Z,"- [X] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python

from zipfile import ZipFile

import numpy as np
import pandas as pd

with ZipFile(""plots.zip"", mode=""w"") as file:
    # write some matplotlib plots into the zip file

    # write CSV into zip file
    data = pd.DataFrame(np.random.rand(10, 3))
    with file.open(""data.csv"", mode=""w"") as csv_file:
        data.to_csv(csv_file)

```

Last part of the stack trace:
```
~/miniforge3/envs/panama/lib/python3.8/site-packages/pandas/io/formats/csvs.py in _save_header(self)
    278         if not has_mi_columns or has_aliases:
    279             encoded_labels += list(write_cols)
--> 280             writer.writerow(encoded_labels)
    281         else:
    282             # write out the mi

~/miniforge3/envs/panama/lib/python3.8/zipfile.py in write(self, data)
   1134         nbytes = len(data)
   1135         self._file_size += nbytes
-> 1136         self._crc = crc32(data, self._crc)
   1137         if self._compressor:
   1138             data = self._compressor.compress(data)

TypeError: a bytes-like object is required, not 'str'
```
#### Problem description

I know that `to_csv` supports compression but I want to write a DataFrame into a zip file with multiple other files. I'm not sure whether the above error is an error in pandas or a missing feature/bug in zipfiles.

#### Expected Output

`to_csv` supports writing to a ZipFile's file handle.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-693.5.2.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.4
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.1.1.post20200529
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None


</details>
"
383360619,23854,df.to_csv() ignores encoding when given a file object or any other filelike object.,eode,closed,2018-11-22T02:44:59Z,2020-08-07T11:53:10Z,"#### Code Sample, a copy-pastable example if possible
```python
import pandas as pd
import io

# !! NOTE
# This example uses `io.BytesIO`, however this also applies to file buffers that are
# returned by `io.open` (the `open` function) when opened in binary mode.
buf = io.BytesIO('a, b, 🐟\n1, 2, 3\n4, 5, 6'.encode('utf-8'))
df = pd.read_csv(buf)   # reads in fine using default encoding (utf-8)
buf = io.BytesIO()

df.to_csv(buf, encoding='utf-8')  # this should work, but doesn't.
# TypeError: a bytes-like object is required, not 'str'

buf = io.StringIO()
df.to_csv(buf, encoding='utf-8')  # this 'works', but should fail.  Data is passed in without encoding.
buf.getvalue()
# ',a, b, 🐟\n0,1,2,3\n1,4,5,6\n'
```
#### Problem description
Currently, the 'encoding' parameter is accepted *and* doesn't do anything when dealing with an in-memory object.  This is deceptive, and can introduce encoding flaws.

I *presume* that pandas just sets the encoding on the file it opens.  In the case of receiving an already-open filelike object, pandas should encode the string and attempt to write the bytes into the file.  If it  fails, that's a valid and appropriate failure, and that failure should be raised.

However, in the interest of backwards compatibility, if it fails, it should probably try to write the unencoded string into the file, and perhaps display a warning.

I'm on Pandas 0.23.4.
https://pandas-docs.github.io/pandas-docs-travis/

#### Expected Output
```python
# a buffer that contains the following:
b',a, b, \xf0\x9f\x90\x9f\n0,1,2,3\n1,4,5,6\n'
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.7.final.0
python-bits: 64
OS: Linux
OS-release: 4.19.3-041903-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: 4.0.0
pip: 9.0.1
setuptools: 39.0.1
Cython: None
numpy: 1.15.4
scipy: None
pyarrow: 0.11.1
xarray: None
IPython: 7.1.1
sphinx: None
patsy: None
dateutil: 2.7.5
pytz: 2018.7
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.999999999
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
299133878,19827,"File mode in `to_csv` is ignored, when passing a file object instead of a path",colobas,closed,2018-02-21T21:24:14Z,2020-08-07T11:53:10Z,"#### Code Sample, a copy-pastable example if possible

```
>>> import pandas as pd
>>> df = pd.read_csv(""example.csv"")
>>> df.head()
   just  a  file
0     1  2     3
1     4  5     6
2     7  8     9
>>> with open(""someother.csv"", ""wb"") as f:
...     df.to_csv(f, mode=""wb"")
... 
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/usr/lib/python3.6/site-packages/pandas/core/frame.py"", line 1524, in to_csv
    formatter.save()
  File ""/usr/lib/python3.6/site-packages/pandas/io/formats/format.py"", line 1652, in save
    self._save()
  File ""/usr/lib/python3.6/site-packages/pandas/io/formats/format.py"", line 1740, in _save
    self._save_header()
  File ""/usr/lib/python3.6/site-packages/pandas/io/formats/format.py"", line 1708, in _save_header
    writer.writerow(encoded_labels)
TypeError: a bytes-like object is required, not 'str'
```
#### Problem description

When passing a file opened in binary mode to `df.to_csv` and also passing `mode='wb'`, this mode is ignored. I think it's because of these lines: https://github.com/pandas-dev/pandas/blob/master/pandas/io/common.py#L407-L411 and these ones: https://github.com/pandas-dev/pandas/blob/master/pandas/io/formats/format.py#L1660-L1662

It seems that `is_text` isn't passed, and so it assumes the default value of `True`

#### Expected Output
A file should just be written.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.4.final.0
python-bits: 64
OS: Linux
OS-release: 4.14.20-1-lts
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.22.0
pytest: 3.4.0
pip: 9.0.1
setuptools: 38.5.1
Cython: 0.27.3
numpy: 1.14.0
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: 1.7.0
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2018.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.1.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: 4.6.0
html5lib: 1.0b10
sqlalchemy: 1.2.3
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: 0.1.2
fastparquet: 0.1.3
pandas_gbq: None
pandas_datareader: None

</details>
"
152827225,13068,Python 3 writing to_csv file ignores encoding argument.,graingert,closed,2016-05-03T17:13:57Z,2020-08-07T11:53:10Z,"``` python
# is missing the UTF8 BOM (encoded with default encoding UTF8)
with open('path_to_f', 'w') as f:
    df.to_csv(f, encoding='utf-8-sig')

# is not missing the UTF8 BOM (encoded with passed encoding utf-8-sig)
df.to_csv('path_to_f', encoding='utf-8-sig')
```

I expect:

``` python
with open('path_to_f', 'w') as f:
    df.to_csv(f, encoding='utf-8-sig')
```

To crash with `TypeError: write() argument must be str, not bytes`

and I expect:

``` python
with open('path_to_f', 'wb') as f:
    df.to_csv(f, encoding='utf-8-sig')
```

To write the file correctly.
### Copy pasta

``` python
#!/usr/bin/env python3
import pandas as pd
df = pd.DataFrame()
with open('file_one', 'w') as f:
    df.to_csv(f, encoding='utf-8-sig')

assert open('file_one', 'rb').read() == b'""""\n'

# is not missing the UTF8 BOM (encoded with passed encoding utf-8-sig)
df.to_csv('file_two', encoding='utf-8-sig')
assert open('file_two', 'rb').read() == b'\xef\xbb\xbf""""\n'
```
"
673912827,35580,BUG:dtype=object columns with NaNs coerce integers to floats,ashtonteng,closed,2020-08-05T23:34:29Z,2020-08-07T11:59:51Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame({'name': [np.nan, ""hi""],
                   'mask': [True, 4],
                   'weapon': [5, np.nan]}).astype(object)
print(df) 
""""""
name  mask weapon
0  NaN  True      5
1   hi     4    NaN
"""""" # everything prints out correctly
print(df.iloc[0].values) # [nan True 5.0] # during indexing, 5 gets coerced to 5.0
# ultimately I want to be able to do df.to_csv(), and get unmodified data
```
#### Problem description

I have a dataframe of mixed types, which I cast to dtype=object, and NaNs in certain grids. Printing out the dataframe works fine, but when I try indexing into specific rows, or exporting to_csv, the values in the columns with NaNs are quietly modified - integers become floats. I believe this is a bug because I would expect to_csv to produce a csv file that is similar to printing the dataframe, without data types being changed. I cannot cast the dataframe to type Int64, since I have other types in the dataframe as well.

#### Expected Output

Instead of [nan True 5.0], I should get [nan True 5]

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.8.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Sun Jul  5 00:43:10 PDT 2020; root:xnu-6153.141.1~9/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 49.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
674958205,35600,Backport PR #35547 on branch 1.1.x (Bug fix one element series truncate),meeseeksmachine,closed,2020-08-07T11:39:43Z,2020-08-07T12:29:44Z,Backport PR #35547: Bug fix one element series truncate
646410490,35019,BUG: to_json not allowing uploads to S3,rohan-gt,closed,2020-06-26T17:32:30Z,2020-08-07T12:45:37Z,This issue exists on the latest Pandas version (v1.0.5). `read_json` works perfectly with S3 paths.
672866235,35547,Bug fix one element series truncate,gabicca,closed,2020-08-04T15:07:30Z,2020-08-07T12:48:06Z,"- [Y] closes #35544 
- [Y] tests added / passed
- [Y] passes `black pandas`
- [Y] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [Y] whatsnew entry

Fix when trying to truncate a one-element series within a correct date range returned an empty series instead of the originally passed-in series. "
674963378,35602,Backport PR #35582 on branch 1.1.x (BUG: to_timedelta fails on Int64 Series with null values),meeseeksmachine,closed,2020-08-07T11:50:37Z,2020-08-07T12:52:48Z,Backport PR #35582: BUG: to_timedelta fails on Int64 Series with null values
674961877,35601,"Backport PR #35578 on branch 1.1.x (BUG: df.shift(n, axis=1) with multiple blocks)",meeseeksmachine,closed,2020-08-07T11:47:28Z,2020-08-07T13:34:25Z,"Backport PR #35578: BUG: df.shift(n, axis=1) with multiple blocks"
646668745,35028,BUG: df.groupby().count() returns NaN instead of Zero,smithto1,closed,2020-06-27T12:04:41Z,2020-08-07T15:21:12Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python


import pandas as pd

df = pd.DataFrame(
    {
        ""cat_1"": pd.Categorical(list(""AABB""), categories=list(""ABC"")),
        ""cat_2"": pd.Categorical(list(""1111""), categories=list(""12"")),
        ""value"": [0.1, 0.1, 0.1, 0.1],
    }
)


# SeriesGroupBy on one pd.Categorical: unobserved categories have a count of 0
srg_grp = df.groupby(['cat_1'], observed=False)['value']
print(srg_grp.count())

# SeriesGroupBy on two pd.Categorical: unobserved categories have a count of 0
srs_grp = df.groupby(['cat_1', 'cat_2'], observed=False)['value']
print(srs_grp.count())

# DataFrameGroupBy on one pd.Categorical: unobserved categories have a count of 0
df_grp = df.groupby(['cat_1'], observed=False)
print(df_grp.count())

# DataFrameGroupBy on two pd.Categorical: unobserved categories have a count of NaN
df_grp = df.groupby(['cat_1', 'cat_2'], observed=False)
print(df_grp.count())
```

#### Problem description

When grouping by multiple pd.Categorical columns, DataFrameGroupBy.count() returns NaN for missing categories, and the dtype is float. 

A similar problem is reported for .sum() in #31422 

#### Expected Output

.count() should return zero for missing categories with a dtype of int. 

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 248c191478aabacffec47105c3fe6caf22b0dcc1
python           : 3.8.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United Kingdom.1252

pandas           : 1.1.0.dev0+1973.g248c19147.dirty
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1.post20200616
Cython           : 0.29.20
pytest           : 5.4.3
hypothesis       : 5.18.0
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : 0.4.0
gcsfs            : 0.6.2
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.3.2
sqlalchemy       : 1.3.17
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0

</details>
"
656998425,35280,BUG: GroupBy.count() and GroupBy.sum() incorreclty return NaN instead of 0 for missing categories,smithto1,closed,2020-07-15T01:40:16Z,2020-08-07T15:21:19Z,"- [x] closes #31422 
- [x] closes #35028 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

*Behavioural Changes*
Fixing two related bugs: when grouping on multiple categoricals, `.sum()` and `.count()` would return `NaN` for the missing categories, but they are expected to return `0` for the missing categories. Both these bugs are fixed.

*Tests*
Tests were added in PR #35022 when these bugs were discovered and the tests were marked with an `xfail`. For this PR the `xfails` are removed and the tests are passing normally. As well, a few other existing tests were expecting `sum()` to return `NaN`; these have been updated so that the tests now expect to get `0` (which is the desired behaviour).

*Pivot*
The change to `.sum()` also impacts the `df.pivot_table()` if it is called with `aggfunc=sum` and is pivoted on a Categorical column with `observed=False`. This is not explicitly mentioned in either of the bugs, but it does make the behaviour consistent (i.e. the sum of a missing category is zero, not `NaN`). One test on test_pivot.py was updated to reflect this change. 
"
673949128,35581,Adding function to calculate years since a reference timestamp,bryan-woods,closed,2020-08-06T01:32:07Z,2020-08-07T15:30:47Z,"- [ ] closes #xxxx
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
634971276,34656,BUG: groupby.min has a side effect on groupby.apply,gshimansky,closed,2020-06-08T22:18:35Z,2020-08-07T16:56:13Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame(
    {
        ""col1"": [0, 1, 2, 3],
        ""col4"": [17, 13, 16, 15],
        ""col5"": [-4, -5, -6, -7],
    }
)
by=[""col4"", ""col5""]
apply_function = min

gb = df.groupby(by, as_index=True)

df1 = gb.apply(apply_function)
print(df1)

df2 = gb.min()
print(df2)

df3 = gb.apply(apply_function)
print(df3)
```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

In the code above two calls to `gb.apply(apply_function)` produce different output. The reason for this is that `groupby.min` is called before 2nd `apply` and makes its output different and incorrect.

#### Expected Output

Expected that both calls to `gb.apply(apply_function)` produce the same output.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-26-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.4
numpy            : 1.18.4
pytz             : 2019.2
dateutil         : 2.7.3
pip              : 20.1.1
setuptools       : 47.1.0
Cython           : 0.29.17
pytest           : 5.4.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : 0.4.1
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.1
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : 0.13.2
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.4.2
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : 3.6.1
tabulate         : None
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.46.0
</details>
"
621583701,34271,BUG: Same function calls on the same DataFrameGroupBy object give different results,leetschau,closed,2020-05-20T08:53:34Z,2020-08-07T16:56:13Z,"- [x] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

# Source codes

```python
import pandas as pd                                    
inp = pd.DataFrame([[1, 2, 3, 4],
                    [5, 6, 7, 8],
                    [1, 10, 11, 12],
                    [5, 14, 15, 16],
                    [1, 18, 19, 20],
                    [21, 22, 23, 24],
                    [5, 26, 27, 28]],
                columns=['group', 'fa', 'fb', 'fc'])
print('pandas version:', pd.__version__)
grps = inp.groupby('group', as_index=True)
print('Column number in all groups:',
    grps.apply(lambda x: x.shape[1]).unique())
print('Column number in all groups:',
    grps.apply(lambda x: x.shape[1]).unique())
print('ID:', id(grps))
print('Shape of first dataframe in the group:', grps.first().shape)
print('ID:', id(grps))
print('Column number in all groups:',
    grps.apply(lambda x: x.shape[1]).unique())
```
# Problem description

In above codes, same function calls `grps.apply(lambda x: x.shape[1]).unique()` give different results:

In the first 2 times before `grps.first().shape` is called, it returns 4.
While after `grps.first().shape` is called, it returns 3.

## Running output

```
Column number in all groups: [4]
Column number in all groups: [4]
ID: 140686222651664
Shape of first dataframe in the group: (3, 3)
ID: 140686222651664
Column number in all groups: [3]
```

## Expected Output

```
Column number in all groups: [4]
Column number in all groups: [4]
ID: 140686222651664
Shape of first dataframe in the group: (3, 3)
ID: 140686222651664
Column number in all groups: [4]
```
# Environments

Python 3.7.7, Ubuntu 18.04.

Output of ``pd.show_versions()``:
```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-96-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.2
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.1
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```
"
658661613,35314,BUG: GroupBy.apply() returns different results if a different GroupBy method is called first,smithto1,closed,2020-07-16T23:28:41Z,2020-08-07T16:56:25Z,"- [x] closes #34656 
- [x] closes #34271 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

_Behavioural Changes_
_.apply()_
Calls  to `self._set_group_selection` have been replaced with `with _group_selection_context(self):` for `_agg_general`, `_make_wrapper`, and `nth`. 

Previously these calls to `self._set_group_selection` created a bug in `GroupBy.apply` where calling another method before `.apply` would change the output of `.apply`. This bug is now fixed. 


_Tests_ 
One new test is added to check that the output of `.apply` is constant whether another method is called on the same grouper first. 

Two existing tests were actually dependent on the old buggy-behaviour (i.e. they called GroupBy.sum first and then expected that GroupBy.apply(sum) would exclude the index columns from the results). All of these tests have been amended in a manner that enforces the new consistent output format while preserving the existing test. 

Both of the copy-pastable examples in the linked bug-reports are fixed. "
657717188,35301,BUG: xs not working with slice,wiso,closed,2020-07-15T23:09:33Z,2020-08-07T16:58:25Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

#### Code Sample, a copy-pastable example

```python
data = """"""
C1  C2 V
A1  0  10
A1  1  20
A2  0  2
A2  1  3
B1  0  2
B2  1  3
""""""

import pandas as pd
from io import StringIO
df = pd.read_csv(StringIO(data), sep=' +').set_index(['C1', 'C2'])

df.xs(pd.IndexSlice['A1', :])
```

#### Problem description

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/***/lib/python3.7/site-packages/pandas/core/generic.py"", line 3535, in xs
    loc, new_index = self.index.get_loc_level(key, drop_level=drop_level)
  File ""/home/***/lib/python3.7/site-packages/pandas/core/indexes/multi.py"", line 2835, in get_loc_level
    raise TypeError(key)
TypeError: ('A1', slice(None, None, None))

```

also similar code produce the same problem (`df.xs(('A1', slice(None)))`). Strangely this works:
```python
df = pd.DataFrame({'a': [1, 2, 3, 1], 'b': ['a', 'b', 'c', 'd'], 'v': [2, 3, 4, 5]}).set_index(['a', 'b'])
df.xs(pd.IndexSlice[1, :])
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.7.7-100.fc31.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : it_IT.UTF-8
LOCALE           : it_IT.UTF-8

pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2019.2
dateutil         : 2.7.5
pip              : 20.1.1
setuptools       : 41.6.0
Cython           : 0.29.15
pytest           : 4.0.0
hypothesis       : None
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 7.16.1
pandas_datareader: 0.8.0
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.0
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 4.0.0
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.1
sqlalchemy       : None
tables           : 3.5.2
tabulate         : 0.8.5
xarray           : 0.12.1
xlrd             : 1.2.0
xlwt             : 1.1.2
xlsxwriter       : None
numba            : 0.48.0

</details>
"
665593320,35411,CLN: clarify TypeError for IndexSlice argument to pd.xs,arw2019,closed,2020-07-25T14:20:48Z,2020-08-07T17:19:12Z,"- [x] closes #35301 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

As per discussion in #35301 we do not support `IndexSlice` arguments to `xs`. This PR aims to clarify the `TypeError` thrown when that happens.

I created a separate issue to house the discussion re: functionality of `loc` vs `xs` (#35418)."
673013343,35552,BUG: Segmentation fault in RollingGroupby.mean() with center=True and input with odd length,wfvining,closed,2020-08-04T19:07:10Z,2020-08-07T18:09:30Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

series = pd.Series(range(1, 5))
series.groupby(series).rolling(center=True, window=3).mean()
```

#### Problem description

With an odd-length series and `center=True`, `RollingGroupby.mean()` causes a segfault. If  `center=True` is not provided, or the input series has an even length, then there is no no segfault (example below).
```python
series = pd.Series(range(1, 6))
series.groupby(series).rolling(center=True, window=3).mean()
```
Outputs:
```
1  0              NaN
2  1              NaN
3  2              NaN
4  3              NaN
5  4    1.474760e+250
dtype: float64
```

The series can be grouped in any way, but the example above seemed simplest.

#### Expected Output

Should output a Series.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.18.0-25-generic
Version          : #26-Ubuntu SMP Mon Jun 24 09:32:08 UTC 2019
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : None
pytest           : 6.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None


</details>
"
675188476,35610,Backport PR #35562 on branch 1.1.x (BUG: Ensure rolling groupby doesn't segfault with center=True),meeseeksmachine,closed,2020-08-07T18:19:30Z,2020-08-07T19:00:47Z,Backport PR #35562: BUG: Ensure rolling groupby doesn't segfault with center=True
627981537,34498,DOC: improve the .equals() docstring,jorisvandenbossche,closed,2020-05-31T15:06:47Z,2020-08-07T19:16:40Z,"The current docstring for Series/DataFrame `equals` method can use some improvement (https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.equals.html):

- "" The column headers do not need to have the same type"" is not very clear, and also doesn't apply to Series.equals. Something like ""The row/column index do not need to have the same type (as long as the values are still considered equal)"" might be better
- The ""Notes"" section is mostly duplicating the long description at the top."
628147673,34508,DOC: Docstring updated for DataFrame.equals,pandeydeepak0,closed,2020-06-01T04:44:14Z,2020-08-07T19:16:54Z,"- [ ] closes #34498 
"
673083180,35555,Merge pull request #1 from pandas-dev/master -  #34498 improve the .equals() docstring,data-RanDan,closed,2020-08-04T21:06:02Z,2020-08-07T19:19:12Z,"
#34498 improve the .equals() docstring

"
673289654,35562,BUG: Ensure rolling groupby doesn't segfault with center=True,mroeschke,closed,2020-08-05T06:31:03Z,2020-08-07T19:29:31Z,"- [x] closes #35552
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
673905954,35578,"BUG: df.shift(n, axis=1) with multiple blocks",jbrockmendel,closed,2020-08-05T23:14:29Z,2020-08-07T20:49:51Z,"- [x] closes #35488
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
672675539,35541,QST: ExtensionDtype with complex arguments,vnmabus,closed,2020-08-04T10:12:41Z,2020-08-07T21:15:32Z,"#### Question about pandas

I am trying to add support to store columns of functional data in Pandas. This kind of data requires that the associated dtype store some information, such as the discretization points for discretized functions, which is not as simple as the arguments in other ExtensionDtypes (for example, in this case would be a numpy array). This presents several challenges:

- The `ExtensionDtype` will not be hashable unless I take extra steps to make it so, hashing numpy arrays like in the Categorical dtype.
- If the name of the dtype has to include the value of the arguments it would be unreadable.
- `construct_from_string` should have to parse a lot of structured information (maybe possible using `eval`, but what is the point?). I would prefer that my `ExtensionDtype` could not be constructed from a string. However, at least one test require the dtype to be constructable from its name.

What suggestions do you have to solve these kind of problems?"
662223593,35357,PERF: BlockManager.equals blockwise,jbrockmendel,closed,2020-07-20T20:19:40Z,2020-08-07T21:32:41Z,
646285073,35014,BUG: DataFrameGroupBy.__getitem__ fails to propagate dropna,TomAugspurger,closed,2020-06-26T14:02:11Z,2020-08-07T21:33:06Z,"#### Code Sample, a copy-pastable example

```pytb
In [1]: import pandas as pd
In [2]: df = pd.DataFrame({""A"": [0, 0, 1, None], ""B"": [1, 2, 3, None]})
In [3]: gb = df.groupby(""A"", dropna=False)
In [6]: gb['B'].transform(len)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-6-3bae7d67a46f> in <module>
----> 1 gb['B'].transform(len)

~/sandbox/pandas/pandas/core/groupby/generic.py in transform(self, func, engine, engine_kwargs, *args, **kwargs)
    471         if not isinstance(func, str):
    472             return self._transform_general(
--> 473                 func, *args, engine=engine, engine_kwargs=engine_kwargs, **kwargs
    474             )
    475

~/sandbox/pandas/pandas/core/groupby/generic.py in _transform_general(self, func, engine, engine_kwargs, *args, **kwargs)
    537
    538         result.name = self._selected_obj.name
--> 539         result.index = self._selected_obj.index
    540         return result
    541

~/sandbox/pandas/pandas/core/generic.py in __setattr__(self, name, value)
   5141         try:
   5142             object.__getattribute__(self, name)
-> 5143             return object.__setattr__(self, name, value)
   5144         except AttributeError:
   5145             pass

~/sandbox/pandas/pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__()
     64
     65     def __set__(self, obj, value):
---> 66         obj._set_axis(self.axis, value)

~/sandbox/pandas/pandas/core/series.py in _set_axis(self, axis, labels, fastpath)
    422         if not fastpath:
    423             # The ensure_index call above ensures we have an Index object
--> 424             self._mgr.set_axis(axis, labels)
    425
    426     # ndarray compatibility

~/sandbox/pandas/pandas/core/internals/managers.py in set_axis(self, axis, new_labels)
    213         if new_len != old_len:
    214             raise ValueError(
--> 215                 f""Length mismatch: Expected axis has {old_len} elements, new ""
    216                 f""values have {new_len} elements""
    217             )

ValueError: Length mismatch: Expected axis has 3 elements, new values have 4 elements
```

#### Problem description

Compare that with the following

```python
In [4]: gb.transform(len)
Out[4]:
   B
0  2
1  2
2  1
3  1

In [5]: gb[['B']].transform(len)
Out[5]:
   B
0  2
1  2
2  1
3  1
```

So it's just when slicing down to a SeriesGroupBy object.

#### Expected Output

A series:

```python
Out[5]:
0  2
1  2
2  1
3  1
```"
674290285,35586,BUG: Pandas Styler cell_ids Arg,attack68,closed,2020-08-06T12:52:49Z,2020-08-07T21:35:22Z,"Pandas Styler has the following argument:

> cell_ids: bool, default True
If True, each cell will have an id attribute in their HTML tag. The id takes the form T_<uuid>_row<num_row>_col<num_col> where <uuid> is the unique identifier, <num_row> is the row number and <num_col> is the column number.

This doesn't seem to work. 

Consider the example in the docs:

```
import pandas as pd  # NOTE VERSION 1.1.0
from pandas.io.formats.style import Styler
def highlight_max(s):
    x = s == s.max()
    x = x.replace(False, '')
    x = x.replace(True, 'background-color: yellow; color: brown')
    return x
df = pd.DataFrame(data=[[0,1], [1,0]])
s = Styler(df, uuid='_', cell_ids=False)
s.apply(highlight_max)
s.render()
```
This will still render `id` css tags for all cells even though it should have been ignored on some.

#### Reason and Solution
Upon render, within the code a `ctx` defaultdict object contains the properties for each cell:

> ctx = defaultdict(<class 'list'>, {(1,0): ['background-color: yellow', 'color: brown'], (0,1): ['background-color: yellow', 'color: brown']})

On line 393 we have the condition:
```
if self.cell_ids or not (len(ctx[r, c]) == 1 and ctx[r, c][0] == """"):
    row_dict[""id""] = ""_"".join(cs[1:])
```
This fails because the `_update_ctx` function handles empty string by not adding to the `ctx` - previously I suspect it performed differently..

We should really change line 393 to:

    if self.cell_ids or (r,c) in ctx:

I added a PR for this.."
648630082,35078,BUG: DataFrameGroupBy.__getitem__ fails to propagate dropna,arw2019,closed,2020-07-01T02:50:08Z,2020-08-07T21:53:39Z,"- [x] closes #35014 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
160946165,13475,BUG: index.name not preserved in concat in case of unequal object index,dllahr,closed,2016-06-17T18:17:26Z,2020-08-07T22:22:57Z,"xref #13742 for addl cases.

```
In [23]: df1 = pd.DataFrame({'a':[1,2]}, index=pd.Index(['a', 'b'], name='idx'))

In [24]: df2 = pd.DataFrame({'b':[2,3]}, index=pd.Index(['b', 'c'], name='idx'))

In [26]: pd.concat([df1, df2], axis=1)
Out[26]:
     a    b
a  1.0  NaN
b  2.0  2.0
c  NaN  3.0

In [27]: print pd.concat([df1, df2], axis=1).index.name
None
```

So the issue seems to be with a string index that is not equal, as when the index of the two frames is equal (no NaNs are introduced), the name is kept and also when using numerical indexes, see https://github.com/pydata/pandas/issues/13475#issuecomment-232310977

---

When I use the concat function with input dataframes that have index.name assigned, sometimes the resulting dataframe has the index.name assigned, sometimes it does not.

I ran the code below from the python interpreter, using a conda environment with pandas-0.18.1

I don't see any odd / extra characters around the ""pert_well"" column in the files between the files.
#### Code Sample, a copy-pastable example if possible

``` python
import pandas

a_data = """"""x_amount_mg x_annotation    x_mmoles_per_liter  mfc_plate_name  x_avg_mol_weight    x_volume_ul pert_mfc_desc   pert_iname  x_purity    pert_id_vendor  pert_well   pert_vehicle    pert_mfc_id x_smiles    x_mg_per_ml pert_dose_unit  pert_dose   pert_id pert_plate  pert_type
0.04784 ACCEPT  10.0    B-REPO-01-B64-101   405.4084    11  Taltirelin  Taltirelin  86.52   HY-B0596    C18 DMSO    BRD-K93869735-001-01-1  CN1C(=O)C[C@H](NC1=O)C(=O)N[C@@H](Cc1cnc[nH]1)C(=O)N1CCC[C@H]1C(N)=O    4.054084    um  20.0    BRD-K93869735   PMEL008 trt_cp""""""

b_data = """"""pert_well   pert_2_type pert_2_id   pert_2_mfc_id   pert_2_mfc_desc pert_2_id_vendor    pert_2_iname    pert_2_dose pert_2_dose_unit    pert_2_vehicle  pert_3_type pert_3_idpert_3_mfc_id  pert_3_mfc_desc pert_3_id_vendor    pert_3_iname    pert_3_dose pert_3_dose_unit    pert_3_vehicle
A01 ctl_vehicle DMSO    DMSO    DMSO    -666    DMSO    -666    -666    -666    ctl_untrt   CMAP-000    -666    UnTrt   -666    -666    -666    -666    -666""""""

d_data = """"""x_amount_mg x_annotation    x_mmoles_per_liter  mfc_plate_name  x_avg_mol_weight    x_volume_ul pert_mfc_desc   pert_iname  x_purity    pert_id_vendor  pert_well   pert_vehicle    pert_mfc_id x_smiles    x_mg_per_ml pert_dose_unit  pert_dose   pert_id pert_plate  pert_type
0.0 -666    -666    B-REPO-01-B64-107   -666    0   -666    -666    -666    -666    A01 -666    -666    -666    -666    -666    -666    CMAP-000    PMEL001 ctl_untrt""""""

a = pandas.read_csv(StringIO(a_data), sep=""\t"", index_col=""pert_well"")
b = pandas.read_csv(StringIO(b_data), sep=""\t"", index_col=""pert_well"")
c = pandas.concat([a,b], axis=1)
c.index

d = pandas.read_csv(StringIO(d_data), sep=""\t"", index_col=""pert_well"")
e = pandas.concat([d,b], axis=1)
e.index
```

results:

``` python
Index([u'A01', u'A02', u'A03', u'A04', u'A05', u'A06', u'A07', u'A08', u'A09',
       u'A10',
       ...
       u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23',
       u'P24'],
      dtype='object', length=384)

Index([u'A01', u'A02', u'A03', u'A04', u'A05', u'A06', u'A07', u'A08', u'A09',
       u'A10',
       ...
       u'P15', u'P16', u'P17', u'P18', u'P19', u'P20', u'P21', u'P22', u'P23',
       u'P24'],
      dtype='object', name=u'pert_well', length=384)
```
#### Expected Output

c.index.name should be ""pert_well""
#### output of `pd.show_versions()`
## INSTALLED VERSIONS

commit: None
python: 2.7.11.final.0
python-bits: 64
OS: Linux
OS-release: 2.6.32-573.7.1.el6.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: C

pandas: 0.18.1
nose: None
pip: 8.1.2
setuptools: 23.0.0
Cython: None
numpy: 1.11.0
scipy: None
statsmodels: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.5.3
pytz: 2016.4
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
boto: None
pandas_datareader: None

[PMEL_input_files_for_pandas_issue.zip](https://github.com/pydata/pandas/files/321051/PMEL_input_files_for_pandas_issue.zip)
"
660345939,35338,BUG: assign consensus name to index union in array case GH13475,iamlemec,closed,2020-07-18T19:53:49Z,2020-08-07T22:23:03Z,"- [x] closes #13475
- [x] tests added / passed (except known datetime64 issue in #35080)
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
651136748,35129,support binary file handles in to_csv,twoertwein,closed,2020-07-05T20:07:33Z,2020-08-07T22:45:53Z,"- [x] fixes #19827, fixes #35058, fixes #23854 *, fixes #13068 *, and fixes #22555
- [x] 3 tests added
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The first commit addresses https://github.com/pandas-dev/pandas/issues/35058#issuecomment-653844610: python's `open` cannot take an `encoding` argument when `mode` contains a `'b'` (opened in binary mode). This avoids an error when executing `df.to_csv(""output.csv"", mode=""w+b"")`.

The second commit fixes #35058, #19827,  #23854 *, and #13068 *: `to_csv` supports file handles in binary mode if `mode` contains a `b` and it honors `encoding`. `to_csv` re-invented a lot that was already done in `get_handle`. Let `get_handle` do the heavy lifting and remove all special cases from `to_csv`.

The third commit fixes #22555: some compression algorithms did not set the mode to be writeable for file handles. Together with the re-factoring in the second commit, it is now possible to write to binary file handles with compression!

*requesting an encoding for a non-binary file handles through `to_csv` still doesn't work but imho also doesn't make sense: specify the encoding yourself when opening the file or use a binary file handle"
602222474,33613,Fixed bug. Added in check for ufunc and evaluates inner expression be…,SurajH1,closed,2020-04-17T20:43:11Z,2020-08-07T22:56:51Z,"…fore evaluating outer expression

- [x] closes  #24670 
- [x] passes `black pandas`
"
667435081,35441,BUG: GroupBy.apply() throws erroneous ValueError with duplicate axes,smithto1,closed,2020-07-28T23:02:37Z,2020-08-07T23:29:39Z,"- [x] closes #16646 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

*Behavioural Change*
`GroupBy.apply` would sometimes throw an erroneous `ValueError: cannot reindex from duplicate axis` because when it checked if the output was mutated from its original shape it would only check the index, when sometimes it should check the columns. 

*Tests*
One new test added on `test_apply.py` which fails on master because of the bug; passes with this PR. One existing test was marked *xfail* but the *xfail* is now removed. One existing test previously had to manipulate the expected index, but that is no longer necessary. 

All of the copy-pasteable examples from #16646 are fixed with this PR. "
668278161,35473,REGR: Fix conversion of mixed dtype DataFrame to numpy str,dsaxton,closed,2020-07-30T01:20:16Z,2020-08-08T01:14:35Z,"- [x] closes https://github.com/pandas-dev/pandas/issues/35455
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
675325326,35615,DOC: compression support for file objects in to_csv,twoertwein,closed,2020-08-07T22:45:08Z,2020-08-08T01:27:40Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I made a mistake in #35129: `to_csv` supports the compression argument for file objects in binary mode (not for file objects in text/non-binary mode)."
622495438,34291,TYP: Add type hints to pd.read_html,topper-123,closed,2020-05-21T13:22:05Z,2020-08-08T08:30:06Z,
650888465,35122,CLN: remove kwargs in Index.format,topper-123,closed,2020-07-04T13:06:52Z,2020-08-08T08:30:27Z,"Removes kwargs in ``Index.format`` and subclasses. Also changes ``Index._format_with_header`` to have parameter ``na_rep`` not have a default value (in order to not have default value set in two locations).

Related to #35118."
650779730,35118,CLN: Index._format_with_header (remove kwargs etc.),topper-123,closed,2020-07-03T22:10:37Z,2020-08-08T08:30:29Z,"Refactor of ``Index._format_with_header`` and subclass methods:
* remove ``kwargs`` for cleaner signature.
* Add return type
* Move functionality specific to ``CategoricalIndex`` to that class.
"
619261561,34200,CLN/TYP: Groupby agg methods,topper-123,closed,2020-05-15T21:20:41Z,2020-08-08T08:30:55Z,"Same as #34178. I had some trouble rebasing that PR, so just made a new PR instead.

This takes care of the comment in #34178 about staticmethod."
578906697,32601,TST: assert monotonic is True,topper-123,closed,2020-03-10T22:52:20Z,2020-08-08T08:31:25Z,Makes tests for monoticity stricter. Follow-up to #23256.
675485343,35619,Backport PR #35588 on branch 1.1.x (BUG: fix styler cell_ids arg so that blank style is ignored on False),meeseeksmachine,closed,2020-08-08T08:13:08Z,2020-08-08T09:21:19Z,Backport PR #35588: BUG: fix styler cell_ids arg so that blank style is ignored on False
667809198,35455,REGR: DataFrame.to_numpy(dtype=str) raises RuntimeError in pandas 1.1.0,aciba90,closed,2020-07-29T12:33:12Z,2020-08-08T09:43:11Z,"- [x ] I have checked that this issue has not already been reported.

- [x ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame(data=[[pd.Timestamp('2011-07-03 00:00:00'), 6628480.0]])
print(df.to_numpy(dtype=str))
```

#### Problem description

I try to convert the DataFrame to a Numpy array with dtype equal to str and I got the next error.

```python-traceback
Traceback (most recent call last):
  File ""/main.py"", line 4, in <module>
    df.to_numpy(dtype=str)
  File ""/venv/lib/python3.7/site-packages/pandas/core/frame.py"", line 1371, in to_numpy
    transpose=self._AXIS_REVERSED, dtype=dtype, copy=copy, na_value=na_value
  File ""/venv/lib/python3.7/site-packages/pandas/core/internals/managers.py"", line 826, in as_array
    arr = self._interleave(dtype=dtype, na_value=na_value)
  File ""/venv/lib/python3.7/site-packages/pandas/core/internals/managers.py"", line 864, in _interleave
    result[rl.indexer] = arr
RuntimeError: The string provided for NumPy ISO datetime formatting was too short, with length 1
```

#### Expected Output
```python
[['2011-07-03 00:00:00' '6628480.0']]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.104-microsoft-standard
Version          : #1 SMP Wed Feb 19 06:37:35 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
674371938,35588,BUG: fix styler cell_ids arg so that blank style is ignored on False,attack68,closed,2020-08-06T14:47:19Z,2020-08-08T09:44:21Z,"- [x] closes #35586 
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
675338649,35617,Backport PR #35473 on branch 1.1.x (REGR: Fix conversion of mixed dtype DataFrame to numpy str),meeseeksmachine,closed,2020-08-07T23:17:28Z,2020-08-08T09:46:39Z,Backport PR #35473: REGR: Fix conversion of mixed dtype DataFrame to numpy str
675492741,35621,CI: Linux py36_locale failures with pytest DeprecationWarning,simonjayhawkins,closed,2020-08-08T09:11:12Z,2020-08-08T09:50:57Z,xref #35620
667655555,35447,Updated chunksize docstring for DataFrame.to_csv(),Mandera,closed,2020-07-29T08:31:46Z,2020-08-08T11:37:08Z,"Added the documentation for what happens if chunksize is left as None.
Is there a good way to make this DRY somehow?"
675497786,35623,Backport PR #35621 on branch 1.1.x (CI: Linux py36_locale failures with pytest DeprecationWarning),meeseeksmachine,closed,2020-08-08T09:50:46Z,2020-08-08T12:23:06Z,Backport PR #35621: CI: Linux py36_locale failures with pytest DeprecationWarning
304073469,20144,Updated doc for pandas.Series.str.find method,ChiragSehra,closed,2018-03-10T14:10:35Z,2020-08-08T14:11:27Z,"Checklist for the pandas documentation sprint (ignore this if you are doing
an unrelated PR):

- [x] PR title is ""DOC: update the pandas.Series.str.find docstring""
- [x] The validation script passes: `scripts/validate_docstrings.py pandas.Series.str.find`
- [x] The PEP8 style check passes: `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] The html version looks good: `python doc/make.py --single pandas.Series.str.find`
- [x] It has been proofread on language by another sprint participant

Please include the output of the validation script below between the ""```"" ticks:

```
################################################################################
###################### Docstring (pandas.Series.str.find) ######################
################################################################################

Retrieve least index of substring.

Return lowest indexes in each strings in the Series/Index
where the substring is fully contained between [start:end].
Return -1 on failure. Equivalent to standard :meth:`str.find`.

Parameters
----------
sub : str
    Substring being searched.
start : int
    Left edge index.
end : int
    Right edge index.

Returns
-------
found : Series/Index of integer values

Examples
--------
>>> s = pd.Series('Finding with pandas')
>>> s.str.find('wi')
0    8
dtype: int64

Returns -1 if the substring is not found in S

>>> s.str.find('wi', start=0, end=5)
0   -1
dtype: int64

See Also
--------
rfind : Return highest indexes in each strings

################################################################################
################################## Validation ##################################
################################################################################
```

If the validation script still gives errors, but you think there is a good reason
to deviate in this case (and there are certainly such cases), please state this
explicitly.


Checklist for other PRs (remove this part if you are doing a PR for the pandas documentation sprint):

- [ ] closes #xxxx
- [x] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
675530911,35629,DOC: docstrings for __array_wrap__,simonjayhawkins,closed,2020-08-08T14:08:27Z,2020-08-08T16:33:26Z,non-code changes broken-off #35334

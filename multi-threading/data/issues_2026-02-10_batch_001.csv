id,number,title,user,state,created_at,updated_at,body
549229154,30989,Update _json.py,copeland3300,closed,2020-01-13T22:53:55Z,2020-01-14T09:40:54Z,"Without this change, use of the chunksize argument in read_json causes the following error:

`TypeError: initial_value must be str or None, not bytes`

- [x] closes #28906
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
549467426,30996,STY: Whitespaces placed at the beginning instead at the end of a line,ShaharNaveh,closed,2020-01-14T10:20:39Z,2020-01-14T11:09:16Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
430267234,26026,Problem with reading .dat file with missing data ,Divyamurali1996,closed,2019-04-08T05:54:28Z,2020-01-14T11:57:07Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
df=pd.read_table('out_missingdata.dat',sep='\s+',header=None)
print(df.iloc[:37,:])

```
#### Problem description
I have a .dat file with three columns with some columns having missing values(empty) and delimiter is a whitespace.When I am printing dataframe data is getting shifted to empty spaces and NaN is displayed at last column.I don't know what is mistake in code.

#### Expected Output
24   3869089  64529089  1296.0
25     NaN       64513024   1296     
26   3876961  64496961  1296.0
27   3880900  64480900  1296.0
28   3884841  64464841  1296.0
29   3888784  64448784  1296.0
30   3892729  64432729  1296.0
31   3896676  64416676  1296.0
32   3900625  64400625  1296.0
33   3904576  64384576  1296.0
34   3908529          NaN   1296
35   3912484  64352484  1296.0
36   3916441  64336441  1296.0

#### Output of ``pd.show_versions()``
24   3869089  64529089  1296.0
25   64513024      1296     NaN
26   3876961  64496961  1296.0
27   3880900  64480900  1296.0
28   3884841  64464841  1296.0
29   3888784  64448784  1296.0
30   3892729  64432729  1296.0
31   3896676  64416676  1296.0
32   3900625  64400625  1296.0
33   3904576  64384576  1296.0
34   3908529      1296     NaN
35   3912484  64352484  1296.0
36   3916441  64336441  1296.0

"
548966959,30966,str accessor functions returns float(NaN) instead of pd.NA,tsvikas,closed,2020-01-13T14:28:56Z,2020-01-14T12:39:48Z,"#### Code Sample

```python
import pandas as pd
s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'], dtype=""string"")
type(s.str.lower()[5])  # returns <class 'float'>
```
#### Problem description
the `str` accessor, when working on string-typed series, should return a string-typed series, which should be an array of [`string`, `pd.NA`] only, but it seems that some functions (see list below) can return series that contains `float('nan')`.

#### Affected functions
As of now, I found these str accessor functions to be affected:
`upper` `lower` `replace`
also, `extract(expand=False)` on a string type series returns an object type series, which seems unintended as well.

#### Expected Output
`<class 'pandas._libs.missing.NAType'>`
#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.0.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.0.0-38-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_IL
LOCALE           : en_IL.UTF-8

pandas           : 1.0.0rc0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 18.1
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```
</details>
"
549130517,30980,API: Disallow NaN in StringArray constructor,TomAugspurger,closed,2020-01-13T19:21:36Z,2020-01-14T12:39:51Z,"Closes https://github.com/pandas-dev/pandas/issues/30966

~There were a few ways we could have done this. I opted for this way since it still only requires a single pass over the data to validate the values. Other ways like checking for `np.isnan` after checking `is_string_array` would have required a second pass.~

I changed the implementation in subsequent commits. The basic idea is the same, don't allow NaN in the array passed to StringArray, so that we only make a single pass over the data. We do this by changing `StringValidator.is_valid_null` to only allow NA. This required a small change to `PandasArray._from_sequence`, since previously we relied on creating a temporarily invalid StringArray before doing an inplace `__setitem__` to replace NaNs with NA.

cc @tsvikas.
"
524345298,29684,OutOfBoundsDatetime for big-endian np.datetime64 arrays,lr4d,closed,2019-11-18T12:44:53Z,2020-01-14T12:52:26Z,"#### Code Sample, a copy-pastable example if possible

```python
ipdb> value = np.array([np.datetime64(1, 'ms')], dtype=""<M8[ms]"")
ipdb> pd.Series(value)
0   1970-01-01 00:00:00.001
dtype: datetime64[ns]
ipdb> value = np.array([np.datetime64(1, 'ms')], dtype="">M8[ms]"")
ipdb> pd.Series(value)
*** pandas._libs.tslibs.np_datetime.OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 2285384-04-02 23:52:07
```
#### Problem description

Seems like something weird is going on when converting the big-endian timestamp, which does not happen for the little-endian one.

#### Expected Output
The same as for the little-endian data type.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

ipdb>  pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.5.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.6.0.post20191101
Cython           : None
pytest           : 5.0.1
hypothesis       : 4.44.2
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
549541290,31000,Backport PR #30980 on branch 1.0.x (API: Disallow NaN in StringArray constructor),meeseeksmachine,closed,2020-01-14T12:40:21Z,2020-01-14T13:24:42Z,Backport PR #30980: API: Disallow NaN in StringArray constructor
549590993,31004,docs: removed wrong value in `pd.NA ** 0`,tsvikas,closed,2020-01-14T14:07:18Z,2020-01-14T14:14:45Z,"`pd.NA ** 0` returns 1. The docs fixed to auto-calculate these values.

- [x] closes #31003
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
549085413,30976,BUG: ensure_datetime64ns with bigendian array,jbrockmendel,closed,2020-01-13T17:47:21Z,2020-01-14T16:14:06Z,"- [x] closes #29684
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
548371499,30909,CLN: remove geopandas compat code,jbrockmendel,closed,2020-01-11T03:00:58Z,2020-01-14T16:24:48Z,CC @jorisvandenbossche its safe to remove this now right?
549663866,31009,Backport PR #30959 on branch 1.0.x (ENH: Add Stata 119 writer),meeseeksmachine,closed,2020-01-14T16:02:44Z,2020-01-14T16:33:50Z,Backport PR #30959: ENH: Add Stata 119 writer
318984632,20887,`drop_duplicates` on non-existent column should raise warning,ConstantinoSchillebeeckx,closed,2018-04-30T18:16:28Z,2020-01-14T18:18:42Z,"```python
import pandas as pd

dat = pd.DataFrame([
    {'a':1,'b':2,'c':3},
    {'a':1,'b':2,'c':3}
])

dat.drop_duplicates(subset=['a','d'])
```
```python
	a	b	c
0	1	2	3

```
#### Problem description

When using `drop_duplicates()` with the `subset` option, a warning should probably be raised when trying to subset on a column that doesn't exist.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.3.final.0
python-bits: 64
OS: Darwin
OS-release: 17.5.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.22.0
pytest: 3.3.1
pip: 10.0.1
setuptools: 28.8.0
Cython: None
numpy: 1.13.3
scipy: 1.0.1
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.2.2
openpyxl: None
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0.1
sqlalchemy: 1.1.15
pymysql: None
psycopg2: 2.7 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
549733973,31011,DOC: Changed links to sphinx documentation in contributing.rst file,tonywu1999,closed,2020-01-14T18:12:43Z,2020-01-14T20:21:05Z,"GH31010: fixed this issue where inactive links for sphinx documentation were replaced with active links in the contributing documentation.
"
549667396,31010,DOC: contributing.rst sphinx link leads to 400 bad request,tonywu1999,closed,2020-01-14T16:08:32Z,2020-01-14T20:22:29Z,"When reading through the contributing guidelines (https://dev.pandas.io/docs/development/contributing.html), there are links to the Sphinx documentation and an introduction to reST.

_Sphinx documentation_:  http://sphinx.pocoo.org/
_Introduction to reST_:  http://sphinx.pocoo.org/rest.html

However, these links are not working and are leading to 400 bad requests.

I propose we replace these links with the following links:
_Sphinx documentation_:  http://www.sphinx-doc.org/en/master/
_Introduction to reST_:  https://www.sphinx-doc.org/en/master/usage/restructuredtext/index.html

I think this will be an easy fix, editing the links in the contributing.rst file in the documentation folder.


"
549804078,31014,"Revert ""DOC: removed wrong value of `pd.NA ** 0`""",TomAugspurger,closed,2020-01-14T20:36:52Z,2020-01-14T20:39:25Z,"Reverts pandas-dev/pandas#31005

Made to the wrong branch."
549796209,31012,Backport PR #31011 on branch 1.0.x (DOC: Changed links to sphinx documentation in contributing.rst file),meeseeksmachine,closed,2020-01-14T20:21:03Z,2020-01-14T21:00:49Z,Backport PR #31011: DOC: Changed links to sphinx documentation in contributing.rst file
549804933,31015,DOC: Fixed documented value of `pd.NA ** 0` (#31005),TomAugspurger,closed,2020-01-14T20:38:39Z,2020-01-14T21:33:15Z,"(cherry picked from commit da04c9a8fe6ddbc7028e8721c57d3e62d1da11e3)

Closes https://github.com/pandas-dev/pandas/issues/31003"
549570647,31003,incorrect docs: pd.NA ** 0 should return 1,tsvikas,closed,2020-01-14T13:33:21Z,2020-01-14T21:33:15Z,"`pd.NA ** 0` returns 1. which is correct, as far as I understand <sup>1</sup>
However, the [documents, line 832](https://dev.pandas.io/docs/user_guide/missing_data.html) incorrectly claims that `pd.NA ** 0` returns 0.

----
<sup>1</sup> although one can ask if `NULL**0` should behave like `NAN**0`"
549831251,31019,Backport PR #31015 on branch 1.0.x (DOC: Fixed documented value of `pd.NA ** 0` (#31005)),meeseeksmachine,closed,2020-01-14T21:33:28Z,2020-01-14T22:16:33Z,Backport PR #31015: DOC: Fixed documented value of `pd.NA ** 0` (#31005)
548641792,30949,DOC: Clarified documentation for convert_dates and use_default_dates params,amilbourne,closed,2020-01-12T23:03:40Z,2020-01-14T23:25:06Z,"This hopefully clarifies the documentation for the behaviour of the convert_dates and keep_default_dates parameters on the read_json function.

I recently had to use this function and found the existing docs a bit confusing.  I actually created a truth table (below), but this is probably a bit much for the docs.

| convert_dates | keep_default_dates | Result                      |
| ------------- | ------------------ | --------------------------- |
| True          | True               | Convert default date fields |
| True          | False              | Don’t convert anything |
| False         | True               | Don’t convert anything |
| False         | False              | Don’t convert anything |
| List          | True               | Convert listed and default date fields |
| List          | False              | Convert just listed date fields |

I hope you agree that my changes make the behaviour clearer - but perhaps people will disagree..."
545432157,30710,CI: Fix numpydev build,alimcmaster1,closed,2020-01-05T15:33:20Z,2020-01-14T23:42:25Z,"- [x] closes #30709

hmm.. im not sure on this. But don't see any usage of this ret value.

The C extensions now build with this change

Potentially related change on the numpy side:
https://github.com/numpy/numpy/pull/15232

cc. @WillAyd @jreback "
545324147,30693,CI: Fix pytest junit_family warnings,alimcmaster1,closed,2020-01-04T19:40:42Z,2020-01-14T23:43:05Z,"- [x] closes #30433

As per https://docs.pytest.org/en/latest/deprecations.html#junit-family-default-value-change-to-xunit2

This will mean we produce xml output with xsd as per: (opposed to old legacy v1)
https://github.com/jenkinsci/xunit-plugin/blob/xunit-2.3.2/src/main/resources/org/jenkinsci/plugins/xunit/types/model/xsd/junit-10.xsd

Done following check
- [x] Warning no longer present in builds -> checked [here](https://travis-ci.org/pandas-dev/pandas/jobs/632732296?utm_medium=notification&utm_source=github_status)
- [x] CodeCov unaffected since it uses the output xml -> [here](https://codecov.io/gh/pandas-dev/pandas/compare/b29d58d1a3e0ebb1ed3bf3bed1e1cf4848092c77...2a2d21a14a78e028e8e9a9c3bbf815ea76501036)
"
549898446,31026,Backport PR #31024 on branch 1.0.x (CI: Travis Remove Unrequired Exclude),meeseeksmachine,closed,2020-01-15T00:36:26Z,2020-01-15T01:38:36Z,Backport PR #31024: CI: Travis Remove Unrequired Exclude
549028180,30969,str.extract(expand=False) on a string typed series returns an object typed dataframe,tsvikas,closed,2020-01-13T16:04:47Z,2020-01-15T02:56:36Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
df = pd.Series(['a1', 'b2', 'c3'], dtype=""string"").str.extract(r'([ab])(\d)', expand=False)
df.dtypes
```
returns
```
0    object
1    object
dtype: object
```
#### Problem description
the str accessor, when working on string-typed series, should return a string-typed dataframe. It seems that `extract(expand=False)` returns an object-typed dataframe

see @TomAugspurger in https://github.com/pandas-dev/pandas/issues/30966#issuecomment-573724531


#### Expected Output
```
0    string
1    string
dtype: object
```

#### Output of ``pd.show_versions()``

<details>


```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.0.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.0.0-38-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_IL
LOCALE           : en_IL.UTF-8

pandas           : 1.0.0rc0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 18.1
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```
</details>
"
549818960,31018,BUG: Preserve string dtype in extract,TomAugspurger,closed,2020-01-14T21:08:01Z,2020-01-15T02:56:40Z,"specifically with multiple capture groups and expand=False

Closes https://github.com/pandas-dev/pandas/issues/30969"
549857238,31022,doc: update copyright year,simon04,closed,2020-01-14T22:32:47Z,2020-01-15T03:05:05Z,
549803040,31013,BUG: pivot_table with multi-index columns only fails,charlesdong1991,closed,2020-01-14T20:34:53Z,2020-01-15T03:19:30Z,"- [x] closes #17038
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
549849691,31020,CLN: remove checks for inferred_dtype==unicode,jbrockmendel,closed,2020-01-14T22:14:59Z,2020-01-15T03:22:54Z,
549904292,31027,REF: implement Block._split_op_result,jbrockmendel,closed,2020-01-15T00:59:10Z,2020-01-15T03:38:03Z,"Broken off from a branch working on arithmetic performance for `op(df1, df2)`, two changes here:

- implement _split_op_result, discussed before [citation needed]
- change an unnecessarily-deep copy to non-deep"
549935022,31030,Backport PR #31018 on branch 1.0.x (BUG: Preserve string dtype in extract),meeseeksmachine,closed,2020-01-15T02:56:49Z,2020-01-15T04:11:23Z,Backport PR #31018: BUG: Preserve string dtype in extract
244359996,17038,BUG/API: pivot_table with multi-index columns only,chris-b1,closed,2017-07-20T13:19:56Z,2020-01-15T08:01:04Z,"#### Code Sample, a copy-pastable example if possible

```python
In [21]: df = pd.DataFrame({'k': [1, 2, 3], 'v': [4, 5, 6]})

In [22]: df.pivot_table(values='v', columns='k')
Out[22]: 
k  1  2  3
v  4  5  6

In [23]: df.pivot_table(values='v', index='k')
Out[23]: 
   v
k   
1  4
2  5
3  6

In [24]: df2 = pd.DataFrame({'k1': [1, 2, 3], 'k2': [1, 2, 3], 'v': [4, 5, 6]})

In [25]: df2.pivot_table(values='v', index=('k1','k2'))
Out[25]: 
       v
k1 k2   
1  1   4
2  2   5
3  3   6

In [26]: df2.pivot_table(values='v', columns=('k1','k2'))
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-26-80d7fdeb9743> in <module>()
----> 1 df2.pivot_table(values='v', columns=('k1','k2'))

~\Anaconda\envs\py36\lib\site-packages\pandas\core\reshape\pivot.py in pivot_table(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name)
    172     # discard the top level
    173     if values_passed and not values_multi and not table.empty and \
--> 174        (table.columns.nlevels > 1):
    175         table = table[values[0]]
    176 

~\Anaconda\envs\py36\lib\site-packages\pandas\core\generic.py in __getattr__(self, name)
   3075         if (name in self._internal_names_set or name in self._metadata or
   3076                 name in self._accessors):
-> 3077             return object.__getattribute__(self, name)
   3078         else:
   3079             if name in self._info_axis:

AttributeError: 'Series' object has no attribute 'columns'
```
#### Expected Output
No error, symmetrical between rows/columns and single/multi case

#### Output of ``pd.show_versions()``
`pandas 0.20.2`"
548413386,30915,ENH/TST: Allow more keywords to ensure_clean,gfyoung,closed,2020-01-11T10:37:17Z,2020-01-15T08:15:58Z,"These keywords will be passed through to `tempfile` constructor functions.

Follow-up:

https://github.com/pandas-dev/pandas/pull/30771#discussion_r363684666"
545477803,30717,REF: move sharable methods to ExtensionIndex,jbrockmendel,closed,2020-01-05T22:47:12Z,2020-01-15T08:26:11Z,
536062304,30194,TST: Split test_offsets.py,Raalsky,closed,2019-12-10T23:58:46Z,2020-01-15T09:21:48Z,"- [x] closes #27085
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
546922378,30814,BLD: More lightweight mypy pre-commit hook,xhochy,closed,2020-01-08T15:10:33Z,2020-01-15T09:49:50Z,"Fixes #30811 
This now will only detect typing problems in the files you have edited. This may still lead to typing problems detected in CI but is hopefully a compromise between speed and early detection of issues before they hit public CI. One line changes in a single file still stay for me in the <1s mark.

cc @gfyoung @jorisvandenbossche "
524021941,29676,CI: Clipboard Test Failures on Travis,alimcmaster1,closed,2019-11-17T18:12:01Z,2020-01-15T13:40:35Z,"```
[gw1] linux -- Python 3.6.6 /home/travis/miniconda3/envs/pandas-dev/bin/python
data = '👍...'
    @pytest.mark.single
    @pytest.mark.clipboard
    @pytest.mark.skipif(not _DEPS_INSTALLED, reason=""clipboard primitives not installed"")
    @pytest.mark.parametrize(""data"", [""\U0001f44d..."", ""Ωœ∑´..."", ""abcd...""])
    def test_raw_roundtrip(data):
        # PR #25040 wide unicode wasn't copied correctly on PY3 on windows
        clipboard_set(data)
>       assert data == clipboard_get()
E       AssertionError: assert '👍...' == ''
E         - 👍...
pandas/tests/io/test_clipboard.py:264: AssertionError
```
Examples:

https://travis-ci.org/pandas-dev/pandas/jobs/613141276?utm_medium=notification&utm_source=github_status

https://travis-ci.org/pandas-dev/pandas/builds/613153224?utm_source=github_status&utm_medium=notification

Potentially related change:
https://github.com/pandas-dev/pandas/pull/28531"
550193230,31042,Backport PR #29712 on branch 1.0.x (CI: Fix clipboard problems),meeseeksmachine,closed,2020-01-15T13:41:20Z,2020-01-15T14:14:38Z,Backport PR #29712: CI: Fix clipboard problems
550214620,31044,Backport PR #30905 on branch 1.0.x (BUG: SystemError in df.sum),meeseeksmachine,closed,2020-01-15T14:19:29Z,2020-01-15T15:03:48Z,Backport PR #30905: BUG: SystemError in df.sum
548336668,30905,BUG: SystemError in df.sum,jbrockmendel,closed,2020-01-10T23:21:16Z,2020-01-15T16:12:40Z,"- [x] closes #30886
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
392002961,24329,BUG: Timestamp(Timestamp(Ambiguous time)) modifies .value with dateutil tz,mroeschke,closed,2018-12-18T05:31:27Z,2020-01-15T16:32:37Z,"Pretty obscure bug, but this seems fishy:

```
In [7]: pd.__version__
Out[7]: '0.24.0.dev0+1300.ge0a68076a.dirty'

# Ambiguous time
In [8]: t = pd.Timestamp(1382835600000000000, tz='dateutil/Europe/London')

# Repr is consistent
In [11]: t
Out[11]: Timestamp('2013-10-27 01:00:00+0100', tz='dateutil//usr/share/zoneinfo/Europe/London')

In [12]: pd.Timestamp(t)
Out[12]: Timestamp('2013-10-27 01:00:00+0100', tz='dateutil//usr/share/zoneinfo/Europe/London')

# .value changes
In [13]: t.value
Out[13]: 1382835600000000000

In [14]: pd.Timestamp(t).value
Out[14]: 1382832000000000000

```

pytz timezones behave consistently though

```
In [15]: t = pd.Timestamp(1382835600000000000, tz='Europe/London')

In [16]: t
Out[16]: Timestamp('2013-10-27 01:00:00+0000', tz='Europe/London')

In [17]: pd.Timestamp(t)
Out[17]: Timestamp('2013-10-27 01:00:00+0000', tz='Europe/London')

In [18]: t.value
Out[18]: 1382835600000000000

In [19]: pd.Timestamp(t).value
Out[19]: 1382835600000000000
```

The fact that the repr between dateutil timezones and pytz timezones don't match can be possible be seen in a change in dateutil somewhere around 2.6? But the main issue that is `.value` changes.

https://github.com/pandas-dev/pandas/blob/216986d4691297d5cfec33b5c62be7890b9a54d7/pandas/tests/indexes/datetimes/test_timezones.py#L564-L571"
548484167,30930,PERF: RangeIndex.get_loc,jbrockmendel,closed,2020-01-11T21:47:01Z,2020-01-15T16:40:21Z,"Small simplification gives a small speedup

```
In [2]: rng = pd.Index(range(10**5)) 

In [3]: %timeit rng.get_loc(5.0)                                                
6.38 µs ± 381 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)  <-- master
1.15 µs ± 40.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  <-- PR

In [5]: %timeit rng.get_loc(5)                                                  
845 ns ± 5.71 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  <--master
In [5]: %timeit rng.get_loc(5)                                                                                                                                                                            
860 ns ± 16.7 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  <-- PR

In [9]: def foo(): 
   ...:     try: 
   ...:         return rng.get_loc(None) 
   ...:     except KeyError: 
   ...:         pass 
   ...:                                                                         

In [10]: %timeit foo()                                                          
6.72 µs ± 656 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)  <-- master
1.11 µs ± 94 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  <-- PR
```"
548505110,30935,CLN: remove unused NDFrame methods,jbrockmendel,closed,2020-01-12T01:31:18Z,2020-01-15T16:41:09Z,
548630881,30948,REF: do all casting _before_ call to DatetimeEngine.get_loc,jbrockmendel,closed,2020-01-12T21:35:59Z,2020-01-15T16:51:32Z,"Fixes: incorrectly letting integers or mismatched types through.

Removes: maybe_datetimelike_to_i8, along with the recently-removed _to_M8 and he hopefully soon-removed pydt_to_i8 (xred #30854) i think we'll be rid of our kludgy datetime casting functions.

Performance neutral:

```
       before           after         ratio
     [28e909c6]       [890204ca]
     <master>         <cln-maybe_datetimelike_to_i8>
+         459±3μs        694±300μs     1.51  groupby.GroupByMethods.time_dtype_as_field('int', 'tail', 'transformation')
+      1.84±0.2μs         2.38±2μs     1.29  index_cached_properties.IndexCache.time_shape('Float64Index')
+     1.09±0.07μs      1.27±0.09μs     1.16  index_cached_properties.IndexCache.time_inferred_type('Float64Index')
+     1.43±0.01μs       1.63±0.2μs     1.14  tslibs.timestamp.TimestampConstruction.time_fromordinal
+         510±7μs         576±60μs     1.13  ctors.SeriesConstructors.time_series_constructor(<function list_of_str at 0x7fe19dbc09d8>, True, 'int')
+     6.75±0.06μs      7.51±0.05μs     1.11  index_object.Indexing.time_get_loc_non_unique_sorted('Int')
+         517±8μs         573±70μs     1.11  ctors.SeriesConstructors.time_series_constructor(<function list_of_str at 0x7fe19dbc09d8>, True, 'float')
+     4.83±0.06ms      5.33±0.07ms     1.10  index_object.SetOperations.time_operation('int', 'symmetric_difference')
+     1.08±0.09μs      1.19±0.07μs     1.10  index_cached_properties.IndexCache.time_is_all_dates('Float64Index')
+     1.74±0.02ms      1.92±0.04ms     1.10  inference.ToNumericDowncast.time_downcast('datetime64', 'float')
-      25.4±0.8μs       23.1±0.2μs     0.91  indexing.NumericSeriesIndexing.time_getitem_scalar(<class 'pandas.core.indexes.numeric.Float64Index'>, 'unique_monotonic_inc')
-     8.31±0.06ms       7.53±0.2ms     0.91  inference.DateInferOps.time_timedelta_plus_datetime
-      16.1±0.7ms       14.6±0.3ms     0.91  timedelta.ToTimedelta.time_convert_string_days
-      14.0±0.8μs      12.7±0.08μs     0.90  index_object.Indexing.time_get_loc('Float')
-     1.94±0.01ms      1.75±0.02ms     0.90  index_object.Indexing.time_get_loc_non_unique('Int')
-     12.8±0.04ms       11.5±0.1ms     0.90  multiindex_object.GetLoc.time_med_get_loc_warm
-      14.1±0.7μs       12.6±0.1μs     0.89  index_object.Indexing.time_get_loc_sorted('Float')
-     12.5±0.04μs       11.1±0.1μs     0.89  multiindex_object.GetLoc.time_string_get_loc
-      10.2±0.6μs      9.05±0.05μs     0.89  index_object.Float64IndexMethod.time_get_loc
-      12.3±0.1ms       10.8±0.1ms     0.88  multiindex_object.GetLoc.time_small_get_loc_warm
-      34.4±0.6μs       30.2±0.6μs     0.88  index_object.Indexing.time_get_loc_non_unique_sorted('Float')
-         104±3μs         91.0±2μs     0.88  multiindex_object.GetLoc.time_large_get_loc
-      4.90±0.1ms      4.21±0.02ms     0.86  timeseries.ResampleSeries.time_resample('period', '1D', 'ohlc')
-      3.78±0.2ms      3.23±0.01ms     0.85  timeseries.ResampleSeries.time_resample('datetime', '5min', 'mean')
-      10.4±0.6μs      8.74±0.03μs     0.84  tslibs.timedelta.TimedeltaConstructor.time_from_iso_format
-      4.38±0.1ms      3.69±0.02ms     0.84  timeseries.ResampleSeries.time_resample('period', '5min', 'mean')
-      3.23±0.5μs       2.24±0.7μs     0.69  index_cached_properties.IndexCache.time_values('TimedeltaIndex')
-       641±200μs          443±1μs     0.69  groupby.GroupByMethods.time_dtype_as_field('int', 'head', 'direct')
-        2.73±1μs       1.45±0.2μs     0.53  index_cached_properties.IndexCache.time_values('UInt64Index')

```"
549857687,31023,REF: DatetimeIndex.get_loc,jbrockmendel,closed,2020-01-14T22:33:59Z,2020-01-15T17:06:33Z,"xref #30874 which does similar cleanup for TimedeltaIndex, #31021 for PeriodIndex

Do all parsing/casting/conversion before call to self._engine or super() methods  (xref #30948)

Needs test for preventing integers and Timedeltas through incorrectly.

Some of the casting may be simplified pending #30994."
549857133,31021,REF: PeriodIndex.get_loc,jbrockmendel,closed,2020-01-14T22:32:30Z,2020-01-15T18:40:32Z,"xref #30874 which does similar cleanup for TimedeltaIndex.

Do all parsing/casting/conversion before call to self._engine or super() methods (xref #30948)

Fix+test integers incorrectly getting through.

"
549565287,31001,TST: bare pytest raises,ShaharNaveh,closed,2020-01-14T13:24:19Z,2020-01-15T21:24:54Z,"- [x] ref #30999 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
549494390,30997,TST: Insert 'match' to bare pytest raises,ShaharNaveh,closed,2020-01-14T11:07:52Z,2020-01-15T21:26:05Z,"- [x] ref #30999
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
550047752,31033,DOC: typo a -> an,louwers,closed,2020-01-15T08:53:48Z,2020-01-16T00:15:25Z,
535475233,30174,Remove Dead JSON Code,WillAyd,closed,2019-12-10T03:50:52Z,2020-01-16T00:33:14Z,"I think this function is somewhat misleading as it implies that any ndarray type goes through it, but realistically the flag that sends you through to this function is only set for datetime arrays"
535503587,30179,Simplified JSON string handling,WillAyd,closed,2019-12-10T05:30:36Z,2020-01-16T00:33:16Z,"Provides a slight boost to performance as well

```sh
       before           after         ratio
     [52f5fdf1]       [325833e0]
     <master>         <faster-json-str>
-         112±1ms        101±0.7ms     0.90  io.json.ToJSON.time_to_json('index', 'df')

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE INCREASED.
```"
537219175,30243,Remove yapf references,WillAyd,closed,2019-12-12T21:19:44Z,2020-01-16T00:33:18Z,I'm not aware that we actually use this
537859643,30271,Simplified objToJSON signatures,WillAyd,closed,2019-12-14T03:33:15Z,2020-01-16T00:33:21Z,"Pre-cursor to a change that splits datetime functions that return `char *` off from datetime functions that return int.  Right now these are intermixed in functions that return void pointers, which makes type checking and grokking more difficult than they need to be"
539897458,30330,Cleaned up Tempita refs and Cython import,WillAyd,closed,2019-12-18T20:08:44Z,2020-01-16T00:33:22Z,"It has been 6 years since last Tempita release on Pypi so I think this can be removed.

"
542216181,30459,Removed dead intervaltree code,WillAyd,closed,2019-12-24T22:24:28Z,2020-01-16T00:33:23Z,While looking to reduce build warnings I came across these methods and couldn't find any public use to them. The only one that seems to get hit is `get_loc` and that is in tests only so I think dead code
542713978,30504,Mark unused parameters in objToJSON,WillAyd,closed,2019-12-27T01:57:44Z,2020-01-16T00:33:25Z,If compiled with `-Wextra` you get 58 warnings; this brings things down to 2 and I think helps the reader
540691535,30369,Clean Up nogil Warning From Parsers,WillAyd,closed,2019-12-20T01:47:44Z,2020-01-16T00:33:26Z,"I think just easier to ensure The GIL is acquired within the C function rather than all of the current casting / indirection.

Eliminates the following warning:

```sh
warning: pandas/_libs/parsers.pyx:1656:34: Casting a GIL-requiring function into a nogil function circumvents GIL validation
```"
544416985,30611,Improve ASV Environment Creation Performance,WillAyd,closed,2020-01-02T00:46:35Z,2020-01-16T00:33:27Z,"...by leveraging a lot of the recent work to enable parallel CI

Overriding default values documented here:

https://asv.readthedocs.io/en/stable/asv.conf.json.html#build-command-install-command-uninstall-command

I've picked 4 as the value assuming most people running benchmarks will have 2-4 cores. YMMV "
522630977,29613,Fixed tokenizer build warnings,WillAyd,closed,2019-11-14T05:19:27Z,2020-01-16T00:33:34Z,"Fixes the following warnings:

```sh
pandas/_libs/parsers.c:5873:52: warning: incompatible pointer types assigning to 'double (*)(const char *, char **, char, char, char, int)' from 'double (const char *,
      char **, char, char, char, int, int *, int *)' [-Wincompatible-pointer-types]
    __pyx_v_self->parser->double_converter_withgil = round_trip;
                                                   ^ ~~~~~~~~~~
pandas/_libs/parsers.c:5911:50: warning: incompatible pointer types assigning to 'double (*)(const char *, char **, char, char, char, int)' from 'double (const char *,
      char **, char, char, char, int, int *, int *)' [-Wincompatible-pointer-types]
    __pyx_v_self->parser->double_converter_nogil = precise_xstrtod;
                                                 ^ ~~~~~~~~~~~~~~~
pandas/_libs/parsers.c:5940:50: warning: incompatible pointer types assigning to 'double (*)(const char *, char **, char, char, char, int)' from 'double (const char *,
      char **, char, char, char, int, int *, int *)' [-Wincompatible-pointer-types]
    __pyx_v_self->parser->double_converter_nogil = xstrtod;
                                                 ^ ~~~~~~~
pandas/_libs/parsers.c:23829:93: warning: incompatible function pointer types passing 'double (*)(const char *, char **, char, char, char, int)' to parameter of type
      '__pyx_t_5numpy_float64_t (*)(const char *, char **, char, char, char, int, int *, int *)' (aka 'double (*)(const char *, char **, char, char, char, int, int *, int
      *)') [-Wincompatible-function-pointer-types]
          __pyx_v_error = __pyx_f_6pandas_5_libs_7parsers__try_double_nogil(__pyx_v_parser, __pyx_v_parser->double_converter_nogil, __pyx_v_col, __pyx_v_line_start...
                                                                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
pandas/_libs/parsers.c:2716:115: note: passing argument to parameter here
static CYTHON_INLINE int __pyx_f_6pandas_5_libs_7parsers__try_double_nogil(parser_t *, __pyx_t_5numpy_float64_t (*)(char const *, char **, char, char, char, int, i...
                                                                                                                  ^
4 warnings generated.
```"
521250724,29557,WIP: Dict Array Extension,WillAyd,closed,2019-11-11T23:52:56Z,2020-01-16T00:33:35Z,"modeled after some of @TomAugspurger great work in #27949 here is a dirty implementation of a DictArray. Marking as a WIP as still need to work through all of the remaining extension array types, though figure can put out there for feedback if anyone cares in the meantime

Definitely some overlap with the existing JSONArray; could integrate some aspects of that into this or vice versa"
520648137,29534,Clean Up Case Insensitive Comps in Tokenizer,WillAyd,closed,2019-11-10T19:50:46Z,2020-01-16T00:33:36Z,"Some of this is unused; other aspects were reinventing the wheel

to_boolean was doing a lot of extra work to have a case insensitive comparison. This PR removes that code (and heap memory management) while using a cross-platform ""strcasecmp"" function as defined in `portable.h` already
"
515553354,29313,Backports for 0.25.3,WillAyd,closed,2019-10-31T16:03:09Z,2020-01-16T00:33:38Z,"This includes pieces of #27826 except for the whatsnew note, which appeared in 0.25.1. Not sure if we want to move that here or keep as is, but test and implementation were included here"
513517632,29256,DOC: Added whatsnew for GH-29174,WillAyd,closed,2019-10-28T19:44:16Z,2020-01-16T00:33:41Z,follow up to #29174
511624127,29196,Added io Annotation,WillAyd,closed,2019-10-23T23:31:58Z,2020-01-16T00:33:42Z,"This is a quick follow up to https://github.com/pandas-dev/pandas/pull/26024/files#r317413254

The last remaining argument still has a slew of issues with it (provided below for reference) so leaving that for now

```sh
pandas/io/common.py:439: error: Incompatible types in assignment (expression has type ""GzipFile"", variable has type ""Union[str, Path, IO[Any]]"")
pandas/io/common.py:439: error: Argument ""fileobj"" to ""GzipFile"" has incompatible type ""Union[str, Path, IO[Any]]""; expected ""Optional[IO[bytes]]""
pandas/io/common.py:478: error: Argument 1 to ""append"" of ""list"" has incompatible type ""Union[str, Path, IO[Any]]""; expected ""IO[Any]""
pandas/io/common.py:483: error: Argument 1 to ""open"" has incompatible type ""Union[str, Path, IO[Any]]""; expected ""Union[str, bytes, int, _PathLike[Any]]""
pandas/io/common.py:486: error: Argument 1 to ""open"" has incompatible type ""Union[str, Path, IO[Any]]""; expected ""Union[str, bytes, int, _PathLike[Any]]""
pandas/io/common.py:489: error: Argument 1 to ""open"" has incompatible type ""Union[str, Path, IO[Any]]""; expected ""Union[str, bytes, int, _PathLike[Any]]""
pandas/io/common.py:496: error: Argument 1 to ""TextIOWrapper"" has incompatible type ""Union[str, Path, IO[Any]]""; expected ""IO[bytes]""
pandas/io/common.py:503: error: Argument 1 to ""MMapWrapper"" has incompatible type ""Union[str, Path, IO[Any]]""; expected ""IO[Any]""
pandas/io/common.py:504: error: Item ""str"" of ""Union[str, Path, IO[Any]]"" has no attribute ""close""
pandas/io/common.py:504: error: Item ""Path"" of ""Union[str, Path, IO[Any]]"" has no attribute ""close""
pandas/io/common.py:505: error: Incompatible types in assignment (expression has type ""MMapWrapper"", variable has type ""Union[str, Path, IO[Any]]"")
```
"
511415073,29185,Debug 38 build,WillAyd,closed,2019-10-23T15:44:56Z,2020-01-16T00:33:42Z,"Seeing if this helps with 38 build hanging until something more permanent comes along

@TomAugspurger @jorisvandenbossche "
506386774,28958,CLN: Consistent and Annotated Return Type of _iterate_slices,WillAyd,closed,2019-10-13T21:47:03Z,2020-01-16T00:33:45Z,"General pre-cursor to getting block management out of groupby. This is also a pre-cursor to fixing #21668 but needs to be coupled with a few more changes as a follow up

On master calls to _iterate_slices look up by label, potentially yielding a DataFrame if there were duplicated columns. This takes the surprise out of that and simply returns a Tuple of label / series for each item along the axis

@jbrockmendel "
510080772,29133,Reduce Circular Imports with pandas.core.reshape.concat,WillAyd,closed,2019-10-21T16:03:54Z,2020-01-16T00:33:46Z,"While working on #29124 I found a lot of general uses for this, but had to keep importing in the function body to prevent circular imports. By switching the isinstance checks to using ABC classes and moving the import of Index objects those circular imports can *mostly* be done away with

The only thing left that was causing circular imports was in the `Categorical` space. Can try to figure that out later, though cc @TomAugspurger in case that's a path that has already been gone down"
505537362,28912,"Maintain Timezone Awareness with to_json and date_format=""iso""",WillAyd,closed,2019-10-10T22:11:53Z,2020-01-16T00:33:47Z,"- [X] closes #12997
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

This vendors updates from numpy that allow for tz-aware ISO date formatting. Note this does slightly change the behavior of UTC dates. Previously they would write out as `2013-01-01T05:00:00.000Z` but now are `2013-01-01T05:00:00.000+00:00`. Both are valid ISO 8601

There is a follow up that needs to be addressed with reading these dates"
504337526,28863,Vectorized ISO Format,WillAyd,closed,2019-10-09T00:00:43Z,2020-01-16T00:33:48Z,"- [ ] closes #28180
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Needs work but could certainly use some guidance from @jbrockmendel 

This uses the dt -> ISO string formatting that is deeply nested in the JSON code. It doesn't handle time zones properly (see #12997), doesn't match what you would get by default from `Timestamp.isoformat` (different precision) and doesn't support Timedeltas yet. When Timedeltas are supported this could ease some of the performance issues @cbertinato is seeing in #28595

In any case looking for guidance and thoughts on how to properly implement this, if this is even in the right direction

Here's a rough benchmark on performance:

```ipython
"
503836888,28835,Remove NDFrameGroupBy Class,WillAyd,closed,2019-10-08T05:35:22Z,2020-01-16T00:33:51Z,"Was going to do a larger refactor but this diff looks more confusing than it actually is, so figured I'd stop here for readability. All I've done in this PR is:

  - Remove NDFrameGroupBy, consolidating it's methods into DataFrameGroupBy
  - Merge NDFrameGroupBy.aggregate into DataFrameGroupBy.aggregate (the latter previously called the former)

After this I am looking to find a more logical home for the functions, as the current hierarchy isn't super clear. For example, `_iterate_slices()` should probably be abstract in the base `GroupBy` class, but right now the Series implementation is in the superclass while DataFrameGroupBy overrides that for itself. Series.pct_change is practically the same as GroupBy.pct_change, so is probably unnecessary. Will be a few more follow ups mixed in"
502393639,28782,Remove blocks from GroupBy Code,WillAyd,closed,2019-10-04T02:40:04Z,2020-01-16T00:33:51Z,"Just trying to simplify things...

Currently fails (locally at least) with 5 tests, which interestingly enough seem mostly related to `mean`. Need to debug further but submitting now for feedback"
478876195,27838,Parametrize JSON tests,WillAyd,closed,2019-08-09T08:56:09Z,2020-01-16T00:33:52Z,lot more to be done here but split these up as some of the large functions are difficult to highlight failures while working on extension module. Improved coverage and identified some inconsistencies in process (marked with TODOs) which can be tackled as follow ups
490428547,28322,Removed PyString refs from extension modules,WillAyd,closed,2019-09-06T17:01:05Z,2020-01-16T00:33:53Z,"We have some old PyString references hanging around in our extension modules. These are a relic of the old Py2 string handling and will be removed in Python 4 I think, so figured worth modernizing

https://docs.python.org/3/howto/cporting.html"
490411461,28320,Added cpp files to build clean,WillAyd,closed,2019-09-06T16:16:58Z,2020-01-16T00:33:53Z,"Looks like the Cython-compiled `window.cpp` hangs around after running `python setup.py clean --all` - this should take care of that
"
444745127,26420,Add Annotations to GroupBy,WillAyd,closed,2019-05-16T04:09:09Z,2020-01-16T00:33:58Z,"Not fully complete but got started on these on a flight today. Should be able to review failures in a few more details.

@gwrome @vaibhavhrt would love any feedback from you"
444495358,26413,Added conda update step and re-added gcsfs,WillAyd,closed,2019-05-15T15:11:16Z,2020-01-16T00:33:59Z,"- [X] closes #26345
- [ ] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
443705073,26383,Prefixed batch script commands with call,WillAyd,closed,2019-05-14T04:30:21Z,2020-01-16T00:33:59Z,maybe fixes #26345
442530740,26332,Remove Panel References from Tests,WillAyd,closed,2019-05-10T04:14:41Z,2020-01-16T00:34:00Z,"ref #25632 this removes a good deal of Panel references from tests though not all (excluding anything for msgpack). 

I think after this and #25567 get merged we can starting finalizing the Panel removal and get out of docs as well"
439418494,26264,Remove usage of register keyword in Extension Modules,WillAyd,closed,2019-05-02T04:00:39Z,2020-01-16T00:34:01Z,"- [X] closes #26263
- [ ] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

ASV results show no difference:

```sh
BENCHMARKS NOT SIGNIFICANTLY CHANGED.
```

As I am fairly certain this keyword was being entirely ignored by the compilers anyway (see issue for reasoning)"
439880651,26272,WIP: Maintain Int64 Precision on Construction,WillAyd,closed,2019-05-03T04:22:50Z,2020-01-16T00:34:05Z,"- [X] closes #26259
- [X] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This is not the final implementation but providing for discussion and feedback. Consideration points are:

 - `_from_sequence_of_strings` was hacked to just return an IntervalArray, which isn't correct. Do we want to potentially pass through a mask to `_from_sequence` -> `integer_array` -> `IntegerArray()` instead? I think there is also a bug when construction the DataFrame correctly which this might fix
 - I'm not super familiar with the IntervalArray tests just yet, but I think I need to add a construction test in `arrays.test_integer` which may either supplement or replace the CSV test herein
 - There appears to be a bug with `tm.assert_frame_equal` and large integers where precision could be lost. This probably applies to all of the `tm.assert_*_equal` functions"
439837455,26270,Python2 String Handling Cleanup in parsers.pyx,WillAyd,closed,2019-05-03T00:07:27Z,2020-01-16T00:34:06Z,Just removing some old Py2 cruft hanging around in parsers.pyx
439427480,26266,Clean up Py2 Influence in move.c,WillAyd,closed,2019-05-02T04:58:51Z,2020-01-16T00:34:06Z,More extension module review / cleanup to get rid of Python2 traces
438437916,26239,Fix Memory Leak in to_json with Numeric Values,WillAyd,closed,2019-04-29T18:03:12Z,2020-01-16T00:34:07Z,"- [X] closes #24889
- [ ] tests added / passed
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

It looks like the extension module is unnecessarily incrementing the reference count for numeric objects and never releasing it, which causes a leak in to_json.

Still trying to grok exactly what the purpose of npyCtxtPassthru (comment in same file mentions it is required when encoding multi-dimensional arrays).

Removing the check for PyArray_ISDATETIME caused segfaults but that doesn't appear to leak memory anyway so most likely intentional to increment that ref count.

ASV results:

```sh
       before           after         ratio
     [9feb3ad9]       [9bfd45d3]
     <master>         <json-mem-fix~1>
-           69.1M            57.7M     0.84  io.json.ToJSONMem.peakmem_float
-           69.1M            57.7M     0.83  io.json.ToJSONMem.peakmem_int

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
```"
435239483,26152,Fix Bug with NA value in Grouping for Groupby.nth,WillAyd,closed,2019-04-19T16:54:38Z,2020-01-16T00:34:07Z,"- [X] closes #26011
- [X] tests added / passed
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

Would ideally like to combine first, nth and last implementations. Consider this a precursor
"
437858162,26222,Continued ujson cleanups,WillAyd,closed,2019-04-26T21:49:12Z,2020-01-16T00:34:08Z,"ref some of the comments by @jbrockmendel in #26212

"
437306912,26211,Removed Unnecessary Compat Directives from ujson,WillAyd,closed,2019-04-25T17:09:15Z,2020-01-16T00:34:11Z,Continuing cleanup of ujson source there are some preprocessor directives in there which are no longer valid given minimum Python and NumPy versions
435920490,26188,Fixed code issue from compat with new numpydoc,WillAyd,closed,2019-04-22T22:36:45Z,2020-01-16T00:34:11Z,"- [X] closes #26187
- [ ] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
433048782,26090,Cleanup of GroupBy Code,WillAyd,closed,2019-04-14T23:54:15Z,2020-01-16T00:34:12Z,"Removing some dead code I came across on review of the module which appears in NDFrameGroupBy but is overridden by its only public child class DataFrameGroupBy

FWIW with the removal of PanelGroupBy the NDFrameGroupBy is an unnecessary layer of inheritance and could probably be merged in with DataFrameGroupBy, though I figure that could be a separate dedicated PR to merge those if so desired"
433048295,26089,Fix Up Typing in GroupBy,WillAyd,closed,2019-04-14T23:49:31Z,2020-01-16T00:34:12Z,"More typing cleanups

Original failures:

```sh
pandas/core/groupby/ops.py:13: error: Module 'pandas._libs' has no attribute 'groupby'
pandas/core/groupby/ops.py:13: error: Module 'pandas._libs' has no attribute 'reduction'
pandas/core/groupby/groupby.py:22: error: Module 'pandas._libs' has no attribute 'groupby'
pandas/core/groupby/groupby.py:329: error: Need type annotation for '_apply_whitelist'
pandas/core/groupby/generic.py:220: error: Incompatible types in assignment (expression has type ""Callable[[Arg(Any, 'arg'), VarArg(Any), KwArg(Any)], Any]"", base class ""SelectionMixin"" defined the type as ""Callable[[Arg(Any, 'func'), VarArg(Any), KwArg(Any)], Any]"")
```"
428022396,25958,WIP: Panel Transpose Cleanup,WillAyd,closed,2019-04-02T04:47:14Z,2020-01-16T00:34:13Z,"Progress towards #25632 

Prefacing this PR with the statement that this is pretty wonky, but I would consider that largely an artifact of cleaning up the existing code base. 

Right now `transpose` is defined in `NDFrame` but is only used by `DataFrame` and `Panel`, as `Series` objects end up using a separate `transpose` defined in `IndexOpsMixin`. 

There are duplicate calls to `validate_transpose*` with various signatures that aren't well defined, and everything is passed rather ambiguously via args and kwargs which makes validation rather difficult.

Existing validation is arguably rough. Here's a sample call and response on master:

```python-traceback
>>> df = pd.DataFrame(np.ones((2, 2)))
>>> df.transpose('foo')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/williamayd/clones/pandas/pandas/core/frame.py"", line 2643, in transpose
    nv.validate_transpose(args, dict())
  File ""/Users/williamayd/clones/pandas/pandas/compat/numpy/function.py"", line 55, in __call__
    self.defaults)
  File ""/Users/williamayd/clones/pandas/pandas/util/_validators.py"", line 218, in validate_args_and_kwargs
    validate_kwargs(fname, kwargs, compat_args)
  File ""/Users/williamayd/clones/pandas/pandas/util/_validators.py"", line 157, in validate_kwargs
    _check_for_default_values(fname, kwds, compat_args)
  File ""/Users/williamayd/clones/pandas/pandas/util/_validators.py"", line 69, in _check_for_default_values
    format(fname=fname, arg=key)))
ValueError: the 'axes' parameter is not supported in the pandas implementation of transpose()
```

As you can see the error message mentions the 'axes' parameter, but that isn't part of the signature of `transpose` and is instead ambiguously defined based off of args.

What I've done here is removed the Panel implementation and tried to make the `NDFrame` implementation more explicit. Because it's only used by `DataFrame` however, an alternate and arguably preferable implementation would be to raise `NotImplementedError` in `NDFrame` and handle everything in the `DataFrame` implementation once Panel is gone"
426292423,25904,Cleanup Typing in pandas.core.strings,WillAyd,closed,2019-03-28T04:25:52Z,2020-01-16T00:34:14Z,"- [x] closes #25902 
- [ ] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
426019539,25894,Skipped flaky part of test_time,WillAyd,closed,2019-03-27T15:13:50Z,2020-01-16T00:34:15Z,"Workaround for #25875 to get CI passing - I kept intact the working part of the test and split off the failing piece into a separate test, which may be more explicit anyway

Haven't been able to reproduce this locally so plan to either keep the original issue open or create a new one for a more permanent fix, which may require a total refactor of the test

@gfyoung "
348115930,22226,WIP - Bool Extension Array,WillAyd,closed,2018-08-06T23:34:29Z,2020-01-16T00:34:24Z,"This is nowhere near complete as I have a ton of broken tests that need to be resolved, but theoretically progress towards #21778 as I get my feet wet with EAs.

My thought here was to leverage the masking operations used by the Integer EAs to implement an easy Boolean EA on top of that. I've essentially copied over all of the integer tests as well, though someone may have thoughts on a better way to structure all of this.

Any and all direction greatly appreciated"
302918515,20024,Cythonized GroupBy mad,WillAyd,closed,2018-03-07T00:20:50Z,2020-01-16T00:34:26Z,"- [ ] progress towards #19165 
- [X] tests added / passed
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

```bash
       before           after         ratio
     [01b91c26]       [9d7f0ac9]
+         678±3μs         807±10μs     1.19  groupby.GroupByMethods.time_method('object', 'value_counts')
+       116±0.8μs          135±1μs     1.16  groupby.GroupByMethods.time_method('object', 'shift')
+        777±10μs         888±10μs     1.14  groupby.GroupByMethods.time_method('object', 'unique')
+      71.6±0.9μs       81.4±0.4μs     1.14  groupby.GroupByMethods.time_method('object', 'size')
-           730ms       2.87±0.4ms     0.00  groupby.GroupByMethods.time_method('int', 'mad')
-           1.23s       3.05±0.5ms     0.00  groupby.GroupByMethods.time_method('float', 'mad')

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
```"
355631490,22543,Added capture_stderr decorator to test_validate_docstrings,WillAyd,closed,2018-08-30T15:13:19Z,2020-01-16T00:34:28Z,"- [X] closes #22483
- [X] tests added / passed
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Originally thought it would be nice to have this as a class decorator, but usage of this conflicts with the `capsys` fixture used by `test_bad_examples` so had to pick and choose where to apply. Additionally the decorator doesn't work out of the box to wrap classes, so it would take a decent amount more code.

cc @gfyoung will present minor merge conflicts with your work in #22531"
329300622,21323,Fixed Issue Preventing Agg on RollingGroupBy Objects,WillAyd,closed,2018-06-05T04:47:09Z,2020-01-16T00:34:29Z,"- [X] closes #15072
- [X] tests added / passed
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

AFAICT the fact that RollingGroupBy could not use `agg` with a list of functions is simply due to the fact that the GroupByMixing it inherits from could not handle the reduction in dimensions that occurs via the normal aggregation functions."
419028333,25614,Use Sphinx RTD Theme,WillAyd,closed,2019-03-09T03:29:55Z,2020-01-16T00:34:32Z,"- [X] closes #15556 

TOC on the left hand side is pinned during navigation and also expandable/collapsible

<img width=""1089"" alt=""image"" src=""https://user-images.githubusercontent.com/609873/54065608-3cc81e00-41d8-11e9-830d-124d759bf791.png"">


<img width=""1099"" alt=""image"" src=""https://user-images.githubusercontent.com/609873/54065614-62552780-41d8-11e9-8edc-3bd7db452ca1.png"">

"
419131535,25622,WIP for MyPy CI Integration,WillAyd,closed,2019-03-10T00:37:14Z,2020-01-16T00:34:33Z,"progress towards #25601 

Not complete just pushing for now. Main goal here is to get a CI run that works for any modules existing with comments."
420619731,25713,Auto backport of pr 25701 on 0.24.x,WillAyd,closed,2019-03-13T17:13:18Z,2020-01-16T00:34:34Z,"@TomAugspurger @jorisvandenbossche @jreback want to double check I did this right. I didn't use the -m flag during the cherry-pick process as that was causing failures

0.24.3 at this point?"
395109734,24538,Cython language level 3,WillAyd,closed,2019-01-01T20:25:59Z,2020-01-16T00:34:36Z,"- [X] closes #23927
"
424454950,25844,MyPy CI Configuration,WillAyd,closed,2019-03-23T02:44:14Z,2020-01-16T00:34:37Z,"- [X] closes #14468

I think this is the minimal amount of work required to get MyPy configured for CI. Note that the list of files is the only [documented](https://mypy.readthedocs.io/en/latest/running_mypy.html#reading-a-list-of-files-from-a-file) way to whitelist things that I could find (see also https://github.com/python/mypy/issues/1476)

I think we want to add this to the Checks_and_doc job though I couldn't see where that was actually configured (maybe an access thing on Azure?) so can do as a quick follow up

@jreback @gwrome "
226263791,16232,"DEPR: pandas.core, pandas.util.testing",jreback,closed,2017-05-04T12:24:45Z,2020-01-03T19:34:02Z,"xref #16223 

These are the only remaining 'public'-ish directories that are not explicitly exposed. In 0.20.0 we make these 'private' in documentation. We should actually deprecate these and move to ``_core`` and ``pandas.util.testing`` -> ``pandas.core.testing``

This will need a DeprecationWarning as likely lots of code that is out there that uses this."
544756122,30628,DOC: Add strings for dtypes in basic.rst,Dr-Irv,closed,2020-01-02T21:33:29Z,2020-01-03T21:39:21Z,"- [x] closes #30590
- [ ] tests added / passed
    - N/A
- [ ] passes `black pandas`
    - N/A
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
    - N/A
- [ ] whatsnew entry
    - N/A

Decided to add all valid strings for all the types in the table about dtypes.  I didn't want to decide which ones to leave out, and if we want to leave some of them out, we should decide whether they should be removed from the code as well (e.g., `'Sparse[int, 0]'`)

Had to reformat the table so it would look nice, by splitting the list of strings into multiple lines (e.g., a merged cell)

"
544232484,30585,BUG: Disable parallel cythonize on Windows (GH 30214),Dr-Irv,closed,2019-12-31T17:09:11Z,2020-01-03T21:41:05Z,"- [x ] closes #30356
- [ ] tests added / passed
  - N/A
- [ ] passes `black pandas`
  - Didn't run - changes too many files
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Added test in `setup.py` to check if nthreads is positive and on Windows.
"
545108458,30653,TST: Test for merge_asof groupby=multiple with categorical column,dwhu,closed,2020-01-03T18:37:31Z,2020-01-03T22:08:21Z,"- [x] closes #16454
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
545176318,30663,MAINT: Change all SO links to use HTTPS,gfyoung,closed,2020-01-03T21:42:31Z,2020-01-03T22:37:13Z,Title is self-explanatory
545177580,30664,DOC: Update info regarding pydatastream,gfyoung,closed,2020-01-03T21:46:32Z,2020-01-03T22:46:32Z,Title is self-explanatory
544790997,30631,CLN: Update old string formatting to f-string,thepaullee,closed,2020-01-02T23:28:55Z,2020-01-03T23:17:46Z,"- [x] contributes to #29547 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Updates:
 pandas/core/ops/array_ops.py
 pandas/core/ops/dispatch.py
 pandas/core/ops/docstrings.py
 pandas/core/ops/invalid.py
 pandas/core/ops/methods.py
 pandas/core/ops/roperator.py
"
102164318,10863,BUG: frame creation with multi-index and tuples in dict,ruidc,closed,2015-08-20T15:22:31Z,2020-01-03T23:23:43Z,"```
In [1]: import pandas;import numpy;import datetime;
In [2]: v = datetime.date.today()
In [3]: pandas.DataFrame({v : pandas.Series(range(3),index=range(3))}, columns=[v])
Out[3]:
   2015-08-20
0           0
1           1
2           2
In [4]: v = v, v
In [5]: pandas.DataFrame({v : pandas.Series(range(3),index=range(3))}, columns=[v])
Out[5]:
  (2015-08-20, 2015-08-20)
0                      NaN
1                      NaN
2                      NaN
```
"
74080242,10078,Pandas attempts to convert some strings to timestamps when grouping by a timestamp and aggregating?,marcuscollins,closed,2015-05-07T18:22:18Z,2020-01-03T23:23:43Z,"I am working through logs of web requests, and when I want to find the most common, say, user agent string for a (disguised) user, I run something like the following:

```
from pandas import Series, DataFrame, Timestamp

tdf = DataFrame({'day': {0: Timestamp('2015-02-24 00:00:00'),  1: Timestamp('2015-02-24 00:00:00'),
                                      2: Timestamp('2015-02-24 00:00:00'), 3: Timestamp('2015-02-24 00:00:00'),
                                      4: Timestamp('2015-02-24 00:00:00')},
                            'userAgent': {0: 'some UA string', 1: 'some UA string', 2: 'some UA string',
                                                 3: 'another UA string', 4: 'some UA string'},
                             'userId': {0: '17661101',  1: '17661101', 2: '17661101', 3: '17661101', 4: '17661101'}})

def most_common_values(df):
    return Series({c: s.value_counts().index[0] for c,s in df.iteritems()})

tdf.groupby('day').apply(most_common_values)
```

Note that in this (admittedly unusual) example, all of the lines are identical. I'm not sure if that is necessary to recreate the issue. And, I'm obscuring the exact purpose of this code, but it reproduces the bug: The 'userId' comes back as a Timestamp, not a string. This happens after the function most_common_values returns, since that userId string is not returned as a timestamp. if we change the value of the userId to an int:

```
tdf['userId'] = tdf.userId.astype(int)
```

or if the value of the associated integer  is small enough:

```
tdf['userId'] = '15320104`
```

then the results are what we'd expect (the most common value as its original type is returned.)

I imagine that for some reason something like a dateutil parser is being called on strings by default but that probably shoulnd't be happening...
"
63205161,9687,"DataFrame.interpolate(axis=1, method=""time"", inplace=True) ",markrichardson,closed,2015-03-20T10:26:23Z,2020-01-03T23:23:43Z,"In pandas 0.15.2, DataFrame.interpolate( ... ) doesn't work with the keyword combination axis=1, method=""time"", inplace=True. It just returns the original DataFrame.

```
import pandas as pd
import numpy as np

periods=5
idx = pd.date_range(start=""2014-01-01"", periods=periods)
data = np.random.rand(periods, periods)
data[data < 0.5] = np.nan
df0 = pd.DataFrame(index=idx, columns=idx, data=data)
df1 = df0.copy()

print ""(1) Original df""
print df0

print ""\n(2a) axis=0, inplace=False""
print df0.interpolate(axis=0, method=""time"")
print ""\n(2b) axis=0, inplace=True""
df0.interpolate(axis=0, method=""time"", inplace=True)
print df0

print ""\n(3a) axis=1, inplace=False""
print df1.interpolate(axis=1, method=""time"")
print ""\n(3b) axis=1, inplace=True""
df1.interpolate(axis=1, method=""time"", inplace=True)
print df1
```
"
49685038,8870,BUG: groupby with categorical drops empty groups when aggregating over a series,aimboden,closed,2014-11-21T11:15:57Z,2020-01-03T23:23:43Z,"- [ ] Series groupby excluding NaN groups with Categorical (DataFrame DOES include)
- [ ] sorting via a returned Interval-like-Index (string based)

Hello,

When grouping a DataFrame over more than one column including a categorical, the empty groups are kept in the aggregation result. A test for this behaviour was introduced in #8138.

However, when performing aggregation on only one column of the DataFrame, the empty groups are dropped. This seems inconsistent to me and I guess that it's an edge case that wasn't thought of at the time.

``` python
d = {'foo': [10, 8, 4, 1], 'bar': [10, 20, 30, 40],
     'baz': ['d', 'c', 'd', 'c']}
df = pd.DataFrame(d)
cat = pd.cut(df['foo'], np.linspace(0, 20, 5))
df['range'] = cat
groups = df.groupby(['range', 'baz'], as_index=True, sort=True)

# Expected result, fixed as part of #8138
fixed = groups.agg('mean')

# Inconsistent behaviour with series
inconsistent = groups['foo'].agg('mean')

# Expected result
expected = fixed['foo']
```

``` python
fixed
```

|  |  | bar | foo |
| --- | --- | --- | --- |
| range | baz |  |  |
| (0, 5] | c | 1 | 40 |
|  | d | 4 | 30 |
| (10, 15] | c | NaN | NaN |
|  | d | NaN | NaN |
| (15, 20] | c | NaN | NaN |
|  | d | NaN | NaN |
| (5, 10] | c | 8 | 20 |
|  | d | 10 | 10 |

``` python
inconsistent
```

| range | baz |  |
| --- | --- | --- |
| (0, 5] | c | 1 |
|  | d | 4 |
| (5, 10] | c | 8 |
|  | d | 10 |

``` python
expected
```

|  |  |  |
| --- | --- | --- |
| range | baz |  |
| (0, 5] | c | 1 |
|  | d | 4 |
| (10, 15] | c | NaN |
|  | d | NaN |
| (15, 20] | c | NaN |
|  | d | NaN |
| (5, 10] | c | 8 |
|  | d | 10 |

Note the strange ordering of the categorical index. I would expect `sorted = True` to sort by categorical level and not by lexical order?

Also note that using `as_index=False` fails due to #8869
"
104803409,10984,BUG? Parser adds empty MultiIndex level names,chris-b1,closed,2015-09-03T23:51:45Z,2020-01-03T23:23:44Z,"```
In [179]: midx = MultiIndex.from_tuples([('A', 1, 2), ('A', 1, 2), ('B', 1, 2)])
    ...:  df = DataFrame(np.random.randn(3, 3), index=midx,
    ...:                 columns=['x', 'y', 'z'])

In [180]: df
Out[180]: 
            x         y         z
A 1 2  1.236013  2.239172 -1.513059
    2 -0.159620  0.222250 -1.635894
B 1 2  0.211036 -0.441065 -0.661737

In [181]: df.to_csv('test.csv')

In [182]: result = pd.read_csv('test.csv', index_col=[0, 1, 2])

In [183]: result
Out[183]: 
                                x         y         z
Unnamed: 1 Unnamed: 2                              
A 1          2           1.236013  2.239172 -1.513059
             2          -0.159620  0.222250 -1.635894
B 1          2           0.211036 -0.441065 -0.661737
```
"
95275958,10586,Interpreting passed slicers in using .loc(axis=1)  |#AssertionError: Start slice bound is non-scalar,dickster77,closed,2015-07-15T19:55:50Z,2020-01-03T23:23:44Z,"From [SO](http://stackoverflow.com/questions/31439314/axis-argument-to-loc-to-interpret-the-passed-slicers-on-a-axis-1)

The documentation suggests:

```
You can also specify the axis argument to .loc to interpret the passed slicers on a single axis.
see http://pandas.pydata.org/pandas-docs/stable/advanced.html#using-slicers
```

However I get an error trying to slice along the column index.

```
    import pandas as pd
    import numpy as np

    cols= [(yr,m) for yr in [2014,2015] for m in [7,8,9,10]]
    df = pd.DataFrame(np.random.randint(1,100,(10,8)),index=tuple('ABCDEFGHIJ'))
    df.columns =pd.MultiIndex.from_tuples(cols)

    print df.head()

      2014             2015            
        7   8   9   10   7   8   9   10
    A   68  51   6  48   24   3   4  85
    B   79  75  68  62   19  40  63  45
    C   60  15  32  32   37  95  56  38
    D    4  54  81  50   13  64  65  13
    E   78  21  84   1   83  18  39  57


    #This does not work as expected
    print df.loc(axis=1)[(2014,9):(2015,8)]

    #AssertionError: Start slice bound is non-scalar

    #but an arbitrary transpose and changing axis works!
    df = df.T
    print df.loc(axis=0)[(2014,9):(2015,8)]

              A   B   C   D   E   F   G   H   I   J
    2014 9    6  68  32  81  84  60  83  39  94  93
         10  48  62  32  50   1  84  18  14  92  33
    2015 7   24  19  37  13  83  69  31  91  69  90
         8    3  40  95  64  18   8  32  93  16  25

    #So I could always assign the slice and re-transpose. 
    #That though feels like a hack and the axis=1 setting should have worked.

        df = df.loc(axis=0)[(2014,9):(2015,8)]

    df = df.T


    print df

      2014     2015    
        9   10   7   8 
    A   64  98   99  87
    B   43  36   22  84
    C   32  78   86  66
    D   67   8   34  73
    E   83  54   96  33
    F   18  83   36  71
    G   13  25   76   8
    H   69   4   99  84
    I    3  52   50  62
    J   67  60    9  49
```

The canonical way to do this is:

```
In [6]: df.loc()[:,(2014,9):(2015,8)]
Out[6]: 
  2014     2015    
    9   10   7   8 
A   26   2   44  69
B   41   7    5   1
C    8  27   23  22
D   54  72   81  93
E   18  23   54   7
F   11  81   37  83
G   60  38   59  29
H    3  95   89  96
I    6   9   77   9
J   90  92   10  32
```

So prob a bug
"
65956662,9790,ERR: better error reporting for passing list to level grouper on non-MultiIndex,eXcuvator,closed,2015-04-02T15:53:03Z,2020-01-03T23:23:44Z,"`groupy(level=[0])` doesn't work on a unique index. I guess it is semantics whether one allows not only level=0, but also level=[0], but if not, the error message should be different:

```
df = pd.DataFrame(np.arange(0,9).reshape(3,3))
df.groupby(level=[0])
ValueError: level > 0 only valid with MultiIndex
```
"
53262851,9186,KeyError when using pivot_table with an aggfunc and and empty column,brandonkane,closed,2015-01-02T16:50:11Z,2020-01-03T23:23:44Z,"This is a bit of an edge case, but in Pandas 0.15.2 when you try to pivot on an empty column you should get back an empty dataframe.  This is the behaviour when the default aggregation function is used, but if you specify an aggfunc argument it fails.

Setup dataframe with empty column:

```
>>> df = pd.DataFrame({'A': [2,2,3,3,2], 'id': [5,6,7,8,9], 'C':['p', 'q', 'q', 'p', 'q'], 'D':[None, None, None, None, None]})
>>> df
   A  C     D  id
0  2  p  None   5
1  2  q  None   6
2  3  q  None   7
3  3  p  None   8
4  2  q  None   9
```

Expected behaviour when pivoting on the empty column:

```
>>> df.pivot_table(index='A', columns='D', values='id')
Empty DataFrame
Columns: []
Index: []
```

But if you specify an aggfunc it blows up:

```
>>> df.pivot_table(index='A', columns='D', values='id', aggfunc=np.size)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/data/virtualenv/default/lib/python2.7/site-packages/pandas/util/decorators.py"", line 88, in wrapper
    return func(*args, **kwargs)
  File ""/data/virtualenv/default/lib/python2.7/site-packages/pandas/util/decorators.py"", line 88, in wrapper
    return func(*args, **kwargs)
  File ""/data/virtualenv/default/lib/python2.7/site-packages/pandas/tools/pivot.py"", line 151, in pivot_table
    table = table[values[0]]
  File ""/data/virtualenv/default/lib/python2.7/site-packages/pandas/core/frame.py"", line 1780, in __getitem__
    return self._getitem_column(key)
  File ""/data/virtualenv/default/lib/python2.7/site-packages/pandas/core/frame.py"", line 1787, in _getitem_column
    return self._get_item_cache(key)
  File ""/data/virtualenv/default/lib/python2.7/site-packages/pandas/core/generic.py"", line 1068, in _get_item_cache
    values = self._data.get(item)
  File ""/data/virtualenv/default/lib/python2.7/site-packages/pandas/core/internals.py"", line 2849, in get
    loc = self.items.get_loc(item)
  File ""/data/virtualenv/default/lib/python2.7/site-packages/pandas/core/index.py"", line 1402, in get_loc
    return self._engine.get_loc(_values_from_object(key))
  File ""pandas/index.pyx"", line 134, in pandas.index.IndexEngine.get_loc (pandas/index.c:3812)
  File ""pandas/index.pyx"", line 154, in pandas.index.IndexEngine.get_loc (pandas/index.c:3692)
  File ""pandas/hashtable.pyx"", line 696, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12299)
  File ""pandas/hashtable.pyx"", line 704, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12250)
KeyError: 'id'
```
"
52834154,9149,aggregating with a dictionary that specifies a column that has all nan's fails to use numpy,flyingchipmunk,closed,2014-12-24T19:41:44Z,2020-01-03T23:23:44Z,"As the subject says, if I try to call .agg() with a dictionary with a column that has all np.nan's it falls back to python agg functions instead of numpy.

To reproduce: (my dataset is 60 cols, 100000 rows)

I imported a csv and one column was all null (np.nan). The column dtype was set to object. (that's one issue, why the large upcast container to store np.nan?)

```
sq = pd.read_table(sqFile, sep='\t', skiprows = 1, nrows=None, header=0)
sq_g=sq.groupby(all_key_cols, as_index=False, sort=False)
```

`sq_g.agg(sum)`
Without specifying a dictionary and using sum over the entire dataframe it correctly uses the cython optimized numpy.sum:
`10 loops, best of 3: 48.3 ms per loop`

`sq_g.agg(collections.OrderedDict({'ColumnRef53':'sum'}))`
Specifying a dictionary and a column that is of dtype object that is entirely rows of np.nan falls back to python (bad):
`1 loops, best of 3: 7.26 s per loop`

For reference (ColumnRef20 has floats, ColumnRef53 has entirely np.nan's):

```
sq.dtypes
# Row               float64
Rowdesc.iphost       object
...
ColumnRef20         float64
ColumnRef53          object
...
dtype: object
```

My workaround is to downcast these np.nan filled columns back to float64, then the dictionary aggregation correctly uses the numpy optimized functions and not python:

```
# workaround for numpy groupby issue:
#  downcast columns with all NaN from object to float64 so agg() doesn't fallback to python.

# first find all columns with all np.nan rows
data_cols = [x for x in sq_concat.columns.tolist() if x.startswith('Column')]
all_nan = pd.isnull(sq_concat[data_cols]).all()
all_nan_cols = all_nan[all_nan == True].index.values.tolist()

# only need to downcast if type is object
obj_downcast = sq_concat[all_nan_cols].dtypes == object
obj_downcast_cols = obj_downcast[obj_downcast == True].index.values.tolist()

# downcast object to np.float64
for nan_col in obj_downcast_cols:
    sq_concat[nan_col] = sq_concat[nan_col].apply(np.float64)
```

Then the dictionary .agg() works as expected:
`sq_g.agg(collections.OrderedDict({'ColumnRef53':'sum'}))`
`100 loops, best of 3: 6.2 ms per loop`
"
87138204,10326,pivot_table passes junk to aggfunc when value column does not exist,edparcell,closed,2015-06-10T23:08:56Z,2020-01-03T23:23:45Z,"Expected behavior is that pivot_table would throw an exception explaining the missing column. Instead pivot_table passes DataFrames to aggfunc before checking.

Example:

``` python
def agg(l):
    it = iter(l)
    try:
        value = next(it)
    except StopIteration:
        raise Exception(""0 items in iterator"")
    try:
        next(it)
        raise Exception(""More than 1 item in iterator"")
    except StopIteration:
        return value

foo = pd.DataFrame({""X"": [0, 0, 1, 1], ""Y"": [0, 1, 0, 1], ""Z"": [10, 20, 30, 40]})
foo.pivot_table('Z', 'X', 'Y', aggfunc=agg)
```

gives, as expected:

```
Y   0   1
X        
0  10  20
1  30  40
```

But the following throws an Exception from agg, rather than an exception from pivot_table about the missing value column:

``` python
foo.pivot_table('notpresent', 'X', 'Y', aggfunc=agg)
```
"
545041855,30646,TST: Regression testing for fixed issues,mroeschke,closed,2020-01-03T15:36:25Z,2020-01-03T23:23:50Z,"- [x] closes #10863
- [x] closes #9687
- [x] closes #10078
- [x] closes #8870
- [x] closes #9790
- [x] closes #9892
- [x] closes #10586
- [x] closes #10984
- [x] closes #9149
- [x] closes #9186
- [x] closes #10326
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
545154313,30660,Fix flakey base setitem test,TomAugspurger,closed,2020-01-03T20:40:22Z,2020-01-03T23:59:30Z,"This test makes a potentially incorrect assertion about the data provided to the test. We already require that `data[0] != data[1]`, so it can be used instead.

This failed in https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=24731&view=logs&j=a67b4c4c-cd2e-5e3c-a361-de73ac9c05f9&t=33d2fdd0-c376-5f94-e6d3-957bdd23a3b8."
545211338,30669,MAINT: Change all pandas links to use HTTPS,gfyoung,closed,2020-01-04T00:00:29Z,2020-01-04T01:58:27Z,Also update the link to pandas' `whatsnew` page
545156110,30661,TYP: check_untyped_defs pandas/io/sql.py,simonjayhawkins,closed,2020-01-03T20:45:32Z,2020-01-04T08:02:35Z,"pandas\io\sql.py:785: error: ""insert"" of ""list"" does not return a value
pandas\io\sql.py:1450: error: ""insert"" of ""list"" does not return a value
"
545147699,30658,TYP: --disallow-any-generics pandas\core\reshape\concat.py,simonjayhawkins,closed,2020-01-03T20:21:58Z,2020-01-04T08:03:15Z,"xref #30539

pandas\core\reshape\concat.py:477: error: Missing type parameters for generic type ""List""
pandas\core\reshape\concat.py:504: error: Missing type parameters for generic type ""List"""
545161648,30662,TYP: check_untyped_defs plotting._matplotlib.timeseries,simonjayhawkins,closed,2020-01-03T21:01:03Z,2020-01-04T08:04:00Z,"pandas\plotting\_matplotlib\timeseries.py:120: error: ""type"" has no attribute ""_plot"""
545267988,30678,TYP: simplify NDFrame.(iloc|loc|iat|at),topper-123,closed,2020-01-04T10:18:09Z,2020-01-04T11:09:53Z,The signature of .iloc/.loc/.iat/.at is very complex and mypy (and maybe many humans too:-)) can't work through it to see their types. This simplifies them and allows mypy to see their types.
542710562,30502,"Replace ""foo!r"" to ""repr(foo)"" syntax #29886",naomi172839,closed,2019-12-27T01:35:59Z,2020-01-04T11:48:03Z,"xref #29886 
Fixed the remaining files in:

Replace ""foo!r"" to ""repr(foo)"" syntax #29886


"
542399141,30475,STYLE: Using f-strings in pandas/__init__.py,naomi172839,closed,2019-12-26T00:41:52Z,2020-01-04T11:48:09Z,"Replaced the old style strings with the newer f-strings.

(Sorry for the confusion with the earlier draft)

#29886"
544807225,30633,CI: Remove pin google-cloud-bigquery,alimcmaster1,closed,2020-01-03T00:40:51Z,2020-01-04T15:16:04Z,"xref: https://github.com/pydata/pandas-gbq/issues/271 is now resolved.

This can be removed since `pandas-gbq` depends on `google-cloud-bigquery` ref: https://github.com/pydata/pandas-gbq/blob/master/setup.py"
144811644,12754,Large multiindex dataframe shows only NaN values after selected rows are dropped,sylvaticus,closed,2016-03-31T07:18:55Z,2020-01-04T17:25:15Z,"_NOTE: This issue has been already [submitted to SO](http://stackoverflow.com/questions/36221441/why-large-pandas-dataframe-shows-only-nan-values-after-i-drop-selected-rows/36223039)  where the only person replying did replicate the behaviour in pandas 18.0._

Using the pandas library v. 17.1, I am trying to remove the rows from a large (882504 rows) dataframe named productDataNat where parName=='rt', but then all the other rows become NaN:
#### Code Sample, a copy-pastable example if possible

Importing and showing the multiaxis dataframe..

```
productDataNat = pd.read_csv('https://lobianco.org/temp/productData_P0-Mi-Ei.csv',sep=';',  dtype={'value': np.float64})
productDataNat = productDataNat.drop(['Unnamed: 8'],axis=1)
productDataNat.set_index(['scen','country','region','prod','freeDim','year','parName'], inplace=True)
productDataNat.head()
```

![screenshot from 2016-03-25 14 26 00](https://cloud.githubusercontent.com/assets/2077159/14168168/f3df1112-f720-11e5-8530-0ba4116e3df4.png)

Here I drop by same values in an internal axis.. all the remaning values are NaN!:
`productDataNat.drop('rt', level='parName', axis=0)`
![screenshot from 2016-03-25 14 29 30](https://cloud.githubusercontent.com/assets/2077159/14168189/1616534e-f721-11e5-9925-b8ef72fb6275.png)
#### Expected Output

When instead I play with an example dataframe it works as expected:

```
midx = pd.MultiIndex(levels=[['one', 'two'], ['x','y']], labels=[[1,1,1,0],[1,0,1,0]])
dfmix = pd.DataFrame({'A' : [1, 2, 3, 4], 'B': [5, 6, 7, 8]}, index=midx)
dfmix
```

![screenshot from 2016-03-25 14 32 51](https://cloud.githubusercontent.com/assets/2077159/14168195/2b956cd2-f721-11e5-8087-6091523c8727.png)

`dfmix.drop('x',level=1,axis=0)`
![screenshot from 2016-03-25 14 34 01](https://cloud.githubusercontent.com/assets/2077159/14168204/365c2a02-f721-11e5-806f-99cd2664ec41.png)
#### output of `pd.show_versions()`

```
INSTALLED VERSIONS
------------------
commit: None
python: 3.4.3.final.0
python-bits: 64
OS: Linux
OS-release: 3.13.0-83-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.17.1
nose: 1.3.1
pip: 1.5.4
setuptools: 3.3
Cython: None
numpy: 1.10.4
scipy: 0.13.3
statsmodels: None
IPython: 4.0.0
sphinx: 1.2.2
patsy: None
dateutil: 2.5.0
pytz: 2015.7
blosc: None
bottleneck: 1.0.0
tables: None
numexpr: 2.5
matplotlib: 1.3.1
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.999
httplib2: 0.8
apiclient: None
sqlalchemy: None
pymysql: None
psycopg2: None
Jinja2: None
```
"
132194367,12261,sort_index doesn't sort MultiIndex when .loc[] referred to non-existent label on other axis,toobaz,closed,2016-02-08T16:46:28Z,2020-01-04T17:25:15Z,"```
In [2]: df = pd.DataFrame(0, columns=[], index=pd.MultiIndex.from_product([[], []]))

In [3]: df.loc['b', '2'] = 1

In [4]: df.loc['a', '3'] = 1

In [5]: df.sort_index()
Out[5]: 
      2    3
b   1.0  NaN
a   NaN  1.0

In [6]: df.sort_index().index.is_monotonic
Out[6]: False
```

Am I missing anything obvious?!
(Notice that this is not related to any of the two axes being initialized as empty)
"
109845663,11244,BUG: index sorting with strings & timestamps,dniku,closed,2015-10-05T17:25:06Z,2020-01-04T17:25:15Z,"Pandas 0.16.2, Python 3.4.

``` python
test = pd.DataFrame({
    pd.Timestamp('2012-01-01 00:00:00'): ['a', 'b'],
    pd.Timestamp('2012-01-02 00:00:00'): ['c', 'd'],
    'name': ['e', 'e'],
    'aaaa': ['f', 'g']
    })
print(test)
print(test.groupby('name').first())
```

Fails with:

```
  File ""/usr/lib64/python3.4/site-packages/pandas/core/groupby.py"", line 106, in f
    self._set_selection_from_grouper()
  File ""/usr/lib64/python3.4/site-packages/pandas/core/groupby.py"", line 489, in _set_selection_from_grouper
    self._group_selection = ax.difference(Index(groupers)).tolist()
  File ""/usr/lib64/python3.4/site-packages/pandas/core/index.py"", line 1506, in difference
    theDiff = sorted(set(self) - set(other))
  File ""pandas/tslib.pyx"", line 836, in pandas.tslib._Timestamp.__richcmp__ (pandas/tslib.c:15612)
TypeError: Cannot compare type 'Timestamp' with type 'str'
```
"
105247285,11020,"pandas.read_csv(..., skiprows=2, engine='c') with unix-style line breaks crashes python on windows",e-pet,closed,2015-09-07T16:59:35Z,2020-01-04T17:25:15Z,"The following makes python crash (""Kernel died, restarting"" in IPython) on my windows 7 machine:

```
import pandas
myfile = open(""test.csv"", ""w"", newline=""\n"")
myfile.write(""blah\n\ncol_1,col_2,col_3\n\n"")
myfile.close()
dat = pandas.read_csv(""test.csv"", skiprows=2, encoding=""utf-8"", engine=""c"")
```

Note the unix-style line breaks.

The test case seems to be pretty precise, since changing about anything leads to working code. I tried, e.g.,
- with Windows-style line breaks ('\r\n' instead of '\n')
- without the two initial lines and the `skiprows` parameter
- with two empty initial lines
- with just one initial line, containing text
- with the 'python' engine,

and everything worked.

Here is the output of `pandas.show_versions()`:

```
INSTALLED VERSIONS
------------------
commit: None
python: 3.4.3.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 61 Stepping 4, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None

pandas: 0.15.2
nose: 1.3.4
Cython: 0.22
numpy: 1.9.2
scipy: 0.15.1
statsmodels: 0.6.1
IPython: 3.0.0
sphinx: 1.2.3
patsy: 0.3.0
dateutil: 2.4.2
pytz: 2015.4
bottleneck: None
tables: 3.1.1
numexpr: 2.3.1
matplotlib: 1.4.3
openpyxl: 1.8.5
xlrd: 0.9.3
xlwt: None
xlsxwriter: 0.6.7
lxml: 3.4.2
bs4: 4.3.2
html5lib: None
httplib2: None
apiclient: None
rpy2: None
sqlalchemy: 0.9.9
pymysql: None
psycopg2: None
```
"
471110065,27519,Multiple lambdas for the same column return KeyError in DataFrameGroupBy.agg with named aggregation,badge,closed,2019-07-22T13:55:31Z,2020-01-04T17:27:52Z,"#### Multiple lambdas for the same column return `KeyError` in `DataFrameGroupBy.agg`

```python
In [1]: import pandas as pd

In [2]: df = pd.DataFrame({""A"": [1, 2]})

In [3]: df.groupby([1, 1]).agg(foo=('A', lambda x: x.max()), bar=('A', lambda x: x.min()))
```

```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-58-5b7e2c8bacf8> in <module>
      3 df = pd.DataFrame({""A"": [1, 2]})
      4 
----> 5 df.groupby([1, 1]).agg(foo=('A', lambda x: x.max()), bar=(""A"", lambda x: x.min()))

~\AppData\Local\Continuum\anaconda3\envs\insight\lib\site-packages\pandas\core\groupby\generic.py in aggregate(self, arg, *args, **kwargs)
   1453     @Appender(_shared_docs[""aggregate""])
   1454     def aggregate(self, arg=None, *args, **kwargs):
-> 1455         return super().aggregate(arg, *args, **kwargs)
   1456 
   1457     agg = aggregate

~\AppData\Local\Continuum\anaconda3\envs\insight\lib\site-packages\pandas\core\groupby\generic.py in aggregate(self, func, *args, **kwargs)
    262 
    263         if relabeling:
--> 264             result = result[order]
    265             result.columns = columns
    266 

~\AppData\Local\Continuum\anaconda3\envs\insight\lib\site-packages\pandas\core\frame.py in __getitem__(self, key)
   2979             if is_iterator(key):
   2980                 key = list(key)
-> 2981             indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True)
   2982 
   2983         # take() does not accept boolean indexers

~\AppData\Local\Continuum\anaconda3\envs\insight\lib\site-packages\pandas\core\indexing.py in _convert_to_indexer(self, obj, axis, is_setter, raise_missing)
   1269                 # When setting, missing keys are not allowed, even with .loc:
   1270                 kwargs = {""raise_missing"": True if is_setter else raise_missing}
-> 1271                 return self._get_listlike_indexer(obj, axis, **kwargs)[1]
   1272         else:
   1273             try:

~\AppData\Local\Continuum\anaconda3\envs\insight\lib\site-packages\pandas\core\indexing.py in _get_listlike_indexer(self, key, axis, raise_missing)
   1076 
   1077         self._validate_read_indexer(
-> 1078             keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing
   1079         )
   1080         return keyarr, indexer

~\AppData\Local\Continuum\anaconda3\envs\insight\lib\site-packages\pandas\core\indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing)
   1161                 raise KeyError(
   1162                     ""None of [{key}] are in the [{axis}]"".format(
-> 1163                         key=key, axis=self.obj._get_axis_name(axis)
   1164                     )
   1165                 )

KeyError: ""None of [MultiIndex([('A', '<lambda>'),\n            ('A', '<lambda>')],\n           )] are in the [columns]""
```
#### Problem description

When using the new groupby aggregation with relabeling API in pandas 0.25.0, a `KeyError` is raised when the same source column is used with multiple lambdas, as in the example above. This issue isn't present when using multiple lambdas with `SeriesGroupBy`, as in [the release notes](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.25.0.html#groupby-aggregation-with-multiple-lambdas).

@TomAugspurger notes also that in `DataFrameGroupby.aggregate`, `order` needs to be mangled too.

#### Expected Output

```python
Out[3]:
   foo  bar
1    2    2
```

##### Bonus related issue

If the applied function has the same name, a `SpecificationError` is raised with the message `Function names must be unique, found multiple named mean`, even though the kwargs are different:

```python
df.groupby([1, 1]).agg(mean=('A', 'mean'), another_mean=('A', 'mean'))
```

(Obviously this is a silly example, but I encountered it having defined a closure for `np.percentile` to get around the lambda issue!)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.0
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 2.0.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.2 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : 1.3.3
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

</details>
"
543418429,30537,Unpin ipython 7.10.1 once ipython 7.11.1 released,simonjayhawkins,closed,2019-12-29T14:22:59Z,2020-01-04T17:37:46Z,"xref https://github.com/cython/cython/pull/3291#issuecomment-569403562


"
545179584,30665,CI: unpin IPython,TomAugspurger,closed,2020-01-03T21:52:39Z,2020-01-04T17:38:13Z,Closes https://github.com/pandas-dev/pandas/issues/30537
545183501,30666,ENH: Implement PeriodIndex.intersection without object-dtype cast,jbrockmendel,closed,2020-01-03T22:04:07Z,2020-01-04T17:40:05Z,PeriodIndex._simple_new is not as simple as it should be.  In order to get it to have the appropriate signature we need to implement some of the set methods correctly.  This is the first of those.
186797546,14563,Missing marker when using `style=...`,tamasgal,closed,2016-11-02T13:15:50Z,2020-01-04T17:57:30Z,"There is an issue with the markers in the legend when working with the built-in matplotlib plot-wrapper in pandas. The problem is also in earlier versions and the only workaround I found is using matplotlib directly.

#### A small, complete example of the issue

```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.DataFrame({'A': [1, 2, 3, 4, 5, 6],
                   'B': [2, 4, 1, 3, 2, 4],
                   'C': [3, 3, 2, 6, 4, 2],
                   'X': [1, 2, 3, 4, 5, 6]})

fig, ax = plt.subplots()

for kind in 'ABC':
    df.plot('X', kind, label=kind, ax=ax, style='.')

plt.show()
```
![markers](https://cloud.githubusercontent.com/assets/1730350/19929894/f55dbd60-a105-11e6-980a-e105b841e518.png)


#### Expected Output
The markers in the legend are missing, except for the last label. It however works if I don't use `style=...`, but the default line-plot, like in this example:
```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.DataFrame({'A': [1, 2, 3, 4, 5, 6],
                   'B': [2, 4, 1, 3, 2, 4],
                   'C': [3, 3, 2, 6, 4, 2],
                   'X': [1, 2, 3, 4, 5, 6]})

fig, ax = plt.subplots()

for kind in 'ABC':
    df.plot('X', kind, label=kind, ax=ax)

plt.show()
```
![markers_with_lines](https://cloud.githubusercontent.com/assets/1730350/19929965/4d4cea46-a106-11e6-8b30-3d2f361186bb.png)



#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Darwin
OS-release: 16.1.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: en_US.UTF-8
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.19.0
nose: None
pip: 8.1.2
setuptools: 25.2.0
Cython: 0.24.1
numpy: 1.11.2
scipy: 0.18.1
statsmodels: None
xarray: None
IPython: 5.1.0
sphinx: None
patsy: None
dateutil: 2.5.3
pytz: 2016.7
blosc: None
bottleneck: None
tables: 3.3.0
numexpr: 2.6.1
matplotlib: 1.5.3
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.8
boto: None
pandas_datareader: None
</details>"
545094131,30651,"CLN: Clean tests for *.sort_index, *.sort_values and df.drop_duplicates",charlesdong1991,closed,2020-01-03T17:53:16Z,2020-01-04T18:05:21Z,"- [ ] xref #30578 #30405 #30402 


This follow-up PR is to parametrize and deduplicate `inplace` cases brought up in the above PRs."
545080815,30650,REF: use _data.take for CI/DTI/TDI/PI.take,jbrockmendel,closed,2020-01-03T17:15:56Z,2020-01-04T18:09:16Z,"cc @jschendel @jreback any idea why IntervalIndex doesn't use _assert_take_fillable like the others?  If it can/should, then we can share this method between all of our EA-backed indexes.  (Actually also need to make sure the slice behavior in the DTI/TDI/PI is OK to do for all of them)"
545308237,30686,Fix PeriodIndex.get_indexer with non-PI,jbrockmendel,closed,2020-01-04T17:13:30Z,2020-01-04T18:10:10Z,"I'm _pretty_ sure that for non-comparable we should be returning an array of -1s, not raising.  Can you double-check me on that @jreback?
"
545074059,30648,REF: move EA wrapping/unwrapping to indexes.extensions,jbrockmendel,closed,2020-01-03T16:57:43Z,2020-01-04T18:18:04Z,Re-use the comparison method wrapper in CategoricalIndex.
509372857,29087,Correcting Warnings from IPython tab completion tests with async tests,gabriellm1,closed,2019-10-19T01:36:09Z,2020-01-04T18:21:28Z,"- [ ] closes #29070 
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I tried to follow what Tom said in the issue but I don't really know how to integrate with CI. I was in doubt too with how to import pytest-asyncio as well. Maybe both questions are related.

If is possible for someone to explain, I will quickly make the necessary changes 😁
"
316256422,20756,BUG: concat with copy=False of ExtensionArray fails,jorisvandenbossche,closed,2018-04-20T12:55:19Z,2020-01-04T18:21:33Z,"```
In [1]: from pandas.tests.extension.decimal.array import DecimalArray, make_data

In [5]: dec_arr = DecimalArray(make_data())

In [6]: df1 = pd.DataFrame({'int1': [1, 2, 3], 'key':[0, 1, 2], 'ext1': dec_arr[:3]})

In [7]: df2 = pd.DataFrame({'int2': [1, 2, 3, 4], 'key':[0, 0, 1, 3], 'ext2': dec_arr[3:7]})

In [8]: pd.concat([df1, df1], axis=1, copy=False)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-60-bd0d3639db5b> in <module>()
----> 1 pd.concat([df1, df1], axis=1, copy=False)

/home/joris/scipy/pandas/pandas/core/reshape/concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)
    211                        verify_integrity=verify_integrity,
    212                        copy=copy)
--> 213     return op.get_result()
    214 
    215 

/home/joris/scipy/pandas/pandas/core/reshape/concat.py in get_result(self)
    406             new_data = concatenate_block_managers(
    407                 mgrs_indexers, self.new_axes, concat_axis=self.axis,
--> 408                 copy=self.copy)
    409             if not self.copy:
    410                 new_data._consolidate_inplace()

/home/joris/scipy/pandas/pandas/core/internals.py in concatenate_block_managers(mgrs_indexers, axes, concat_axis, copy)
   5394     import pdb; pdb.set_trace()
   5395 
-> 5396     for placement, join_units in concat_plan:
   5397 
   5398         if len(join_units) == 1 and not join_units[0].indexers:

AttributeError: 'DecimalArray' object has no attribute 'view'
```

this fails because of:

https://github.com/pandas-dev/pandas/blob/3a2e9e6c201fee07c3417550d2d47dca74066c3d/pandas/core/internals.py#L5396-L5403

so if `copy=False`, it takes a `view` of the data, which is not defined on the extension array interface. 
I am not fully sure why the view is needed here.
"
544725045,30625,TST: Adding test to concat where copy=False for ExtensionArrays,dwhu,closed,2020-01-02T20:03:02Z,2020-01-04T18:21:37Z,"The test ensures that ExtensionArrays are correctly constructed when
concat(copy=False) is used.

- [x] closes #20756
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
545243659,30674,TST: Add more tests for fixed issues,mroeschke,closed,2020-01-04T05:23:03Z,2020-01-04T19:01:04Z,"- [x] closes #11244
- [x] closes #11020
- [x] closes #12754
- [x] closes #12261
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
545125365,30655,REF: change TDI.delete behavior to match DTI.delete,jbrockmendel,closed,2020-01-03T19:21:15Z,2020-01-04T19:46:37Z,"With this change, the two methods behave the same and can be shared."
545321234,30692,BUG: calling Timestamp constructor on Timestamp at DST changes Timestamp value,AlexKirko,closed,2020-01-04T19:12:22Z,2020-01-04T19:46:41Z,"#### Code Sample, a copy-pastable example if possible

```import pandas as pd; pd.__version__

tz = ""dateutil/Europe/London""
ts = pd.Timestamp(""2013-10-27 01:00:00"")
print(""Initial value is {}"".format(ts.value))
ts = ts.tz_localize(tz, ambiguous=False, nonexistent=""raise"")
print(""After localization value is {}"".format(ts.value))
ts = pd.Timestamp(ts)
print(""After casting Timestamp as Timestamp value is {}"".format(ts.value))

Output:
0.26.0.dev0+1608.g06c5d2488

Initial value is 1382835600000000000
After localization value is 1382835600000000000
After casting Timestamp as Timestamp value is 1382832000000000000
```
#### Problem description

TLDR: Calling the Timestamp constructor on a Timestamp shouldn't change the object in any way.

Long version: Ran into this while working on solving #30543. When we localize into a Daylight Savings Time timezone, we are forced to alter the underlying value of a Timestamp to make it fit non-DST timezones. WIthout this, date arithmetic won't work properly between timezones. This is also necessary, for example, to make sure that a `pd.date_range` with one of the ends on a DST shift doesn't break (the test for this is implemented by `test_dti_construction_ambiguous_endpoint` in `pandas.tests.indexes.datetimes.test_timezones.TestDatetimeIndexTimezones`). Unfortunately, what happens currently is that `ts.tz_localize` doesn't change the underlying value on a DST shift. It instead changes when the Timestamp constructor is called on a localized Timestamp (you can take a look, for example, [here](https://github.com/pandas-dev/pandas/blob/06c5d2488e2898497fa4767dc78a7c85ac8d19d4/pandas/core/arrays/_ranges.py#L40)).

#### Expected Output

```
Initial value is 1382835600000000000
After localization value is 1382832000000000000
After casting Timestamp as Timestamp value is 1382832000000000000
```

The value should change on localization, not on calling the constructor.
I'd like to work on a PR, because without fixing this it's impossible to solve #30543 without introducing ugly hacks.

xref: #30543"
464807595,27258,Broken usage of NDFrameIndexer in geopandas,jorisvandenbossche,closed,2019-07-06T02:03:20Z,2020-01-04T20:14:39Z,"xref https://github.com/pandas-dev/pandas/pull/27223#issuecomment-508828776

The change in that PR broke a test in geopandas. We are using some indexing internals in geopandas that were relying on an InvalidIndexError being catched there (and the PR changed it to only TypeError and KeyError.

A small reproducible example without needing geopandas (but it is dummy code, not doing something useful):

```
from pandas.core.indexing import _NDFrameIndexer

class _CoordinateIndexer(_NDFrameIndexer):

    def _getitem_tuple(self, tup):
        obj = self.obj
        xs, ys = tup
        return obj[xs][ys]

pd.Series._create_indexer('cx', _CoordinateIndexer)

s = pd.Series(range(5))
s.cx[:, :]
```

The below passes on 0.24.2, but gives an error now on master.

The actual code in geopandas is:

https://github.com/geopandas/geopandas/blob/7c66c93f0f76de53615c02810e817ef57fb6a8de/geopandas/base.py#L734-L761

I know this is quite internal code of pandas that is being used here, so we should think about making this cleaner, but short term it would be nice to fix this."
545307106,30684,DOC: Mention TYP as a type annotation PR prefix,xhochy,closed,2020-01-04T17:02:47Z,2020-01-04T20:41:47Z,This seems to be quite common nowadays.
545306206,30682,TYP: Add mypy as a pre-commit,xhochy,closed,2020-01-04T16:54:25Z,2020-01-04T20:42:06Z,"This is quite helpful in developing with typing, especially if you plan to update the mypy version.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
197188746,14958,some markers missing from legend,nbecker,closed,2016-12-22T14:40:49Z,2020-01-04T21:33:41Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df = pd.DataFrame(np.random.randn(8, 3), 
                  columns=['A', 'B', 'C'])
ax = df.plot (y=['A'], marker='x', linestyle='solid')
df.plot (y=['B'], marker='o', linestyle='dotted', ax=ax)
df.plot (y=['C'], marker='<', linestyle='dotted', ax=ax)
plt.grid()
plt.show()


```
#### Problem description

The legend is missing some markers.
If I add
plt.legend()
then they show up.

See attached
[figure1.pdf](https://github.com/pandas-dev/pandas/files/668980/figure1.pdf)

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

#### Expected Output

#### Output of ``pd.show_versions()``

<details>
# Paste the output here pd.show_versions() here
INSTALLED VERSIONS
------------------
commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Linux
OS-release: 4.8.14-300.fc25.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.19.1
nose: 1.3.7
pip: 9.0.1
setuptools: 31.0.0
Cython: 0.25.2
numpy: 1.12.0rc1
scipy: 0.18.1
statsmodels: None
xarray: None
IPython: 5.1.0
sphinx: 1.5
patsy: None
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: None
tables: 3.3.0
numexpr: 2.6.1
matplotlib: 2.0.0rc2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: 4.5.1
html5lib: 0.999
httplib2: None
apiclient: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.8
boto: None
pandas_datareader: None

</details>
"
545311420,30687,PLT: Add tests for missing markers,charlesdong1991,closed,2020-01-04T17:45:46Z,2020-01-04T21:33:47Z,"closes #14563 
closes #14958 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
545333175,30698,Fix integer check; also add column with integer name in test case.,hwalinga,closed,2020-01-04T21:14:05Z,2020-01-04T22:06:46Z,A small embarrassing mistake is corrected here that was introduced in the recently merged #28215 
544391718,30604,CLN: Replace old format strings to f-strings in pandas/tests/base,rbharadwaj9,closed,2020-01-01T19:46:24Z,2020-01-04T22:19:51Z,"- [x] Contributes to #29547 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
541856091,30433,Pytest warnings JUnit XML Format,alimcmaster1,closed,2019-12-23T18:14:57Z,2020-01-04T22:57:37Z,"See the following in CI:

```
pytest -m ""not slow and not network"" -n auto --dist=loadfile -s --strict --durations=10 --junitxml=test-data.xml pandas
##[warning]The 'junit_family' default value will change to 'xunit2' in pytest 6.0.
Add 'junit_family=legacy' to your pytest.ini file to silence this warning and make your suite compatible.
```

Unsure if the xunit2.0 format will affect any of our coverage etc..

https://docs.pytest.org/en/latest/reference.html#confval-junit_family"
545329546,30695,REF: share `delete` between DTI/TDI/PI,jbrockmendel,closed,2020-01-04T20:33:45Z,2020-01-04T23:12:01Z,"Besides de-duplicating, this moves the ball down the field in getting rid of PeriodIndex._shallow_copy cases where we pass an object ndarray"
545315004,30689,CI: Fix IPython Tab Completion test async warning,alimcmaster1,closed,2020-01-04T18:20:33Z,2020-01-04T23:12:40Z,"- [x] closes #29070
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

- New PR since https://github.com/pandas-dev/pandas/pull/29087 went stale
"
509057702,29070,Warnings from IPython tab completion tests,TomAugspurger,closed,2019-10-18T12:45:55Z,2020-01-04T23:12:48Z,"At some point, IPython changed their code to be async. Now, our tests using their tab completion stuff causes warnings.

```
$ pytest pandas/tests/ -k test_tab_complete_warning 

================================================================================= warnings summary =================================================================================
pandas/tests/arrays/categorical/test_warnings.py::TestCategoricalWarnings::test_tab_complete_warning
  /Users/taugspurger/sandbox/pandas/pandas/tests/arrays/categorical/test_warnings.py:14: RuntimeWarning: coroutine 'InteractiveShell.run_code' was never awaited
    ip.run_code(code)

pandas/tests/frame/test_api.py::TestDataFrameMisc::test_tab_complete_warning
  /Users/taugspurger/sandbox/pandas/pandas/tests/frame/test_api.py:575: RuntimeWarning: coroutine 'InteractiveShell.run_code' was never awaited
    ip.run_code(code)

pandas/tests/indexes/test_base.py::TestIndex::test_tab_complete_warning
  /Users/taugspurger/sandbox/pandas/pandas/tests/indexes/test_base.py:2420: RuntimeWarning: coroutine 'InteractiveShell.run_code' was never awaited
    ip.run_code(code)

pandas/tests/series/test_api.py::TestSeriesMisc::test_tab_complete_warning
  /Users/taugspurger/sandbox/pandas/pandas/tests/series/test_api.py:508: RuntimeWarning: coroutine 'InteractiveShell.run_code' was never awaited
    ip.run_code(code)

-- Docs: https://docs.pytest.org/en/latest/warnings.html
=========================================================== 4 passed, 11 skipped, 62860 deselected, 4 warnings in 29.50s ===========================================================

```

There's a race condition between the completion of the `ip.run_code(code)` calls and the asserts later on. We need to explicitly wait for the `ip.run_code(code)` to finish before moving on with the test. The easiest way is to

1. Mark the tests as async with the 3rd-party pytest-asyncio (need to add to the CI)
2. Rewrite the tests to use `async def`.
3. Add an `await` to the `ip.run_code`

```python
diff --git a/pandas/tests/series/test_api.py b/pandas/tests/series/test_api.py
index 998f8b6f7d..b471e1dc1e 100644
--- a/pandas/tests/series/test_api.py
+++ b/pandas/tests/series/test_api.py
@@ -499,13 +499,15 @@ class TestSeriesMisc(TestData, SharedWithSparse):
         for full_series in [pd.Series([1]), pd.Series(index=[1])]:
             assert not full_series.empty
 
-    def test_tab_complete_warning(self, ip):
+    @pytest.mark.asyncio
+    async def test_tab_complete_warning(self, ip):
         # https://github.com/pandas-dev/pandas/issues/16409
         pytest.importorskip(""IPython"", minversion=""6.0.0"")
         from IPython.core.completer import provisionalcompleter
 
         code = ""import pandas as pd; s = pd.Series()""
-        ip.run_code(code)
+        await ip.run_code(code)
+
         with tm.assert_produces_warning(None):
             with provisionalcompleter(""ignore""):
                 list(ip.Completer.completions(""s."", 1))
```

~This is currently blocked by https://github.com/pandas-dev/pandas/issues/29034. Once we drop 3.5, we can use the `async` and `await` syntax.~ I forgot 3.5 had async / await."
545326174,30694,CLN: unreachable code in indexes,jbrockmendel,closed,2020-01-04T20:02:26Z,2020-01-04T23:15:13Z,
545314279,30688,CLN: share compatibility-check code,jbrockmendel,closed,2020-01-04T18:13:54Z,2020-01-04T23:20:27Z,
545315582,30690,TYP: NDFrame.(loc|iloc|at|iat),topper-123,closed,2020-01-04T18:25:52Z,2020-01-04T23:22:53Z,"Currently, the NDFrame indexers (.loc/.iloc/.at/.iat) are too complex set up to let mypy understand them. This PR makes their implementations understandable for mypy (and humans also maybe, the old imp. was a bit indirect so took som effort to understand).

One issue is that I  - for the life of me - can't programatically set doc strings of properties and make mypy understand the properties' types at the same time. Python/mypy seem to demand that doc strings be set directly on the property if the type is to be understandable by mypy. E.g. Appender on a property is not supported by mypy. So I've set the doc string on the NDFrame properties, and reference them from the indexer classes instances. This makes the PR seem large, but it's mostly just moving the doc strings.
"
545340522,30701,BUG: TimedeltaIndex.union with sort=False,jbrockmendel,closed,2020-01-04T22:33:28Z,2020-01-04T23:23:09Z,"This matches the DTI behavior, so after this _fast_union can be shared between DTI and TDI."
544805948,30632,TPY: Add Types to gbq.py,alimcmaster1,closed,2020-01-03T00:34:03Z,2020-01-05T00:03:54Z,Add Types to args for `to_gbq` and `read_gbq`.
545124126,30654,REF/TST: PeriodArray comparisons with listlike,jbrockmendel,closed,2020-01-03T19:18:06Z,2020-01-05T01:05:30Z,"Similar edits are going to be made to the DTA and TDA ops, separating PeriodArray to its own PR for exposition."
545338916,30700,CLN: Replace fstring in tests/groupby/*.py files ,Sangarshanan,closed,2020-01-04T22:14:19Z,2020-01-05T06:50:20Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Ref to #29547"
534389726,30129,DOC: Code style documentation,ShaharNaveh,closed,2019-12-07T11:24:17Z,2020-01-05T07:12:17Z,"- [x] ref [(comment)](https://github.com/pandas-dev/pandas/pull/29963#discussion_r353760238)
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This still needs a lot more work.
I will appreciate any advice/idea/fix."
544627499,30619,TYP: enable strict_equality to prohibit comparisons of non-overlappin…,simonjayhawkins,closed,2020-01-02T15:36:41Z,2020-01-05T10:02:51Z,"…g types

By default, mypy allows always-false comparisons like 42 == 'no'. Use this flag to prohibit such comparisons of non-overlapping types, and similar identity and container checks
```
pandas\core\dtypes\base.py:239: error: Non-overlapping equality check (left operand type: ""str"", right operand type: ""Callable[[ExtensionDtype], str]"")      
pandas\core\arrays\period.py:657: error: Non-overlapping container check (element type: ""Callable[[Any], Any]"", container item type: ""Callable[[Any, Any], Any]"")
pandas\core\arrays\period.py:658: error: Non-overlapping identity check (left operand type: ""Callable[[Any], Any]"", right operand type: ""Callable[[Any, Any], Any]"")
pandas\io\formats\format.py:307: error: Non-overlapping equality check (left operand type: ""bool"", right operand type: ""Literal['truncate']"")
pandas\io\formats\format.py:461: error: Non-overlapping equality check (left operand type: ""bool"", right operand type: ""Literal['truncate']"")
pandas\io\excel\_odfreader.py:159: error: Non-overlapping equality check (left operand type: ""str"", right operand type: ""float"")
```"
486873547,28215,Add function to clean up column names with special characters,hwalinga,closed,2019-08-29T10:25:38Z,2020-01-05T13:32:27Z,"Changed the backtick quoting functions so that you can also use backtick quoting to use invalid Python identifiers like ones that start with a digit, start with a number, or are separated by operators instead of spaces. 

- [x] closes #27017
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

As this builds upon #24955 I think @jreback would be again the right person for the code review."
545428357,30709,CI: Numpy Dev Build Failing,alimcmaster1,closed,2020-01-05T15:00:33Z,2020-01-05T16:13:30Z,"Example build failures link [here](https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=24934&view=logs&j=3a03f79d-0b41-5610-1aa4-b4a014d0bc70&t=fe74a338-551b-5fbb-553d-25f48b1836e8)

and on master:
https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=24897&view=logs&j=3a03f79d-0b41-5610-1aa4-b4a014d0bc70&t=fe74a338-551b-5fbb-553d-25f48b1836e8

Believe the error is:

![image](https://user-images.githubusercontent.com/16733618/71781906-f300bd80-2fcb-11ea-8534-06194b0e1a40.png)
"
544190287,30582,"TYP: Add types to top-level funcs, step 2",topper-123,closed,2019-12-31T13:32:48Z,2020-01-05T16:27:52Z,"Next step to #30565.

Next up will be the pd.read_* functions."
167242161,13777,DEPR: deprecations log for removed issues,jreback,closed,2016-07-24T16:49:21Z,2020-01-05T17:26:09Z,"Document removed deprecations, originally indicated in #6581. This reference is in reverse chronological order of release.

## 1.0.0
- [x] #16994 ``inplace`` arg to ``.set_axis()`` (0.21.0) (#27525) 
- [x] #17739 ``.get_value()`` and ``.set_value()`` (0.21.0) (#27377)
- [x] #22264 list-likes IN ``str.cat`` (0.24.0) (#27611) 
- [x] #19339 ``IntervalIndex.from_intervals`` (0.23.0) https://github.com/pandas-dev/pandas/pull/27793
- [x] #29608 support for nested renamers in `aggregate`
- [x] #23592 tm.assert_raises_regex (0.24.0) (#29174)
- [x] #23739 keep_tz=False in DatetimeIndex.to_series (0.24.0) (#29731)
- [x] #19751 factorize(order=) kwarg (0.23.0) (#29732)
- [x] #22672 FrozenNDArray.searchsorted signature (v → value) (0.24.0) (#29732)
- [x] #23585 raise_conflic kwarg replaced by errors in .update() (0.24.0) (#29732)
- [x] #22458 Index.shift n in favor of periods (0.24.0) (#29732)
- [x] #18836 skip_footer in pd.read_excel (0.23.0) (#29721)
- [x] #18902 convert_datetime64 in DataFrame.to_records (0.23.0) (#29721)
- [x] #23053 nthreads arg in read_feather (0.24.0) (#29728)
- [x] #18458 DataFrame/Series.as_matrix (0.23.0) (#29720)
- [x] #18258 Series.from_array (0.23.0) (#29720)
- [x] #18477 .asobject (0.23.0) (#29720)
- [x] #17656 DataFrame.blocks, DataFrame.as_blocks() (0.21.0) (#29720)
- [x] #18529 DataFrame.from_items (0.23.0) (#29720)
- [x] #18627 tseries.plotting.tsplot (0.23.0) (#29726)
- [x] #21400 encoding arg in read_stata (0.24.0) (#29722)
- [x] #18577 broadcast & reduce kwarg in .apply() (0.23.0) (#29730)
- [x] #23917 pd.types.is_period, pd.types.is_datetimetz (0.24.0) (#29744)
- [x] #18800 Series.valid (0.23.0) (#29724)
- [x] #18812 .is_copy (0.23.0) (#29724) 
- [x] #18243 .get_ftype_counts() (0.23.0) (#29724) 
- [x] #20239 .get_duplicates() (0.23.0) (#29724)
- [x] #24203 Series.clip_lower, Series.clip_upper, DataFrame.clip_lower and DataFrame.clip_upper (0.24.0) (#29724)
- [x] #23601 ExtensionArray._formatting_values (0.24.0) (#27774)
- [x] #19239/#26137 SparseSeries, SparseDataFrame (0.25) (#28425)
- [x] #19434 dtype arg in make_block_same_klass (0.23.0) (#29723)
- [x] #19265 fastpath arg in Block (0.23.0) (#29723) 
- [x]  #18217 Index.summary() (0.23.0) (#29819 
- [x] #23635 passing an int for usecols in read_excel (0.24.0) (#29795)
- [x] #20772 .freq setting on a datetimelike index (0.23.0) (#29801) 
- [x] #20730 DatetimeIndex.offset (0.23.0) (#29801)
- [x] #23752 MultiIndex constructor kwarg labels, and .set_lables() deprecated in favor of codes (0.24.0) (#29766)
- [x] #21613 MultiIndex.to_hierarachial (0.24.0) (#29766)
- [x] #20613 pd.concat(...., sort=) (0.23.0) (#29786)
- [x] #21036 floordiv integer ndarray and Timedelta (0.23.0) (#29797)
- [x]  #21896 Series.to_csv signature changes (0.24.0) (#29809)
- [x] #20584 ``Series.rolling.apply``etc for the ``raw`` parameter (0.23.0) (#29829)
- [x] #23110 fastpath keyword in Index constructors (0.24.0) (#29725)
- [x] #23539 allowing datetime64 data passed to TimedeltaIndex (0.24.0) (#29794)
- [x] #23937 allowing timedelta64 data passed to DatetimeIndex (0.24.0) (#29794)
- [x] #26744 DataFrame.ftypes, Series.ftype (0.25.0) (#29895)
- [x] #24050 lib.infer_dtype(...., skipna=None (0.24.0) (#29876)
- [x] #24751 Series.cat.categorical (0.24.0) (#29914)
- [x]  #18164 DTI.weekday_name and .dt.weekday_nam (0.23.0) (#29831)
- [x] #22644 from ``Timestamp.tz_localize``, ``DatetimeIndex.tz_localize``, ``Series.dt.tz_localize`` remove errors keyword, replaced by nonexistent / ambiguous (0.24.0) (#29911)
- [x]  #26839 change Timedelta.resolution to Timedelta.resolution_string (0.25.0) (#29910)
- [X] #18731 listifying tuples for groupby (0.23.0) (#29755)
- [x] #20841 Categorical.take default behaviour with -1 (0.23.0) (#29912)
- [x] #23990 passing dtype alias to DatetimeTZDtype(unit) parameter (0.24.0) (#29927)
- [x] #24114 Categorical where casting to object (0.24.0) (#29913)
- [x] #23396 FrozenNDArray (0.24.0) (#29840)
- [x] #10726 DataFrame.sort_index by keyword (0.17.0) (#29931)
- [x] #17295 reindexing for not-found in .loc (0.21.0) (#29802)
- [x] #17758 non-existant keys in MultiIndex (xref to #17295 for Index) (0.23.0) (#29802)
- [x] #17295 non-existant columns in .to_excel() & non-existant keys in .loc (0.21.0) (#29802)
- [x] #20721 .base, .flags, .stride, .itemsize, .data Series/Index attributes (0.23.0) (#29918)
- [x] #26684 DataFrame & Seriesto_dense/to_sparse (0.25.0) (#29900)
- [x] #27106 Index.dtype_str (0.25.0) (#29900) 
- [x] #27199 Categorical.ravel should return a Categorical and not ndarray (0.25.0) (#29900)
- [x] #27145 .get_dtype_counts (0.25.0) (#29900)
- [x] #24694 .fillna(integer) with timedelta (0.24.0) (#29875)
- [x] #20995 passing multiple axes to DataFrame.dropna (0.23.0) (#29875)
- [x] #24048 Series.nonzero (0.24.0) (#29875)
- [x] #21775 Catetgorical.from_codes accepting float codes (0.24.0) (#29875)
- [x] #23621 passing tz and a datetime.datetime/Timestamp with tzinfo in the Timestamp constructor (0.24.0) (#29929)
- [x] #23919 verify_integrity arg in TimedeltaIndex, DatetimeIndex (0.24.0) (#29930)
- [x] #23919 passing start, end, periods to DatetimeIndex or TimedeltaIndex (0.24.0) (#29930)
- [x] #24157 time_rule kwarg in offsets.generate_range (0.24.0) (#24157)
- [x] #23767 Series.str.partition and Series.str.rpartition, replace pat with sep (0.24.0) (#29986)
- [x] #27106 Series.put (0.25.0) (#29986)
- [x] #27106 Series.real & Series.imag (0.25.0) (#29986)
- [x] #26409 Categorical.get_values, SparseArray.get_values, DataFrame.get_values, Series.get_values, Index.get_values (0.25.0) (#29989)
- [x] #26409 SparseArray.values (0.25.0) (#29989)
- [x] #18307 pandas.tseries.converter.register (???) (#30003)
- [x] #???? pandas.plottting._matplotlib.hist._grouped_plot ""figsize"" kwarg (???) (#30003)
- [x] #???? PlotAccessor keyword-only arguments (???) (#30003)
- [x] #19980 pandas.plotting._timeseries.tsplot (0.23.0) (#30003)
- [x] #26405 Series/DataFrame.compound (0.25) (#30088)
- [x] #22318 join_axes kwarg in pd.concat (0.25.0) (#30090)
- [x] #???? set_axis signature (0.21.0) (#30089)
- [x] #27198 ufunc.outer(Series) (0.25.0) (#30104)
- [x] #24486 box arg in pd.to_datetime (0.25.0) (#30111)
- [x] #24559 i8data for DTI (0.24.0) (#30115)
- [x] #16955 Series.argmin/argmax (0.21.0) (#30113)
- [x] #22535 integer add/sub with DatetimeIndex, TimedeltaIndex, PeriodIndex, Timestamp, Period (0.24.0) (#30117)
- [x] #23264 Y, M in Timedelta and friends (0.25.0) (#30118)
- [x] #???? Index.contains (0.25.0) (#30103)
- [x] #???? how,fill_method kwargs in resample (??? older than Feb 2016) (#30139)
- [x] #26403 ordered=None in CategoricalDtype in favor of ordered=False (0.25.0) (#29955) 
- [x] #9493 remove deprecated `StataReader.data` method (???) (#30176)
- [x] #17328 remove deprecated ""index"" keyword from read_stata, StataReader, StataReader.read (???) (#30176)
- [x] #24806 passing non-nano dtype to TimedeltaIndex/DatetimeIndex (0.24.0) (#30199)
- [x] #15113/#26438 .ix (0.20.0 / DeprecationWarning), (0.25.0 FutureWarning) (#27620)
- [x] #27103 pd.to_msgpack and pd.read_msgpack (0.25.0) (#30112)
- [x] #27112 Series.item & Index.item (0.25.0) (#30175)
- [x] #20613 DataFrame.append ""sort"" keyword default change None to False (0.23.0) (#30251)
- [x] #21614 `Series.ptp (0.24.0) (#30458)
- [x] #21930 ``Series.compress`` (0.24.0) (#30514)

## 0.25.0
- [x] #15601 / #19247``Panel`` (0.20.0) (#27101 et al)
- [x] #15931 ``.agg(..)`` for groupby/rolling/resample with a renaming dict (0.20.0) (depends on #18366 for a proper replacement) (#26399)
- [x] #19269 / #19373 ``pandas.core.categorial`` move (0.23.0) (#25655)
- [x] #16942 ``pd.TimeGrouper`` (0.21.0) (#26477) 
- [x] #16442, #20938 ``ExcelWriter.sheetname`` (0.21.0, 0.23.0) (#26464) 
- [x] #17703 ``pd.TimeGrouper`` (0.21.0) (#26477)
- [x] #16488 ``ExcelWriter.parse_cols`` (0.21.0) (#26522)
- [x] #16970 ``pd.options.html.display`` (0.21.0) (#26540)
- [x] #16915 `convert_objects`(0.21.0), xref #11221, #11173 (initial deprecation) (#26612)
- [x] #17633 ``Series.select``, ``DataFrame.select`` (0.21.0) (#26641)
- [x] #17842 ``reindex_axis`` (0.21.0)
- [x] #17842 mappers for ``rename_axis`` (0.21.0)
- [x] #17982 ``cat.rename_categories`` with a ``Series`` as list-like (0.21.0)
- [x] #17812 ``.from_csv()`` (0.21.0)
- [x] #17820 ``tupleize_cols`` in ``read_csv`` (0.21.0)
- [x] #17877 ``tupleize_cols`` in ``to_csv`` (0.21.0)
- [x] #17774 ``.read_excel(..., parse_cols)`` (0.21.0)
- [x] #17744 ``.where/.mask(..., raise_on_error)``  (0.21.0)
- [x] #17742 ``.astype(...., categories=, ordered=)`` (0.21.0)
- [x] #17691 ``cdate_range`` (0.21.0)
- [x] #17493 ``SeriesGroupBy.nth``, for ``dropna=True`` (0.21.0) (#27168)
- [x] #17352 ``.take(..., convert=)`` param (0.21.0) (#27171)
- [x] #17346 duplicate elements in names for read_csv (0.21.0) (#27175)
- [x] #21359 compare datetime.datetime and datetime.date (0.23.0) (#27109)

## 0.24.0
- [x] #10892 Deprecate `LongPanel/WidePanel` (0.17.0), aliases since < 0.11.0 (#15748, #15802 revert); waiting on statsmodels 0.9.0 release (#18341)
- [x] ``pandas.core.datetools``, deprecated in 0.19.0 (https://github.com/pandas-dev/pandas/commit/3f3839b6cb00a7fbabfa8c0899be69c6ea088b3a) (#19119)
- [x] #14432 ambiguous name in level/column when grouping (#22415)
- [x] #14645 ``Series.repeat(reps)`` (0.20.0) (#22417)
- [x] #14645 ``Index.repeat(n)`` and ``MultiIndex.repeat(n)`` (#22574)
- [x] #15257 the ``as_indexer`` keyword in `str.match()` (0.20.0) (#22626)
- [x] #14645 ``Categorical.searchsorted(v)`` and ``Series.searchsorted(v)`` (#22670)
- [x] #14645 ``TimedeltaIndex.searchsorted(key)``, ``DatetimeIndex.searchsorted(key)``, and ``PeriodIndex.searchsorted(key)`` (#22670)
- [x] #15537 ``pandas.json`` (#22737)
- [x] #14686 ``SparseArray.to_dense(fill)`` (0.20.0) (#22910)
- [x] #14686 ``SparseSeries.to_dense(sparse_only)`` (0.20.0) (#22910)
- [x] #14967 ``.astype(raise_on_error)``  (0.20.0) (#23374)
- [x] #15099 ``Series.sortlevel`` and ``DataFrame.sortlevel`` (0.20.0) (#23375)
- [x] #15358 import of ``concat`` from ``pandas.tools.merge`` (#23376)
- [x] #16005 top-level scatter_matrix and plot_params + plotting methods in pandas.tools.plotting (0.20.0) (#23376)
- [x] #15501 ``.consolidate`` (0.20.0) (#23377)
- [x] #15537 ``pandas.parser``, ``pandas.lib``, ``pandas.tslib`` (0.20.0) (#23378)
- [x] #15538 ``pd.pnow()``, ``pd.match()``, ``pd.groupby()``, ``pd.Expr``, ``pd.Term`` (0.20.0) (#23380)
- [x] #15940 ``pd.get_store()`` (0.20.0) (#23380)
- [x] #16157 ``pandas.computation.expressions.set_use_numexpr()`` (0.20.0) (#23386)
- [x] #16157 ``pandas.types.concat.union_categorical()`` (0.20.0) (#23386)
- [x] #16250 ``shims for util and test`` (0.20.0) (#23386)
- [x] #16189 ``pandas.api.types.is_sequence`` (0.20.0) (#23390)
- [x] #16163 ``pandas.api.types.is_any_int_dtype``, ``is_floating_dtype`` (0.20.0) (#23390)
- [x] #15987 ``.astype(np.datetime64)`` and ``.astype(np.timedelta64)``, unit-less astypes (0.20.0) (#23392)

## 0.23.0
- [x] #11603 `freq`, `how` keywords (0.18.0) (#18601)
- [x] #13706 `pivot_annual` (0.19.0) (#18370)
- [x] #13739 `isleapyear` (0.19.0) (#18370)
- [x] #13358 `pd.ordered_merge` -> `pd.merge_ordered` (0.19.0) (#18459)
- [x] #11603 `freq` from ``.rolling/.expanding`` (#18601)
- [x] #11603 `how` from ``.rolling/.expanding`` (#18668)
- [x] #13735 import errors for io.data and io.wb (0.19.0, but deprecated in 0.17.0) (#18612)
- [x] #14007 `SparseList` (0.19.0) (#18621)
- [x] #13854 `Categorical.from_array` (0.19.0) (#18642)
- [x] #13386 `skip_footer` in `pd.read_csv` (0.19.0) (#18724)
- [x] #13373 `as_recarray` in `pd.read_csv` (0.19.0) (#18804)
- [x] #13360 `compact_ints` and `use_unsigned` in `pd.read_csv` (0.19.0) (#18851)
- [x] #13593 `.freq` in `Timestamp` rather than `.offset` (0.19.0) (#18927)
- [x] #13012 `Series.reshape` and `Categorical.reshape` (0.19.0) (#18954)
- [x] #11603 `pd.rolling_*`, `pd.expanding_*`, `pd.ewm*` (0.18.0); waiting on `dask` release (#18723)
- [x] #13874 `pandas.tseries.frequencies.get_standard_freq` & `to_offset` freqstr kw (0.19.0) (#19023)
- [x] #13776 `Panel4D\Panelnd` (0.19.0) (#19059)
- [x] #19107 ``display.line_width`` and ``display.height``
- [x] ``Categorical.labels`` (#19120)
- [x] ``flavor`` parameter of ``SQLiteDatabase`` (https://github.com/pandas-dev/pandas/commit/8acfad343c88760a6d09fea221996dd50393fa8a) (#19121)
- [x] ``str.extract(expand=None)`` (https://github.com/pandas-dev/pandas/commit/67730ddf46cc16c9c59a3260d37ccaba2603663a) (#19118)
- [x] #16223 ``pandas.tools.hashing``, ``pandas.util.hashing`` (0.20.0) (#19181)
- [x] #13147 `pandas.core.common` dtype introspection functions (0.19.0) https://github.com/pandas-dev/pandas/pull/19769
- [x] resample compat #20554 
- [x] ``axis=None`` param in ``.replace()`` #20789 


## 0.21.0
- [x] #10967 remove `ExcelWriter.has_index_names` (0.17.0) (PR #16522)
- [x] #12882 `Categorical.sort` (0.18.1) (PR #16728)
- [x] #12190 `options.display.mpl_style` (0.18.0) (PR https://github.com/pandas-dev/pandas/pull/16761)
- [x] #12591 `Index.sym_diff` (0.18.1) (PR https://github.com/pandas-dev/pandas/pull/16760)
- [x] #11149 `pd.eval` change `inplace=False` rather than `inplace=None` (0.18.0) (PR #16732) 
- [x] #11834 `get_offset_name` removed (0.18.0) (PR #16863) 
- [x] #2881 ``pd.options.display.line_width`` (PR #16993)
- [x] #3663 ``pd.options.display.height`` (PR #16993)


## 0.20.0
- [x] #10890 remove `TimeSeries` alias (0.17.0) (PR #15098)
- [x] #11135 remove `Series.is_timeseries` (0.17.0) (PR #15098)
- [x] #11308 `pandas.io.ga` (0.17.1) -> (PR #15223)
- [x] #11898 `pd.fama_macbeth`, `pd.ols`, can also remove `statsmodels` as a dep (0.18.0), (PR #15353)
- [x] #10711 `.irow`, `.icol`, `.iget_value` (from 0.11.0, but deprecated in 0.17.0) (PR #15547)
- [x]  #10711 `DataFrame.iterkv` (pretty old, but changed `DeprecationWarning` to `FutureWarning` only in 0.17) (PR #15650)
- [x] #10632 `Categorical.name` (0.17.0) (PR #15654)
- [x] #11121 `generate_bq_schema` remove as no longer needed (0.17.0) (PR #15484)
- [x] #10458 `testing.assert_isinstance` (0.17.0) (PR #15652)
- [x]  #10236`take_last` kw of `duplicated` and `drop_duplicates` (0.17.0) (#15710)
- [x]  #10920`take_last` kw of `Series.nlargest` and `nsmallest` (0.17.0) (#15710)
- [x]  #10792 `take_last` kw of `Groupby.nlargest/nsmallest`(0.17.0) (#15710)
- [x] #10726 `Series.order,sort`, `Index.order`, `DataFrame.sort` (0.17.0) (#15735)
- [x] #12027 back-compat for where with non-strings  in `.select(...., where=)` for `HDFStore` (0.11.0/0.18.0) (#https://github.com/pandas-dev/pandas/pull/15798)
- [x] #10735 `combineAdd` / `combineMult` (0.17.0) (#15805)
- [x] #10748 Deprecate having `np.nan` in `Categorical.categories` (0.17.0) (#15806)

## 0.19.0
- [x] Deprecate colSpace in favor of col_space (no issue, commit 4a5a677) (PR #13857)
- [x] #8227 '+'/'-' for Index set ops (0.15.0) -> https://github.com/pydata/pandas/pull/14127
- [x] #9094 make DTI-DTI subtraction yield a `TimeDeltaIndex` (rather then a set op `.difference()`) (0.16.0) -> #14164 
- [x] #9357,  #3445 remove trellis plot. (0.16.0) (PR #13855)
- [x]  #6910 Change `Panel.shift`'s signature to match `generic.shift()`'s (0.14.0) -> #14041
- [x] `flavor='mysql'` in `to_sql` (0.14.0) -> #13611)
- [x] `tquery`/`uquery` functions (0.14.0) -> #13616
- [x] #11274 `engine` kw in `to_csv()`, from 0.13 (0.17.1), #13419 
- [x] #8376 remove `.levels` from `Categorical` constructor / accessor (0.15.0), #13612 
- [x] #8486 remove `outtype` in `.to_dict()` replaced by `orient` (0.15.0), #13627
- [x] #8227 `diff` for Index set ops (0.15.0), #13669 
- [x] #10951 Legacy offsets (0.17.0) (#13590)
- [x] #9615 remove `pandas.sandbox` (0.16.0), #13670
- [x] #9611, remove ability to set `cat.ordered` directly (0.16.0), #13671 
- [x] #11157 `SparsePanel` (0.17.0), #13778 
- [x] #6919 Remove `copy` keyword from `xs`,`minor_xs`,`major_xs` (4 places) (0.14.0), #13781
- [x] #10085 remove `return_type='series'|'frame'` (replaced by `expand=`) (0.16.1), #13701
- [x] `to_wide` deprecated since 2011 (https://github.com/pydata/pandas/commit/1134c9f6c40398d8eba37c312c88ce294e31c6c5) -> #14039
- [x] boxplot https://github.com/pydata/pandas/pull/7096 (0.14.0) (#12216)
  - [x] change default of `return_type` from None to `'axes'`
  - [x] update return_type section in visualization.rst
## 0.18
- [x] #4950 - `rolling_corr_pairwise()`,  `expanding_corr_pairwise()` replace by `rolling_corr()`, `expanding_corr()` (0.14.0), in #11603 
- [x] #6680, cols was deprecated in DataFrame.to_duplicated and drop_duplicates (0.14.0), #12165 
- [x] #6292, sql functions (#12205)
- [x] #6930 factorize took an `order` argument but didn't do anything with it. (0.14.0) (https://github.com/pydata/pandas/pull/12274)
- [x]  #4892 Float indexers in non-float indexes. (0.14.0) (#12246)
- [x] encoding `items` (in favor of `locs`) in msgpack (#10623) (0.17.0) (#12129)
## 0.17
- [x]  Deprecate colSpace in favor of col_space (no issue, commit 4a5a677c)  (0.8.0 I think) -> Removed #10890
- [x] #2304 Deprecate broadcasting TimeSeries along DataFrame index (0.10.0) -> Removed #10890 
- [x] #5231 `na_last` arg from Series.sort (0.14.0) -> Removed #10726 
- [x] #7088 Deprecate percentile_width in favor of percentiles (0.14.0) ->  Removed in #10881
- [x] #3787 Deprecate load and save in favor of pickle and to_pickle (0.12.0) -> Removed #10892 
- [x]  #4853, #4864 Deprecate timeRule and offset in favor of freq ()  (0.13.0) -> Removed #10892 
- [x]  #4713 Remove kind from read_excel (0.13.0) -> Removed #10892 
- [x]  #4770, #7032 Deprecate infer_types to have no effect (0.13.0) -> Removed #10892 
- [x]  #4645 Deprecate table keyword in HDFStore (0.13.0) -> Removed #10892 
## 0.16
- [x] #5505, DataFrame.pivot_table and crosstab were refactored to use the same arguments as pivot (index and columns) and FutureWarnings added for the old arguments.  Per @jreback, in 0.16, these deprecated arguments should be removed and the docstring updated.  This issue is submitted as a reminder to do so and should be tagged with a 0.156 milestone.
- [x] #6686, cols was deprecated in DataFrame.to_excel and to_csv (rows is not deprecated, don't touch it!)
- [x] #8140, remove `covert_dummies` from `core/reshape` entirely. Deprecated in favor of enhanced `get_dummies`. Deprecated as of 0.15
- [x] #8481, remove `pandas.tools.util.py/value_range`
  ## 0.15
- [x] `DataFrame.delevel` already deprecated in (0.7)"
545351095,30704,REF: Share _fast_union between DTI/TDI,jbrockmendel,closed,2020-01-05T00:46:51Z,2020-01-05T18:07:26Z,
545331261,30697,Implement PeriodIndex.difference without object-dtype cast,jbrockmendel,closed,2020-01-04T20:52:23Z,2020-01-05T18:08:39Z,"Analogous to #30666.  After this we'll be able to share some code between the set operations.

The edit in pd._testing is mostly unrelated.  Both are motivated by tracking down the places where object-dtype ndarray is passed to PeriodIndex._shallow_copy."
545355244,30705,BUG: listlike comparisons for DTA and TDA,jbrockmendel,closed,2020-01-05T01:43:31Z,2020-01-05T18:09:02Z,"These will now match PeriodArray, and we can move to share the methods."
545342671,30702,DEPR: CategoricalIndex.take_nd,jbrockmendel,closed,2020-01-04T22:58:59Z,2020-01-05T18:56:35Z,matching Categorical.take_nd deprecation
545331023,30696,Make DTI/TDI _union behavior match,jbrockmendel,closed,2020-01-04T20:49:39Z,2020-01-05T18:57:34Z,Following this we'll be able to share the method.
543467976,30541,"TYP: Implicit generic ""Any"" for builtins",simonjayhawkins,closed,2019-12-29T16:31:27Z,2020-01-05T19:48:25Z,xref #30539
537231558,30245,Implement NA.__array_ufunc__,TomAugspurger,closed,2019-12-12T21:48:00Z,2020-01-05T20:43:25Z,"This gives us consistent comparisons with NumPy scalars.

I have a few implementation questions:

1. I wanted to reuse `maybe_dispatch_ufunc_to_dunder_op`. That was in `core/ops/dispatch.py`, but NA is defend in `_libs/missing.pyx`. For now I just moved it to `_libs/missing.pyx`, is there a better home for it? I don't think `_libs/ops.pyx` is an option, as it imports missing.
2. How should we handle comparisons with ndarrays? I see a few options:

  a. Return an `ndarray[object]`, where each element is NA
  b. Really take over and return our extension arrays. So `np.array([1, 2]) == pd.NA` would return an `IntegerArray([NA, NA])`. Boolean results would return a BooleanArray. And float results would return a ... PandasArray? (so maybe this isn't a good idea. But worth considering)
3. What scalars should we handle (in `__array_ufunc__` and in the regular methods)? Any of date time, Timestamp, time delta, Period, Interval? 


One item 2, I seem to have messed something up, as we return just a scalar `NA` right now. Will want to sort that out."
545458222,30712,CLN: remove Index/Series._is_homogeneous_type,jbrockmendel,closed,2020-01-05T19:43:26Z,2020-01-05T20:47:52Z,
387835001,24112,IntervalArray equality operators,TomAugspurger,closed,2018-12-05T16:20:23Z,2020-01-05T21:33:12Z,"It seems like IntervalArray doesn't implement equality operations correctly

```python
In [32]: iser = pd.Series(pd.IntervalIndex.from_breaks([1, 2, 3, 4]))

In [33]: iser.array == iser[0]
Out[33]: False
```

I would expect Out[33] to be

```python
np.array([True, False, False])
```

This bubbles up to Series[interval]

cc @jschendel 

"
544920803,30640,BUG: Fix IntervalArray equality comparisions,jschendel,closed,2020-01-03T09:45:37Z,2020-01-05T21:33:16Z,"- [X] closes #24112
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

cc @TomAugspurger "
545344645,30703,Implement ExtensionIndex,jbrockmendel,closed,2020-01-04T23:22:49Z,2020-01-05T21:35:09Z,"So far this just puts `take` in it, but there are a handful of other methods we'll be able to move from our existing EA Indexes."
544299776,30593,Pandas date_range does not work when using periods and adding holiday,tqdo,closed,2020-01-01T02:01:00Z,2020-01-05T21:39:10Z,"This code works fine

````
pd.date_range(start='2020-11-25 10:00',periods=14,
              freq=pd.offsets.CustomBusinessHour(start='10:00'))
````

but if I add holidays then it produces more than 14 periods


````
pd.date_range(start='2020-11-25 10:00',periods=14,
              freq=pd.offsets.CustomBusinessHour(start='10:00',holidays=['2020-11-26']))
````

Output:

````
DatetimeIndex(['2020-11-25 10:00:00', '2020-11-25 11:00:00',
               '2020-11-25 12:00:00', '2020-11-25 13:00:00',
               '2020-11-25 14:00:00', '2020-11-25 15:00:00',
               '2020-11-25 16:00:00', '2020-11-27 10:00:00',
               '2020-11-27 11:00:00', '2020-11-27 12:00:00',
               '2020-11-27 13:00:00', '2020-11-27 14:00:00',
               '2020-11-27 15:00:00', '2020-11-27 16:00:00',
               '2020-11-30 10:00:00', '2020-11-30 11:00:00',
               '2020-11-30 12:00:00', '2020-11-30 13:00:00',
               '2020-11-30 14:00:00', '2020-11-30 15:00:00',
               '2020-11-30 16:00:00', '2020-12-01 10:00:00',
               '2020-12-01 11:00:00', '2020-12-01 12:00:00',
               '2020-12-01 13:00:00', '2020-12-01 14:00:00',
               '2020-12-01 15:00:00', '2020-12-01 16:00:00',
               '2020-12-02 10:00:00', '2020-12-02 11:00:00',
               '2020-12-02 12:00:00'],
              dtype='datetime64[ns]', freq='CBH')
````

If I replace `periods` with the corresponding `end` then everything works fine.

````
pd.date_range(start='2020-11-25 10:00',end='2020-11-27 16:00:00',
              freq=pd.offsets.CustomBusinessHour(start='10:00',holidays=['2020-11-26']))
````

I am not sure why date_range has this weird behavior. Appreciate any help."
544848514,30639,BLD: address build warnings,jbrockmendel,closed,2020-01-03T04:42:21Z,2020-01-06T01:04:48Z,xref #30609
545246458,30675,BUG: bug in date_range with custom business hours and given periods,fujiaxiang,closed,2020-01-04T06:01:07Z,2020-01-06T02:00:11Z,"- [x] closes #30593
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
427359332,25939,TYP: Assorted_Typing_Fix in pandas.io,ryankarlos,closed,2019-03-31T05:13:52Z,2020-01-06T02:49:39Z,"- [x] closes #25933
- [ ] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
427361623,25940,TYP:  Remove pandas.io.stata from Typing Blacklist,ryankarlos,closed,2019-03-31T05:53:19Z,2020-01-06T02:50:21Z,"- [x] closes ##25932
- [x] passes tests 
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
430155537,26019,"TYP: Replace wildcardimports in toplevelinit as precursor for reshape,stata,io PRs #25936 #25940 #25939",ryankarlos,closed,2019-04-07T15:03:51Z,2020-01-06T02:50:41Z,"- [x] xref #25932, #25933, #25934
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
427354949,25936,TYP: Adding absolute imports for pandas.core.reshape,ryankarlos,closed,2019-03-31T03:55:25Z,2020-01-06T02:51:06Z,"- [x] closes #25934
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
545304008,30680,TYP: Fix chainmap typing for mypy 0.740+,xhochy,closed,2020-01-04T16:32:18Z,2020-01-06T08:02:06Z,"This will otherwise raise a failure during a `mypy` update. Also increase type precision of this file to 100%.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @simonjayhawkins "
544348744,30600,BUG: Ensure df.itertuples() uses plain tuples correctly,simongibbons,closed,2020-01-01T11:53:53Z,2020-01-06T09:15:27Z,"Currently DataFrame.itertuples() has an off by one error
when it inspects whether or not it should return namedtuples
or plain tuples in it's response.

This PR addresses that bug by correcting the condition
that is used when making the check.

Closes: #28282

- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
545614000,30725,DOC: fix see also in docstring of check_bool_array_indexer,jorisvandenbossche,closed,2020-01-06T08:55:41Z,2020-01-06T12:02:17Z,xref https://github.com/pandas-dev/pandas/pull/30308#pullrequestreview-338522525
544206632,30583,WIP: Restructuring all builds (NOT TO MERGE),datapythonista,closed,2019-12-31T14:55:24Z,2020-01-06T13:06:24Z,"- [X] closes #29685

So far just testing how this looks like in the CI. Will open small PRs and get this implementing iteratively if this looks good."
378519555,23557,BUG: concat(Series[sparse]) raises ValueError,TomAugspurger,closed,2018-11-07T23:52:56Z,2020-01-06T13:22:19Z,"```python
In [1]: import pandas as pd

In [2]: pd.__version__
Out[2]: '0.23.4'

In [3]: a = pd.Series(pd.SparseArray([0, 1, 2]))

In [4]: pd.concat([a, a], axis=1)
```

```pytb
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-4-a3e65bc6fb67> in <module>()
----> 1 pd.concat([a, a], axis=1)

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/reshape/concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    224                        verify_integrity=verify_integrity,
    225                        copy=copy, sort=sort)
--> 226     return op.get_result()
    227
    228

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/reshape/concat.py in get_result(self)
    398
    399                 index, columns = self.new_axes
--> 400                 df = cons(data, index=index)
    401                 df.columns = columns
    402                 return df.__finalize__(self, method='concat')

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy)
    346                                  dtype=dtype, copy=copy)
    347         elif isinstance(data, dict):
--> 348             mgr = self._init_dict(data, index, columns, dtype=dtype)
    349         elif isinstance(data, ma.MaskedArray):
    350             import numpy.ma.mrecords as mrecords

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/frame.py in _init_dict(self, data, index, columns, dtype)
    457             arrays = [data[k] for k in keys]
    458
--> 459         return _arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)
    460
    461     def _init_ndarray(self, values, index, columns, dtype=None, copy=False):

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/frame.py in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)
   7362     axes = [_ensure_index(columns), _ensure_index(index)]
   7363
-> 7364     return create_block_manager_from_arrays(arrays, arr_names, axes)
   7365
   7366

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/internals.py in create_block_manager_from_arrays(arrays, names, axes)
   4875         return mgr
   4876     except ValueError as e:
-> 4877         construction_error(len(arrays), arrays[0].shape, axes, e)
   4878
   4879

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/internals.py in construction_error(tot_items, block_shape, axes, e)
   4837     implied = tuple(map(int, [len(ax) for ax in axes]))
   4838     if passed == implied and e is not None:
-> 4839         raise e
   4840     if block_shape[0] == 0:
   4841         raise ValueError(""Empty data passed with indices specified."")

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/internals.py in create_block_manager_from_arrays(arrays, names, axes)
   4870
   4871     try:
-> 4872         blocks = form_blocks(arrays, names, axes)
   4873         mgr = BlockManager(blocks, axes)
   4874         mgr._consolidate_inplace()

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/internals.py in form_blocks(arrays, names, axes)
   4916
   4917     if len(items_dict['IntBlock']):
-> 4918         int_blocks = _multi_blockify(items_dict['IntBlock'])
   4919         blocks.extend(int_blocks)
   4920

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/internals.py in _multi_blockify(tuples, dtype)
   4993     for dtype, tup_block in grouper:
   4994
-> 4995         values, placement = _stack_arrays(list(tup_block), dtype)
   4996
   4997         block = make_block(values, placement=placement)

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/internals.py in _stack_arrays(tuples, dtype)
   5037     stacked = np.empty(shape, dtype=dtype)
   5038     for i, arr in enumerate(arrays):
-> 5039         stacked[i] = _asarray_compat(arr)
   5040
   5041     return stacked, placement

ValueError: could not broadcast input array from shape (2) into shape (3)
```

Right now on master, we return a SparseDataFrame. I would like to instead return a DataFrame with sparse values. On master, the rule for the result type is currently ""sparse if any of the inputs are sparse"". I would amend that to specifically be ""sparse if any of the inputs are a SparseDataFrame or SparseSeries"". This will require breaking a couple places like `SparseSeries.unstack()`, unless we hack in some special cases for sparse, which I'd like to avoid."
545039289,30645,TST: Adding test to concat Sparse arrays,SdgJlbl,closed,2020-01-03T15:30:08Z,2020-01-06T13:22:22Z,"- [ ] closes #23557 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
545500196,30723,REF: Create test_encoding file for CSV,gfyoung,closed,2020-01-06T01:44:32Z,2020-01-06T13:24:47Z,This is 99.99% copy and paste
378557160,23563,BUG: Series(list_of_intervals) results in object dtype,jschendel,closed,2018-11-08T02:53:48Z,2020-01-06T13:29:36Z,"#### Code Sample, a copy-pastable example if possible

Constructing a `Series` from a list of `Interval` objects results in an object dtype, and is not backed by an `IntervalArray`:
```python
In [2]: s = pd.Series([pd.Interval(0, 1), pd.Interval(1, 2), pd.Interval(2, 3)])

In [3]: s.dtype
Out[3]: dtype('O')

In [4]: s.values
Out[4]:
array([Interval(0, 1, closed='right'), Interval(1, 2, closed='right'),
       Interval(2, 3, closed='right')], dtype=object)
```

Note that constructing a `Series` from an `IntervalArray` results in the correct dtype and is backed by an `IntervalArray`:
```python
In [5]: s2 = pd.Series(pd.core.arrays.IntervalArray.from_breaks(range(4)))

In [6]: s2.dtype
Out[6]: interval[int64]

In [7]: s2.values
Out[7]:
IntervalArray([(0, 1], (1, 2], (2, 3]],
              closed='right',
              dtype='interval[int64]')
```
#### Problem description
The input data is not being inferred as interval dtype, but rather as object dtype, and is not being backed by an `IntervalArray`.

#### Expected Output
I'd expect to the `Series` to have an interval dtype and be backed by an `IntervalArray`.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: 82120016e6af85afb052d95710e498a6dc58c1ce
python: 3.6.1.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.0.dev0+948.g82120016e
pytest: 3.8.2
pip: 9.0.1
setuptools: 39.0.1
Cython: 0.28.2
numpy: 1.13.3
scipy: 1.0.0
pyarrow: 0.6.0
xarray: 0.9.6
IPython: 6.1.0
sphinx: 1.5.6
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.4
feather: 0.4.0
matplotlib: 2.0.2
openpyxl: 2.4.8
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 0.9.8
lxml: 3.8.0
bs4: None
html5lib: 0.999
sqlalchemy: 1.1.13
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
fastparquet: 0.1.5
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
484569404,28115,COMPAT: Implement Timestamp.fromisocalendar,TomAugspurger,closed,2019-08-23T14:47:35Z,2020-01-06T13:30:42Z,"https://docs.python.org/3.8/whatsnew/3.8.html#datetime / https://docs.python.org/3.8/library/datetime.html#datetime.datetime.fromisocalendar

> Added new alternate constructors datetime.date.fromisocalendar() and datetime.datetime.fromisocalendar(), which construct date and datetime objects respectively from ISO year, week number and weekday; these are the inverse of each class’s isocalendar method. (Contributed by Paul Ganssle in bpo-36004.)
"
541375802,30395,TST: Added fromisocalendar test cases,ShaharNaveh,closed,2019-12-21T21:34:17Z,2020-01-06T13:37:27Z,"- [x] closes #28115
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
545514115,30724,annotations,jbrockmendel,closed,2020-01-06T02:59:02Z,2020-01-06T15:26:17Z,"grepped for `__contains__`, annotated those where possible and a few things around it"
545493968,30720,REF: cosmetic differences between DTA/TDA/PA comparison methods,jbrockmendel,closed,2020-01-06T01:04:23Z,2020-01-06T15:27:19Z,"There are 2 remaining non-cosmetic differences between these methods remaining, which will be the subjects of upcoming PRs.  Once those are addressed, we'll be able to de-duplicate the methods completely."
540499394,30356,#30214 (Parallelized Build / CI) caused a build failure for me,topper-123,closed,2019-12-19T18:56:53Z,2020-01-01T01:34:36Z,"Currently I can't get pandas build.

I get the message when running ``python setup.py build_ext --inplace -j 4``:

```
Compiling pandas\_libs/parsers.pyx because it changed.
Compiling pandas\_libs/tslibs/timestamps.pyx because it changed.
Compiling pandas\_libs/window/aggregations.pyx because it changed.
Compiling pandas\io/msgpack/_packer.pyx because it changed.
Compiling pandas\io/msgpack/_unpacker.pyx because it changed.
Compiling pandas\_libs/groupby.pyx because it changed.
Compiling pandas\_libs/index.pyx because it changed.
Compiling pandas\_libs/internals.pyx because it changed.
Compiling pandas\_libs/lib.pyx because it changed.
Compiling pandas\_libs/parsers.pyx because it changed.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 105, in spawn_main
Compiling pandas\_libs/tslibs/timestamps.pyx because it changed.
Compiling pandas\_libs/window/aggregations.pyx because it changed.
Compiling pandas\io/msgpack/_packer.pyx because it changed.
Compiling pandas\io/msgpack/_unpacker.pyx because it changed.
Compiling pandas\_libs/groupby.pyx because it changed.
Compiling pandas\_libs/index.pyx because it changed.
Compiling pandas\_libs/internals.pyx because it changed.
Compiling pandas\_libs/lib.pyx because it changed.
Compiling pandas\_libs/parsers.pyx because it changed.
    exitcode = _main(fd)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 114, in _main
    prepare(preparation_data)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 277, in _fixup_main_from_path
    run_name=""__mp_main__"")
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\runpy.py"", line 263, in run_path
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 105, in spawn_main
    exitcode = _main(fd)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 114, in _main
    prepare(preparation_data)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 277, in _fixup_main_from_path
    run_name=""__mp_main__"")
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\runpy.py"", line 263, in run_path
Compiling pandas\_libs/tslibs/timestamps.pyx because it changed.
Compiling pandas\_libs/window/aggregations.pyx because it changed.
Compiling pandas\io/msgpack/_packer.pyx because it changed.
Compiling pandas\io/msgpack/_unpacker.pyx because it changed.
    pkg_name=pkg_name, script_name=fname)
    pkg_name=pkg_name, script_name=fname)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\runpy.py"", line 85, in _run_code
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\runpy.py"", line 96, in _run_module_code
    exec(code, run_globals)
  File ""C:\Users\TP\Documents\Python\pandasdev\pandasdev\setup.py"", line 815, in <module>
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 105, in spawn_main
    mod_name, mod_spec, pkg_name, script_name)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\TP\Documents\Python\pandasdev\pandasdev\setup.py"", line 815, in <module>
    exitcode = _main(fd)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 114, in _main
    ext_modules=maybe_cythonize(extensions, compiler_directives=directives),
  File ""C:\Users\TP\Documents\Python\pandasdev\pandasdev\setup.py"", line 543, in maybe_cythonize
    prepare(preparation_data)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 277, in _fixup_main_from_path
    run_name=""__mp_main__"")
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\runpy.py"", line 263, in run_path
    return cythonize(extensions, *args, **kwargs)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\site-packages\Cython\Build\Dependencies.py"", line 1073, in cythonize
    pkg_name=pkg_name, script_name=fname)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\runpy.py"", line 96, in _run_module_code
    ext_modules=maybe_cythonize(extensions, compiler_directives=directives),
  File ""C:\Users\TP\Documents\Python\pandasdev\pandasdev\setup.py"", line 543, in maybe_cythonize
    mod_name, mod_spec, pkg_name, script_name)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\runpy.py"", line 85, in _run_code
    return cythonize(extensions, *args, **kwargs)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\site-packages\Cython\Build\Dependencies.py"", line 1073, in cythonize
    exec(code, run_globals)
  File ""C:\Users\TP\Documents\Python\pandasdev\pandasdev\setup.py"", line 815, in <module>
    ext_modules=maybe_cythonize(extensions, compiler_directives=directives),
    nthreads, initializer=_init_multiprocessing_helper)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\context.py"", line 119, in Pool
  File ""C:\Users\TP\Documents\Python\pandasdev\pandasdev\setup.py"", line 543, in maybe_cythonize
    nthreads, initializer=_init_multiprocessing_helper)
    return cythonize(extensions, *args, **kwargs)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\site-packages\Cython\Build\Dependencies.py"", line 1073, in cythonize
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\context.py"", line 119, in Pool
    nthreads, initializer=_init_multiprocessing_helper)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\context.py"", line 119, in Pool
Compiling pandas\_libs/groupby.pyx because it changed.
Compiling pandas\_libs/index.pyx because it changed.
    context=self.get_context())
    context=self.get_context())
    context=self.get_context())
Compiling pandas\_libs/internals.pyx because it changed.
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\pool.py"", line 176, in __init__
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\pool.py"", line 176, in __init__
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\pool.py"", line 176, in __init__
Compiling pandas\_libs/lib.pyx because it changed.
Compiling pandas\_libs/parsers.pyx because it changed.
Compiling pandas\_libs/tslibs/timestamps.pyx because it changed.
Compiling pandas\_libs/window/aggregations.pyx because it changed.
Compiling pandas\io/msgpack/_packer.pyx because it changed.
Compiling pandas\io/msgpack/_unpacker.pyx because it changed.
    self._repopulate_pool()
    self._repopulate_pool()
    self._repopulate_pool()
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\pool.py"", line 241, in _repopulate_pool
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\pool.py"", line 241, in _repopulate_pool
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\pool.py"", line 241, in _repopulate_pool
Traceback (most recent call last):
    w.start()
    w.start()
    w.start()
  File ""<string>"", line 1, in <module>
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\process.py"", line 112, in start
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\process.py"", line 112, in start
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\process.py"", line 112, in start
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 105, in spawn_main
    exitcode = _main(fd)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 114, in _main
    prepare(preparation_data)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 277, in _fixup_main_from_path
    run_name=""__mp_main__"")
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\runpy.py"", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\TP\Documents\Python\pandasdev\pandasdev\setup.py"", line 815, in <module>
    ext_modules=maybe_cythonize(extensions, compiler_directives=directives),
  File ""C:\Users\TP\Documents\Python\pandasdev\pandasdev\setup.py"", line 543, in maybe_cythonize
    return cythonize(extensions, *args, **kwargs)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\site-packages\Cython\Build\Dependencies.py"", line 1073, in cythonize
    nthreads, initializer=_init_multiprocessing_helper)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\context.py"", line 119, in Pool
    context=self.get_context())
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\pool.py"", line 176, in __init__
    self._repopulate_pool()
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\pool.py"", line 241, in _repopulate_pool
    w.start()
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\process.py"", line 112, in start
    self._popen = self._Popen(self)
    self._popen = self._Popen(self)
    self._popen = self._Popen(self)
    self._popen = self._Popen(self)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\context.py"", line 322, in _Popen
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\context.py"", line 322, in _Popen
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\context.py"", line 322, in _Popen
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\context.py"", line 322, in _Popen
    return Popen(process_obj)
    return Popen(process_obj)
    return Popen(process_obj)
    return Popen(process_obj)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\popen_spawn_win32.py"", line 46, in __init__
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\popen_spawn_win32.py"", line 46, in __init__
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\popen_spawn_win32.py"", line 46, in __init__
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\popen_spawn_win32.py"", line 46, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
    prep_data = spawn.get_preparation_data(process_obj._name)
    prep_data = spawn.get_preparation_data(process_obj._name)
    prep_data = spawn.get_preparation_data(process_obj._name)
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 143, in get_preparation_data
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 143, in get_preparation_data
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 143, in get_preparation_data
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 143, in get_preparation_data
    _check_not_importing_main()
    _check_not_importing_main()
    _check_not_importing_main()
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 136, in _check_not_importing_main
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 136, in _check_not_importing_main
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 136, in _check_not_importing_main
    _check_not_importing_main()
    is not going to be frozen to produce an executable.''')
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.    is not going to be frozen to produce an executable.''')
  File ""C:\Users\TP\Miniconda3\envs\pandas-dev\lib\multiprocessing\spawn.py"", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')

RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.    is not going to be frozen to produce an executable.''')
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...
```

I can't really parse what's going on, but the build seems to go back to the same RuntimeError repeatedly.

I've triaged the issue to stem from #30214. Any idea what's happening, @WillAyd ?

## Workaround

I can work around the issue by building with parallelization, i.e. run ``python setup.py build_ext --inplace -j 0`` instead of ``python setup.py build_ext --inplace -j 4``.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 95e1a63dd3382db6663bc8a2b334b422b93dd7fe
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.26.0.dev0+1358.g95e1a63dd
numpy            : 1.17.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 42.0.2.post20191203
Cython           : 0.29.13
pytest           : 5.2.2
hypothesis       : 4.28.2
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.6.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.2.2
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
</details>
"
543770718,30554,TST: Regression testing for fixed issues,mroeschke,closed,2019-12-30T07:00:09Z,2020-01-01T01:43:01Z,"- [x] closes #5764
- [x] closes #7883
- [x] closes #7820
- [x] closes #8720
- [x] closes #6783
- [x] closes #7072
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
544292559,30591,CLN: datetimelike EA and Index cleanups,jbrockmendel,closed,2020-01-01T00:19:57Z,2020-01-01T02:07:29Z,Working on sharing code between these classes better (xref #20587) and closing in on implementing ExtensionIndex.
544240527,30587,REF: share code between DatetimeIndex and TimedeltaIndex,jbrockmendel,closed,2019-12-31T17:55:12Z,2020-01-01T02:08:09Z,
544127605,30577,CLN: Clean _test_moments_consistency in common.py,charlesdong1991,closed,2019-12-31T08:38:02Z,2020-01-01T02:17:37Z,"- [ ] xref #30486 #30542 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
544295589,30592,CLN: remove no-longer-reachable addsub_int_array,jbrockmendel,closed,2020-01-01T00:56:04Z,2020-01-01T02:28:33Z,
544007913,30566,CLN: Clean test moments for expanding,charlesdong1991,closed,2019-12-30T20:51:29Z,2020-01-01T02:36:27Z,"- xref #30486 #30542 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
462886959,27169,Build warnings in template for intervaltree.pxi ,ghost,closed,2019-07-01T20:03:19Z,2020-01-01T02:45:55Z,"The `scalar_t` fused type https://github.com/pandas-dev/pandas/blob/b115a6bf5553445fdf29824623ea2b7c3a424660/pandas/_libs/intervaltree.pxi.in#L9-L14 mixes signed and unsigned types. Later on, the code does comparisons with unsigned types, which creates a bunch of compiler warning. 

xref http://docs.cython.org/en/latest/src/userguide/fusedtypes.html#type-checking-specializations

From comments in #27157 it appears there's no established guideline on how different cases for different types should look like in the templates. That's a hindrance to fixing this (and other warnings) as well."
540600801,30365,IntervalIndex Comparisons of Signed / Unsigned,WillAyd,closed,2019-12-19T22:07:08Z,2020-01-01T02:45:56Z,"Compiling intervaltree.pxi.in yields 128 warnings in the build, most of which deal with the comparison of signed and unsigned data. 

These warnings add to the compile time, but also for sure highlight bugs like this:

```python
# Create an IntervalIndex using values only in the range of uint64
>>> ser = pd.Series([1], index=pd.IntervalIndex.from_tuples([((2 ** 63), (2 ** 64)-1)]))
# Negative value isn't in those ranges, but overflow yields undefined behavior
>>> ser[[-20]]
(9223372036854775808, 18446744073709551615]    1
dtype: int64
```

I think should probably separate unsigned vs signed parametrization"
544236142,30586,REF: separate casting out of Index.__new__,jbrockmendel,closed,2019-12-31T17:29:20Z,2020-01-01T03:09:51Z,"first of two PRs to separate array casting/inference out of `Index.__new__`.  Once both are in place, we'll be able to do all inference/casting up-front and simplify the constructor quite a bit.  We'll also be able to look into sharing code between Index/Series/array, and address a handful of outstanding issues with the Index constructor."
542708167,30499,DOC: Make pyplot import explicit in the 10 minutes to pandas page,yuseitahara,closed,2019-12-27T01:19:33Z,2020-01-01T03:18:34Z,"Beginners don't know what is `plt`, thus it is better to show how to import it explicitly.

This is a continuation of https://github.com/pandas-dev/pandas/pull/30274

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
543953902,30560,BLD: Fix IntervalTree build warnings,jschendel,closed,2019-12-30T17:28:58Z,2020-01-01T03:32:42Z,"- [X] closes #27169
- [X] closes #30365
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

xref #30366 
cc @WillAyd 

"
541355602,30391,df.plot() with kind='scatter' and datetime on x axis bug,Sbrowneo,closed,2019-12-21T18:19:58Z,2020-01-01T03:39:08Z,"I noticed while helping a student that there's a bug when calling a scatter plot as a method on a DataFrame. Regardless of what the index is or what the x argument is, it appears that pandas makes the x argument into the index, and then searches for the x in the columns, where it no longer exists. When calling the same directly through matplotlib as plt.scatter, it worked properly.

<img width=""1074"" alt=""Pandas_Error_scatter"" src=""https://user-images.githubusercontent.com/35703276/71311989-e7ba6900-23f3-11ea-9911-ebf4e2b1c463.png"">
"
541857914,30434,ENH: Allow scatter plot to plot objects and datetime type data,charlesdong1991,closed,2019-12-23T18:21:38Z,2020-01-01T03:39:12Z,"closes #18755 
closes #30391 

xref #8113

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Because this is a plot PR, so I put the figures gotten by the two examples in those two PRs in this description to facilitate seeing output, and more cases are tested in test files.
![Screen Shot 2019-12-23 at 7 34 21 PM](https://user-images.githubusercontent.com/9269816/71374603-91982200-25bb-11ea-8ffd-4328fb1ea882.png)


"
544302975,30594,BUG: DTA/TDA/PA add/sub object-dtype,jbrockmendel,closed,2020-01-01T02:54:24Z,2020-01-01T03:47:01Z,Bonus: we get to get rid of is_offsetlike
544303199,30595,REF: share join methods for DTI/TDI,jbrockmendel,closed,2020-01-01T02:59:51Z,2020-01-01T04:37:15Z,"The only actual changed behavior is in the new `_is_convertible_to_index_for_join` method where it first checks:

```
if isinstance(other, cls):
    return False
```

In master, TimedeltaIndex uses _is_convertible_to_index which returns True here.  The DTI method behavior is unchanged."
534610541,30148,NoneType error during json_normalize due to schema change,bolkedebruin,closed,2019-12-08T22:08:12Z,2020-01-01T05:29:56Z,"#### Code sample
```
meta_df = json_normalize(json_struct, record_path='my_data')
```
#### Problem description

Normalizing a json with an absent field at a certain point in time due to a schema change(s) results in a NoneType error

#### Expected Output

Continue normalization

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 37b4b33075ed66721147027028d41b8faacd68a2
python           : 3.7.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.0.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 0.26.0.dev0+1247.g37b4b3307.dirty
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
534586361,30145,Fix NoneType error when pulling non existent field,bolkedebruin,closed,2019-12-08T19:13:38Z,2020-01-01T05:30:15Z,"If normalizing a jsonstruct a field can be absent
due to a schema change.

- [X] closes #30148
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
539359471,30314,DOC/CLN: move NDFrame.groupby to (DataFrame|Series).groupby,topper-123,closed,2019-12-17T23:08:14Z,2020-01-01T11:37:23Z,"By moving this method up to dataFrame/Series, we get better doc strings and more precise type hints.
"
544363760,30601,CLN: Use fstring instead of .format in io/excel and test/generic,bharatr21,closed,2020-01-01T14:25:47Z,2020-01-01T16:07:15Z,"- [x] Contributes to #29547 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
499285383,28652,groupby indexing is giving the wrong index,landmann,closed,2019-09-27T07:32:14Z,2020-01-01T16:21:27Z,"#### Code Sample, a copy-pastable example if possible

```python
a = pd.DataFrame([range(3)]*3,index=map(str, range(3))).T
a.iloc[:,0] = [10, 11, 11]

a.groupby('0').apply(lambda x: print(x.index))

indexes = a.groupby('0').apply(lambda x: x.index)
print(indexes)

index_lengths = a.groupby('0').apply(lambda x: len(x.index))
print(index_lengths)

indexes_lengths = a.groupby('0').apply(lambda x: x.index).apply(len)
print(indexes_lengths)

assert indexes_lengths.equals(index_lengths), ""Apply indexes are returning the wrong index""
```
#### Problem description

Groupby indexing is returning the wrong value. It is returning the indexes of the last group for all the groups.

#### Expected Output

```
0
10    Int64Index([0], dtype='int64')
11    Int64Index([1, 2], dtype='int64')
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.2.0-arch2-1-ARCH
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : C
LOCALE           : None.None

pandas           : 0.25.1
numpy            : 1.15.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : 0.28.5
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : 0.4.0
xlsxwriter       : None
lxml.etree       : 4.3.4
html5lib         : None
pymysql          : None
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : None
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 2.2.2
numexpr          : 2.6.9
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
s3fs             : 0.1.5
scipy            : 1.3.1
sqlalchemy       : None
tables           : 3.4.4
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None


</details>
"
529329494,29883,CLN: Clean up of locale testing,datapythonista,closed,2019-11-27T12:55:58Z,2020-01-01T16:25:42Z,"- [X] xref #23923, https://github.com/pandas-dev/pandas/pull/29852#discussion_r351228283

There are couple of things I don't understand from the locale:

- For what I see, we test with different locales (e.g. `it_IT.UTF-8`) but pandas never uses the language (`it_IT`) only the encoding (`UTF-8`). So, not sure if testing with different locales is being useful.
- The variable `LOCALE_OVERRIDE` seems to add more complexity than value. Unless `LC_ALL`/`LANG` are being set afterwards, but I don't think so.
- There is a test that is using `LOCALE_OVERRIDE` but doesn't seem to depend on the actual locale of the system. It probably makes more sense to parametrize and test all the locales we want, than use the variable value (when testing locally this test will be more useful).
- There is a ""test"" being skipped in `run_tests.sh` that is probably worth converting to an actual test.

Addressing these things here, but I may be missing something here. Please let me know if that's the case."
41098901,8113,VIS: DataFrame.plot drops datetime data when kind is scatter,TomAugspurger,closed,2014-08-25T19:55:47Z,2020-01-01T16:26:02Z,"This works fine:

``` python
In [13]: df = pd.DataFrame(np.random.randn(300), columns=['a'])
In [14]: df['dtime'] = pd.DatetimeIndex(start='2014-01-01', freq='h', periods=300).time
In [15]: df.plot(x='dtime', y='a')
Out[15]: <matplotlib.axes._subplots.AxesSubplot at 0x118fd17f0>
```

This raises a `KeyError`

``` python
In [17]: df.plot(x='dtime', y='a', kind='scatter')
```

We call `df._get_numeric_data()` which excludes datetimes.
May happen for other kinds too.

I'll fix this; just have to decide how much refactoring.

Matplotlib is ok with `datetime.time` values, it chokes on `datetime` values.
"
544371817,30602,TST: Add test for TypeError when using datetime.time in scatter plot,charlesdong1991,closed,2020-01-01T15:52:57Z,2020-01-01T16:26:05Z,"- [ ] closes #8113 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
542729293,30507,"BUG: pass 2D ndarray and EA-dtype to DataFrame, closes #12513",jbrockmendel,closed,2019-12-27T03:28:12Z,2020-01-01T16:31:44Z,"- [x] closes #12513
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
544312282,30599,REF: share _wrap_joined_index,jbrockmendel,closed,2020-01-01T05:12:17Z,2020-01-01T16:32:50Z,"this is the last of the trivial ones AFAICT.  To get the others we're going to have to smooth out small differences in behavior, which should happen in non-refactoring PRs."
532350584,30024,Type Annotations for df.boxplot,JojoYY,closed,2019-12-04T00:04:11Z,2020-01-01T17:22:44Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
531194135,29967,DOC: .get_slice_bound in MultiIndex needs documentation.,proost,closed,2019-12-02T15:15:27Z,2020-01-01T17:27:21Z,"```
In [10]: i = Index(['c','a','d','b'])                                           
In [12]: i.get_slice_bound('a',side=""left"",kind=""ix"")                           
Out[12]: 1
In [13]: mi = MultiIndex.from_arrays([['c','a','d','b']])                       
In [15]: mi.get_slice_bound('a',side=""left"",kind=""ix"")                          
---------------------------------------------------------------------------
UnsortedIndexError                        Traceback (most recent call last)
<ipython-input-15-fbc28595630f> in <module>
----> 1 mi.get_slice_bound('a',side=""left"",kind=""ix"")
~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/multi.py in get_slice_bound(self, label, side, kind)
   2523         if not isinstance(label, tuple):
   2524             label = (label,)
-> 2525         return self._partial_tup_index(label, side=side)
   2526 
   2527     def slice_locs(self, start=None, end=None, step=None, kind=None):
~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/multi.py in _partial_tup_index(self, tup, side)
   2585             raise UnsortedIndexError(
   2586                 ""Key length (%d) was greater than MultiIndex""
-> 2587                 "" lexsort depth (%d)"" % (len(tup), self.lexsort_depth)
   2588             )
   2589 
UnsortedIndexError: 'Key length (1) was greater than MultiIndex lexsort depth (0)'
```

But, If `MultiIndex` is sorted,

```
In [17]: mi = MultiIndex.from_arrays([['a','b','c','d']])                       
In [18]: mi.get_slice_bound('a',side=""left"",kind=""ix"")                          
Out[18]: 0
``` 

If only sorted 'MultiIndex' can work is not bug  but intended it. then I think need documentation for 'MultiIndex'.  Because just  documentation on 'Index.get_slice_bound' is not enough to recognize '.get_slice_bound' only works with sorted 'MultiIndex'



<details>
INSTALLED VERSIONS

------------------
commit           : None
python           : 3.7.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.0.0-36-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : ko_KR.UTF-8
LOCALE           : ko_KR.UTF-8

pandas           : 0.25.1
numpy            : 1.16.1
pytz             : 2018.7
dateutil         : 2.7.5
pip              : 19.3.1
setuptools       : 40.6.3
Cython           : 0.29.2
pytest           : 5.1.0
hypothesis       : None
sphinx           : 1.8.2
blosc            : None
feather          : None
xlsxwriter       : 1.1.2
lxml.etree       : 4.2.5
html5lib         : 1.0.1
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.2.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.5
matplotlib       : 3.0.2
numexpr          : 2.6.8
odfpy            : None
openpyxl         : 2.5.12
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : 1.2.15
tables           : 3.4.4
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.2


</details>
"
542415906,30478,CI: Fix GBQ Tests,alimcmaster1,closed,2019-12-26T03:03:31Z,2020-01-01T18:04:12Z,"Believe these test only run on master on travis? So will keep xfail for now

Fixturize as per @jreback comment [here](https://github.com/pandas-dev/pandas/issues/30470#issuecomment-568942914).

closes #30470 "
531296168,29975,pd.NA `na_rep` truncated in to_csv,WillAyd,closed,2019-12-02T16:59:14Z,2020-01-01T18:18:10Z,"Just found this while messing around with how the new pd.NA sentinel would work with extension modules

```python
>>> pd.Series([""a"", pd.NA, ""c""]).to_csv(na_rep=""ZZZZZ"")
',0\n0,a\n1,ZZZZZ\n2,c\n'
>>> pd.Series([""a"", pd.NA, ""c""], dtype=""string"").to_csv(na_rep=""ZZZZZ"")
',0\n0,a\n1,ZZ\n2,c\n'
```

Note the latter is truncated to two letters

@jorisvandenbossche "
534605075,30146,BUG: Fix pd.NA `na_rep` truncated in to_csv,jbman223,closed,2019-12-08T21:30:33Z,2020-01-01T18:18:14Z,"- [x] closes #29975
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
520965174,29543,Add type hinting to public-facing API  in pandas/core/generic.py #26792,tobyjamez,closed,2019-11-11T13:31:19Z,2020-01-01T18:20:47Z,"- [x] Part of #26792
- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
516716343,29376,TST: new test for sort index when Nan in other axis.,ainsleyto,closed,2019-11-02T20:45:37Z,2020-01-01T18:33:43Z,"closes #12261 
test added / passed
passes black pandas"
511542682,29193,Test For MultiIndex not contained item,gabriellm1,closed,2019-10-23T20:05:14Z,2020-01-01T18:34:42Z,"- [x] closes #21094
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I just made the test that was requested in this issue but in fact it isn't actually passing like mentioned there. Maybe my master is not updated for this. Should tests be added to whatsnew entry too?
"
494963049,28492,BUG: Raise when casting NaT to int,dsaxton,closed,2019-09-18T03:17:27Z,2020-01-01T18:52:11Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] whatsnew entry

Fixes a bug in `astype_nansafe` where `NaT` was ignored when casting a datetime or timedelta to `int`.  I put the test in `pandas/tests/dtypes/test_common.py` since I couldn't find another place where `astype_nansafe` was tested.  Also adds various other tests for `astype_nansafe`.

Related: https://github.com/pandas-dev/pandas/pull/28438"
544174660,30579,STY: Concat string,ShaharNaveh,closed,2019-12-31T12:11:41Z,2020-01-01T19:22:34Z,"- [x] ref #30454
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
422003358,25760,RecursionError when aligning DataFrames based on MultiIndex with different order of names,abirkmanis,closed,2019-03-18T01:40:01Z,2020-01-01T20:46:46Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np
x=pd.DataFrame(np.arange(4),pd.MultiIndex.from_product([[1,2],[3,4]],names=['a','b']))
y=pd.DataFrame(np.arange(4),pd.MultiIndex.from_product([[3,4],[1,2]],names=['b','a']))
print(x+y)
```
#### Problem description
RecursionError in align()/join().
The ideal expected behavior is to calculate the sum.
If different orders of names in MultiIndex are not supported by design, then a clear error message stating that would be preferable to RecursionError.

#### Expected Output
```
     0
a b   
1 3  0
  4  3
2 3  3
  4  6
```
or
```
     0
b a   
3 1  0
  2  3
4 1  3
  2  6
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.1.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.125-linuxkit
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: en_US.UTF-8
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: None
pip: 19.0.3
setuptools: 40.8.0
Cython: 0.29.6
numpy: 1.15.4
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 7.3.0
sphinx: None
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: 2.6.9
feather: None
matplotlib: 3.0.3
openpyxl: None
xlrd: 1.2.0
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: 4.7.1
html5lib: None
sqlalchemy: 1.3.1
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
513596984,29260,Multiindex recurse error fix,endremborza,closed,2019-10-28T22:43:15Z,2020-01-01T20:46:53Z,"test and 3 line fix for a small bug mentioned in 2 issues (1 closed). test might be expanded, I only checked for the error disappearing

- [x] closes #25760 (and #28956 )
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
534479688,30138,BUG: incorrect output of first('1M') in case if first index is the last day of the month (#29623),Franklinluo17,closed,2019-12-08T02:00:54Z,2020-01-01T20:52:12Z,"- [ ] closes #29623 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
495455151,28511,Pick and choose numpy version based on Python 2 or 3.,didip,closed,2019-09-18T21:06:07Z,2020-01-01T21:00:02Z,"- [x] closes #27435
- [ ] tests added / passed
- [ ] passes `black pandas` (not applicable, `black setup.py` changes too many quotes, polluting the diff).
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @jorisvandenbossche"
509302116,29081,"`values=['c']` returns `aggfunc` on all columns, instead of just the passed column",quanghm,closed,2019-10-18T20:44:46Z,2020-01-01T21:02:02Z,"- [ ] closes #29080

"
544305875,30598,CLN: Remove int32 and float32 dtypes from IntervalTree,jschendel,closed,2020-01-01T03:43:53Z,2020-01-01T21:14:22Z,"There isn't a practical way to actually get an `IntervalTree` with `int32`/`float32` dtypes since `IntervalIndex` is backed by two pandas indexes and we don't have a `Int32Index` or `Float32Index`. These indexes are used to create the underlying `IntervalTree` that backs an `IntervalIndex`, so we're guaranteed to have `int64`/`float64` dtype data when initializing.

The only way that comes to mind that a user could create an `IntervalTree` with `int32`/`float32` dtype would be by explicitly initializing a standalone `IntervalTree` with `int32`/`float32` arrays, which doesn't seem particularly likely.

This should also help with build times as it results in 8 less node classes being generated.

Didn't add a whatsnew note since I don't think `IntervalTree` is user facing (could be wrong?) but can add one if desired since this is technically a breaking change."
528423418,29847,PERF: perform reductions block-wise,jbrockmendel,closed,2019-11-26T00:58:43Z,2020-01-01T23:33:51Z,
540606215,30366,Removed unsigned parametrization from Intervaltree,WillAyd,closed,2019-12-19T22:17:03Z,2020-01-01T23:58:07Z,"closes #30365 , #27169

Unclear if we currently consider this a feature, but I couldn't find anything explicitly asking for it so trying the removal route first

Not sure what the other alternative would be for parametrization, but I think generally we need to be careful when doing comparisons between signed and unsigned in templating or fused types

Side benefit - this gets rid of 128 build warnings and a slight perf boost to compilation
"
538830875,30295,"JSON Support for parsing NaN, Infinity and -Infinity",WillAyd,closed,2019-12-17T04:50:17Z,2020-01-02T00:40:50Z,"- [X] closes #12213
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

Not sure if we totally want this but figured I'd give it a shot any way. `simplejson` and the stdlib by comparison would decode these values
"
489109007,28282,DataFrame.itertuples() incorrectly determines when plain tuples should be used,plamut,closed,2019-09-04T11:43:15Z,2020-01-02T00:58:16Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import pandas, sys
>>> sys.version
'3.6.7 (default, Oct 25 2018, 09:16:13) \n[GCC 5.4.0 20160609]'
>>> pandas.__version__
'0.25.1'
>>> df = pandas.DataFrame([{f""foo_{i}"": f""bar_{i}"" for i in range(255)}])
>>> df.itertuples(index=False)
...
SyntaxError: more than 255 arguments
```
The issue seems to have been caused/revealed by [this commit](https://github.com/pandas-dev/pandas/commit/81f5c0164e6cb26cc8fea00ebc22f4fe93771ff6#diff-1e79abbbdd150d4771b91ea60a4e1cc7R903) that removed the try-catch block around the namedtuple class creation.

FWIW, this issue is _not_ reproducible in version `0.24.2`, and is also not a problem in Python 3.7+, as the limit of the max number of arguments that can be passed to a function has been removed (AFAIK).

#### Problem description

The [condition](https://github.com/pandas-dev/pandas/blob/v0.25.1/pandas/core/frame.py#L970) in `itertuples()` method does not correctly determine when plain tuples should be used instead of named tuples.

This how the named tuple [class template](https://github.com/python/cpython/blob/3.6/Lib/collections/__init__.py#L301-L349) defines the `__new__()` method (in Python 3.6 at least):
```py
""""""
...
def __new__(_cls, {arg_list}):
    ...
""""""
```

If there are 255 column names given, the total number of arguments to `__new__()` will be 256, because of that extra `cls`, causing a syntax error.

"
542678822,30496,Improve ISO Date Performance for JSON,WillAyd,closed,2019-12-26T21:53:14Z,2020-01-02T01:09:59Z,"benchmarks below 

```sh
       before           after         ratio
     [9c6771c5]       [5c0f5682]
     <master>         <json-index-dates>
-        231±60ms          188±3ms     0.82  io.json.ToJSON.time_iso_format('split', 'df_td_int_ts')
-        496±20ms         218±40ms     0.44  io.json.ToJSON.time_iso_format('index', 'df_int_float_str')
-        486±20ms          207±2ms     0.42  io.json.ToJSON.time_iso_format('columns', 'df_int_float_str')
-         499±9ms          210±1ms     0.42  io.json.ToJSON.time_iso_format('index', 'df_date_idx')
-        503±20ms          206±2ms     0.41  io.json.ToJSON.time_iso_format('columns', 'df_int_floats')
-        515±20ms          210±1ms     0.41  io.json.ToJSON.time_iso_format('index', 'df')
-        524±80ms        209±0.5ms     0.40  io.json.ToJSON.time_iso_format('index', 'df_int_floats')
-        528±10ms          206±2ms     0.39  io.json.ToJSON.time_iso_format('columns', 'df_td_int_ts')
diff --git a/asv_bench/asv.conf.json b/asv_bench/asv.conf.json
index c04bbf53a..897de2f85 100644
--- a/asv_bench/asv.conf.json
+++ b/asv_bench/asv.conf.json
-        546±80ms        209±0.7ms     0.38  io.json.ToJSON.time_iso_format('index', 'df_td_int_ts')
-        568±60ms          208±3ms     0.37  io.json.ToJSON.time_iso_format('columns', 'df_date_idx')
-       598±100ms          206±2ms     0.35  io.json.ToJSON.time_iso_format('columns', 'df')

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE INCREASED.
```

Note that this mostly improves on DTI which on 0.25.3 can't even be written as ISO format, so I didn't add a whatsnew. Timedelta is the big bottleneck remaining"
542202895,30454,"CI: code check for "" "" introduced by black",jbrockmendel,closed,2019-12-24T20:07:34Z,2020-01-02T01:10:32Z,"Sometimes when we run `black` on something like

```
foo = (
    ""bar ""
    ""baz""
)       
```

we end up with `foo = ""bar "" ""baz""` when we would want `foo = ""bar baz""`.

This is [not](https://github.com/psf/black/issues/1051) considered a bug in `black`, so is something we're currently checking for manually.  We should add it to code_checks."
542378102,30467,CI: Add test case for unwanted patterns,ShaharNaveh,closed,2019-12-25T19:29:47Z,2020-01-02T01:31:05Z,"- [x] closes #30454
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Can be merged, after merging #30464"
527044972,29788,ENH: Allow map with abc mapping,ohad83,closed,2019-11-22T07:58:44Z,2020-01-02T01:35:39Z,"- [x] closes #29733 (partly, look at my issue comment for details on why it won't exactly work on the issue's example)
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
541441619,30400,TYP: Type hints in pandas/io/formats/excel.py,ShaharNaveh,closed,2019-12-22T11:56:16Z,2020-01-02T01:44:15Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
544406115,30606,"CLN: remove CategoricalIndex.itemsize, should have gone in #29918",jbrockmendel,closed,2020-01-01T22:34:38Z,2020-01-02T01:44:41Z,
544413958,30608,CLN: remove warnings clearing,jbrockmendel,closed,2020-01-02T00:11:05Z,2020-01-02T02:40:33Z,"the tm.assert_produces_warning `clear` kwarg is no longer used, we could consider removing it"
544411733,30607,REF: Delegate more methods for DTI/TDI/PI,jbrockmendel,closed,2020-01-01T23:43:35Z,2020-01-02T02:42:23Z,
542642436,30489,DEPR: Deprecate pandas.datetime,ryankarlos,closed,2019-12-26T18:48:06Z,2020-01-02T04:24:54Z,"- [x] closes #30610
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
542705214,30497,CLN: OrderedDict -> Dict,alimcmaster1,closed,2019-12-27T00:59:18Z,2020-01-02T07:45:43Z,"- [x] ref #30469

"
130841150,12213,"Support Infinity, -Infinity and NaN in read_json",maxnoe,closed,2016-02-02T22:33:18Z,2020-01-02T11:10:16Z,"While these special values are not strictly standard conform, most implementations do allow or use them.

For example Google's GSON Library or python's json module in the standard library.
"
544556056,30617,merge errors with pandas 0.25.3,ranedk,closed,2020-01-02T12:15:24Z,2020-01-02T12:44:16Z,"I am using using pandas version 0.25.3 and Python 3.7.4

```python
students = pd.DataFrame({
    'name': ['Dev', 'Prem', 'Kiran', 'Manish', 'Sandeep', 'Deepika', 'Amit'],
    'class': ['7A', '7B', '7D', '7A', '7B', '7B', '7C'],
    'gender': ['M', 'M', 'F', 'M', 'M', 'F', 'M'],
    'roll': [1, 2, 3, 4, 5, 6, 7]
}).set_index('roll')

maths = pd.DataFrame({
    'sr': [1, 2, 3, 5, 6],
    'marks': [45, 23, 76, 87, 45],
}).set_index('sr')

students.merge(maths, left_on='roll', right_on='sr')
# KeyError: ""None of ['roll'] are in the columns""

pd.merge(students, maths, left_on='roll', right_on='sr')
# KeyError: ""None of ['roll'] are in the columns""
```
#### Problem description
When I used pandas 0.25.1, `students.merge` worked but `pd.merge` did not work.
Now both aren't working.

#### Output of ``pd.show_versions()``

<details>
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.0.0-36-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.2
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>"
544010454,30567,Boolean indexing into a series with assignment,IvanUkhov,closed,2019-12-30T21:02:16Z,2020-01-02T13:59:22Z,"I am curious if the following is the intended behavior:

```python
x = pd.Series([None] * 10)
y = [False] * 3 + [True] * 5 + [False] * 2
x[y] = range(5)
x

# 0     None
# 1     None
# 2     None
# 3        3
# 4        4
# 5        0
# 6        1
# 7        2
# 8     None
# 9     None
```

The expectation was that it would work as with `x.loc[y] = range(5)`:

```python
# 0    None
# 1    None
# 2    None
# 3       0
# 4       1
# 5       2
# 6       3
# 7       4
# 8    None
# 9    None
```

If so, where can one read about the logic behind?

<details>
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2018.4
dateutil         : 2.5.0
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : 5.1.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 3.8.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.8
IPython          : 6.5.0
pandas_datareader: None
xlsxwriter       : None
pandas_gbq       : None
pyarrow          : 0.15.1
bs4              : 4.7.1
numexpr          : None
xarray           : None
fastparquet      : None
tables           : None
s3fs             : None
odfpy            : None
pytables         : None
xlrd             : None
gcsfs            : None
xlwt             : None
sqlalchemy       : 1.3.4
scipy            : 1.3.1
openpyxl         : None
lxml.etree       : 3.8.0
matplotlib       : 3.0.3
bottleneck       : None
</details>
"
362590732,22798,Support numpy ufuncs for ExtensionArrays,jorisvandenbossche,closed,2018-09-21T12:20:01Z,2020-01-02T15:10:40Z,"Currently calling numpy ufuncs such as `np.exp` on a Series[EA] or EA does not work yet:

```
In [44]: s = pd.Series([1, 2, 3, 4], dtype='Int64')

In [45]: np.exp(s)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-45-fb7258693ae9> in <module>()
----> 1 np.exp(s)

~/scipy/pandas/pandas/core/series.py in __array_prepare__(self, result, context)
    671                                 obj=type(obj).__name__,
    672                                 dtype=getattr(obj, 'dtype', None),
--> 673                                 op=context[0].__name__))
    674         return result
    675 

TypeError: Series with dtype Int64 cannot perform the numpy op exp

In [46]: np.exp(s.values)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-46-69f0b0471ea8> in <module>()
----> 1 np.exp(s.values)

AttributeError: 'int' object has no attribute 'exp'

In [47]: np.exp(s.astype(int))   # but works for numpy dtyped series
Out[47]: 
0     2.718282
1     7.389056
2    20.085537
3    54.598150
dtype: float64
```

I think it would be nice to have this working, and without looking in detail into it, I would assume the best way to go is to actually support the `__array_ufunc__` protocol on ExtensionArrays itself and to ensure Series then properly uses that?"
347942208,22216,ExtensionArray.astype and missing values,TomAugspurger,closed,2018-08-06T14:15:18Z,2020-01-02T15:16:04Z,"How should ExtensionArray.astype handle coercing values to dtype?

Currently, we seem to follow NumPy:

```python
In [19]: np.array([1, 2, np.nan]).astype('i8')
Out[19]: array([                   1,                    2, -9223372036854775808])

In [20]: pd.Categorical([1, 2, None]).astype('i8')
Out[20]: array([                   1,                    2, -9223372036854775808])
``` 

Do we want that behavior? SparseArray currently raises.

Note that at least for Categorical, this will require scanning the data for missing values, which is rarely a good idea."
544405787,30605,"REF: delegate attrs for CategoricalIndex, IntervalIndex",jbrockmendel,closed,2020-01-01T22:30:17Z,2020-01-02T16:52:59Z,"Working towards creating an ExtensionIndexMixin to be shared by all our backed-be-EA indexes, which would ideally grow into ExtensionIndex.

Removes CategoricalIndex._codes_for_groupby, which would raise AttributeError if it were ever called."
544268553,30589,How can I force Pandas `read_html` function to read digit field as string not integer,Zhenye-Na,closed,2019-12-31T20:49:23Z,2020-01-02T17:57:43Z,"Is there a possible way to convert the field from `int` to `str`?

> I have explored the issues like https://github.com/pandas-dev/pandas/issues/10534, https://github.com/pandas-dev/pandas/issues/21379, https://github.com/gte620v/pandas/blob/5cb8243f2dd31cc2155627f29cfc89bbf6d4b84b/pandas/io/tests/test_html.py#L715
>
> I do not think `converters` arg fit for our usage since the table is updated everyday and it may add a new column, then we need manually add a new key to the parameter


Here is the entire stacktrace when I used the function

```
PS C:\Users\Zhenye.na\Desktop> python3 .\dash-prod.py
.\dash-prod.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  from collections import Mapping, Iterable
Traceback (most recent call last):
  File "".\dash-prod.py"", line 59, in <module>
    df = pd.read_html(response.text, skiprows=1)
  File ""C:\Users\Zhenye.na\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\pandas\io\html.py"", line 1105, in read_html
    displayed_only=displayed_only,
  File ""C:\Users\Zhenye.na\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\pandas\io\html.py"", line 915, in _parse
    for table in tables:
  File ""C:\Users\Zhenye.na\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\pandas\io\html.py"", line 213, in <genexpr>
    return (self._parse_thead_tbody_tfoot(table) for table in tables)
  File ""C:\Users\Zhenye.na\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\pandas\io\html.py"", line 411, in _parse_thead_tbody_tfoot
    header = self._expand_colspan_rowspan(header_rows)
  File ""C:\Users\Zhenye.na\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\pandas\io\html.py"", line 459, in _expand_colspan_rowspan
    colspan = int(self._attr_getter(td, ""colspan"") or 1)
ValueError: invalid literal for int() with base 10: '\\""1\\""'
```

The core usage of `read_html` function code is as follows:

```python
response = requests.get(url, headers=hdrs)
df = pd.read_html(response.text, skiprows=1)[0]
print(df)
```

I would love to use the `read_html` function to extract the table in the response returned from the REST API. I have test the function in a small scale table, which contains only digits and it works. But for the data returned from REST API contains characters and digits.

Here is a demo of what the table looks like: (Assume `DC1` and `Location 1` has one `'\n'` symbol separated)


|  Date | DC 1  Location 1 | DC 2   Location 2 | DC 3   Location 3 |
|:-----:|:----------------:|:-----------------:|:-----------------:|
| 03/04 |      1.23.4      |       1.23.4      |       1.23.4      |
| 04/05 |      1.23.4      |       1.23.4      |       1.23.4      |


I assume the error message may because of the `'.'` symbol in field like `1.23.4` but I am not sure how to fix it.

Any ideas or thoughts are appreciated!

Thanks!
"
542305829,30463,"pct_change can't work well with groupby, when fill_method =None",ForrestLin0805,closed,2019-12-25T09:40:33Z,2020-01-02T18:09:58Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np
data = pd.DataFrame(np.random.random((10,2)), index=['a', 'b']*5)
data.iloc[1:3,:] = np.nan
```
for pct_change function, when the fill_method = None, it works

```
data.pct_change(1, fill_method=None, limit=1)
	0	1
a	NaN	NaN
b	NaN	NaN
a	NaN	NaN
b	NaN	NaN
a	-0.498169	-0.568501
b	-0.315982	1.340587
a	1.341901	-0.489576
b	0.088594	-0.691063
a	-0.514451	0.054695
b	0.844514	-0.604511
```
but when use it with gourpby , it raise a error

```
data.reset_index().groupby('index')[0].pct_change(1, fill_method=None, limit=1)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-18-60898304743e> in <module>
----> 1 data.reset_index().groupby('index')[0].pct_change(1, fill_method=None, limit=1)

~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py in pct_change(self, periods, fill_method, limit, freq)
   1344                 )
   1345             )
-> 1346         filled = getattr(self, fill_method)(limit=limit)
   1347         fill_grp = filled.groupby(self.grouper.labels)
   1348         shifted = fill_grp.shift(periods=periods, freq=freq)

TypeError: getattr(): attribute name must be string
```
else, `limit = 0` worked with gourpby, but can't run in pct_change

```
data.reset_index().groupby('index')[0].pct_change(1, fill_method='pad', limit=0)  
0         NaN
1         NaN
2         NaN
3         NaN
4         NaN
5   -0.656739
6    0.601904
7    1.549380
8   -0.471434
9   -0.104398
Name: 0, dtype: float64  
``` 
```
data[0].pct_change(1, fill_method='pad', limit=0)  

~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/missing.py in pad_2d(values, limit, mask, dtype)
    546 
    547     if np.all(values.shape):
--> 548         algos.pad_2d_inplace(values, mask, limit=limit)
    549     else:
    550         # for test coverage

pandas/_libs/algos.pyx in pandas._libs.algos.pad_2d_inplace()

ValueError: Limit must be greater than 0
```

pandas version is 0.25.1
"
532709149,30050,TypeError in to_datetime when passing Int64 column,sanderland,closed,2019-12-04T14:10:14Z,2020-01-02T20:38:41Z,"#### Code Sample

```python
df = pd.DataFrame({'a':[1,2,3]})
df.a = df.a.astype(""Int64"")
pd.to_datetime(df.a, unit='ms')
```
#### Problem description
When a nullable int type is used, to_datetime gives the error:

`TypeError: Argument 'values' has incorrect type (expected numpy.ndarray, got IntegerArray)`

The error is unexpected and hard to track down (reliant on pandas internal storage formats).

#### Expected Output
Converted date times or NaT when input is NaN

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.0.0-36-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.0
pytz             : 2018.9
dateutil         : 2.8.0
pip              : 19.0.3
setuptools       : 41.0.1
Cython           : 0.29.6
pytest           : 4.3.1
hypothesis       : None
sphinx           : 1.8.5
blosc            : None
feather          : None
xlsxwriter       : 1.1.5
lxml.etree       : 4.3.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 7.4.0
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.2
matplotlib       : 3.0.3
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.6.1
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : 1.3.1
tables           : 3.5.1
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.5
</details>
"
544450780,30616,encode,shaneson0,closed,2020-01-02T05:26:21Z,2020-01-02T21:13:28Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here

```
#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
544720927,30624,whatsnew fixups,TomAugspurger,closed,2020-01-02T19:51:48Z,2020-01-02T21:42:31Z,
543975084,30561,WIP: Remove clipboard xfail,alimcmaster1,closed,2019-12-30T18:45:29Z,2020-01-02T21:53:12Z,"- [x] closes #29676

Ref work done here: https://github.com/pandas-dev/pandas/pull/29712"
544423802,30612,CLN: replacing str.format with f-strings in several files. #29547,jlamborn324,closed,2020-01-02T02:00:41Z,2020-01-03T00:36:54Z,"- [ ] xref #29547 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
544746881,30627,REF: remove IntervalIndex.copy,jbrockmendel,closed,2020-01-02T21:06:42Z,2020-01-03T00:52:19Z,cc @jschendel the existing behavior treats `name` differently than pretty much all our other copy methods.  Was there a reason for that?  It doesn't appear to be tested.
544716233,30623,REF: standardize usage with _make_wrapped_arith_op,jbrockmendel,closed,2020-01-02T19:39:09Z,2020-01-03T00:53:10Z,
398101493,24716,DataFrame.to_xarray produces FutureWarning for DatetimeTZ data,TomAugspurger,closed,2019-01-11T02:16:20Z,2020-01-03T01:17:46Z,"```python
In [16]: df = pd.DataFrame({""A"": pd.date_range('2000', periods=12, tz='US/Central')})

In [17]: df.to_xarray()
/Users/taugspurger/Envs/pandas-dev/lib/python3.7/site-packages/xarray/core/dataset.py:3111: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.
        To accept the future behavior, pass 'dtype=object'.
        To keep the old behavior, pass 'dtype=""datetime64[ns]""'.
  data = np.asarray(series).reshape(shape)
Out[17]:
<xarray.Dataset>
Dimensions:  (index: 12)
Coordinates:
  * index    (index) int64 0 1 2 3 4 5 6 7 8 9 10 11
Data variables:
    A        (index) datetime64[ns] 2000-01-01T06:00:00 ... 2000-01-12T06:00:00
```

@shoyer thoughts on how to resolve this? We can continue dropping the timezone and passing datetime64[ns], break API and return an object-dtype array of timestamps, or add a parameter so that the user can control this."
251811937,17304,feature wanted: pd.DataFrame.info() should show line numbers,victor-luu191,closed,2017-08-22T01:24:38Z,2020-01-03T01:18:53Z,"#### Problem description

When we load data frames, sometimes there are warning of mixed types on some columns via index of columns. It will be then easier for us to check which column is it (by name) if the indices (i.e. line numbers) are shown in pd.DataFrame.info()."
544813552,30634,pandas.DataFrame.rolling.apply Can't deal with np.nan,LiuHang9527,closed,2020-01-03T01:13:01Z,2020-01-03T02:03:02Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
import pandas as pd
import numpy as np

df = pd.DataFrame({'a':[1,2,3,4,5], 'b':[10,9,8,7,np.nan], 'c':[8,7,6,5,4], 'd':[4,np.nan,6,7,8]})
df

def sum_(data):
    return data.sum()
res= df.rolling(3,center=False).apply(sum_,raw=True)
res
```
#### Problem description

[pd.DataFrame.rolling.apply    Can't deal with np.nan]
"
544759873,30629,REF: implement indexes.extension to share delegation,jbrockmendel,closed,2020-01-02T21:44:35Z,2020-01-03T02:09:43Z,"In following steps, I plan to

- remove the DatetimeDelegateMixin entirely and just use inherit_names
- de-duplicate the slightly-different comparison method code from CategoricalIndex vs DatetimelikeIndex
- move the remaining wrapping utilities from datetimelike to extension.py, after double-checking if we can remove any layers
- delegate more methods, some of which depends on smoothing out small idiosyncrasies, e.g. #30627."
543330427,30533,Inconsistent index in result of groupby apply ,fujiaxiang,closed,2019-12-29T07:15:53Z,2020-01-03T02:12:53Z,"### I'm not really sure if this is an ""issue"" or the intended behavior, but it just seems unnatural to me to get this result.

```python
import pandas as pd

df = pd.DataFrame({
    'a': [1, 1, 2, 2],
    'b': [3, 3, 4, 5],
})
df

   a  b
0  1  3
1  1  3
2  2  4
3  2  5
```
```python
df1 = df.groupby('a').apply(lambda x: x)
df1

   a  b
0  1  3
1  1  3
2  2  4
3  2  5
```
```python
df2 = df.groupby('a').apply(lambda x: x.copy())
df2

     a  b
a
1 0  1  3
  1  1  3
2 2  2  4
  3  2  5
```
Notice that the two produces different results.
A simple code trace tells me this difference comes from `BaseGrouper.apply` which return `mutated=False` for `.apply(lambda x: x)` and `mutate=True` for `.apply(lambda x: x.copy())`

This could cause unwanted result when we try to concatenate them.
```python
pd.concat([df1, df2])

        a  b
0       1  3
1       1  3
2       2  4
3       2  5
(1, 0)  1  3
(1, 1)  1  3
(2, 2)  2  4
(2, 3)  2  5
```

Good thing is, the `apply` method is able to handle this concatenation automatically.
```python
def func(data):
    if 1 in data['a'].values:
        return data
    else:
        return data.copy()

df.groupby('a').apply(func)

   a  b
0  1  3
1  1  3
2  2  4
3  2  5
```


#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.2.post20191203
Cython           : 0.29.14
pytest           : 5.3.2
hypothesis       : 4.44.2
sphinx           : 2.3.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.6
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.10.2
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : 1.3.11
tables           : 3.6.1
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.6
</details>
"
507954150,29032,Deprecate getting the name from MultiIndex.levels,TomAugspurger,closed,2019-10-16T16:15:00Z,2020-01-03T02:16:40Z,"Follow up to https://github.com/pandas-dev/pandas/pull/27242.

https://github.com/pandas-dev/pandas/pull/27242#issuecomment-542602004

"
544034858,30574,API: Raise when setting name via level,TomAugspurger,closed,2019-12-30T22:52:50Z,2020-01-03T02:16:43Z,"Closes https://github.com/pandas-dev/pandas/issues/29032

This is extremely ugly, but gets the job done. It's probably worth doing to avoid silently failing, but not sure.

cc @topper-123."
521163557,29551,Add pandarallel to ecosystem documentation,nalepae,closed,2019-11-11T20:13:02Z,2020-01-03T02:24:24Z,"As proposed by datapythonista here: https://github.com/nalepae/pandarallel/issues/28
"
496657092,28562,DEPR: Deprecate numpy argument in read_json,lucaionescu,closed,2019-09-21T13:39:59Z,2020-01-03T02:44:28Z,"- [x] closes #28512 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
544790108,30630,CI: Fix Flakey GBQ Tests,alimcmaster1,closed,2020-01-02T23:25:10Z,2020-01-03T02:45:10Z,"-ref https://github.com/pandas-dev/pandas/pull/30478#issuecomment-570068174
We see the below in the logs:
```
google.api_core.exceptions.Conflict: 409 POST https://bigquery.googleapis.com/bigquery/v2/projects/pandas-travis/datasets: Already Exists: Dataset pandas-travis:pydata_pandas_bq_testing_py31
```
https://travis-ci.org/pandas-dev/pandas/jobs/631599036

Despite attempting to delete the dataset in the previous line.
`self.client.delete_dataset(self.dataset, delete_contents=True)`

Since we run with `dist=loadfile` these tests are run sequentially by pytest. But they could potentially clash across builds?

We now create a unique dataset name per test function and teardown when complete

GBQ Tests will run against my fork will post results on here.

cc. @jreback, @tswast"
544305386,30596,REF: move inference/casting out of Index.__new__,jbrockmendel,closed,2020-01-01T03:38:59Z,2020-01-03T02:48:19Z,"The new function operates only on arrays, not Indexes.  This gets us close to being able to just use `pd.array` here."
543549207,30546,DEPR: DataFrame GroupBy indexing with single items DeprecationWarning,yehoshuadimarsky,closed,2019-12-29T20:11:10Z,2020-01-03T03:05:19Z,"

- [x] closes #23566
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
544827387,30635,BUG: Index.__new__ with Interval/Period data and object dtype,jbrockmendel,closed,2020-01-03T02:34:15Z,2020-01-03T03:47:55Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref #17246, #21311

After this we now use the same pattern when calling DatetimeIndex, TimedeltaIndex, PeriodIndex, and IntervalIndex, so we can make a helper function and de-duplicate this code.

The cases of categorical or range data with object-dtype are not yet handled."
542965449,30516,DEPR: tz-aware Series/DatetimIndex.__array__ returns an object array of Timestamps,mroeschke,closed,2019-12-27T19:17:05Z,2020-01-03T05:23:39Z,"- [x] closes #24716
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
537184670,30241,BUG: to_datetime with unit with Int64,AlexKirko,closed,2019-12-12T20:01:05Z,2020-01-03T06:14:23Z,"- [X] closes #30050
- [X] tests added 1 / passed 1
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

Test output:
```
$ py.test pandas/tests/tools/test_datetime.py
============================= test session starts =============================
platform win32 -- Python 3.7.3, pytest-5.3.1, py-1.8.0, pluggy-0.13.0
rootdir: C:\git_contrib\pandas\pandas, inifile: setup.cfg
plugins: hypothesis-4.50.6, cov-2.8.1, forked-1.1.2, xdist-1.30.0
collected 1 item

pandas\tests\tools\test_datetime.py .                                    [100%]

============================== 1 passed in 0.03s ==============================

```

Notes: Hello. The fix introduces one additional type check if the Series passed to `to_datetime` stores anything except and IntegerArray. It does nothing else in this case. If we try to pass an IntegerArray, it pulls all the values that aren't NaN into an ndarray, converts that into datetime and adds NaT in the places where NaNs were. No precision is lost."
504852025,28876,ENH: show numbers on .info() with verbose flag,Roymprog,closed,2019-10-09T19:38:52Z,2020-01-03T08:54:26Z,"- [x] closes #17304
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

"
544182937,30580,BUG: Series __setitem__ gives wrong result with bool indexer,fujiaxiang,closed,2019-12-31T12:55:01Z,2020-01-03T11:41:09Z,"Series.__setitem__ gives wrong result with bool indexer and when length of new data matches the number of Trues and new data is neither a Series nor a numpy array

- [x] closes #30567
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
543081447,30526,BUG: pct_change wrong result when there are duplicated indices,fujiaxiang,closed,2019-12-28T06:53:53Z,2020-01-03T11:41:19Z,"- [x] closes  #30463
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
542260475,30462,BUG/ENH: groupby quantile arraylike fails with integer columns,fujiaxiang,closed,2019-12-25T05:41:33Z,2020-01-03T11:45:33Z,"When columns are integers, `df.groupby(label).quantile(<arraylike>)` fails.
This PR fixes this issue by telling `MultiIndex` to only look for positional levels during an reordering operation.
Two new internal methods in `MultiIndex` class, and one new parameter for `DataFrame.reorder_levels` and `MultiIndex.reorder_levels` are added during the debugging process.

- [ ] closes #30289
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
219801134,15914,read_excel does not work on excel file binary text or buffered binary text object,alexanderwhatley,closed,2017-04-06T06:43:08Z,2020-01-03T11:55:36Z,"Python 3.5, Pandas 0.19.2

I have the following excel file, and I am trying to read its binary content, and then have read_excel read the binary content in, except this is not working, and there may be a bug somewhere, as I have specified the engine for reading.

>>> import pandas as pd
>>> f = open(""Test_Prom_Data.xlsx"", ""rb"")
>>> df = pd.read_excel(f.read(), engine = ""xlrd"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/alexander/anaconda3/lib/python3.5/site-packages/pandas/io/excel.py"", line 191, in read_excel
    io = ExcelFile(io, engine=engine)
  File ""/home/alexander/anaconda3/lib/python3.5/site-packages/pandas/io/excel.py"", line 251, in __init__
    raise ValueError('Must explicitly set engine if not passing in'
ValueError: Must explicitly set engine if not passing in buffer or path for io.
[Test_Prom_Data.xlsx](https://github.com/pandas-dev/pandas/files/901685/Test_Prom_Data.xlsx)
"
543011969,30519,ENH: add support for reading binary Excel files (#15914),ftruzzi,closed,2019-12-27T22:46:28Z,2020-01-03T11:58:55Z,"Hi, this is a quick fix for #15914.

After reading the issue I'm not sure if this is meant to be supported though, please let me know!

I ran into this issue when trying to pass to `read_excel` a binary file downloaded with `requests` so I think it would make sense to support this natively without the user having to figure out the need to call `io.BytesIO` first. If so, do we need to update other docstrings?

Thank you!

- [x] closes #15914
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
534242477,30114,API: add ignore_index keyword to .sort_* & .drop_duplicates,jreback,closed,2019-12-06T20:26:55Z,2020-01-03T13:10:49Z,"I see lots of code that goes like this

```
In [17]: df = pd.DataFrame({'A': [1, 2, 3]})                                                                                                                                        

In [18]: df                                                                                                                                                                         
Out[18]: 
   A
0  1
1  2
2  3

In [19]: df.sort_values('A', ascending=False)                                                                                                                                       
Out[19]: 
   A
2  3
1  2
0  1

In [20]: df.sort_values('A', ascending=False).reset_index(drop=True)                                                                                                                
Out[20]: 
   A
0  3
1  2
2  1
```

might be nice from an API / consistency perspective to add a ``ignore_index=False|True`` keyword to ``.sort_values()``, and ``.drop_duplicates()`` that does the reset in-line; this would give consistency similar to a ``pd.concat([......], ignore_index=True)`` operation which users are very familiar
"
510426779,29143,DOC: Edit MultiIndex.set_levels() docstring to clarify that set_levels() interprets passed values as new components of the .levels attribute (#28294),hweecat,closed,2019-10-22T04:46:55Z,2020-01-03T13:13:18Z,"Added documentation on MultiIndex.set_levels to clarify that MultiIndex.set_levels() interprets passed values as new components of the .levels attribute. Continuation of #28797 due to issues with Travis CI for Python 3.8 and subsequent git diff issues resulting from resolution of Travis CI issues.

- [x] closes #28294 - or at least attempts to
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
544283036,30590,DOC: Add info on dtype strings,Dr-Irv,closed,2019-12-31T22:44:06Z,2020-01-03T13:18:34Z,"#### Problem description

I've been studying the new `string`, `boolean` and `Intxx` dtypes and think it would be worthwhile to add something about the strings that you are allowed to use with extension arrays in specifying the dtypes.  It could be an additional column in the dtypes table here:
https://dev.pandas.io/docs/getting_started/basics.html#dtypes

I think the following table is correct:

Data Type | Array | Possible Strings
------------|--------------|---------------------------------------|
`DatetimeTZDtype` | `DatetimeArray` | `'datetime64[ns, <tz>]'`  
`CategoricalDtype` | `Categorical` | `'category'`
`PeriodDtype` | `PeriodArray` | `'period[<freq>]' or 'Period[<freq>]'`
`SparseDtype` | `SparseArray` | `'Sparse'`, `'Sparse[int]'`, `'Sparse[int32, 0]'`, `'Sparse[int64, 0]'`, `'Sparse[float64, nan]'`, `'Sparse[float32, nan]'`
`IntervalDtype` | `IntervalArray` | `'interval'`, `'Interval'`, `'Interval[<np.numeric>]'`, `'Interval[datetime64[ns, <tz>]]'`, `'Interval[timedelta64[<freq>]]'`
`Int64Dtype` (and others) | `IntegerArray` | `'Int8'`, `'Int16'`, `'Int32'`, `'Int64'`, `'UInt8'`, `'UInt16'`, `'UInt32'`, `'UInt64'`
`StringDtype` | `StringArray` | `'string'`
`BooleanDtype` | `BooleanArray` | `'boolean'`

I also think we may want to make it clear that if you specify a string not in that table, it needs to be a string acceptable as a `numpy` dtype.

If people like @TomAugspurger and @jorisvandenbossche think this is useful, I'll add a column to that table in the docs (or maybe have to use a separate table because of the length of the last column above). 

Also, should we consider allowing `'Boolean'` and `'String'` and `'Category'`, i.e. type names with a leading capital letter?  We're inconsistent in terms of what case is allowed in different places for the  strings representing dtypes (see `period/Period` and `interval/Interval`)




"
544016899,30570,CLN: refactor tests in test_moments_ewm.py,charlesdong1991,closed,2019-12-30T21:28:36Z,2020-01-03T13:26:37Z,"- [ ] xref: #30486 #30542 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
29065993,6581,DEPR: Clean up list of deprecations from prior versions,jsexauer,closed,2014-03-09T23:47:17Z,2020-01-03T14:13:55Z,"Log for previous deprecations, once they are actually removed, move the issue to #13777 

We try to keep these for *three* major versions as actual deprecations. e.g. deprecate in 0.17, 0.18 & 0.19 get the warning, removed in 0.20.

## 0.24.0
- [x] #24596 `__array__` for tzaware Series/Index (0.24.0)
"
493504113,28436,BUG: Fix merging non-indexes causes Index dtype promotion in when keys are missing,dworvos,closed,2019-09-13T20:28:31Z,2020-01-03T14:44:03Z,"... from left or right side. (GH28220)
Also closes GH24897, GH24212, and GH17257

- [x] closes #28220, #24897, #24212, and #17257
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Probably requires a bit more cleanup and reduction of spaghetti... Not in love with the the solution (as it's quite similar to using both indexes) and requires extension test case changes, but hoping to get some comments and feedback on making it better.
"
544834911,30637,REF/BUG: DTA/TDA/PA comparison ops inconsistencies,jbrockmendel,closed,2020-01-03T03:21:47Z,2020-01-03T15:21:12Z,"It will take a couple of steps to get these three methods to all behave the same.  Following that we can move to share code between them.

This came up when trying to make the indexes dispatch `searchsorted` and it turns out that the DTI/TDI/PI searchsorted methods are not consistent with their DTA/TDA/PA counterparts.  The fix is to have shared validation/casting methods, which should in turn be consistent with the comparison ops."
545051741,30647,REF: restructure api import,TomAugspurger,closed,2020-01-03T15:59:59Z,2020-01-04T03:48:09Z,"Previously, `pandas.api` was imported indirectly via `pandas.io.json._table_schema`. This makes things a bit more direct.

I think we'll want a code check for importing from `pandas.api` within `pandas.core` but I wasn't able to easily write one (things like docstrings may want to have `import pandas.api` for example)."
547133673,30826,BUG: DTI.searchsorted accepting invalid types/dtypes,jbrockmendel,closed,2020-01-08T22:14:47Z,2020-01-09T05:10:31Z,"Analogous to #30763 which fixed the same issue for PeriodIndex.  After this will be a PR to fix TimedeltaIndex, then one to move the fixes up to the EA methods (and share searchsorted)."
547235736,30837,CI: Fix linting error,jschendel,closed,2020-01-09T04:09:48Z,2020-01-09T05:10:35Z,"Fixing a linting error that got merged into `master` and is breaking CI, e.g. see [this build](https://github.com/pandas-dev/pandas/runs/380663768).
"
547189647,30831,BUG: TimedeltaIndex.searchsorted accepting invalid types/dtypes,jbrockmendel,closed,2020-01-09T01:07:04Z,2020-01-09T05:11:15Z,TimedeltaIndex analogue to #30826.
547249742,30840,CLN: remove _to_M8,jbrockmendel,closed,2020-01-09T04:57:17Z,2020-01-09T05:47:59Z,a couple of recently-merged PRs removed the last usages of this function
546409797,30789,REF: Implement BaseMaskedArray class for integer/boolean ExtensionArrays,jorisvandenbossche,closed,2020-01-07T17:25:23Z,2020-01-09T08:26:16Z,"Todo item of https://github.com/pandas-dev/pandas/issues/29556, consolidating common code for IntegerArray and BooleanArray.

This is only a start, there is more to share."
491748865,28371,ENH: add and register Arrow extension types for Period and Interval,jorisvandenbossche,closed,2019-09-10T15:33:31Z,2020-01-09T08:34:28Z,"Related to https://github.com/pandas-dev/pandas/pull/28368, but now for Period and Interval for which we define extension types to store them with metadata in arrow.

Still needs some more tests and fixing corner cases. 
We probably also want to consolidate the pyarrow import checking somewhat.

I think a main question is how to deal with the import of pyarrow. The way I did it now makes that it is no longer a lazy import (as it was currently for the parquet functionality)."
547060352,30820,API: Limit assert_*_equal functions in public API,TomAugspurger,closed,2020-01-08T19:34:24Z,2020-01-09T09:03:10Z,As discussed on today's call. Just adding `assert_extension_array_equal` relative to 0.25.
546844734,30812,COMPAT: bump minimum version to pyarrow 0.13,jorisvandenbossche,closed,2020-01-08T12:59:36Z,2020-01-09T09:34:17Z,"xref discussion in https://github.com/pandas-dev/pandas/pull/28371

If in the future we want to always try to import pyarrow, having pyarrow 0.13 (instead of 0.12) as the minimum required version will make this easier."
505688973,28919,ENH: pd.MultiIndex.get_loc(np.nan),proost,closed,2019-10-11T07:35:50Z,2020-01-09T10:18:40Z,"MultiIndex.get_loc could not find nan with values including missing
values as a input.

Background: In `MultiIndex`, missing value is denoted by -1 in codes and doesn't exist in `self.levels`

So, could not find NA value in `self.levels`.

Before PR xref #28783

- [x] closes #19132
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
544969497,30642,"SparseArray not in arrays module - inconsistent with IntegerArray, StringArray, etc.",Dr-Irv,closed,2020-01-03T12:11:06Z,2020-01-09T12:29:05Z,"#### Code Sample, a copy-pastable example if possible

```python
In [1]: import pandas as pd

In [2]: pd.__version__
Out[2]: '0.26.0.dev0+1563.g1feefc692'

In [3]: pd.SparseArray
Out[3]: pandas.core.arrays.sparse.array.SparseArray

In [4]: pd.arrays.SparseArray
Out[4]: pandas.core.arrays.sparse.array.SparseArray

In [5]: pd.arrays.IntegerArray
Out[5]: pandas.core.arrays.integer.IntegerArray

In [6]: pd.IntegerArray
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-6-12476104dd13> in <module>
----> 1 pd.IntegerArray

C:\Code\pandas_dev\pandas\pandas\__init__.py in __getattr__(name)
    246             return type(name, (), {})
    247
--> 248         raise AttributeError(f""module 'pandas' has no attribute '{name}'"")
    249
    250

AttributeError: module 'pandas' has no attribute 'IntegerArray'

In [7]: pd.arrays.StringArray
Out[7]: pandas.core.arrays.string_.StringArray

In [8]: pd.StringArray
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-8-86553ff3c48c> in <module>
----> 1 pd.StringArray

C:\Code\pandas_dev\pandas\pandas\__init__.py in __getattr__(name)
    246             return type(name, (), {})
    247
--> 248         raise AttributeError(f""module 'pandas' has no attribute '{name}'"")
    249
    250

AttributeError: module 'pandas' has no attribute 'StringArray'
```
#### Problem description

I discovered this while working on #30628 .  The docs for `SparseArray` are at the top level (https://dev.pandas.io/docs/reference/api/pandas.SparseArray.html), while the docs for `IntegerArray` (https://dev.pandas.io/docs/reference/api/pandas.arrays.IntegerArray.html), `StringArray`(https://dev.pandas.io/docs/reference/api/pandas.arrays.StringArray.html), etc. are at the `pandas.arrays` level.

In the code `SparseArray` is at both levels, but `IntegerArray`, `StringArray`, etc. is only at the `arrays` level.

#### Expected Output

Unsure.  

It seems that this should be consistent.  Options are:
1. Put all `*Array` classes at top level, and document them that way. (i.e., use the pattern currently used for `SparseArray`).  That would involve code and documentation changes for all of the arrays except `SparseArray`.
2. Put all `*Array` classes at both levels (like `SparseArray`), but document them at the `pandas.arrays` level (like `IntegerArray` and `StringArray`).  That would involve code changes for all of the arrays, and doc changes for `SparseArray`.
3. Put all `*Array` classes only at the `pandas.arrays` level and document them all that way.  That would involve only changing code and docs for `SparseArray` and leaving the others alone.

It's not clear to me which is preferred.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 1feefc69241a8944bbdf3e7cd042336ef9554be7
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 13, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.26.0.dev0+1563.g1feefc692
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.2.post20191203
Cython           : 0.29.14
pytest           : 5.3.2
hypothesis       : 4.54.2
sphinx           : 2.3.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.6
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.10.2
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.2
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : 1.3.11
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.6
numba            : 0.46.0

</details>
"
512346676,29218,ENH: Show column name in assert_frame_equal,moi90,closed,2019-10-25T07:17:34Z,2020-01-09T12:59:57Z,"The output of the AssertionError raised by assert_frame_equal is changed:

- old: `AssertionError: Attributes of DataFrame.iloc[:, 1] are different`
- new: `AssertionError: Attributes of DataFrame.loc[:, 'b'] are different`

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
535154667,30163,"pandas.read_pickle can take buffer object as well, not only str",anatoly-khomenko,closed,2019-12-09T18:59:43Z,2020-01-09T13:16:26Z,"https://github.com/pandas-dev/pandas/blob/62a87bf4a2af02a8d3bc271ad26e5994292b8e6a/pandas/io/pickle.py#L95

read_pickle does not accept google storage URL (in form ""gs://bucket-name/path/file.pkl"") as input

While this code works fine:
`
with tf.io.gfile.GFile(""gs://bucket-name/path/file.pkl"", ""rb"") as infile:
      df = pd.read_pickle(infile, compression=None)
`

Might be reasonable to amend the documentation with this if the above is true for the recent version of pandas.

"
547162553,30828,STY: absolute imports in __init__ files,ShaharNaveh,closed,2020-01-08T23:30:57Z,2020-01-09T13:38:33Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
547348684,30844,Value Counts with categorical dtypes has unexpected behavior,datajanko,closed,2020-01-09T09:18:13Z,2020-01-09T13:59:17Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame({""A"": [1, 2, 3], ""B"":[2, 3, 4], 
                   ""C"": pd.Categorical([""A"", ""B"", ""C""]), 
                   ""D"": pd.Categorical([""B"", ""C"", ""D""])})
df.dtypes.value_counts()
```
gives
```
int64       2
category    1
category    1
dtype: int64
```
#### Problem description

I would have expected to have an aggregated count for categories here. This also seems to be related to groupby on dtypes

#### Expected Output
```
int64       2
category    2
dtype: int64
```
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-1054-aws
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 42.0.2
Cython           : 0.29.14
pytest           : 5.3.2
hypothesis       : 4.36.2
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.6
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : 0.9.3
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: 0.8.1
bs4              : 4.8.1
bottleneck       : 1.3.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.2
pandas_gbq       : 0.13.0
pyarrow          : 0.11.1
pytables         : None
s3fs             : 0.4.0
scipy            : 1.3.2
sqlalchemy       : 1.3.10
tables           : 3.6.1
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.6

</details>
"
547285594,30841,TestBaseCasting.test_to_numpy fails for ExtensionArrays that yield nested numpy arrays,xhochy,closed,2020-01-09T06:51:25Z,2020-01-09T15:51:26Z,"The test case fails when data is e.g. `np.array([np.array([np.nan, None], dtype=""object""), None], dtype=""object"")`. 

```python
def test_to_numpy(self, data):
        expected = np.asarray(data)

        result = data.to_numpy()
>       self.assert_equal(result, expected)

```

This happens in `fletcher` for the `list<item: string>` type."
547175103,30830,DOC: Fix indentation in docstring example,datapythonista,closed,2020-01-09T00:12:54Z,2020-01-09T15:52:31Z,"Not sure why we didn't see it before, but the example in the first code block of the docstring guidelines seem to be missing the indentation: https://pandas.io/docs/development/contributing_docstring.html#about-docstrings-and-standards

We should fix it, and indent the function properly: https://github.com/pandas-dev/pandas/blame/master/doc/source/development/contributing_docstring.rst#L25"
547046042,30818,date_range calculates ranges off by one bin when passing period,lcandeago,closed,2020-01-08T19:03:02Z,2020-01-09T16:06:07Z,"#### Code Sample, a copy-pastable example if possible

```python

import pandas as pd
start_t = pd.Timestamp('2008-11-24')
end_t = pd.Timestamp('2008-12-08')
print(end_t)
# 2008-12-08 00:00:00
diff = end_t-start_t
dates = pd.date_range(start_t, end_t, periods=bins, tz=None)
diff = dates[1] - dates[0]
print(diff)
# 0 days 03:23:38.181818
wrong = start_t + 100*diff
# 2008-12-08 03:23:38.181818100

correct = start_t + 99*diff
#2008-12-07 23:59:59.999999919 
```
#### Problem description
the date range is created with the frequency of 101 bins when passing period = 100.

#### Expected Output
multiplying the time length of a bin by the number of bins should return the end date.
instead it returns the end date +1
#### Output of ``pd.show_versions()``

<details>
NSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.2.13-arch1-1-ARCH
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.18.0
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
545948209,30757,BUG: TDI.insert with empty TDI raising IndexError,jbrockmendel,closed,2020-01-06T21:26:35Z,2020-01-09T16:26:05Z,"This started out as a cosmetic-only branch and ended up finding a broken corner case.  The relevant change is in timedeltas L416 where `if self.freq is not None:` is now `if self.size and self.freq is not None:`

Using _check_compatible_with causes us to raise TypeError instead of ValueError in a couple of the DatetimeIndex.insert cases."
547558615,30850,to_timedelta str int discrepancy,harishmk,closed,2020-01-09T15:44:12Z,2020-01-09T16:33:35Z,"#### Code Sample, a copy-pastable example if possible

```python
assert pd.to_timedelta(1,unit='s') == pd.to_timedelta('1',unit='s')
```
fails
```python
>>> pd.to_timedelta(20000,unit='s')
Timedelta('0 days 05:33:20')

>>> pd.to_timedelta('20000',unit='s')
Timedelta('0 days 00:00:00.000020')
```

#### Problem description
it looks like the `unit` parameter has no effect in `pd.to_timedelta('1',unit='s')`
#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-957.27.2.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.0.1
Cython           : 0.29.13
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.3.3
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.3
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.7
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

</details>
"
547204977,30836,DOC: whatsnew fixes,jschendel,closed,2020-01-09T02:05:16Z,2020-01-09T16:34:17Z,Some small fixes I noticed reading over the whatsnew.
547286371,30842,BUG: Handle nested arrays in array_equivalent_object,xhochy,closed,2020-01-09T06:53:25Z,2020-01-09T18:11:49Z,"- [x] closes #30841
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
547633379,30855,DOC: fixed about link,TomAugspurger,closed,2020-01-09T18:00:36Z,2020-01-09T18:38:09Z,"
"
547249119,30839,TYP: __array__,jbrockmendel,closed,2020-01-09T04:55:16Z,2020-01-09T19:00:15Z,
545770565,30740,DataFrame.unstack() with list of levels ignores fill_value,simonjayhawkins,closed,2020-01-06T14:59:27Z,2020-01-09T19:19:07Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import pandas as pd
>>> df = (
...     pd.DataFrame(
...         {
...             ""name"": [""Alice"", ""Bob""],
...             ""score"": [9.5, 8],
...             ""employed"": [False, True],
...             ""kids"": [0, 0],
...             ""gender"": [""female"", ""male""],
...         }
...     )
...     .set_index([""name"", ""employed"", ""kids"", ""gender""])
...     .unstack([""gender""], fill_value=0)
... )
>>> df.unstack([""employed"", ""kids""], fill_value=0)
          score
gender   female        male
employed  False True  False True
kids          0     0     0     0
name
Alice       9.5   NaN   0.0   NaN
Bob         NaN   0.0   NaN   8.0
```
#### Problem description

when unstacking with a list of levels on a DataFrame that already has a columns MultiIndex, fill_value is ignored.

#### Expected Output

```python
>>> df.unstack(""employed"", fill_value=0).unstack(""kids"", fill_value=0)
          score
gender   female        male
employed  False True  False True
kids          0     0     0     0
name
Alice       9.5   0.0   0.0   0.0
Bob         0.0   0.0   0.0   8.0
>>>
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 4206fd42cc5cd20204c0c5f192f7e59f204ad48d
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 58 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : None.None

pandas           : 0.26.0.dev0+1622.g4206fd42c
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.6.0.post20191030
Cython           : 0.29.13
pytest           : 5.2.2
hypothesis       : 4.36.2
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.2
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.2.2
s3fs             : 0.3.4
scipy            : 1.3.1
sqlalchemy       : 1.3.10
tables           : 3.5.1
tabulate         : None
xarray           : 0.13.0
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.2
numba            : 0.46.0

</details>
"
547245773,30838,BUG: added missing fill_na parameter to DataFrame.unstack() with list of levels,tanmay1618,closed,2020-01-09T04:44:29Z,2020-01-09T19:19:10Z,"- [x] closes #30740
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
545307832,30685,"BooleanArray, StringArray value_counts na_value",jbrockmendel,closed,2020-01-04T17:09:19Z,2020-01-09T19:19:35Z,"```
arr = pd.array([True, False, np.nan])

>>> arr
<BooleanArray>
[True, False, NA]
Length: 3, dtype: boolean

>>> arr.value_counts(dropna=False)
True     1
False    1
False    1
dtype: int64
```

Instead of two False entries, one of them should be pd.NA right?  cc @TomAugspurger 

```
arr = pd.array(['foo', 'bar', None])

>>> arr.value_counts(dropna=False)
foo    1
bar    1
NaN    1
dtype: int64
```

The NaN should be pd.NA right?

Side-note: the defaults for dropna are not consistent across our EAs"
547121887,30824,BUG: BooleanArray.value_counts dropna,TomAugspurger,closed,2020-01-08T21:49:10Z,2020-01-09T19:19:40Z,Closes https://github.com/pandas-dev/pandas/issues/30685
547070580,30821,Update NA repr,TomAugspurger,closed,2020-01-08T19:56:20Z,2020-01-09T19:26:10Z,"Closes https://github.com/pandas-dev/pandas/issues/30415

```python

In [2]: df = pd.DataFrame({""A"": pd.array([1, 2, None])})

In [3]: df
Out[3]:
      A
0     1
1     2
2  <NA>

In [4]: df.A
Out[4]:
0       1
1       2
2    <NA>
Name: A, dtype: Int64

In [5]: df.A.array
Out[5]:
<IntegerArray>
[1, 2, <NA>]
Length: 3, dtype: Int64

```

I think adding color with ANSI codes / custom HTML formatting is still worth doing as in #30778, but this is an improvement for now."
547676039,30861,BUG: ExtensionArray._formatter does not need to handle nulls,jorisvandenbossche,closed,2020-01-09T19:31:40Z,2020-01-09T20:19:43Z,"See https://github.com/pandas-dev/pandas/pull/30821/files#r364917242

Currently, I don't think this is really specified whether the `_formatter` of ExtensionArray should be able to handle NAs/nulls or not, but in practice, it didn't need to do that. And eg the geopandas one is also implemented right now that it would fail if passed a NA (https://github.com/geopandas/geopandas/blob/add5fe9139883fa54d14fa28426478619948349c/geopandas/array.py#L1026-L1047)

This is certainly something that can be discussed how to specify this, but for now I think it is best to preserve the existing behaviour."
546487118,30795,DOC: whatsnew updates,TomAugspurger,closed,2020-01-07T20:19:46Z,2020-01-09T20:45:00Z,"Primarily reordering roughly in order of importance.

1. Some rewording for clarity
2. Fixed some links
3. Simplified the SemVer discussion"
510193016,29136,DataFrame.rename only validates column arguments,WillAyd,closed,2019-10-21T18:35:29Z,2020-01-09T21:04:27Z,"Found this while trying to clean up axis handling in core.generic

This fails as you would hope
```python
>>> df = pd.DataFrame([[1]])
>>> df.rename({0: 1}, columns={0: 2}, axis=1)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/williamayd/clones/pandas/pandas/util/_decorators.py"", line 235, in wrapper
    return func(*args, **kwargs)
  File ""/Users/williamayd/clones/pandas/pandas/core/frame.py"", line 4143, in rename
    axes = validate_axis_style_args(self, args, kwargs, ""mapper"", ""rename"")
  File ""/Users/williamayd/clones/pandas/pandas/util/_validators.py"", line 287, in validate_axis_style_args
    raise TypeError(msg)
TypeError: Cannot specify both 'axis' and any of 'index' or 'columns'.
```

This doesn't
```python
>>> df.rename({0: 1}, index={0: 2})
   0
1  1
```

And perhaps even more surprising is that you will get a different result depending on whether the first argument is passed by position or keyword
```python
>>> df.rename(mapper={0: 1}, index={0: 2})
   0
2  1
```"
547712116,30865,DOC: add note about many removals in pandas 1.0,jorisvandenbossche,closed,2020-01-09T20:45:50Z,2020-01-09T21:14:10Z,"cc @TomAugspurger 
Adding a note about the recommended way to upgrade to pandas 1.0, if that sounds OK"
462314427,27125,BUG: Index constructor should not allow an ndarray with ndim > 2,jschendel,closed,2019-06-29T15:49:06Z,2020-01-09T21:17:06Z,"#### Code Sample, a copy-pastable example if possible

On master:
```python
In [1]: import numpy as np; import pandas as pd; pd.__version__
Out[1]: '0.25.0.dev0+833.gad18ea35b'

In [2]: pd.Index(np.arange(8).reshape(2, 2, 2))
Out[2]: Int64Index([[[0, 1], [2, 3]], [[4, 5], [6, 7]]], dtype='int64')
```

If the first dimension is greater than 2 it appears to flatten but does not actually do so:
```python
In [3]: pd.Index(np.arange(12).reshape(3, 2, 2))
Out[3]: Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype='int64')

In [4]: _.values
Out[4]: 
array([[[ 0,  1],
        [ 2,  3]],

       [[ 4,  5],
        [ 6,  7]],

       [[ 8,  9],
        [10, 11]]])

```
#### Problem description
The `Index` constructor accepts ndarrays with ndim > 2 and will even convert them to specialized subclasses, e.g. `Int64Index`.

#### Expected Output
I'd expect the operations above to raise, or at the very least should result in an `object` dtype `Index`, though I'd prefer to raise.

xref #17246

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : ad18ea35ba461a92b1ea2204f4edc55bb42e9d71
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.14-041914-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.0.dev0+833.gad18ea35b
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 40.8.0
Cython           : 0.29.10
pytest           : 4.6.2
hypothesis       : 4.23.6
sphinx           : 1.8.5
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.3.3
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : 0.3.0
gcsfs            : None
lxml.etree       : 4.3.3
matplotlib       : 3.1.0
numexpr          : 2.6.9
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : 0.11.1
pytables         : None
s3fs             : 0.2.1
scipy            : 1.2.1
sqlalchemy       : 1.3.4
tables           : 3.5.2
xarray           : 0.12.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8

</details>
"
164671642,13601,Assigning array of >1 dim to index produces inconsistent index,toobaz,closed,2016-07-09T15:29:56Z,2020-01-09T21:17:06Z,"#### Code Sample, a copy-pastable example if possible

```
In [3]: s = pd.Series(0, range(4))

In [4]: s.index = np.array([[2,3]]*4)

In [5]: s
Out[5]: ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/home/nobackup/repo/ipython/IPython/core/formatters.py in __call__(self, obj)
    683                 type_pprinters=self.type_printers,
    684                 deferred_pprinters=self.deferred_printers)
--> 685             printer.pretty(obj)
    686             printer.flush()
    687             return stream.getvalue()

/home/nobackup/repo/ipython/IPython/lib/pretty.py in pretty(self, obj)
    381                             if callable(meth):
    382                                 return meth(obj, self, cycle)
--> 383             return _default_pprint(obj, self, cycle)
    384         finally:
    385             self.end_group()

/home/nobackup/repo/ipython/IPython/lib/pretty.py in _default_pprint(obj, p, cycle)
    501     if _safe_getattr(klass, '__repr__', None) not in _baseclass_reprs:
    502         # A user-provided repr. Find newlines and replace them with p.break_()
--> 503         _repr_pprint(obj, p, cycle)
    504         return
    505     p.begin_group(1, '<')

/home/nobackup/repo/ipython/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)
    692     """"""A pprint that just redirects to the normal repr function.""""""
    693     # Find newlines and replace them with p.break_()
--> 694     output = repr(obj)
    695     for idx,output_line in enumerate(output.splitlines()):
    696         if idx:

/home/nobackup/repo/pandas/pandas/core/base.py in __repr__(self)
     65         Yields Bytestring in Py2, Unicode String in py3.
     66         """"""
---> 67         return str(self)
     68 
     69 

/home/nobackup/repo/pandas/pandas/core/base.py in __str__(self)
     44 
     45         if compat.PY3:
---> 46             return self.__unicode__()
     47         return self.__bytes__()
     48 

/home/nobackup/repo/pandas/pandas/core/series.py in __unicode__(self)
    979 
    980         self.to_string(buf=buf, name=self.name, dtype=self.dtype,
--> 981                        max_rows=max_rows)
    982         result = buf.getvalue()
    983 

/home/nobackup/repo/pandas/pandas/core/series.py in to_string(self, buf, na_rep, float_format, header, index, length, dtype, name, max_rows)
   1020         the_repr = self._get_repr(float_format=float_format, na_rep=na_rep,
   1021                                   header=header, index=index, length=length,
-> 1022                                   dtype=dtype, name=name, max_rows=max_rows)
   1023 
   1024         # catch contract violations

/home/nobackup/repo/pandas/pandas/core/series.py in _get_repr(self, name, header, index, length, dtype, na_rep, float_format, max_rows)
   1048                                         float_format=float_format,
   1049                                         max_rows=max_rows)
-> 1050         result = formatter.to_string()
   1051 
   1052         # TODO: following check prob. not neces.

/home/nobackup/repo/pandas/pandas/formats/format.py in to_string(self)
    227             return 'Series([], ' + footer + ')'
    228 
--> 229         fmt_index, have_header = self._get_formatted_index()
    230         fmt_values = self._get_formatted_values()
    231 

/home/nobackup/repo/pandas/pandas/formats/format.py in _get_formatted_index(self)
    213         else:
    214             have_header = index.name is not None
--> 215             fmt_index = index.format(name=True)
    216         return fmt_index, have_header
    217 

/home/nobackup/repo/pandas/pandas/indexes/base.py in format(self, name, formatter, **kwargs)
   1525             return header + list(self.map(formatter))
   1526 
-> 1527         return self._format_with_header(header, **kwargs)
   1528 
   1529     def _format_with_header(self, header, na_rep='NaN', **kwargs):

/home/nobackup/repo/pandas/pandas/indexes/base.py in _format_with_header(self, header, na_rep, **kwargs)
   1549 
   1550         else:
-> 1551             result = _trim_front(format_array(values, None, justify='left'))
   1552         return header + result
   1553 

/home/nobackup/repo/pandas/pandas/formats/format.py in format_array(values, formatter, float_format, na_rep, digits, space, justify, decimal)
   2021                         space=space, justify=justify, decimal=decimal)
   2022 
-> 2023     return fmt_obj.get_result()
   2024 
   2025 

/home/nobackup/repo/pandas/pandas/formats/format.py in get_result(self)
   2040 
   2041     def get_result(self):
-> 2042         fmt_values = self._format_strings()
   2043         return _make_fixed_width(fmt_values, self.justify)
   2044 

/home/nobackup/repo/pandas/pandas/formats/format.py in _format_strings(self)
   2226     def _format_strings(self):
   2227         formatter = self.formatter or (lambda x: '% d' % x)
-> 2228         fmt_values = [formatter(x) for x in self.values]
   2229         return fmt_values
   2230 

/home/nobackup/repo/pandas/pandas/formats/format.py in <listcomp>(.0)
   2226     def _format_strings(self):
   2227         formatter = self.formatter or (lambda x: '% d' % x)
-> 2228         fmt_values = [formatter(x) for x in self.values]
   2229         return fmt_values
   2230 

/home/nobackup/repo/pandas/pandas/formats/format.py in <lambda>(x)
   2225 class IntArrayFormatter(GenericArrayFormatter):
   2226     def _format_strings(self):
-> 2227         formatter = self.formatter or (lambda x: '% d' % x)
   2228         fmt_values = [formatter(x) for x in self.values]
   2229         return fmt_values

TypeError: %d format: a number is required, not numpy.ndarray

```
#### Expected Output

It would be cool if this became a `MultiIndex` automatically, but otherwise it should just raise an error (or maybe it's just the string representation which is broken?)
#### output of `pd.show_versions()`

```
In [6]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit: a63bd12529ff309d957d714825b1753d0e02b7fa
python: 3.5.1.final.0
python-bits: 64
OS: Linux
OS-release: 4.5.0-2-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: it_IT.utf8
LOCALE: it_IT.UTF-8

pandas: 0.18.1+174.ga63bd12
nose: 1.3.7
pip: 1.5.6
setuptools: 18.4
Cython: 0.23.4
numpy: 1.10.4
scipy: 0.16.0
statsmodels: 0.8.0.dev0+111ddc0
xarray: None
IPython: 5.0.0.dev
sphinx: 1.3.1
patsy: 0.3.0-dev
dateutil: 2.2
pytz: 2012c
blosc: None
bottleneck: 1.1.0dev
tables: 3.2.2
numexpr: 2.5
matplotlib: 1.5.1
openpyxl: None
xlrd: 0.9.4
xlwt: 1.1.2
xlsxwriter: 0.7.3
lxml: None
bs4: 4.4.0
html5lib: 0.999
httplib2: 0.9.1
apiclient: 1.5.0
sqlalchemy: 1.0.11
pymysql: None
psycopg2: None
jinja2: 2.8
boto: 2.38.0
pandas_datareader: 0.2.1

```
"
547799657,30875,CLN: remove unnecessary overriding in subclasses,jbrockmendel,closed,2020-01-10T00:35:00Z,2020-01-10T02:18:23Z,
538875131,30301,ENH: meth 'DataFrame.to_pickle' and func 'read_pickle' to accept URL GH#30163,suzutomato,closed,2019-12-17T06:56:11Z,2020-01-10T04:10:38Z,"- [ ] closes #30163
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
547921533,30878,DEPR: fix missing stacklevel in pandas.core.index deprecation,jorisvandenbossche,closed,2020-01-10T07:50:41Z,2020-01-10T14:04:27Z,xref https://github.com/pandas-dev/pandas/issues/30872
116948061,11604,pandas not found when it's supposed to be installed,datomnurdin,closed,2015-11-14T19:39:37Z,2020-01-10T14:08:50Z,"I got this error message. Already installed pandas.

```python
>>> import pandas as pd
Traceback (most recent call last):
  File ""analyze_tweets.py"", line 9, in <module>
    import pandas as pd 
ImportError: No module named pandas
```

**SOLUTION** (by @datapythonista)

This error happens when Python can't find pandas in the list of available libraries. Python has internally a list of directories where it'll look for packages. These directories can be obtained with `import sys; sys.path`.

In one machine you can (and should) have several Python installations. Meaning that you can have pandas installed in one, and be using another Python installation (this is likely the case here).

In Linux/Mac you can run `which python` and will tell you which is the Python you're using. If it's something like `/usr/bin/python`, you're using the Python from the system, which is not a great practice, and rarely used in the Python community.

If you used Python before you may have used virtual environments and pip. While this is fine for many Python projects (e.g. Django), when using data projects (pandas, numpy, tensorflow...) this is discouraged. It's easy that you have installation errors, and also the libraries can run much slower when using pip.

The widely used solution to this problem is to use conda. You can find simple installation instructions for pandas in this document: https://dev.pandas.io/getting_started.html

For more advanced users, installing miniconda, and then manually install pandas (and any other required package) with conda can be preferred (but avoid this if you're starting, since you'll probably have errors of missing dependencies when using many of the pandas features)."
490770862,28341,Remove compiled Cython files from sdist,TomAugspurger,closed,2019-09-08T15:55:59Z,2020-01-10T14:09:53Z,"NumPy is considering this in https://github.com/numpy/numpy/pull/14453. I suspect other projects will follow suite.

The tldr is that as long as we ship the cythonized code, our older sdists have no hope of working with newer Pythons. The have to be compiled with a modern enough Cython.

I think this makes even more sense for us. We already require that NumPy is present as a build dependency. Requiring Cython on top of that is not a big deal.

Do we want to do this for 0.25.2?"
461052761,27055,DOC: Validation of docsting validates Python type doc,datapythonista,open,2019-06-26T15:52:26Z,2020-01-10T14:11:00Z,"Our script `scripts/validate_docstrings.py` validates that docstrings in our code follow certain standards (the right parameters are documented, the docstring contain examples, capitalization and punctuation...).

There is currently one case when the validation is incorrect. See this (hypothetical) example:
```python
class Series:
    ndim = 1
```

If we execute `./scripts/validate_docstrings.py pandas.Series.ndim`, we'd expect the script to detect that `ndim` doesn't have a docstring. But instead, the script is validating the docstring of the type `int` (the doc of the scalar `1`).

This is happening in practice for `offset`, and is preventing us to validate some error types to the CI. See: https://github.com/pandas-dev/pandas/issues/26982#issuecomment-505499869

We should detect this case in the script, and not validate the docstrings of attributes."
546557710,30799,Tests for Deprecate SparseArray for python 3.6 and 3.7 and fixes to other deprecation tests,Dr-Irv,closed,2020-01-07T23:14:28Z,2020-01-10T17:02:42Z,"- [x] closes #30642
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
   - N/A

Turns out that the existing tests were not testing deprecation correctly for both python 3.7 and python 3.6, so had to change some of the code in `test_api.py` to make that work right.

For `pd.SparseArray` in python 3.6, we can only issue a warning when the constructor `pd.SparseArray` is used.  But this is consistent with `pd.datetime` and `pd.np` with python 3.6, which will issue warnings when things like `pd.datetime.now()` are called.  However, for python 3.6, I could not figure out a way to issue a warning on `pd.datetime(2015, 10, 11, 0, 0)`, so we may just have to live with that.  


"
545995791,30765,"in tests, change pd.arrays.SparseArray to SparseArray",Dr-Irv,closed,2020-01-06T23:34:09Z,2020-01-10T17:02:42Z,"- [x] closes https://github.com/pandas-dev/pandas/pull/30656#discussion_r363060184
- [x] tests added / passed
    - modified most tests that use `pd.arrays.SparseArray` to just import `SparseArray`
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
    - N/A

For @jreback to review based on his comment previous PR #30656 

In a few files (see below), left it as is because usage was pretty local (and allows `pd.arrays.SparseArray` reference to be tested in code)
```bash
$ grep -c -r arrays.SparseArray . | grep -v "":0""
./dtypes/test_generic.py:1
./frame/methods/test_quantile.py:2
./series/test_missing.py:2
```


"
548096173,30883,Backport PR #30878 on branch 1.0.x (DEPR: fix missing stacklevel in pandas.core.index deprecation),meeseeksmachine,closed,2020-01-10T14:01:42Z,2020-01-10T19:23:34Z,Backport PR #30878: DEPR: fix missing stacklevel in pandas.core.index deprecation
548172036,30890,WEB: Removing Discourse links,datapythonista,closed,2020-01-10T16:26:59Z,2020-01-10T19:26:27Z,"Just realized that the discourse links are still in the web. They should be removed, since we're not planning to use discourse for now."
546301244,30781,STY: Spaces in wrong place,ShaharNaveh,closed,2020-01-07T14:03:33Z,2020-01-10T20:42:06Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
547193327,30833,DOC: Fix indentation in docstring example,ShaharNaveh,closed,2020-01-09T01:19:40Z,2020-01-10T20:42:33Z,"- [x] closes #30830
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
547191480,30832,"CLN: Removed ""# noqa: F401"" comments",ShaharNaveh,closed,2020-01-09T01:13:40Z,2020-01-10T20:43:01Z,"Implemented ```__all__``` for each changed file

- [x] ref https://github.com/pandas-dev/pandas/pull/30828#discussion_r364500688
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
548191724,30893,WEB: Remove from roadmap moving the docstring script,datapythonista,closed,2020-01-10T17:06:52Z,2020-01-10T23:21:21Z,"The docstring validation script was moved to numpydoc, so it's probably worth removing this task from the roadmap.

The integration of the new numpydoc script in pandas is ready in #30746"
547706241,30864,DOC: Encourage use of pre-commit in the docs,gfyoung,closed,2020-01-09T20:33:30Z,2020-01-10T23:52:45Z,"Previously, we stated it as merely optional.

xref https://github.com/pandas-dev/pandas/pull/30773, https://github.com/pandas-dev/pandas/pull/30814
"
548215903,30897,TYP: offsets,jbrockmendel,closed,2020-01-10T18:01:56Z,2020-01-11T00:19:03Z,
548406012,30911,Added documentation for ImportError in docs/source/getting_started/in…,gonemad97,closed,2020-01-11T09:24:18Z,2020-01-11T09:25:11Z,"…stall.rst

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
548409251,30913,Backport PR #30906 on branch 1.0.x (BUG: pickle files left behind by tm.round_trip_pickle),meeseeksmachine,closed,2020-01-11T09:54:20Z,2020-01-11T10:35:36Z,Backport PR #30906: BUG: pickle files left behind by tm.round_trip_pickle
274115022,18301,Deprecate automatically registering matplotlib units (partial revert of 0.21.0),jorisvandenbossche,closed,2017-11-15T10:59:27Z,2020-01-11T12:37:31Z,"Given the feedback we have got (https://github.com/pandas-dev/pandas/issues/18153, https://github.com/pandas-dev/pandas/issues/18192, https://github.com/pandas-dev/pandas/issues/18212,  https://github.com/pandas-dev/pandas/issues/18283, https://github.com/matplotlib/matplotlib/issues/9577, https://github.com/matplotlib/matplotlib/issues/9610, https://github.com/matplotlib/matplotlib/issues/9771, https://github.com/pydata/xarray/issues/166), it seems we underestimated the impact and we should consider (partly) reverting this change to properly deprecate it instead. 

This should also give matplotlib the time to implement basic datetime64 support (https://github.com/matplotlib/matplotlib/issues/9610, https://github.com/matplotlib/matplotlib/pull/9779)

Depending on how we can do this, this might have the consequence that have to undo temporarily the lazy import of matplotlib (affecting the import time). See also discussion in https://github.com/pandas-dev/pandas/issues/18283

Opening this issue to keep track of it, should be decided/done for 0.21.1"
548353577,30906,BUG: pickle files left behind by tm.round_trip_pickle,jbrockmendel,closed,2020-01-11T00:39:12Z,2020-01-11T15:30:12Z,
548433113,30919,replace syntax with f-string,HH-MWB,closed,2020-01-11T13:57:37Z,2020-01-11T23:11:44Z,"- [x] contributes to #29547
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
548416918,30917,DOC: Fixture docs in pandas/conftest.py,gfyoung,closed,2020-01-11T11:13:05Z,2020-01-11T23:25:48Z,xref: https://github.com/pandas-dev/pandas/issues/19159
548487814,30932,CLN: remove unnecessary _date_check_type,jbrockmendel,closed,2020-01-11T22:23:43Z,2020-01-12T00:06:57Z,"The check doesn't do anything, and we still a raise `KeyError` anyways"
364115827,22839,BUG: DatetimeIndex.where with timezone aware and timezone naive data not returning mixed result ,mroeschke,closed,2018-09-26T16:34:06Z,2020-01-12T04:43:43Z,"```
In [7]: pd.__version__
Out[7]: '0.24.0.dev0+639.gcff2bca3b.dirty'

In [8]:         fill_val = pd.Timestamp('2012-01-01', tz='US/Eastern')
   ...:
   ...:

In [9]:         values = pd.Index(pd.date_range(fill_val, periods=4))
   ...:

In [10]: values
Out[10]:
DatetimeIndex(['2012-01-01 00:00:00-05:00', '2012-01-02 00:00:00-05:00',
               '2012-01-03 00:00:00-05:00', '2012-01-04 00:00:00-05:00'],
              dtype='datetime64[ns, US/Eastern]', freq='D')

# The replace values coerced to tz-naive
In [11]: values.tz_localize(None).where(pd.Index([True, False, True, False]), values)
Out[11]:
DatetimeIndex(['2012-01-01 00:00:00', '2012-01-02 05:00:00',
               '2012-01-03 00:00:00', '2012-01-04 05:00:00'],
              dtype='datetime64[ns]', freq='D')

# The replace values coerced to tz-aware
In [12]: values.where(pd.Index([True, False, True, False]), values.tz_localize(None))
Out[12]:
DatetimeIndex(['2012-01-01 00:00:00-05:00', '2012-01-01 19:00:00-05:00',
               '2012-01-03 00:00:00-05:00', '2012-01-03 19:00:00-05:00'],
              dtype='datetime64[ns, US/Eastern]', freq='D')
```

The result should be coerced to `object` in order to preserve the tz-aware and tz-naive data.

```
In [13]: values_naive = values.tz_localize(None)

# Should be result for Out[12]
In [14]: pd.Index([values[0], values_naive[1], values[2], values_naive[3]], dtype=object)
Out[14]:
Index([2012-01-01 00:00:00-05:00,       2012-01-02 00:00:00,
       2012-01-03 00:00:00-05:00,       2012-01-04 00:00:00],
      dtype='object')
```"
548302578,30901,TYP: typing annotations,ShaharNaveh,closed,2020-01-10T21:37:12Z,2020-01-12T09:12:26Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
548414885,30916,CLN: F-strings,dinasv,closed,2020-01-11T10:52:45Z,2020-01-12T09:37:58Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
548541834,30939,DOC: Fix SS03 docstring error,galuhsahid,closed,2020-01-12T09:02:20Z,2020-01-12T10:07:30Z,"Fixes SS03 errors. 

```
None:None:SS03:pandas.DatetimeIndex.freqstr:Summary does not end with a period
None:None:SS03:pandas.PeriodIndex.freqstr:Summary does not end with a period
pandas/pandas/core/window/indexers.py:34:SS03:pandas.api.indexers.BaseIndexer:Summary does not end with a period
```

Related to #27977, #30733 cc @datapythonista

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes black pandas
- [x] passes git diff upstream/master -u -- ""*.py"" | flake8 --diff
- [ ] whatsnew entry

When I ran the script, there are others left, but they might be false positives since I can't find them in the codebase."
548573675,30941,replace old formatting syntax in shared_docs and Appender,HH-MWB,closed,2020-01-12T13:55:10Z,2020-01-12T14:29:39Z,"- [X] sample of #30933
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
545977333,30761,TYP: type up parts of series.py,topper-123,closed,2020-01-06T22:38:46Z,2020-01-12T14:55:37Z,More typing.
548550083,30940,STY: wrong placed space in strings,ShaharNaveh,closed,2020-01-12T10:22:39Z,2020-01-12T17:15:57Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
474820653,27665,BUG: Allow plotting boolean values,charlesdong1991,closed,2019-07-30T21:05:20Z,2020-01-12T21:49:40Z,"- [ ] closes #23719
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
548532953,30938,"Getting ValueError: no results with apply(pd.to_numeric(df2.BATTING_Tests_Inns, errors='coerce'))",subhobrata,closed,2020-01-12T07:21:47Z,2020-01-13T06:09:37Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
# convert just selected columns to numeric
df2[[""BATTING_Tests_Mat"", ""BATTING_Tests_Inns"",""BATTING_Tests_NO"",""BATTING_Tests_Runs"",""BATTING_Tests_HS"",""BATTING_Tests_Ave"",""BATTING_Tests_SR""]]= df2[[""BATTING_Tests_Mat"", ""BATTING_Tests_Inns"",""BATTING_Tests_NO"",""BATTING_Tests_Runs"",""BATTING_Tests_HS"",""BATTING_Tests_Ave"",""BATTING_Tests_SR""]].apply(pd.to_numeric(df2.BATTING_Tests_Inns, errors='coerce'))
```
#### Getting ValueError: no results with apply(pd.to_numeric(df2.BATTING_Tests_Inns, errors='coerce'))

[Getting ValueError: no results with apply(pd.to_numeric(df2.BATTING_Tests_Inns, errors='coerce'))]

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output
It should convert to numeric without any error

#### Output of ``pd.show_versions()``

'0.25.3' in Google colab

[paste the output of ``pd.show_versions()`` here below this line]

</details>
The dataset is taken from below website.
https://data.world/raghav333/cricket-players-espn
Full code is in my Github below
https://github.com/subhobrata/PandasIssue/blob/master/POC_Dataset.ipynb
"
548778065,30955,Backport PR #30926 on branch 1.0.x (DOC: Fix whatsnew contributors section),meeseeksmachine,closed,2020-01-13T08:31:45Z,2020-01-13T09:48:16Z,Backport PR #30926: DOC: Fix whatsnew contributors section
548814126,30957,Backport PR #30952 on branch 1.0.x (CI: numpydev changed double to single quote),meeseeksmachine,closed,2020-01-13T09:48:48Z,2020-01-13T10:56:00Z,Backport PR #30952: CI: numpydev changed double to single quote
406013091,25097,DOC: frame.py doctest fixing,stijnvanhoey,closed,2019-02-02T19:17:41Z,2020-01-13T11:22:21Z,"- [ ] fixes part of #22459 for the `DataFrame` methods
- [ ] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The PR provides some onging fixes for doctetsts #22459 focusing on the methods of `frame.py`. At the same time, the involved docstrings were corrected using the validation script. I removed the methods I effectively adapted from the `ci/code_checks.sh`"
304067836,20117,DOC: update the pandas.Index.duplicated and pandas.Series.duplicated docstring,stijnvanhoey,closed,2018-03-10T12:42:20Z,2020-01-13T11:22:21Z,"Checklist for the pandas documentation sprint (ignore this if you are doing
an unrelated PR):

- [x] PR title is ""DOC: update the <your-function-or-method> docstring""
- [ ] The validation script passes: `scripts/validate_docstrings.py <your-function-or-method>`
- [x] The PEP8 style check passes: `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] The html version looks good: `python doc/make.py --single <your-function-or-method>`
- [x] It has been proofread on language by another sprint participant

Please include the output of the validation script below between the ""```"" ticks:

Method `pandas.Index.duplicated`:
```
################################################################################
##################### Docstring (pandas.Index.duplicated)  #####################
################################################################################

Indicate duplicate index values.

Duplicated values are indicated as ``True`` values in the resulting
array. Either all duplicates, all except the first or all except the
last occurrence of duplicates can be indicated.

Parameters
----------
keep : {'first', 'last', False}, default 'first'
    - 'first' : Mark duplicates as ``True`` except for the first
      occurrence.
    - 'last' : Mark duplicates as ``True`` except for the last
      occurrence.
    - ``False`` : Mark all duplicates as ``True``.

Examples
--------
By default, for each set of duplicated values, the first occurrence is
set on False and all others on True:

>>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])
>>> idx.duplicated()
array([False, False,  True, False,  True], dtype=bool)

which is equivalent to

>>> idx.duplicated(keep='first')
array([False, False,  True, False,  True], dtype=bool)

By using 'last', the last occurrence of each set of duplicated values
is set on False and all others on True:

>>> idx.duplicated(keep='last')
array([ True, False,  True, False, False], dtype=bool)

By setting keep on ``False``, all duplicates are True:

>>> idx.duplicated(keep=False)
array([ True, False,  True, False,  True], dtype=bool)

Returns
-------
numpy.ndarray

See Also
--------
pandas.Series.duplicated : equivalent method on pandas.Series

################################################################################
################################## Validation ##################################
################################################################################

Errors found:
	Errors in parameters section
		Parameter ""keep"" description should start with capital letter
```

Method `pandas.Series.duplicated`:
```
################################################################################
##################### Docstring (pandas.Series.duplicated) #####################
################################################################################

Indicate duplicate Series values.

Duplicated values are indicated as ``True`` values in the resulting
Series. Either all duplicates, all except the first or all except the
last occurrence of duplicates can be indicated.

Parameters
----------
keep : {'first', 'last', False}, default 'first'
    - 'first' : Mark duplicates as ``True`` except for the first
      occurrence.
    - 'last' : Mark duplicates as ``True`` except for the last
      occurrence.
    - ``False`` : Mark all duplicates as ``True``.

Examples
--------
By default, for each set of duplicated values, the first occurrence is
set on False and all others on True:

>>> animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'])
>>> animals.duplicated()
0    False
1    False
2     True
3    False
4     True
dtype: bool

which is equivalent to

>>> animals.duplicated(keep='first')
0    False
1    False
2     True
3    False
4     True
dtype: bool

By using 'last', the last occurrence of each set of duplicated values
is set on False and all others on True:

>>> animals.duplicated(keep='last')
0     True
1    False
2     True
3    False
4    False
dtype: bool

By setting keep on ``False``, all duplicates are True:

>>> animals.duplicated(keep=False)
0     True
1    False
2     True
3    False
4     True
dtype: bool

Returnsinde
-------
pandas.core.series.Series

See Also
--------
pandas.Index.duplicated : equivalent method on pandas.Index

################################################################################
################################## Validation ##################################
################################################################################

Errors found:
	Errors in parameters section
		Parameter ""keep"" description should start with capital letter
```

If the validation script still gives errors, but you think there is a good reason
to deviate in this case (and there are certainly such cases), please state this
explicitly.

- We (Ghent chapter) decided that an additional line of text (with capital) was less useful than starting with explaining the list of options.

Instead of using the template-based version used before, we split out both docstrings and made a separate for the `Index` versus the `Series`. This introduces some redundancy and overlap (basically, the `keep` argument, also shared with `drop_duplicated`), but provides a cleaner option by having the examples written inside the docstring of the methods and not *somewhere* else in the code."
212835588,15624,DOC: adapt example of groupby filter and transform,stijnvanhoey,closed,2017-03-08T19:42:04Z,2020-01-13T11:22:21Z,"In this pull request the `filter` and `transform` examples are converted to a running doctest
"
29277874,6618,BUG: to_csv extra header line with multiindex columns,dsm054,closed,2014-03-12T16:22:58Z,2020-01-13T12:46:31Z,"This seems strange to me, but I don't often use a MultiIndex so I might be missing something obvious.

```
>>> pd.__version__
'0.13.1-420-g6899ed6'
>>> df2 = pd.DataFrame([1], columns=pd.MultiIndex.from_arrays([[1],[2]]))
>>> df2
   1
   2
0  1

[1 rows x 1 columns]
>>> df2.columns
MultiIndex(levels=[[1], [2]],
           labels=[[0], [0]])
>>> print df2.to_csv()
,1
,2
,
0,1
```

Is there supposed to be that empty line at the end of the header?  Compare

```
>>> print df2.to_csv(header=False)
0,1
```
"
546594851,30808,DOC: Move import conventions from wiki to docs,datapythonista,closed,2020-01-08T01:28:01Z,2020-01-13T13:05:17Z,"The section about imports here: https://github.com/pandas-dev/pandas/wiki/Code-Style-and-Conventions#imports-aim-for-absolute

Can be moved to the code style guide in the documentation: https://dev.pandas.io/docs/development/code_style.html

This way we can remove the page from the wiki, that is mostly outdated."
548477203,30927,DOC: Whatsnew for 1.0 has deprecations listed in Enhancements section,Dr-Irv,closed,2020-01-11T20:35:17Z,2020-01-13T13:08:37Z,"In 1.0.0RC0, at https://dev.pandas.io/docs/whatsnew/v1.0.0.html#other-enhancements (i.e., the list of enhancements), we list the following:

- The pandas.np submodule is now deprecated. Import numpy directly instead (GH30296)
- The pandas.datetime class is now deprecated. Import from datetime instead (GH30296)

Shouldn't those two lines appear in the deprecations section https://dev.pandas.io/docs/whatsnew/v1.0.0.html#deprecations instead?

"
548894026,30961,DOC: Move couple of deprecations whatsnew to correct section,ryankarlos,closed,2020-01-13T12:18:14Z,2020-01-13T13:08:52Z,"- [x] closes #30927
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
548606681,30946,STY: spaces in wrong place (DO NOT MERGE),ShaharNaveh,closed,2020-01-12T18:16:55Z,2020-01-13T14:16:51Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
548918528,30963,Backport PR #30961 on branch 1.0.x (DOC: Move couple of deprecations whatsnew to correct section),meeseeksmachine,closed,2020-01-13T13:08:48Z,2020-01-13T14:19:59Z,Backport PR #30961: DOC: Move couple of deprecations whatsnew to correct section
548587674,30942,STY: concat strings that should not be seperated,ShaharNaveh,closed,2020-01-12T15:46:22Z,2020-01-13T14:52:32Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
548140674,30887,Regression in hash_pandas_object with hash_key=None and object dtype,TomAugspurger,closed,2020-01-10T15:26:58Z,2020-01-13T15:22:04Z,"On 1.0.0rc0, this raises

```pytb
In [7]: pd.util.hash_pandas_object(pd.Series(['a', 'b']), hash_key=None)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-7-00d1f153287e> in <module>
----> 1 pd.util.hash_pandas_object(pd.Series(['a', 'b']), hash_key=None)

~/sandbox/pandas/pandas/core/util/hashing.py in hash_pandas_object(obj, index, encoding, hash_key, categorize)
     93
     94     elif isinstance(obj, ABCSeries):
---> 95         h = hash_array(obj.values, encoding, hash_key, categorize).astype(
     96             ""uint64"", copy=False
     97         )

~/sandbox/pandas/pandas/core/util/hashing.py in hash_array(vals, encoding, hash_key, categorize)
    302             codes, categories = factorize(vals, sort=False)
    303             cat = Categorical(codes, Index(categories), ordered=False, fastpath=True)
--> 304             return _hash_categorical(cat, encoding, hash_key)
    305
    306         try:

~/sandbox/pandas/pandas/core/util/hashing.py in _hash_categorical(c, encoding, hash_key)
    221     # Convert ExtensionArrays to ndarrays
    222     values = np.asarray(c.categories.values)
--> 223     hashed = hash_array(values, encoding, hash_key, categorize=False)
    224
    225     # we have uint64, as we don't directly support missing values

~/sandbox/pandas/pandas/core/util/hashing.py in hash_array(vals, encoding, hash_key, categorize)
    305
    306         try:
--> 307             vals = hashing.hash_object_array(vals, hash_key, encoding)
    308         except TypeError:
    309             # we have mixed types

~/sandbox/pandas/pandas/_libs/hashing.pyx in pandas._libs.hashing.hash_object_array()

AttributeError: 'NoneType' object has no attribute 'encode'
```

On 0.25.3

```python
In [7]: pd.util.hash_pandas_object(pd.Series(['a', 'b']), hash_key=None)
Out[7]:
0     4578374827886788867
1    17338122309987883691
dtype: uint64
```

It's only for object dtype."
548282426,30900,REGR: Fixed hash_key=None for object values,TomAugspurger,closed,2020-01-10T20:46:18Z,2020-01-13T15:22:07Z,Closes https://github.com/pandas-dev/pandas/issues/30887
548682396,30952,CI: numpydev changed double to single quote,jbrockmendel,closed,2020-01-13T03:27:44Z,2020-01-13T15:41:19Z,
548673109,30951,CLN: leftover ix checks,jbrockmendel,closed,2020-01-13T02:39:44Z,2020-01-13T15:41:43Z,
549029743,30970,default set to openpyxl,SuvigyaJain1,closed,2020-01-13T16:07:16Z,2020-01-13T16:26:29Z,"- [x] closes #28546 
- [ ] tests added / passed

"
549000633,30968,Backport PR #30900 on branch 1.0.x (REGR: Fixed hash_key=None for object values),meeseeksmachine,closed,2020-01-13T15:22:14Z,2020-01-13T16:32:34Z,Backport PR #30900: REGR: Fixed hash_key=None for object values
548471619,30926,DOC: Fix whatsnew contributors section,jschendel,closed,2020-01-11T19:42:32Z,2020-01-13T17:13:38Z,"Was reading over the whatsnew and noticed that the [1.0 Contributors section 
](https://dev.pandas.io/docs/whatsnew/v1.0.0.html#contributors) is empty, and the [0.25.3 Contributors section](https://dev.pandas.io/docs/whatsnew/v0.25.3.html#contributors) looks way to big."
548503447,30934,CLN: remove no-op from indexing,jbrockmendel,closed,2020-01-12T01:14:44Z,2020-01-13T17:26:26Z,
548152211,30888,DOC: Move import conventions from wiki to docs #30808,souvik3333,closed,2020-01-10T15:48:25Z,2020-01-13T18:10:26Z,"- [x] closes #30808
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
541475061,30406,WIP/ DOC: Move 'For Developers' content from wiki to contributing docs. #30232,souvik3333,closed,2019-12-22T16:46:05Z,2020-01-13T18:10:29Z,"
"
548805346,30956,BUG: rpow with -1 and pd.NA should return pd.NA,jorisvandenbossche,closed,2020-01-13T09:30:48Z,2020-01-13T19:03:26Z,"In https://github.com/pandas-dev/pandas/pull/30097, we added special cases for 1 and -1 not propagating pd.NA:

```
In [59]: 1 ** pd.NA  
Out[59]: 1

In [60]: (-1) ** pd.NA
Out[60]: -1
```

But for -1 this should probably be pd.NA as result in the end, see https://mail.python.org/pipermail/pandas-dev/2020-January/001174.html

```
In [58]: (-1) ** np.array([2, 3, 2.5]) 
/home/joris/miniconda3/envs/dev/bin/ipython:1: RuntimeWarning: invalid value encountered in power
  #!/home/joris/miniconda3/envs/dev/bin/python
Out[58]: array([ 1., -1., nan])
```

"
548624127,30947,TYP: NDFrame.resample,topper-123,closed,2020-01-12T20:40:49Z,2020-01-13T20:22:55Z,"This PR types up the method ``NDFrame.resample``.

Also renames the func ``resample`` to ``get_resampler`` and import the groupby types used for type checking in frame.py/series.py undir the ``if TYPE_CHECKING:`` clause."
547767888,30869,pandas.util.testing no longer imported with pandas,TomAugspurger,closed,2020-01-09T22:49:44Z,2020-01-13T20:45:43Z,"0.25.3

```python
In [1]: import pandas

In [2]: pandas.util.testing
Out[2]: <module 'pandas.util.testing' from '/private/tmp/pandas/util/testing.py'>
```

1.0

```python
In [1]: import pandas

In [2]: pandas.util.testing
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-2-968e027d039f> in <module>
----> 1 pandas.util.testing

AttributeError: module 'pandas.util' has no attribute 'testing'
```"
549180861,30983,Backport PR #30960 on branch 1.0.x ([BUG] -1 to the power of pd.NA was returning -1),meeseeksmachine,closed,2020-01-13T21:05:06Z,2020-01-13T21:40:20Z,Backport PR #30960: [BUG] -1 to the power of pd.NA was returning -1
549171853,30981,Backport PR #30973 on branch 1.0.x (Compat for util.testing import),meeseeksmachine,closed,2020-01-13T20:45:53Z,2020-01-13T21:40:40Z,Backport PR #30973: Compat for util.testing import
532464692,30035,COMPAT: np 1.18 wants explicit dtype=object),jbrockmendel,closed,2019-12-04T05:47:02Z,2020-01-13T22:02:44Z,"xref #30030, xref https://github.com/numpy/numpy/issues/15041, https://github.com/numpy/numpy/issues/15045

ATM we have 85 tests in the npdev build failing, this gets it down to 14 (at least for me locally).  "
549126255,30979,STY: concat strings,ShaharNaveh,closed,2020-01-13T19:12:37Z,2020-01-13T22:05:23Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
549205981,30987,FIX: Added small corrections to the test for interpolate limit_area,cchwala,closed,2020-01-13T21:57:44Z,2020-01-13T22:54:26Z,"- [ ] closes # nothing to close here
- [ ] tests added / passed
- [ ] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry # not required for this minor fix

Some `tm.assert_series_equal()` have been forgotten in https://github.com/pandas-dev/pandas/blob/2baf788819cd1073d9c15444ebcefcfab8e4b9c8/pandas/tests/series/test_missing.py#L1338

There was also a typo in https://github.com/pandas-dev/pandas/blob/2baf788819cd1073d9c15444ebcefcfab8e4b9c8/pandas/tests/series/test_missing.py#L1374 where `direction` has to be `limit_direction`.

These things have been fixed in this small PR."
549253058,30991,STY: concat strings,ShaharNaveh,closed,2020-01-14T00:03:03Z,2020-01-14T00:51:08Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
549063067,30973,Compat for util.testing import,TomAugspurger,closed,2020-01-13T17:03:46Z,2020-01-14T04:17:39Z,Closes #30869
547854881,30877,CLN: misc cleanups,jbrockmendel,closed,2020-01-10T04:12:59Z,2020-01-14T04:20:44Z,Things I find myself doing in multiple branches.
549254961,30992,CLN: de-duplicate _getitem_scalar,jbrockmendel,closed,2020-01-14T00:08:57Z,2020-01-14T04:47:50Z,
545460774,30714,REF: share _union between DTI/TDI,jbrockmendel,closed,2020-01-05T20:08:33Z,2020-01-06T15:31:16Z,
545459553,30713,Fix PeriodIndex._shallow_copy allowing object-dtype,jbrockmendel,closed,2020-01-05T19:56:33Z,2020-01-06T15:32:06Z,
544738370,30626,REF: delegate more IntervalIndex methods,jbrockmendel,closed,2020-01-02T20:41:34Z,2020-01-06T15:38:22Z,cc @jschendel is the usage of _simple_new instead of _shallow_copy `take` important?  Should it always be the case that `idx.close == idx._data.closed`?
492585718,28399,BUG: Fix Series(List[Interval]) to infer interval dtype,jschendel,closed,2019-09-12T04:56:42Z,2020-01-06T15:41:43Z,"- [X] closes #23563
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
544698382,30622,DOC: Fixing PR09 formatting errors,HughKelley,closed,2020-01-02T18:46:30Z,2020-01-06T16:46:55Z,"-  part of #28602 
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

just the good stuff from #29530 "
520610916,29530,"DOC: Fixes to docstrings, mainly PR09 errors",HughKelley,closed,2019-11-10T14:48:17Z,2020-01-06T16:46:57Z,closes #28602
519356451,29466,PR09 batch 4,HughKelley,closed,2019-11-07T15:58:25Z,2020-01-06T16:47:00Z,chunk of #28602 
518606670,29434,Pr09 batch 3,HughKelley,closed,2019-11-06T17:03:42Z,2020-01-06T16:47:00Z,part of #28602 
517207951,29396,PR09 Batch 2,HughKelley,closed,2019-11-04T14:58:19Z,2020-01-06T16:47:02Z,Another batch of commits for #28602. Also fixes a few summary formatting errors and PR08 capitalization errors. 
512547671,29224,Period index 29204,HughKelley,closed,2019-10-25T14:26:15Z,2020-01-06T16:47:18Z,"- [ ] closes #29204 
"
513530609,29257,Timedelta index 29236,HughKelley,closed,2019-10-28T20:11:15Z,2020-01-06T16:47:22Z,"- [ ] closes #29236 

only remaining docstring errors are for `TimedeltaIndex.mean`

    Parameters {*args, **kwargs} not documented
    Unknown parameters {skipna}


"
510713886,29159,Period 29073,HughKelley,closed,2019-10-22T14:55:02Z,2020-01-06T16:47:24Z,"- [ ] closes #29073
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
516290045,29324,Pr09 batch 1,HughKelley,closed,2019-11-01T18:54:28Z,2020-01-06T16:47:26Z,"fixing PR09 missing period formatting errors in various parameter descriptions. 

Subset of #28602, mainly functions with single parameter's missing periods that didn't make sense to open an entire issue for. 

I'm periodically updating the main issue for PR09 with the current list of errors remaining as they're removed. "
545377775,30706,CLN: replacing '.format' with f-strings in various files,AlfredoGJ,closed,2020-01-05T06:17:42Z,2020-01-06T17:45:18Z,"- [x]  contributes to #29547
- [ ] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
545495211,30721,Make DTA _check_compatible_with less strict by default,jbrockmendel,closed,2020-01-06T01:14:01Z,2020-01-06T17:47:48Z,"One of the two non-cosmetic things mentioned in #30720.

There are a bunch of places where DTA or DTI do a compatibility check that for tz_awareness_compat, but not requiring the same tz.  This check is analogous to `PeriodArray._check_compatible_with` and `TimedeltaArray._check_compatible_with`, so this adds a kwarg to _check_compatible_with so that we can use _check_compatible_with in all the relevant places and subsequently de-duplicate a bunch of code.

In addition to the comparisons, this is going to be relevant for searchsorted and insert, where we have slightly different behavior in a bunch of EA/Index subclasses."
545496428,30722,BUG: PeriodArray comparisons inconsistent with Period comparisons,jbrockmendel,closed,2020-01-06T01:20:48Z,2020-01-06T17:53:25Z,The second of two non-cosmetic changes mentioned in #30720.
545474964,30716,ValueError when reading JSON lines file ,danijar,closed,2020-01-05T22:21:20Z,2020-01-06T17:58:04Z,"## Overview

Using `pandas==0.25.1` with `Python 3.7.1` on Debian, loading the following JSON lines file fails using `pandas.read_json()` but succeeds when read manually.

After looking into this a bit, I think it might be related to `NaN` in the JSON file which is not supported by the spec but accepted by `json.loads()`. If that turns out to be the case, it would be good to have an option to ignore those entries or at least provide a detailed error message.

Data file: https://gist.github.com/danijar/37ba75a6991d61de9e77755329bb5ef4

## Manual

Reading the file manually using `json.loads()` and passing it to a `pd.DataFrame` works fine:

```python
import json
import pandas as pd
with open(filename) as f:
  df = pd.DataFrame([json.loads(l) for l in f.readlines()])
print(df)  # Shows data frame as expected
```

<details><summary>Terminal output</summary>

```text
       step  train/return  train/length  episodes  ...  value_loss  action_loss  action_ent        fps
0      1000           1.0         500.0       1.0  ...         NaN          NaN         NaN        NaN
1      2000           0.0         500.0       2.0  ...         NaN          NaN         NaN        NaN
2      3000         163.0         500.0       3.0  ...         NaN          NaN         NaN        NaN
3      4000           0.0         500.0       4.0  ...         NaN          NaN         NaN        NaN
4      5000           0.0         500.0       5.0  ...         NaN          NaN         NaN        NaN
..      ...           ...           ...       ...  ...         ...          ...         ...        ...
798  383000           0.0         500.0     383.0  ...         NaN          NaN         NaN        NaN
799  383000           NaN           NaN       NaN  ...         NaN          NaN         NaN  19.500059
800  384000           0.0         500.0     384.0  ...         NaN          NaN         NaN        NaN
801  384000           NaN           NaN       NaN  ...         NaN          NaN         NaN  19.608651
802  385000        1000.0         500.0     385.0  ...         NaN          NaN         NaN        NaN

[803 rows x 19 columns]
```

</details>

## Pandas

But reading the same file with `pandas.read_json()` fails with an Pandas internal error:

```python
import pandas as pd
df = pd.read_json(filename, lines=True)  # ValueError: Expected object or value
```

<details><summary>Terminal output</summary>


```text
<path-to-python3.7>/site-packages/pandas/io/json/_json.py in read_json(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)
    590         return json_reader
    591
--> 592     result = json_reader.read()
    593     if should_close:
    594         try:

<path-to-python3.7>/site-packages/pandas/io/json/_json.py in read(self)
    713         elif self.lines:
    714             data = ensure_str(self.data)
--> 715             obj = self._get_object_parser(self._combine_lines(data.split(""\n"")))
    716         else:
    717             obj = self._get_object_parser(self.data)

<path-to-python3.7>/site-packages/pandas/io/json/_json.py in _get_object_parser(self, json)
    737         obj = None
    738         if typ == ""frame"":
--> 739             obj = FrameParser(json, **kwargs).parse()
    740
    741         if typ == ""series"" or obj is None:

<path-to-python3.7>/site-packages/pandas/io/json/_json.py in parse(self)
    847
    848         else:
--> 849             self._parse_no_numpy()
    850
    851         if self.obj is None:

<path-to-python3.7>/site-packages/pandas/io/json/_json.py in _parse_no_numpy(self)
   1091         if orient == ""columns"":
   1092             self.obj = DataFrame(
-> 1093                 loads(json, precise_float=self.precise_float), dtype=None
   1094             )
   1095         elif orient == ""split"":

ValueError: Expected object or value
```

</details>"
545868404,30749,Fix PR08 errors,galuhsahid,closed,2020-01-06T18:22:38Z,2020-01-06T18:59:49Z,"Fixes PR08 errors. When I ran the script, a lot of them seem to be false positives, these are the ones I'm pretty sure should be fixed:
```
pandas.infer_freq: Parameter ""index"" description should start with a capital letter
pandas.MultiIndex.get_loc_level: Parameter ""drop_level"" description should start with a capital letter
```
Related to #27977 cc @datapythonista

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
545482103,30718,TYP: type up parts of frame.py,topper-123,closed,2020-01-05T23:29:10Z,2020-01-06T19:03:46Z,Type up methods with a single return value type.
545817005,30744,PERF: Categorical indexing performance regression,jorisvandenbossche,closed,2020-01-06T16:28:56Z,2020-01-06T19:28:11Z,"Recent regression in the `categoricals.CategoricalSlicing.time_getitem_list` benchmark: https://pandas.pydata.org/speed/pandas/#categoricals.CategoricalSlicing.time_getitem_list?commits=6efc2379-b9de33e3

Reproducible example for this benchmark:

```
N = 10 ** 6
categories = [""a"", ""b"", ""c""]
values = [0] * N + [1] * N + [2] * N
data = pd.Categorical.from_codes(values, categories=categories)

list_ = list(range(10000))

%timeit data[list_]
```

Now, this slowdown is due to the changes in https://github.com/pandas-dev/pandas/pull/30308. Categorical `__getitem__` now checks if the key is a boolean indexer: https://github.com/pandas-dev/pandas/pull/30308/files#diff-f3b2ea15ba728b55cab4a1acd97d996d

So this slowdown is of course expected, and also only for Categorical itself (eg pd.Series indexing already handles this boolean checking). So in that light, we can certainly ignore this regression. 
But, this led me think: maybe the ExtensionArrays are a good place to start not supporting object dtype as boolean indexer? (and so not add support for it now, which also avoids this performance regression)"
545851201,30747,PERF: Categorical getitem perf,TomAugspurger,closed,2020-01-06T17:42:48Z,2020-01-06T19:29:39Z,"Convert to an array earlier on.
Closes https://github.com/pandas-dev/pandas/issues/30744"
545888258,30752,REF: share _validate_fill_value,jbrockmendel,closed,2020-01-06T19:09:07Z,2020-01-06T19:51:00Z,Made feasible by #30721.
395317662,24559,Changes to i8data for DatetimeIndex,TomAugspurger,closed,2019-01-02T17:14:21Z,2020-01-06T20:24:04Z,"Master currently has an (undocumented) (maybe-) API-breaking change from 0.23.4 when passed integer values

0.23.4

```python
In [2]: i8data = np.arange(5) * 3600 * 10**9

In [3]: pd.DatetimeIndex(i8data, tz=""US/Central"")
Out[3]:
DatetimeIndex(['1970-01-01 00:00:00-06:00', '1970-01-01 01:00:00-06:00',
               '1970-01-01 02:00:00-06:00', '1970-01-01 03:00:00-06:00',
               '1970-01-01 04:00:00-06:00'],
              dtype='datetime64[ns, US/Central]', freq=None)
```

Master

```python
In [3]: pd.DatetimeIndex(i8data, tz=""US/Central"")
Out[3]:
DatetimeIndex(['1969-12-31 18:00:00-06:00', '1969-12-31 19:00:00-06:00',
               '1969-12-31 20:00:00-06:00', '1969-12-31 21:00:00-06:00',
               '1969-12-31 22:00:00-06:00'],
              dtype='datetime64[ns, US/Central]', freq=None)

```

---

Attempt to explain the behavior: In 0.23.4, passing an `ndarray[i8]` was equivalent to passing `data.view(""M8[ns]"")`

```python
# 0.23.4
In [4]: pd.DatetimeIndex(i8data.view(""M8[ns]""), tz=""US/Central"")
Out[4]:
DatetimeIndex(['1970-01-01 00:00:00-06:00', '1970-01-01 01:00:00-06:00',
               '1970-01-01 02:00:00-06:00', '1970-01-01 03:00:00-06:00',
               '1970-01-01 04:00:00-06:00'],
              dtype='datetime64[ns, US/Central]', freq=None)
```

On master, integer values are treated as unix timestamps, while M8[ns] values are treated as wall-times in the given timezone.

```python
# master
In [4]: pd.DatetimeIndex(i8data.view(""M8[ns]""), tz=""US/Central"")
Out[4]:
DatetimeIndex(['1970-01-01 00:00:00-06:00', '1970-01-01 01:00:00-06:00',
               '1970-01-01 02:00:00-06:00', '1970-01-01 03:00:00-06:00',
               '1970-01-01 04:00:00-06:00'],
              dtype='datetime64[ns, US/Central]', freq=None)
```

---

**Reason for the change**

There are four cases of interest:

```
In [4]: arr = np.arange(5) * 24 * 3600 * 10**9
In [5]: tz = 'US/Pacific'

In [6]: a = pd.DatetimeIndex(arr, tz=tz)
In [7]: b = pd.DatetimeIndex(arr.view('M8[ns]'), tz=tz)
In [8]: c = pd.DatetimeIndex._simple_new(arr, tz=tz)
In [9]: d = pd.DatetimeIndex._simple_new(arr.view('M8[ns]'), tz=tz)

In [10]: a
Out[10]: 
DatetimeIndex(['1970-01-01 00:00:00-08:00', '1970-01-02 00:00:00-08:00',
               '1970-01-03 00:00:00-08:00', '1970-01-04 00:00:00-08:00',
               '1970-01-05 00:00:00-08:00'],
              dtype='datetime64[ns, US/Pacific]', freq=None)

In [11]: b
Out[11]: 
DatetimeIndex(['1970-01-01 00:00:00-08:00', '1970-01-02 00:00:00-08:00',
               '1970-01-03 00:00:00-08:00', '1970-01-04 00:00:00-08:00',
               '1970-01-05 00:00:00-08:00'],
              dtype='datetime64[ns, US/Pacific]', freq=None)

In [12]: c
Out[12]: 
DatetimeIndex(['1969-12-31', '1970-01-01', '1970-01-02', '1970-01-03',
               '1970-01-04'],
              dtype='datetime64[ns, US/Pacific]', freq=None)

In [13]: d
Out[13]: 
DatetimeIndex(['1969-12-31', '1970-01-01', '1970-01-02', '1970-01-03',
               '1970-01-04'],
              dtype='datetime64[ns, US/Pacific]', freq=None)
```

In 0.23.4 we have `a.equals(b)` and `c.equals(d)` but no way to pass data in a way that was constructor-neutral.  In master we now have `a` match `c` and `d`.  At some point in the refactoring process we changed that, but off the top of my head I don't remember when or if this was the precise motivation or just a side-benefit.

BTW _simple_new was also way too much:

```
        if getattr(values, 'dtype', None) is None:
            # empty, but with dtype compat
            if values is None:
                values = np.empty(0, dtype=_NS_DTYPE)
                return cls(values, name=name, freq=freq, tz=tz,
                           dtype=dtype, **kwargs)
            values = np.array(values, copy=False)

        if is_object_dtype(values):
            return cls(values, name=name, freq=freq, tz=tz,
                       dtype=dtype, **kwargs).values
        elif not is_datetime64_dtype(values):
            values = _ensure_int64(values).view(_NS_DTYPE)
```

---

**Was this documented**?

http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DatetimeIndex.html mentions that it's ""represented internally as int64"".

The (imprecise) type on `data` is ""Optional datetime-like data""

I don't see anything in http://pandas.pydata.org/pandas-docs/stable/timeseries.html suggesting that integers can be passed to DatetimeIndex."
436849646,26206,REGR? no error anymore when converting out of bounds datetime64[non-ns] data,jorisvandenbossche,closed,2019-04-24T18:41:36Z,2020-01-06T20:53:21Z,"Didn't directly find a related issue, but on master / 0.24 / 0.23, we see:

```
In [1]: pd.Series(np.array(['2262-04-12'], dtype='datetime64[D]'))
Out[1]: 
0   1677-09-21 00:25:26.290448384
dtype: datetime64[ns]
```

while on pandas 0.22.0:

```
In [1]: pd.Series(np.array(['2262-04-12'], dtype='datetime64[D]'))
---------------------------------------------------------------------------
OutOfBoundsDatetime                       Traceback (most recent call last)
<ipython-input-1-b3f7cbbf1054> in <module>()
----> 1 pd.Series(np.array(['2262-04-12'], dtype='datetime64[D]'))

~/miniconda3/envs/pandas022/lib/python3.6/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath)
    264                                        raise_cast_failure=True)
    265 
--> 266                 data = SingleBlockManager(data, index, fastpath=True)
    267 
    268         generic.NDFrame.__init__(self, data, fastpath=True)

~/miniconda3/envs/pandas022/lib/python3.6/site-packages/pandas/core/internals.py in __init__(self, block, axis, do_integrity_check, fastpath)
   4400         if not isinstance(block, Block):
   4401             block = make_block(block, placement=slice(0, len(axis)), ndim=1,
-> 4402                                fastpath=True)
   4403 
   4404         self.blocks = [block]

~/miniconda3/envs/pandas022/lib/python3.6/site-packages/pandas/core/internals.py in make_block(values, placement, klass, ndim, dtype, fastpath)
   2955                      placement=placement, dtype=dtype)
   2956 
-> 2957     return klass(values, ndim=ndim, fastpath=fastpath, placement=placement)
   2958 
   2959 # TODO: flexible with index=None and/or items=None

~/miniconda3/envs/pandas022/lib/python3.6/site-packages/pandas/core/internals.py in __init__(self, values, placement, fastpath, **kwargs)
   2468     def __init__(self, values, placement, fastpath=False, **kwargs):
   2469         if values.dtype != _NS_DTYPE:
-> 2470             values = tslib.cast_to_nanoseconds(values)
   2471 
   2472         super(DatetimeBlock, self).__init__(values, fastpath=True,

pandas/_libs/tslib.pyx in pandas._libs.tslib.cast_to_nanoseconds()

pandas/_libs/tslib.pyx in pandas._libs.tslib._check_dts_bounds()

OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 2262-04-12 00:00:00

In [2]: pd.__version__
Out[2]: '0.22.0'
```

cc @jbrockmendel any idea if this was changed on purpose or to what refactoring could have been the cause of this change?
"
545864558,30748,`pandas.DataFrame.explode()` combines rows with repeated index values,gsganden,closed,2020-01-06T18:13:38Z,2020-01-06T20:57:37Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

df = pd.DataFrame({""A"": [[1, 2], [3, 4]], ""B"": [""x"", ""y""]}, index=[0, 0])
df.explode(""A"")
```

Output:

```
   A  B
0  1  x
0  2  x
0  3  x
0  4  x
0  1  y
0  2  y
0  3  y
0  4  y
```

#### Problem description

We are getting rows with e.g. `A=3` and `B=x`, which never appears in the original data. The `.explode()` method appears to be effectively combining rows with the same index value before splitting them, which is surprising at least to me.

#### Expected Output

```
   A  B
0  1  x
0  2  x
0  3  y
0  4  y
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.77-70.82.amzn1.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : None
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.2
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.2.0
Cython           : 0.28.4
pytest           : 5.2.0
hypothesis       : 4.38.1
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.7.7 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
s3fs             : 0.4.0
scipy            : 1.3.1
sqlalchemy       : 1.3.10
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None


</details>
"
545946058,30756,TST: Use datapath fixture,TomAugspurger,closed,2020-01-06T21:21:37Z,2020-01-06T22:26:26Z,"This was failing the wheel build.

https://travis-ci.org/MacPython/pandas-wheels/jobs/633451994.

I tried briefly to write a code check for this, but didn't succeed."
545034787,30644,DOC: Change refs in docs from pandas.SparseArray to pandas.arrays.SparseArray,Dr-Irv,closed,2020-01-03T15:18:57Z,2020-01-06T23:10:20Z,"- [x ] closes #30642
- [ ] tests added / passed
  - N/A
- [ ] passes `black pandas`
  - N/A
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
  - N/A
- [x] whatsnew entry

Change all references in docs to use `arrays.SparseArray` rather than just `SparseArray`
"
545973951,30760,DOC: new EAs,TomAugspurger,closed,2020-01-06T22:29:19Z,2020-01-06T23:37:14Z,
545977987,30762,DOC: Fix the string example.,TomAugspurger,closed,2020-01-06T22:40:35Z,2020-01-06T23:39:57Z,"After moving StringArray to use pd.NA `.astype(object)` had
NA instead of NaN, so the output was object rather than float."
545402893,30707,STY: Spaces over concat strings - batch 1,ShaharNaveh,closed,2020-01-05T11:03:40Z,2020-01-06T23:42:16Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
545953623,30758,DataFrame accessors can be overridden by column names,TomAugspurger,closed,2020-01-06T21:39:42Z,2020-01-06T23:52:02Z,"I don't think we want this

```python
In [1]: import pandas as pd

In [2]: df = pd.DataFrame({""sparse"": [1, 2], ""b"": pd.SparseArray([1, 2])})
/Users/taugspurger/.virtualenvs/pandas-dev/bin/ipython:1: FutureWarning: The pandas.SparseArray class is deprecated and will be removed from pandas in a future version. Use pandas.arrays.SparseArray instead.
  #!/Users/taugspurger/Envs/pandas-dev/bin/python

In [3]: df.sparse
Out[3]:
0    1
1    2
Name: sparse, dtype: int64
```

That should instead return the accessor."
545955750,30759,BUG: Fixed getattr for frame with column sparse,TomAugspurger,closed,2020-01-06T21:44:39Z,2020-01-06T23:52:05Z,Closes https://github.com/pandas-dev/pandas/issues/30758
545718733,30735,BUG/DEPR: the deprecation of util.testing fails for direct imports,jorisvandenbossche,closed,2020-01-06T13:11:30Z,2020-01-07T00:01:14Z,"```
In [1]: from pandas.util.testing import assert_frame_equal  
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-79d99d902fdd> in <module>
----> 1 from pandas.util.testing import assert_frame_equal

ImportError: cannot import name 'assert_frame_equal' from 'pandas.util.testing' (/home/joris/scipy/pandas/pandas/util/testing/__init__.py)
```

It works when accessing from top-level import:

```
In [3]: pd.util.testing.assert_frame_equal
/home/joris/miniconda3/envs/dev/bin/ipython:1: FutureWarning: pandas._testing.assert_frame_equal is deprecated. Please use pandas.testing.assert_frame_equal instead.
  #!/home/joris/miniconda3/envs/dev/bin/python
Out[3]: <function pandas._testing.assert_frame_equal(left, right, check_dtype=True, check_index_type='equiv', check_column_type='equiv', check_frame_type=True, check_less_precise=False, check_names=True, by_blocks=False, check_exact=False, check_datetimelike_compat=False, check_categorical=True, check_like=False, obj='DataFrame')>
```"
545827283,30745,DEPR/REGR: Fix pandas.util.testing deprecation,TomAugspurger,closed,2020-01-06T16:50:02Z,2020-01-07T00:01:18Z,"Closes https://github.com/pandas-dev/pandas/issues/30735

This avoids using _DeprecatedModule, which doesn't work for
direct imports from a module. Sorry for the importlib magic, but
I think this is the correct way to do things.

cc @jorisvandenbossche."
499690584,28662,BUG: Fix groupby.apply,dsaxton,closed,2019-09-27T23:27:41Z,2020-01-07T00:30:25Z,"- [x] closes #28652
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Makes sure that the output of `groupby.apply` is built up by value instead of by reference in `reduction.pyx` to avoid the behavior from #28652."
545995096,30764,BUG: TDI/DTI _shallow_copy creating invalid arrays,jbrockmendel,closed,2020-01-06T23:31:50Z,2020-01-07T01:40:36Z,"Following this we should be able to use shallow_copy in indexes.extension more, which will help with perf (xref #30717)"
545945138,30754,BUG: DTI/TDI .insert accepting incorrectly-dtyped NaT,jbrockmendel,closed,2020-01-06T21:19:39Z,2020-01-07T01:41:30Z,"Also TDI.insert trying to parse strings to Timedelta, which neither DTI nor PI do."
545882419,30751,REF: share comparison methods for DTA/TDA/PA,jbrockmendel,closed,2020-01-06T18:55:08Z,2020-01-07T01:56:31Z,
545685742,30730,Nightly Builds,marco-neumann-by,closed,2020-01-06T11:44:08Z,2020-01-07T12:06:51Z,"Until 2019-11-02, https://7933911d6844c6c53a7d-47bd50c35cd79bd838daf386af554a83.ssl.cf2.rackcdn.com/ contains nightly builds of pandas for different platforms. Is there any chance that this will be continued? I am asking because nightly testing is very helpful for some downstream artifacts to discover unintentional breaking chances or regressions before a new pandas version gets released. Building pandas from source on CI takes very long though, up to a duration where it gets kinda unpractical for downstream projects."
546219764,30775,DOC: Fixtures docs in io/parser/conftest.py,gfyoung,closed,2020-01-07T11:00:27Z,2020-01-07T12:09:44Z,"Partially addresses:

https://github.com/pandas-dev/pandas/issues/19159"
546002865,30767,STY: spaces in wrong place,ShaharNaveh,closed,2020-01-07T00:00:28Z,2020-01-07T13:33:32Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
508389734,29050,Support customised S3 servers endpoint URL,xieqihui,closed,2019-10-17T10:42:31Z,2020-01-07T14:24:53Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Add support for customised S3 servers via checking the environment variable `S3_ENDPOINT`.

If set, the value of `S3_ENDPOINT` will be passed to the `endpoint_url` parameter for `boto3.Session`. It should be the complete URL to S3 service following the format: http(s)://{host}:{port}.

This feature is useful for companies who use their own S3 servers like MinIO, Ceph etc, as mentioned in issue  #26195"
545130979,30656, CLN: Deprecate pandas.SparseArray for pandas.arrays.SparseArray,Dr-Irv,closed,2020-01-03T19:36:17Z,2020-01-07T14:35:23Z,"- [x] closes #30642 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
  - in #30644

Change all references in code from `pd.SparseArray` to `pd.arrays.SparseArray` .  Add deprecation message for `pd.SparseArray`

Per comment from @TomAugspurger here: https://github.com/pandas-dev/pandas/pull/30644#issuecomment-570671439, this may require discussion."
545741666,30739,DOC: Capitalize the 'p' in 'pandas code style guide',ShaharNaveh,closed,2020-01-06T14:00:34Z,2020-01-07T14:39:40Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
546164834,30774,BLD: more informative error message when trying to cythonize with old cython version,jorisvandenbossche,closed,2020-01-07T09:08:53Z,2020-01-07T16:09:34Z,"cc @jbrockmendel building upon your https://github.com/pandas-dev/pandas/pull/30498, but making the error message more specific when cython is actually installed but too old."
546084839,30768,CLN: Simplify logic in _format_labels function for cut/qcut,jschendel,closed,2020-01-07T05:18:48Z,2020-01-07T16:11:42Z,"Small simplification: modify the `breaks` metadata before creating an `IntervalIndex` then create and an `IntervalIndex` from the modified `breaks`.  The existing approach creates an `IntervalIndex`, modifies the first `Interval`, then creates a new `IntervalIndex` with the updated first `Interval`.

This yields a slight performance improvement but doesn't seem dramatic enough to warrant a whatsnew entry, though I can add one if desired.

On this branch:
```python
In [1]: import numpy as np; import pandas as pd; pd.__version__
Out[1]: '0.26.0.dev0+1668.ga6c08fc02'

In [2]: a = np.arange(10**5)

In [3]: %timeit pd.qcut(a, 10**4)
273 ms ± 914 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

On `master`:
```python
In [1]: import numpy as np; import pandas as pd; pd.__version__
Out[1]: '0.26.0.dev0+1667.g40bff2fed'

In [2]: a = np.arange(10**5)

In [3]: %timeit pd.qcut(a, 10**4)
317 ms ± 1.14 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```"
539690087,30322,ENH: add ExtensionArray.to_numpy to have control over conversion to numpy array,jorisvandenbossche,closed,2019-12-18T13:45:15Z,2020-01-07T16:26:17Z,"xref https://github.com/pandas-dev/pandas/issues/30038

Would still need to add this to the other arrays with NA (IntegerArray, StringArray), and need to pass through such option from Series.to_numpy. But already putting this up to check if we are OK with such interface and behaviour."
544670822,30620,DEPR: pandas.util.testing,TomAugspurger,closed,2020-01-02T17:27:01Z,2020-01-07T16:29:07Z,"Closes https://github.com/pandas-dev/pandas/issues/16232.

I tried to keep the commits somewhat clean.

https://github.com/pandas-dev/pandas/commit/e96624b8a3f903798f7977fb92da09fc7417ec98 and https://github.com/pandas-dev/pandas/commit/d52d35f9d6a5e721ae0e6aa1579e44dac820ed7a can probably be ignored. That's just moving `pandas.util.testing` to `pandas.util._testing` and updating all the relevant imports.

Main question: do we want to make the `tm.make*` methods public? I'd say we probably should, but wanted to confirm."
546379663,30785,Rename api.extensions._no_default to extension.no_default,jorisvandenbossche,closed,2020-01-07T16:27:15Z,2020-01-07T19:03:02Z,"Follow-up on https://github.com/pandas-dev/pandas/pull/30322, which exposed `lib._no_default` in `pandas.api.extensions`"
546399979,30788,API: no_default,TomAugspurger,closed,2020-01-07T17:05:11Z,2020-01-07T19:03:06Z,"Changes lib._no_default to lib.no_default, uses it in more places.

Closes #30785 "
388326965,24130,read_csv can't roundtrip with UTF16/32 encodings,WillAyd,closed,2018-12-06T17:50:52Z,2020-01-07T20:46:54Z,"This works fine:

```python
In [4]: with tempfile.TemporaryFile(mode='w+', encoding='utf8') as outfile: 
   ...:     outfile.write('foo') 
   ...:     outfile.seek(0) 
   ...:     pd.read_csv(outfile, encoding='utf8') 
```

Not quite so lucky on these:

```python-traceback
In [4]: with tempfile.TemporaryFile(mode='w+', encoding='utf16') as outfile: 
   ...:     outfile.write('foo') 
   ...:     outfile.seek(0) 
   ...:     pd.read_csv(outfile, encoding='utf16') 
UnicodeDecodeError: 'utf-16-le' codec can't decode byte 0x6f in position 2: truncated data

In [4]: with tempfile.TemporaryFile(mode='w+', encoding='utf32') as outfile: 
   ...:     outfile.write('foo') 
   ...:     outfile.seek(0) 
   ...:     pd.read_csv(outfile, encoding='utf32') 
UnicodeDecodeError: 'utf-32-le' codec can't decode bytes in position 0-2: truncated data
```

I believe this is strictly a problem with the C parser. 

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: b78aa8d8506ac119124619a9e03ff1482262e0cc
python: 3.6.7.final.0
python-bits: 64
OS: Darwin
OS-release: 18.2.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.0.dev0+1223.gb78aa8d85
pytest: 4.0.0
pip: 18.1
setuptools: 40.6.2
Cython: 0.29
numpy: 1.15.4
scipy: 1.1.0
pyarrow: 0.11.1
xarray: 0.11.0
IPython: 7.1.1
sphinx: 1.8.2
patsy: 0.5.1
dateutil: 2.7.5
pytz: 2018.7
blosc: None
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.8
feather: None
matplotlib: 3.0.1
openpyxl: 2.5.9
xlrd: 1.1.0
xlwt: 1.2.0
xlsxwriter: 1.1.2
lxml.etree: 4.2.5
bs4: 4.6.3
html5lib: 1.0.1
sqlalchemy: 1.2.14
pymysql: 0.9.2
psycopg2: None
jinja2: 2.10
s3fs: 0.1.6
fastparquet: 0.1.6
pandas_gbq: None
pandas_datareader: None
gcsfs: 0.2.0
</details>
"
546102820,30771,BUG: Expand encoding for C engine beyond utf-16,gfyoung,closed,2020-01-07T06:20:00Z,2020-01-07T20:46:58Z,"And by `utf-16`, we mean the string `""utf-16""`

Closes https://github.com/pandas-dev/pandas/issues/24130"
546382164,30786,"REV: move unique, _get_unique_index to ExtensionIndex",jbrockmendel,closed,2020-01-07T16:31:38Z,2020-01-07T21:00:36Z,Broken off from #30717.
157402882,13318,ERR: cut/qcut need better error message when passing invalid input,simonm3,closed,2016-05-29T20:02:46Z,2020-01-07T21:18:35Z,"Labels=False means use integers as category names

To use the category labels I would expect to say labels=True but instead you have to say labels=None.

It seems illogical to say labels=None when you want labels.
"
546449194,30791,BUG: DTI/TDI/PI `where` accepting non-matching dtypes,jbrockmendel,closed,2020-01-07T18:53:35Z,2020-01-07T23:06:40Z,"This bug was hidden by _ensure_datetimelike_to_i8, and the only other place where that is used is in _round.  _round is clearer without using it, so ensure_datetimelike_to_i8 gets ripped out, and with it we can get rid of _ensure_localized."
155799966,13230,Different order in datetime-column using sort_values on multiple columns,marcomayer,closed,2016-05-19T18:09:42Z,2020-01-07T23:45:40Z,"#### Code Sample, a copy-pastable example if possible

```
#0.17.1:
df = pd.DataFrame([1,2,3,4,5], columns=list('A'), index=pd.date_range('2010-01-01 09:00:00', periods=5, freq='s')).reset_index()

df['date'] = df['index']
del df['index']
df.loc[4,'A'] = 4
df.loc[4,'date'] = pd.NaT

print(df.sort_values(['A','date']))

  A                date
0  1 2010-01-01 09:00:00
1  2 2010-01-01 09:00:01
2  3 2010-01-01 09:00:02
4  4                 NaT
3  4 2010-01-01 09:00:03

#0.18.1:
df = pd.DataFrame([1,2,3,4,5], columns=list('A'), index=pd.date_range('2010-01-01 09:00:00', periods=5, freq='s')).reset_index()

df['date'] = df['index']
del df['index']
df.loc[4,'A'] = 4
df.loc[4,'date'] = pd.NaT

print(df.sort_values(['A','date']))

   A                date
0  1 2010-01-01 09:00:00
1  2 2010-01-01 09:00:01
2  3 2010-01-01 09:00:02
3  4 2010-01-01 09:00:03
4  4                 NaT
```
#### Expected Output

This one was hard to find. The order stays the same as in 0.17.1 when using only sort_values('date'), but using multiple cols, it changes sorting datetimes with NaT. Couldn't find anything in the Changelogs that points to a reason for this.
#### output of `pd.show_versions()`
## INSTALLED VERSIONS

commit: None
python: 3.5.1.final.0
python-bits: 64
OS: Darwin
OS-release: 15.5.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: de_DE.UTF-8

pandas: 0.18.1
nose: 1.3.7
pip: 8.1.2
setuptools: 20.3
Cython: 0.23.4
numpy: 1.10.4
scipy: 0.17.1
statsmodels: 0.6.1
xarray: None
IPython: 4.2.0
sphinx: 1.3.5
patsy: 0.4.0
dateutil: 2.5.1
pytz: 2016.2
blosc: None
bottleneck: 1.0.0
tables: 3.2.2
numexpr: 2.5.2
matplotlib: 1.5.1
openpyxl: 2.3.2
xlrd: 0.9.4
xlwt: 1.0.0
xlsxwriter: 0.8.4
lxml: 3.6.0
bs4: 4.4.1
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 1.0.12
pymysql: None
psycopg2: None
jinja2: 2.8
boto: 2.39.0
pandas_datareader: 0.2.1
"
167885574,13820,ERR: better error message on invalid return from .apply,mhabets,closed,2016-07-27T15:37:16Z,2020-01-07T23:45:41Z,"#### Code Sample

``` python
def apply_list (row):
     return [2*row['A'], 4*row['C'], 3*row['B']]

df = pd.DataFrame(np.random.randn(6,4), columns=list('ABCD'))
df['E'] = pd.Timestamp('20130102')

df['L'] = df.apply(apply_list, axis=1)
```
#### (Wrong) Error Output

```
ValueError: Shape of passed values is (6, 3), indices imply (6, 7)
```
#### Expected Result & Use case

Expected: a Serie containing a list for each row which are **in my real case the result of a matrix multiplication processed with values coming from other columns** (see a complete example in the next section)
Note: It works with a DataFrame without a datetime column (but not with a DataFrame with it):

```
0  [0.513057023122, -0.473155481431, -4.51039058299]  
1    [-0.331758428452, 3.92166465759, 2.75920806524]  
2    [-2.07257568656, 1.22070341071, 0.809676040678]  
3       [3.38079201699, 2.77074189984, 1.4351938036]  
4      [2.27838740024, 3.04253558763, 3.57504651688]  
5     [1.55497385554, 8.01021203173, -2.22893240905]  
```
#### Complete example with matrix multiplication

``` python
from numpy import pi, mat, cos, sin

def rotation(lon_rad, lat_rad):
    c_lat = cos(lat_rad); s_lat = sin(lat_rad)
    c_lon = cos(lon_rad); s_lon = sin(lon_rad)
    R = mat([[-s_lon, -s_lat*c_lon, c_lat*c_lon],
             [ c_lon, -s_lat*s_lon, c_lat*s_lon],
             [     0,        c_lat,       s_lat]])
    return R.T

def row_rotation(row):
    return rotation_ECEF_to_ENU(row['A']*pi/180.0, row['B']*pi/180).tolist()

df = pd.DataFrame(np.random.randn(6,4), columns=list('ABCD'))
# If I don't add the line below, I get what I want
df['E'] = pd.Timestamp('20130102')

df['M'] = df.apply(row_rotation, axis=1)
```

It works as I want if there is no datetime column in df:

```
          A         B         C         D  \
0  0.498796  0.209267 -0.551132 -0.066941   
1 -0.576161 -0.160802  0.895587 -0.355315   
2 -0.604028  0.959712 -1.388824 -0.356734   
3 -0.351012  0.242736  0.339068 -0.531425   
4 -1.355900  0.058885  1.458610  1.891502   
5 -0.560297  0.750459  1.288340  0.904650   

                                                   M  
0  [[-0.008705521998618918, 0.9999621062253967, 0...  
1  [[0.010055737814803208, 0.9999494397903326, 0....  
2  [[0.010542084651556644, 0.9999444306816251, 0....  
3  [[0.006126282255403784, 0.9999812341567851, 0....  
4  [[0.023662708414555287, 0.99971999891494, 0.0]...  
5  [[0.00977887456308289, 0.9999521856630343, 0.0...  
```
#### output of `pd.show_versions()`

```
INSTALLED VERSIONS
------------------
commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Linux
OS-release: 3.13.0-92-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8

pandas: 0.18.1
nose: 1.3.7
pip: 8.1.2
setuptools: 22.0.5
Cython: 0.24
numpy: 1.10.4
scipy: 0.17.1
statsmodels: None
xarray: None
IPython: 4.2.0
sphinx: 1.4.1
patsy: 0.4.1
dateutil: 2.5.3
pytz: 2016.4
blosc: None
bottleneck: 1.0.0
tables: 3.2.2
numexpr: 2.5.2
matplotlib: 1.5.1
openpyxl: 2.3.2
xlrd: 1.0.0
xlwt: 1.1.1
xlsxwriter: 0.8.9
lxml: 3.6.0
bs4: 4.4.1
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 1.0.13
pymysql: None
psycopg2: 2.6.1 (dt dec pq3 ext)
jinja2: 2.8
boto: 2.40.0
pandas_datareader: None
```
"
167115701,13758,Issue with Zero's Broadcasting down a column in Heterogeneous data columns ...,mmcky,closed,2016-07-22T19:31:27Z,2020-01-07T23:45:41Z,"I had a recent use case which produced some unexpected output and I have isolated the problem down to a much simpler case to replicate the behavior. 

Please see simplified code example below. 
#### Code Sample, a copy-pastable example if possible

``` python
import pandas as pd
import numpy as np
data = {
    'One' : pd.Series(['A', 1.2, np.nan]),
    'Two' : pd.Series([1.4, 3.2, 4.5])
}
df = pd.DataFrame(data)
```

Doing the sum across the first column (which in this case is redundant). [**Note:** In my case I was doing groupby operations where some columns were being summed and others were single columns as indexed by a level of a MultiIndex. The issue was that some of the columns ended up being full of 0's)

``` python
df[['One']].sum(axis=1)
```

produces an **unexpected** outcome:

``` ipython
0    0.0
1    0.0
2    0.0
dtype: float64
```

I think this might be an **error** and it produces a valid column of 0's that is of `dtype=float64`.
#### Expected Output

``` ipython
0      A
1    1.2
2    NaN
Name: One, dtype: object
```
#### output of `pd.show_versions()`
## INSTALLED VERSIONS

commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Linux
OS-release: 3.16.0-38-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.18.1
nose: 1.3.7
pip: 8.1.2
setuptools: 23.0.0
Cython: 0.24
numpy: 1.11.1
scipy: 0.17.1
statsmodels: 0.6.1
xarray: None
IPython: 4.2.0
sphinx: 1.4.1
patsy: 0.4.1
dateutil: 2.5.3
pytz: 2016.4
blosc: None
bottleneck: 1.1.0
tables: 3.2.2
numexpr: 2.6.0
matplotlib: 1.5.1
openpyxl: 2.3.2
xlrd: 1.0.0
xlwt: 1.1.2
xlsxwriter: 0.9.2
lxml: 3.6.0
bs4: 4.4.1
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 1.0.13
pymysql: None
psycopg2: None
jinja2: 2.8
boto: 2.40.0
pandas_datareader: None
"
155753871,13228,Using str() in .map() on floats gives string with higher precision than before,marcomayer,closed,2016-05-19T14:41:19Z,2020-01-07T23:45:41Z,"#### Code Sample, a copy-pastable example if possible

```

# In 0.17.1:
x = pd.Series(1/3)
x.map(lambda x: str(x)).to_dict()
{0: '0.333333333333'}

x.astype(str).map(lambda x: x).to_dict()
{0: '0.333333333333'}

# In 0.18.1:
x = pd.Series(1/3)
x.map(lambda x: str(x)).to_dict()
{0: '0.3333333333333333'}

x.astype(str).map(lambda x: x).to_dict()
{0: '0.333333333333'}

```
#### Expected Output

I'd expect the same output as in 0.17.x and before.

I do this a lot to convert floats to decimal.decimal with .map(lambda x: D(str(x))) which is slightly faster than using .astype(str).map(D).

This also messed up many of my unit-tests where I convert DFs to string dicts. Thanks to those I found this at all.

I checked the change docs but couldn't find something that points to why this should have changed.
#### output of `pd.show_versions()`
"
546093677,30769,TST: Add tests for fixed issues,mroeschke,closed,2020-01-07T05:50:58Z,2020-01-07T23:45:46Z,"- [x] closes #13230
- [x] closes #13820
- [x] closes #13758
- [x] closes #13228
- [x] closes #13208
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
546278570,30780,CLN: Fix FutureWarnings in the benchmarks,datapythonista,closed,2020-01-07T13:18:45Z,2020-01-08T01:24:20Z,"I see couple of `FutureWarning` in the benchmarks that can be fixed by simply updating the code to the proposed version.

```
·· /home/runner/work/pandas/pandas/asv_bench/benchmarks/algorithms.py:8: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.
     from pandas.util import testing as tm
   /home/runner/work/pandas/pandas/asv_bench/benchmarks/sparse.py:5: FutureWarning: The pandas.SparseArray class is deprecated and will be removed from pandas in a future version. Use pandas.arrays.SparseArray instead.
     from pandas import MultiIndex, Series, SparseArray, date_range
```
See for example: https://github.com/pandas-dev/pandas/pull/30746/checks#step:13:21"
546574342,30803,REF: PeriodIndex._union,jbrockmendel,closed,2020-01-08T00:13:06Z,2020-01-08T02:22:39Z,"Let's us get rid of PeriodIndex._wrap_setop_result, soon we'll share code among the PeriodIndex set ops, so this will be less verbose"
545317339,30691,ERR: Improve error message and doc for invalid labels in cut/qcut,ryankarlos,closed,2020-01-04T18:44:34Z,2020-01-08T03:26:09Z,"- [x] closes #13318
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] add whats new note

"
546460326,30792,IntegerArray.to_numpy,TomAugspurger,closed,2020-01-07T19:19:16Z,2020-01-08T04:37:13Z,"This implements IntegerArray.to_numpy with similar semantics to BooleanArray.to_numpy. The implementation is now identical between BooleanArray & IntegerArray. #30789 will merge them.

1. `.to_numpy(dtype=float/bool/int)` will raise if there are missing values
2. `.astype(float)` will convert NA to NaN.

I've made a slight change from the BooleanArray implementation on master, which I'll annotate inline.

Closes https://github.com/pandas-dev/pandas/issues/30038"
546263789,30779,ASV: use pandas.util.testing for back compat,jorisvandenbossche,closed,2020-01-07T12:44:08Z,2020-01-08T07:50:07Z,"cc @TomAugspurger I propose to keep the old deprecated imports (as long as they are not removed), so the benchmarks can still be run when eg doing a comparison of 0.25 with current master. 

"
546512660,30797,PERF: cache IntervalIndex._ndarray_values,jbrockmendel,closed,2020-01-07T21:18:19Z,2020-01-08T08:28:52Z,closes #30742
544442599,30615,DEPR: is_copy arg of take,ryankarlos,closed,2020-01-02T04:28:17Z,2020-01-08T09:54:00Z,"- [x] closes #27357
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
546573056,30802,CI: Fix spelling in requirements-dev generator script,mastersplinter,closed,2020-01-08T00:08:06Z,2020-01-08T10:22:08Z,"Fixes the spelling of _dependency_ in `script/generate_pip_deps_from_conda.py`.

This script creates `requirements-dev.txt` which contains the spelling mistake, so is also fixed (using the updated script)."
546307889,30783,pandas can not load Stata 16 data,shuai-zhou,closed,2020-01-07T14:16:37Z,2020-01-08T11:08:28Z,"#### Code Sample
```python
df = pd.read_stata('xx.dta')
```
#### Problem description

I was trying to use the above command to load Stata 16 data, but got an error saying
```python
Version of given Stata file is not 104, 105, 108, 111 (Stata 7SE), 113 (Stata 8/9), 114 (Stata 10/11), 115 (Stata 12), 117 (Stata 13), or 118 (Stata 14)
```
I updated pandas to version 0.25.1, the issue persists. How could I load Stata 16 data without degrading the dataset? Thanks.
"
546257825,30777,datatype converstion(.astype) error for bool type.,DSivaji,closed,2020-01-07T12:29:29Z,2020-01-08T11:10:35Z,"Datatype converstion(.astype) error for bool type.

```python
import pandas as pd
pd.__version__

df_1 = pd.DataFrame({'c_bool': [None, None, True, True, True,False]})
print(df_1['c_bool'].dtypes)
df_1['c_bool'] = df_1['c_bool'].astype(bool)
print(df_1['c_bool'].dtypes)
df_1

# '0.24.2'
# object
# bool
#   c_bool
# 0   False
# 1   False
# 2    True
# 3    True
# 4    True
# 5   False
```
I have a column that has some None/True/False values. Initially, the datatype of that column is 'object'. After I convert it to bool as datatype, Null values are converting to False. Which is not expected behavior."
546698930,30810,ASV: compatibility import for testing module,jorisvandenbossche,closed,2020-01-08T07:49:36Z,2020-01-08T14:06:42Z,"See https://github.com/pandas-dev/pandas/pull/30779, this avoids a warning in the benchmarks"
529029844,29874,CI: Building docs in GitHub actions,datapythonista,closed,2019-11-26T23:36:43Z,2020-01-08T15:47:14Z,"Building the documentation in GitHub actions.

In Azure-pipelines we publish the built docs to GitHub pages, served at dev.pandas.io.

After this is merged, I'll work on publishing the Actions docs to the new OVH server. Possibly, publishing the docs of every PR.
"
545471645,30715,IntervalArray equality follow-ups ,jschendel,closed,2020-01-05T21:50:47Z,2020-01-08T15:55:24Z,"Follow-ups to #30640 based on @jbrockmendel's comments.

Haven't addressed all the comments yet but pushing this up now so there's a record of it.

Changes thus far:
- Created `tests/arithmetic/test_interval.py ` and moved the tests there
- Used `make_wrapped_comparison_op` to add `__eq__` and `__ne__` to `IntervalArray`."
546584907,30805,Multi Phase JSON Initialization,WillAyd,closed,2020-01-08T00:53:34Z,2020-01-08T16:17:01Z,"Feature in Python 3.5 that should simplify instantiation of the JSON module and make it more ""pythonic""

https://docs.python.org/3/c-api/module.html?highlight=multi%20phase#multi-phase-initialization
https://www.python.org/dev/peps/pep-0489/

Also removed a version string from within the extension, as I don't see where that is useful
"
546874889,30813,[DOC] add example of rolling with win_type gaussian,MarcoGorelli,closed,2020-01-08T13:53:43Z,2020-01-08T16:47:30Z,"Admittedly this is not the first issue I address, but this one's been open for several months now and so I figured I'd take it

- [x] closes #26462
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
546594244,30807,CLN: remove Index __setstate__ methods,jbrockmendel,closed,2020-01-08T01:26:01Z,2020-01-08T18:01:46Z,"They are not hit in tests, AFAICT they are subsumed by `__reduce__` methods"
546571675,30801,REF: remove PeriodIndex._coerce_scalar_to_index,jbrockmendel,closed,2020-01-08T00:02:52Z,2020-01-08T18:07:47Z,"It is only used by Index.insert, but PeriodIndex now overrides insert."
545994343,30763,BUG: PeriodIndex.searchsorted accepting invalid inputs,jbrockmendel,closed,2020-01-06T23:29:22Z,2020-01-08T18:17:11Z,also a bug in `DataFrame.asof` with a PeriodIndex returning an incorrectly-named Series.
546570594,30800,REF: move astype to ExtensionIndex,jbrockmendel,closed,2020-01-07T23:58:34Z,2020-01-08T18:20:44Z,Broken off from #30717
546621429,30809,REF: move repeat to ExtensionIndex,jbrockmendel,closed,2020-01-08T03:17:45Z,2020-01-08T18:21:39Z,
546307265,30782,CLN: Removed outdated comment,ShaharNaveh,closed,2020-01-07T14:15:27Z,2020-01-08T20:28:00Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I think this is a bit outdated, since we are using black and it's doing the work for us, right?

according to git blame, this is 2 years old."
544390619,30603,Added 'pearson' to methods list in pandas/core/nanops.py,ShaharNaveh,closed,2020-01-01T19:32:52Z,2020-01-08T20:30:05Z,"- [x] ref https://github.com/pandas-dev/pandas/pull/30461#discussion_r362323260
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
545728920,30737,CI: Disallow bare pytest raise,ShaharNaveh,closed,2020-01-06T13:32:31Z,2020-01-08T20:31:01Z,"- [x] closes #23922
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
545405870,30708,CI: Test case for wrong placed space,ShaharNaveh,closed,2020-01-05T11:36:22Z,2020-01-08T20:32:56Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Test case for:
```python
foo = (
            ""bar""
            "" baz""
)
```"
277194118,18532,PERF: Regressions since v0.21,mroeschke,closed,2017-11-27T22:36:55Z,2020-01-08T22:22:00Z,"[xref](https://github.com/pandas-dev/pandas/pull/18503#issuecomment-347167230)

```
$ asv continuous -f 1.1 81372093f1fdc0c07e4b45ba0f47b upstream/master

+        54.0±9μs       1.40±0.01s 25895.28  indexing.IntervalIndexing.time_loc_list
+       65.6±20μs       1.39±0.02s 21250.19  indexing.IntervalIndexing.time_getitem_list
+     14.2±0.04μs      1.51±0.03ms   106.31  categoricals.CategoricalSlicing.time_getitem_bool_array('monotonic_decr')
+      35.6±0.5ms       1.99±0.01s    55.86  offset.ApplyIndex.time_apply_index(<BusinessDay>)
+      36.4±0.3ms       1.98±0.02s    54.20  offset.ApplyIndex.time_apply_index(<SemiMonthEnd: day_of_month=15>)
+      36.9±0.7ms          1.99±0s    53.86  offset.ApplyIndex.time_apply_index(<SemiMonthBegin: day_of_month=15>)
+         443±1ns       22.9±0.2μs    51.76  timestamp.TimestampProperties.time_weekday_name(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, 'B')
+         444±5ns       23.0±0.2μs    51.71  timestamp.TimestampProperties.time_weekday_name(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, None)
+      22.3±0.5ms       1.04±0.01s    46.47  period.DataFramePeriodColumn.time_setitem_period_column
+     4.65±0.02ms          203±2ms    43.80  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<BusinessDay>)
+     4.87±0.06ms          202±1ms    41.57  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<SemiMonthBegin: day_of_month=15>)
+     5.01±0.09ms        202±0.6ms    40.33  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<SemiMonthEnd: day_of_month=15>)
+     5.15±0.03ms        204±0.9ms    39.65  offset.OffsetSeriesArithmetic.time_add_offset(<BusinessDay>)
+      13.8±0.1ms          522±2ms    37.99  timeseries.Iteration.time_iter_preexit(<function period_range at 0x1124ecea0>)
+     5.45±0.02ms          206±2ms    37.74  offset.OffsetSeriesArithmetic.time_add_offset(<SemiMonthBegin: day_of_month=15>)
+     5.51±0.02ms        206±0.7ms    37.43  offset.OffsetSeriesArithmetic.time_add_offset(<SemiMonthEnd: day_of_month=15>)
+       374±0.9ns      12.4±0.03μs    33.08  indexing.MethodLookup.time_lookup_ix
+     3.42±0.03ms        104±0.7ms    30.46  period.PeriodIndexConstructor.time_from_pydatetime('D')
+     1.71±0.01ms         50.1±1ms    29.26  indexing.CategoricalIndexIndexing.time_get_indexer_list('monotonic_decr')
+         438±3ns      8.69±0.04μs    19.85  timestamp.TimestampProperties.time_weekday_name(None, 'B')
+     5.01±0.08ms         99.1±4ms    19.78  timeseries.DatetimeIndex.time_timeseries_is_month_start('tz_aware')
+         444±2ns      8.65±0.08μs    19.50  timestamp.TimestampProperties.time_weekday_name(None, None)
+     8.82±0.09ms          170±2ms    19.24  multiindex_object.Values.time_datetime_level_values_copy
+      7.02±0.2μs        113±0.9μs    16.07  period.Indexing.time_get_loc
+      60.9±0.8ms          655±3ms    10.75  plotting.TimeseriesPlotting.time_plot_regular
+      6.36±0.1μs       67.8±0.5μs    10.67  period.Indexing.time_shallow_copy
+     7.30±0.02ms         75.1±3ms    10.29  frame_methods.Repr.time_frame_repr_wide
+     7.20±0.07μs       60.3±0.6μs     8.38  index_object.Indexing.time_slice('Int')
+      22.0±0.1ms          183±3ms     8.32  binary_ops.Ops2.time_frame_float_floor_by_zero
+     7.16±0.05μs       59.5±0.5μs     8.30  index_object.Indexing.time_slice_step('Int')
+       113±0.4μs        841±300μs     7.44  groupby.GroupByMethods.time_dtype_as_field('int', 'cummin', 'direct')
+      72.0±0.2μs        525±0.9μs     7.28  groupby.GroupByMethods.time_dtype_as_field('float', 'cummin', 'direct')
+      73.0±0.6μs          524±2μs     7.18  groupby.GroupByMethods.time_dtype_as_field('float', 'cummin', 'transformation')
+      73.1±0.7μs          519±3μs     7.10  groupby.GroupByMethods.time_dtype_as_field('float', 'cummax', 'transformation')
+      18.0±0.2μs        127±0.5μs     7.06  period.PeriodUnaryMethods.time_now('M')
+       116±0.9μs        814±300μs     7.03  groupby.GroupByMethods.time_dtype_as_field('int', 'cummax', 'transformation')
+      73.3±0.6μs          514±3μs     7.00  groupby.GroupByMethods.time_dtype_as_field('float', 'cummax', 'direct')
+      77.2±0.4μs          506±4μs     6.56  groupby.GroupByMethods.time_dtype_as_field('datetime', 'cummin', 'direct')
+      77.4±0.5μs          503±4μs     6.50  groupby.GroupByMethods.time_dtype_as_field('datetime', 'cummin', 'transformation')
+      81.3±0.3μs          528±5μs     6.49  groupby.GroupByMethods.time_dtype_as_field('float', 'cumprod', 'direct')
+      80.9±0.3μs          525±5μs     6.49  groupby.GroupByMethods.time_dtype_as_field('float', 'cumprod', 'transformation')
+        86.0±5μs          527±5μs     6.13  groupby.GroupByMethods.time_dtype_as_field('float', 'cumsum', 'direct')
+        86.8±5μs          531±3μs     6.12  groupby.GroupByMethods.time_dtype_as_field('float', 'cumsum', 'transformation')
+      18.3±0.1ms          112±7ms     6.11  frame_methods.Dropna.time_dropna('any', 1)
+      18.0±0.2ms          106±3ms     5.91  frame_methods.Dropna.time_dropna('any', 0)
+      30.9±0.7μs          178±1μs     5.78  period.PeriodUnaryMethods.time_asfreq('min')
+      31.1±0.4μs        178±0.9μs     5.73  period.PeriodUnaryMethods.time_asfreq('M')
+         116±1μs        628±200μs     5.40  groupby.GroupByMethods.time_dtype_as_field('int', 'cummax', 'direct')
+       107±0.3μs          570±5μs     5.31  groupby.GroupByMethods.time_dtype_as_group('datetime', 'cummin', 'transformation')
+      64.8±0.1μs          341±4μs     5.27  period.PeriodProperties.time_property('M', 'end_time')
+         108±1μs          566±4μs     5.22  groupby.GroupByMethods.time_dtype_as_group('float', 'cummin', 'direct')
+         110±1μs          572±4μs     5.22  groupby.GroupByMethods.time_dtype_as_group('datetime', 'cummin', 'direct')
+       109±0.5μs          567±4μs     5.20  groupby.GroupByMethods.time_dtype_as_group('float', 'cummin', 'transformation')
+       109±0.4μs          567±4μs     5.19  groupby.GroupByMethods.time_dtype_as_group('float', 'cummax', 'direct')
+         109±1μs          565±2μs     5.17  groupby.GroupByMethods.time_dtype_as_group('float', 'cummax', 'transformation')
+      65.7±0.7μs          340±1μs     5.17  period.PeriodProperties.time_property('min', 'end_time')
+         116±1μs         591±90μs     5.10  groupby.GroupByMethods.time_dtype_as_field('int', 'cummin', 'transformation')
+         116±2μs         578±10μs     5.00  groupby.GroupByMethods.time_dtype_as_group('int', 'cummin', 'transformation')
+         114±1μs          569±9μs     4.99  groupby.GroupByMethods.time_dtype_as_group('int', 'cummax', 'direct')
+       114±0.6μs          566±3μs     4.95  groupby.GroupByMethods.time_dtype_as_group('int', 'cummin', 'direct')
+         115±2μs          565±4μs     4.90  groupby.GroupByMethods.time_dtype_as_group('int', 'cummax', 'transformation')
+      32.2±0.1ms          155±4ms     4.83  eval.Eval.time_and('python', 1)
+      3.54±0.1μs       16.7±0.1μs     4.71  indexing.DataFrameStringIndexing.time_ix
+         124±4μs          583±6μs     4.68  groupby.GroupByMethods.time_dtype_as_group('float', 'cumsum', 'transformation')
+         125±4μs         583±30μs     4.68  groupby.GroupByMethods.time_dtype_as_field('int', 'cumsum', 'transformation')
+         124±5μs          573±2μs     4.63  groupby.GroupByMethods.time_dtype_as_group('int', 'cumsum', 'direct')
+         123±5μs          569±5μs     4.63  groupby.GroupByMethods.time_dtype_as_field('int', 'cumsum', 'direct')
+         124±6μs          574±5μs     4.62  groupby.GroupByMethods.time_dtype_as_group('int', 'cumsum', 'transformation')
+         128±5μs          577±3μs     4.52  groupby.GroupByMethods.time_dtype_as_group('float', 'cumsum', 'direct')
+      38.2±0.4ms          160±2ms     4.18  eval.Eval.time_and('python', 'all')
+      59.4±0.5μs          232±1μs     3.91  period.PeriodUnaryMethods.time_to_timestamp('min')
+      60.2±0.3μs          233±2μs     3.87  period.PeriodUnaryMethods.time_to_timestamp('M')
+      60.6±0.8μs        234±0.9μs     3.86  period.PeriodProperties.time_property('min', 'start_time')
+      60.5±0.5μs          232±2μs     3.84  period.PeriodProperties.time_property('M', 'start_time')
+      40.6±0.2ms          153±9ms     3.76  frame_methods.Dropna.time_dropna('all', 1)
+      38.4±0.3ms          144±9ms     3.75  frame_methods.Dropna.time_dropna('all', 0)
+     3.18±0.01μs       11.7±0.2μs     3.66  multiindex_object.GetLoc.time_string_get_loc
+     3.12±0.01ms       11.3±0.1ms     3.62  multiindex_object.GetLoc.time_small_get_loc_warm
+         102±3ms          360±4ms     3.52  groupby.Groups.time_series_groups('int64_large')
+     3.19±0.02ms      10.8±0.08ms     3.40  multiindex_object.GetLoc.time_med_get_loc_warm
+     27.3±0.08ms         90.3±2ms     3.30  binary_ops.Ops.time_frame_multi_and(False, 1)
+      51.2±0.4μs        169±0.7μs     3.30  period.Indexing.time_unique
+     3.36±0.09μs       11.1±0.1μs     3.30  multiindex_object.GetLoc.time_med_get_loc
+     5.58±0.02ms         18.3±2ms     3.28  frame_methods.Equals.time_frame_nonunique_equal
+      27.4±0.2ms         89.2±2ms     3.25  binary_ops.Ops.time_frame_multi_and(False, 'default')
+      53.7±0.5μs          172±2μs     3.21  period.PeriodUnaryMethods.time_now('min')
+     5.58±0.04ms         17.8±2ms     3.19  frame_methods.Equals.time_frame_nonunique_unequal
+      84.9±0.6μs          267±1μs     3.14  period.Algorithms.time_drop_duplicates('index')
+       227±0.9μs          696±8μs     3.06  groupby.GroupByMethods.time_dtype_as_field('int', 'cumprod', 'direct')
+         230±1μs          692±3μs     3.01  groupby.GroupByMethods.time_dtype_as_field('int', 'cumprod', 'transformation')
+         142±2μs          426±5μs     3.00  period.Indexing.time_intersection
+       139±0.9μs          415±4μs     2.98  ctors.SeriesConstructors.time_series_constructor(<function SeriesConstructors.<lambda> at 0x1133397b8>, False)
+      31.2±0.1ms         92.9±1ms     2.98  binary_ops.Ops.time_frame_multi_and(True, 1)
+       139±0.7μs          412±4μs     2.97  ctors.SeriesConstructors.time_series_constructor(<function SeriesConstructors.<lambda> at 0x1133396a8>, False)
+       139±0.7μs          410±3μs     2.96  ctors.SeriesConstructors.time_series_constructor(<function SeriesConstructors.<lambda> at 0x11332b730>, False)
+       246±0.8μs          723±2μs     2.94  groupby.GroupByMethods.time_dtype_as_group('float', 'cumprod', 'transformation')
+       151±0.4μs          442±5μs     2.93  ctors.SeriesConstructors.time_series_constructor(<function SeriesConstructors.<lambda> at 0x1133397b8>, True)
+       106±0.8μs        310±0.3μs     2.93  period.PeriodIndexConstructor.time_from_date_range('D')
+         150±1μs          437±5μs     2.92  ctors.SeriesConstructors.time_series_constructor(<function SeriesConstructors.<lambda> at 0x1133396a8>, True)
+       248±0.9μs          723±4μs     2.92  groupby.GroupByMethods.time_dtype_as_group('float', 'cumprod', 'direct')
+       150±0.7μs          437±3μs     2.91  ctors.SeriesConstructors.time_series_constructor(<function SeriesConstructors.<lambda> at 0x11332b730>, True)
+       139±0.6μs          403±3μs     2.90  ctors.SeriesConstructors.time_series_constructor(<function SeriesConstructors.<lambda> at 0x113339840>, False)
+     9.12±0.08μs       26.5±0.2μs     2.90  timestamp.TimestampProperties.time_is_quarter_end(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, 'B')
+         151±2μs          435±4μs     2.89  ctors.SeriesConstructors.time_series_constructor(<function SeriesConstructors.<lambda> at 0x113339840>, True)
+     9.30±0.09μs       26.9±0.7μs     2.89  timestamp.TimestampProperties.time_is_year_start(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, 'B')
+         249±2μs          718±6μs     2.88  groupby.GroupByMethods.time_dtype_as_group('int', 'cumprod', 'direct')
+         251±5μs          721±3μs     2.87  groupby.GroupByMethods.time_dtype_as_group('int', 'cumprod', 'transformation')
+      9.24±0.1μs       26.3±0.1μs     2.85  timestamp.TimestampProperties.time_is_year_end(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, 'B')
+         142±5ms          404±7ms     2.84  groupby.Groups.time_series_groups('object_large')
+      21.1±0.1ms         59.9±2ms     2.83  groupby.ApplyDictReturn.time_groupby_apply_dict_return
+      9.24±0.1μs       26.1±0.2μs     2.82  timestamp.TimestampProperties.time_is_month_start(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, 'B')
+      24.6±0.1μs       69.1±0.8μs     2.80  indexing.NumericSeriesIndexing.time_iloc_list_like(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'nonunique_monotonic_inc')
+      9.28±0.1μs       25.9±0.1μs     2.79  timestamp.TimestampProperties.time_is_quarter_start(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, 'B')
+     24.6±0.08μs       68.7±0.1μs     2.79  indexing.NumericSeriesIndexing.time_iloc_list_like(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'unique_monotonic_inc')
+      53.8±0.2μs        149±0.8μs     2.78  period.Indexing.time_series_loc
+        11.3±2ms       31.3±0.1ms     2.77  io.msgpack.MSGPack.time_read_msgpack
+      9.53±0.2μs       26.3±0.2μs     2.76  timestamp.TimestampProperties.time_is_leap_year(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, 'B')
+     6.58±0.04μs       18.1±0.6μs     2.75  timestamp.TimestampAcrossDst.time_replace_across_dst
+      25.0±0.1μs         68.8±1μs     2.75  indexing.NumericSeriesIndexing.time_iloc_list_like(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
+      36.5±0.2ms         99.3±1ms     2.72  binary_ops.Ops.time_frame_multi_and(True, 'default')
+         725±3μs      1.95±0.02ms     2.68  io.csv.ReadCSVParseDates.time_multiple_date
+      25.7±0.2μs       68.9±0.3μs     2.68  indexing.NumericSeriesIndexing.time_iloc_list_like(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
+         330±2μs          882±2μs     2.68  period.Algorithms.time_value_counts('index')
+     9.77±0.05μs       25.9±0.2μs     2.65  timestamp.TimestampProperties.time_is_month_end(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, 'B')
+      36.8±0.1ms        97.0±10ms     2.64  frame_methods.Interpolate.time_interpolate(None)
+        44.2±1ms          116±3ms     2.62  join_merge.MergeAsof.time_by_int
+      54.2±0.3μs          140±2μs     2.58  indexing.NumericSeriesIndexing.time_ix_slice(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
+       120±0.6ms          307±2ms     2.56  groupby.GroupByMethods.time_dtype_as_group('float', 'unique', 'transformation')
+      52.0±0.2ms          133±1ms     2.55  groupby.GroupByMethods.time_dtype_as_field('int', 'unique', 'direct')
+      76.5±0.7ms          195±2ms     2.55  groupby.GroupByMethods.time_dtype_as_group('int', 'unique', 'direct')
+       120±0.6ms          306±3ms     2.54  groupby.GroupByMethods.time_dtype_as_group('float', 'unique', 'direct')
+      77.2±0.4ms          196±3ms     2.54  groupby.GroupByMethods.time_dtype_as_group('int', 'unique', 'transformation')
+      53.8±0.4ms          137±3ms     2.54  groupby.GroupByMethods.time_dtype_as_field('float', 'unique', 'direct')
+      39.2±0.3μs       99.2±0.4μs     2.53  indexing.NumericSeriesIndexing.time_ix_slice(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
+      53.2±0.3ms          133±2ms     2.51  groupby.GroupByMethods.time_dtype_as_field('int', 'unique', 'transformation')
+        93.8±3ms        235±0.7ms     2.50  reshape.WideToLong.time_wide_to_long_big
+     7.76±0.04μs       19.4±0.1μs     2.49  timestamp.TimestampOps.time_replace_tz(None)
+       123±0.7ms          306±2ms     2.49  groupby.GroupByMethods.time_dtype_as_group('datetime', 'unique', 'transformation')
+      54.3±0.1ms          135±1ms     2.49  groupby.GroupByMethods.time_dtype_as_field('float', 'unique', 'transformation')
+         170±2μs          418±4μs     2.47  indexing.NonNumericSeriesIndexing.time_getitem_label_slice('datetime', 'nonunique_monotonic_inc')
+         125±1ms          305±2ms     2.45  groupby.GroupByMethods.time_dtype_as_group('datetime', 'unique', 'direct')
+      25.1±0.2μs       61.1±0.2μs     2.43  indexing.NumericSeriesIndexing.time_iloc_slice(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'unique_monotonic_inc')
+        1.90±0ms      4.56±0.03ms     2.40  binary_ops.Timeseries.time_timestamp_series_compare(None)
+     1.90±0.01ms      4.54±0.05ms     2.39  binary_ops.Timeseries.time_series_timestamp_compare(None)
+      25.6±0.5μs       60.8±0.2μs     2.38  indexing.NumericSeriesIndexing.time_iloc_slice(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'nonunique_monotonic_inc')
+      25.6±0.1μs       60.4±0.2μs     2.36  indexing.NumericSeriesIndexing.time_iloc_slice(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
+      25.7±0.1μs       60.2±0.6μs     2.34  indexing.NumericSeriesIndexing.time_iloc_slice(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
+      8.03±0.1μs       18.8±0.2μs     2.34  ctors.SeriesDtypesConstructors.time_dtindex_from_series
+        844±10ms       1.97±0.03s     2.34  stat_ops.FrameMultiIndexOps.time_op([0, 1], 'mad')
+      13.1±0.2μs       30.4±0.3μs     2.33  timestamp.TimestampOps.time_replace_tz('US/Eastern')
+      68.1±0.5ms        157±0.6ms     2.30  groupby.GroupByMethods.time_dtype_as_field('object', 'unique', 'direct')
+      68.4±0.5ms          157±1ms     2.30  groupby.GroupByMethods.time_dtype_as_field('object', 'unique', 'transformation')
+      66.2±0.8ms          149±1ms     2.25  groupby.GroupByMethods.time_dtype_as_field('datetime', 'unique', 'direct')
+      33.9±0.2μs       75.9±0.2μs     2.24  indexing.NumericSeriesIndexing.time_getitem_slice(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'unique_monotonic_inc')
+         171±3μs        381±0.8μs     2.23  multiindex_object.Values.time_datetime_level_values_sliced
+      65.8±0.6ms        145±0.5ms     2.21  groupby.GroupByMethods.time_dtype_as_field('datetime', 'unique', 'transformation')
+        98.4±1ms          217±2ms     2.20  stat_ops.FrameMultiIndexOps.time_op(1, 'mad')
+      34.7±0.2μs         76.3±1μs     2.20  indexing.NumericSeriesIndexing.time_getitem_slice(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'nonunique_monotonic_inc')
+         745±5μs      1.64±0.01ms     2.20  io.csv.ReadCSVParseDates.time_baseline
+      48.4±0.6μs          106±2μs     2.18  indexing.NumericSeriesIndexing.time_loc_slice(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
+      34.6±0.2μs       75.2±0.2μs     2.18  indexing.NumericSeriesIndexing.time_getitem_slice(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
+      34.8±0.3μs       75.6±0.7μs     2.17  indexing.NumericSeriesIndexing.time_getitem_slice(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
+         816±8μs      1.77±0.02ms     2.17  indexing.NonNumericSeriesIndexing.time_getitem_list_like('datetime', 'unique_monotonic_inc')
+         494±5μs      1.06±0.01ms     2.15  groupby.GroupByMethods.time_dtype_as_group('object', 'unique', 'transformation')
+        500±10μs      1.07±0.01ms     2.14  groupby.GroupByMethods.time_dtype_as_group('object', 'unique', 'direct')
+      36.7±0.1μs       78.0±0.5μs     2.12  indexing.NumericSeriesIndexing.time_ix_scalar(<class 'pandas.core.indexes.numeric.Float64Index'>, 'nonunique_monotonic_inc')
+      49.5±0.9μs          105±1μs     2.11  timeseries.AsOf.time_asof_single_early('DataFrame')
+      61.7±0.2ms          130±7ms     2.11  frame_methods.Interpolate.time_interpolate('infer')
+         880±6μs       1.84±0.1ms     2.09  frame_methods.Interpolate.time_interpolate_some_good(None)
+      43.1±0.3μs         89.6±1μs     2.08  indexing.NumericSeriesIndexing.time_iloc_array(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'nonunique_monotonic_inc')
+      36.4±0.2μs         75.4±2μs     2.07  indexing.NumericSeriesIndexing.time_loc_slice(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
+      43.0±0.2μs       88.2±0.3μs     2.05  indexing.NumericSeriesIndexing.time_iloc_array(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'unique_monotonic_inc')
+      43.9±0.3μs       88.6±0.4μs     2.02  indexing.NumericSeriesIndexing.time_iloc_array(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
+      28.4±0.5μs       57.0±0.6μs     2.01  indexing.NumericSeriesIndexing.time_ix_scalar(<class 'pandas.core.indexes.numeric.Float64Index'>, 'unique_monotonic_inc')
+      44.0±0.3μs       88.2±0.6μs     2.00  indexing.NumericSeriesIndexing.time_iloc_array(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
+      1.64±0.02s       3.28±0.08s     2.00  sparse.SparseDataFrameConstructor.time_constructor
+      80.4±0.9ms        160±0.6ms     1.99  join_merge.MergeAsof.time_by_object
+      32.5±0.2μs         64.6±1μs     1.98  indexing.NumericSeriesIndexing.time_ix_scalar(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
+      47.8±0.2μs       93.6±0.9μs     1.96  groupby.GroupByMethods.time_dtype_as_group('float', 'size', 'transformation')
+      47.8±0.7μs         93.5±1μs     1.96  groupby.GroupByMethods.time_dtype_as_field('int', 'size', 'transformation')
+      47.7±0.6μs       93.3±0.2μs     1.96  groupby.GroupByMethods.time_dtype_as_group('datetime', 'size', 'transformation')
+      8.76±0.1ms       17.0±0.2ms     1.94  frame_methods.Repr.time_repr_tall
+      1.19±0.01s       2.30±0.01s     1.94  timeseries.ToDatetimeNONISO8601.time_different_offset
+     1.66±0.01ms      3.23±0.01ms     1.94  reshape.SimpleReshape.time_stack
+      47.9±0.2μs       92.4±0.9μs     1.93  groupby.GroupByMethods.time_dtype_as_field('float', 'size', 'direct')
+      47.7±0.3μs         92.2±1μs     1.93  groupby.GroupByMethods.time_dtype_as_field('object', 'size', 'direct')
+      47.9±0.3μs         92.4±1μs     1.93  groupby.GroupByMethods.time_dtype_as_field('datetime', 'size', 'direct')
+      47.6±0.1μs       91.9±0.5μs     1.93  groupby.GroupByMethods.time_dtype_as_field('int', 'size', 'direct')
+      47.5±0.2μs       91.6±0.5μs     1.93  groupby.GroupByMethods.time_dtype_as_field('float', 'size', 'transformation')
+      48.0±0.5μs       92.2±0.8μs     1.92  groupby.GroupByMethods.time_dtype_as_group('float', 'size', 'direct')
+      48.0±0.3μs       92.2±0.5μs     1.92  groupby.GroupByMethods.time_dtype_as_group('int', 'size', 'direct')
+      47.9±0.6μs       91.7±0.5μs     1.92  groupby.GroupByMethods.time_dtype_as_field('datetime', 'size', 'transformation')
+      48.3±0.2μs       92.5±0.4μs     1.91  groupby.GroupByMethods.time_dtype_as_group('datetime', 'size', 'direct')
+      48.1±0.2μs       92.0±0.9μs     1.91  groupby.GroupByMethods.time_dtype_as_group('int', 'size', 'transformation')
+      48.3±0.2μs       91.8±0.7μs     1.90  groupby.GroupByMethods.time_dtype_as_group('object', 'size', 'transformation')
+      82.2±0.5μs          156±2μs     1.90  indexing.CategoricalIndexIndexing.time_getitem_bool_array('monotonic_decr')
+      48.1±0.3μs       91.4±0.5μs     1.90  groupby.GroupByMethods.time_dtype_as_field('object', 'size', 'transformation')
+      63.0±0.4μs          119±5μs     1.90  groupby.GroupByMethods.time_dtype_as_field('int', 'count', 'direct')
+      63.2±0.8μs          119±1μs     1.88  groupby.GroupByMethods.time_dtype_as_field('int', 'count', 'transformation')
+       230±0.8ms          432±3ms     1.87  groupby.Transform.time_transform_lambda_max
+      62.7±0.8μs          117±1μs     1.86  groupby.GroupByMethods.time_dtype_as_group('int', 'count', 'transformation')
+      62.1±0.5μs        115±0.6μs     1.86  groupby.GroupByMethods.time_dtype_as_field('float', 'count', 'transformation')
+      61.8±0.2μs        115±0.8μs     1.85  groupby.GroupByMethods.time_dtype_as_field('float', 'count', 'direct')
+      63.4±0.6μs        118±0.5μs     1.85  groupby.GroupByMethods.time_dtype_as_group('datetime', 'count', 'direct')
+      63.6±0.1μs          118±1μs     1.85  groupby.GroupByMethods.time_dtype_as_group('datetime', 'count', 'transformation')
+      25.7±0.2μs       47.5±0.4μs     1.85  indexing.NumericSeriesIndexing.time_ix_scalar(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
+      63.0±0.6μs          116±1μs     1.85  groupby.GroupByMethods.time_dtype_as_group('int', 'count', 'direct')
+     13.6±0.08μs       25.1±0.4μs     1.85  ctors.SeriesDtypesConstructors.time_index_from_array_floats
+     2.92±0.02ms         5.39±1ms     1.85  gil.ParallelRolling.time_rolling('var')
+      49.3±0.3μs       91.0±0.6μs     1.85  groupby.GroupByMethods.time_dtype_as_group('object', 'size', 'direct')
+      63.8±0.4μs        117±0.7μs     1.84  groupby.GroupByMethods.time_dtype_as_group('float', 'count', 'transformation')
+        63.5±1μs          116±3μs     1.83  groupby.GroupByMethods.time_dtype_as_group('float', 'count', 'direct')
+      95.6±0.9μs          174±6μs     1.83  frame_methods.GetDtypeCounts.time_frame_get_dtype_counts
+         972±3μs      1.77±0.01ms     1.82  io.csv.ReadCSVDInferDatetimeFormat.time_read_csv(False, 'ymd')
+      61.7±0.4μs        112±0.7μs     1.82  groupby.GroupByMethods.time_dtype_as_field('datetime', 'count', 'transformation')
+      63.4±0.7μs        115±0.3μs     1.82  groupby.GroupByMethods.time_dtype_as_group('object', 'count', 'transformation')
+      61.6±0.4μs        112±0.8μs     1.81  groupby.GroupByMethods.time_dtype_as_field('datetime', 'count', 'direct')
+      64.4±0.6μs          115±1μs     1.79  groupby.GroupByMethods.time_dtype_as_group('object', 'count', 'direct')
+        29.1±2ms         51.9±1ms     1.78  binary_ops.Ops.time_frame_comparison(False, 1)
+        28.7±2ms       51.1±0.6ms     1.78  binary_ops.Ops.time_frame_comparison(False, 'default')
+     18.6±0.09μs       33.0±0.4μs     1.77  ctors.SeriesDtypesConstructors.time_dtindex_from_index_with_series
+     2.48±0.03ms      4.40±0.02ms     1.77  reindex.DropDuplicates.time_frame_drop_dups_bool(True)
+        1.08±0ms      1.90±0.02ms     1.76  io.csv.ReadCSVDInferDatetimeFormat.time_read_csv(False, 'iso8601')
+      14.8±0.1μs       26.2±0.2μs     1.76  inference.ToNumeric.time_from_float('ignore')
+         378±3μs         660±10μs     1.74  indexing.NumericSeriesIndexing.time_getitem_list_like(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
+      15.0±0.1μs       25.9±0.4μs     1.73  inference.ToNumeric.time_from_float('coerce')
+     2.00±0.01ms       3.45±0.1ms     1.73  frame_methods.Interpolate.time_interpolate_some_good('infer')
+     3.44±0.01μs      5.94±0.02μs     1.72  inference.ToNumericDowncast.time_downcast('int32', None)
+      29.2±0.2μs       50.3±0.7μs     1.72  ctors.SeriesConstructors.time_series_constructor(<function SeriesConstructors.<lambda> at 0x11332b7b8>, True)
+      84.8±0.4μs          145±1μs     1.71  indexing.NumericSeriesIndexing.time_ix_slice(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'unique_monotonic_inc')
+      19.9±0.3μs       33.9±0.2μs     1.71  ctors.SeriesDtypesConstructors.time_index_from_array_string
+         116±1ms          197±9ms     1.70  frame_methods.Iteration.time_iterrows
+       154±0.2μs          260±5μs     1.69  ctors.SeriesConstructors.time_series_constructor(<function SeriesConstructors.<lambda> at 0x11332b840>, True)
+         178±1ms          301±5ms     1.69  sparse.SparseDataFrameConstructor.time_from_scipy
+         254±1μs          427±5μs     1.68  indexing.NumericSeriesIndexing.time_ix_list_like(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'unique_monotonic_inc')
+     1.00±0.01ms      1.69±0.01ms     1.68  timeseries.ResampleDataFrame.time_method('max')
+      80.1±0.9μs          134±2μs     1.67  indexing.NonNumericSeriesIndexing.time_getitem_pos_slice('datetime', 'nonunique_monotonic_inc')
+      70.6±0.6μs          117±1μs     1.66  indexing.NumericSeriesIndexing.time_ix_slice(<class 'pandas.core.indexes.numeric.Float64Index'>, 'nonunique_monotonic_inc')
+     3.02±0.02ms      5.02±0.01ms     1.66  reindex.DropDuplicates.time_frame_drop_dups_bool(False)
+         196±2ns          324±2ns     1.65  multiindex_object.Integer.time_is_monotonic
+     1.02±0.01ms      1.69±0.03ms     1.65  timeseries.ResampleDataFrame.time_method('min')
+         142±3ms          234±2ms     1.65  indexing.NumericSeriesIndexing.time_getitem_lists(<class 'pandas.core.indexes.numeric.Float64Index'>, 'unique_monotonic_inc')
+         140±2ms          231±3ms     1.65  indexing.NumericSeriesIndexing.time_loc_array(<class 'pandas.core.indexes.numeric.Float64Index'>, 'unique_monotonic_inc')
+         123±1μs         203±70μs     1.65  groupby.GroupByMethods.time_dtype_as_field('int', 'first', 'transformation')
+      24.9±0.2μs       40.9±0.5μs     1.64  indexing.NumericSeriesIndexing.time_iloc_list_like(<class 'pandas.core.indexes.numeric.Float64Index'>, 'nonunique_monotonic_inc')
+      72.4±0.3μs          119±2μs     1.64  series_methods.SeriesConstructor.time_constructor(None)
+         241±1μs          395±4μs     1.64  indexing.NumericSeriesIndexing.time_ix_list_like(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
+         141±3ms          231±3ms     1.64  indexing.NumericSeriesIndexing.time_getitem_array(<class 'pandas.core.indexes.numeric.Float64Index'>, 'unique_monotonic_inc')
+     1.71±0.04ms      2.79±0.01ms     1.63  reshape.Melt.time_melt_dataframe
+     3.36±0.02μs      5.47±0.02μs     1.63  offset.OnOffset.time_on_offset(<MonthBegin>)
+       140±0.6ms          228±2ms     1.63  indexing.NumericSeriesIndexing.time_loc_list_like(<class 'pandas.core.indexes.numeric.Float64Index'>, 'unique_monotonic_inc')
+       143±0.8μs          231±3μs     1.62  ctors.SeriesConstructors.time_series_constructor(<function SeriesConstructors.<lambda> at 0x11332b840>, False)
+       141±0.9ms          229±3ms     1.62  indexing.NumericSeriesIndexing.time_getitem_list_like(<class 'pandas.core.indexes.numeric.Float64Index'>, 'unique_monotonic_inc')
+     2.90±0.01μs       4.68±0.2μs     1.62  categoricals.CategoricalSlicing.time_getitem_scalar('non_monotonic')
+         107±1μs          173±2μs     1.61  timeseries.DatetimeIndex.time_unique('dst')
+       123±0.9ms          198±3ms     1.61  groupby.GroupByMethods.time_dtype_as_group('int', 'skew', 'transformation')
+     10.3±0.03μs       16.5±0.2μs     1.61  offset.OffestDatetimeArithmetic.time_apply(<DateOffset: days=2, months=2>)
+         191±2ms          306±1ms     1.60  groupby.GroupByMethods.time_dtype_as_group('float', 'skew', 'direct')
+      24.9±0.2μs       39.9±0.2μs     1.60  indexing.NumericSeriesIndexing.time_iloc_list_like(<class 'pandas.core.indexes.numeric.Float64Index'>, 'unique_monotonic_inc')
+       124±0.4ms          198±1ms     1.59  groupby.GroupByMethods.time_dtype_as_group('int', 'skew', 'direct')
+      9.81±0.1ms       15.6±0.7ms     1.59  eval.Query.time_query_datetime_column
+      73.9±0.9ms        117±0.8ms     1.59  sparse.SparseDataFrameConstructor.time_from_dict
+       192±0.8ms          304±4ms     1.59  groupby.GroupByMethods.time_dtype_as_group('float', 'skew', 'transformation')
+      40.9±0.5μs       64.9±0.3μs     1.58  timeseries.SortIndex.time_get_slice(False)
+     4.25±0.03ms      6.72±0.03ms     1.58  categoricals.Rank.time_rank_int
+      81.2±0.2ms        128±0.9ms     1.57  groupby.GroupByMethods.time_dtype_as_field('float', 'skew', 'transformation')
+     1.43±0.01ms      2.26±0.01ms     1.57  io.csv.ReadCSVDInferDatetimeFormat.time_read_csv(True, 'iso8601')
+      51.6±0.5μs       81.1±0.8μs     1.57  indexing.NumericSeriesIndexing.time_ix_slice(<class 'pandas.core.indexes.numeric.Float64Index'>, 'unique_monotonic_inc')
+         448±3μs          703±9μs     1.57  indexing.MultiIndexing.time_series_ix
+        1.40±0ms      2.19±0.01ms     1.57  io.csv.ReadCSVDInferDatetimeFormat.time_read_csv(True, 'ymd')
+         227±2μs         354±20μs     1.56  frame_ctor.FromRecords.time_frame_from_records_generator(1000)
+        82.1±1ms          128±1ms     1.56  groupby.GroupByMethods.time_dtype_as_field('float', 'skew', 'direct')
+     3.20±0.01ms       4.99±0.2ms     1.56  frame_methods.Apply.time_apply_pass_thru
+      85.1±0.6ms        132±0.7ms     1.56  groupby.GroupByMethods.time_dtype_as_field('int', 'skew', 'direct')
+      85.1±0.6ms          132±1ms     1.55  groupby.GroupByMethods.time_dtype_as_field('int', 'skew', 'transformation')
+       111±0.9μs          171±2μs     1.55  indexing.DataFrameNumericIndexing.time_iloc_dups
+     4.41±0.05ms      6.82±0.03ms     1.55  categoricals.Rank.time_rank_int_cat_ordered
+     4.44±0.04ms      6.82±0.06ms     1.54  categoricals.Rank.time_rank_string_cat_ordered
+     3.86±0.02ms         5.93±2ms     1.53  gil.ParallelRolling.time_rolling('skew')
+        83.1±1ms          127±9ms     1.53  frame_methods.Apply.time_apply_axis_1
+     4.58±0.05ms      6.99±0.07ms     1.53  categoricals.Rank.time_rank_int_cat
+      6.00±0.1μs      9.17±0.04μs     1.53  timestamp.TimestampOps.time_replace_None('US/Eastern')
+        83.5±2μs        127±0.6μs     1.53  groupby.GroupByMethods.time_dtype_as_field('float', 'first', 'direct')
+         111±2ms          169±4ms     1.52  sparse.SparseSeriesToFrame.time_series_to_frame
+      22.9±0.2ms         34.7±1ms     1.52  frame_methods.Equals.time_frame_object_unequal
+     6.12±0.07ms       9.28±0.4ms     1.52  frame_methods.Apply.time_apply_lambda_mean
+        82.2±1μs          124±4μs     1.51  indexing.NumericSeriesIndexing.time_loc_slice(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'unique_monotonic_inc')
+      83.3±0.3μs          126±1μs     1.51  groupby.GroupByMethods.time_dtype_as_field('float', 'first', 'transformation')
+     2.33±0.01ms      3.52±0.05ms     1.51  rolling.VariableWindowMethods.time_rolling('DataFrame', '50s', 'float', 'max')
+         287±2ms         431±10ms     1.50  groupby.GroupByMethods.time_dtype_as_field('int', 'mad', 'direct')
+     2.37±0.03ms      3.57±0.04ms     1.50  rolling.VariableWindowMethods.time_rolling('DataFrame', '50s', 'int', 'max')
+     2.70±0.03ms      4.06±0.04ms     1.50  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'std')
+         652±4ms          978±5ms     1.50  groupby.GroupByMethods.time_dtype_as_group('float', 'mad', 'transformation')
+     2.80±0.05ms      4.19±0.02ms     1.50  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'std')
+      23.8±0.1ms         35.6±9ms     1.50  gil.ParallelFactorize.time_loop(2)
+     2.79±0.01ms      4.18±0.01ms     1.50  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'std')
+        529±10μs          793±7μs     1.50  indexing.MultiIndexing.time_frame_ix
+         159±1ms          238±1ms     1.50  timeseries.ToDatetimeISO8601.time_iso8601_tz_spaceformat
+     2.74±0.03ms      4.10±0.03ms     1.50  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'std')
+     2.35±0.01ms      3.50±0.02ms     1.49  rolling.VariableWindowMethods.time_rolling('DataFrame', '50s', 'float', 'min')
+        85.2±4μs          127±1μs     1.49  groupby.GroupByMethods.time_dtype_as_field('float', 'max', 'direct')
+         287±2ms         427±10ms     1.49  groupby.GroupByMethods.time_dtype_as_field('int', 'mad', 'transformation')
+        87.0±4μs          129±2μs     1.48  groupby.GroupByMethods.time_dtype_as_field('float', 'min', 'transformation')
+      58.3±0.1μs        86.4±10μs     1.48  frame_methods.Dtypes.time_frame_dtypes
+        87.9±4μs          130±1μs     1.48  groupby.GroupByMethods.time_dtype_as_field('float', 'min', 'direct')
+        87.8±4μs          130±1μs     1.48  groupby.GroupByMethods.time_dtype_as_field('float', 'max', 'transformation')
+     2.40±0.01ms      3.54±0.02ms     1.48  rolling.VariableWindowMethods.time_rolling('DataFrame', '50s', 'int', 'min')
+         324±1μs          478±1μs     1.48  groupby.GroupByMethods.time_dtype_as_field('float', 'sem', 'direct')
+       283±0.8ms          417±3ms     1.47  groupby.GroupByMethods.time_dtype_as_field('float', 'mad', 'direct')
+         663±2ms          978±5ms     1.47  groupby.GroupByMethods.time_dtype_as_group('float', 'mad', 'direct')
+         282±2ms          415±2ms     1.47  groupby.GroupByMethods.time_dtype_as_field('float', 'mad', 'transformation')
+         424±2ms          621±3ms     1.47  groupby.GroupByMethods.time_dtype_as_group('int', 'mad', 'direct')
+        84.4±5μs        124±0.5μs     1.47  groupby.GroupByMethods.time_dtype_as_field('float', 'prod', 'transformation')
+         176±1μs          258±3μs     1.47  groupby.GroupByMethods.time_dtype_as_field('object', 'last', 'direct')
+         425±4ms          621±4ms     1.46  groupby.GroupByMethods.time_dtype_as_group('int', 'mad', 'transformation')
+         325±5μs          476±8μs     1.46  groupby.GroupByMethods.time_dtype_as_field('float', 'sem', 'transformation')
+        84.2±4μs        123±0.6μs     1.46  groupby.GroupByMethods.time_dtype_as_field('float', 'prod', 'direct')
+      81.1±0.7μs          118±1μs     1.46  groupby.GroupByMethods.time_dtype_as_field('float', 'last', 'transformation')
+      25.1±0.3ms       36.5±0.8ms     1.46  strings.Methods.time_get
+       284±0.8μs          412±3μs     1.45  multiindex_object.Duplicates.time_remove_unused_levels
+      81.4±0.3μs          118±1μs     1.45  groupby.GroupByMethods.time_dtype_as_field('float', 'last', 'direct')
+         333±2μs          482±2μs     1.45  indexing.NumericSeriesIndexing.time_loc_list_like(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
+         177±1μs          256±3μs     1.45  groupby.GroupByMethods.time_dtype_as_field('object', 'last', 'transformation')
+         708±4μs      1.02±0.01ms     1.45  timeseries.ResampleDataFrame.time_method('mean')
+     1.08±0.01ms      1.56±0.02ms     1.45  sparse.FromCoo.time_sparse_series_from_coo
+       121±0.5μs          174±1μs     1.44  groupby.GroupByMethods.time_dtype_as_group('datetime', 'min', 'direct')
+        85.1±5μs        122±0.9μs     1.44  groupby.GroupByMethods.time_dtype_as_field('float', 'sum', 'transformation')
+         182±2μs        262±0.5μs     1.44  groupby.GroupByMethods.time_dtype_as_field('object', 'first', 'direct')
+         742±4μs      1.07±0.01ms     1.44  indexing.PanelIndexing.time_subset
+       112±0.3μs          161±8μs     1.43  groupby.GroupByMethods.time_dtype_as_field('int', 'last', 'direct')
+      9.18±0.1μs       13.2±0.1μs     1.43  timestamp.TimestampConstruction.time_parse_iso8601_tz
+       125±0.1μs        179±0.8μs     1.43  groupby.GroupByMethods.time_dtype_as_field('float', 'std', 'transformation')
+         107±1μs        154±0.7μs     1.43  groupby.GroupByMethods.time_dtype_as_group('object', 'last', 'transformation')
+       182±0.6μs          259±1μs     1.43  groupby.GroupByMethods.time_dtype_as_field('object', 'first', 'transformation')
+         376±2μs          537±4μs     1.43  groupby.GroupByMethods.time_dtype_as_group('float', 'sem', 'direct')
+     1.57±0.02ms      2.24±0.05ms     1.43  io.csv.ReadCSVFloatPrecision.time_read_csv(';', '_', None)
+     2.38±0.01ms      3.39±0.04ms     1.43  categoricals.Concat.time_union
+        91.2±5μs        130±0.6μs     1.42  groupby.GroupByMethods.time_dtype_as_field('float', 'var', 'direct')
+        85.4±5μs        121±0.9μs     1.42  groupby.GroupByMethods.time_dtype_as_field('float', 'sum', 'direct')
+     6.63±0.03ms       9.42±0.5ms     1.42  frame_methods.Apply.time_apply_np_mean
+       108±0.5μs        153±0.4μs     1.42  groupby.GroupByMethods.time_dtype_as_group('object', 'last', 'direct')
+        1.58±0ms      2.24±0.02ms     1.42  io.csv.ReadCSVFloatPrecision.time_read_csv(',', '_', 'high')
+       121±0.5μs        172±0.5μs     1.42  groupby.GroupByMethods.time_dtype_as_field('object', 'count', 'direct')
+       118±0.6μs          167±2μs     1.42  groupby.GroupByMethods.time_dtype_as_group('float', 'first', 'direct')
+         785±7μs      1.11±0.01ms     1.42  period.Indexing.time_align
+       121±0.4μs          172±1μs     1.42  groupby.GroupByMethods.time_dtype_as_group('datetime', 'max', 'transformation')
+         374±3μs          528±4μs     1.41  groupby.GroupByMethods.time_dtype_as_field('int', 'sem', 'direct')
+       115±0.7μs        163±0.9μs     1.41  groupby.GroupByMethods.time_dtype_as_group('float', 'last', 'direct')
+     2.66±0.02ms      3.76±0.05ms     1.41  rolling.VariableWindowMethods.time_rolling('Series', '50s', 'float', 'min')
+         376±4μs          532±6μs     1.41  groupby.GroupByMethods.time_dtype_as_field('int', 'sem', 'transformation')
+       109±0.7μs          154±2μs     1.41  groupby.GroupByMethods.time_dtype_as_group('object', 'first', 'transformation')
+        2.71±0ms      3.83±0.05ms     1.41  rolling.VariableWindowMethods.time_rolling('Series', '50s', 'int', 'max')
+     1.51±0.02ms      2.13±0.07ms     1.41  io.csv.ReadCSVFloatPrecision.time_read_csv(';', '.', 'high')
+         112±1μs          158±4μs     1.41  groupby.GroupByMethods.time_dtype_as_field('int', 'last', 'transformation')
+     2.99±0.05μs       4.21±0.4μs     1.41  categoricals.CategoricalSlicing.time_getitem_scalar('monotonic_incr')
+         122±1μs        171±0.5μs     1.41  groupby.GroupByMethods.time_dtype_as_group('datetime', 'max', 'direct')
+      15.5±0.2ms       21.8±0.3ms     1.41  io.msgpack.MSGPack.time_write_msgpack
+       120±0.8μs          168±1μs     1.41  groupby.GroupByMethods.time_dtype_as_group('datetime', 'first', 'direct')
+       116±0.7μs          163±1μs     1.41  groupby.GroupByMethods.time_dtype_as_group('float', 'last', 'transformation')
+         221±3ms        310±0.9ms     1.41  frame_methods.Duplicated.time_frame_duplicated_wide
+      47.4±0.6ms       66.6±0.8ms     1.41  index_object.IndexAppend.time_append_range_list
+     1.57±0.01ms      2.21±0.06ms     1.40  io.csv.ReadCSVFloatPrecision.time_read_csv(';', '_', 'high')
+         238±3μs          334±6μs     1.40  frame_ctor.FromRecords.time_frame_from_records_generator(None)
+       113±0.7μs          159±1μs     1.40  groupby.GroupByMethods.time_dtype_as_group('int', 'last', 'transformation')
+        1.57±0ms      2.21±0.05ms     1.40  io.csv.ReadCSVFloatPrecision.time_read_csv(',', '_', None)
+       120±0.8μs          168±3μs     1.40  groupby.GroupByMethods.time_dtype_as_group('datetime', 'first', 'transformation')
+         435±3μs         609±10μs     1.40  groupby.GroupByMethods.time_dtype_as_group('int', 'sem', 'transformation')
+         246±1μs         344±30μs     1.40  groupby.GroupByMethods.time_dtype_as_field('int', 'head', 'transformation')
+       114±0.6μs        160±0.5μs     1.40  groupby.GroupByMethods.time_dtype_as_group('int', 'last', 'direct')
+         127±4μs          177±4μs     1.40  groupby.GroupByMethods.time_dtype_as_field('int', 'max', 'transformation')
+         126±1μs        176±0.8μs     1.40  groupby.GroupByMethods.time_dtype_as_field('float', 'std', 'direct')
+         382±2μs          533±8μs     1.40  groupby.GroupByMethods.time_dtype_as_group('float', 'sem', 'transformation')
+        86.5±5μs        121±0.5μs     1.39  groupby.GroupByMethods.time_dtype_as_field('float', 'mean', 'direct')
+         121±4μs          168±1μs     1.39  groupby.GroupByMethods.time_dtype_as_field('object', 'count', 'transformation')
+         899±4μs      1.25±0.02ms     1.39  groupby.SumMultiLevel.time_groupby_sum_multiindex
+       110±0.4μs        153±0.8μs     1.39  groupby.GroupByMethods.time_dtype_as_field('float', 'median', 'direct')
+        92.7±4μs        129±0.8μs     1.39  groupby.GroupByMethods.time_dtype_as_field('float', 'var', 'transformation')
+       125±0.6μs          175±2μs     1.39  groupby.GroupByMethods.time_dtype_as_group('int', 'first', 'direct')
+         419±1μs         582±30μs     1.39  categoricals.CategoricalSlicing.time_getitem_list('non_monotonic')
+     1.60±0.03ms      2.22±0.07ms     1.39  io.csv.ReadCSVFloatPrecision.time_read_csv(',', '_', 'round_trip')
+      13.5±0.3μs      18.7±0.06μs     1.39  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<DateOffset: days=2, months=2>)
+       119±0.4μs          166±1μs     1.39  groupby.GroupByMethods.time_dtype_as_group('float', 'first', 'transformation')
+       111±0.8μs          154±3μs     1.39  groupby.GroupByMethods.time_dtype_as_group('object', 'first', 'direct')
+         128±4μs          177±7μs     1.38  groupby.GroupByMethods.time_dtype_as_field('int', 'max', 'direct')
+     2.70±0.04ms      3.74±0.04ms     1.38  rolling.VariableWindowMethods.time_rolling('Series', '50s', 'float', 'max')
+       124±0.5μs         172±40μs     1.38  groupby.GroupByMethods.time_dtype_as_field('int', 'first', 'direct')
+       171±0.8μs          236±2μs     1.38  groupby.GroupByMethods.time_dtype_as_field('int', 'std', 'direct')
+         123±1μs          171±1μs     1.38  groupby.GroupByMethods.time_dtype_as_group('datetime', 'min', 'transformation')
+         110±1μs          152±1μs     1.38  groupby.GroupByMethods.time_dtype_as_field('float', 'median', 'transformation')
+         127±4μs          175±1μs     1.38  groupby.GroupByMethods.time_dtype_as_group('int', 'max', 'transformation')
+         119±1μs          164±2μs     1.38  groupby.GroupByMethods.time_dtype_as_group('datetime', 'last', 'direct')
+      26.5±0.1ms       36.5±0.1ms     1.38  join_merge.Concat.time_concat_small_frames(0)
+     2.75±0.04ms      3.79±0.07ms     1.38  rolling.VariableWindowMethods.time_rolling('Series', '50s', 'int', 'min')
+         127±4μs          175±1μs     1.38  groupby.GroupByMethods.time_dtype_as_field('int', 'min', 'transformation')
+      18.0±0.4μs       24.7±0.4μs     1.37  ctors.SeriesConstructors.time_series_constructor(<function SeriesConstructors.<lambda> at 0x11332b7b8>, False)
+       118±0.5μs        162±0.5μs     1.37  groupby.GroupByMethods.time_dtype_as_group('datetime', 'last', 'transformation')
+     1.55±0.03ms      2.13±0.01ms     1.37  io.csv.ReadCSVFloatPrecision.time_read_csv(';', '_', 'round_trip')
+         174±1μs          239±1μs     1.37  groupby.GroupByMethods.time_dtype_as_group('float', 'std', 'direct')
+        1.53±0ms      2.10±0.04ms     1.37  io.csv.ReadCSVFloatPrecision.time_read_csv(',', '.', 'high')
+         124±4μs          170±1μs     1.37  groupby.GroupByMethods.time_dtype_as_group('float', 'min', 'direct')
+         169±1μs          232±1μs     1.37  groupby.GroupByMethods.time_dtype_as_field('int', 'std', 'transformation')
+     1.60±0.01ms      2.19±0.05ms     1.37  io.csv.ReadCSVFloatPrecision.time_read_csv(',', '.', None)
+         440±2μs          602±4μs     1.37  groupby.GroupByMethods.time_dtype_as_group('int', 'sem', 'direct')
+         127±5μs          174±2μs     1.37  groupby.GroupByMethods.time_dtype_as_group('int', 'max', 'direct')
+     1.60±0.03ms      2.18±0.07ms     1.36  io.csv.ReadCSVFloatPrecision.time_read_csv(';', '.', None)
+      5.63±0.2ms      7.69±0.06ms     1.36  strings.Cat.time_cat(0, None, None, 0.001)
+         127±4μs        173±0.9μs     1.36  groupby.GroupByMethods.time_dtype_as_group('int', 'min', 'transformation')
+         124±5μs        169±0.9μs     1.36  groupby.GroupByMethods.time_dtype_as_group('float', 'min', 'transformation')
+         127±1μs        172±0.6μs     1.36  groupby.GroupByMethods.time_dtype_as_group('int', 'first', 'transformation')
+      43.8±0.2μs       59.6±0.5μs     1.36  indexing.NumericSeriesIndexing.time_iloc_array(<class 'pandas.core.indexes.numeric.Float64Index'>, 'nonunique_monotonic_inc')
+         134±8μs          183±4μs     1.36  groupby.GroupByMethods.time_dtype_as_field('int', 'var', 'direct')
+         205±2μs          279±5μs     1.36  timeseries.DatetimeIndex.time_normalize('dst')
+         420±2μs         572±40μs     1.36  categoricals.CategoricalSlicing.time_getitem_list('monotonic_incr')
+         126±4μs          172±4μs     1.36  groupby.GroupByMethods.time_dtype_as_group('float', 'max', 'transformation')
+     6.00±0.01ms      8.16±0.03ms     1.36  categoricals.Rank.time_rank_string_cat
+        89.3±5μs        122±0.4μs     1.36  groupby.GroupByMethods.time_dtype_as_field('float', 'mean', 'transformation')
+         136±8μs          185±4μs     1.36  groupby.GroupByMethods.time_dtype_as_group('float', 'var', 'transformation')
+     3.41±0.01ms      4.64±0.01ms     1.36  rolling.Methods.time_rolling('Series', 1000, 'float', 'std')
+      55.3±0.4ms       75.2±0.3ms     1.36  stat_ops.Correlation.time_corr('spearman')
+         129±4μs        175±0.3μs     1.36  groupby.GroupByMethods.time_dtype_as_field('int', 'min', 'direct')
+      17.6±0.2ms       23.9±0.2ms     1.35  stat_ops.FrameMultiIndexOps.time_op(0, 'kurt')
+     3.50±0.02ms      4.73±0.01ms     1.35  rolling.Methods.time_rolling('Series', 10, 'int', 'std')
+     3.49±0.01ms      4.72±0.02ms     1.35  rolling.Methods.time_rolling('Series', 1000, 'int', 'std')
+     7.50±0.03μs      10.1±0.05μs     1.35  offset.OnOffset.time_on_offset(<YearEnd: month=12>)
+         175±1μs          237±2μs     1.35  groupby.GroupByMethods.time_dtype_as_group('float', 'std', 'transformation')
+      3.45±0.1ms      4.65±0.03ms     1.35  rolling.Methods.time_rolling('Series', 10, 'float', 'std')
+      29.9±0.2μs       40.2±0.4μs     1.35  offset.OffestDatetimeArithmetic.time_subtract(<DateOffset: days=2, months=2>)
+         130±4μs        174±0.9μs     1.35  groupby.GroupByMethods.time_dtype_as_group('int', 'min', 'direct')
+     6.77±0.03μs      9.11±0.05μs     1.34  index_object.Indexing.time_get_loc('Int')
+      9.66±0.1ms       13.0±0.4ms     1.34  categoricals.CategoricalSlicing.time_getitem_bool_array('non_monotonic')
+      10.1±0.2ms      13.6±0.03ms     1.34  timedelta.TimedeltaOps.time_add_td_ts
+      44.3±0.3μs       59.3±0.5μs     1.34  indexing.NumericSeriesIndexing.time_iloc_array(<class 'pandas.core.indexes.numeric.Float64Index'>, 'unique_monotonic_inc')
+        76.2±2ms        102±0.9ms     1.34  stat_ops.FrameMultiIndexOps.time_op(1, 'kurt')
+     1.35±0.01ms      1.80±0.05ms     1.34  join_merge.Merge.time_merge_dataframe_integer_key(False)
+         127±4μs          170±1μs     1.33  groupby.GroupByMethods.time_dtype_as_group('float', 'max', 'direct')
+        1.33±0ms      1.78±0.06ms     1.33  groupby.Datelike.time_sum('date_range')
+     5.32±0.02ms       7.09±0.2ms     1.33  reindex.DropDuplicates.time_frame_drop_dups(True)
+         293±4ms         391±30ms     1.33  frame_methods.Nunique.time_frame_nunique
+      96.4±0.7μs        128±0.7μs     1.33  join_merge.Concat.time_concat_empty_right(0)
+      61.7±0.1μs       81.9±0.6μs     1.33  indexing.NumericSeriesIndexing.time_getitem_slice(<class 'pandas.core.indexes.numeric.Float64Index'>, 'nonunique_monotonic_inc')
+     10.8±0.08ms       14.3±0.2ms     1.33  categoricals.Constructor.time_regular
+         106±2ms        140±0.6ms     1.32  index_object.IndexAppend.time_append_obj_list
+      48.5±0.9μs       64.1±0.1μs     1.32  frame_ctor.FromNDArray.time_frame_from_ndarray
+         329±6μs          434±1μs     1.32  timeseries.ResetIndex.time_reest_datetimeindex(None)
+        98.5±2μs        130±0.4μs     1.32  join_merge.Concat.time_concat_empty_left(0)
+      12.7±0.1ms      16.7±0.07ms     1.32  reshape.PivotTable.time_pivot_table
+         139±6μs          184±3μs     1.32  groupby.GroupByMethods.time_dtype_as_group('float', 'var', 'direct')
+      62.9±0.4μs       82.8±0.5μs     1.32  inference.ToNumeric.time_from_str('ignore')
+      50.1±0.6μs       65.8±0.6μs     1.32  indexing.NonNumericSeriesIndexing.time_getitem_pos_slice('datetime', 'unique_monotonic_inc')
+         248±9μs          326±8μs     1.31  groupby.GroupByMethods.time_dtype_as_group('int', 'prod', 'direct')
+         218±2μs          286±2μs     1.31  groupby.GroupByMethods.time_dtype_as_group('object', 'head', 'transformation')
+     10.5±0.08μs       13.8±0.3μs     1.31  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<YearBegin: month=1>)
+      7.04±0.2μs       9.23±0.2μs     1.31  index_object.Indexing.time_get_loc_sorted('Int')
+       216±0.9μs          283±3μs     1.31  groupby.GroupByMethods.time_dtype_as_group('object', 'head', 'direct')
+     7.26±0.04ms       9.52±0.1ms     1.31  indexing.InsertColumns.time_assign_with_setitem
+      59.8±0.4μs         78.3±3μs     1.31  frame_ctor.FromSeries.time_mi_series
+      88.5±0.3μs          115±1μs     1.30  groupby.GroupByMethods.time_dtype_as_field('int', 'shift', 'direct')
+         598±2μs         780±50μs     1.30  frame_methods.Quantile.time_frame_quantile(1)
+       226±0.7μs          294±2μs     1.30  groupby.GroupByMethods.time_dtype_as_group('object', 'tail', 'transformation')
+     2.15±0.02ms      2.80±0.01ms     1.30  groupby.Transform.time_transform_multi_key4
+         226±9μs          294±5μs     1.30  groupby.GroupByMethods.time_dtype_as_field('int', 'prod', 'transformation')
+     3.10±0.02ms      4.04±0.03ms     1.30  io.sas.SAS.time_read_msgpack('xport')
+       241±0.7μs          313±2μs     1.30  groupby.GroupByMethods.time_dtype_as_group('int', 'std', 'transformation')
+      88.6±0.7μs          115±1μs     1.30  groupby.GroupByMethods.time_dtype_as_field('int', 'shift', 'transformation')
+      80.4±0.1μs        105±0.9μs     1.30  groupby.GroupByMethods.time_dtype_as_field('datetime', 'shift', 'direct')
+         229±2μs          298±5μs     1.30  groupby.GroupByMethods.time_dtype_as_group('object', 'tail', 'direct')
+         866±9μs      1.13±0.01ms     1.30  series_methods.ValueCounts.time_value_counts('int')
+         629±4μs          817±5μs     1.30  reindex.DropDuplicates.time_series_drop_dups_int(False)
+     3.75±0.01ms      4.86±0.04ms     1.30  rolling.Pairwise.time_pairwise(1000, 'corr', False)
+         226±1μs          294±2μs     1.30  groupby.GroupByMethods.time_dtype_as_group('float', 'head', 'direct')
+       237±0.3μs          308±3μs     1.30  groupby.GroupByMethods.time_dtype_as_group('datetime', 'tail', 'transformation')
+      89.5±0.2μs          116±1μs     1.30  groupby.GroupByMethods.time_dtype_as_group('int', 'shift', 'transformation')
+      13.0±0.2μs       16.8±0.1μs     1.29  offset.OffestDatetimeArithmetic.time_add(<DateOffset: days=2, months=2>)
+     5.16±0.09ms      6.68±0.03ms     1.29  groupby.Transform.time_transform_multi_key2
+      89.8±0.4μs          116±1μs     1.29  groupby.GroupByMethods.time_dtype_as_group('int', 'shift', 'direct')
+       114±0.5μs        147±0.8μs     1.29  inference.NumericInferOps.time_subtract(<class 'numpy.int8'>)
+      7.83±0.1ms       10.1±0.2ms     1.29  stat_ops.FrameOps.time_op('mad', 'float', 0, False)
+       228±0.6μs          294±3μs     1.29  groupby.GroupByMethods.time_dtype_as_group('float', 'head', 'transformation')
+        71.9±2μs       92.7±0.6μs     1.29  indexing.NumericSeriesIndexing.time_ix_scalar(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'unique_monotonic_inc')
+         399±8μs          515±2μs     1.29  timeseries.ResetIndex.time_reest_datetimeindex('US/Eastern')
+         237±1μs          306±2μs     1.29  groupby.GroupByMethods.time_dtype_as_group('datetime', 'tail', 'direct')
+         236±1μs          305±2μs     1.29  groupby.GroupByMethods.time_dtype_as_group('float', 'tail', 'transformation')
+      74.7±0.9μs       96.2±0.9μs     1.29  groupby.GroupByMethods.time_dtype_as_field('float', 'shift', 'direct')
+      80.4±0.8μs        104±0.6μs     1.29  groupby.GroupByMethods.time_dtype_as_field('datetime', 'shift', 'transformation')
+      75.5±0.9μs       97.1±0.6μs     1.29  groupby.GroupByMethods.time_dtype_as_field('float', 'shift', 'transformation')
+         242±3μs          311±2μs     1.29  groupby.GroupByMethods.time_dtype_as_group('int', 'std', 'direct')
+     2.06±0.01ms      2.64±0.06ms     1.28  io.csv.ReadCSVFloatPrecision.time_read_csv(';', '.', 'round_trip')
+         456±3ms          586±7ms     1.28  stat_ops.SeriesMultiIndexOps.time_op([0, 1], 'mad')
+     5.76±0.08ms      7.40±0.09ms     1.28  reindex.DropDuplicates.time_frame_drop_dups_na(True)
+      58.9±0.2μs       75.7±0.8μs     1.28  timeseries.SortIndex.time_sort_index(True)
+         229±1μs          294±2μs     1.28  groupby.GroupByMethods.time_dtype_as_group('datetime', 'head', 'direct')
+         249±3μs          320±4μs     1.28  groupby.GroupByMethods.time_dtype_as_group('int', 'head', 'direct')
+         142±8μs        182±0.8μs     1.28  groupby.GroupByMethods.time_dtype_as_field('int', 'var', 'transformation')
+      45.7±0.5μs       58.6±0.6μs     1.28  indexing.NumericSeriesIndexing.time_getitem_slice(<class 'pandas.core.indexes.numeric.Float64Index'>, 'unique_monotonic_inc')
+         228±2μs          292±3μs     1.28  groupby.GroupByMethods.time_dtype_as_group('datetime', 'head', 'transformation')
+         245±7μs          315±1μs     1.28  groupby.GroupByMethods.time_dtype_as_field('int', 'sum', 'transformation')
+     2.61±0.04ms      3.35±0.01ms     1.28  rolling.Pairwise.time_pairwise(1000, 'cov', False)
+       250±0.9μs          320±1μs     1.28  groupby.GroupByMethods.time_dtype_as_field('datetime', 'head', 'transformation')
+      71.5±0.6ms       91.7±0.6ms     1.28  join_merge.Concat.time_concat_series(1)
+         162±2ms          207±2ms     1.28  stat_ops.SeriesMultiIndexOps.time_op([0, 1], 'kurt')
+       114±0.5μs          147±1μs     1.28  inference.NumericInferOps.time_subtract(<class 'numpy.uint8'>)
+         117±2μs        150±0.5μs     1.28  inference.NumericInferOps.time_add(<class 'numpy.int8'>)
+     13.4±0.04ms       17.1±0.2ms     1.28  join_merge.Concat.time_concat_series(0)
+     7.82±0.05ms      10.0±0.05ms     1.28  stat_ops.FrameOps.time_op('mad', 'float', 0, True)
+     2.07±0.02ms      2.66±0.08ms     1.28  io.csv.ReadCSVFloatPrecision.time_read_csv(',', '.', 'round_trip')
+         239±1μs          305±3μs     1.28  groupby.GroupByMethods.time_dtype_as_group('float', 'tail', 'direct')
+         249±8μs          319±2μs     1.28  groupby.GroupByMethods.time_dtype_as_group('int', 'sum', 'direct')
+         204±9μs          260±4μs     1.28  groupby.GroupByMethods.time_dtype_as_group('int', 'var', 'direct')
+     1.58±0.01ms      2.02±0.01ms     1.28  join_merge.Merge.time_merge_dataframe_integer_key(True)
+         565±4μs         722±10μs     1.28  groupby.GroupByMethods.time_dtype_as_group('object', 'value_counts', 'transformation')
+         248±1μs          317±5μs     1.28  groupby.GroupByMethods.time_dtype_as_field('int', 'head', 'direct')
+       148±0.9ms          188±1ms     1.28  replace.Convert.time_replace('DataFrame', 'Timedelta')
+         568±1μs          724±4μs     1.28  groupby.GroupByMethods.time_dtype_as_group('object', 'value_counts', 'direct')
+     1.07±0.01ms      1.37±0.01ms     1.27  groupby.SumBools.time_groupby_sum_booleans
+         257±2μs          328±4μs     1.27  groupby.GroupByMethods.time_dtype_as_field('int', 'tail', 'transformation')
+      29.8±0.3ms       37.9±0.3ms     1.27  stat_ops.FrameMultiIndexOps.time_op(0, 'mad')
+         258±2μs          329±3μs     1.27  groupby.GroupByMethods.time_dtype_as_group('int', 'tail', 'direct')
+      3.62±0.03s       4.61±0.02s     1.27  period.DataFramePeriodColumn.time_set_index
+         649±2ms         826±10ms     1.27  stat_ops.FrameMultiIndexOps.time_op([0, 1], 'kurt')
+         251±8μs          319±4μs     1.27  groupby.GroupByMethods.time_dtype_as_group('int', 'sum', 'transformation')
+         149±1ms          190±1ms     1.27  replace.Convert.time_replace('DataFrame', 'Timestamp')
+         225±9μs        286±0.6μs     1.27  groupby.GroupByMethods.time_dtype_as_field('int', 'mean', 'transformation')
+         158±2ms          200±2ms     1.27  stat_ops.SeriesMultiIndexOps.time_op([0, 1], 'skew')
+         247±9μs          313±2μs     1.27  groupby.GroupByMethods.time_dtype_as_field('int', 'sum', 'direct')
+         255±9μs          323±4μs     1.27  groupby.GroupByMethods.time_dtype_as_group('float', 'mean', 'transformation')
+         259±3μs          328±1μs     1.27  groupby.GroupByMethods.time_dtype_as_field('int', 'tail', 'direct')
+     8.74±0.06μs      11.1±0.06μs     1.27  offset.OffestDatetimeArithmetic.time_apply(<YearEnd: month=12>)
+      64.7±0.5μs         82.0±2μs     1.27  indexing.NonNumericSeriesIndexing.time_get_value('datetime', 'nonunique_monotonic_inc')
+        51.4±1μs       65.2±0.3μs     1.27  timeseries.SortIndex.time_get_slice(True)
+       256±0.6μs          324±2μs     1.27  groupby.GroupByMethods.time_dtype_as_field('float', 'tail', 'transformation')
+        73.6±1μs       93.3±0.8μs     1.27  series_methods.Clip.time_clip
+         743±1μs          942±3μs     1.27  reindex.DropDuplicates.time_series_drop_dups_string(False)
+     2.57±0.02ms      3.26±0.01ms     1.27  rolling.Pairwise.time_pairwise(None, 'cov', False)
+         247±1μs        313±0.4μs     1.27  groupby.GroupByMethods.time_dtype_as_group('int', 'head', 'transformation')
+      10.6±0.1μs       13.4±0.3μs     1.27  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<YearEnd: month=12>)
+         687±3μs         870±10μs     1.27  groupby.GroupByMethods.time_dtype_as_group('datetime', 'value_counts', 'direct')
+      86.4±0.4μs          109±1μs     1.27  groupby.GroupByMethods.time_dtype_as_field('datetime', 'min', 'direct')
+     1.18±0.02μs      1.49±0.01μs     1.27  index_object.Indexing.time_get('Int')
+       116±0.5μs        147±0.5μs     1.26  inference.NumericInferOps.time_add(<class 'numpy.uint8'>)
+         698±6μs         882±20μs     1.26  groupby.GroupByMethods.time_dtype_as_field('int', 'value_counts', 'direct')
+        206±10μs          260±2μs     1.26  groupby.GroupByMethods.time_dtype_as_group('int', 'var', 'transformation')
+         251±7μs          317±5μs     1.26  groupby.GroupByMethods.time_dtype_as_group('int', 'prod', 'transformation')
+       108±0.4μs          136±1μs     1.26  groupby.GroupByMethods.time_dtype_as_field('object', 'shift', 'transformation')
+        32.1±1ms       40.5±0.5ms     1.26  io.csv.ReadCSVCategorical.time_convert_direct
+      82.7±0.5μs        104±0.8μs     1.26  groupby.GroupByMethods.time_dtype_as_group('object', 'shift', 'direct')
+         782±3μs          986±2μs     1.26  groupby.GroupByMethods.time_dtype_as_field('float', 'value_counts', 'direct')
+      83.4±0.4μs        105±0.6μs     1.26  groupby.GroupByMethods.time_dtype_as_group('object', 'shift', 'transformation')
+       107±0.4μs          135±2μs     1.26  groupby.GroupByMethods.time_dtype_as_field('object', 'shift', 'direct')
+         260±1μs          327±3μs     1.26  groupby.GroupByMethods.time_dtype_as_field('datetime', 'tail', 'direct')
+         252±1μs          317±2μs     1.26  groupby.GroupByMethods.time_dtype_as_field('datetime', 'head', 'direct')
+       247±0.9μs          311±3μs     1.26  groupby.GroupByMethods.time_dtype_as_field('float', 'head', 'direct')
+       259±0.9μs          326±3μs     1.26  groupby.GroupByMethods.time_dtype_as_group('int', 'tail', 'transformation')
+         150±5μs          188±4μs     1.26  indexing.AssignTimeseriesIndex.time_frame_assign_timeseries_index
+     2.52±0.01ms      3.17±0.03ms     1.26  io.csv.ReadCSVFloatPrecision.time_read_csv_python_engine(',', '_', 'high')
+        598±10ns          751±8ns     1.26  index_object.Indexing.time_get('String')
+         269±1μs          338±3μs     1.26  groupby.GroupByMethods.time_dtype_as_group('float', 'median', 'transformation')
+        226±10μs          283±2μs     1.26  groupby.GroupByMethods.time_dtype_as_field('int', 'mean', 'direct')
+      8.34±0.1ms      10.5±0.05ms     1.26  stat_ops.Rank.time_rank('Series', True)
+         257±2μs          322±2μs     1.25  groupby.GroupByMethods.time_dtype_as_field('float', 'tail', 'direct')
+     7.99±0.05ms       10.0±0.2ms     1.25  stat_ops.FrameOps.time_op('mad', 'int', 0, True)
+     2.62±0.03ms      3.28±0.01ms     1.25  rolling.Pairwise.time_pairwise(10, 'cov', False)
+         261±8μs          328±1μs     1.25  join_merge.Append.time_append_homogenous
+     8.01±0.05ms       10.0±0.1ms     1.25  stat_ops.FrameOps.time_op('mad', 'int', 0, False)
+      20.8±0.2ms       26.1±0.4ms     1.25  join_merge.MergeAsof.time_on_int
+         785±7μs          983±6μs     1.25  groupby.GroupByMethods.time_dtype_as_field('float', 'value_counts', 'transformation')
+         269±1μs          336±2μs     1.25  groupby.GroupByMethods.time_dtype_as_group('float', 'median', 'direct')
+       248±0.7μs          310±1μs     1.25  groupby.GroupByMethods.time_dtype_as_field('float', 'head', 'transformation')
+      40.6±0.1μs       50.8±0.5μs     1.25  indexing.NonNumericSeriesIndexing.time_getitem_pos_slice('string', 'nonunique_monotonic_inc')
+         260±8μs          326±1μs     1.25  groupby.GroupByMethods.time_dtype_as_group('float', 'sum', 'transformation')
+      57.7±0.6ms       72.1±0.3ms     1.25  io.sas.SAS.time_read_msgpack('sas7bdat')
+     7.36±0.08μs      9.19±0.06μs     1.25  index_object.Indexing.time_slice_step('Float')
+        88.5±1μs          111±2μs     1.25  groupby.GroupByMethods.time_dtype_as_field('datetime', 'min', 'transformation')
+        258±10μs          323±5μs     1.25  groupby.GroupByMethods.time_dtype_as_group('float', 'sum', 'direct')
+      8.31±0.1ms      10.4±0.04ms     1.25  stat_ops.Rank.time_rank('Series', False)
+     2.52±0.01ms      3.15±0.01ms     1.25  io.csv.ReadCSVFloatPrecision.time_read_csv_python_engine(',', '_', 'round_trip')
+     3.85±0.04ms      4.80±0.05ms     1.25  rolling.Pairwise.time_pairwise(None, 'corr', False)
+     2.51±0.02ms      3.13±0.01ms     1.25  io.csv.ReadCSVFloatPrecision.time_read_csv_python_engine(';', '_', 'high')
+     1.79±0.02ms      2.23±0.03ms     1.25  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'count')
+      84.6±0.1μs        105±0.6μs     1.25  groupby.GroupByMethods.time_dtype_as_group('float', 'shift', 'transformation')
+         700±3μs          872±3μs     1.25  groupby.GroupByMethods.time_dtype_as_field('int', 'value_counts', 'transformation')
+       264±0.8μs          329±3μs     1.25  groupby.GroupByMethods.time_dtype_as_field('datetime', 'tail', 'transformation')
+      41.1±0.1μs       51.2±0.5μs     1.25  indexing.NonNumericSeriesIndexing.time_getitem_pos_slice('string', 'unique_monotonic_inc')
+      85.5±0.8μs        106±0.3μs     1.24  groupby.GroupByMethods.time_dtype_as_group('float', 'shift', 'direct')
+     2.53±0.01ms      3.15±0.02ms     1.24  io.csv.ReadCSVFloatPrecision.time_read_csv_python_engine(';', '_', None)
+     3.85±0.04ms      4.79±0.01ms     1.24  rolling.Pairwise.time_pairwise(10, 'corr', False)
+      85.4±0.5μs        106±0.4μs     1.24  groupby.GroupByMethods.time_dtype_as_group('datetime', 'shift', 'direct')
+         151±1μs          188±2μs     1.24  inference.NumericInferOps.time_add(<class 'numpy.int16'>)
+     2.53±0.02ms      3.14±0.01ms     1.24  io.csv.ReadCSVFloatPrecision.time_read_csv_python_engine(';', '_', 'round_trip')
+     1.20±0.05μs      1.49±0.02μs     1.24  index_object.Indexing.time_get('Float')
+        259±10μs          321±2μs     1.24  groupby.GroupByMethods.time_dtype_as_group('float', 'mean', 'direct')
+         232±8μs          288±2μs     1.24  groupby.GroupByMethods.time_dtype_as_group('int', 'mean', 'direct')
+         649±9μs          805±4μs     1.24  groupby.GroupByMethods.time_dtype_as_field('object', 'value_counts', 'transformation')
+      67.9±0.3μs       84.3±0.9μs     1.24  indexing.NonNumericSeriesIndexing.time_getitem_label_slice('datetime', 'unique_monotonic_inc')
+      6.25±0.1ms      7.75±0.02ms     1.24  groupby.Transform.time_transform_multi_key1
+         680±6μs         844±20μs     1.24  groupby.GroupByMethods.time_dtype_as_group('int', 'value_counts', 'direct')
+         649±2μs          806±1μs     1.24  groupby.GroupByMethods.time_dtype_as_field('object', 'value_counts', 'direct')
+         689±5μs          854±4μs     1.24  groupby.GroupByMethods.time_dtype_as_group('datetime', 'value_counts', 'transformation')
+         261±2μs          323±5μs     1.24  groupby.GroupByMethods.time_dtype_as_field('int', 'median', 'direct')
+     2.65±0.05ms       3.28±0.1ms     1.24  io.csv.ReadUint64Integers.time_read_uint64
+     9.29±0.07ms       11.5±0.2ms     1.24  frame_methods.MaskBool.time_frame_mask_floats
+     5.57±0.06ms      6.90±0.04ms     1.24  rolling.VariableWindowMethods.time_rolling('DataFrame', '50s', 'float', 'std')
+     1.88±0.01ms      2.33±0.07ms     1.24  stat_ops.FrameMultiIndexOps.time_op(1, 'prod')
+         258±2μs          319±2μs     1.24  groupby.GroupByMethods.time_dtype_as_field('int', 'median', 'transformation')
+     14.8±0.07ms      18.3±0.08ms     1.24  reindex.DropDuplicates.time_frame_drop_dups(False)
+      78.5±0.5μs       97.1±0.7μs     1.24  groupby.GroupByMethods.time_dtype_as_field('datetime', 'last', 'direct')
+      51.0±0.5ms       63.1±0.7ms     1.24  stat_ops.SeriesMultiIndexOps.time_op(1, 'mad')
+      66.3±0.5μs         81.9±1μs     1.24  indexing.NumericSeriesIndexing.time_loc_slice(<class 'pandas.core.indexes.numeric.Float64Index'>, 'nonunique_monotonic_inc')
+     1.43±0.01ms      1.76±0.03ms     1.23  stat_ops.SeriesMultiIndexOps.time_op(0, 'sum')
+      8.69±0.1μs      10.7±0.05μs     1.23  offset.OffestDatetimeArithmetic.time_apply(<YearBegin: month=1>)
+     1.00±0.01ms         1.23±0ms     1.23  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'sum')
+        86.2±1μs        106±0.6μs     1.23  groupby.GroupByMethods.time_dtype_as_group('datetime', 'shift', 'transformation')
+      73.2±0.6ms       90.2±0.9ms     1.23  frame_methods.ToHTML.time_to_html_mixed
+     2.24±0.02ms      2.76±0.06ms     1.23  stat_ops.FrameMultiIndexOps.time_op(0, 'var')
+      7.63±0.4μs      9.38±0.09μs     1.23  index_object.Indexing.time_slice('Float')
+         996±5μs      1.22±0.01ms     1.23  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'sum')
+      85.5±0.7μs        105±0.9μs     1.23  groupby.GroupByMethods.time_dtype_as_field('datetime', 'first', 'direct')
+      83.6±0.8μs          103±2μs     1.23  groupby.GroupByMethods.time_dtype_as_field('datetime', 'max', 'transformation')
+         135±1μs          165±1μs     1.23  join_merge.Concat.time_concat_empty_right(1)
+         264±7μs          324±3μs     1.23  groupby.GroupByMethods.time_dtype_as_group('float', 'prod', 'direct')
+         152±2μs          187±4μs     1.23  inference.NumericInferOps.time_multiply(<class 'numpy.uint8'>)
+      83.2±0.6μs        102±0.4μs     1.23  groupby.GroupByMethods.time_dtype_as_field('datetime', 'max', 'direct')
+     4.27±0.05ms      5.24±0.04ms     1.23  stat_ops.FrameMultiIndexOps.time_op(0, 'sem')
+     2.57±0.03ms      3.15±0.03ms     1.23  io.csv.ReadCSVFloatPrecision.time_read_csv_python_engine(',', '_', None)
+         153±1μs          187±3μs     1.23  inference.NumericInferOps.time_multiply(<class 'numpy.uint16'>)
+         293±4μs          360±1μs     1.23  groupby.GroupByMethods.time_dtype_as_field('object', 'tail', 'transformation')
+         152±2μs          186±2μs     1.23  inference.NumericInferOps.time_add(<class 'numpy.uint16'>)
+      9.09±0.1ms       11.1±0.1ms     1.23  stat_ops.Rank.time_average_old('Series', True)
+       682±0.9μs          836±7μs     1.23  groupby.GroupByMethods.time_dtype_as_group('float', 'value_counts', 'transformation')
+         681±3μs          835±5μs     1.23  groupby.GroupByMethods.time_dtype_as_group('float', 'value_counts', 'direct')
+     22.6±0.05μs       27.7±0.1μs     1.22  indexing.NonNumericSeriesIndexing.time_getitem_scalar('datetime', 'nonunique_monotonic_inc')
+         286±2μs          350±2μs     1.22  groupby.GroupByMethods.time_dtype_as_field('object', 'head', 'transformation')
+         683±2μs         836±10μs     1.22  groupby.GroupByMethods.time_dtype_as_group('int', 'value_counts', 'transformation')
+      79.4±0.3μs         97.2±1μs     1.22  groupby.GroupByMethods.time_dtype_as_field('datetime', 'last', 'transformation')
+     10.2±0.03ms      12.4±0.03ms     1.22  gil.ParallelRolling.time_rolling('std')
+         155±2μs          190±3μs     1.22  inference.NumericInferOps.time_multiply(<class 'numpy.int16'>)
+      26.6±0.2μs      32.5±0.07μs     1.22  indexing.NumericSeriesIndexing.time_iloc_slice(<class 'pandas.core.indexes.numeric.Float64Index'>, 'unique_monotonic_inc')
+     1.81±0.01ms      2.22±0.02ms     1.22  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'count')
+     3.29±0.02ms       4.02±0.2ms     1.22  binary_ops.Ops.time_frame_mult(False, 'default')
+         238±5μs          290±3μs     1.22  groupby.GroupByMethods.time_dtype_as_group('int', 'mean', 'transformation')
+     5.67±0.02ms      6.94±0.02ms     1.22  rolling.VariableWindowMethods.time_rolling('DataFrame', '50s', 'int', 'std')
+     9.03±0.07ms       11.0±0.2ms     1.22  stat_ops.Rank.time_average_old('Series', False)
+     1.84±0.02ms      2.24±0.02ms     1.22  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'count')
+       295±0.9μs          360±3μs     1.22  groupby.GroupByMethods.time_dtype_as_field('object', 'tail', 'direct')
+     1.85±0.02ms      2.25±0.04ms     1.22  stat_ops.FrameMultiIndexOps.time_op(1, 'mean')
+         263±3ms          321±3ms     1.22  groupby.Apply.time_copy_overhead_single_col
+         266±9μs          325±2μs     1.22  groupby.GroupByMethods.time_dtype_as_group('float', 'prod', 'transformation')
+     9.14±0.07μs       11.2±0.4μs     1.22  timestamp.TimestampProperties.time_is_year_start(None, 'B')
+     3.30±0.06ms       4.03±0.2ms     1.22  binary_ops.Ops.time_frame_add(False, 1)
+     4.28±0.04ms      5.21±0.06ms     1.22  stat_ops.FrameMultiIndexOps.time_op(1, 'sem')
+     1.79±0.03ms      2.18±0.01ms     1.22  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'count')
+      13.5±0.1μs       16.4±0.3μs     1.22  indexing.NumericSeriesIndexing.time_getitem_scalar(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
+      85.8±0.6μs          104±1μs     1.22  groupby.GroupByMethods.time_dtype_as_field('datetime', 'first', 'transformation')
+      43.6±0.7ms         53.1±1ms     1.22  frame_methods.Equals.time_frame_object_equal
+       135±0.7μs          165±2μs     1.22  join_merge.Concat.time_concat_empty_left(1)
+        1.04±0ms         1.27±0ms     1.22  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'sum')
+      27.4±0.4ms         33.4±2ms     1.22  binary_ops.Timeseries.time_timestamp_ops_diff_with_shift('US/Eastern')
+         151±1μs          184±4μs     1.22  inference.NumericInferOps.time_subtract(<class 'numpy.uint16'>)
+     1.46±0.02ms      1.77±0.02ms     1.22  stat_ops.SeriesMultiIndexOps.time_op(1, 'mean')
+      5.72±0.1ms       6.95±0.1ms     1.22  strings.Cat.time_cat(0, ',', '-', 0.001)
+       153±0.7μs          186±2μs     1.21  inference.NumericInferOps.time_multiply(<class 'numpy.int8'>)
+        1.05±0ms         1.27±0ms     1.21  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'sum')
+     3.30±0.05ms       4.01±0.2ms     1.21  binary_ops.Ops.time_frame_add(False, 'default')
+     1.83±0.01ms      2.22±0.02ms     1.21  stat_ops.FrameMultiIndexOps.time_op(0, 'mean')
+     5.40±0.06ms      6.56±0.01ms     1.21  rolling.VariableWindowMethods.time_rolling('DataFrame', '1d', 'float', 'std')
+         157±2μs          191±2μs     1.21  indexing.DataFrameStringIndexing.time_boolean_rows
+       285±0.9μs          346±5μs     1.21  groupby.GroupByMethods.time_dtype_as_field('object', 'head', 'direct')
+         262±3μs          318±2μs     1.21  groupby.GroupByMethods.time_dtype_as_group('int', 'median', 'direct')
+      5.50±0.1ms      6.67±0.05ms     1.21  rolling.VariableWindowMethods.time_rolling('DataFrame', '1d', 'int', 'std')
+         228±4μs          276±2μs     1.21  join_merge.JoinNonUnique.time_join_non_unique_equal
+     9.06±0.04μs       11.0±0.1μs     1.21  timestamp.TimestampProperties.time_is_leap_year(None, 'B')
+      9.27±0.1μs       11.2±0.1μs     1.21  timestamp.TimestampProperties.time_is_quarter_start(None, 'B')
+      5.62±0.1ms      6.81±0.08ms     1.21  strings.Cat.time_cat(0, None, '-', 0.001)
+     1.45±0.02ms      1.75±0.01ms     1.21  stat_ops.SeriesMultiIndexOps.time_op(1, 'prod')
+     1.50±0.03ms      1.81±0.04ms     1.21  stat_ops.SeriesMultiIndexOps.time_op(0, 'mean')
+      5.69±0.1ms      6.88±0.07ms     1.21  rolling.VariableWindowMethods.time_rolling('DataFrame', '1h', 'float', 'std')
+      48.9±0.4μs       59.1±0.3μs     1.21  indexing.NumericSeriesIndexing.time_loc_slice(<class 'pandas.core.indexes.numeric.Float64Index'>, 'unique_monotonic_inc')
+      17.8±0.3μs       21.5±0.4μs     1.21  index_object.Indexing.time_get_loc('Float')
+     1.45±0.02ms      1.75±0.02ms     1.21  stat_ops.SeriesMultiIndexOps.time_op(1, 'sum')
+     5.80±0.02ms      7.00±0.08ms     1.21  frame_methods.MaskBool.time_frame_mask_bools
+         238±9μs          287±1μs     1.21  groupby.GroupByMethods.time_dtype_as_field('int', 'prod', 'direct')
+         151±1μs          183±3μs     1.21  inference.NumericInferOps.time_subtract(<class 'numpy.int16'>)
+      73.0±0.3μs         88.0±5μs     1.21  frame_methods.GetNumericData.time_frame_get_numeric_data
+        306±20ms          369±4ms     1.21  groupby.GroupStrings.time_multi_columns
+         138±1μs        167±0.8μs     1.21  groupby.GroupByMethods.time_dtype_as_group('float', 'cumcount', 'transformation')
+         264±4μs          318±2μs     1.21  groupby.GroupByMethods.time_dtype_as_group('int', 'median', 'transformation')
+     3.11±0.03ms      3.75±0.02ms     1.21  io.csv.ReadCSVFloatPrecision.time_read_csv_python_engine(',', '.', None)
+     9.17±0.09μs       11.1±0.1μs     1.20  timestamp.TimestampProperties.time_is_month_start(None, 'B')
+         192±2ms        231±0.9ms     1.20  strings.Split.time_split(True)
+        1.45±0ms      1.75±0.01ms     1.20  stat_ops.SeriesMultiIndexOps.time_op(0, 'prod')
+     3.63±0.05ms       4.37±0.3ms     1.20  binary_ops.Ops.time_frame_add(True, 1)
+       132±0.2μs          159±2μs     1.20  groupby.GroupByMethods.time_dtype_as_group('object', 'cumcount', 'transformation')
+     3.18±0.05μs      3.83±0.07μs     1.20  indexing.CategoricalIndexIndexing.time_getitem_scalar('monotonic_incr')
+     3.12±0.02ms      3.75±0.03ms     1.20  io.csv.ReadCSVFloatPrecision.time_read_csv_python_engine(';', '.', None)
+     3.63±0.03ms       4.36±0.3ms     1.20  binary_ops.Ops.time_frame_mult(True, 1)
+     4.13±0.02ms      4.96±0.06ms     1.20  groupby.Apply.time_scalar_function_single_col
+         407±2μs         488±30μs     1.20  frame_methods.Quantile.time_frame_quantile(0)
+     3.14±0.03ms      3.77±0.02ms     1.20  io.csv.ReadCSVFloatPrecision.time_read_csv_python_engine(',', '.', 'high')
+         151±1ms          181±3ms     1.20  binary_ops.Ops2.time_frame_float_div_by_zero
+         945±6μs         1.13±0ms     1.20  reshape.SparseIndex.time_unstack
+     3.11±0.04ms      3.73±0.02ms     1.20  io.csv.ReadCSVFloatPrecision.time_read_csv_python_engine(';', '.', 'high')
+     2.20±0.05ms      2.63±0.05ms     1.20  timeseries.AsOf.time_asof_nan_single('DataFrame')
+         138±1μs          166±1μs     1.20  groupby.GroupByMethods.time_dtype_as_group('float', 'cumcount', 'direct')
+      25.9±0.2ms         31.0±1ms     1.20  eval.Query.time_query_datetime_index
+     3.30±0.02ms       3.95±0.2ms     1.20  binary_ops.Ops.time_frame_mult(False, 1)
+         427±1μs         511±60μs     1.20  frame_methods.Isnull.time_isnull_floats_no_null
+         159±1μs          190±9μs     1.19  groupby.GroupByMethods.time_dtype_as_field('int', 'cumcount', 'transformation')
+      27.0±0.3μs       32.2±0.2μs     1.19  indexing.NumericSeriesIndexing.time_iloc_slice(<class 'pandas.core.indexes.numeric.Float64Index'>, 'nonunique_monotonic_inc')
+     4.29±0.03ms       5.12±0.7ms     1.19  groupby.Categories.time_groupby_extra_cat_nosort
+         133±1μs          159±1μs     1.19  groupby.GroupByMethods.time_dtype_as_group('object', 'cumcount', 'direct')
+     2.87±0.01ms      3.42±0.03ms     1.19  timeseries.ToDatetimeISO8601.time_iso8601_nosep
+     9.08±0.06ms       10.8±0.2ms     1.19  groupby.MultiColumn.time_cython_sum
+      9.17±0.1μs      10.9±0.08μs     1.19  timestamp.TimestampProperties.time_is_quarter_end(None, 'B')
+     4.52±0.02ms      5.39±0.02ms     1.19  io.csv.ReadCSVDInferDatetimeFormat.time_read_csv(True, 'custom')
+     3.14±0.01ms      3.74±0.02ms     1.19  io.csv.ReadCSVFloatPrecision.time_read_csv_python_engine(';', '.', 'round_trip')
+      38.8±0.2ms       46.2±0.4ms     1.19  algorithms.Factorize.time_factorize_float(True)
+         659±2μs          783±3μs     1.19  groupby.GroupByMethods.time_dtype_as_field('datetime', 'value_counts', 'direct')
+     3.15±0.03μs       3.74±0.1μs     1.19  indexing.CategoricalIndexIndexing.time_getitem_scalar('non_monotonic')
+     9.08±0.05μs      10.8±0.04μs     1.19  timestamp.TimestampProperties.time_is_month_end(None, 'B')
+      63.9±0.2ms       75.8±0.9ms     1.19  indexing.NumericSeriesIndexing.time_ix_array(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
+     3.12±0.01ms      3.71±0.04ms     1.19  io.csv.ReadCSVFloatPrecision.time_read_csv_python_engine(',', '.', 'round_trip')
+         138±1μs          164±3μs     1.19  groupby.GroupByMethods.time_dtype_as_group('datetime', 'cumcount', 'direct')
+     1.87±0.01ms      2.22±0.02ms     1.19  stat_ops.FrameMultiIndexOps.time_op(1, 'sum')
+      15.6±0.3ms      18.5±0.08ms     1.19  strings.Methods.time_len
+         159±1μs          189±1μs     1.18  groupby.GroupByMethods.time_dtype_as_field('datetime', 'cumcount', 'direct')
+     5.12±0.05ms       6.06±0.2ms     1.18  strings.Cat.time_cat(0, None, '-', 0.0)
+     2.91±0.04ms      3.44±0.02ms     1.18  timeseries.ToDatetimeISO8601.time_iso8601
+         662±4μs          782±4μs     1.18  groupby.GroupByMethods.time_dtype_as_field('datetime', 'value_counts', 'transformation')
+     2.28±0.02ms      2.70±0.03ms     1.18  stat_ops.FrameMultiIndexOps.time_op(1, 'var')
+      14.3±0.1ms       16.9±0.8ms     1.18  gil.ParallelReadCSV.time_read_csv('object')
+      8.09±0.1ms       9.55±0.1ms     1.18  groupby.MultiColumn.time_col_select_numpy_sum
+         984±4ns      1.16±0.01μs     1.18  timestamp.TimestampConstruction.time_parse_iso8601_no_tz
+     2.85±0.05ms      3.36±0.01ms     1.18  timeseries.ToDatetimeISO8601.time_iso8601_format_no_sep
+      86.7±0.4ms          102±2ms     1.18  groupby.DateAttributes.time_len_groupby_object
+     2.07±0.01ms      2.43±0.02ms     1.18  series_methods.IsIn.time_isin('object')
+       159±0.5μs        187±0.5μs     1.18  groupby.GroupByMethods.time_dtype_as_field('float', 'cumcount', 'direct')
+     18.4±0.06μs       21.6±0.3μs     1.18  index_object.Indexing.time_get_loc_sorted('Float')
+     13.1±0.08ms         15.4±2ms     1.18  eval.Eval.time_mult('python', 1)
+     5.89±0.09ms      6.92±0.03ms     1.18  rolling.VariableWindowMethods.time_rolling('DataFrame', '1h', 'int', 'std')
+         159±1μs          187±2μs     1.18  groupby.GroupByMethods.time_dtype_as_field('float', 'cumcount', 'transformation')
+         155±2ms          182±3ms     1.17  binary_ops.Ops2.time_frame_int_div_by_zero
+     6.31±0.01ms      7.41±0.06ms     1.17  rolling.VariableWindowMethods.time_rolling('Series', '50s', 'int', 'std')
+     5.06±0.05ms      5.94±0.08ms     1.17  strings.Cat.time_cat(0, None, None, 0.0)
+      63.0±0.2μs       74.0±0.4μs     1.17  offset.OffestDatetimeArithmetic.time_add_10(<DateOffset: days=2, months=2>)
+     1.16±0.01ms      1.36±0.01ms     1.17  algorithms.Hashing.time_series_int
+     2.37±0.02ms      2.78±0.01ms     1.17  stat_ops.FrameMultiIndexOps.time_op(1, 'std')
+     4.50±0.04ms      5.28±0.01ms     1.17  groupby.Transform.time_transform_multi_key3
+     1.90±0.01ms      2.22±0.01ms     1.17  groupby.TransformNaN.time_first
+       163±0.6μs          191±1μs     1.17  groupby.GroupByMethods.time_dtype_as_group('int', 'cumcount', 'direct')
+         412±2μs          483±4μs     1.17  reindex.Reindex.time_reindex_columns
+     1.81±0.04ms      2.12±0.02ms     1.17  reindex.DropDuplicates.time_frame_drop_dups_int(True)
+         544±3μs          637±3μs     1.17  ctors.SeriesConstructors.time_series_constructor(<class 'list'>, True)
+     2.24±0.02ms      2.63±0.05ms     1.17  groupby.CountMultiInt.time_multi_int_count
+         160±2μs        187±0.4μs     1.17  groupby.GroupByMethods.time_dtype_as_field('int', 'cumcount', 'direct')
+     1.22±0.01ms      1.43±0.04ms     1.17  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 1, 'nearest')
+      18.1±0.2ms       21.2±0.1ms     1.17  reindex.DropDuplicates.time_frame_drop_dups_na(False)
+      20.6±0.4ms       24.1±0.2ms     1.17  stat_ops.SeriesMultiIndexOps.time_op(1, 'kurt')
+         529±2μs          619±8μs     1.17  ctors.SeriesConstructors.time_series_constructor(<class 'list'>, False)
+      53.6±0.3μs       62.6±0.3μs     1.17  frame_methods.XS.time_frame_xs(0)
+     6.26±0.05ms      7.31±0.04ms     1.17  rolling.VariableWindowMethods.time_rolling('Series', '50s', 'float', 'std')
+     5.34±0.05ms       6.23±0.1ms     1.17  strings.Cat.time_cat(0, ',', '-', 0.0)
+     6.25±0.06ms      7.29±0.02ms     1.17  rolling.VariableWindowMethods.time_rolling('Series', '1h', 'float', 'std')
+       159±0.4μs          185±2μs     1.17  groupby.GroupByMethods.time_dtype_as_field('object', 'cumcount', 'direct')
+     2.97±0.03ms      3.46±0.01ms     1.17  timeseries.ToDatetimeISO8601.time_iso8601_format
+         360±3μs          419±3μs     1.17  reindex.ReindexMethod.time_reindex_method('pad')
+         184±1μs          214±3μs     1.17  indexing.DataFrameStringIndexing.time_boolean_rows_object
+     1.99±0.03ms      2.31±0.05ms     1.16  stat_ops.FrameMultiIndexOps.time_op(0, 'prod')
+     2.14±0.04ms      2.49±0.06ms     1.16  timeseries.AsOf.time_asof_single('DataFrame')
+         386±9μs          449±3μs     1.16  timeseries.DatetimeIndex.time_unique('repeated')
+     7.98±0.02ms       9.27±0.2ms     1.16  ctors.MultiIndexConstructor.time_multiindex_from_iterables
+     1.17±0.02ms      1.36±0.01ms     1.16  algorithms.Hashing.time_series_float
+     1.94±0.01ms      2.25±0.04ms     1.16  stat_ops.SeriesMultiIndexOps.time_op(1, 'std')
+      6.15±0.2ms      7.12±0.06ms     1.16  rolling.VariableWindowMethods.time_rolling('Series', '1d', 'int', 'std')
+     1.90±0.04ms      2.20±0.03ms     1.16  stat_ops.SeriesMultiIndexOps.time_op(0, 'var')
+         306±1ms          354±9ms     1.16  stat_ops.FrameMultiIndexOps.time_op([0, 1], 'skew')
+         217±5μs          251±2μs     1.16  timeseries.DatetimeIndex.time_add_timedelta('dst')
+     8.92±0.04ms       10.3±0.1ms     1.16  stat_ops.SeriesMultiIndexOps.time_op(0, 'mad')
+     1.94±0.01ms      2.25±0.03ms     1.16  stat_ops.SeriesMultiIndexOps.time_op(0, 'std')
+     9.14±0.04ms      10.6±0.08ms     1.16  join_merge.Merge.time_merge_2intkey(False)
+     5.50±0.09ms       6.35±0.1ms     1.16  strings.Cat.time_cat(0, ',', None, 0.0)
+     2.05±0.03ms      2.37±0.04ms     1.16  binary_ops.Ops.time_frame_comparison(True, 1)
+         162±2μs          187±2μs     1.16  groupby.GroupByMethods.time_dtype_as_group('int', 'cumcount', 'transformation')
+     1.16±0.02ms         1.34±0ms     1.16  algorithms.Hashing.time_series_timedeltas
+     1.90±0.03ms      2.20±0.02ms     1.16  stat_ops.FrameMultiIndexOps.time_op(0, 'sum')
+     2.38±0.01ms      2.75±0.02ms     1.15  stat_ops.FrameMultiIndexOps.time_op(0, 'std')
+         209±4μs          241±5μs     1.15  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<MonthEnd>)
+      35.1±0.5μs       40.5±0.3μs     1.15  indexing.NumericSeriesIndexing.time_loc_scalar(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
+       117±0.9ms        135±0.7ms     1.15  replace.Convert.time_replace('Series', 'Timedelta')
+     6.10±0.05ms      7.04±0.02ms     1.15  rolling.VariableWindowMethods.time_rolling('Series', '1d', 'float', 'std')
+      63.3±0.4ms         73.1±2ms     1.15  indexing.NumericSeriesIndexing.time_ix_array(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'nonunique_monotonic_inc')
+     1.54±0.01ms      1.77±0.01ms     1.15  timeseries.ResampleDatetetime64.time_resample
+         160±2μs          184±1μs     1.15  groupby.GroupByMethods.time_dtype_as_field('object', 'cumcount', 'transformation')
+     1.16±0.01ms      1.33±0.01ms     1.15  algorithms.Hashing.time_series_dates
+     4.90±0.05ms       5.64±0.1ms     1.15  io.csv.ReadUint64Integers.time_read_uint64_neg_values
+     1.91±0.01ms      2.20±0.05ms     1.15  stat_ops.SeriesMultiIndexOps.time_op(1, 'var')
+         687±3ms          790±4ms     1.15  join_merge.MergeAsof.time_multiby
+         251±1μs          288±4μs     1.15  inference.NumericInferOps.time_subtract(<class 'numpy.uint32'>)
+         214±4μs          246±2μs     1.15  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<Day>)
+         181±1μs          208±2μs     1.15  strings.Encode.time_encode_decode
+      15.0±0.3ms         17.2±2ms     1.15  eval.Eval.time_mult('numexpr', 'all')
+         163±3μs          188±1μs     1.15  groupby.GroupByMethods.time_dtype_as_field('datetime', 'cumcount', 'transformation')
+         117±1ms        134±0.7ms     1.15  replace.Convert.time_replace('Series', 'Timestamp')
+      54.9±0.8ms       62.9±0.7ms     1.15  join_merge.MergeOrdered.time_merge_ordered
+     2.62±0.01ms      3.00±0.04ms     1.14  stat_ops.SeriesMultiIndexOps.time_op(1, 'sem')
+         204±1μs        233±0.5μs     1.14  groupby.GroupByMethods.time_dtype_as_group('object', 'nunique', 'transformation')
+         203±2μs          232±3μs     1.14  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<MonthBegin>)
+        1.02±0ms      1.16±0.01ms     1.14  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'mean')
+      29.0±0.6ms         33.1±2ms     1.14  eval.Eval.time_chained_cmp('python', 'all')
+     1.48±0.04ms      1.69±0.05ms     1.14  timeseries.DatetimeIndex.time_add_timedelta('tz_aware')
+       203±0.5μs          232±2μs     1.14  groupby.GroupByMethods.time_dtype_as_group('object', 'nunique', 'direct')
+     4.67±0.03ms       5.32±0.1ms     1.14  stat_ops.SeriesMultiIndexOps.time_op(0, 'skew')
+     4.72±0.03ms      5.38±0.04ms     1.14  stat_ops.SeriesMultiIndexOps.time_op(0, 'kurt')
+     4.84±0.04ms      5.50±0.02ms     1.14  join_merge.Merge.time_merge_dataframe_integer_2key(False)
+      19.3±0.2μs       21.9±0.3μs     1.14  indexing.NumericSeriesIndexing.time_getitem_scalar(<class 'pandas.core.indexes.numeric.Float64Index'>, 'nonunique_monotonic_inc')
+     2.64±0.01ms      3.00±0.02ms     1.14  stat_ops.SeriesMultiIndexOps.time_op(0, 'sem')
+         449±2μs          509±6μs     1.13  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<DateOffset: days=2, months=2>)
+     1.47±0.01ms      1.67±0.02ms     1.13  series_methods.IsInForObjects.time_isin_long_series_short_values
+        1.23±0ms         1.39±0ms     1.13  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0, 'nearest')
+         258±2μs          292±6μs     1.13  inference.NumericInferOps.time_add(<class 'numpy.uint32'>)
+         493±6μs          558±4μs     1.13  indexing.DataFrameNumericIndexing.time_bool_indexer
+     1.07±0.01ms      1.21±0.01ms     1.13  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'mean')
+     1.82±0.02ms      2.05±0.02ms     1.13  rolling.VariableWindowMethods.time_rolling('DataFrame', '50s', 'float', 'sum')
+     5.86±0.03ms      6.62±0.02ms     1.13  stat_ops.FrameMultiIndexOps.time_op([0, 1], 'sem')
+       107±0.4μs          121±1μs     1.13  inference.ToNumericDowncast.time_downcast('int32', 'float')
+        846±20μs         956±20μs     1.13  series_methods.IsInForObjects.time_isin_short_series_long_values
+      65.6±0.5μs       74.1±0.3μs     1.13  indexing.DataFrameNumericIndexing.time_loc
+     1.23±0.01ms      1.39±0.01ms     1.13  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0, 'nearest')
+      19.6±0.2ms       22.1±0.3ms     1.13  eval.Eval.time_chained_cmp('python', 1)
+     1.83±0.02ms      2.07±0.04ms     1.13  rolling.VariableWindowMethods.time_rolling('DataFrame', '1d', 'int', 'sum')
+      2.05±0.01s       2.31±0.02s     1.13  groupby.GroupByMethods.time_dtype_as_group('float', 'describe', 'direct')
+     1.81±0.02ms      2.04±0.03ms     1.13  rolling.VariableWindowMethods.time_rolling('DataFrame', '1h', 'float', 'sum')
+     1.61±0.05ms      1.82±0.01ms     1.13  groupby.GroupManyLabels.time_sum(1)
+     1.23±0.01ms      1.39±0.01ms     1.13  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0, 'midpoint')
+      6.53±0.1ms      7.36±0.03ms     1.13  rolling.VariableWindowMethods.time_rolling('Series', '1h', 'int', 'std')
+     1.24±0.01ms         1.39±0ms     1.13  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0, 'lower')
+     13.1±0.06ms       14.7±0.2ms     1.13  eval.Eval.time_add('python', 1)
+     1.39±0.02μs      1.57±0.01μs     1.13  timestamp.TimestampConstruction.time_parse_today
+         885±3ms         995±20ms     1.13  groupby.GroupByMethods.time_dtype_as_field('int', 'describe', 'transformation')
+     1.26±0.04ms      1.42±0.04ms     1.12  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 1, 'linear')
+        1.32±0ms      1.48±0.02ms     1.12  rolling.Methods.time_rolling('Series', 10, 'float', 'sum')
+      69.5±0.5ms       78.2±0.7ms     1.12  io.hdf.HDFStoreDataFrame.time_read_store_table_mixed
+     1.23±0.01ms         1.38±0ms     1.12  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 1, 'midpoint')
+        1.23±0ms      1.38±0.01ms     1.12  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 1, 'lower')
+     1.98±0.01ms      2.22±0.03ms     1.12  binary_ops.Timeseries.time_series_timestamp_compare('US/Eastern')
+     1.04±0.01ms      1.17±0.01ms     1.12  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'mean')
+     1.86±0.01ms      2.09±0.02ms     1.12  rolling.VariableWindowMethods.time_rolling('DataFrame', '50s', 'int', 'sum')
+         1.31±0s       1.47±0.05s     1.12  groupby.GroupByMethods.time_dtype_as_group('int', 'describe', 'direct')
+     1.23±0.01ms         1.38±0ms     1.12  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 1, 'higher')
+     1.07±0.01ms         1.20±0ms     1.12  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'mean')
+        1.24±0ms      1.39±0.01ms     1.12  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0, 'linear')
+     5.05±0.02ms      5.66±0.02ms     1.12  categoricals.Concat.time_concat
+       228±0.8μs          255±1μs     1.12  groupby.GroupByMethods.time_dtype_as_field('datetime', 'nunique', 'direct')
+     4.96±0.09ms       5.56±0.1ms     1.12  io.csv.ReadUint64Integers.time_read_uint64_na_values
+     2.78±0.03ms      3.11±0.04ms     1.12  gil.ParallelRolling.time_rolling('mean')
+      58.6±0.5ms       65.6±0.2ms     1.12  frame_ctor.FromDicts.time_nested_dict_int64
+     1.36±0.01ms      1.52±0.02ms     1.12  rolling.Methods.time_rolling('Series', 1000, 'int', 'sum')
+      19.7±0.2ms       22.0±0.8ms     1.12  frame_methods.Iteration.time_iteritems
+        1.15±0ms      1.28±0.06ms     1.12  index_object.Ops.time_divide('float')
+        1.32±0ms      1.48±0.01ms     1.12  rolling.Methods.time_rolling('Series', 1000, 'float', 'sum')
+     1.97±0.01ms      2.20±0.02ms     1.12  binary_ops.Timeseries.time_timestamp_series_compare('US/Eastern')
+      2.06±0.01s       2.30±0.01s     1.12  groupby.GroupByMethods.time_dtype_as_group('float', 'describe', 'transformation')
+     1.97±0.03ms      2.20±0.02ms     1.12  timeseries.ResampleSeries.time_resample('period', '5min', 'ohlc')
+     1.65±0.02ms      1.85±0.03ms     1.12  timeseries.ResampleSeries.time_resample('period', '1D', 'ohlc')
+     1.36±0.01ms      1.52±0.01ms     1.12  rolling.Methods.time_rolling('Series', 10, 'int', 'sum')
+     1.23±0.01ms      1.38±0.01ms     1.12  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0, 'higher')
+     1.23±0.01ms      1.37±0.01ms     1.12  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 1, 'nearest')
+      22.3±0.2ms       24.9±0.1ms     1.12  join_merge.Merge.time_merge_2intkey(True)
+     1.80±0.02ms      2.01±0.03ms     1.12  rolling.VariableWindowMethods.time_rolling('DataFrame', '1d', 'float', 'sum')
+         989±6μs      1.10±0.01ms     1.12  replace.FillNa.time_replace(True)
+      2.30±0.02s       2.56±0.01s     1.11  io.json.ReadJSON.time_read_json('index', 'int')
+     2.20±0.01ms      2.45±0.06ms     1.11  groupby.TransformBools.time_transform_mean
+        1.24±0ms      1.38±0.01ms     1.11  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 1, 'lower')
+     1.87±0.02ms      2.08±0.01ms     1.11  rolling.VariableWindowMethods.time_rolling('DataFrame', '1h', 'int', 'sum')
+        1.23±0ms      1.37±0.01ms     1.11  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 1, 'higher')
+      12.0±0.3μs      13.4±0.07μs     1.11  indexing.NumericSeriesIndexing.time_getitem_scalar(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
+      2.30±0.03s       2.56±0.03s     1.11  io.json.ReadJSON.time_read_json('index', 'datetime')
+      6.92±0.1ms       7.69±0.2ms     1.11  binary_ops.Ops.time_frame_mult(True, 'default')
+     1.25±0.01ms      1.39±0.01ms     1.11  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0, 'midpoint')
+      1.32±0.01s       1.46±0.01s     1.11  groupby.GroupByMethods.time_dtype_as_group('int', 'describe', 'transformation')
+     10.0±0.06ms       11.1±0.5ms     1.11  eval.Eval.time_mult('numexpr', 1)
+       303±0.6μs          337±2μs     1.11  groupby.GroupByMethods.time_dtype_as_group('int', 'nunique', 'transformation')
+     1.24±0.01ms         1.38±0ms     1.11  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0, 'lower')
+     1.24±0.01ms         1.37±0ms     1.11  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 1, 'midpoint')
+     1.75±0.03ms      1.94±0.01ms     1.11  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 1, 'midpoint')
+         250±4μs          277±2μs     1.11  reindex.Reindex.time_reindex_dates
+         897±4ms         993±10ms     1.11  groupby.GroupByMethods.time_dtype_as_field('int', 'describe', 'direct')
+       161±0.5μs        178±0.5μs     1.11  reindex.Fillna.time_float_32('backfill')
+      17.3±0.4ms       19.1±0.1ms     1.11  frame_ctor.FromDictwithTimestamp.time_dict_with_timestamp_offsets(<Nano>)
+      45.0±0.5ms       49.7±0.7ms     1.10  stat_ops.FrameMultiIndexOps.time_op(1, 'skew')
+      20.7±0.3ms       22.9±0.1ms     1.10  frame_ctor.FromDictwithTimestamp.time_dict_with_timestamp_offsets(<Hour>)
+     1.60±0.01ms      1.76±0.04ms     1.10  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0, 'lower')
+      22.3±0.3ms       24.6±0.4ms     1.10  frame_methods.SortIndexByColumns.time_frame_sort_values_by_columns
+         303±2μs          334±3μs     1.10  groupby.GroupByMethods.time_dtype_as_group('int', 'nunique', 'direct')
+         238±4ms         262±10ms     1.10  frame_methods.GetDtypeCounts.time_info
+         298±1μs          328±5μs     1.10  groupby.GroupByMethods.time_dtype_as_group('datetime', 'nunique', 'direct')
+         894±6ms          984±8ms     1.10  groupby.GroupByMethods.time_dtype_as_field('float', 'describe', 'direct')


SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
```

Speedups
<details>

```
-         246±2ns        224±0.8ns     0.91  timedelta.TimedeltaProperties.time_timedelta_nanoseconds
-      42.2±0.1ms       38.3±0.2ms     0.91  io.sql.WriteSQLDtypes.time_to_sql_dataframe_column('sqlalchemy', 'bool')
-        795±10μs          722±7μs     0.91  indexing.NumericSeriesIndexing.time_ix_array(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'unique_monotonic_inc')
-     7.47±0.05μs      6.78±0.06μs     0.91  offset.OnOffset.time_on_offset(<BusinessMonthBegin>)
-        70.4±3ms       63.8±0.2ms     0.91  join_merge.ConcatDataFrames.time_f_ordered(1, False)
-      33.9±0.3μs       30.7±0.9μs     0.91  offset.OffestDatetimeArithmetic.time_subtract_10(<Day>)
-     2.14±0.01ms      1.94±0.04ms     0.91  groupby.Datelike.time_sum('date_range_tz')
-      8.59±0.1μs       7.74±0.1μs     0.90  indexing.NumericSeriesIndexing.time_iloc_scalar(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
-      40.2±0.2ms      36.0±0.08ms     0.90  io.sql.WriteSQLDtypes.time_to_sql_dataframe_column('sqlalchemy', 'string')
-     6.35±0.09μs      5.69±0.06μs     0.90  timedelta.TimedeltaConstructor.time_from_datetime_timedelta
-     39.3±0.07ms      35.2±0.05ms     0.90  io.sql.WriteSQLDtypes.time_to_sql_dataframe_column('sqlalchemy', 'float')
-         133±3ms          119±4ms     0.89  gil.ParallelGroupbyMethods.time_loop(4, 'var')
-      39.0±0.1ms       34.8±0.2ms     0.89  io.sql.WriteSQLDtypes.time_to_sql_dataframe_column('sqlalchemy', 'int')
-       143±0.9ms          128±1ms     0.89  categoricals.Constructor.time_with_nan
-     2.71±0.02ms      2.42±0.04ms     0.89  frame_methods.NSort.time_nlargest_one_column('last')
-      28.4±0.3ms       25.3±0.2ms     0.89  stat_ops.FrameOps.time_op('kurt', 'int', 1, False)
-      7.94±0.2μs      7.06±0.07μs     0.89  offset.OnOffset.time_on_offset(<SemiMonthEnd: day_of_month=15>)
-        97.7±1μs       86.7±0.5μs     0.89  index_object.SetOperations.time_operation('datetime', 'union')
-     8.58±0.04μs      7.59±0.05μs     0.88  indexing.NumericSeriesIndexing.time_iloc_scalar(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'nonunique_monotonic_inc')
-     8.56±0.04μs      7.54±0.03μs     0.88  indexing.NumericSeriesIndexing.time_iloc_scalar(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'unique_monotonic_inc')
-        1.13±0ms          994±4μs     0.88  indexing.CategoricalIndexIndexing.time_getitem_bool_array('non_monotonic')
-      5.51±0.1ms      4.84±0.03ms     0.88  timeseries.Factorize.time_factorize(None)
-      8.63±0.1μs      7.59±0.04μs     0.88  indexing.NumericSeriesIndexing.time_iloc_scalar(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
-        72.2±2ms       63.4±0.3ms     0.88  join_merge.ConcatDataFrames.time_f_ordered(1, True)
-      16.4±0.5μs       14.4±0.2μs     0.88  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<BusinessDay>)
-      20.5±0.3ms       17.8±0.2ms     0.87  timeseries.DatetimeIndex.time_normalize('tz_aware')
-      18.3±0.4μs       15.9±0.2μs     0.87  offset.OffestDatetimeArithmetic.time_add_10(<BusinessDay>)
-         114±3ms         98.5±3ms     0.86  gil.ParallelGroupbyMethods.time_loop(4, 'prod')
-       125±0.9μs          107±1μs     0.86  offset.OffestDatetimeArithmetic.time_add(<CustomBusinessMonthBegin>)
-     14.1±0.06μs      12.1±0.05μs     0.86  offset.OffestDatetimeArithmetic.time_apply(<BusinessDay>)
-         139±2μs          119±2μs     0.86  offset.OffestDatetimeArithmetic.time_add_10(<CustomBusinessMonthBegin>)
-      18.6±0.1μs       15.9±0.4μs     0.86  timeseries.AsOf.time_asof_single('Series')
-      9.41±0.1ms       8.03±0.2ms     0.85  strings.Cat.time_cat(0, ',', None, 0.001)
-       124±0.2μs        105±0.5μs     0.85  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<CustomBusinessMonthBegin>)
-         192±1ms          163±1ms     0.85  io.stata.Stata.time_read_stata('tc')
-         193±1ms          163±3ms     0.84  io.stata.Stata.time_read_stata('td')
-      9.00±0.3μs      7.57±0.05μs     0.84  timeseries.AsOf.time_asof_single_early('Series')
-         105±1ms         88.6±1ms     0.84  indexing.NumericSeriesIndexing.time_loc_list_like(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'unique_monotonic_inc')
-      36.2±0.2ms       30.4±0.2ms     0.84  plotting.TimeseriesPlotting.time_plot_irregular
-      5.95±0.1ms      4.97±0.09ms     0.84  timeseries.Factorize.time_factorize('Asia/Tokyo')
-       122±0.8μs        101±0.3μs     0.83  offset.OffestDatetimeArithmetic.time_apply(<CustomBusinessMonthBegin>)
-      37.4±0.3μs       30.9±0.1μs     0.83  indexing.NumericSeriesIndexing.time_loc_scalar(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
-       129±0.9ms          107±2ms     0.82  offset.OffsetSeriesArithmetic.time_add_offset(<CustomBusinessMonthBegin>)
-      35.4±0.5ms       28.9±0.5ms     0.82  plotting.TimeseriesPlotting.time_plot_regular_compat
-         130±1ms        106±0.8ms     0.81  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<CustomBusinessMonthBegin>)
-      15.1±0.3μs       12.3±0.2μs     0.81  timedelta.TimedeltaConstructor.time_from_components
-      23.5±0.1μs      19.0±0.03μs     0.81  offset.OffestDatetimeArithmetic.time_subtract_10(<BusinessDay>)
-     4.69±0.03ms       3.79±0.2ms     0.81  frame_methods.NSort.time_nsmallest_two_columns('first')
-        11.1±2ms       8.93±0.2ms     0.80  strings.Cat.time_cat(0, ',', None, 0.15)
-         120±1μs         95.9±2μs     0.80  offset.OffestDatetimeArithmetic.time_subtract_10(<CustomBusinessMonthBegin>)
-         116±1μs       92.1±0.6μs     0.79  offset.OffestDatetimeArithmetic.time_subtract(<CustomBusinessMonthBegin>)
-      16.3±0.4μs       12.9±0.1μs     0.79  offset.OffestDatetimeArithmetic.time_add(<BusinessDay>)
-      9.86±0.2ms      7.81±0.03ms     0.79  series_methods.ValueCounts.time_value_counts('object')
-     4.80±0.02ms      3.76±0.07ms     0.78  frame_methods.NSort.time_nsmallest_two_columns('last')
-         365±5ns          284±2ns     0.78  indexing.MethodLookup.time_lookup_iloc
-     1.83±0.05ms      1.42±0.02ms     0.77  series_methods.NSort.time_nlargest('last')
-         140±2ms          108±3ms     0.77  reshape.Unstack.time_without_last_row
-         373±3ns          284±2ns     0.76  indexing.MethodLookup.time_lookup_loc
-     1.25±0.01ms         946±20μs     0.76  stat_ops.SeriesOps.time_op('median', 'int', True)
-     4.99±0.04μs      3.78±0.05μs     0.76  timeseries.DatetimeIndex.time_get('dst')
-      19.0±0.1ms       14.3±0.4ms     0.76  algorithms.Factorize.time_factorize_int(True)
-     1.25±0.01ms          941±8μs     0.75  stat_ops.SeriesOps.time_op('median', 'int', False)
-     5.04±0.09μs      3.79±0.04μs     0.75  timeseries.DatetimeIndex.time_get('tz_naive')
-      20.9±0.2ms       15.6±0.6ms     0.75  index_object.SetOperations.time_operation('strings', 'symmetric_difference')
-        10.8±1ms       8.05±0.2ms     0.74  timeseries.AsOf.time_asof_nan('DataFrame')
-     1.78±0.01ms      1.33±0.02ms     0.74  series_methods.NSort.time_nlargest('first')
-        94.6±1ms         69.9±4ms     0.74  frame_methods.Describe.time_series_describe
-         319±1ms          235±6ms     0.74  frame_methods.Describe.time_dataframe_describe
-      11.9±0.1ms      8.78±0.02ms     0.74  io.hdf.HDFStoreDataFrame.time_store_info
-       103±0.2μs       74.6±0.3μs     0.73  indexing.NumericSeriesIndexing.time_loc_scalar(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'unique_monotonic_inc')
-      21.7±0.2μs       15.8±0.2μs     0.73  offset.OffestDatetimeArithmetic.time_subtract(<YearBegin: month=1>)
-      54.1±0.8μs       39.1±0.3μs     0.72  indexing.NumericSeriesIndexing.time_loc_scalar(<class 'pandas.core.indexes.numeric.Float64Index'>, 'unique_monotonic_inc')
-       736±200μs          527±1μs     0.72  timeseries.InferFreq.time_infer_freq(None)
-     5.11±0.09ms      3.66±0.02ms     0.72  offset.OnOffset.time_on_offset(<CustomBusinessMonthBegin>)
-         188±2ms        133±0.3ms     0.71  timeseries.DatetimeIndex.time_to_pydatetime('tz_aware')
-     1.08±0.01ms         765±10μs     0.71  indexing.NumericSeriesIndexing.time_getitem_array(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
-      10.9±0.1ms      7.59±0.02ms     0.70  inference.DateInferOps.time_subtract_datetimes
-      22.2±0.1μs       15.5±0.2μs     0.70  offset.OffestDatetimeArithmetic.time_subtract(<YearEnd: month=12>)
-     2.37±0.02ms      1.64±0.03ms     0.69  groupby.RankWithTies.time_rank_ties('int64', 'dense')
-      21.2±0.5μs      14.6±0.09μs     0.69  offset.OffestDatetimeArithmetic.time_add_10(<YearBegin: month=1>)
-         282±3ns          194±1ns     0.69  timedelta.TimedeltaProperties.time_timedelta_days
-     2.37±0.01ms      1.63±0.01ms     0.69  groupby.RankWithTies.time_rank_ties('int64', 'min')
-        1.74±0ms      1.19±0.01ms     0.69  series_methods.NSort.time_nsmallest('first')
-         145±2ms       99.2±0.5ms     0.68  indexing.NumericSeriesIndexing.time_getitem_lists(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
-     11.1±0.06ms       7.55±0.1ms     0.68  frame_methods.ToString.time_to_string_floats
-      1.14±0.3ms          777±4μs     0.68  timeseries.InferFreq.time_infer_freq('B')
-     2.37±0.03ms      1.61±0.01ms     0.68  groupby.RankWithTies.time_rank_ties('int64', 'average')
-     2.38±0.01ms      1.62±0.02ms     0.68  groupby.RankWithTies.time_rank_ties('int64', 'max')
-     2.40±0.01ms      1.63±0.01ms     0.68  groupby.RankWithTies.time_rank_ties('float32', 'first')
-        17.0±1ms       11.4±0.1ms     0.67  categoricals.ValueCounts.time_value_counts(False)
-     2.41±0.02ms      1.61±0.01ms     0.67  groupby.RankWithTies.time_rank_ties('int64', 'first')
-     2.41±0.02ms      1.61±0.01ms     0.67  groupby.RankWithTies.time_rank_ties('float64', 'first')
-     1.54±0.01ms      1.03±0.01ms     0.67  series_methods.NSort.time_nsmallest('last')
-     6.44±0.03ms      4.28±0.05ms     0.67  stat_ops.FrameOps.time_op('median', 'int', 0, False)
-         173±1μs        115±0.7μs     0.66  offset.OffestDatetimeArithmetic.time_subtract(<CustomBusinessMonthEnd>)
-     2.47±0.02ms      1.63±0.01ms     0.66  groupby.RankWithTies.time_rank_ties('float64', 'dense')
-     6.53±0.03ms      4.30±0.04ms     0.66  stat_ops.FrameOps.time_op('median', 'int', 0, True)
-     2.46±0.01ms      1.62±0.02ms     0.66  groupby.RankWithTies.time_rank_ties('float64', 'max')
-     2.46±0.02ms      1.61±0.01ms     0.66  groupby.RankWithTies.time_rank_ties('float64', 'min')
-     2.44±0.01ms      1.59±0.01ms     0.65  groupby.RankWithTies.time_rank_ties('datetime64', 'dense')
-     2.48±0.01ms      1.62±0.01ms     0.65  groupby.RankWithTies.time_rank_ties('float64', 'average')
-     2.48±0.02ms      1.61±0.01ms     0.65  groupby.RankWithTies.time_rank_ties('float32', 'max')
-     2.46±0.02ms      1.60±0.01ms     0.65  groupby.RankWithTies.time_rank_ties('datetime64', 'average')
-     2.45±0.02ms      1.59±0.02ms     0.65  groupby.RankWithTies.time_rank_ties('datetime64', 'min')
-        2.49±0ms      1.61±0.01ms     0.65  groupby.RankWithTies.time_rank_ties('float32', 'min')
-     2.50±0.02ms      1.61±0.01ms     0.64  groupby.RankWithTies.time_rank_ties('float32', 'dense')
-     2.49±0.03ms      1.60±0.02ms     0.64  groupby.RankWithTies.time_rank_ties('datetime64', 'first')
-     2.51±0.03ms         1.61±0ms     0.64  groupby.RankWithTies.time_rank_ties('float32', 'average')
-         304±2ns        195±0.9ns     0.64  timedelta.TimedeltaProperties.time_timedelta_microseconds
-     5.25±0.05ms      3.35±0.02ms     0.64  offset.OnOffset.time_on_offset(<CustomBusinessMonthEnd>)
-        948±20μs         605±10μs     0.64  indexing.NumericSeriesIndexing.time_loc_array(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
-     2.53±0.07ms      1.61±0.02ms     0.64  groupby.RankWithTies.time_rank_ties('datetime64', 'max')
-     3.32±0.01ms      2.10±0.01ms     0.63  ctors.SeriesConstructors.time_series_constructor(<function SeriesConstructors.<lambda> at 0x11332b6a8>, True)
-         105±1ms       66.0±0.6ms     0.63  index_object.IndexAppend.time_append_int_list
-      25.7±0.2μs      16.1±0.08μs     0.62  offset.OffestDatetimeArithmetic.time_subtract(<QuarterEnd: startingMonth=3>)
-     2.38±0.01ms      1.47±0.01ms     0.62  period.Algorithms.time_value_counts('series')
-      29.1±0.5μs       17.9±0.1μs     0.62  offset.OffestDatetimeArithmetic.time_subtract_10(<QuarterEnd: startingMonth=3>)
-         471±3ms          284±2ms     0.60  indexing.NonNumericSeriesIndexing.time_getitem_list_like('datetime', 'nonunique_monotonic_inc')
-       141±0.7μs       84.5±0.3μs     0.60  offset.OffestDatetimeArithmetic.time_subtract_10(<CustomBusinessMonthEnd>)
-      50.1±0.5μs       30.0±0.5μs     0.60  categoricals.IsMonotonic.time_categorical_series_is_monotonic_increasing
-      26.8±0.2μs       15.9±0.3μs     0.59  offset.OffestDatetimeArithmetic.time_subtract(<BusinessQuarterEnd: startingMonth=3>)
-      49.7±0.6μs       29.0±0.2μs     0.58  categoricals.IsMonotonic.time_categorical_series_is_monotonic_decreasing
-     30.7±0.08μs       17.8±0.1μs     0.58  offset.OffestDatetimeArithmetic.time_subtract_10(<BusinessQuarterEnd: startingMonth=3>)
-     3.13±0.03ms      1.81±0.02ms     0.58  indexing.DataFrameNumericIndexing.time_loc_dups
-      23.9±0.7μs       13.8±0.1μs     0.58  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<BusinessQuarterEnd: startingMonth=3>)
-      29.7±0.4μs      17.2±0.09μs     0.58  offset.OffestDatetimeArithmetic.time_subtract_10(<YearBegin: month=1>)
-       137±0.5μs       78.2±0.9μs     0.57  offset.OffestDatetimeArithmetic.time_add_10(<CustomBusinessMonthEnd>)
-      26.6±0.3μs       15.2±0.3μs     0.57  offset.OffestDatetimeArithmetic.time_add_10(<QuarterEnd: startingMonth=3>)
-       359±0.9ms          205±2ms     0.57  reindex.Reindex.time_reindex_multiindex
-      23.8±0.2μs       13.6±0.5μs     0.57  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<QuarterEnd: startingMonth=3>)
-        20.2±1ms       11.5±0.1ms     0.57  categoricals.ValueCounts.time_value_counts(True)
-     20.2±0.07μs      11.5±0.05μs     0.57  offset.OffestDatetimeArithmetic.time_apply(<QuarterEnd: startingMonth=3>)
-         346±2ns          196±1ns     0.57  timedelta.TimedeltaProperties.time_timedelta_seconds
-      86.2±0.8ms         48.6±3ms     0.56  reshape.Unstack.time_full_product
-      41.5±0.2ms       23.3±0.2ms     0.56  categoricals.Constructor.time_all_nan
-         379±1ms          211±2ms     0.56  series_methods.SeriesConstructor.time_constructor('dict')
-         544±7ns          303±2ns     0.56  timestamp.TimestampProperties.time_freqstr(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, 'B')
-     13.6±0.07ms       7.37±0.4ms     0.54  binary_ops.Timeseries.time_timestamp_ops_diff('US/Eastern')
-       141±0.6ms         75.9±1ms     0.54  indexing.NumericSeriesIndexing.time_getitem_array(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
-     22.7±0.08μs      12.2±0.06μs     0.54  offset.OffestDatetimeArithmetic.time_add(<QuarterEnd: startingMonth=3>)
-      21.4±0.2μs       11.4±0.2μs     0.53  offset.OffestDatetimeArithmetic.time_apply(<BusinessQuarterEnd: startingMonth=3>)
-       141±0.6ms       75.4±0.6ms     0.53  indexing.NumericSeriesIndexing.time_loc_array(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
-         294±3μs          156±2μs     0.53  indexing.CategoricalIndexIndexing.time_getitem_bool_array('monotonic_incr')
-      23.8±0.1μs      12.5±0.05μs     0.53  offset.OffestDatetimeArithmetic.time_add(<BusinessQuarterEnd: startingMonth=3>)
-       128±0.9ms         66.8±2ms     0.52  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<CustomBusinessMonthEnd>)
-        582±50ns          303±4ns     0.52  timestamp.TimestampProperties.time_freqstr(None, 'B')
-       126±0.8μs       65.1±0.3μs     0.52  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<CustomBusinessMonthEnd>)
-      31.3±0.2μs      16.0±0.04μs     0.51  offset.OffestDatetimeArithmetic.time_subtract(<QuarterBegin: startingMonth=3>)
-      35.3±0.2μs       18.0±0.3μs     0.51  offset.OffestDatetimeArithmetic.time_subtract_10(<QuarterBegin: startingMonth=3>)
-         129±1ms       64.8±0.8ms     0.50  offset.OffsetSeriesArithmetic.time_add_offset(<CustomBusinessMonthEnd>)
-         122±1μs       61.3±0.9μs     0.50  offset.OffestDatetimeArithmetic.time_apply(<CustomBusinessMonthEnd>)
-       124±0.6μs       62.1±0.4μs     0.50  offset.OffestDatetimeArithmetic.time_add(<CustomBusinessMonthEnd>)
-      30.2±0.1μs      14.9±0.05μs     0.49  offset.OffestDatetimeArithmetic.time_add_10(<YearEnd: month=12>)
-      32.7±0.6μs       16.2±0.1μs     0.49  offset.OffestDatetimeArithmetic.time_subtract(<BusinessQuarterBegin: startingMonth=3>)
-      36.2±0.2μs       17.7±0.1μs     0.49  offset.OffestDatetimeArithmetic.time_subtract_10(<BusinessQuarterBegin: startingMonth=3>)
-      31.7±0.1μs       15.4±0.2μs     0.49  offset.OffestDatetimeArithmetic.time_add_10(<QuarterBegin: startingMonth=3>)
-     1.39±0.01ms          671±8μs     0.48  series_methods.Map.time_map('dict')
-      95.7±0.5ms       45.8±0.3ms     0.48  rolling.Quantile.time_quantile('Series', 10, 'float', 0.5, 'linear')
-      95.6±0.8ms       45.4±0.4ms     0.48  rolling.Quantile.time_quantile('Series', 10, 'float', 0.5, 'midpoint')
-      28.3±0.3μs       13.4±0.1μs     0.47  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<MonthBegin>)
-      28.7±0.2μs      13.6±0.06μs     0.47  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<QuarterBegin: startingMonth=3>)
-      36.9±0.1μs       17.5±0.1μs     0.47  offset.OffestDatetimeArithmetic.time_subtract_10(<YearEnd: month=12>)
-      95.2±0.4ms       44.4±0.2ms     0.47  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0.5, 'midpoint')
-      96.3±0.4ms       44.8±0.6ms     0.47  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0.5, 'linear')
-      95.2±0.7ms       44.1±0.5ms     0.46  rolling.Quantile.time_quantile('Series', 10, 'float', 0.5, 'higher')
-      28.8±0.3μs      13.3±0.07μs     0.46  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<BusinessYearBegin: month=1>)
-      28.7±0.3μs      13.2±0.06μs     0.46  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<BusinessMonthEnd>)
-        95.3±1ms       43.9±0.3ms     0.46  rolling.Quantile.time_quantile('Series', 10, 'float', 0.5, 'lower')
-      30.0±0.2μs      13.7±0.08μs     0.46  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<BusinessQuarterBegin: startingMonth=3>)
-      33.7±0.3μs       15.4±0.6μs     0.46  offset.OffestDatetimeArithmetic.time_add_10(<BusinessQuarterBegin: startingMonth=3>)
-      95.3±0.1ms       43.1±0.3ms     0.45  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0.5, 'higher')
-      96.0±0.3ms      43.4±0.09ms     0.45  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0.5, 'nearest')
-      95.4±0.5ms       43.0±0.5ms     0.45  rolling.Quantile.time_quantile('Series', 10, 'float', 0.5, 'nearest')
-      57.8±0.8ms         26.1±1ms     0.45  binary_ops.Timeseries.time_timestamp_ops_diff_with_shift(None)
-      95.4±0.3ms       42.9±0.4ms     0.45  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0.5, 'lower')
-     1.52±0.02ms          683±3μs     0.45  offset.OffsetSeriesArithmetic.time_add_offset(<QuarterBegin: startingMonth=3>)
-      95.1±0.3ms       42.6±0.5ms     0.45  rolling.Quantile.time_quantile('Series', 10, 'int', 0.5, 'linear')
-     29.0±0.06μs       12.9±0.2μs     0.45  offset.OffestDatetimeArithmetic.time_add(<BusinessQuarterBegin: startingMonth=3>)
-     4.48±0.03ms      1.99±0.03ms     0.44  ctors.SeriesConstructors.time_series_constructor(<function SeriesConstructors.<lambda> at 0x11332b6a8>, False)
-      32.3±0.1μs       14.3±0.3μs     0.44  offset.OffestDatetimeArithmetic.time_add_10(<MonthBegin>)
-      95.9±0.7ms       42.3±0.2ms     0.44  rolling.Quantile.time_quantile('Series', 10, 'int', 0.5, 'midpoint')
-      95.2±0.6ms       41.9±0.6ms     0.44  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0.5, 'linear')
-     25.3±0.05μs      11.1±0.06μs     0.44  offset.OffestDatetimeArithmetic.time_apply(<QuarterBegin: startingMonth=3>)
-      36.4±0.6μs       16.0±0.5μs     0.44  offset.OffestDatetimeArithmetic.time_subtract_10(<MonthBegin>)
-     28.1±0.05μs       12.3±0.2μs     0.44  offset.OffestDatetimeArithmetic.time_add(<QuarterBegin: startingMonth=3>)
-      25.2±0.4μs       11.0±0.2μs     0.44  offset.OffestDatetimeArithmetic.time_apply(<BusinessMonthEnd>)
-      95.9±0.3ms       41.5±0.1ms     0.43  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0.5, 'midpoint')
-      26.3±0.2μs       11.4±0.1μs     0.43  offset.OffestDatetimeArithmetic.time_apply(<BusinessQuarterBegin: startingMonth=3>)
-     1.56±0.02ms          675±3μs     0.43  offset.OffsetSeriesArithmetic.time_add_offset(<YearBegin: month=1>)
-      33.5±0.3μs       14.5±0.3μs     0.43  offset.OffestDatetimeArithmetic.time_subtract(<MonthBegin>)
-      25.0±0.3μs      10.8±0.03μs     0.43  offset.OffestDatetimeArithmetic.time_apply(<MonthBegin>)
-      94.4±0.8ms       40.6±0.4ms     0.43  rolling.Quantile.time_quantile('Series', 10, 'int', 0.5, 'nearest')
-      33.8±0.2μs       14.5±0.4μs     0.43  offset.OffestDatetimeArithmetic.time_add_10(<BusinessMonthEnd>)
-      95.3±0.8ms       40.7±0.4ms     0.43  rolling.Quantile.time_quantile('Series', 10, 'int', 0.5, 'lower')
-      95.4±0.3ms       40.6±0.9ms     0.43  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0.5, 'higher')
-      95.9±0.9ms       40.7±0.1ms     0.42  rolling.Quantile.time_quantile('Series', 10, 'int', 0.5, 'higher')
-      28.1±0.2μs       11.9±0.1μs     0.42  offset.OffestDatetimeArithmetic.time_add(<MonthBegin>)
-      34.9±0.4μs       14.7±0.3μs     0.42  offset.OffestDatetimeArithmetic.time_subtract(<BusinessMonthEnd>)
-     28.8±0.08μs      12.1±0.04μs     0.42  offset.OffestDatetimeArithmetic.time_add(<BusinessYearBegin: month=1>)
-      95.3±0.8ms      39.9±0.08ms     0.42  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0.5, 'lower')
-     31.9±0.04μs       13.4±0.1μs     0.42  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<BusinessMonthBegin>)
-      28.6±0.5μs       11.9±0.2μs     0.42  offset.OffestDatetimeArithmetic.time_add(<BusinessMonthEnd>)
-      26.5±0.2μs      11.0±0.07μs     0.42  offset.OffestDatetimeArithmetic.time_apply(<BusinessYearBegin: month=1>)
-      38.9±0.3μs      16.1±0.08μs     0.41  offset.OffestDatetimeArithmetic.time_subtract_10(<BusinessMonthBegin>)
-      95.7±0.3ms       39.5±0.4ms     0.41  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0.5, 'nearest')
-      38.7±0.5μs       15.9±0.1μs     0.41  offset.OffestDatetimeArithmetic.time_subtract_10(<BusinessMonthEnd>)
-      35.5±0.3μs       14.5±0.3μs     0.41  offset.OffestDatetimeArithmetic.time_subtract(<BusinessMonthBegin>)
-      48.9±0.5μs       19.3±0.2μs     0.39  offset.OffestDatetimeArithmetic.time_subtract_10(<SemiMonthBegin: day_of_month=15>)
-      50.9±0.6ms       20.0±0.2ms     0.39  indexing.CategoricalIndexIndexing.time_get_indexer_list('monotonic_incr')
-         197±2ms       77.1±0.8ms     0.39  indexing.NumericSeriesIndexing.time_getitem_lists(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'nonunique_monotonic_inc')
-        49.7±1μs       19.3±0.1μs     0.39  offset.OffestDatetimeArithmetic.time_subtract_10(<SemiMonthEnd: day_of_month=15>)
-         193±3ms       74.1±0.6ms     0.38  indexing.NumericSeriesIndexing.time_loc_array(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'nonunique_monotonic_inc')
-      32.0±0.7μs       12.2±0.2μs     0.38  offset.OffestDatetimeArithmetic.time_add(<BusinessMonthBegin>)
-         196±2ms       74.5±0.7ms     0.38  indexing.NumericSeriesIndexing.time_getitem_array(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'nonunique_monotonic_inc')
-     1.86±0.01ms          708±6μs     0.38  period.Algorithms.time_drop_duplicates('series')
-      29.2±0.2μs      11.0±0.06μs     0.38  offset.OffestDatetimeArithmetic.time_apply(<BusinessMonthBegin>)
-        43.3±2μs       16.3±0.1μs     0.38  offset.OffestDatetimeArithmetic.time_subtract(<BusinessYearBegin: month=1>)
-      40.0±0.5μs       15.1±0.2μs     0.38  offset.OffestDatetimeArithmetic.time_add_10(<BusinessYearBegin: month=1>)
-      37.5±0.4μs      14.1±0.07μs     0.38  offset.OffestDatetimeArithmetic.time_add_10(<BusinessMonthBegin>)
-      39.7±0.2μs       14.9±0.2μs     0.38  offset.OffestDatetimeArithmetic.time_add_10(<BusinessYearEnd: month=12>)
-        42.1±2μs       15.7±0.3μs     0.37  offset.OffestDatetimeArithmetic.time_subtract(<BusinessYearEnd: month=12>)
-      46.8±0.6μs       17.2±0.4μs     0.37  offset.OffestDatetimeArithmetic.time_subtract(<SemiMonthBegin: day_of_month=15>)
-      44.9±0.1μs       16.3±0.1μs     0.36  offset.OffestDatetimeArithmetic.time_add_10(<SemiMonthBegin: day_of_month=15>)
-         1.75±0s          630±3ms     0.36  reshape.GetDummies.time_get_dummies_1d_sparse
-      47.7±0.5μs       17.0±0.2μs     0.36  offset.OffestDatetimeArithmetic.time_subtract(<SemiMonthEnd: day_of_month=15>)
-      50.0±0.5μs       17.8±0.3μs     0.36  offset.OffestDatetimeArithmetic.time_subtract_10(<BusinessYearEnd: month=12>)
-      46.0±0.2μs       16.2±0.1μs     0.35  offset.OffestDatetimeArithmetic.time_add_10(<SemiMonthEnd: day_of_month=15>)
-      50.1±0.4μs       17.4±0.4μs     0.35  offset.OffestDatetimeArithmetic.time_subtract_10(<BusinessYearBegin: month=1>)
-        43.8±2μs      14.2±0.04μs     0.32  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<SemiMonthEnd: day_of_month=15>)
-        44.2±3μs      14.2±0.09μs     0.32  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<SemiMonthBegin: day_of_month=15>)
-      41.8±0.2μs       13.2±0.1μs     0.32  offset.OffestDatetimeArithmetic.time_add(<SemiMonthBegin: day_of_month=15>)
-      39.3±0.5μs      12.0±0.07μs     0.31  offset.OffestDatetimeArithmetic.time_apply(<SemiMonthBegin: day_of_month=15>)
-      52.7±0.6μs       15.9±0.1μs     0.30  offset.OffestDatetimeArithmetic.time_subtract_10(<MonthEnd>)
-        43.3±1μs       13.0±0.2μs     0.30  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<MonthEnd>)
-      43.1±0.6μs      12.9±0.08μs     0.30  offset.OffestDatetimeArithmetic.time_add(<SemiMonthEnd: day_of_month=15>)
-         233±3ms         69.4±2ms     0.30  rolling.Quantile.time_quantile('Series', 1000, 'float', 0.5, 'linear')
-      49.2±0.2μs      14.7±0.07μs     0.30  offset.OffestDatetimeArithmetic.time_subtract(<MonthEnd>)
-         233±3ms         69.3±1ms     0.30  rolling.Quantile.time_quantile('Series', 1000, 'float', 0.5, 'midpoint')
-         232±2ms       67.6±0.5ms     0.29  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 0.5, 'linear')
-      40.4±0.3μs      11.8±0.09μs     0.29  offset.OffestDatetimeArithmetic.time_apply(<SemiMonthEnd: day_of_month=15>)
-      48.1±0.8μs       14.0±0.3μs     0.29  offset.OffestDatetimeArithmetic.time_add_10(<MonthEnd>)
-        52.4±1μs       15.2±0.3μs     0.29  offset.OffestDatetimeArithmetic.time_add_10(<BusinessQuarterEnd: startingMonth=3>)
-     71.1±0.09μs       20.6±0.2μs     0.29  indexing.CategoricalIndexIndexing.time_getitem_list_like('non_monotonic')
-         235±3ms       67.0±0.1ms     0.29  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 0.5, 'midpoint')
-      70.4±0.3μs       20.1±0.1μs     0.29  indexing.CategoricalIndexIndexing.time_getitem_list_like('monotonic_incr')
-         230±2ms         65.5±4ms     0.28  rolling.Quantile.time_quantile('Series', 1000, 'float', 0.5, 'nearest')
-      42.2±0.1μs       11.9±0.1μs     0.28  offset.OffestDatetimeArithmetic.time_add(<MonthEnd>)
-         205±1ms       57.3±0.2ms     0.28  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0.5, 'midpoint')
-         234±2ms       65.2±0.3ms     0.28  rolling.Quantile.time_quantile('Series', 1000, 'float', 0.5, 'lower')
-         233±3ms         64.7±2ms     0.28  rolling.Quantile.time_quantile('Series', 1000, 'float', 0.5, 'higher')
-         208±2ms       57.5±0.5ms     0.28  rolling.Quantile.time_quantile('Series', 1000, 'int', 0.5, 'midpoint')
-        233±10ms       64.4±0.8ms     0.28  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 0.5, 'nearest')
-      39.3±0.3μs      10.8±0.05μs     0.28  offset.OffestDatetimeArithmetic.time_apply(<MonthEnd>)
-       206±0.9ms       56.7±0.6ms     0.27  rolling.Quantile.time_quantile('Series', 1000, 'int', 0.5, 'linear')
-         205±1ms       55.6±0.4ms     0.27  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0.5, 'linear')
-         236±3ms       63.0±0.3ms     0.27  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 0.5, 'lower')
-      50.6±0.5μs      13.3±0.03μs     0.26  offset.OffestDatetimeArithmetic.time_apply_np_dt64(<BusinessYearEnd: month=12>)
-        242±10ms         63.7±1ms     0.26  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 0.5, 'higher')
-         625±4μs          157±1μs     0.25  groupby.GroupByMethods.time_dtype_as_group('object', 'bfill', 'direct')
-         810±3μs          203±3μs     0.25  series_methods.Map.time_map('Series')
-         625±6μs          156±1μs     0.25  groupby.GroupByMethods.time_dtype_as_group('object', 'ffill', 'direct')
-         628±2μs        156±0.6μs     0.25  groupby.GroupByMethods.time_dtype_as_group('object', 'ffill', 'transformation')
-        636±10μs          156±2μs     0.25  groupby.GroupByMethods.time_dtype_as_group('object', 'bfill', 'transformation')
-      49.1±0.5μs       12.0±0.1μs     0.24  offset.OffestDatetimeArithmetic.time_add(<BusinessYearEnd: month=12>)
-         206±1ms       49.8±0.5ms     0.24  rolling.Quantile.time_quantile('Series', 1000, 'int', 0.5, 'lower')
-         206±1ms       49.5±0.7ms     0.24  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0.5, 'nearest')
-     5.56±0.03ms      1.34±0.01ms     0.24  series_methods.Dir.time_dir_strings
-         206±2ms       49.5±0.3ms     0.24  rolling.Quantile.time_quantile('Series', 1000, 'int', 0.5, 'nearest')
-       206±0.7ms       49.2±0.3ms     0.24  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0.5, 'higher')
-         206±2ms       49.1±0.6ms     0.24  rolling.Quantile.time_quantile('Series', 1000, 'int', 0.5, 'higher')
-      46.7±0.6μs      11.1±0.09μs     0.24  offset.OffestDatetimeArithmetic.time_apply(<BusinessYearEnd: month=12>)
-       206±0.9ms       48.7±0.5ms     0.24  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0.5, 'lower')
-     1.04±0.02ms          242±2μs     0.23  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<QuarterBegin: startingMonth=3>)
-     1.06±0.01ms          239±1μs     0.22  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<YearBegin: month=1>)
-         302±1μs       67.4±0.4μs     0.22  groupby.GroupByMethods.time_dtype_as_group('object', 'all', 'transformation')
-         302±1μs       66.9±0.9μs     0.22  groupby.GroupByMethods.time_dtype_as_group('object', 'any', 'direct')
-         302±3μs       66.8±0.6μs     0.22  groupby.GroupByMethods.time_dtype_as_group('object', 'all', 'direct')
-         305±2μs       67.0±0.4μs     0.22  groupby.GroupByMethods.time_dtype_as_group('object', 'any', 'transformation')
-         512±9ms          111±2ms     0.22  timeseries.DatetimeIndex.time_to_date('tz_aware')
-         528±5ms          111±2ms     0.21  timeseries.DatetimeIndex.time_to_time('tz_aware')
-      17.2±0.2ms      3.49±0.09ms     0.20  groupby.Datelike.time_sum('period_range')
-         457±3ms         92.0±1ms     0.20  multiindex_object.GetLoc.time_large_get_loc_warm
-      63.2±0.3μs       12.5±0.1μs     0.20  indexing.CategoricalIndexIndexing.time_getitem_slice('monotonic_decr')
-     2.47±0.06ms        460±0.6μs     0.19  offset.ApplyIndex.time_apply_index(<QuarterBegin: startingMonth=3>)
-     2.37±0.02ms          416±2μs     0.18  offset.ApplyIndex.time_apply_index(<YearBegin: month=1>)
-      76.8±0.4μs       12.2±0.1μs     0.16  indexing.CategoricalIndexIndexing.time_getitem_slice('monotonic_incr')
-      76.7±0.3μs      11.9±0.04μs     0.16  indexing.CategoricalIndexIndexing.time_getitem_slice('non_monotonic')
-     3.64±0.01ms         540±10μs     0.15  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.UInt64Engine'>, <class 'numpy.float64'>, 'non_monotonic')
-        58.2±2ms       8.51±0.1ms     0.15  categoricals.Isin.time_isin_categorical('object')
-     3.65±0.01ms         527±10μs     0.14  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.UInt64Engine'>, <class 'numpy.float64'>, 'monotonic_decr')
-      45.7±0.5μs      5.97±0.04μs     0.13  offset.OnOffset.time_on_offset(<QuarterEnd: startingMonth=3>)
-     3.54±0.01ms          414±3μs     0.12  indexing.CategoricalIndexIndexing.time_get_loc_scalar('non_monotonic')
-     3.59±0.02ms          417±5μs     0.12  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.Int64Engine'>, <class 'numpy.float64'>, 'non_monotonic')
-     3.59±0.01ms          415±8μs     0.12  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.Int64Engine'>, <class 'numpy.float64'>, 'monotonic_decr')
-     5.11±0.04ms          588±4μs     0.12  timeseries.DatetimeIndex.time_to_time('dst')
-     6.05±0.06ms          678±1μs     0.11  offset.OffsetSeriesArithmetic.time_add_offset(<QuarterEnd: startingMonth=3>)
-        3.51±0ms          392±1μs     0.11  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.UInt64Engine'>, <class 'numpy.int64'>, 'monotonic_decr')
-         145±1ms      15.9±0.03ms     0.11  timeseries.DatetimeIndex.time_to_time('repeated')
-       145±0.7ms      15.9±0.03ms     0.11  timeseries.DatetimeIndex.time_to_time('tz_naive')
-     3.52±0.04ms          385±5μs     0.11  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.UInt64Engine'>, <class 'numpy.int64'>, 'non_monotonic')
-     3.57±0.01ms         390±10μs     0.11  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.Int64Engine'>, <class 'numpy.uint64'>, 'non_monotonic')
-     3.56±0.01ms          388±3μs     0.11  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.Int64Engine'>, <class 'numpy.uint64'>, 'monotonic_decr')
-     6.30±0.08ms          682±3μs     0.11  offset.OffsetSeriesArithmetic.time_add_offset(<YearEnd: month=12>)
-      5.07±0.1ms          515±2μs     0.10  timeseries.DatetimeIndex.time_to_date('dst')
-         142±1ms      13.9±0.02ms     0.10  timeseries.DatetimeIndex.time_to_date('repeated')
-         143±2ms      13.9±0.05ms     0.10  timeseries.DatetimeIndex.time_to_date('tz_naive')
-      91.7±0.4ms      8.06±0.05ms     0.09  inference.DateInferOps.time_timedelta_plus_datetime
-     6.15±0.01ms         510±10μs     0.08  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.Float64Engine'>, <class 'numpy.uint64'>, 'monotonic_decr')
-      67.0±0.8ms      5.01±0.09ms     0.07  sparse.Arithmetic.time_divide(0.1, nan)
-     3.36±0.01ms          245±3μs     0.07  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.UInt64Engine'>, <class 'numpy.uint64'>, 'monotonic_decr')
-     3.37±0.01ms          244±3μs     0.07  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.UInt64Engine'>, <class 'numpy.uint64'>, 'non_monotonic')
-     3.42±0.01ms          248±8μs     0.07  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.Int64Engine'>, <class 'numpy.int64'>, 'monotonic_decr')
-     7.20±0.08ms          516±7μs     0.07  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.Float64Engine'>, <class 'numpy.uint64'>, 'non_monotonic')
-        66.7±1ms      4.73±0.02ms     0.07  sparse.Arithmetic.time_divide(0.01, nan)
-        3.42±0ms          240±2μs     0.07  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.Int64Engine'>, <class 'numpy.int64'>, 'non_monotonic')
-     6.07±0.02ms          415±5μs     0.07  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.Float64Engine'>, <class 'numpy.int64'>, 'monotonic_decr')
-      7.17±0.2ms         417±10μs     0.06  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.Float64Engine'>, <class 'numpy.int64'>, 'non_monotonic')
-      74.1±0.5ms      3.71±0.09ms     0.05  rolling.VariableWindowMethods.time_rolling('Series', '1h', 'float', 'max')
-      74.2±0.2ms      3.65±0.05ms     0.05  rolling.VariableWindowMethods.time_rolling('Series', '1h', 'float', 'min')
-      65.4±0.9ms      3.18±0.07ms     0.05  sparse.Arithmetic.time_add(0.01, nan)
-      65.6±0.7ms      3.17±0.03ms     0.05  sparse.Arithmetic.time_add(0.1, nan)
-     74.3±0.09ms      3.55±0.02ms     0.05  rolling.VariableWindowMethods.time_rolling('Series', '1h', 'int', 'min')
-      74.6±0.4ms      3.54±0.03ms     0.05  rolling.VariableWindowMethods.time_rolling('Series', '1h', 'int', 'max')
-      2.88±0.02s          136±2ms     0.05  plotting.Plotting.time_frame_plot
-      73.8±0.5ms      3.49±0.05ms     0.05  rolling.VariableWindowMethods.time_rolling('DataFrame', '1h', 'float', 'max')
-      2.82±0.01s         131±10ms     0.05  plotting.Plotting.time_series_plot
-      73.9±0.4ms      3.38±0.03ms     0.05  rolling.VariableWindowMethods.time_rolling('DataFrame', '1h', 'float', 'min')
-      73.6±0.1ms      3.33±0.01ms     0.05  rolling.VariableWindowMethods.time_rolling('DataFrame', '1h', 'int', 'max')
-      74.0±0.4ms      3.33±0.05ms     0.04  rolling.VariableWindowMethods.time_rolling('DataFrame', '1h', 'int', 'min')
-     5.50±0.05ms          243±2μs     0.04  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<QuarterEnd: startingMonth=3>)
-     8.19±0.07μs          352±3ns     0.04  timestamp.TimestampProperties.time_is_month_end(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, None)
-        5.89±0ms          252±3μs     0.04  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.Float64Engine'>, <class 'numpy.float64'>, 'monotonic_decr')
-     8.11±0.07μs          346±3ns     0.04  timestamp.TimestampProperties.time_is_month_end(None, None)
-     5.80±0.04ms          243±3μs     0.04  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<YearEnd: month=12>)
-     7.98±0.09μs          298±1ns     0.04  timestamp.TimestampProperties.time_week(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, 'B')
-      8.06±0.1μs        301±0.9ns     0.04  timestamp.TimestampProperties.time_week(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, None)
-     7.99±0.08μs          298±1ns     0.04  timestamp.TimestampProperties.time_week(None, None)
-      6.77±0.3ms          250±3μs     0.04  indexing_engines.NumericEngineIndexing.time_get_loc(<class 'pandas._libs.index.Float64Engine'>, <class 'numpy.float64'>, 'non_monotonic')
-     7.88±0.06μs          290±2ns     0.04  timestamp.TimestampProperties.time_dayofyear(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, 'B')
-     8.10±0.04μs        297±0.8ns     0.04  timestamp.TimestampProperties.time_week(None, 'B')
-     8.03±0.08μs          293±1ns     0.04  timestamp.TimestampProperties.time_dayofyear(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, None)
-     7.97±0.03μs        290±0.7ns     0.04  timestamp.TimestampProperties.time_dayofyear(None, 'B')
-     7.89±0.04μs          287±1ns     0.04  timestamp.TimestampProperties.time_dayofyear(None, None)
-     8.06±0.04μs          275±7ns     0.03  timestamp.TimestampProperties.time_days_in_month(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, None)
-     8.05±0.07μs          274±5ns     0.03  timestamp.TimestampProperties.time_days_in_month(None, 'B')
-     8.23±0.09μs         276±10ns     0.03  timestamp.TimestampProperties.time_is_year_end(None, None)
-     8.14±0.07μs        269±0.7ns     0.03  timestamp.TimestampProperties.time_days_in_month(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, 'B')
-     8.24±0.09μs          272±4ns     0.03  timestamp.TimestampProperties.time_days_in_month(None, None)
-     8.04±0.02μs          260±6ns     0.03  timestamp.TimestampProperties.time_quarter(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, 'B')
-     7.92±0.03μs          255±2ns     0.03  timestamp.TimestampProperties.time_quarter(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, None)
-     8.18±0.06μs          260±1ns     0.03  timestamp.TimestampProperties.time_is_year_start(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, None)
-     8.22±0.05μs          261±5ns     0.03  timestamp.TimestampProperties.time_is_quarter_end(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, None)
-      8.29±0.2μs        262±0.7ns     0.03  timestamp.TimestampProperties.time_is_year_end(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, None)
-     8.13±0.06μs        257±0.6ns     0.03  timestamp.TimestampProperties.time_quarter(None, 'B')
-     8.32±0.04μs          261±5ns     0.03  timestamp.TimestampProperties.time_is_quarter_end(None, None)
-      7.97±0.1μs          250±2ns     0.03  timestamp.TimestampProperties.time_quarter(None, None)
-      8.39±0.1μs          263±4ns     0.03  timestamp.TimestampProperties.time_is_leap_year(None, None)
-      8.18±0.2μs          256±6ns     0.03  timestamp.TimestampProperties.time_is_year_start(None, None)
-     8.28±0.05μs        258±0.2ns     0.03  timestamp.TimestampProperties.time_is_leap_year(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, None)
-     8.15±0.07μs          253±3ns     0.03  timestamp.TimestampProperties.time_is_quarter_start(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, None)
-     8.14±0.04μs          253±1ns     0.03  timestamp.TimestampProperties.time_is_quarter_start(None, None)
-     8.21±0.07μs          254±3ns     0.03  timestamp.TimestampProperties.time_is_month_start(None, None)
-     8.40±0.09μs          257±1ns     0.03  timestamp.TimestampProperties.time_is_month_start(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, None)
-      25.7±0.1ms          694±5μs     0.03  offset.OffsetSeriesArithmetic.time_add_offset(<BusinessQuarterEnd: startingMonth=3>)
-      31.2±0.2ms          694±2μs     0.02  offset.OffsetSeriesArithmetic.time_add_offset(<BusinessMonthEnd>)
-      31.4±0.1ms          685±2μs     0.02  offset.OffsetSeriesArithmetic.time_add_offset(<BusinessYearBegin: month=1>)
-      31.5±0.2ms          688±7μs     0.02  offset.OffsetSeriesArithmetic.time_add_offset(<BusinessQuarterBegin: startingMonth=3>)
-      1.82±0.01s       37.6±0.2ms     0.02  stat_ops.FrameOps.time_op('median', 'float', 1, False)
-      1.83±0.01s       37.4±0.2ms     0.02  stat_ops.FrameOps.time_op('median', 'float', 1, True)
-      1.82±0.01s      36.1±0.08ms     0.02  stat_ops.FrameOps.time_op('median', 'int', 1, False)
-     34.6±0.06ms          686±5μs     0.02  offset.OffsetSeriesArithmetic.time_add_offset(<BusinessMonthBegin>)
-      1.83±0.07s       36.1±0.2ms     0.02  stat_ops.FrameOps.time_op('median', 'int', 1, True)
-        55.7±1ms          702±5μs     0.01  offset.OffsetSeriesArithmetic.time_add_offset(<BusinessYearEnd: month=12>)
-      45.9±0.7ms          529±1μs     0.01  offset.ApplyIndex.time_apply_index(<YearEnd: month=12>)
-      43.4±0.6ms          490±1μs     0.01  offset.ApplyIndex.time_apply_index(<QuarterEnd: startingMonth=3>)
-      70.4±0.3ms          749±2μs     0.01  groupby.GroupByMethods.time_dtype_as_field('float', 'rank', 'transformation')
-      70.5±0.5ms          750±3μs     0.01  groupby.GroupByMethods.time_dtype_as_field('float', 'rank', 'direct')
-        73.1±1ms         768±10μs     0.01  groupby.GroupByMethods.time_dtype_as_field('int', 'rank', 'direct')
-      72.6±0.9ms          760±8μs     0.01  groupby.GroupByMethods.time_dtype_as_field('int', 'rank', 'transformation')
-      19.6±0.2μs          201±1ns     0.01  timedelta.DatetimeAccessor.time_dt_accessor
-     25.8±0.05ms        258±0.9μs     0.01  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<BusinessQuarterEnd: startingMonth=3>)
-      55.3±0.4ms          541±8μs     0.01  indexing.NumericSeriesIndexing.time_getitem_list_like(<class 'pandas.core.indexes.numeric.Float64Index'>, 'nonunique_monotonic_inc')
-         183±7ms      1.77±0.03ms     0.01  index_object.Indexing.time_get_loc_non_unique('Float')
-      1.66±0.01s       14.9±0.1ms     0.01  rolling.Pairwise.time_pairwise(1000, 'corr', True)
-      1.67±0.01s      14.5±0.08ms     0.01  rolling.Pairwise.time_pairwise(10, 'corr', True)
-      1.68±0.01s      14.5±0.04ms     0.01  rolling.Pairwise.time_pairwise(None, 'corr', True)
-         169±2ms         1.45±0ms     0.01  index_object.Indexing.time_get_loc_non_unique_sorted('Float')
-      30.6±0.2ms          258±2μs     0.01  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<BusinessMonthEnd>)
-      1.65±0.02s       13.4±0.1ms     0.01  rolling.Pairwise.time_pairwise(1000, 'cov', True)
-      31.7±0.5ms          252±1μs     0.01  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<BusinessYearBegin: month=1>)
-         1.66±0s       13.1±0.1ms     0.01  rolling.Pairwise.time_pairwise(10, 'cov', True)
-      31.5±0.5ms          248±3μs     0.01  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<BusinessQuarterBegin: startingMonth=3>)
-      1.67±0.03s      12.9±0.06ms     0.01  rolling.Pairwise.time_pairwise(None, 'cov', True)
-      54.8±0.5ms          419±3μs     0.01  indexing.NumericSeriesIndexing.time_ix_list_like(<class 'pandas.core.indexes.numeric.Float64Index'>, 'nonunique_monotonic_inc')
-         268±4ms      2.03±0.02ms     0.01  multiindex_object.Integer.time_get_indexer
-      57.7±0.9ms          430±5μs     0.01  indexing.NumericSeriesIndexing.time_ix_list_like(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'nonunique_monotonic_inc')
-      31.8±0.6μs          234±4ns     0.01  categoricals.IsMonotonic.time_categorical_index_is_monotonic_decreasing
-      58.4±0.3ms          425±5μs     0.01  indexing.NumericSeriesIndexing.time_ix_list_like(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
-       106±0.7ms          766±3μs     0.01  groupby.GroupByMethods.time_dtype_as_group('int', 'rank', 'direct')
-      31.8±0.3μs          230±2ns     0.01  categoricals.IsMonotonic.time_categorical_index_is_monotonic_increasing
-       106±0.6ms          759±2μs     0.01  groupby.GroupByMethods.time_dtype_as_group('int', 'rank', 'transformation')
-      15.9±0.1μs          114±1ns     0.01  timeseries.DatetimeAccessor.time_dt_accessor
-      34.3±0.3ms          241±2μs     0.01  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<BusinessMonthBegin>)
-      54.6±0.3ms          377±3μs     0.01  indexing.NumericSeriesIndexing.time_loc_list_like(<class 'pandas.core.indexes.numeric.Float64Index'>, 'nonunique_monotonic_inc')
-      57.5±0.3ms        388±100μs     0.01  groupby.GroupByMethods.time_dtype_as_field('int', 'ffill', 'direct')
-      97.3±0.3ms          657±4μs     0.01  groupby.GroupByMethods.time_dtype_as_field('datetime', 'rank', 'transformation')
-      98.2±0.9ms          661±4μs     0.01  groupby.GroupByMethods.time_dtype_as_field('datetime', 'rank', 'direct')
-     1.87±0.01ms      11.1±0.07μs     0.01  categoricals.Contains.time_categorical_contains
-      57.2±0.2ms         328±90μs     0.01  groupby.GroupByMethods.time_dtype_as_field('int', 'bfill', 'direct')
-       257±0.7ms         1.47±0ms     0.01  index_object.Indexing.time_get_loc_non_unique_sorted('Int')
-         259±8ms      1.47±0.03ms     0.01  index_object.Indexing.time_get_loc_non_unique('Int')
-      57.5±0.6ms         323±90μs     0.01  groupby.GroupByMethods.time_dtype_as_field('int', 'ffill', 'transformation')
-     1.91±0.01ms      10.2±0.08μs     0.01  offset.OnOffset.time_on_offset(<BusinessYearEnd: month=12>)
-     1.28±0.01ms      6.08±0.06μs     0.00  offset.OnOffset.time_on_offset(<BusinessQuarterBegin: startingMonth=3>)
-         130±2ms          606±9μs     0.00  indexing.NumericSeriesIndexing.time_getitem_list_like(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'nonunique_monotonic_inc')
-       164±0.3ms          763±5μs     0.00  groupby.GroupByMethods.time_dtype_as_group('float', 'rank', 'transformation')
-        55.5±1ms          257±2μs     0.00  offset.OffsetDatetimeIndexArithmetic.time_add_offset(<BusinessYearEnd: month=12>)
-     1.23±0.01ms       5.66±0.1μs     0.00  offset.OnOffset.time_on_offset(<QuarterBegin: startingMonth=3>)
-       165±0.8ms          761±2μs     0.00  groupby.GroupByMethods.time_dtype_as_group('float', 'rank', 'direct')
-         167±2ms          767±2μs     0.00  groupby.GroupByMethods.time_dtype_as_group('datetime', 'rank', 'transformation')
-         169±1ms         773±10μs     0.00  groupby.GroupByMethods.time_dtype_as_group('datetime', 'rank', 'direct')
-     1.51±0.03ms       6.76±0.1μs     0.00  offset.OnOffset.time_on_offset(<BusinessMonthEnd>)
-       132±0.8ms          587±6μs     0.00  indexing.NumericSeriesIndexing.time_getitem_list_like(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
-      57.6±0.4ms          252±8μs     0.00  groupby.GroupByMethods.time_dtype_as_field('int', 'bfill', 'transformation')
-     1.55±0.01ms      6.00±0.02μs     0.00  offset.OnOffset.time_on_offset(<BusinessQuarterEnd: startingMonth=3>)
-       129±0.3ms          401±6μs     0.00  indexing.NumericSeriesIndexing.time_loc_list_like(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'nonunique_monotonic_inc')
-      84.1±0.2ms          253±2μs     0.00  groupby.GroupByMethods.time_dtype_as_group('int', 'bfill', 'direct')
-      84.4±0.6ms          252±3μs     0.00  groupby.GroupByMethods.time_dtype_as_group('int', 'bfill', 'transformation')
-      83.9±0.5ms          250±3μs     0.00  groupby.GroupByMethods.time_dtype_as_group('int', 'ffill', 'transformation')
-      85.5±0.7ms          252±1μs     0.00  groupby.GroupByMethods.time_dtype_as_group('int', 'ffill', 'direct')
-         131±3ms          384±2μs     0.00  indexing.NumericSeriesIndexing.time_loc_list_like(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
-        1.46±0ms      3.77±0.02μs     0.00  offset.OnOffset.time_on_offset(<BusinessYearBegin: month=1>)
-      58.7±0.8ms          143±4μs     0.00  groupby.GroupByMethods.time_dtype_as_field('object', 'any', 'transformation')
-      58.0±0.4ms          135±3μs     0.00  groupby.GroupByMethods.time_dtype_as_field('object', 'all', 'direct')
-         250±2ms          579±6μs     0.00  groupby.GroupByMethods.time_dtype_as_field('int', 'pct_change', 'direct')
-      58.3±0.4ms        134±0.7μs     0.00  groupby.GroupByMethods.time_dtype_as_field('object', 'any', 'direct')
-       248±0.8ms          572±3μs     0.00  groupby.GroupByMethods.time_dtype_as_field('int', 'pct_change', 'transformation')
-      58.2±0.1ms        132±0.7μs     0.00  groupby.GroupByMethods.time_dtype_as_field('object', 'all', 'transformation')
-      1.61±0.01s      3.52±0.03ms     0.00  rolling.VariableWindowMethods.time_rolling('Series', '1d', 'int', 'max')
-         1.61±0s      3.51±0.07ms     0.00  rolling.VariableWindowMethods.time_rolling('Series', '1d', 'int', 'min')
-         1.62±0s      3.47±0.03ms     0.00  rolling.VariableWindowMethods.time_rolling('Series', '1d', 'float', 'max')
-         1.62±0s      3.45±0.04ms     0.00  rolling.VariableWindowMethods.time_rolling('Series', '1d', 'float', 'min')
-      98.5±0.5ms          207±2μs     0.00  groupby.GroupByMethods.time_dtype_as_field('float', 'bfill', 'transformation')
-        99.5±1ms          208±2μs     0.00  groupby.GroupByMethods.time_dtype_as_field('float', 'bfill', 'direct')
-      98.9±0.6ms          206±1μs     0.00  groupby.GroupByMethods.time_dtype_as_field('float', 'ffill', 'direct')
-      1.61±0.01s      3.33±0.04ms     0.00  rolling.VariableWindowMethods.time_rolling('DataFrame', '1d', 'int', 'min')
-      99.8±0.6ms          205±2μs     0.00  groupby.GroupByMethods.time_dtype_as_field('float', 'ffill', 'transformation')
-      1.61±0.01s      3.30±0.03ms     0.00  rolling.VariableWindowMethods.time_rolling('DataFrame', '1d', 'int', 'max')
-      1.61±0.01s      3.25±0.04ms     0.00  rolling.VariableWindowMethods.time_rolling('DataFrame', '1d', 'float', 'max')
-         1.61±0s      3.23±0.03ms     0.00  rolling.VariableWindowMethods.time_rolling('DataFrame', '1d', 'float', 'min')
-         281±2ms          521±3μs     0.00  groupby.GroupByMethods.time_dtype_as_field('float', 'pct_change', 'direct')
-       158±0.6ms          291±1μs     0.00  groupby.GroupByMethods.time_dtype_as_field('object', 'ffill', 'direct')
-       159±0.9ms          294±5μs     0.00  groupby.GroupByMethods.time_dtype_as_field('object', 'bfill', 'transformation')
-       160±0.8ms          292±2μs     0.00  groupby.GroupByMethods.time_dtype_as_field('object', 'bfill', 'direct')
-         280±2ms          510±3μs     0.00  groupby.GroupByMethods.time_dtype_as_field('float', 'pct_change', 'transformation')
-         610±4ms         1.10±0ms     0.00  timedelta.DatetimeAccessor.time_timedelta_days
-         161±2ms          290±3μs     0.00  groupby.GroupByMethods.time_dtype_as_field('object', 'ffill', 'transformation')
-       132±0.7ms          235±2μs     0.00  groupby.GroupByMethods.time_dtype_as_group('float', 'ffill', 'transformation')
-       133±0.7ms          237±2μs     0.00  groupby.GroupByMethods.time_dtype_as_group('float', 'bfill', 'transformation')
-         626±2ms      1.11±0.01ms     0.00  timedelta.DatetimeAccessor.time_timedelta_nanoseconds
-       132±0.5ms          234±3μs     0.00  groupby.GroupByMethods.time_dtype_as_group('float', 'ffill', 'direct')
-       133±0.8ms        236±0.9μs     0.00  groupby.GroupByMethods.time_dtype_as_group('float', 'bfill', 'direct')
-         639±6ms      1.12±0.03ms     0.00  timedelta.DatetimeAccessor.time_timedelta_seconds
-        643±20ms      1.10±0.01ms     0.00  timedelta.DatetimeAccessor.time_timedelta_microseconds
-         363±3ms          578±5μs     0.00  groupby.GroupByMethods.time_dtype_as_group('int', 'pct_change', 'direct')
-         366±2ms          570±2μs     0.00  groupby.GroupByMethods.time_dtype_as_group('int', 'pct_change', 'transformation')
-      48.4±0.2ms         67.9±1μs     0.00  groupby.GroupByMethods.time_dtype_as_field('int', 'all', 'transformation')
-       151±0.7ms          211±1μs     0.00  groupby.GroupByMethods.time_dtype_as_field('datetime', 'bfill', 'direct')
-         152±2ms          212±2μs     0.00  groupby.GroupByMethods.time_dtype_as_field('datetime', 'ffill', 'direct')
-       152±0.9ms        211±0.9μs     0.00  groupby.GroupByMethods.time_dtype_as_field('datetime', 'bfill', 'transformation')
-      49.1±0.3ms       68.0±0.4μs     0.00  groupby.GroupByMethods.time_dtype_as_field('int', 'any', 'transformation')
-         151±2ms          210±1μs     0.00  groupby.GroupByMethods.time_dtype_as_field('datetime', 'ffill', 'transformation')
-      48.0±0.2ms       66.4±0.3μs     0.00  groupby.GroupByMethods.time_dtype_as_field('float', 'any', 'transformation')
-      48.0±0.4ms       66.1±0.2μs     0.00  groupby.GroupByMethods.time_dtype_as_field('float', 'all', 'direct')
-      49.1±0.4ms       67.2±0.6μs     0.00  groupby.GroupByMethods.time_dtype_as_field('int', 'all', 'direct')
-      49.2±0.5ms       67.2±0.5μs     0.00  groupby.GroupByMethods.time_dtype_as_field('int', 'any', 'direct')
-      48.9±0.6ms       66.8±0.7μs     0.00  groupby.GroupByMethods.time_dtype_as_field('float', 'any', 'direct')
-      48.1±0.2ms         65.7±1μs     0.00  groupby.GroupByMethods.time_dtype_as_field('float', 'all', 'transformation')
-      52.9±0.2ms       65.5±0.4μs     0.00  groupby.GroupByMethods.time_dtype_as_field('datetime', 'any', 'transformation')
-      53.3±0.1ms       65.0±0.3μs     0.00  groupby.GroupByMethods.time_dtype_as_field('datetime', 'any', 'direct')
-      53.5±0.3ms       64.6±0.7μs     0.00  groupby.GroupByMethods.time_dtype_as_field('datetime', 'all', 'transformation')
-      53.5±0.2ms       64.4±0.4μs     0.00  groupby.GroupByMethods.time_dtype_as_field('datetime', 'all', 'direct')
-       137±0.2ms          161±1μs     0.00  groupby.GroupByMethods.time_dtype_as_group('datetime', 'bfill', 'direct')
-       135±0.6ms          158±1μs     0.00  groupby.GroupByMethods.time_dtype_as_group('datetime', 'ffill', 'direct')
-         135±1ms        157±0.6μs     0.00  groupby.GroupByMethods.time_dtype_as_group('datetime', 'ffill', 'transformation')
-         137±2ms          157±1μs     0.00  groupby.GroupByMethods.time_dtype_as_group('datetime', 'bfill', 'transformation')
-     1.87±0.01ms      2.08±0.07μs     0.00  categoricals.Contains.time_categorical_index_contains
-         565±1ms          562±5μs     0.00  groupby.GroupByMethods.time_dtype_as_group('float', 'pct_change', 'direct')
-         565±2ms          555±3μs     0.00  groupby.GroupByMethods.time_dtype_as_group('float', 'pct_change', 'transformation')
-      70.9±0.2ms         68.7±1μs     0.00  groupby.GroupByMethods.time_dtype_as_group('int', 'any', 'transformation')
-      71.6±0.6ms       68.5±0.5μs     0.00  groupby.GroupByMethods.time_dtype_as_group('int', 'any', 'direct')
-      72.1±0.3ms       68.0±0.5μs     0.00  groupby.GroupByMethods.time_dtype_as_group('int', 'all', 'direct')
-      72.7±0.6ms       68.1±0.6μs     0.00  groupby.GroupByMethods.time_dtype_as_group('int', 'all', 'transformation')
-       111±0.8ms       69.5±0.4μs     0.00  groupby.GroupByMethods.time_dtype_as_group('float', 'all', 'direct')
-         112±1ms       68.8±0.2μs     0.00  groupby.GroupByMethods.time_dtype_as_group('float', 'any', 'transformation')
-       112±0.8ms       69.0±0.9μs     0.00  groupby.GroupByMethods.time_dtype_as_group('float', 'any', 'direct')
-         112±1ms       68.2±0.3μs     0.00  groupby.GroupByMethods.time_dtype_as_group('float', 'all', 'transformation')
-       114±0.4ms       68.9±0.2μs     0.00  groupby.GroupByMethods.time_dtype_as_group('datetime', 'all', 'transformation')
-       115±0.4ms       69.7±0.3μs     0.00  groupby.GroupByMethods.time_dtype_as_group('datetime', 'any', 'direct')
-       115±0.9ms       69.4±0.7μs     0.00  groupby.GroupByMethods.time_dtype_as_group('datetime', 'any', 'transformation')
-         116±1ms       69.4±0.4μs     0.00  groupby.GroupByMethods.time_dtype_as_group('datetime', 'all', 'direct')
-      1.49±0.01s          471±5μs     0.00  series_methods.IsInForObjects.time_isin_nans
-       290±0.6ms       19.7±0.2μs     0.00  multiindex_object.GetLoc.time_large_get_loc
-        937±30ms      2.28±0.01μs     0.00  series_methods.SeriesGetattr.time_series_datetimeindex_repr
```

</details"
547040209,30816,replacing '.format' with f-strings in some test files,ssikdar1,closed,2020-01-08T18:49:58Z,2020-01-08T22:22:52Z,"- [X] contributes to #29547
- [ ] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

One format that I left untouched in `pandas/tests/util/test_assert_frame_equal.py`:

```
def test_frame_equal_unicode(df1, df2, msg, by_blocks_fixture, obj_fixture):
    # see gh-20503
    #
    # Test ensures that `tm.assert_frame_equals` raises the right exception
    # when comparing DataFrames containing differing unicode objects.
    msg = msg.format(obj=obj_fixture)
    with pytest.raises(AssertionError, match=msg):
        tm.assert_frame_equal(df1, df2, by_blocks=by_blocks_fixture, obj=obj_fixture)
```

However it looks the the test function isnt being used anywhere:
```
$ grep -rnw . -e ""test_frame_equal_unicode""
./pandas/tests/util/test_assert_frame_equal.py:217:def test_frame_equal_unicode(df1, df2, msg, by_blocks_fixture, obj_fixture):
```

Wasn't sure if I should leave as is? or assume person calling the function will format `msg` properly before passing to the test function."
547082165,30822,STY: Absolute imports,ShaharNaveh,closed,2020-01-08T20:22:03Z,2020-01-08T23:07:58Z,"- [x] ref #30808
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
547154634,30827,STY: absolute imports,ShaharNaveh,closed,2020-01-08T23:07:17Z,2020-01-08T23:50:56Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
547128775,30825,TST: Added 'match=' to some bare pytest.raises,ShaharNaveh,closed,2020-01-08T22:04:20Z,2020-01-08T23:57:45Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
254280376,17393,Add indexName to DataFrame.to_json(orient='split'),Suor,closed,2017-08-31T09:57:40Z,2020-01-09T00:05:03Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

df = pd.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])
df.index = df.a
df_copy = pd.io.read.read_json(df.to_json(orient='split'), orient='split')
assert df_copy.index.name == df.index.name
```
#### Problem description

I use `.to_json(orient='split')` as a compact way to serialize and store dataframes, however, I loose index name on dump/load cycle. I would suggest adding ""indexName"" to json, which should be fairly backwards compatible.

#### Expected Output

```python
>>>  df.to_json(orient='split')
'{""columns"":[""a"",""b"",""c""],""index"":[1,4],""indexName"":""a"",""data"":[[1,2,3],[4,5,6]]}'
``` 
Behavior should probably stay as it is when index name is `None`.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.13.final.0
python-bits: 64
OS: Linux
OS-release: 4.10.0-28-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.20.3
pytest: 2.6.4
pip: 9.0.1
setuptools: 36.2.5
Cython: None
numpy: 1.13.1
scipy: 0.16.0
xarray: None
IPython: 5.4.1
sphinx: None
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: 2.6.2
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: 2.6 (dt dec pq3 ext lo64)
jinja2: 2.7.3
s3fs: None
pandas_gbq: None
pandas_datareader: None
</details>
"
547172074,30829,DOC: Added a single '-',ShaharNaveh,closed,2020-01-09T00:03:19Z,2020-01-09T01:20:21Z,"Feels weird to open a PR just for that, but I must get this fixed ;)"
286718285,19132,pd.MultiIndex.get_loc(np.nan),toobaz,closed,2018-01-08T11:34:26Z,2020-01-09T03:11:42Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: mi = pd.MultiIndex.from_product([[np.nan, 1]] * 3)

In [3]: mi.get_loc(np.nan)
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/home/pietro/nobackup/repo/pandas/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2513             try:
-> 2514                 return self._engine.get_loc(key)
   2515             except KeyError:

/home/pietro/nobackup/repo/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5834)()

/home/pietro/nobackup/repo/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5639)()

/home/pietro/nobackup/repo/pandas/pandas/_libs/index_class_helper.pxi in pandas._libs.index.Int64Engine._check_type (pandas/_libs/index.c:19008)()

KeyError: nan

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
<ipython-input-3-641c024055e9> in <module>()
----> 1 mi.get_loc(np.nan)

/home/pietro/nobackup/repo/pandas/pandas/core/indexes/multi.py in get_loc(self, key, method)
   2121 
   2122         if not isinstance(key, tuple):
-> 2123             loc = self._get_level_indexer(key, level=0)
   2124             return _maybe_to_slice(loc)
   2125 

/home/pietro/nobackup/repo/pandas/pandas/core/indexes/multi.py in _get_level_indexer(self, key, level, indexer)
   2411         else:
   2412 
-> 2413             loc = level_index.get_loc(key)
   2414             if isinstance(loc, slice):
   2415                 return loc

/home/pietro/nobackup/repo/pandas/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2514                 return self._engine.get_loc(key)
   2515             except KeyError:
-> 2516                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2517 
   2518         indexer = self.get_indexer([key], method=method, tolerance=tolerance)

/home/pietro/nobackup/repo/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5834)()

/home/pietro/nobackup/repo/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5639)()

/home/pietro/nobackup/repo/pandas/pandas/_libs/index_class_helper.pxi in pandas._libs.index.Int64Engine._check_type (pandas/_libs/index.c:19008)()

KeyError: nan

```

#### Problem description

Opening a separate issue from #18485 (full label containing NaN) because, differently from that, this is _not_ fixed by #19074.

xref #5286 because they are probably worth testing together.

#### Expected Output

``slice(0, 4, None)``

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.0-5-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.22.0.dev0+448.gdbec3c92e
pytest: 3.0.6
pip: 9.0.1
setuptools: 33.1.1
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.18.1
pyarrow: None
xarray: None
IPython: 5.2.2
sphinx: None
patsy: 0.4.1+dev
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: 1.2.0
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.0.0
openpyxl: 2.3.0
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: None
lxml: 3.7.1
bs4: 4.5.3
html5lib: 0.999999999
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.8
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None


</details>
"
495481523,28512,DEPR: Deprecate numpy argument in read_json,WillAyd,closed,2019-09-18T22:18:24Z,2020-01-09T03:25:48Z,"I've never really been clear on the purpose of the `numpy` argument in to_json. Some digging brought me here https://github.com/pandas-dev/pandas/pull/3876#issuecomment-19390383 where it is explained that this maintains some kind of sequence to elements. To illustrate the only difference I could find

```python
>>> pd.read_json('[{""a"": 1, ""b"": 2}, {""b"":2, ""a"" :1}]',numpy=False, orient='records')
   a  b
0  1  2
1  1  2

>>> pd.read_json('[{""a"": 1, ""b"": 2}, {""b"":2, ""a"" :1}]',numpy=True, orient='records')
   a  b
0  1  2
1  2  1
```

I might be missing the point but I don't understand why this would be useful. Objects in JSON are by definition not ordered, so this is non-compliant and I think just plain confusing.

So I think good to deprecate unless anyone has objections."
544828511,30636,DEPR: Deprecate numpy argument in read_json,alimcmaster1,closed,2020-01-03T02:41:41Z,2020-01-09T03:26:09Z,"Co-authored-by: Luca Ionescu <lucaionescu@users.noreply.github.com>

- Continuing https://github.com/pandas-dev/pandas/pull/28562
- [x] closes #28512

@lucaionescu - i've merged master and pushed to here (I don't have the permissions to push to your branch), I will aim to fix up the tests. Or feel free to take it from here if you have time?

Opening up WIP PR to see what test failures need addressing. 

I've also co-authored the commit."
496593694,28560,read_json and numpy=True and dtype=False does not preserve int64 dtype roundtrip on Windows,WillAyd,closed,2019-09-21T00:43:55Z,2020-01-09T03:34:30Z,"I don't have access to a PC so cannot replicate directly (if anyone else can would be appreciated) but going through #28510 there were a few Windows with dtype preservation, which would look something as follows:

```python
>>> df = pd.DataFrame(range(3))
>>> df.dtypes
0    int64
dtype: object
>>> pd.read_json(df.to_json()).dtypes
0    int32
dtype: object
```

Note that this may also be irrelevant if #28512 happens"
496593108,28559,"read_json with orient=""values"" and numpy=True provides strange column",WillAyd,closed,2019-09-21T00:38:38Z,2020-01-09T03:34:38Z,"Another edge case revealed in #28510 . Perhaps won't be relvant if we go through with #28512 but opening for posterity

For whatever reason, this particular combination of values produces a RangeIndex for the columns.

```python
>>> pd.read_json(""[]"", orient=""values"", numpy=True).columns
RangeIndex(start=0, stop=1, step=1)
```

Nothing else appears to do that"
547199663,30835,REF: get_value do less inside try,jbrockmendel,closed,2020-01-09T01:43:34Z,2020-01-09T05:03:51Z,Separating out mostly-cosmetic bits from other get_value work
546592766,30806,REF: use shareable code for DTI/TDI.insert,jbrockmendel,closed,2020-01-08T01:20:50Z,2020-01-09T05:06:37Z,"xref #30757 should go in before this because it contains the tests.  After this, we'll be able to de-duplicate the two methods."
547059024,30819,CLN: de-duplicate boxing in DTI.get_value,jbrockmendel,closed,2020-01-08T19:31:40Z,2020-01-09T05:08:52Z,"Also add a dedicated test or it to hit currently un-covered case of np.datetime64 key.  That case works in master, but doesn't go through the expected code path"
547042763,30817,REF: share comparison methods between ExtensionIndex subclasses,jbrockmendel,closed,2020-01-08T18:55:37Z,2020-01-09T05:09:30Z,

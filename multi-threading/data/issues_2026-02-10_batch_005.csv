id,number,title,user,state,created_at,updated_at,body
591038969,33177,Issue #33161 - Use empty_frame fixture,anish29292,closed,2020-03-31T11:46:30Z,2020-04-01T12:50:18Z,"Replaced Empty data frames with fixture empty_frame()

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
522212031,29596,"Lambda: Unable to allocate array with shape (133271, 319) and data type object: MemoryError",dcostelloe2019,closed,2019-11-13T13:08:11Z,2020-04-01T13:47:50Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
obj=s3.get_object(Bucket=bucket,Key=file_name)
 body = obj['Body'].read()
 df = pd.read_excel(io.BytesIO(body),encoding='utf-8',sheet_name='Sheet1',skiprows=4,usecols=use_cols,header=None,names=col_Names,)
```
#### Problem description

Excel read file crashes when reading. Specific excel sheet has 300 columns and 130,000 records, reduced columns to  40 by defining filter.

Error Reported:
Unable to allocate array with shape (133271, 319) and data type object: MemoryError
Traceback (most recent call last):
  File ""/var/task/lambda_function.py"", line 49, in lambda_handler
    df = pd.read_excel(io.BytesIO(body),encoding='utf-8',sheet_name='Products',skiprows=4,usecols=use_cols,header=None,names=col_Names,)
  File ""/opt/python/lib/python3.6/site-packages/pandas/util/_decorators.py"", line 208, in wrapper
    return func(*args, **kwargs)
  File ""/opt/python/lib/python3.6/site-packages/pandas/io/excel/_base.py"", line 340, in read_excel
    **kwds
  File ""/opt/python/lib/python3.6/site-packages/pandas/io/excel/_base.py"", line 883, in parse
    **kwds
  File ""/opt/python/lib/python3.6/site-packages/pandas/io/excel/_base.py"", line 516, in parse
    output[asheetname] = parser.read(nrows=nrows)
  File ""/opt/python/lib/python3.6/site-packages/pandas/io/parsers.py"", line 1154, in read
    ret = self._engine.read(nrows)
  File ""/opt/python/lib/python3.6/site-packages/pandas/io/parsers.py"", line 2493, in read
    alldata = self._rows_to_cols(content)
  File ""/opt/python/lib/python3.6/site-packages/pandas/io/parsers.py"", line 3160, in _rows_to_cols
    zipped_content = list(lib.to_object_array(content, min_width=col_len).T)
  File ""pandas/_libs/lib.pyx"", line 2279, in pandas._libs.lib.to_object_array
MemoryError: Unable to allocate array with shape (133271, 319) and data type object


If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
pandas 0.25.3
</details>
"
506492205,28968,BUG: groupby rolling ignoring min number of values,FCheda,closed,2019-10-14T07:39:54Z,2020-04-01T13:51:53Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
import pandas as pd
import numpy as np


#set folder 
folder_processed="" your folder here ""

files=[
'B0005_discharge.csv',
'B0006_discharge.csv',
'B0007_discharge.csv',
'B0018_discharge.csv',
]
def loadData(folder=folder_processed,files=files):
    df=None
    for file in files:
        if df is None:
            print(file)
            df=pd.read_csv(folder+file)
            df[""battery_id""]=file
        else:
            print(file)
            foo=pd.read_csv(folder+file)
            foo[""battery_id""]=file
            df=pd.concat([df,foo],axis=0)
    return df    
df= loadData()


def group_rolling(df,columns=[""battery_id"",""cicle_id""]):                                                  
    df['Voltage_measured_mean_5'] = df.groupby(columns)[['Voltage_measured']].rolling(5).mean().astype(""float32"").reset_index()['Voltage_measured']
    return df 

foo=group_rolling(df)

foo[foo[""internal_cicle_possition_id""]<5]
```

#### Problem description

I’m trying to do rolling windows over a grouped dataset using 2 columns for the groupby. 

With a rolling window of 5, pandas should return NaNs in the first 4 rows of each group.

This works correctly for each of my files individually, for random datasets, and for a group of multiple files, even using replicated ones (f1,f1copy1,f1copy2...). 

My last file “'B0018_discharge.csv'” works ok by itself, but if I load it next to any other file, only the first group (cicle_id) gets the NaNs, while the other groups of that file return a numeric mean value in the positions where it should not.

Files are all generated in the same way, from the discharge values in nasa Li-Ion Dataset

 https://data.nasa.gov/dataset/Li-ion-Battery-Aging-Datasets/uj5r-zjdb

I’m not including the conversion code from .mat files to .csv but I’m attaching a zip with the .csv files. [shared.zip](https://github.com/pandas-dev/pandas/files/3723349/shared.zip)

I’ve looked into the last file, and see no reason for this weird behaviour.


#### Expected Output

The first 4 rows in any (cicle-id,battery_id) group should be NaN as we are looking for 5 to make the mean.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 85 Stepping 4, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.1
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : 0.29.12
pytest           : 5.0.1
hypothesis       : None
sphinx           : 2.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.3.4
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.6.1
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.1.1
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : 1.3.5
tables           : 3.5.2
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8
Type Markdown and LaTeX:  α2
</details>
"
490022750,28305,Pandas df.rolling.mean() abnormal behavior for a Series having larger numbers (in the scale of billions),bobnvk99,closed,2019-09-05T21:39:19Z,2020-04-01T14:02:05Z,"#### Code Sample, a copy-pastable example if possible

```python
#Sample residual pcts
residuals_pct =[0.044516001,0.031137117,1.06758E+64,0.003522454,0.065171486,0.06033751,0.01325514,-0.005620799,-0.006225719,0.045713825,0.039280786,0.000531307]

#Creating a dataframe with residual percent
d = pd.DataFrame(residuals, columns=['residual_pct'])
#Shifting the pct 1 row down.
d['residual_pct_shift'] = d.residual_pct.shift(1)
#Calculating the adjusted residual pct.
d['adj_residual_pct'] = d['residual_pct_shift'][3:].rolling(window=3).mean()
```
#### Problem description

I am trying to implement adjusted residual percent as the rolling average of the previous three residual percents. 
Due to some data issue, model forecasted the target variable in the scale of billions, consequently effecting the residual percentage calculation. 
However, this is irrelevant to the current problem, the issue is when doing the rolling average using  .....rolling(window=3).mean(), **the average for the rows after the abnormal residual percent got affected**. 
**If I do take those same numbers and do the normal way of averaging or use MS Excel, I got it right.** 

Please see the image below:
![pandas_rolling_mean_issue](https://user-images.githubusercontent.com/5788470/64381815-aa270100-cff9-11e9-8f60-5d5469211f5d.PNG)

#### Output of ``pd.show_versions()``
<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.1
numpy            : 1.16.4
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.2
setuptools       : 41.0.1
Cython           : 0.29.13
pytest           : 5.0.1
hypothesis       : None
sphinx           : 2.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.7
tables           : 3.5.2
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8
</details>

Is there anything that I am missing or unaware. I tried to check in the pandas' source code and found nothing. Irrespective of the scale of the numbers, excel and the normal way of averaging works fine. 

Please advise.
I greatly appreciate your help. 

Thank you."
591938156,33207,Everything works fine,ODemidenko,closed,2020-04-01T13:57:52Z,2020-04-01T14:24:24Z,I made a stupid error) No bugs)
585645136,32901,DOC: Fix EX02 in pandas.Series.factorize,farhanreynaldo,closed,2020-03-22T04:49:12Z,2020-04-01T15:16:04Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. 

output of `python scripts/validate_docstrings.py pandas.Series.factorize`:
```
################################################################################
################################## Validation ##################################
################################################################################
```

"
579007229,32613,DOC: Validating that the word pandas is correctly capitalized,joybh98,closed,2020-03-11T04:48:02Z,2020-04-01T15:36:21Z,"- [x] closes #32316 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

As suggested by @datapythonista , I've added checks in `ci/code_checks.sh` to look if the `pandas` is being referenced in a standardized way i.e `pandas`,  not ` *pandas* `  or  `Pandas`

@datapythonista I've only added checks to see if pandas is being referenced correctly, but not showing where it's changed or how many references are wrong. 
Should I add checks for that also ? What do you think ?"
591538294,33192,CLN: Rename ordered_fixture --> ordered,jschendel,closed,2020-04-01T00:47:49Z,2020-04-01T15:44:47Z,A pretty small and straightforward rename to make things more readable and consistent with our existing naming convention for fixtures.
591540545,33193,REF: test_mutate_columns -> test_setitem,jbrockmendel,closed,2020-04-01T00:55:59Z,2020-04-01T17:56:25Z,
592071177,33211,CLN: remove Block.is_categorical_astype,jbrockmendel,closed,2020-04-01T17:10:17Z,2020-04-01T18:38:16Z,
592087888,33212,"CI: Make `isort` to check the ""scripts"" folder as well",ShaharNaveh,closed,2020-04-01T17:38:43Z,2020-04-01T18:50:11Z,
591083724,33179,DOC: Fixed examples in pandas/core/arrays/,ShaharNaveh,closed,2020-03-31T12:52:44Z,2020-04-01T19:09:11Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
208544937,15445,BUG: testing on windows,jreback,closed,2017-02-17T20:17:43Z,2020-04-01T20:21:34Z,"- we are passing builds which actually have an error
- fix the small dtype issue"
572400278,32312,DOC: Add examples to the method MultiIndex.is_lexsorted(),raisadz,closed,2020-02-27T22:26:43Z,2020-04-02T00:13:33Z,"- [ ] xref #32179
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
591587163,33197,Requested ASV,jbrockmendel,closed,2020-04-01T03:19:34Z,2020-04-02T00:13:49Z,I think this was requested a while ago and has been sitting in a local branch
591787530,33202,"row sum and column mean works but row mean gives all NaN, on heterogenous data types",xuancong84,closed,2020-04-01T09:57:58Z,2020-04-02T02:40:40Z,"When different DataFrame rows are of different types, row sum works but row mean gives all NaN values.

```
df=pd.DataFrame({'a':[1,pd.to_timedelta('1D'),3.], 'b':[4,pd.to_timedelta('1.5D'),6.], 'c':[7,pd.to_timedelta('0.5D'),np.nan]})
# df=pd.DataFrame({'a':[1,2,3.], 'b':[4,5,6.], 'c':[7,8,np.nan]})
display(df.transpose())
display(df.transpose().sum(axis=0))
display(df.transpose().mean(axis=0))
display(df)
display(df.sum(axis=1))
display(df.mean(axis=1))  # bug
```
<img width=""332"" alt=""image"" src=""https://user-images.githubusercontent.com/10172392/78124015-a4838380-7441-11ea-84fe-9bd45de54f1b.png"">

As shown above, the last statement shows the bug by returning all NaN values which is incorrect. Interestingly, if the DataFrame entries are of the same type (uncomment the 2nd line), this bug does not occur."
478350789,27817,Conversion from Arrow to datetime not working since pandas 0.22.0,insysion,closed,2019-08-08T09:23:23Z,2020-04-02T05:02:02Z,"#### Code Sample

```python
import arrow
import pandas as pd

date = arrow.Arrow(2019, 8, 8)
data = {'date': [date]}
df = pd.DataFrame(data)
df.astype('<M8[ns]')
```
#### Problem description

```
Traceback (most recent call last):
  File ""bug.py"", line 7, in <module>
    df.astype('<M8[ns]')
  File ""/lib/python3.5/site-packages/pandas/core/generic.py"", line 5691, in astype
    **kwargs)
  File ""/lib/python3.5/site-packages/pandas/core/internals/managers.py"", line 531, in astype
    return self.apply('astype', dtype=dtype, **kwargs)
  File ""/lib/python3.5/site-packages/pandas/core/internals/managers.py"", line 395, in apply
    applied = getattr(b, f)(**kwargs)
  File ""/lib/python3.5/site-packages/pandas/core/internals/blocks.py"", line 534, in astype
    **kwargs)
  File ""/lib/python3.5/site-packages/pandas/core/internals/blocks.py"", line 633, in _astype
    values = astype_nansafe(values.ravel(), dtype, copy=True)
  File ""/lib/python3.5/site-packages/pandas/core/dtypes/cast.py"", line 690, in astype_nansafe
    return astype_nansafe(to_datetime(arr).values, dtype, copy=copy)
  File ""/lib/python3.5/site-packages/pandas/core/tools/datetimes.py"", line 609, in to_datetime
    result = convert_listlike(arg, box, format)
  File ""/lib/python3.5/site-packages/pandas/core/tools/datetimes.py"", line 302, in _convert_listlike_datetimes
    allow_object=True)
  File ""/lib/python3.5/site-packages/pandas/core/arrays/datetimes.py"", line 1857, in objects_to_datetime64ns
    require_iso8601=require_iso8601
  File ""pandas/_libs/tslib.pyx"", line 460, in pandas._libs.tslib.array_to_datetime
  File ""pandas/_libs/tslib.pyx"", line 685, in pandas._libs.tslib.array_to_datetime
  File ""pandas/_libs/tslib.pyx"", line 813, in pandas._libs.tslib.array_to_datetime_object
  File ""pandas/_libs/tslib.pyx"", line 659, in pandas._libs.tslib.array_to_datetime
TypeError: <class 'arrow.arrow.Arrow'> is not convertible to datetime
```

#### Expected Output

DataFrame with datetime data. 

#### Output of ``pd.show_versions()``

Failing version:

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-55-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.24.2
pytest: None
pip: 19.2.1
setuptools: 39.1.0
Cython: None
numpy: 1.17.0
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2019.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>

Working version:

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-55-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.22.0
pytest: None
pip: 19.2.1
setuptools: 39.1.0
Cython: None
numpy: 1.17.0
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2019.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
485934563,28180,to_iso methods for DatetimeLikeArray,WillAyd,open,2019-08-27T17:20:38Z,2020-04-02T05:04:59Z,"to_json (and potentially others) handles the conversion of datetime-like objects to either epoch or isoformats depending on user input. All of this logic is currently housed in the objToJSON.c file and is a major reason why that extension module is rather complex / convoluted.

I think these methods may be general purpose enough to define in a vectorized manner on DatetimeLikeArrays. This wouldn't fully eliminate the need for the objToJSON.c extension module to define serialization for datetimes, timedelta and period object (as items stuck within an object array still need to be serialized), but it would definitely allow it to be more performant and move some of the logic out of the C layer back into Python

@jbrockmendel this is potentially also a precursor to #19486"
498158746,28610,DatetimeIndex + DateOffset broken around DST switching time,torfsen,open,2019-09-25T09:20:52Z,2020-04-02T05:12:06Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import pandas as pd
>>> idx = pd.DatetimeIndex(['2000-03-26 04:00'], tz='Europe/Berlin')  # Switch to DST was on that day
>>> idx
DatetimeIndex(['2000-03-26 04:00:00+02:00'], dtype='datetime64[ns, Europe/Berlin]', freq=None)
>>> idx + pd.DateOffset(hours=-2)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/var/local/conda/envs/pandas-debug/lib/python3.7/site-packages/pandas/core/indexes/datetimelike.py"", line 510, in __add__
    result = self._data.__add__(maybe_unwrap_index(other))
  File ""/var/local/conda/envs/pandas-debug/lib/python3.7/site-packages/pandas/core/arrays/datetimelike.py"", line 1212, in __add__
    result = self._add_offset(other)
  File ""/var/local/conda/envs/pandas-debug/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py"", line 832, in _add_offset
    result = result.tz_localize(self.tz)
  File ""/var/local/conda/envs/pandas-debug/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py"", line 1151, in tz_localize
    self.asi8, tz, ambiguous=ambiguous, nonexistent=nonexistent
  File ""pandas/_libs/tslibs/tzconversion.pyx"", line 276, in pandas._libs.tslibs.tzconversion.tz_localize_to_utc
pytz.exceptions.NonExistentTimeError: 2000-03-26 02:00:00
>>> idx[0] + pd.DateOffset(hours=-2)
Timestamp('2000-03-26 01:00:00+0100', tz='Europe/Berlin')
```
#### Problem description

The vectorized operation `DatetimeIndex + DateOffset` is broken around the DST switching time. However, the same operation works element-wise (`Timestamp + DateOffset`), so I would expect either both versions to work (preferably) or both versions to fail with the same error.

#### Expected Output

```python
DatetimeIndex(['2000-03-26 01:00:00+01:00'], dtype='datetime64[ns, Europe/Berlin]', freq=None)
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.0.0-29-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.16.5
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
589781167,33117,DOC: Partial fix SA04 errors in docstrings #28792,dilex42,closed,2020-03-29T12:30:19Z,2020-04-02T12:38:10Z,"- [x] xref #28792
"
584336218,32823,DOC: Partial fix SA04 errors in docstrings #28792 (feedback needed),dilex42,closed,2020-03-19T11:13:11Z,2020-04-02T12:38:22Z,"- [x] xref #28792

Would like feedback on a few issues before continuing to work on this issue :
- File pandas/core/accessor.py . How to handle multiline text? Couldn't find anywhere good recomendations.
- File pandas/core/ops/docstrings.py . Changed logic of templates. Is this okay? Also new lines with formatted text are too long, would like a reccomended sollution for this.
- File pandas/core/window/rolling.py and some others in regard to window functions have broken links that can be fixed by adding pandas. prefix to them(as in changed code). Is this right thing to do and should I make this changes whenever i see this?

Sorry for obvious mistakes, just starting to contribute. Thanks."
592191722,33221,TST: misplaced validate_indices tests,jbrockmendel,closed,2020-04-01T20:42:28Z,2020-04-02T15:22:43Z,
592252283,33227,CLN: De-privatize names,jbrockmendel,closed,2020-04-01T22:58:54Z,2020-04-02T15:23:16Z,"xref #32942, these are some of the more common offenders.

I'd actually prefer to not have the _NS_DTYPE and _TD_DTYPE objects and just use the pertinent strings, but if we are going to use the objects, might as well de-code-smell them."
538834223,30296,DEPR: Remove pandas.np,datapythonista,closed,2019-12-17T05:02:13Z,2020-04-02T15:23:30Z,"Not sure if it was added intentionally, but it's possible to call numpy with the `np` attribute of the pandas module:
```python
import pandas
x = pandas.np.array([1, 2, 3])
```
While this is not documented, I've seen couple of places suggesting this as a ""trick"" to avoid importing numpy directly.

I personally find this hacky, and I think should be removed. "
592183672,33219,REF: DataFrame.mask tests,jbrockmendel,closed,2020-04-01T20:27:39Z,2020-04-02T15:25:48Z,we already did this for Series.mask
592208402,33223,REF: misplaced Index.__contains__ tests,jbrockmendel,closed,2020-04-01T21:15:02Z,2020-04-02T15:32:22Z,
592248958,33226,Fix typos in 09_timeseries.rst,rogererens,closed,2020-04-01T22:48:58Z,2020-04-02T16:18:08Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
544414486,30609,Error on Warnings From C Extensions,WillAyd,closed,2020-01-02T00:16:52Z,2020-04-02T18:38:17Z,"May differ on other platforms, but currently I see about 19 warnings from C extensions still left. I think we should aim to clean these up and set the `Werror` flag in CI as can help be proactive in identifying bugs from extensions

Here is the list of remaining warnings from a parallel build, so order may not be relevant

```sh
warning: pandas/_libs/groupby.pyx:1088:26: Unreachable code
warning: pandas/_libs/window/aggregations.pyx:59:47: Buffer unpacking not optimized away.
warning: pandas/_libs/window/aggregations.pyx:59:47: Buffer unpacking not optimized away.
warning: pandas/_libs/window/aggregations.pyx:60:47: Buffer unpacking not optimized away.
warning: pandas/_libs/window/aggregations.pyx:60:47: Buffer unpacking not optimized away.
pandas/_libs/hashing.c:25387:20: warning: unused function '__pyx_memview_get_object' [-Wunused-function]
  static PyObject *__pyx_memview_get_object(const char *itemp) {
                   ^
pandas/_libs/hashing.c:25392:12: warning: unused function '__pyx_memview_set_object' [-Wunused-function]
static int __pyx_memview_set_object(const char *itemp, PyObject *obj) {
           ^
pandas/_libs/hashtable.c:63498:20: warning: unused function '__pyx_memview_get_object' [-Wunused-function]
  static PyObject *__pyx_memview_get_object(const char *itemp) {
                   ^
pandas/_libs/hashtable.c:63503:12: warning: unused function '__pyx_memview_set_object' [-Wunused-function]
static int __pyx_memview_set_object(const char *itemp, PyObject *obj) {
           ^
pandas/_libs/algos.c:81415:3: warning: code will never be executed [-Wunreachable-code]
  __Pyx_SafeReleaseBuffer(&__pyx_pybuffernd_arr.rcbuffer->pybuffer);
  ^~~~~~~~~~~~~~~~~~~~~~~
pandas/_libs/algos.c:83050:3: warning: code will never be executed [-Wunreachable-code]
  __Pyx_SafeReleaseBuffer(&__pyx_pybuffernd_arr.rcbuffer->pybuffer);
  ^~~~~~~~~~~~~~~~~~~~~~~
pandas/_libs/algos.c:83956:3: warning: code will never be executed [-Wunreachable-code]
  __Pyx_SafeReleaseBuffer(&__pyx_pybuffernd_arr.rcbuffer->pybuffer);
  ^~~~~~~~~~~~~~~~~~~~~~~
pandas/_libs/algos.c:84862:3: warning: code will never be executed [-Wunreachable-code]
  __Pyx_SafeReleaseBuffer(&__pyx_pybuffernd_arr.rcbuffer->pybuffer);
  ^~~~~~~~~~~~~~~~~~~~~~~
pandas/_libs/algos.c:85039:3: warning: code will never be executed [-Wunreachable-code]
  __Pyx_SafeReleaseBuffer(&__pyx_pybuffernd_arr.rcbuffer->pybuffer);
  ^~~~~~~~~~~~~~~~~~~~~~~
pandas/_libs/algos.c:85945:3: warning: code will never be executed [-Wunreachable-code]
  __Pyx_SafeReleaseBuffer(&__pyx_pybuffernd_arr.rcbuffer->pybuffer);
  ^~~~~~~~~~~~~~~~~~~~~~~
pandas/_libs/algos.c:89901:19: warning: assigning to '__pyx_t_5numpy_uint8_t *' (aka 'unsigned char *') from 'const __pyx_t_5numpy_uint8_t *' (aka 'const unsigned char *') discards qualifiers [-Wincompatible-pointer-types-discards-qualifiers]
        __pyx_v_v = (&(*((__pyx_t_5numpy_uint8_t const  *) ( /* dim=1 */ (( /* dim=0 */ (__pyx_v_values.data + __pyx_t_13 * __pyx_v_values.strides[0]) ) + __pyx_t_14 * __pyx_v_values.strides[1]) ))));
                  ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
pandas/_libs/algos.c:92699:19: warning: assigning to '__pyx_t_5numpy_int8_t *' (aka 'signed char *') from 'const __pyx_t_5numpy_int8_t *' (aka 'const signed char *') discards qualifiers [-Wincompatible-pointer-types-discards-qualifiers]
        __pyx_v_v = (&(*((__pyx_t_5numpy_int8_t const  *) ( /* dim=1 */ (( /* dim=0 */ (__pyx_v_values.data + __pyx_t_13 * __pyx_v_values.strides[0]) ) + __pyx_t_14 * __pyx_v_values.strides[1]) ))));
                  ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
pandas/_libs/algos.c:98041:19: warning: assigning to '__pyx_t_5numpy_int16_t *' (aka 'short *') from 'const __pyx_t_5numpy_int16_t *' (aka 'const short *') discards qualifiers [-Wincompatible-pointer-types-discards-qualifiers]
        __pyx_v_v = (&(*((__pyx_t_5numpy_int16_t const  *) ( /* dim=1 */ (( /* dim=0 */ (__pyx_v_values.data + __pyx_t_13 * __pyx_v_values.strides[0]) ) + __pyx_t_14 * __pyx_v_values.strides[1]) ))));
                  ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
pandas/_libs/algos.c:103383:19: warning: assigning to '__pyx_t_5numpy_int32_t *' (aka 'int *') from 'const __pyx_t_5numpy_int32_t *' (aka 'const int *') discards qualifiers [-Wincompatible-pointer-types-discards-qualifiers]
        __pyx_v_v = (&(*((__pyx_t_5numpy_int32_t const  *) ( /* dim=1 */ (( /* dim=0 */ (__pyx_v_values.data + __pyx_t_13 * __pyx_v_values.strides[0]) ) + __pyx_t_14 * __pyx_v_values.strides[1]) ))));
                  ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
pandas/_libs/algos.c:107434:19: warning: assigning to '__pyx_t_5numpy_int64_t *' (aka 'long *') from 'const __pyx_t_5numpy_int64_t *' (aka 'const long *') discards qualifiers [-Wincompatible-pointer-types-discards-qualifiers]
        __pyx_v_v = (&(*((__pyx_t_5numpy_int64_t const  *) ( /* dim=1 */ (( /* dim=0 */ (__pyx_v_values.data + __pyx_t_13 * __pyx_v_values.strides[0]) ) + __pyx_t_14 * __pyx_v_values.strides[1]) ))));
                  ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
pandas/_libs/algos.c:110194:19: warning: assigning to '__pyx_t_5numpy_float32_t *' (aka 'float *') from 'const __pyx_t_5numpy_float32_t *' (aka 'const float *') discards qualifiers [-Wincompatible-pointer-types-discards-qualifiers]
        __pyx_v_v = (&(*((__pyx_t_5numpy_float32_t const  *) ( /* dim=1 */ (( /* dim=0 */ (__pyx_v_values.data + __pyx_t_13 * __pyx_v_values.strides[0]) ) + __pyx_t_14 * __pyx_v_values.strides[1]) ))));
                  ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
pandas/_libs/algos.c:112955:19: warning: assigning to '__pyx_t_5numpy_float64_t *' (aka 'double *') from 'const __pyx_t_5numpy_float64_t *' (aka 'const double *') discards qualifiers [-Wincompatible-pointer-types-discards-qualifiers]
        __pyx_v_v = (&(*((__pyx_t_5numpy_float64_t const  *) ( /* dim=1 */ (( /* dim=0 */ (__pyx_v_values.data + __pyx_t_13 * __pyx_v_values.strides[0]) ) + __pyx_t_14 * __pyx_v_values.strides[1]) ))));
                  ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2 warnings generated.
2 warnings generated.
13 warnings generated.
pandas/_libs/tslibs/conversion.c:33343:20: warning: unused function '__pyx_memview_get_object' [-Wunused-function]
  static PyObject *__pyx_memview_get_object(const char *itemp) {
                   ^
pandas/_libs/tslibs/conversion.c:33348:12: warning: unused function '__pyx_memview_set_object' [-Wunused-function]
static int __pyx_memview_set_object(const char *itemp, PyObject *obj) {
           ^
2 warnings generated.
```"
587734984,33015,Added RuntimeWarning to lib.fast_unique_multiple,ShaharNaveh,closed,2020-03-25T14:25:39Z,2020-04-02T18:57:57Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
592826480,33243,DOC: Contributing - escaping backslash.,MarianD,closed,2020-04-02T18:05:19Z,2020-04-02T23:25:34Z,"Non-escaped backslash disappeared in resulting doc.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
367461450,23015,Cannot create python environment due to broken dependency,MatanCohe,closed,2018-10-06T14:23:17Z,2020-04-03T03:43:28Z,"#### Problem description
Following the  [documentation](https://pandas.pydata.org/pandas-docs/stable/contributing.html#contributing-to-pandas) I've tried to [create a python environment](https://pandas.pydata.org/pandas-docs/stable/contributing.html#creating-a-python-environment-pip)  and encountered this error message:

```bash
python -m pip install -r ci/requirements_dev.txt

Collecting PyEnchant>=1.6.5 (from sphinxcontrib-spelling->-r ci/requirements_dev.txt (line 14))
  Using cached https://files.pythonhosted.org/packages/9e/54/04d88a59efa33fefb88133ceb638cdf754319030c28aadc5a379d82140ed/pyenchant-2.0.0.tar.gz
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/tmp/pip-install-comptn20/PyEnchant/setup.py"", line 212, in <module>
        import enchant
      File ""/tmp/pip-install-comptn20/PyEnchant/enchant/__init__.py"", line 92, in <module>
        from enchant import _enchant as _e
      File ""/tmp/pip-install-comptn20/PyEnchant/enchant/_enchant.py"", line 145, in <module>
        raise ImportError(msg)
    ImportError: The 'enchant' C library was not found. Please install it via your OS package manager, or use a pre-built binary wheel from PyPI. 
```

Looking it up online I've found that the project [pyenchant](https://github.com/rfk/pyenchant) is no longer maintained.

```bash
uname -sro
Linux 4.18.10-arch1-1-ARCH GNU/Linux
```
```bash
pip -V
pip 18.0 from /path (python 3.7) 
```

```bash
pacman -Qi enchant

Name            : enchant
Version         : 2.2.3-1
Description     : A wrapper library for generic spell checking
Architecture    : x86_64
URL             : https://abiword.github.io/enchant/
Licenses        : LGPL
Groups          : None
Provides        : None
Depends On      : aspell  hunspell  hspell  libvoikko  glib2
Optional Deps   : None
Required By     : python-pyenchant  webkit2gtk
Optional For    : None
Conflicts With  : None
Replaces        : None
Installed Size  : 148.00 KiB
Packager        : Felix Yan <felixonmars@archlinux.org>
Build Date      : Thu Feb 22 11:36:06 2018
Install Date    : Sun Sep 30 16:13:39 2018
Install Reason  : Installed as a dependency for another package
Install Script  : No
Validated By    : Signature
```
"
302154656,19988,Shared Benchmark suite for Pandas-like projects,mrocklin,open,2018-03-05T02:45:11Z,2020-04-03T03:53:04Z,"It would be valuable to have a benchmark suite for Pandas-like projects.  This would help users reasonably compare the performance tradeoffs of different implementations and help developers identify possible performance issues.

There are, I think, a few axes that such a benchmark suite might engage:

1.  Operation type: filters, aggregations, random access, groupby-aggregate, set-index, merge, time series stuff, assignment, uniqueness, ...
2.  Datatype: grouping on ints, floats, strings, categoricals, etc.
3.  Cardinality: Lots of distinct floats, just a few common strings
4.  Data Size: How well do projects scale up?  How well do they scale down?
5.  Cluster size: for those projects for which this is appropriate
6.   (probably lots of other things I'm missing)

Additionally, there are a few projects that I think might benefit from such an endeavor

1.  Pandas itself
2.  Newer pandas developments (whatever gets built on top of arrow memory), which may have enough API compatibility to take advantage of this?
3.  Pandas on Ray (see this nice blogpost: https://rise.cs.berkeley.edu/blog/pandas-on-ray/)
4.  Dask.dataframe
5.  Spark dataframes?  If we can build in API tweaking (which I suspect will be necessary).

Some operational questions:

1.  How does one socially organize such a collection of benchmarks in a sensible way?  My guess is that no one individual is likely to have time to put this together (though I would *love* to be proved wrong here).  The objectives here are somewhat different from what currently lives in `asv_bench/benchmarks`.  
2.  How does one consistently execute such a benchmark?  I was looking at http://pytest-benchmark.readthedocs.io/en/latest 
3.  What challenges are we likely to observe due to the differences in each project?  How do we reasonably work around them?
4.  How do we avoid [developer bias](http://matthewrocklin.com/blog/work/2017/03/09/biased-benchmarks) when forming benchmarks?
5.  Does anyone have enthusiasm about working on this?

Anyway, those are some thoughts.  Please let me know if this is out of scope for this issue tracker."
584645321,32834,added cut as method to Series,R4HMATT,closed,2020-03-19T19:24:45Z,2020-04-03T04:08:56Z,"- [x] closes #28925 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Not sure if we need to totally re-implement cut. Please feel free to give any advice, I am new to contributing."
376264314,23440,BUG: IntervalTree construction fails for 32bit when n_elements > leaf_size,jschendel,open,2018-11-01T05:59:29Z,2020-04-03T04:15:39Z,"#### Code Sample, a copy-pastable example if possible

See https://travis-ci.org/MacPython/pandas-wheels/jobs/448674095 for some specific test failures.  The basic setup on a 32bit system is along the lines of:

```python
In [2]: left = np.arange(5)

In [3]: tree = pd._libs.interval.IntervalTree(left, left+2, leaf_size=2)
---------------------------------------------------------------------------
TypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'
```
xref https://github.com/pandas-dev/pandas/pull/23353#issuecomment-434642190

#### Problem description
`IntervalTree` construction fails for 32bit when n_elements > leaf_size.

#### Expected Output
I'd expect construction to be successful."
413743174,25422,ISO 8601 durations returned by Libreoffice/odfpy aren't compatible with current Timedelta parser,detrout,closed,2019-02-23T21:33:55Z,2020-04-03T04:32:15Z,"#### Code Sample, a copy-pastable example if possible

pandas.Timedelta('PT03H45M00S')

#### Problem description
Raises ValueError invalid abbreviation.

It should behave like:

pandas.Timedelta('P0DT0H1M0S')
Timedelta('0 days 00:01:00')

According to https://www.w3.org/TR/2004/REC-xmlschema-2-20041028/#duration

> If the number of years, months, days, hours, minutes, or seconds in any expression equals zero, the number and its corresponding designator ·may· be omitted. However, at least one number and its designator ·must· be present. 

Implies that durations like PT3H45M0S or P30D or PT2S are syntactically valid.

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.2.final.0
python-bits: 64
OS: Linux
OS-release: 4.19.0-2-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.3
pytest: 3.10.1
pip: 18.1
setuptools: 40.7.1
Cython: 0.29.2
numpy: 1.16.1
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 5.8.0
sphinx: 1.8.3
patsy: 0.5.0+dev
dateutil: 2.7.3
pytz: 2018.9
blosc: 1.7.0
bottleneck: None
tables: 3.4.4
numexpr: 2.6.9
feather: None
matplotlib: 3.0.2
openpyxl: 2.4.9
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: 4.3.0
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.2.15
pymysql: None
psycopg2: 2.7.7 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
568403090,32129,Feature request Multi Index pivot,kirialis,closed,2020-02-20T16:01:38Z,2020-04-03T04:34:16Z,"Hi, Would it be possible to implement pivot with a list of index ?

This will throw an error :
```python
df.pivot(index=['time', 'category'], columns='bar', values='baz')
```
But this does : (Found on Stack Overflow)
```python
def multiindex_pivot(df, index=None, columns=None, values=None):
    if index is None:
        names = list(df.index.names)
        df = df.reset_index()
    else:
        names = index
    list_index = df[names].values
    tuples_index = [tuple(i) for i in list_index] # hashable
    df = df.assign(tuples_index=tuples_index)
    df = df.pivot(index=""tuples_index"", columns=columns, values=values)
    tuples_index = df.index  # reduced
    index = pd.MultiIndex.from_tuples(tuples_index, names=names)
    df.index = index
    return df

df.pipe(multiindex_pivot, index=['time', 'category'], columns='bar', values='baz')
```

I just think it would be nice to have something directly available :)
"
505097539,28892,A question about pd.qcut,Janhonho,closed,2019-10-10T07:48:14Z,2020-04-03T05:47:04Z,"code:
t = [1]*4782+[2]*1030+[3]*251+[4]*122+[5]*177
t = pd.Series(t)
result = pd.qcut(t,50,duplicates = 'drop').value_counts()

problem:
print(result), we get
(0.999, 2.0]    5812
(2.0, 3.0]       251
(4.0, 5.0]       177
(3.0, 4.0]       122

t.value_counts(), we get
1    4782
2    1030
3     251
5     177
4     122

why 1 and 2 combines in the same bin?"
591981119,33208,DOC: Fixed examples in pandas/core/indexes/,ShaharNaveh,closed,2020-04-01T14:56:35Z,2020-04-03T10:57:32Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
546535060,30798,API: Store name outside attrs,TomAugspurger,closed,2020-01-07T22:10:47Z,2020-04-03T11:15:44Z,"This aligns with xarray and h5py:
https://github.com/pandas-dev/pandas/pull/29062#issuecomment-545703586"
592201153,33222,CLN: Use C-API for datetime.date,ShaharNaveh,closed,2020-04-01T21:00:37Z,2020-04-03T11:38:33Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
593305508,33258,import pandas error.,HarishGajjar,closed,2020-04-03T11:12:28Z,2020-04-03T12:03:44Z,"dear pandas-dev,
I am writing this to yo because I have facing problem with pandas lib to use it with python 3.8,
when I try to import pandas its shows error, please check attachment for more details.
Thanks.

=========================================================
error message
==========================================================
>>> import pandas
Traceback (most recent call last):
  File ""<pyshell#1>"", line 1, in <module>
    import pandas
  File ""C:\Python\Python38-32\lib\site-packages\pandas\__init__.py"", line 55, in <module>
    from pandas.core.api import (
  File ""C:\Python\Python38-32\lib\site-packages\pandas\core\api.py"", line 29, in <module>
    from pandas.core.groupby import Grouper, NamedAgg
  File ""C:\Python\Python38-32\lib\site-packages\pandas\core\groupby\__init__.py"", line 1, in <module>
    from pandas.core.groupby.generic import DataFrameGroupBy, NamedAgg, SeriesGroupBy
  File ""C:\Python\Python38-32\lib\site-packages\pandas\core\groupby\generic.py"", line 60, in <module>
    from pandas.core.frame import DataFrame
  File ""C:\Python\Python38-32\lib\site-packages\pandas\core\frame.py"", line 124, in <module>
    from pandas.core.series import Series
  File ""C:\Python\Python38-32\lib\site-packages\pandas\core\series.py"", line 4572, in <module>
    Series._add_series_or_dataframe_operations()
  File ""C:\Python\Python38-32\lib\site-packages\pandas\core\generic.py"", line 10349, in _add_series_or_dataframe_operations
    from pandas.core.window import EWM, Expanding, Rolling, Window
  File ""C:\Python\Python38-32\lib\site-packages\pandas\core\window\__init__.py"", line 1, in <module>
    from pandas.core.window.ewm import EWM  # noqa:F401
  File ""C:\Python\Python38-32\lib\site-packages\pandas\core\window\ewm.py"", line 5, in <module>
    import pandas._libs.window.aggregations as window_aggregations
ImportError: DLL load failed while importing aggregations: The specified module could not be found.
==============================================================
[import pandas error report.pdf](https://github.com/pandas-dev/pandas/files/4426773/import.pandas.error.report.pdf)
"
593304466,33257,CLN: Reorgenized doctests order,ShaharNaveh,closed,2020-04-03T11:10:20Z,2020-04-03T14:01:48Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

This does not changes anything, just moving things around, to make it more manageable (IMO)
"
591655557,33199,DOC: Fix Error in pandas.Series.last,farhanreynaldo,closed,2020-04-01T06:13:23Z,2020-04-03T14:42:43Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. 

output of `python scripts/validate_docstrings.py pandas.Series.last`:
```
################################################################################
################################## Validation ##################################
################################################################################
```

"
469368371,27435,Cannot install pandas 0.24.2 from source with Python 2.7 into an environment without NumPy,didip,closed,2019-07-17T18:09:20Z,2020-04-03T14:57:32Z,"#### Code Sample, a copy-pastable example if possible

```bash
RuntimeError: Python version >= 3.5 required.
```
#### Problem description

Under this docs: https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.24.0.html

0.24.x is supposed to be backward compatible to Python 2.

```
The 0.24.x series of releases will be the last to support Python 2. Future feature releases will support Python 3 only. See Plan for dropping Python 2.7 for more details
```

#### Expected Output

0.24.2 should still work on Python 2.
"
591001093,33176,DOC: Fix examples in pandas/core/ops/,ShaharNaveh,closed,2020-03-31T10:50:36Z,2020-04-03T16:17:15Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
421988696,25758,groupby with daily frequency fails with AmbiguousTimeError on clock change day in Cuba,pierremoulinier,closed,2019-03-17T23:41:52Z,2020-04-03T16:18:28Z,"#### Code Sample
```python
import pandas as pd
from datetime import datetime
start = datetime(2018, 11, 3, 12)
end = datetime(2018, 11, 5, 12)
index = pd.date_range(start, end, freq=""1H"")
index = index.tz_localize('UTC').tz_convert('America/Havana')
data = list(range(len(index)))
dataframe = pd.DataFrame(data, index=index)
groups = dataframe.groupby(pd.Grouper(freq='1D'))
```
#### Problem description
On a long clock-change day in Cuba, e.g 2018-11-04, midnight local time is an ambiguous timestamp. pd.Grouper does not handle this as I expect. More precisely the call to `groupby` in the code above raises an `AmbiguousTimeError`.

This issue is of a similar nature to https://github.com/pandas-dev/pandas/issues/23742 but it seems #23742 was fixed in 0.24 whereas this was not.

#### Expected Output
The call to `groupby` should return three groups (one for each day, 3rd, 4th, and 5th of november). The group for the 4th of november should be labelled as '2018-11-04 00:00:00-04:00' (that is the first midnight, before the clock change) and it should contain the 25 hourly data points for this day.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.8.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.125-linuxkit
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.2
pytest: 3.3.2
pip: None
setuptools: 40.6.3
Cython: 0.29.6
numpy: 1.15.4
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2016.6.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
589920696,33137,BUG: DataFrame.resample raised AmbiguousTimeError at a midnight DST transition,mroeschke,closed,2020-03-30T00:27:12Z,2020-04-03T16:18:34Z,"- [x] closes #25758
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
592608487,33237,"Warning ""Try using .loc[row_indexer,col_indexer] = value instead"" is given when using "".loc""",mborus,closed,2020-04-02T12:53:19Z,2020-04-03T16:23:49Z,"#### Code Sample, a copy-pastable example if possible

TLDR; Pandas warning is confusing since it recommends what seems to cause the warning.

```python

    # run this in one cell of a freshly started Jupyterlab Notebook
    # the warning is shown once, to see it again, restart the kernel

    import pandas as pd
    df = pd.DataFrame([list(range(1000)), list(range(1000)), list(range(1000)), list(range(1000))]).T
    df2 = df[[1, 3]]
    df2.loc[df2[1] > 500, 'marker'] = True
   

```
#### Problem description

This gives out the following warning:

```
C:\Python37\lib\site-packages\pandas\core\indexing.py:844: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  self.obj[key] = _infer_fill_value(value)
C:\Python37\lib\site-packages\pandas\core\indexing.py:965: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  self.obj[item] = s

```
This warning has confused me a many, many times, mainly, because it suggests
""try using .loc"" when that's exactly what I'm doing.

(I followed the link and read the doc and now know that I should have made a copy the
line before) - still the suggestion to use .loc is not helful here. 

#### Expected Output

Remove the line ""Try using .loc[row_indexer,col_indexer] = value instead""
when the code is using "".loc"". The rest of the warning is fine.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 7
machine          : AMD64
processor        : Intel64 Family 6 Model 42 Stepping 7, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : DE
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.16.2
pytz             : 2018.7
dateutil         : 2.7.5
pip              : 20.0.2
setuptools       : 40.8.0
Cython           : None
pytest           : 5.2.0
hypothesis       : None
sphinx           : 1.7.5
blosc            : None
feather          : None
xlsxwriter       : 1.1.2
lxml.etree       : 4.3.2
html5lib         : None
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.0.1
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.2
matplotlib       : 3.0.3
numexpr          : None
odfpy            : None
openpyxl         : 2.5.12
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.2.0
pyxlsb           : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : 1.2.15
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : 1.1.2
numba            : None


</details>
"
592956802,33248,REF: do slicing before calling Block.to_native_types,jbrockmendel,closed,2020-04-02T22:31:00Z,2020-04-03T17:38:24Z,"Related upcoming steps:

- Dispatch DatetimeBlock.to_native_types to DatetimeArray._format_native_types (identical behavior
- Dispatch TimeDeltaBlock.to_native_types to TimedeltaArray._format_native_types (behavior changing, but there is a FIXME in the Block version)
- Dispatch FloatBlock.to_native_types to Float64Index._format_native_types
- Possibly rename to_native_types to something more informative?  (i think there's an issue about deprecating/renaming the Index method)
- in io.csvs use `df._data.apply` instead of iterating over blocks.  Still touches internals, but its a lighter touch."
592799858,33240,CLN: remove Block.merge,jbrockmendel,closed,2020-04-02T17:22:50Z,2020-04-03T17:39:09Z,"Then move _merge_blocks to the one module where it is used, and add annotations"
54893880,9309,BUG: Timedelta repr does not show nanoseconds,sinhrks,closed,2015-01-20T15:09:02Z,2020-04-03T18:53:37Z,"Not sure this is intended, but `Timestamp` and `Timedelta` displays nanoseconds information inconsistently.
- `Timestamp` shows nanoseconds when it isn't 0

```
dt = pd.Timestamp(datetime.datetime.now())
h = pd.Timedelta(hours=1)

dt
#2015-01-21 00:01:05.260660
dt + offsets.Nano()
#2015-01-21 00:01:05.260660001
```
-  But `Timedelta` doesn't.

```
pd.Timedelta(nanoseconds=1)
#0 days 00:00:00.000000
pd.Timedelta(nanoseconds=1).nanoseconds
#1
```

I think `Timestamp` behavior is prefferable to avoid any misunderstanding. 
"
593026885,33250,Display Timedelta nanoseconds,jbrockmendel,closed,2020-04-03T02:07:48Z,2020-04-03T19:13:12Z,"- [x] closes #9309
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This allows us to make TimedeltaBlock.to_native_types dispatch to TimedeltaArray._format_native_types.  ATM TDBlock.to_native_types has a comment

```
        # FIXME:
        # should use the formats.format.Timedelta64Formatter here
        # to figure what format to pass to the Timedelta
        # e.g. to not show the decimals say
```"
593488868,33262,TST: don't assert that matplotlib rejects shorthand hex colors,rebecca-palmer,closed,2020-04-03T16:09:29Z,2020-04-03T19:14:34Z,"From 3.2 [it accepts them](https://matplotlib.org/users/prev_whats_new/whats_new_3.2.0.html#digit-and-4-digit-hex-colors).

[Test failure log](https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=954647) (from Debian's pandas 0.25, so includes other issues we don't now have - this one is DID NOT RAISE)."
593003203,33249,REF: dispatch DatetimeBlock.to_native_types to DTA._format_native_types,jbrockmendel,closed,2020-04-03T00:44:48Z,2020-04-03T19:42:46Z,xref #33248
87261929,10329,Strange error message when summing datetime64 and datetime.time column,jorisvandenbossche,closed,2015-06-11T08:40:14Z,2020-04-03T20:20:04Z,"You get `TypeError: Argument 'values' has incorrect type (expected numpy.ndarray, got Series)`:

```
In [29]: df = pd.DataFrame({'date':pd.date_range('2012-01-01', periods=3), 'time':[datetime.time(i, i, i) for i in range(3)]})

In [30]: df
Out[30]:
        date      time
0 2012-01-01  00:00:00
1 2012-01-02  01:01:01
2 2012-01-03  02:02:02

In [31]: df['date'] + df['time']
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-31-5101228e303e> in <module>()
----> 1 df['date'] + df['time']

C:\Anaconda\lib\site-packages\pandas\core\ops.pyc in wrapper(left, right, name)
    491             return NotImplemented
    492
--> 493         time_converted = _TimeOp.maybe_convert_for_time_op(left, right,
name)
    494
    495         if time_converted is None:

C:\Anaconda\lib\site-packages\pandas\core\ops.pyc in maybe_convert_for_time_op(c
ls, left, right, name)
    455         if name.startswith('__r'):
    456             name = ""__"" + name[3:]
--> 457         return cls(left, right, name)
    458
    459

C:\Anaconda\lib\site-packages\pandas\core\ops.pyc in __init__(self, left, right,
 name)
    272         self.right = right
    273         lvalues = self._convert_to_array(left, name=name)
--> 274         rvalues = self._convert_to_array(right, name=name, other=lvalues
)
    275
    276         self.is_timedelta_lhs = com.is_timedelta64_dtype(left)

C:\Anaconda\lib\site-packages\pandas\core\ops.pyc in _convert_to_array(self, val
ues, name, other)
    354             elif not (isinstance(values, (np.ndarray, pd.Series)) and
    355                       com.is_datetime64_dtype(values)):
--> 356                 values = tslib.array_to_datetime(values)
    357         elif inferred_type in ('timedelta', 'timedelta64'):
    358             # have a timedelta, convert to to ns here

TypeError: Argument 'values' has incorrect type (expected numpy.ndarray, got Series)
```
"
584712739,32841,SegmentationFault/BusError: Groupby + count,tv3141,closed,2020-03-19T21:32:49Z,2020-04-04T00:01:46Z,"#### Code Sample

```python
import pandas as pd
import numpy as np
df = pd.DataFrame({""A"": [0, 1, np.NaN], ""B"": [1, 2, 3]})
df.groupby([""A""]).count()
```

#### Problem description

This randomly causes a SegFault or Bus error.

With bounds checking enabled I get the following IndexError.

https://github.com/pandas-dev/pandas/blob/master/pandas/_libs/lib.pyx#L778
```python
@cython.boundscheck(True)
@cython.wraparound(False)
def count_level_2d(ndarray[uint8_t, ndim=2, cast=True] mask,
...
```

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/pandas/pandas/core/groupby/generic.py"", line 1784, in count
    blocks = [make_block(val, placement=loc) for val, loc in zip(counted, locs)]
  File ""/pandas/pandas/core/groupby/generic.py"", line 1784, in <listcomp>
    blocks = [make_block(val, placement=loc) for val, loc in zip(counted, locs)]
  File ""/pandas/pandas/core/groupby/generic.py"", line 1782, in <genexpr>
    lib.count_level_2d(x, labels=ids, max_bin=ngroups, axis=1) for x in vals
  File ""pandas/_libs/lib.pyx"", line 803, in pandas._libs.lib.count_level_2d
    counts[i, labels[j]] += mask[i, j]
IndexError: Out of bounds on buffer access (axis 1)
```

#### Expected Output

```python
     B
A     
0.0  1
1.0  1
```

### Note

This issue is related to https://github.com/pandas-dev/pandas/issues/21824. The Segfault happens in the same function, but the call stack is different.

Code sample from https://github.com/pandas-dev/pandas/issues/21824:
```python
import numpy as  np
import pandas as pd

df = pd.DataFrame({""Person"": [""John"", ""Myla"", None, ""John"", ""Myla""],
                   ""Age"": [24., np.nan, 21., 33, 26],
                   ""Single"": [False, True, True, True, False]})
                  
res = df.set_index([""Person"", ""Single""]).count(level=""Person"")
```

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/pandas/pandas/core/frame.py"", line 7786, in count
    return self._count_level(level, axis=axis, numeric_only=numeric_only)
  File ""/pandas/pandas/core/frame.py"", line 7842, in _count_level
    counts = lib.count_level_2d(mask, level_codes, len(level_index), axis=0)
  File ""pandas/_libs/lib.pyx"", line 796, in pandas._libs.lib.count_level_2d
    counts[labels[i], j] += mask[i, j]
IndexError: Out of bounds on buffer access (axis 0)
```


#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : 981252e7adb74b3aca2d9dfd075b0b64c3552975
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Thu Jan 23 06:52:12 PST 2020; root:xnu-4903.278.25~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.0.dev0+870.g981252e7a
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0.post20200311
Cython           : 0.29.15
pytest           : 5.4.1
hypothesis       : 5.6.0
sphinx           : 2.4.4
blosc            : 1.8.3
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : 0.3.3
gcsfs            : None
matplotlib       : 3.2.0
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : 0.15.0
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0

</details>
"
339267974,21824,Segfault on clean-up with count example from docstrings,jorisvandenbossche,closed,2018-07-08T23:32:32Z,2020-04-04T00:01:47Z,"In https://github.com/pandas-dev/pandas/pull/19952 I have problems with a segfault when running doctests, and I could isolate at least the following code snippet that segfaults on clean-up (so only when exiting the process; an explicit del + gc.collect does not yet trigger the segfault).

```
import numpy as  np
import pandas as pd

df = pd.DataFrame({""Person"": [""John"", ""Myla"", None, ""John"", ""Myla""],
                   ""Age"": [24., np.nan, 21., 33, 26],
                   ""Single"": [False, True, True, True, False]})
                  
res = df.set_index([""Person"", ""Single""]).count(level=""Person"")
```

"
584713212,32842,BUG: Fix segfault in GroupBy.count and DataFrame.count,tv3141,closed,2020-03-19T21:33:52Z,2020-04-04T00:01:54Z,"- [x] closes #32841 
- [x] closes #21824
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
589591959,33100,TYP: require Index objects earlier in internals,jbrockmendel,closed,2020-03-28T14:54:30Z,2020-04-04T00:43:59Z,
591919620,33204,Add test for #24615 (Accept PandasArray (with correct dtype) in DatetimeArray constructor),gorogoroumaru,closed,2020-04-01T13:32:47Z,2020-04-04T01:01:23Z,"issue #24615

I added a test for the issue 24615."
40502857,8055,read_csv segfault with file open in LibreOffice,rockg,closed,2014-08-18T15:53:43Z,2020-04-04T03:34:10Z,"I'm trying to read a simple file which works fine when I don't have the file open.  However, when I have the file open in LibreOffice it reliably segfaults (memory steadily increases and network traffic is at 70MB/s for a good 30 seconds).  When open in the TextEditor it works fine.

csv data:

ID,Start Date,End Date
1,2014-09-16 00:00:00-05:00,2014-12-31 23:00:00-05:00
2,2014-10-02 00:00:00-05:00,2014-10-10 15:00:00-05:00

```
(gdb) backtrace
#0  0x00007ffff7879d79 in realloc () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007fffede557de in safe_realloc (buffer=0x7fff6b6e9010, size=size@entry=2) at pandas/src/parser/tokenizer.c:55
#2  0x00007fffede55858 in grow_buffer (buffer=<optimized out>, buffer@entry=0x7fff6b6e9010, length=<optimized out>, capacity=capacity@entry=0x11fe66c, space=space@entry=524288, 
    elsize=elsize@entry=1, error=error@entry=0x7fffffffd25c) at pandas/src/parser/tokenizer.c:115
#3  0x00007fffede558c4 in make_stream_space (self=self@entry=0x11fe630, nbytes=262144) at pandas/src/parser/tokenizer.c:294
#4  0x00007fffede56641 in tokenize_delimited (self=0x11fe630, line_limit=262143) at pandas/src/parser/tokenizer.c:670
#5  0x00007fffede58021 in _tokenize_helper (self=0x11fe630, nrows=262143, all=all@entry=0) at pandas/src/parser/tokenizer.c:1573
#6  0x00007fffede58297 in tokenize_nrows (self=<optimized out>, nrows=<optimized out>) at pandas/src/parser/tokenizer.c:1590
#7  0x00007fffede215c6 in __pyx_f_6pandas_6parser_10TextReader__tokenize_rows (__pyx_v_self=0x7fffeda38dc0, __pyx_v_nrows=<optimized out>) at pandas/parser.c:7784
#8  0x00007fffede24aef in __pyx_f_6pandas_6parser_10TextReader__read_rows (__pyx_v_self=0x7fffeda38dc0, __pyx_v_rows=0xf55488, __pyx_v_trim=0) at pandas/parser.c:7978
#9  0x00007fffede23a2e in __pyx_f_6pandas_6parser_10TextReader__read_low_memory (__pyx_v_self=0x7fffeda38dc0, __pyx_v_rows=0x920190 <_Py_NoneStruct>) at pandas/parser.c:7369
#10 0x00007fffede22474 in __pyx_pf_6pandas_6parser_10TextReader_8read (__pyx_v_rows=0x920190 <_Py_NoneStruct>, __pyx_v_self=0x7fffeda38dc0) at pandas/parser.c:7145
#11 __pyx_pw_6pandas_6parser_10TextReader_9read (__pyx_v_self=0x7fffeda38dc0, __pyx_args=<optimized out>, __pyx_kwds=<optimized out>) at pandas/parser.c:7107
#12 0x000000000052c6d5 in PyEval_EvalFrameEx ()
#13 0x000000000055c594 in PyEval_EvalCodeEx ()
#14 0x000000000052ca8d in PyEval_EvalFrameEx ()
#15 0x000000000055c594 in PyEval_EvalCodeEx ()
#16 0x000000000052ca8d in PyEval_EvalFrameEx ()
#17 0x000000000055c594 in PyEval_EvalCodeEx ()
#18 0x000000000052ca8d in PyEval_EvalFrameEx ()
#19 0x000000000055c594 in PyEval_EvalCodeEx ()
#20 0x000000000052ca8d in PyEval_EvalFrameEx ()
#21 0x000000000055c594 in PyEval_EvalCodeEx ()
#22 0x00000000005b7392 in PyEval_EvalCode ()
#23 0x0000000000469663 in ?? ()
#24 0x00000000004699e3 in PyRun_FileExFlags ()
#25 0x0000000000469f1c in PyRun_SimpleFileExFlags ()
#26 0x000000000046ab81 in Py_Main ()
#27 0x00007ffff7817ec5 in __libc_start_main () from /lib/x86_64-linux-gnu/libc.so.6
#28 0x000000000057497e in _start ()
```
"
267818667,17961,DataFrameGroupBy.quantile breaks on axis=1 using a multi-indexed dataframe,levlitichev,closed,2017-10-23T21:10:04Z,2020-04-04T04:45:35Z,"I am trying to group by a particular level in a dataframe with multi-indexed columns. In particular, I want to use the `quantile` function. It looks like `quantile` breaks for columns but not for rows, and other functions like `mean` work fine.

```python
df = pd.DataFrame(np.random.rand(3, 4), columns=[[""A"", ""A"", ""B"", ""B""], range(4)])
df.columns.names = [""first"", ""second""]
df
```

first | A | A | B |  B
:--- | :--- | :--- | :--- | :---
**second** | **0** | **1** | **2** | **3**
0 | 0.942337 | 0.090621 | 0.977834 | 0.332177
1 | 0.301687 | 0.907762 | 0.062494 | 0.091152
2 | 0.554201 | 0.282348 | 0.344425 | 0.941074

Calling `quantile` on the columns breaks:
```python
df.groupby(level=""first"", axis=1).quantile(0.75)
```

> TypeError                                 Traceback (most recent call last)
> <ipython-input-108-ca58d6677903> in <module>()
> ----> 1 df.groupby(level=""first"", axis=1).quantile(0.75)
> 
> /Users/lev/miniconda2/envs/cmappy/lib/python2.7/site-packages/pandas/core/groupby.pyc in quantile(self, q, axis, numeric_only, interpolation)
> 
> /Users/lev/miniconda2/envs/cmappy/lib/python2.7/site-packages/pandas/core/groupby.pyc in wrapper(*args, **kwargs)
>     610                     try:
>     611                         return self._aggregate_item_by_item(name,
> --> 612                                                             *args, **kwargs)
>     613                     except (AttributeError):
>     614                         raise ValueError
> 
> /Users/lev/miniconda2/envs/cmappy/lib/python2.7/site-packages/pandas/core/groupby.pyc in _aggregate_item_by_item(self, func, *args, **kwargs)
>    3554             # GH6337
>    3555             if not len(result_columns) and errors is not None:
> -> 3556                 raise errors
>    3557 
>    3558         return DataFrame(result, columns=result_columns)
> 
> TypeError: quantile() got an unexpected keyword argument 'numeric_only'


But `mean` works okay:
```python
df.groupby(level=""first"", axis=1).mean()
```

first	| A | B
--- | --- | ---
0 | 0.516479 | 0.655005
1 | 0.604725 | 0.076823
2 | 0.418274 | 0.642749

Transposing fixes the problem:
```python
df.T.groupby(level=""first"", axis=0).quantile(0.75).T
```

first | A | B |
--- | --- | --- |
0.75 | | |
0 |	0.729408 |	0.816419 |
1 |	0.756243 |	0.083987 |
2 |	0.486237 |	0.791911 |

#### Expected Output

The expected output is the last result above, but it'd be nice not to have to transpose and then to re-transpose to get it to work.

Thank you very much for all your good work!

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.13.final.0
python-bits: 64
OS: Darwin
OS-release: 16.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.20.3
pytest: None
pip: 9.0.1
setuptools: 33.1.1.post20170320
Cython: None
numpy: 1.12.1
scipy: 0.19.0
xarray: None
IPython: 5.4.1
sphinx: None
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.999
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.9.5
s3fs: None
pandas_gbq: None
pandas_datareader: None
None
</details>
"
319176352,20900,TypeError for mixed-type indices in python3,normanius,closed,2018-05-01T11:30:28Z,2020-04-04T04:48:13Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np
table = pd.DataFrame(data=np.random.randn(5,2), index=[4,1,'foo',3,'bar'])
table = table.sort_index()
```
#### Problem description

The code above works in python2. However, in python3 a `TypeError` is raised: 
```
TypeError: '<' not supported between instances of 'str' and 'int'
```
The reason for this is described [here](https://stackoverflow.com/questions/50097704): mixed-type sequencing cannot be sorted anymore just like this. 

#### Expected Output

No exception when calling `sort_index` in python3.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None

pandas: 0.22.0
pytest: None
pip: 10.0.0
setuptools: 39.0.1
Cython: None
numpy: 1.14.2
scipy: 1.0.1
pyarrow: None
xarray: None
IPython: 6.3.1
sphinx: None
patsy: None
dateutil: 2.7.2
pytz: 2018.4
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.2.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
592287929,33228,Clean Up Categorical Test for JSON,WillAyd,closed,2020-04-02T00:55:27Z,2020-04-04T15:25:12Z,"This is a little wonky in its current state, bolting categoricals onto an existing object / fixture

Instead just localized this to the one test that covers it for now"
593710001,33280,Troubleshoot Travis,jbrockmendel,closed,2020-04-04T00:59:49Z,2020-04-04T17:17:08Z,"Reverts #33228, which appears to be the first build that started failing on Travis."
593257899,33255,BUG: Cant access some `TimedeltaProperties` components directly,ShaharNaveh,closed,2020-04-03T09:47:06Z,2020-04-04T18:32:25Z,"XREF: https://github.com/pandas-dev/pandas/pull/33208#discussion_r402181910

---

#### Note:

I'm using ""hours"" here for the examples, but this applies to

- hours
- minutes
- milliseconds

---

#### Short explanation:

Not all components are exposed directly:

Example:

```
In [1]: import pandas as pd                                                                                   

In [2]: s = pd.Series(pd.timedelta_range(start=""1 day"", periods=5, freq=""H""))                                 

In [3]: s.dt.components                                                                                       
Out[3]: 
   days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds
0     1      0        0        0             0             0            0
1     1      1        0        0             0             0            0
2     1      2        0        0             0             0            0
3     1      3        0        0             0             0            0
4     1      4        0        0             0             0            0

In [4]: s.dt.days                                                                                             
Out[4]: 
0    1
1    1
2    1
3    1
4    1
dtype: int64

In [5]: s.dt.hours                                                                                            
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-5-86f203890f2a> in <module>
----> 1 s.dt.hours

AttributeError: 'TimedeltaProperties' object has no attribute 'hours'
```


---

#### Long explanation:

If we create a series that contains a timedelta, we can access only some of the components directly.

Example setup:

```
In [1]: import pandas as pd                                                                                   

In [2]: s = pd.Series(pd.timedelta_range(start=""1 day"", periods=5, freq=""H""))                                 

In [3]: s                                                                                                     
Out[3]: 
0   1 days 00:00:00
1   1 days 01:00:00
2   1 days 02:00:00
3   1 days 03:00:00
4   1 days 04:00:00
dtype: timedelta64[ns]

In [4]: s.dt.components                                                                                       
Out[4]: 
   days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds
0     1      0        0        0             0             0            0
1     1      1        0        0             0             0            0
2     1      2        0        0             0             0            0
3     1      3        0        0             0             0            0
4     1      4        0        0             0             0            0
```

We can access ""days"" and ""seconds"" directly, for example:

```
In [5]: s.dt.days                                                                                             
Out[5]: 
0    1
1    1
2    1
3    1
4    1
dtype: int64

In [6]: s.dt.seconds                                                                                          
Out[6]: 
0        0
1     3600
2     7200
3    10800
4    14400
dtype: int64
```

But if we try to access ""hours"" for example directly:

```
In [7]: s.dt.hours                                                                                            
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-7-86f203890f2a> in <module>
----> 1 s.dt.hours

AttributeError: 'TimedeltaProperties' object has no attribute 'hours'
```

---

Output of ``` pd.show_versions() ```:

<details>

INSTALLED VERSIONS
------------------
commit           : 37dc5dc391333fede424964a3c8ab034318d6ed4
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.5.13.a-1-hardened
Version          : #1 SMP PREEMPT Wed, 25 Mar 2020 21:46:24 +0000
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0.dev0+1089.g37dc5dc39
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0.post20200311
Cython           : 0.29.16
pytest           : 5.4.1
hypothesis       : 5.8.0
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : 0.3.3
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0

</details>"
583125964,32777,Backport PR #32734 on branch 1.0.x,jbrockmendel,closed,2020-03-17T16:08:20Z,2020-04-04T20:41:41Z,
593790467,33282,DOC: Fix error in Series.clip and DataFrame.clip,farhanreynaldo,closed,2020-04-04T10:13:53Z,2020-04-04T21:23:10Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. 

output of `python scripts/validate_docstrings.py pandas.Series.clip and pandas.DataFrame.clip`:
```
################################################################################
################################## Validation ##################################
################################################################################
```

"
485441006,28155,Inconsistency with ints and floats,vlbrown,closed,2019-08-26T20:41:13Z,2020-04-05T00:12:40Z,"#### Problem description

   1. pandas statistics sometimes produce numpy data types and sometimes not. This is inconsistent.
   2. json_dumps cannot serialize int64 (not a pandas issue but that's how I got here)

 I created a dictionary containing pandas stats.  The input is a pandas series, s, dtype float64.

The methods used are
```
s.count()
s.max()
s.min()
s.mean()
s.median()
s.mode()[0]
s.std()
```

Apparently, pandas returns numpy ints and numpy floats (sometimes).
```
count 564 <class 'numpy.int64'>
max 33.46 <class 'float'>
min 7.36 <class 'float'>
mean 24.727588652482286 <class 'float'>
median 26.28 <class 'float'>
mode 25.6 <class 'numpy.float64'>
std 5.857237135129123 <class 'float'>
````
Note:

This discrepancy matters because  `json.dump` cannot currently serialize objects of type int64. 

Apparently pandas DataFrame.to_json() method has no such issues. Converting the dictionary to a dataframe and then writing it out to JSON works without error.

Perhaps pandas could share the `to_json` code with the Python json library devs, resolving [Python issue 24313](https://bugs.python.org/issue24313)


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 15.6.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.0
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : 0.29.12
pytest           : 5.0.1
hypothesis       : None
sphinx           : 2.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.3.4
html5lib         : 1.0.1
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.7.0
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.1.0
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : 1.3.5
tables           : 3.5.2
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8
</details>
"
491361940,28365,"pandas.DataFrame.boxplot(by=""class"", ...) doesn't plot correctly",yuriikoval1997,closed,2019-09-09T22:45:13Z,2020-04-05T00:13:11Z,"#### Problem description
**pandas=0.24.2**
I uploaded the jupyter notebook file on [my GitHub account](https://github.com/yuriikoval1997/pandas0.24.2-has-a-problem-with-cyrillic/blob/master/pandas%20bugs.ipynb), so here I will be short.
For some reason, pandas.DataFrame.boxplot(by=""class"") method doesn't work correctly with Cyrillic letters in the table.

```python
for col in list(dataSetCyrillic.columns[:-1]):
    fig, ax = pyplot.subplots(figsize=(6, 6))
    dataSetCyrillic.boxplot(column=[col], 
                        by=""class"", 
                        ax=ax, 
                        widths=0.5)
    tmp = map(lambda x: x.replace("" "", ""\n"", 1), dataSetCyrillic[""class""].unique())
    ax.set_xticklabels(tmp, fontsize=""20"")
    ax.set_yticklabels(ax.get_yticks(), fontsize=""xx-large"")
    ax.set_title(label=col, fontsize=""xx-large"", fontstyle=""oblique"", fontweight=""bold"")
    pyplot.xlabel("""")
    pyplot.suptitle("""")
    fig.tight_layout()
```"
549263771,30993,BUG: Kurtosis bug?,mindryu,closed,2020-01-14T00:38:59Z,2020-04-05T00:15:54Z,"Why pandas kurtosis and scipy kurtosis result is different?
During the test, I also realized that the Pandas kurtosis value was constantly changing according to the starting point.

![1578962134989](https://user-images.githubusercontent.com/3617014/72303684-6633bf00-36b1-11ea-8d50-92d75cd77de7.png)
"
353682595,22497,Saving and reloading a csv on Windows with a \n-character in a field replaces \n with \r\n.,Khris777,open,2018-08-24T07:56:23Z,2020-04-05T01:19:34Z,"#### Code Sample, a copy-pastable example if possible

```python
filename = ""E:\\Temp\\newline_test.csv""
import pandas as pd
df1 = pd.DataFrame({""A"":[""test"",""te\nst""]})
df1.to_csv(filename,index=False)
df2 = pd.read_csv(filename)
print(df1)
print(df2)
```
#### Problem description
Take a dataframe that has a `\n`-newline character in a field:

            A
    0    test
    1  te\nst

Save it normally on Windows 10.

After reloading the `\n` has been converted to `\r\n`:

              A
    0      test
    1  te\r\nst

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.6.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 79 Stepping 1, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en
LOCALE: None.None

pandas: 0.23.4
pytest: 3.5.1
pip: 18.0
setuptools: 39.0.1
Cython: 0.28.4
numpy: 1.14.5
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.5.0
sphinx: 1.7.4
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: 1.2.1
tables: 3.4.3
numexpr: 2.6.4
feather: None
matplotlib: 2.2.3
openpyxl: 2.5.4
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.5
lxml: 4.2.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.7
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: 0.1.5
pandas_gbq: None
pandas_datareader: None
</details>
"
391498520,24310,BUG: Series.__repr__ crashing with tzlocal(),h-vetinari,open,2018-12-16T19:44:39Z,2020-04-05T01:19:51Z,"Encountered while writing tests for PR to #23833, where I wanted to use `tz_aware_fixture` for testing promotions of datetimes with tz. The following is an equivalent minimal example, but constructed directly.

```
>>> import pandas as pd
>>> from dateutil.tz import tzlocal
>>> from pandas.core.dtypes.dtypes import DatetimeTZDtype
>>> dtype = DatetimeTZDtype(tz = tzlocal())
>>> s = pd.Series([10 ** 9], dtype=dtype)
```
The constructor passes, but worryingly, already the repr of `s` fails with a huge stacktrace:
```
>>> s
Traceback (most recent call last):
[~130 lines of stacktrace]
OSError: [Errno 22] Invalid argument
```
Same goes for `s[0]`.

The underlying values are still there though:
```
>>> s.values
array(['1970-01-01T00:00:01.000000000'], dtype='datetime64[ns]')
>>> s._data
SingleBlockManager
Items: RangeIndex(start=0, stop=1, step=1)
DatetimeTZBlock: 1 dtype: datetime64[ns, tzlocal()]
```

This seems like a pretty serious regression to me. @mroeschke @jbrockmendel @jreback @TomAugspurger @jorisvandenbossche "
95845375,10616,read_fwf will not convert columns to specified data type when initial row instances are blank,LoneStar134,open,2015-07-18T17:59:46Z,2020-04-05T01:24:42Z,"I am trying to read a fixed width file using read_fwf function and coerce the column data types by using the 'converters' parameter.  I created a dictionary that specifies a function that converts to the type that I desire for each of the columns in the data file.  

The problem is that not all columns are getting set to the type I specify and it appears that any column for which the first few rows of the data set are blank will be inferred or defaulted and not coerced to the data type I explicitly specified in the dictionary passed via the converters parameter.

As a test, I tried to coerce all column types to a string by passing a dictionary that looked something like this:

type_dict = {0: conv_str, 1: conv_str, 2: conv_str ... (n-1): conv_str), where n = number of columns 

and conv_str is a function that is defined as follows:

def conv_str(x):
    return str(x)

As previously explained, in the resulting data frame, all columns get converted to type string as desired, with the exception of the columns that had blank values for the first few rows of the data set.  Those columns that had blank values get defaulted to the 'float' data type.
"
197012888,14945,Column inference in read_fwf() has unexpected outcome for sparse files,jonemo,open,2016-12-21T19:24:50Z,2020-04-05T01:27:24Z,"#### Code Sample

```python
import io
import pandas as pd

filecontent = u""""""
Qty Value                                Parts                        Description                       MAN_PART_NO        VENDOR
1   200kOhm                              R5                           Resistor                          MCR01MRTF2003      Digikey
5   FACE_BOARD_CONNECTOR                 U$10, U$11, U$12, U$13, U$14                                   1-84981-0          Digikey
1   I2C_VOLTAGE_LEVEL_TRANSLATOR         U$27                                                           PCA9306DCUR        Digikey
2   LED                                  GREEN, ORGNE                 Small inidicator LED              SML-P11MTT86       Digikey
1   MEGA164A/PA/324A/PA/644A/PA/1284/PAU U1                           TQFP44 package of Atmega1284      ATMEGA1284-AUR     Digikey
1   POWER_REG5.0V-SOT23                  VR5.0                        Linear Power Regulator            MIC5219-5.0YM5 TR  Digikey
2   RESISTOR_ARRAY                       RA1, RA2                                                       CRA04S08310K0JTD   Digikey
""""""

print pd.read_fwf(io.StringIO(filecontent), colspecs='infer', skiprows=1)

```
#### Problem description

In electronics, some older CAD softwares export the Bill of Materials (BOM) document as fixed width text file. Importing such files using `pd.read_fwf(..., colspec='infer')` often results in unexpected outcomes. This is because the colspec inference isn't written with files in mind that exhibit the following two properties:

* high variance in content length of cells in a column
* space (or other delimiter) characters within the cell content

The code sample uses a simplified example BOM document from an open source electronics project that exhibits these properties: Note that the ""Parts"" column is split up into multiple columns.

#### Actual Output

```
   Qty                                 Value              Parts Unnamed: 3  \
0    1                               200kOhm                 R5        NaN   
1    5                  FACE_BOARD_CONNECTOR  U$10, U$11, U$12,      U$13,   
2    1          I2C_VOLTAGE_LEVEL_TRANSLATOR               U$27        NaN   
3    2                                   LED       GREEN, ORGNE        NaN   
4    1  MEGA164A/PA/324A/PA/644A/PA/1284/PAU                 U1        NaN   
5    1                   POWER_REG5.0V-SOT23              VR5.0        NaN   
6    2                        RESISTOR_ARRAY           RA1, RA2        NaN   

  Unnamed: 4                   Description        MAN_PART_NO   VENDOR  
0        NaN                      Resistor      MCR01MRTF2003  Digikey  
1       U$14                           NaN          1-84981-0  Digikey  
2        NaN                           NaN        PCA9306DCUR  Digikey  
3        NaN          Small inidicator LED       SML-P11MTT86  Digikey  
4        NaN  TQFP44 package of Atmega1284     ATMEGA1284-AUR  Digikey  
5        NaN        Linear Power Regulator  MIC5219-5.0YM5 TR  Digikey  
6        NaN                           NaN   CRA04S08310K0JTD  Digikey
```

#### Expected Output

```
   Qty                                 Value                         Parts   \
0    1                               200kOhm                            R5  
1    5                  FACE_BOARD_CONNECTOR  U$10, U$11, U$12, U$13, U$14   
2    1          I2C_VOLTAGE_LEVEL_TRANSLATOR                          U$27   
3    2                                   LED                  GREEN, ORGNE   
4    1  MEGA164A/PA/324A/PA/644A/PA/1284/PAU                            U1   
5    1                   POWER_REG5.0V-SOT23                         VR5.0   
6    2                        RESISTOR_ARRAY                      RA1, RA2   

                    Description        MAN_PART_NO   VENDOR  
0                      Resistor      MCR01MRTF2003  Digikey  
1                           NaN          1-84981-0  Digikey  
2                           NaN        PCA9306DCUR  Digikey  
3          Small inidicator LED       SML-P11MTT86  Digikey  
4  TQFP44 package of Atmega1284     ATMEGA1284-AUR  Digikey  
5        Linear Power Regulator  MIC5219-5.0YM5 TR  Digikey  
6                           NaN   CRA04S08310K0JTD  Digikey
```

#### Possible Improvements

Right now any set of neighboring columns of the text file that contain delimiter characters only across all rows is considered a boundary. This is prone to include false positives. Possible alternative strategies for colspec inference:

* Require more than one column to transition from delimiter to non-delimiter character. A possible cutoff might be the median number of occurrences of this transition across all columns of the file.
* Consider the header row (if present) separately from the content rows and give it additional weight.

I realize that this use case falls outside the usual/common use cases of pandas and wouldn't be surprised if this gets triaged as #wontfix or if my suggested improvements interfere with more common use cases that I am not aware of. If maintainers agree that my suggestions would add value, I'd be happy to contribute the PR.

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.12.final.0
python-bits: 64
OS: Darwin
OS-release: 15.6.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.19.1
nose: None
pip: 9.0.1
setuptools: 32.1.2
Cython: None
numpy: 1.11.3
scipy: None
statsmodels: None
xarray: None
IPython: 5.1.0
sphinx: None
patsy: None
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.9999999
httplib2: None
apiclient: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.8
boto: None
pandas_datareader: None
```

</details>
"
311098675,20603,pandas.read_fwf doesn't work with skiprows=callable,ghost,open,2018-04-04T06:17:14Z,2020-04-05T01:28:26Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas

table = """"""\
id8141    360.242940   149.910199   11950.7
id1594    444.953632   166.985655   11788.4
id1849    364.136849   183.628767   11806.2
id1230    413.836124   184.375703   11916.8
id1948    502.953953   173.237159   12468.3""""""

fwf_path = ""fwf.dat""
with open(fwf_path, ""wb"") as fh:
    fh.write(table.encode('utf8'))

def should_skip(row):
    return True # normally condition

table = pandas.read_fwf(fwf_path, skiprows=should_skip)
```
#### Problem description

> 
> Traceback (most recent call last):
>   File ""fwf_bug.py"", line 19, in <module>
>     table = pandas.read_fwf(fwf_path, skiprows=should_skip)
>   File ""some-path/_venv/lib/python3.5/site-packages/pandas/io/parsers.py"", line 741, in read_fwf
>     return _read(filepath_or_buffer, kwds)
>   File ""some-path/_venv/lib/python3.5/site-packages/pandas/io/parsers.py"", line 449, in _read
>     parser = TextFileReader(filepath_or_buffer, **kwds)
>   File ""some-path/_venv/lib/python3.5/site-packages/pandas/io/parsers.py"", line 818, in __init__
>     self._make_engine(self.engine)
>   File ""some-path/_venv/lib/python3.5/site-packages/pandas/io/parsers.py"", line 1059, in _make_engine
>     self._engine = klass(self.f, **self.options)
>   File ""some-path/_venv/lib/python3.5/site-packages/pandas/io/parsers.py"", line 3412, in __init__
>     PythonParser.__init__(self, f, **kwds)
>   File ""some-path/_venv/lib/python3.5/site-packages/pandas/io/parsers.py"", line 2079, in __init__
>     self._make_reader(f)
>   File ""some-path/_venv/lib/python3.5/site-packages/pandas/io/parsers.py"", line 3416, in _make_reader
>     self.comment, self.skiprows)
>   File ""some-path/_venv/lib/python3.5/site-packages/pandas/io/parsers.py"", line 3316, in __init__
>     self.colspecs = self.detect_colspecs(skiprows=skiprows)
>   File ""some-path/_venv/lib/python3.5/site-packages/pandas/io/parsers.py"", line 3373, in detect_colspecs
>     rows = self.get_rows(n, skiprows)
>   File ""some-path/_venv/lib/python3.5/site-packages/pandas/io/parsers.py"", line 3361, in get_rows
>     if i not in skiprows:
> TypeError: argument of type 'function' is not iterable

#### Expected Output

Script runs successfully

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Linux
OS-release: 4.13.0-37-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.22.0
pytest: None
pip: 9.0.3
setuptools: 39.0.1
Cython: None
numpy: 1.14.2
scipy: None
pyarrow: None
xarray: None
IPython: 6.3.0
sphinx: None
patsy: None
dateutil: 2.7.2
pytz: 2018.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: 2.5.1
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
304392781,20301,SAS na values management,kharazu,open,2018-03-12T14:25:09Z,2020-04-05T01:45:18Z,"#### Problem description
Hi, I'm writing this report to ask for a na values management parameter for method .read_sas(), just like it's possible to do for .csv tables properly setting na_values or keep_default_na, it would be really useful to directly set na values for this kind of import without time-consuming workarounds



</details>
"
307546769,20447,UnicodeDecodeError when import sas7bdat with special characters in Chinese variables,witwall,open,2018-03-22T08:29:00Z,2020-04-05T01:45:37Z,"#### Code Sample, a copy-pastable example if possible

here is the sas7bdat files for test (Chinese names end with special characters),

[issue.zip](https://github.com/pandas-dev/pandas/files/1836642/issue.zip)

issue.sas7bdat has Chinese values, can be correctly imported by pandas.

![image](https://user-images.githubusercontent.com/3222056/37758460-40263ffc-2deb-11e8-92b4-3359afa21896.png)

issue1.sas7bdat has Chinese variables

![image](https://user-images.githubusercontent.com/3222056/37759053-6ea711e2-2ded-11e8-8057-44e58ab91931.png)

```python
# Your code here
df1=pd.read_sas('issue1.sas7bdat',encoding='GBK')
```

#### Problem description
```
---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
<ipython-input-2-bcd7a0b4819c> in <module>()
----> 1 df1=pd.read_sas('issue1.sas7bdat',encoding='GBK')

~/.pyenv/versions/3.6.4/envs/ts/lib/python3.6/site-packages/pandas/io/sas/sasreader.py in read_sas(filepath_or_buffer, format, index, encoding, chunksize, iterator)
     59         reader = SAS7BDATReader(filepath_or_buffer, index=index,
     60                                 encoding=encoding,
---> 61                                 chunksize=chunksize)
     62     else:
     63         raise ValueError('unknown SAS format')

~/.pyenv/versions/3.6.4/envs/ts/lib/python3.6/site-packages/pandas/io/sas/sas7bdat.py in __init__(self, path_or_buf, index, convert_dates, blank_missing, chunksize, encoding, convert_text, convert_header_text)
     96 
     97         self._get_properties()
---> 98         self._parse_metadata()
     99 
    100     def close(self):

~/.pyenv/versions/3.6.4/envs/ts/lib/python3.6/site-packages/pandas/io/sas/sas7bdat.py in _parse_metadata(self)
    276                 raise ValueError(
    277                     ""Failed to read a meta data page from the SAS file."")
--> 278             done = self._process_page_meta()
    279 
    280     def _process_page_meta(self):

~/.pyenv/versions/3.6.4/envs/ts/lib/python3.6/site-packages/pandas/io/sas/sas7bdat.py in _process_page_meta(self)
    282         pt = [const.page_meta_type, const.page_amd_type] + const.page_mix_types
    283         if self._current_page_type in pt:
--> 284             self._process_page_metadata()
    285         return ((self._current_page_type in [256] + const.page_mix_types) or
    286                 (self._current_page_data_subheader_pointers is not None))

~/.pyenv/versions/3.6.4/envs/ts/lib/python3.6/site-packages/pandas/io/sas/sas7bdat.py in _process_page_metadata(self)
    312                 self._get_subheader_index(subheader_signature,
    313                                           pointer.compression, pointer.ptype))
--> 314             self._process_subheader(subheader_index, pointer)
    315 
    316     def _get_subheader_index(self, signature, compression, ptype):

~/.pyenv/versions/3.6.4/envs/ts/lib/python3.6/site-packages/pandas/io/sas/sas7bdat.py in _process_subheader(self, subheader_index, pointer)
    382             raise ValueError(""unknown subheader index"")
    383 
--> 384         processor(offset, length)
    385 
    386     def _process_rowsize_subheader(self, offset, length):

~/.pyenv/versions/3.6.4/envs/ts/lib/python3.6/site-packages/pandas/io/sas/sas7bdat.py in _process_columntext_subheader(self, offset, length)
    432 
    433         if self.convert_header_text:
--> 434             cname = cname.decode(self.encoding or self.default_encoding)#cname.decode(self.encoding or self.default_encoding,'ignore')
    435         self.column_names_strings.append(cname)
    436 

UnicodeDecodeError: 'gbk' codec can't decode byte 0x8c in position 0: illegal multibyte sequence
```
[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None

pandas: 0.22.0
pytest: None
pip: 9.0.2
setuptools: 28.8.0
Cython: None
numpy: 1.14.0
scipy: None
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2017.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.9999999
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>




the  code fired issue,
```
        if self.convert_header_text:
            cname = cname.decode(self.encoding or self.default_encoding)
```
if change to

```
        if self.convert_header_text:
            cname = cname.decode(self.encoding or self.default_encoding,'ignore')
```
got 
![image](https://user-images.githubusercontent.com/3222056/37758430-2b022410-2deb-11e8-931e-fc818aa51898.png)


if change to 
```
       # if self.convert_header_text:
       #     cname = cname.decode(self.encoding or self.default_encoding)
```
got 
![image](https://user-images.githubusercontent.com/3222056/37758848-a7428ea6-2dec-11e8-80ff-7b77760aa3a7.png)

and decode columns with the following code,
```
col=df1.columns.tolist()
col = [x.decode('GBK', 'ignore') for x in col]
df1.columns=pd.Index(col)
```
got the correct one,
![image](https://user-images.githubusercontent.com/3222056/37758868-be2d973c-2dec-11e8-8b4c-36269dee5a38.png)


btw, 
[sas7bdat](https://pypi.python.org/pypi/sas7bdat) works well.
![image](https://user-images.githubusercontent.com/3222056/37759396-7404db82-2dee-11e8-8551-6a6a016871d7.png)
"
499620440,28660,Stream compatibility on newest Python IO ,Deamoner,closed,2019-09-27T19:48:17Z,2020-04-05T01:47:16Z,"Python version 3.7.3 or 3.7.4

#### Code Sample, a copy-pastable example if possible

```    decoded = base64.b64decode(content_string)
    try :
        if 'dm.xpt' in filename :
            # df_dict['dm'] = pd.read_sas(filename,encoding=""latin1"")

            df_dict['dm'] = pd.read_sas(io.BytesIO(decoded),format='xport',encoding='latin1')
```
#### Problem description

'bytes' do not have attribute 'encode""

**How to fix: 
 commenting out L279-L282 in Python37\lib\site-packages\pandas\io\sas\sasreader.py**

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
199048766,15069,qcut can fail for highly discontinuous data distributions,wesm,open,2017-01-05T20:16:07Z,2020-04-05T02:01:33Z,"#### Code Sample, a copy-pastable example if possible

This code fails for any `K`:

```python
# Your code here
K = 100

pd.qcut([0] * K + [1] * (K + 1), 2)
```
#### Problem description

With pandas 0.19.2, I have:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-8-782385490865> in <module>()
----> 1 pd.qcut([0] * K + [1] * (K + 1), 2)

pandas/tools/tile.py in qcut(x, q, labels, retbins, precision)
    173     bins = algos.quantile(x, quantiles)
    174     return _bins_to_cuts(x, bins, labels=labels, retbins=retbins,
--> 175                          precision=precision, include_lowest=True)
    176 
    177 

pandas/tools/tile.py in _bins_to_cuts(x, bins, right, labels, retbins, precision, name, include_lowest)
    192 
    193     if len(algos.unique(bins)) < len(bins):
--> 194         raise ValueError('Bin edges must be unique: %s' % repr(bins))
    195 
    196     if include_lowest:

ValueError: Bin edges must be unique: array([0, 1, 1])
```
#### Expected Output

We need some kind of option to decide how to assign values to a quantile bucket in the event that two quantiles have the same value prior to the `searchsorted` call. In this case, the appropriate behavior may be to assign all `1` values to the 50% quantile bucket. "
593561596,33267,TST: add DataFrame test for construct from tuple case from GH-32776,BenjaminLiuPenrose,closed,2020-04-03T18:25:16Z,2020-04-05T02:22:47Z,"- [x] closes #32776 
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
593032795,33251,Pandas using matplot displays an incorrect year on the x-axis date,JunioCalu,closed,2020-04-03T02:28:30Z,2020-04-05T02:59:18Z,"#### Code

```python
# -*- coding: UTF-8 -*-

import pandas as pd
import datetime
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

hoje = datetime.datetime.now()
data = hoje.strftime(""%Y%m%d"")
dados_url = 'https://covid.saude.gov.br/assets/files/COVID19_'+data+'.csv'

try:
    print(dados_url)
    dados = pd.read_csv(dados_url, sep=';', parse_dates=['data'],  dayfirst=True)
except Exception as e:
    print(""Os dados para a data corrente ainda não foram atualizados, buscando dados do dia anterior..."")
    print(e)
    DD = datetime.timedelta(days=1)
    hoje = hoje - DD
    data = hoje.strftime(""%Y%m%d"")
    print(str(hoje))
    dados_url = 'https://covid.saude.gov.br/assets/files/COVID19_'+data+'.csv'
    print(str(dados_url))
    dados = pd.read_csv(dados_url, sep=';', parse_dates=['data'],  dayfirst=True)

#pd.set_option('display.max_rows', None)

dados_alagoas = dados[dados['estado'] == 'AL']
dados_alagoas.set_index('data',inplace=True)
#print(dados_alagoas)

plt.style.use('ggplot')

fig, ax = plt.subplots(figsize=(30,15))
dados_alagoas.plot(ax=ax, rot=75)

#grafico em barras
#dados_alagoas.plot(ax=ax, rot=75, kind='bar', width=2.0)

ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
ax.xaxis.set_major_formatter(mdates.DateFormatter('%d/%m/%y'))
ax.set_title('Incidência do COVID-19 em Alagoas em '+hoje.strftime('%d/%m/%Y'))
ax.set_ylabel('Ocorrências')
ax.set_xlabel('Data')
plt.show()

```
#### Problem description
In the graph, the date axis is not being displayed correctly with date formatting, the year that should be ""2020"" and is displayed as ""51""

#### Output of ``pd.show_versions()``
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.28-1-MANJARO
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : pt_BR.UTF-8
LOCALE           : pt_BR.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3
Cython           : 0.29.16
pytest           : 5.4.1
hypothesis       : None
sphinx           : 2.4.4
blosc            : 1.8.3
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : 0.9.3
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: 0.8.1
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.8
numba            : None
None





![Figure_1](https://user-images.githubusercontent.com/8952225/78317749-81d49600-7539-11ea-9de4-c2c62107c2f0.png)



"
558420201,31523,RLS: 1.0.1,TomAugspurger,closed,2020-01-31T23:04:58Z,2020-04-05T10:45:23Z,"A decent number of regressions have been reported on 1.0.0. I think we should target 1.0.1 for sometime early next week. Perhaps on Tuesday, depending on how much progress we make on the regressions.

cc @pandas-dev/pandas-core.

make sure these are [backported](
https://github.com/pandas-dev/pandas/pulls?q=is%3Apr+label%3A%22Still+Needs+Manual+Backport%22)"
594241950,33298,BUG: Replace methods fills value from previous row when replacing with None,DataSolveProblems,closed,2020-04-05T04:04:17Z,2020-04-05T12:44:34Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import numpy as np
import pandas as pd

df = pd.DataFrame({'x': [10, 20, np.nan], 'y': [30, 40, 50]})
print(df.replace(np.NaN, None))

#       x   y
# 0  10.0  30
# 1  20.0  40
# 2  20.0  50
```

#### Problem description

When replacing NaN with None using replace method, value, NaN value is replaced with value from previous row instead of None. 

#### Expected Output
```
#       x   y
# 0  10.0  30
# 1  20.0  40
# 2  None  50
```

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 13, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : None.None
pandas           : 1.0.0rc0
numpy            : 1.17.2
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : 0.8.6
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```
</details>
"
260396559,17673,DOC: 'replace' docstring lacking / too complex,jorisvandenbossche,closed,2017-09-25T19:58:45Z,2020-04-05T13:53:22Z,"I think the `replace` docstring is lacking in many ways (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html):

- The explanation of `to_replace` keyword is both way too complex and lacking an explanation of the simple cases:
  - the most simple case of a scalar value is not clearly mentioned (like `df.replace(to_replace=0, replace=1)`, it is mentioned in the 'str' explanation, but it is not specific to strings)
  - the simplest dict case of `df.replace({to_replace: replacement})` is not mentioned (the dict explanation starts with explanation of nested dicts)
  - I would personally rewrite this whole explanation of this keyword, start with basic cases, and only after that (or in the notes) explain the complex cases.
- There is a reference to the examples section for ""examples of each of those"", but there is no examples section. We should add one.
- In the 'see also' section it references `reindex`, `asfreq` and `fillna`. `fillna` is fine, but I fail to see the link with the first two. I would rather add a reference to `where` to replace values based on a boolean condition (and the 'see also' should not just refer to the other methods, but also include a sentence on why / the difference)
- The docstring also uses `NDFrame`, and this should never be in a public docstring (failing substituion of docstring in `generic`)
- I would personally also write separate docstrings for the series and dataframe case. This will give some duplication, but I think this gives room to simplify the docstring (or certainly for the simpler Series.replace case). (xref https://github.com/pandas-dev/pandas/issues/13852)

See the tutorial docs (https://pandas.pydata.org/pandas-docs/stable/missing_data.html#replacing-generic-values) with some actual examples.

Underlying reason is that this function of course can do way too many things at the same time (or the same things in too many different ways) ... (orthogonal to this, we could maybe also think if certain functionality could be moved into its own function).



"
585861028,32916,TST: move to indices fixture instead of create_index,jbrockmendel,closed,2020-03-23T01:04:52Z,2020-04-05T17:33:07Z,
560625002,31721,CLN: inconsistent kwarg name,jbrockmendel,closed,2020-02-05T20:58:40Z,2020-04-05T17:34:04Z,"A couple of days ago the ""setting"" kwarg got introduced in core.indexing, but I should have called it ""is_setter"" to match the existing pattern in that module.  This fixes that."
558374173,31519,REF: simplify DTI._parse_string_to_bounds,jbrockmendel,closed,2020-01-31T21:09:53Z,2020-04-05T17:34:10Z,"Sits on top of #31475.  i.e. once that is merged, the only diff remaining here will be in indexes.datetimes.

After this, we are within striking distance of sharing the method between DTI/PI."
547673135,30860,REF: gradually move ExtensionIndex delegation to use inherit_names,jbrockmendel,closed,2020-01-09T19:25:38Z,2020-04-05T17:34:17Z,
541401914,30396,REF: use fused types for part of hashtables code,jbrockmendel,closed,2019-12-22T03:21:51Z,2020-04-05T17:34:46Z,"This implements fused-type functions for the hashtable code that currently uses tempita.  Same idea as #29244, but this breaks off a part that I can confirm is performance-neutral."
540772032,30374,BUG: passing non-printable unicode to datetime parsing functions,jbrockmendel,closed,2019-12-20T05:33:14Z,2020-04-05T17:34:53Z,"ATM these segfault (on OSX at least).  There are more of these that I'll get in upcoming passes (I think no more than 2 more will be needed), keeping this one small to make the issue obvious and improve some of the typing in functions that will be checked in the next passes.

Needs double-checking: is the `val.isprintable()` check equivalent to ""`PyUnicode_AsUTF8AndSize(val, length)` wont segfault""?  or is that just an approximation?"
531616531,29982,"CLN: assorted pytables cleanups, remove unwanted assertion",jbrockmendel,closed,2019-12-02T23:55:35Z,2020-04-05T17:35:01Z,"#29977 included an assertion that I meant to remove before merging.  This removes that, followed by some misc cleanup: annotations, docstrings, remove is_shape_reversed (which was for Panel compat)"
532225998,30009,CI: troubleshoot openpyxl failures,jbrockmendel,closed,2019-12-03T19:53:29Z,2020-04-05T17:35:09Z,"tested on OSX 36 and 37, we'll see if that generalizes"
483147995,28048,BUG: retain extension dtypes in transpose,jbrockmendel,closed,2019-08-21T00:58:41Z,2020-04-05T17:35:17Z,"I'll have to look through the issues to see what this closes.

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
488877832,28275,BUG: passing DataFrame to make_block silently raises,jbrockmendel,closed,2019-09-04T00:09:06Z,2020-04-05T17:35:48Z,"ATM in the `except NotImplementedError:` branch of `_cython_agg_blocks`, we take a difference approach to the operation.  if that approach succeeds, we end up passing a DataFrame to `make_block` on L189, which will raise `ValueError`.  As a result, we'll end up falling back to python-space for the entire operation, which presumably entails a performance hit.

This fixes the incorrect passing of DataFrame, but the fix is kind of kludgy.  Suggestions welcome on how to improve it."
483508799,28061,PERF: cython-optimized datetime constructor,jbrockmendel,closed,2019-08-21T15:57:00Z,2020-04-05T17:35:53Z,"Calling the datetime constructor goes through python space, so the C code generated by `dt = datetime(obj.dts.year, obj.dts.month, obj.dts.day, obj.dts.hour, obj.dts.min, obj.dts.sec, obj.dts.us, obj.tzinfo)` in master is:

```
  __pyx_t_1 = __Pyx_PyInt_From_npy_int64(__pyx_v_obj->dts.year); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 435, __pyx_L1_error)
  __Pyx_GOTREF(__pyx_t_1);
  __pyx_t_3 = __Pyx_PyInt_From_npy_int32(__pyx_v_obj->dts.month); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 435, __pyx_L1_error)
  __Pyx_GOTREF(__pyx_t_3);
  __pyx_t_7 = __Pyx_PyInt_From_npy_int32(__pyx_v_obj->dts.day); if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 435, __pyx_L1_error)
  __Pyx_GOTREF(__pyx_t_7);
  __pyx_t_4 = __Pyx_PyInt_From_npy_int32(__pyx_v_obj->dts.hour); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 436, __pyx_L1_error)
  __Pyx_GOTREF(__pyx_t_4);
  __pyx_t_2 = __Pyx_PyInt_From_npy_int32(__pyx_v_obj->dts.min); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 436, __pyx_L1_error)
  __Pyx_GOTREF(__pyx_t_2);
  __pyx_t_5 = __Pyx_PyInt_From_npy_int32(__pyx_v_obj->dts.sec); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 436, __pyx_L1_error)
  __Pyx_GOTREF(__pyx_t_5);
  __pyx_t_11 = __Pyx_PyInt_From_npy_int32(__pyx_v_obj->dts.us); if (unlikely(!__pyx_t_11)) __PYX_ERR(0, 437, __pyx_L1_error)
  __Pyx_GOTREF(__pyx_t_11);
  __pyx_t_12 = PyTuple_New(8); if (unlikely(!__pyx_t_12)) __PYX_ERR(0, 435, __pyx_L1_error)
  __Pyx_GOTREF(__pyx_t_12);
  __Pyx_GIVEREF(__pyx_t_1);
  PyTuple_SET_ITEM(__pyx_t_12, 0, __pyx_t_1);
  __Pyx_GIVEREF(__pyx_t_3);
  PyTuple_SET_ITEM(__pyx_t_12, 1, __pyx_t_3);
  __Pyx_GIVEREF(__pyx_t_7);
  PyTuple_SET_ITEM(__pyx_t_12, 2, __pyx_t_7);
  __Pyx_GIVEREF(__pyx_t_4);
  PyTuple_SET_ITEM(__pyx_t_12, 3, __pyx_t_4);
  __Pyx_GIVEREF(__pyx_t_2);
  PyTuple_SET_ITEM(__pyx_t_12, 4, __pyx_t_2);
  __Pyx_GIVEREF(__pyx_t_5);
  PyTuple_SET_ITEM(__pyx_t_12, 5, __pyx_t_5);
  __Pyx_GIVEREF(__pyx_t_11);
  PyTuple_SET_ITEM(__pyx_t_12, 6, __pyx_t_11);
  __Pyx_INCREF(__pyx_v_obj->tzinfo);
  __Pyx_GIVEREF(__pyx_v_obj->tzinfo);
  PyTuple_SET_ITEM(__pyx_t_12, 7, __pyx_v_obj->tzinfo);
  __pyx_t_1 = 0;
  __pyx_t_3 = 0;
  __pyx_t_7 = 0;
  __pyx_t_4 = 0;
  __pyx_t_2 = 0;
  __pyx_t_5 = 0;
  __pyx_t_11 = 0;
  __pyx_t_11 = __Pyx_PyObject_Call(((PyObject *)__pyx_ptype_7cpython_8datetime_datetime), __pyx_t_12, NULL); if (unlikely(!__pyx_t_11)) __PYX_ERR(0, 435, __pyx_L1_error)
  __Pyx_GOTREF(__pyx_t_11);
  __Pyx_DECREF(__pyx_t_12); __pyx_t_12 = 0;
  __pyx_v_dt = ((PyDateTime_DateTime *)__pyx_t_11);
  __pyx_t_11 = 0;
```


Using `datetime_new` we instead get:
```
  __pyx_t_1 = __pyx_v_obj->tzinfo;
  __Pyx_INCREF(__pyx_t_1);
  __pyx_t_3 = __pyx_f_7cpython_8datetime_datetime_new(__pyx_v_obj->dts.year, __pyx_v_obj->dts.month, __pyx_v_obj->dts.day, __pyx_v_obj->dts.hour, __pyx_v_obj->dts.min, __pyx_v_obj->dts.sec, __pyx_v_obj->dts.us, __pyx_t_1); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 437, __pyx_L1_error)
  __Pyx_GOTREF(__pyx_t_3);
  __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
  if (!(likely(((__pyx_t_3) == Py_None) || likely(__Pyx_TypeTest(__pyx_t_3, __pyx_ptype_7cpython_8datetime_datetime))))) __PYX_ERR(0, 437, __pyx_L1_error)
  __pyx_v_dt = ((PyDateTime_DateTime *)__pyx_t_3);
  __pyx_t_3 = 0;
```

Still need to run asv to see if this actually matters."
470131999,27466,CLN: remove unused row_bool_subset,jbrockmendel,closed,2019-07-19T04:29:29Z,2020-04-05T17:36:18Z,"@jreback can you confirm my hunch that the `result.ndim == 2` block in groupby.ops was for Panel?

Tracing back the blame, row_bool_subset was introduced [here](https://github.com/pandas-dev/pandas/commit/b15ae85e1f647821755d35bedd6143ad1e9c3678) in 2012 to close #152 and it started being called only for result.ndim == 2 [here](https://github.com/pandas-dev/pandas/commit/9bfdc3c037e926143e708463d76fcdae3fb709f2)"
462919927,27170,CLN: remove fastparquet compat shim,jbrockmendel,closed,2019-07-01T21:29:40Z,2020-04-05T17:36:27Z,Introduced in #19434 Jan 2018.
462446096,27153,EA: preliminary EA reshape ops,jbrockmendel,closed,2019-06-30T20:59:32Z,2020-04-05T17:36:33Z,Ones that we can put in place before allowing anything 2D.  Asks authors to provide a `view` method akin to a shallow copy.
462320310,27130,BUG: Fix Series divmod #26987,jbrockmendel,closed,2019-06-29T16:58:34Z,2020-04-05T17:36:39Z,"- [x] closes #26987 
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
586359113,32937,Fixing scatter plot size (#32904),SultanOrazbayev,closed,2020-03-23T16:55:53Z,2020-03-28T22:04:48Z,"This fixes the marker size in scatter plots (see https://github.com/pandas-dev/pandas/issues/32904).

- [x] closes #32904
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
589416653,33079,DOC: whatsnew for #32831,jbrockmendel,closed,2020-03-27T21:15:10Z,2020-03-29T01:09:31Z,
586547973,32956,CLN: update Appender to doc with case __doc__,HH-MWB,closed,2020-03-23T22:07:14Z,2020-03-29T04:15:25Z,"- [x] working on #31942
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
584674136,32836,ERR: Better error message for missing columns in aggregate,JDTruj2018,closed,2020-03-19T20:18:08Z,2020-03-29T05:06:01Z,"More descriptive SpecificationError message that reports to user non-existing columns causing error

- [x] closes #32755
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
587634053,33009,STY: Boolean values for bint variables,ShaharNaveh,closed,2020-03-25T11:37:32Z,2020-03-29T09:22:41Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
587043470,32980,DOC: Fix examples in reshape,ShaharNaveh,closed,2020-03-24T15:25:40Z,2020-03-29T09:23:40Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
586342518,32935,DOC: Fixed examples in pandas/tseries,ShaharNaveh,closed,2020-03-23T16:31:47Z,2020-03-29T09:24:16Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
589400275,33078,"Revert ""CI: Fix jedi upgrades causes deprecation warning""",ShaharNaveh,closed,2020-03-27T20:37:41Z,2020-03-29T09:26:27Z,"Reverts pandas-dev/pandas#31323

---

Closes #31407"
10927933,2858,"Pandas .fillna() should handle ""inf""",darindillon,closed,2013-02-12T21:39:19Z,2020-03-29T10:18:39Z,"Division by 0 in pandas will give the value ""inf"". But the .fillna() method doesn't recognize that. We should make .fillna() handle ""inf"" the same way it handles ""NaN'. (for reference, the numpy.isfinite() method treats NaN and Inf interchangably -- pandas should do the same).

import pandas
vals = [ 1.1, 1.1, 1.1]
p =  pandas.DataFrame( { 'first' : vals }, columns=['first'])
p['first'] = p['first'] / 0 #Creates a bunch of ""inf"" values
p['first'].fillna(0, inplace=True) #Has no effect
"
525935507,29746,GroupBy Regression with Categorical On Master,WillAyd,closed,2019-11-20T16:56:13Z,2020-03-29T11:37:46Z,"Seems to be an issue on master as this works on 0.25.3:

```python
>>> ser = pd.Series(pd.Categorical([""first"", ""second"", ""third"", ""fourth""], ordered=True))
>>> ser.groupby([1, 1, 1, 1]).first()
[first]
Categories (4, object): [first < fourth < second < third]
```

But fails on master:

```python
>>> ser = pd.Series(pd.Categorical([""first"", ""second"", ""third"", ""fourth""], ordered=True))
>>> ser.groupby([1, 1, 1, 1]).first()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/williamayd/clones/pandas/pandas/core/groupby/groupby.py"", line 1368, in f
    return self._cython_agg_general(alias, alt=npfunc, **kwargs)
  File ""/Users/williamayd/clones/pandas/pandas/core/groupby/groupby.py"", line 880, in _cython_agg_general
    obj._values, how, min_count=min_count
  File ""/Users/williamayd/clones/pandas/pandas/core/groupby/ops.py"", line 572, in aggregate
    ""aggregate"", values, how, axis, min_count=min_count
  File ""/Users/williamayd/clones/pandas/pandas/core/groupby/ops.py"", line 456, in _cython_operation
    ""{dtype} dtype not supported"".format(dtype=values.dtype)
NotImplementedError: category dtype not supported
```

@jbrockmendel for visibility. Looking at this on my end"
589667148,33106,Fix mixup between mean and median,oefe,closed,2020-03-28T21:54:57Z,2020-03-29T15:07:57Z,"Text said ""median"", but code uses ""mean"". Make text match the code

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
589484344,33088,CLN: remove ABCGeneric,jbrockmendel,closed,2020-03-28T01:31:10Z,2020-03-29T15:09:44Z,
589461041,33085,"CLN: avoid internals in tz_convert, tz_localize",jbrockmendel,closed,2020-03-27T23:30:33Z,2020-03-29T15:11:55Z,
589688675,33110,REF: push concat logic out of internals and into concat_compat,jbrockmendel,closed,2020-03-29T00:38:07Z,2020-03-29T15:12:22Z,"This changes some empty-array behavior for concat_compat to match the behavior of pd.concat(list_of_series), see new test.

The follow-up to this basically gets concat out of internals altogether."
589460771,33084,CLN: de-kludge NDFrame.interpolate,jbrockmendel,closed,2020-03-27T23:29:36Z,2020-03-29T15:20:56Z,i think i was responsible for these particular kludges a while back.
589347158,33075,CLN: avoid using internals methods for DataFrame.drop_duplicates,jbrockmendel,closed,2020-03-27T19:02:21Z,2020-03-29T15:21:57Z,
589460263,33083,"CLN: avoid internals, some misc cleanups",jbrockmendel,closed,2020-03-27T23:27:48Z,2020-03-29T15:22:59Z,The only non-trivial thing here is in io.formats.info replacing `frame._data.get_dtype_counts` with an alternative that uses public methods
589627679,33103,Check error message for raised exception,sumanau7,closed,2020-03-28T18:01:58Z,2020-03-29T15:25:50Z,"- [x] xref #30999
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
589681372,33109,"REF: DataFrame delitem, take, pop, filter tests",jbrockmendel,closed,2020-03-28T23:38:56Z,2020-03-29T15:39:08Z,
583330252,32792,to_datetime() throws ValueError: Cannot pass a tz argument when parsing strings with timezone information.,empz,closed,2020-03-17T22:13:16Z,2020-03-29T15:54:50Z,"I think pandas should support passing `%z` in the format but also `utc=True`. In my opinion, one thing is the format, which tells pandas how to parse the datetime string. The other argument is just telling to return the dates in UTC, no matter which timezone they were in the beginning.

Here's a repl that shows the issue: https://repl.it/@eparizzi/Pandas-todatetime-in-UTC-with-format

If you replace that simple CSV with some big 50K row time-series CSV, the call to `to_datetime` without the format takes more than 20 seconds. On the contrary, passing the format and without `utc=True` takes less than 2 seconds. Unfortunately, this doesn't seem to work properly when there are multiple timezones in the column. It simply can't set a proper dtype in this case.

So, why can't we have a way to specify the format including timezone but also specify that we want everything in datetime64(UTC)?

I've already gone over this issue: https://github.com/pandas-dev/pandas/issues/25571 but I still think this deserves a discussion.


```python
import pandas as pd

# I know the format, I want to use it so that Pandas to_datetime() runs faster.
DATETIME_FORMAT = '%m/%d/%Y %H:%M:%S.%f%z'

try:
  data = ['10/11/2018 00:00:00.045-07:00',
  '10/11/2018 01:00:00.045-07:00',
  '10/11/2018 01:00:00.045-08:00',
  '10/11/2018 02:00:00.045-08:00',
  '10/11/2018 04:00:00.045-07:00',
  '10/11/2018 05:00:00.045-07:00']

  df = pd.DataFrame(data, columns=[""Timestamp""])

  # This raises ""ValueError: Cannot pass a tz argument when parsing strings with timezone information.""
  df.Timestamp = pd.to_datetime(df.Timestamp, format=DATETIME_FORMAT, utc=True)

except ValueError as valueError:
  # I don't know why a %z in the format is not compatible with utc=True. The %z is telling pandas that it needs to deal with timezones. Then, utc=True should just convert all to UTC. It shouldn't be more complicated than that I think.
  print(f""ERROR: {str(valueError)}"")
  print(""...why not?"")

  # This works, but it's A LOT slower when parsing a lot of rows.
  df.Timestamp = pd.to_datetime(df.Timestamp, infer_datetime_format=True, utc=True)

finally:
  print(df)
  print(df.Timestamp.dtype)

  # Expected output:
  # 
  #                          Timestamp
  # 0 2018-10-11 07:00:00.045000+00:00
  # 1 2018-10-11 08:00:00.045000+00:00
  # 2 2018-10-11 09:00:00.045000+00:00
  # 3 2018-10-11 10:00:00.045000+00:00
  # 4 2018-10-11 11:00:00.045000+00:00
  # 5 2018-10-11 12:00:00.045000+00:00
  # datetime64[ns, UTC]
```"
588127928,33032,PERF: remove large-array-creating path in fast_xs,jbrockmendel,closed,2020-03-26T03:27:56Z,2020-03-29T15:56:33Z,"When frame.columns is non-unique, `frame.iloc[n]` goes through an unnecessary path that effectively creates `frame.values` and looking up `[n]` on that.  That's a lot of casting to access just one row.

Luckily, that case is obsolete, so this rips it right out."
586317584,32932,"REF: ""bare_pytest_raises"" to use the ast module",ShaharNaveh,closed,2020-03-23T15:57:17Z,2020-03-29T16:34:50Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

There is also a performance boost:

```
In [1]: import os                                                                                             

In [2]: from scripts.validate_unwanted_patterns import bare_pytest_raises, main                               

In [3]: SOURCE_PATH = ""pandas/tests/""                                                                         

In [4]: %timeit main(function=bare_pytest_raises, source_path=SOURCE_PATH, output_format=""{source_path}:{line_number}:{msg}."") 

12.6 s ± 76.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) # Master
4.33 s ± 27 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) # PR
```"
586186972,32927,Added file paths to ignore in linter scripts/validate_unwanted_patterns.py,ShaharNaveh,closed,2020-03-23T12:53:59Z,2020-03-29T16:36:14Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

While running ```./ci/code_checks.sh lint``` on my local machine I got the output of:

```
Check for use of not concatenated strings
./asv_bench/env/40d026129f63c107b1ac48bf9ad6964c/lib/python3.6/_sitebuiltins.py:99:String unnecessarily split in two by black. Please merge them manually..
./asv_bench/env/40d026129f63c107b1ac48bf9ad6964c/lib/python3.6/inspect.py:2768:String unnecessarily split in two by black. Please merge them manually..
./asv_bench/env/40d026129f63c107b1ac48bf9ad6964c/lib/python3.6/inspect.py:2890:String unnecessarily split in two by black. Please merge them manually..

....
```

This fixes it."
588864156,33057,ERR: Raise NotImplementedError with BaseIndexer and certain rolling operations,mroeschke,closed,2020-03-27T03:39:00Z,2020-03-29T16:42:09Z,"- [x] xref #32865
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

As a temporary solution for returning incorrect results when using a BaseIndexer subclass with certain rolling operations, a `NotImplementedError` will now raise with instructions to use `apply` instead."
580266142,32669,PERF: MultiIndex._shallow_copy,topper-123,closed,2020-03-12T22:48:47Z,2020-03-29T17:48:22Z,"- [x] xref #28584, #32568, #32640
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Improves performance of ``MultiIndex._shallow_copy``. Example:

```python
>>> n = 100_000
>>> df = pd.DataFrame({'a': range(n), 'b': range(1, n+1)})
>>> mi = pd.MultiIndex.from_frame(df)
>>> mi.is_lexsorted()
True
>>> mi.get_loc(mi[0])  # also sets up the cache
>>> %timeit mi._shallow_copy().get_loc(mi[0])
8.56 ms ± 127 µs per loop  # master
75.1 µs ± 2.3 µs per loop  # this PR, first commit
46.9 µs ± 792 ns per loop  # this PR, second commit

```

Also adds tests for ``_shallow_copy`` for all index types. This ensures that this issue has been resolved for all index types."
589824102,33122,CLN: Remove unused cdef variables,ShaharNaveh,closed,2020-03-29T16:01:01Z,2020-03-29T18:06:32Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
589669911,33108,Docstring for show_versions in master,MarianD,closed,2020-03-28T22:12:56Z,2020-03-29T18:12:58Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
589798528,33119,"Fix ambiguous reference to ""previous"" section",oefe,closed,2020-03-29T13:59:18Z,2020-03-29T18:19:53Z,"This section originally came just after ""10 minutes to pandas"",
but now it's after the ""Getting started tutorials"".
Also, the examples have diverged a bit, so let's not pretend
that we are creating the same objects.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
589776490,33116,Fix grammar,oefe,closed,2020-03-29T12:03:13Z,2020-03-29T18:20:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
367651782,23037,BUG: Concatenating DataFrames with NaT TZ Timestamps results in incorrect dtype,tonytao2012,closed,2018-10-08T06:23:15Z,2020-03-29T19:09:58Z,"#### Code Sample, a copy-pastable example if possible

```python

ts1 = pd.Timestamp(pd.NaT, tz='UTC')
ts2 = pd.Timestamp('2015-01-01', tz='UTC')

df1 = pd.DataFrame([[ts1]])
df2 = pd.DataFrame([[ts2]])

result = pd.concat([df1, df2])

result[0]
Out[6]: 
0                          NaT
0    2015-01-01 00:00:00+00:00
Name: 0, dtype: object

expected = pd.DataFrame([[ts1], [ts2]])

expected[0]
Out[8]: 
0                         NaT
1   2015-01-01 00:00:00+00:00
Name: 0, dtype: datetime64[ns, UTC]
```
#### Problem description

Concatenating a DataFrame containing NaT with a tz specified with another DataFrame containing an actual date with tz specified results in an object dtype instead of the expected datetime64[ns, UTC] dtype.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.6.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 158 Stepping 9, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en
LOCALE: None.None

pandas: 0.24.0.dev0+708.gce1f81f8b
pytest: 3.8.2
pip: 10.0.1
setuptools: 40.2.0
Cython: 0.28.5
numpy: 1.15.2
scipy: 1.1.0
pyarrow: None
xarray: 0.10.9
IPython: 6.5.0
sphinx: 1.8.1
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.8
feather: None
matplotlib: 3.0.0
openpyxl: 2.5.5
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.1.1
lxml: 4.2.5
bs4: 4.6.3
html5lib: 1.0.1
sqlalchemy: 1.2.12
pymysql: 0.9.2
psycopg2: None
jinja2: 2.10
s3fs: 0.1.6
fastparquet: 0.1.6
pandas_gbq: None
pandas_datareader: None
gcsfs: 0.1.2

</details>"
586604261,32963,TST: Use indices fixture in tests/indexes/test_base.py,SaturnFromTitan,closed,2020-03-24T00:30:05Z,2020-03-29T19:51:58Z,"part of #30914
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
482866149,28034,REGR: Index.union should pick correct dtype for combinations of tstamps with different tzones,zogzog,closed,2019-08-20T13:36:54Z,2020-03-30T02:52:56Z,"Here's a pdb session showing the issue:

```python
ipdb> basei
DatetimeIndex(['2017-10-28 23:00:00+00:00', '2017-10-29 00:00:00+00:00',
               '2017-10-29 01:00:00+00:00', '2017-10-29 02:00:00+00:00'],
              dtype='datetime64[ns, UTC]', freq=None)
ipdb> diffi
DatetimeIndex(['2017-10-29 02:00:00+01:00', '2017-10-29 03:00:00+01:00',
               '2017-10-29 04:00:00+01:00', '2017-10-29 05:00:00+01:00'],
              dtype='datetime64[ns, Europe/Paris]', freq=None)
ipdb> basei.union(diffi)
Index([2017-10-28 23:00:00+00:00, 2017-10-29 00:00:00+00:00,
       2017-10-29 01:00:00+00:00, 2017-10-29 02:00:00+00:00,
       2017-10-29 04:00:00+01:00, 2017-10-29 05:00:00+01:00],
      dtype='object')

```
With pandas 0.24 I got back an index with a `dtype='datetime64[ns, UTC]'`

This looks a bit like a cousin of https://github.com/pandas-dev/pandas/issues/26778
"
502297955,28778,DISCUSS: boolean dtype with missing value support,jorisvandenbossche,closed,2019-10-03T21:05:39Z,2020-03-30T06:56:31Z,"Part of the discussion on missing value handling in https://github.com/pandas-dev/pandas/issues/28095, detailed proposal at https://hackmd.io/@jorisvandenbossche/Sk0wMeAmB.

*if* we go for a new NA value, we also need to decide the behaviour of this value in comparison operations. And consequently, we also need to decide on the behaviour of boolean values with missing data in logical operations and indexing operations.  
So let's use this issue for that part of the discussion.

Some aspects of this:

- Behaviour in **comparison operations**: currently np.nan compares unequal (`value == np.nan -> False`, `values > np.nan -> False`, but we can also propagate missing values (`value == NA -> NA`, ...)
-  Behaviour in **logical operations**: currently we always return False for `|` or `&` with missing data. But we could also use a ""three-valued logic"" like [Julia](https://docs.julialang.org/en/v1/manual/missing/index.html#Logical-operators-1) and SQL (this has, eg, `NA | True = True` or `NA & True = NA`).
- Behaviour in **indexing**: currently you cannot do boolean indexing with a boolean series with missing values (which is object dtype right now). Do we want to change this? For example, interpret it as False (not select it) 
  (TODO: should check how other languages do this)

Julia has a nice documentation page explain how they support [missing values](https://docs.julialang.org/en/v1/manual/missing/index.html), the above ideas largely match with that.

Besides those behavioural API discussions, we also need to decide on how to approach this technically (boolean ExtensionArray with boolean numpy array + mask for missing values?) Shall we discuss that here as well, or keep that separate?

cc @pandas-dev/pandas-core "
549172965,30982,PERF: masked ops for reductions (sum),jorisvandenbossche,closed,2020-01-13T20:48:20Z,2020-03-30T07:07:44Z,"The current `nanops` has quite some complexity that is not needed for the masked arrays. This is a small proof of concept to have separate implementations for our nullable masked arrays, taking the sum case (and still ignoring the additional kwargs).

This is also quite a bit faster. On master:

```
In [1]: a = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, np.nan]*1000)  

In [2]: s1 = pd.Series(a, dtype=""Int64"")

In [3]: s2 = pd.Series(a, dtype=""float64"") 

In [4]: %timeit s1.sum()   
79.1 µs ± 2.39 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)

In [5]: %timeit s2.sum() 
79.1 µs ± 1.48 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```

the nullable Int64 basically does the same as the nanops implementation for float. 
With this PR:

```
In [4]: %timeit s1.sum()  
21.5 µs ± 69.9 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)

In [5]: %timeit s2.sum() 
79.8 µs ± 1.03 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```

(when using bigger arrays, the speed-up becomes less big in relative factor. Here it's almost 4x with 10k elements, but with 1M it's aound 2x).

I think it would be interesting to gradually implement some of the ops specifically for the nullable masked arrays. Personally I think it will be easier to do this in new functions than trying to fit it into the existing nanops function.

"
589866730,33127,DataFrame / Series reductions return NaN rather than NA when below min_count ,dsaxton,closed,2020-03-29T19:38:47Z,2020-03-30T12:33:24Z,"1.0.3 or master:
```python
import numpy as np
import pandas as pd

df = pd.DataFrame({""a"": [1.0, np.nan]})

print(df.sum(min_count=2))
# a   NaN
# dtype: float64
print(df[""a""].sum(min_count=2))
# nan
```
but the documentation says we should get `NA` when we're below `min_count`. This kind of gets to the question of whether this is a ""missing value"" or an ""undefined operation,"" if this actually matters, as well as the actual meaning of `NA`.

In the new masked reductions under `/core/array_algos/masked_reductions.py` we're (correctly?) outputting `NA`.

cc @jorisvandenbossche "
146667371,12823,Time Zone problem with tz_localize,LorenzoBottaccioli,closed,2016-04-07T15:54:27Z,2020-03-30T14:12:14Z,"Hi All,

I'm having a issue with tz_localize, maybe is my misunderstanding.

I'm importing a dataframe and I want to se the timezone for central Europe:

So if I use **Eurpe/Rome** I get:

```
In [303]: df.index.tz_localize('Europe/Rome')
Out[303]: 
DatetimeIndex(['2012-12-15 00:00:00+01:00', '2012-12-15 00:15:00+01:00',
               '2012-12-15 00:30:00+01:00', '2012-12-15 00:45:00+01:00',
               '2012-12-15 01:00:00+01:00', '2012-12-15 01:15:00+01:00',
               '2012-12-15 01:30:00+01:00', '2012-12-15 01:45:00+01:00',
               '2012-12-15 02:00:00+01:00', '2012-12-15 02:15:00+01:00',
               ...
               '2015-10-20 21:30:00+02:00', '2015-10-20 21:45:00+02:00',
               '2015-10-20 22:00:00+02:00', '2015-10-20 22:15:00+02:00',
               '2015-10-20 22:30:00+02:00', '2015-10-20 22:45:00+02:00',
               '2015-10-20 23:00:00+02:00', '2015-10-20 23:15:00+02:00',
               '2015-10-20 23:30:00+02:00', '2015-10-20 23:45:00+02:00'],
              dtype='datetime64[ns, Europe/Rome]', name=u'Date Time', length=8064, freq=None)
```

But I dont want to use dst because my timeseries is with no dst so I tried to use **Etc/GMT+1**

```
In [305]: df.index.tz_localize('Etc/GMT+1')
Out[305]: 
DatetimeIndex(['2012-12-15 00:00:00-01:00', '2012-12-15 00:15:00-01:00',
               '2012-12-15 00:30:00-01:00', '2012-12-15 00:45:00-01:00',
               '2012-12-15 01:00:00-01:00', '2012-12-15 01:15:00-01:00',
               '2012-12-15 01:30:00-01:00', '2012-12-15 01:45:00-01:00',
               '2012-12-15 02:00:00-01:00', '2012-12-15 02:15:00-01:00',
               ...
               '2015-10-20 21:30:00-01:00', '2015-10-20 21:45:00-01:00',
               '2015-10-20 22:00:00-01:00', '2015-10-20 22:15:00-01:00',
               '2015-10-20 22:30:00-01:00', '2015-10-20 22:45:00-01:00',
               '2015-10-20 23:00:00-01:00', '2015-10-20 23:15:00-01:00',
               '2015-10-20 23:30:00-01:00', '2015-10-20 23:45:00-01:00'],
              dtype='datetime64[ns, Etc/GMT+1]', name=u'Date Time', length=8064, freq=None)
```

I don't get why now I have -1 in stead of +1.
If I use **Etc/GMT-1**  I get:

```
In [304]: df.index.tz_localize('Etc/GMT-1')
Out[304]: 
DatetimeIndex(['2012-12-15 00:00:00+01:00', '2012-12-15 00:15:00+01:00',
               '2012-12-15 00:30:00+01:00', '2012-12-15 00:45:00+01:00',
               '2012-12-15 01:00:00+01:00', '2012-12-15 01:15:00+01:00',
               '2012-12-15 01:30:00+01:00', '2012-12-15 01:45:00+01:00',
               '2012-12-15 02:00:00+01:00', '2012-12-15 02:15:00+01:00',
               ...
               '2015-10-20 21:30:00+01:00', '2015-10-20 21:45:00+01:00',
               '2015-10-20 22:00:00+01:00', '2015-10-20 22:15:00+01:00',
               '2015-10-20 22:30:00+01:00', '2015-10-20 22:45:00+01:00',
               '2015-10-20 23:00:00+01:00', '2015-10-20 23:15:00+01:00',
               '2015-10-20 23:30:00+01:00', '2015-10-20 23:45:00+01:00'],
              dtype='datetime64[ns, Etc/GMT-1]', name=u'Date Time', length=8064, freq=None)
```

Central Europe should be **Etc/GMT+1** is it rigth?

Tnx
"
589714254,33112,CLN: update Appender to doc decorator with case __doc__,HH-MWB,closed,2020-03-29T04:29:38Z,2020-03-30T15:39:07Z,"- [x] working for #31942
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
453998844,26761,to_sql Out of bounds nanosecond timestamp,gonghwa,closed,2019-06-10T05:03:03Z,2020-03-30T16:24:41Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
df.to_sql(TableName,index=False,if_exists=""replace"")
```
#### Problem description
I upgraded my pandas to 0.24.2 recently and encountered below error, which will not pop before.
When I am using python to transfer some data to sql server. Some date is set to 9999-12-31 and it will cause error: OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 9999-12-31 00:00:00




#### Expected Output
We should be able to transfer those date. Thanks

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.0.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.2
pytest: 3.8.0
pip: 19.0.2
setuptools: 40.2.0
Cython: 0.28.5
numpy: 1.16.2
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.5.0
sphinx: 1.7.9
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.8
feather: None
matplotlib: 2.2.3
openpyxl: 2.5.6
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.1.0
lxml.etree: 4.2.5
bs4: 4.6.3
html5lib: 1.0.1
sqlalchemy: 1.2.11
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
589947979,33140,BUG: to_sql no longer raises an AttributeError when saving an OBB date,mroeschke,closed,2020-03-30T02:24:35Z,2020-03-30T16:24:45Z,"- [x] closes #26761
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
587336225,32991,DOC: Fix capitalization among headings in documentation files (#32550),themien,closed,2020-03-24T23:16:27Z,2020-03-30T16:34:56Z,"Following #32978 and #32944.

Headers updated for the following files:
```
- [ ] doc/source/getting_started/comparison/comparison_with_stata.rst
- [ ] doc/source/getting_started/intro_tutorials/01_table_oriented.rst
- [ ] doc/source/getting_started/tutorials.rst
- [ ] doc/source/reference/arrays.rst
- [ ] doc/source/reference/window.rst
- [ ] doc/source/user_guide/boolean.rst
```"
589500397,33092,TYP: enforce tighter inputs on SingleBlockManager,jbrockmendel,closed,2020-03-28T03:34:35Z,2020-03-30T18:03:28Z,"there is currently only one place where we pass a list of indexes instead of a single index, so just fixed that and were good to go."
590358636,33142,"CLN: unused import  _libs/reduction + remove ""noqa"" comment in _libs/__init__.py",ShaharNaveh,closed,2020-03-30T14:56:56Z,2020-03-30T18:21:01Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
207065982,15379,Compat with jupyter's new HTML table style,jorisvandenbossche,closed,2017-02-12T17:48:42Z,2020-03-30T21:18:45Z,"See also discussion in https://github.com/jupyterlab/jupyterlab/issues/961

With the upcoming notebook 5.0 release and in current jupyterlab, jupyter has a new (much better!) default HTML table styling. The only problem is that it fixes the width of columns hardcoded at 150 px.

IMO this is a problem. Not the default as such, but the fact that you cannot easily change this when you *want* to see long table content. The result is that when having cells/column names with long content (and sometimes you deliberately create such cells/column names), it is *impossible* to inspect those data with the default repr.
Only solutions are to inspect specifics such as `df.values` or `df.columns`.

Another consequence is that the `pd.options.display.max_colwidth` option becomes useless. 

Possible way forward:

- override the `max-width` for our html repr's to set it to 'none' (@TomAugspurger shows here that this is possible: https://github.com/jupyterlab/jupyterlab/issues/961#issuecomment-252095223)
- if we like the more narrow default, we can always decrease the default for `pd.options.display.max_colwidth` (currently 50 chars) at the same time. The the default looks similar to jupyter's one, but you have still the ability to adjust it with our options.


"
589847565,33123,REF: remove placement kwarg from Block.concat_same_type,jbrockmendel,closed,2020-03-29T17:59:33Z,2020-03-30T21:53:20Z,On the path to getting concat logic out of internals
589853994,33124,REF: DataFrame.isna internals access,jbrockmendel,closed,2020-03-29T18:32:25Z,2020-03-30T21:54:00Z,
590480058,33148,More Json parametrize,WillAyd,closed,2020-03-30T17:56:19Z,2020-03-30T21:55:42Z,"follow on to #31191 - still a few more passes to go
"
589937140,33139,REF: de-curry unstack functions,jbrockmendel,closed,2020-03-30T01:39:12Z,2020-03-30T21:56:02Z,"I find this much easier to follow without the double-partial.  We also avoid re-doing some work, though AFAICT its pretty tiny perf-wise."
590511986,33151,REF: misplaced DataFrame arithmetic tests,jbrockmendel,closed,2020-03-30T18:48:42Z,2020-03-30T21:56:50Z,
590551623,33154,REF: PeriodIndex test_indexing tests,jbrockmendel,closed,2020-03-30T19:55:53Z,2020-03-30T21:57:59Z,This should be it for the PeriodIndex tests for now.
590519144,33152,REF: test_rename_axis,jbrockmendel,closed,2020-03-30T19:01:00Z,2020-03-30T21:58:22Z,
355437594,22541,BUG: index of group not returned correctly in groupby.apply,h-vetinari,closed,2018-08-30T06:08:53Z,2020-03-30T22:15:06Z,"Returning the index of a group is admittedly a somewhat unusual use of `apply` (since the information is available in `groups`), but it's clearly legal and shouldn't be wrong.

```
N = 10
df = pd.DataFrame(np.random.randint(0, int(N/3), (N,)) + 10, columns=['id'])
df
#    id
# 0  11
# 1  11
# 2  11
# 3  10
# 4  11
# 5  12
# 6  12
# 7  12
# 8  10
# 9  12
```

The issue is that the result of last group gets wrongly broadcast to all groups
```
df.groupby('id', as_index=True).apply(lambda gr: gr.index)
# id
# 10    Int64Index([5, 6, 7, 9], dtype='int64')
# 11    Int64Index([5, 6, 7, 9], dtype='int64')
# 12    Int64Index([5, 6, 7, 9], dtype='int64')
# dtype: object
```

Interestingly, with adding any operation I've tried, the behaviour is correct again:
```
df.groupby('id', as_index=True).apply(lambda gr: gr.index + 1 - 1)
# id
# 10          Int64Index([3, 8], dtype='int64')
# 11    Int64Index([0, 1, 2, 4], dtype='int64')
# 12    Int64Index([5, 6, 7, 9], dtype='int64')
# dtype: object
```

"
586506185,32950,REF: move mixed-dtype frame_apply check outside of _reduce try/except,jbrockmendel,closed,2020-03-23T20:43:15Z,2020-03-30T23:08:05Z,"cc @jorisvandenbossche this is the same branch I linked the other day.  Some variant of #32867 may make this irrelevant, but I don't think getting this improvement in the interim will make that any more difficult."
589604540,33102,PERF: fix performance regression in memory_usage(deep=True) for object dtype,neilkg,closed,2020-03-28T16:02:20Z,2020-03-31T00:25:23Z,"- [x] closes #33012  
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The pull request is to update lib.memory_usage_of_objects from taking self.arrays to self._values. An ASV included to benchmark with and without object-dtype columns.

Before:
![Screen Shot 2020-03-28 at 11 57 22 AM](https://user-images.githubusercontent.com/33635204/77827342-6efe3380-70eb-11ea-9f32-6fd835cca76f.png)

After:
![Screen Shot 2020-03-28 at 11 56 42 AM](https://user-images.githubusercontent.com/33635204/77827344-71608d80-70eb-11ea-9745-e8754f6aebf2.png)"
590407416,33145,CLN: Use doc decorator for case using __doc__ ,HH-MWB,closed,2020-03-30T16:02:19Z,2020-03-31T01:50:19Z,"- [ ] works for #31942
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
496925921,28575,[WIP] Annotate DataFrame (Part 3),vaibhavhrt,closed,2019-09-23T07:00:49Z,2020-03-31T02:02:47Z,"- [x] part of #26792 
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
587561445,33003,Extra factorization for inner join(documentation issue),charles-typ,closed,2020-03-25T09:32:32Z,2020-03-31T03:04:18Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here

```
#### Problem description

Hi, 

I'm trying to understand the pandas source code for an **inner join** using pandas.merge. I've got a sense that the procedure is basically factorizing both the left keys and the right keys to values like [0,1,2,3...], and then join the table together based on the factorization result.

The point that I'm confused about is that the keys from the two tables are first factorized at this point https://github.com/pandas-dev/pandas/blob/28e0f18a30451601e8ac37b3ed75777bf1ed8f10/pandas/core/reshape/merge.py#L1274.  Then there is an optional factorization happening in this function: https://github.com/pandas-dev/pandas/blob/28e0f18a30451601e8ac37b3ed75777bf1ed8f10/pandas/core/reshape/merge.py#L1281. Also there is another mandatory factorization at this line: https://github.com/pandas-dev/pandas/blob/28e0f18a30451601e8ac37b3ed75777bf1ed8f10/pandas/core/reshape/merge.py#L1286.

After the first factorization, I could already transform the keys to sth like [0,1,2,3..], what is the point to factorize it again? There are comments mentioning that it's flatting the keys and making it dense. I'm not sure what it means and could you tell me an easy example that could use the later factorizations?

To clarify again, I'm only talking about the inner join case.

Thank you sincerely for your help!

#### Expected Output

#### Output of ``pd.show_versions()``
"
590730856,33162,create rolling values from function that returns a numpy value,andrewczgithub,closed,2020-03-31T02:12:31Z,2020-03-31T03:13:39Z,"Hi all,

I am trying to create rolling values from a function that returns a numpy value, to get an idea of how stable to value is over time 
see my below attempt

' ' 'python

array([-9.51263882e-03, -2.81717483e-02,  9.43949087e-05, ...,
       -9.07504803e-03, -4.77400512e-03,  1.51740085e-03])

[def dist_range(x, y):
  return (np.max(np.abs(x - y), axis=1) - np.min(np.abs(x - y), axis=1)) / (np.max(np.abs(x - y), axis=1) + np.min(np.abs(x - y), axis=1))

##### RangeEn-B (mSampEn)
def RangeEn_B(x, emb_dim=2, tolerance=.1, dist=dist_range):

    n = np.shape(x)
    n = np.max(n)

    tVecs = np.zeros((n - emb_dim, emb_dim + 1))
    for i in range(tVecs.shape[0]):
        tVecs[i, :] = x[i:i + tVecs.shape[1]]
    counts = []
    for m in [emb_dim, emb_dim + 1]:
        counts.append(0)
        # get the matrix that we need for the current m
        tVecsM = tVecs[:n - m + 1, :m]
        # successively calculate distances between each pair of template vectors
        for i in range(len(tVecsM)):
            dsts = dist(tVecsM, tVecsM[i])
            # delete self-matching
            dsts = np.delete(dsts, i, axis=0)
            # delete undefined distances coming from zero segments
            # dsts = [x for i, x in enumerate(dsts) if not np.isnan(x) and not np.isinf(x)]
            # count how many 'defined' distances are smaller than the tolerance
            # if (dsts):
            counts[-1] += np.sum(dsts < tolerance)/(n - m - 1)

    if counts[1] == 0:
        # log would be infinite => cannot determine RangeEn_B
        RangeEn_B = np.nan
    else:
        # compute log of summed probabilities
        RangeEn_B = -np.log(1.0 * counts[1] / counts[0])

    return RangeEn_B](url)


#Assume matrix is a series
t = pd.Series(rs_num)
t=t.rolling(window=5, min_periods=2).apply(RangeEn_B).dropna()

' ' ' "
587638822,33010,add match to bare pyteset.raises() - arrays,sathyz,closed,2020-03-25T11:46:48Z,2020-03-31T03:18:04Z,"- [x] ref #30999
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
590790156,33165,CLN: Replace DataFrame() with empty_frame() in tests,jason3804,closed,2020-03-31T04:48:29Z,2020-03-31T04:56:20Z,"- [ ] closes #33161
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
95163295,10579,"ENH: Enhancement, new functionality request: Irregular time exponential smoothing",azuric,open,2015-07-15T10:53:12Z,2020-03-31T04:58:07Z,"Not sure if it is OK to make the request here, but here you go.

Can a feature be added to exponential smoothing where

alpha = time decay = (time_now - time_previous)/time_scale;

where time_scale and the difference are given in a specific unit eg milli/micro/nanoseconds.

This would really speed up temporally irregular time series analysis. I do understand that there are many ways to derive alpha in irregular time series but this one is I hope reasonably generic neat feature. 
"
304123482,20270,DOC: update the pandas.Series/DataFrame.interpolate docstring,math-and-data,closed,2018-03-11T02:12:06Z,2020-03-31T05:51:35Z,"- [x] PR title is ""DOC: update the <your-function-or-method> docstring""
- [ ] The validation script passes: `scripts/validate_docstrings.py <your-function-or-method>`
- [x] The PEP8 style check passes: `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] The html version looks good: `python doc/make.py --single <your-function-or-method>`
- [ ] It has been proofread on language by another sprint participant

Errors in the validation script:
- Formatting of the 'method' param is not right (not sure how to break option list properly into multiple lines)
- kwargs (to ignore)
- Formatting compaints are due to extra line for ""New in version ...""
```################################################################################
################### Docstring (pandas.DataFrame.interpolate) ###################
################################################################################

Interpolate values according to different methods.

Please note that only ``method='linear'`` is supported for
DataFrames/Series with a MultiIndex.

Parameters
----------
method : {'linear', 'time', 'index', 'values', 'nearest', 'zero',
          'slinear', 'quadratic', 'cubic', 'barycentric', 'krogh',
          'polynomial', 'spline', 'piecewise_polynomial', 'pad',
          'from_derivatives', 'pchip', 'akima'}, default 'linear'
    Interpolation technique to use.

    * 'linear': Ignore the index and treat the values as equally
      spaced. This is the only method supported on MultiIndexes.
      Default.
    * 'time': Interpolation works on daily and higher resolution
      data to interpolate given length of interval.
    * 'index', 'values': use the actual numerical values of the index.
    * 'pad': Fill in NaNs using existing values.
    * 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',
      'barycentric', 'polynomial': Passed to
      ``scipy.interpolate.interp1d``. Both 'polynomial' and 'spline'
      require that you also specify an `order` (int),
      e.g. df.interpolate(method='polynomial', order=4).
      These use the actual numerical values of the index.
    * 'krogh', 'piecewise_polynomial', 'spline', 'pchip', 'akima':
      Wrappers around the scipy interpolation methods of
      similar names. These use the actual numerical values of the
      index. For more information on their behavior, see the
      `scipy documentation
      <http://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation>`__
      and `tutorial documentation
      <http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html>`__.
    * 'from_derivatives': Refers to
      ``scipy.intrepolate.BPoly.from_derivatives`` which
      replaces 'piecewise_polynomial' interpolation method in
      scipy 0.18.

    .. versionadded:: 0.18.1

       Added support for the 'akima' method
       Added interpolate method 'from_derivatives' which replaces
       'piecewise_polynomial' in scipy 0.18; backwards-compatible with
       scipy < 0.18

axis : {0, 1}, default 0
    Axis to interpolate along.

    * 0: Fill column-by-column.
    * 1: Fill row-by-row.
limit : int, default None
    Maximum number of consecutive NaNs to fill. Must be greater than 0.
inplace : bool, default False
    Update the data in place if possible.
limit_direction : {'forward', 'backward', 'both'}, default 'forward'
    If limit is specified, consecutive NaNs will be filled in this
    direction.
limit_area : {'inside', 'outside'}, default None
    If limit is specified, consecutive NaNs will be filled with this
    restriction.

    * None: No fill restriction (default).
    * 'inside': Only fill NaNs surrounded by valid values
      (interpolate).
    * 'outside': Only fill NaNs outside valid values (extrapolate).

    .. versionadded:: 0.21.0

downcast : optional, 'infer' or None, defaults to None
    Downcast dtypes if possible.
kwargs
    Keyword arguments to pass on to the interpolating function.

Returns
-------
Series or DataFrame
    Same-shape object interpolated at the NaN values

See Also
--------
replace : replace a value
fillna : fill missing values

Examples
--------

Filling in NaNs in a Series via linear interpolation.

>>> ser = pd.Series([0, 1, np.nan, 3])
>>> ser.interpolate()
0    0.0
1    1.0
2    2.0
3    3.0
dtype: float64

Filling in NaNs in a Series by padding, but filling at most two
consecutive NaN at a time.

>>> ser = pd.Series([np.nan, ""single_one"", np.nan,
...                  ""fill_two_more"", np.nan, np.nan, np.nan,
...                  4.71, np.nan])
>>> ser
0              NaN
1       single_one
2              NaN
3    fill_two_more
4              NaN
5              NaN
6              NaN
7             4.71
8              NaN
dtype: object
>>> ser.interpolate(method='pad', limit=2)
0              NaN
1       single_one
2       single_one
3    fill_two_more
4    fill_two_more
5    fill_two_more
6              NaN
7             4.71
8             4.71
dtype: object

Create a DataFrame with missing values.

>>> df = pd.DataFrame([[0,1,2,0,4],[1,2,3,-1,8],
...                    [2,3,4,-2,12],[3,4,5,-3,16]],
...                   columns=['a', 'b', 'c', 'd', 'e'])
>>> df
   a  b  c  d   e
0  0  1  2  0   4
1  1  2  3 -1   8
2  2  3  4 -2  12
3  3  4  5 -3  16
>>> df.loc[3,'a'] = np.nan
>>> df.loc[0,'b'] = np.nan
>>> df.loc[1,'d'] = np.nan
>>> df.loc[2,'d'] = np.nan
>>> df.loc[1,'e'] = np.nan
>>> df
     a    b  c    d     e
0  0.0  NaN  2  0.0   4.0
1  1.0  2.0  3  NaN   NaN
2  2.0  3.0  4  NaN  12.0
3  NaN  4.0  5 -3.0  16.0

Fill the DataFrame forward (that is, going down) along each column.
Note how the last entry in column `a` is interpolated differently
(because there is no entry after it to use for interpolation).
Note how the first entry in column `b` remains NA (because there
is no entry befofe it to use for interpolation).

>>> df.interpolate(method='linear', limit_direction='forward', axis=0)
     a    b  c    d     e
0  0.0  NaN  2  0.0   4.0
1  1.0  2.0  3 -1.0   8.0
2  2.0  3.0  4 -2.0  12.0
3  2.0  4.0  5 -3.0  16.0

################################################################################
################################## Validation ##################################
################################################################################

Errors found:
        Errors in parameters section
                Parameter ""method"" description should start with capital letter
                Parameter ""method"" description should finish with "".""
                Parameter ""limit_area"" description should finish with "".""
                Parameter ""kwargs"" has no type
```"
228615552,16354,Handling of `end` in date_range,jnothman,open,2017-05-15T06:18:13Z,2020-03-31T06:17:32Z,"The documentation of `date_range`'s `end` parameter says:
```
      If periods is none, generated index will extend to first conforming
      time on or just past end argument
```

I interpret this to mean that either:
1. the last date in the index will be on or past the given end date, or that
2. the last period will include the given end date.

The first interpretation is clearly not true.

```python
>>> pandas.date_range(start='2017-01-01', end='2017-01-16', freq='7d')
DatetimeIndex(['2017-01-01', '2017-01-08', '2017-01-15'], dtype='datetime64[ns]', freq='7D')
```

But it seems like the second is not certainly true either:

```python
>>> pandas.date_range(start='2017-01-01', end='2017-01-16', freq='M')
DatetimeIndex([], dtype='datetime64[ns]', freq='M')
>>> pandas.date_range(start='2017-01-01', end='2017-02-16', freq='M')
DatetimeIndex(['2017-01-31'], dtype='datetime64[ns]', freq='M')
```

I am not sure if the behaviour with this unequal freq (like `M` which should extract the beginning of the month) is correct. Is this a bug?

Otherwise the documentation needs clarification.

More generally, it seems like the implementation is inegalitarian between starts and ends: you can't easily construct a `DatetimeIndex` based on `freq` which certainly includes `end`. One approach you could consider is to swap `start` and `end`, but without changing `freq` this produces an empty `DatetimeIndex`. Negating `freq` is possible if it has a unit (e.g. `-7d`) but I don't think there's a way to get get `freq='M'` backwards.

```python
>>> pandas.date_range(end='2017-01-01', start='2017-02-16', freq='M')
DatetimeIndex([], dtype='datetime64[ns]', freq='M')
>>> pandas.date_range(end='2017-01-01', start='2017-02-16', freq='-M')
Traceback (most recent call last):
  File ""/Users/joel/repos/pandas/pandas/tseries/frequencies.py"", line 549, in to_offset
    stride = int(stride)
ValueError: invalid literal for int() with base 10: '-'
```

So three potential sub-issues:
1. Clarify docs for `date_range`'s `end` (and related `end` params).
2. Check that behaviour regarding end with `DateOffset`s is intended.
3. A way to easily make `date_range` that starts at an endpoint and applies a `DateOffset` in reverse. (Perhaps this is the same as 2.)"
586900554,32973,PERF: Using _Period for optimization,ShaharNaveh,closed,2020-03-24T11:46:49Z,2020-03-31T06:38:10Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---
I did two bench marks one in a tight loop(```%timeit```), and one with asv.

I don't see a significant performance change (but I'm not sure I read the output correctly), maybe worth just removing the ""TODO"" note.

---

#### Benchmarks:

##### Timeit:

```
In [1]: import pandas as pd                                                                                   

In [2]: idx = pd.period_range(""2000-01-01"", periods=3)                                                        

In [3]: %timeit idx.get_loc(idx[1], ""pad"") 
337 µs ± 15 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) # Master
338 µs ± 7.16 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) # PR
```

##### ASV (period.Indexing)

```
       before           after         ratio
     [f20331d5]       [f71bf172]
     <master>         <PERF-index-_Period>
      1.46±0.02ms      1.45±0.03ms     0.99  period.Indexing.time_align
       8.57±0.2μs       8.51±0.2μs     0.99  period.Indexing.time_get_loc
         512±30μs          506±3μs     0.99  period.Indexing.time_intersection
         30.6±2μs       30.8±0.3μs     1.01  period.Indexing.time_series_loc
       3.95±0.3μs      3.96±0.02μs     1.00  period.Indexing.time_shallow_copy
       87.9±0.5μs       88.5±0.6μs     1.01  period.Indexing.time_unique
```"
590983656,33175,CI: add doctest check for done modules,ShaharNaveh,closed,2020-03-31T10:26:47Z,2020-03-31T13:14:37Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
590772740,33164,REF: collect .get tests,jbrockmendel,closed,2020-03-31T04:02:59Z,2020-03-31T14:00:16Z,
589899046,33131,BUG: Copying PeriodIndex levels on MultiIndex loses weakrefs,topper-123,closed,2020-03-29T22:19:14Z,2020-03-31T17:10:07Z,"As per [comment](https://github.com/pandas-dev/pandas/pull/32669#issuecomment-605525216) by @jacobaustin123:

```python
import pandas as pd
idx = pd.MultiIndex.from_arrays([pd.PeriodIndex([pd.Period(""2019Q1""), pd.Period(""2019Q2"")], name='b')])
idx2 = pd.MultiIndex.from_arrays([idx._get_level_values(level) for level in range(idx.nlevels)])
all(x.is_monotonic for x in idx2.levels) # raises an error
```

#### Problem description

The weakly referenced PeriodIndex er dropped before intended, so the ``PeriodEngine`` gets a ``None`` instead of the PeriodIndex.

#### Expected Output

The above should return True.
"
589901627,33133,BUG: to_datetime with infer_datetime_format dropped timezone names,mroeschke,closed,2020-03-29T22:33:25Z,2020-03-31T17:11:52Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
591137378,33181,Timestamp' object has no attribute 'split' problem,DanananBananan,closed,2020-03-31T14:03:58Z,2020-03-31T17:14:24Z,"import pandas as pd
import numpy as np
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

df1 = pd.read_excel(""npl90.xlsx"",parse_dates=True)
dates = []
npl = []
df1.shape
df=df1.iloc[::-1]

df.tail(1)
df=df.head(len(df)-1)
df
df_dates=df.loc[:, ""Date""]
df_npl=df.loc[:, 'NPL90+']

#Create independent data set x
for date in df_dates:
    dates.append([int(date.split('-')[1])])

def predict_npl(dates, npl, x):
    svr_lin= SVR(kernel=""linear"", C=1e3)
    svr_poly= SVR(kernel=""poly"", C=1e3, degree=2)
    svr_rbf=SVR(kernel=""rbf"", C=1e3, gamma=0.1)
    #Training
    svr_poly.fit(df_dates, df_npl)
    svr_lin.fit(df_dates, df_npl)
    svr_rbf.fit(df_dates, df_npl)
    
    #Create the linear regression
    lin_reg=LinearRegression()
    #Train the Linear regression model
    lin_reg.fit(df_dates, df_npl)
    
    
    #Plot models
    plt.scatter(df_dates, df_npl, color=""black"", label=""original data"")
    plt.plot(df_dates, svr_rbf.predict(df_dates), color='red', label= 'SVR RBF')
    plt.plot(df_dates, svr_poly.predict(df_dates), color='blue', label= 'SVR POLY')
    plt.plot(df_dates, svr_lin.predict(df_dates), color='green', label= 'SVR LIN')
    plt.plot(df_dates, lin_reg.predict(df_dates), color='grey', label= 'LIN Reg')
    plt.xlabel('Date')
    plt.ylabel('NPL')
    plt.title('Regression')
    plt.legend()
    plt.show()
    
    return svr_rbf.predict(x)[0], svr_lin.predict(x)[0], svr_poly.predict(x)[0], lin_reg.predict(x)[0]
    
#Predict price on 
predicted_npl=predict_npl(dates, npl,""2020-03-01"")
print(predicted_npl)

[npl90.xlsx](https://github.com/pandas-dev/pandas/files/4409483/npl90.xlsx)

"
589902488,33134,BUG: create new MI from MultiIndex._get_level_values,topper-123,closed,2020-03-29T22:38:56Z,2020-03-31T17:16:50Z,"- [x] closes #33131
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Closes #33131, a weakref was released too early."
590702782,33158,"BUG: isna_old with td64, dt64tz, period",jbrockmendel,closed,2020-03-31T00:57:28Z,2020-03-31T17:22:32Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Started as a CLN branch and i left that in for now, can separate if desired.  the bugfix is on L287."
590673101,33156,REF: collect .drop tests,jbrockmendel,closed,2020-03-30T23:40:47Z,2020-03-31T17:24:38Z,
590707198,33159,CLN: use ._data less in reshape,jbrockmendel,closed,2020-03-31T01:09:34Z,2020-03-31T17:25:25Z,
590510084,33150,REF: test_reindex_like,jbrockmendel,closed,2020-03-30T18:45:37Z,2020-03-31T17:28:08Z,
589507586,33093,DOC: Fix PR06,farhanreynaldo,closed,2020-03-28T04:40:25Z,2020-03-31T17:34:00Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. "
591231739,33183,REF: fillna tests,jbrockmendel,closed,2020-03-31T16:06:42Z,2020-03-31T17:34:41Z,"This gets them all for tests.indexes, a fraction for tests.series, and none for DataFrame.  This will take multiple passes"
573601277,32388,Create CITATION.md,ivalaginja,closed,2020-03-01T19:29:10Z,2020-03-31T17:39:44Z,"Addresses #24036 by adding a `CITATION.md` file to the repository.

I copied the citation instructions from here: https://pandas.io/about/citing.html
and added a section for the published software on Zenodo. Following [this recommendation](https://github.com/sherpa/sherpa/pull/634#issuecomment-553668211), I did this for the latest released version (v1.0.1) with a note for the user to go fetch the citation from Zenodo for the version they are actually using.

Tagging @TomAugspurger and @jreback since you were active in the linked issue.

I still recommend to update the citation request on the pandas website directly (https://pandas.io/about/citing.html), as well as on the Scipy website (https://www.scipy.org/citing.html#pandas), on https://pandas.pydata.org/ and maybe provide a `<package>.__citation__` variable as suggested [here](https://github.com/pandas-dev/pandas/issues/24036#issuecomment-585672595).
"
577408070,32531,DOC: Update pandas.core.groupby.GroupBy.pipe docstring,hsjsjsj009,closed,2020-03-07T22:09:02Z,2020-03-31T17:41:32Z,"- [x] closes https://github.com/pandanistas/pandanistas_sprint_ui2020/issues/2
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
558595144,31551,ADMIN: Create separate issue templates for different use cases,jschendel,closed,2020-02-01T20:47:13Z,2020-03-31T17:41:49Z,"I recall this being mentioned on one of the core dev calls a few months back, and took inspiration from [rapidsai/cudf](https://github.com/rapidsai/cudf) for the various templates.

Added templates for the following use cases:
- Bug Report
  - This is largely the same as the current issue template
- Documentation Enhancement
- Documentation Error
- Feature Request
- Submit Question
  - This attempts to direct users to StackOverflow for usage questions

We could also add a Blank Template for opening an issue without any template provided, but opted not to do that for now, as I don't want to encourage people to bypass these templates.  Could certainly add one if there's a consensus that we want this.

I've created a [local repo](https://github.com/jschendel/pandas-templates/issues) where you can see these templates in action since I couldn't figure out another way to actually display these.  Click the ""New Issue"" button to see what these changes would look like.

cc @pandas-dev/pandas-core @pandas-dev/pandas-triage "
533525241,30086,Series.searchsorted with different timezones,kenahoo,closed,2019-12-05T18:40:29Z,2020-03-31T18:24:23Z,"#### Code Sample, a copy-pastable example if possible

`DatetimeIndex.searchsorted` works fine with mixed timezones:

```python
>>> series_dt = pd.date_range('2015-02-19', periods=2, freq='1d', tz='UTC')
>>> dt = pd.to_datetime('2015-02-19T12:34:56').tz_localize('America/New_York')
>>> series_dt.searchsorted(dt)
1
```

However, `Series.searchsorted` does not:

```python
>>> from io import StringIO
>>> df = pd.read_csv(StringIO(""datetime\n2015-02-19T00:00:00Z\n2015-02-20T00:00:00Z""), parse_dates=['datetime'])
>>> df.datetime.searchsorted(dt)
Traceback (most recent call last):
  File ""/Users/kwilliams/Library/Application Support/IntelliJIdea2019.2/python/helpers/pydev/_pydevd_bundle/pydevd_exec2.py"", line 3, in Exec
    exec(exp, global_vars, local_vars)
  File ""<input>"", line 1, in <module>
  File ""venv/lib/python3.7/site-packages/pandas/core/series.py"", line 2694, in searchsorted
    return algorithms.searchsorted(self._values, value, side=side, sorter=sorter)
  File ""venv/lib/python3.7/site-packages/pandas/core/algorithms.py"", line 1887, in searchsorted
    result = arr.searchsorted(value, side=side, sorter=sorter)
  File ""venv/lib/python3.7/site-packages/pandas/core/arrays/datetimelike.py"", line 666, in searchsorted
    self._check_compatible_with(value)
  File ""venv/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py"", line 591, in _check_compatible_with
    own=self.tz, other=other.tz
ValueError: Timezones don't match. 'UTC != America/New_York'
```

Ostensibly, the underlying data vector is the same in both cases:

```python
>>> series_dt.dtype
datetime64[ns, UTC]
>>> df.datetime.dtype
datetime64[ns, UTC]
```

#### Problem description

I believe the `DatetimeIndex` is correct (or at least more useful), because even if timezones don't agree, the underlying instants are well-ordered and compare fine.  In fact, both versions compare fine using a simple `>` comparison:

```python
>>> dt > series_dt
array([ True, False])
>>> dt > df.datetime
0     True
1    False
Name: datetime, dtype: bool
```

#### Expected Output

```python
>>> df.datetime.searchsorted(dt)
1
```

A workaround is to wrap the column using `pd.DatetimeIndex()`:

```python
>>> pd.DatetimeIndex(df.datetime).searchsorted(dt)
1
```

#### Output of ``pd.show_versions()``

<details>
<pre>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.0.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_US.UTF-8
pandas           : 0.25.1
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.4.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
##teamcity[testStdOut timestamp='2019-12-05T12:38:05.901' flowId='test.test_driver.MyTestCase.test_driver' locationHint='python</Users/kwilliams/git/dispatcher/rush-springs-simulations>://test.test_driver.MyTestCase.test_driver' name='test_driver' nodeId='75' out='feather          : None|n' parentNodeId='74']
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : 0.3.5
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : None
##teamcity[testStdOut timestamp='2019-12-05T12:38:05.904' flowId='test.test_driver.MyTestCase.test_driver' locationHint='python</Users/kwilliams/git/dispatcher/rush-springs-simulations>://test.test_driver.MyTestCase.test_driver' name='test_driver' nodeId='75' out='xlrd             : None|n' parentNodeId='74']
xlwt             : None
xlsxwriter       : None
</pre>
</details>
"
591279158,33185,TST: cover search_sorted scalar mixed timezones case,jamescobonkerr,closed,2020-03-31T17:17:12Z,2020-03-31T18:24:29Z,"- [X] closes #30086
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Raises a `ValueError` on version 0.25.1 (reporter's version), as expected:

```
>>> import pandas as pd
>>> pd.__version__
'0.25.1'
>>> ser = pd.Series(date_range(""20120101"", periods=10, freq=""2D"", tz=""UTC""))
>>> val = pd.Timestamp(""20120102"", tz=""America/New_York"")
>>> res = ser.searchsorted(val)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/jamescobonkerr/venv/lib/python3.7/site-packages/pandas/core/series.py"", line 2694, in searchsorted
    return algorithms.searchsorted(self._values, value, side=side, sorter=sorter)
  File ""/Users/jamescobonkerr/venv/lib/python3.7/site-packages/pandas/core/algorithms.py"", line 1887, in searchsorted
    result = arr.searchsorted(value, side=side, sorter=sorter)
  File ""/Users/jamescobonkerr/venv/lib/python3.7/site-packages/pandas/core/arrays/datetimelike.py"", line 666, in searchsorted
    self._check_compatible_with(value)
  File ""/Users/jamescobonkerr/venv/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py"", line 591, in _check_compatible_with
    own=self.tz, other=other.tz
ValueError: Timezones don't match. 'UTC != America/New_York'
```"
589305735,33073,DOC: Added docstring for show_versions(),MarianD,closed,2020-03-27T17:55:55Z,2020-03-31T19:05:17Z,"- [x] closes #33044
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
579356854,32631,[WIP] Numpy dev 32bit (do not accept),toddrme2178,closed,2020-03-11T15:38:26Z,2020-03-31T19:08:07Z,"This will hopefull catch 32bit issues with newer numpy versions

Do NOT accept this at this point.  Right now I am trying to figure out where a bug is originating and I need the tests to run."
590925081,33170,Feature request: convert period to timedelta,yohplala,closed,2020-03-31T09:00:08Z,2020-03-31T20:32:12Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

mPI = pd.period_range(start='2020-02-01 10:00', end='2020-05-20 10:00', freq='1M')

mPI[0]
>>> Period('2020-02', 'M')

td = pd.Timedelta(mPI[0])
>>> ValueError: Value must be Timedelta, string, integer, float, timedelta or convertible, not Period

td = pd.to_timedelta(mPI[0])
>>> ValueError: Value must be Timedelta, string, integer, float, timedelta or convertible, not Period

mPI[0].to_timedelta()
>>> AttributeError: 'Period' object has no attribute 'to_timedelta'

# ok... write some more code then..
td = mPI[1].start_time - mPI[0].start_time
>>> Timedelta('29 days')
```
#### Problem description

Hi,
I would like to retrieve the duration of a period as a timedelta.
The 1st three examples are code I thought could be handled by pandas, but they are not.
The last one is an example of result I would like to retrieve.
Here, it is fine, I have the next period, so I can do the difference between the two successive periods.
If you don't have it, you are forced to create it.

Please, can pd.Period.to_timedelta() be defined in pandas?
Thanks for your help and support!
Bests,"
589669584,33107,ENH/VIZ: Allowing `s` parameter of scatter plots to be a column name,SultanOrazbayev,closed,2020-03-28T22:10:52Z,2020-03-31T21:28:17Z,"- [x] closes #32904 
- [x] tests added
- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This PR is a continuation of #32937 (made an error when pulling changes from the master).

I had a more elaborate decision tree for what to do with a passed size variable `s`, here https://github.com/pandas-dev/pandas/pull/32937, but in the end decided to go with the simpler version since the other checks are redundant (they would contain `s=s` or `pass`)."
591422337,33189,REF: set_axis tests,jbrockmendel,closed,2020-03-31T20:57:56Z,2020-03-31T21:44:26Z,Fully parametrized over Series/DataFrame
310178321,20560,Docstring and method behaviour differ,tv3141,closed,2018-03-30T21:58:17Z,2020-04-01T00:25:16Z,"```python
    def _validate_n(self, n):
        """"""
        Require that `n` be a nonzero integer.
        Parameters
        ----------
        n : int
        Returns
        -------
        nint : int
        Raises
        ------
        TypeError if `int(n)` raises
        ValueError if n != int(n)
        """"""
        try:
            nint = int(n)
        except (ValueError, TypeError):
            raise TypeError('`n` argument must be an integer, '
                            'got {ntype}'.format(ntype=type(n)))
        if n != nint:
            raise ValueError('`n` argument must be an integer, '
                             'got {n}'.format(n=n))
        return nint
```
`_validate_n` does not test whether `n` is nonzero.

https://github.com/pandas-dev/pandas/blob/601b8c9c45b3cb06ee4ceaf34456bbfd3f5e5d1d/pandas/_libs/tslibs/offsets.pyx#L355

`_validate_n` is only used in https://github.com/pandas-dev/pandas/blob/601b8c9c45b3cb06ee4ceaf34456bbfd3f5e5d1d/pandas/tseries/offsets.py

It may be that `n` does not have to be nonzero. Issue #20517 investigates whether `n` actually has to be nonzero in a case where `n` is tested again to be nonzero after calling `_validate_n`:

https://github.com/pandas-dev/pandas/blob/601b8c9c45b3cb06ee4ceaf34456bbfd3f5e5d1d/pandas/tseries/offsets.py#L1458-L1465

"
591479978,33190,CLN: Correct docstring to describe actual functionality.,tv3141,closed,2020-03-31T22:37:05Z,2020-04-01T00:25:23Z,"- [x] closes #20560

"
591504115,33191,REF: misplaced sort_index test,jbrockmendel,closed,2020-03-31T23:25:51Z,2020-04-01T00:53:42Z,
589306639,33074,"TYP: require _update_inplace gets Frame/Series, never BlockManager",jbrockmendel,closed,2020-03-27T17:56:51Z,2020-04-01T01:20:13Z,
292932503,19470,API: Categorical.get_values returns DatetimeIndex for datetime categories,TomAugspurger,closed,2018-01-30T20:49:58Z,2020-04-01T03:00:31Z,"```python
In [6]: pd.Categorical(['a', 'a', 'b', 'b']).get_values()
Out[6]: array(['a', 'a', 'b', 'b'], dtype=object)

In [7]: pd.Categorical(pd.DatetimeIndex(['2017', '2017', '2018', '2018'])).get_values()
Out[7]: DatetimeIndex(['2017-01-01', '2017-01-01', '2018-01-01', '2018-01-01'], dtype='datetime64[ns]', freq=None)
```

Meanwhile, `Series[Categorical[datetime]].get_values` returns a NumPy array.

```python
In [8]: pd.Series(pd.Categorical(pd.DatetimeIndex(['2017', '2017', '2018', '2018']))).get_values()
Out[8]:
array(['2017-01-01T00:00:00.000000000', '2017-01-01T00:00:00.000000000',
       '2018-01-01T00:00:00.000000000', '2018-01-01T00:00:00.000000000'],
      dtype='datetime64[ns]')
```

This seemed strange to have the return type depend on the categories' dtype. Going to try to find the original motivation. We could consider deprecating this behavior."
332067688,21466,Hdf5/ TimedeltaIndex storing not working property (table format),JerSefJeTu,open,2018-06-13T16:17:07Z,2020-04-01T03:22:53Z,"#### When i save DataFrame with method to_hdf and i passed parameter format=""table"" I get strange behavior when I read the file, because my index is a different datatype than I recorded a file.

~~~python
import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.normal(size=(10,5)))
df.index = pd.timedelta_range(start='0s',periods=10,freq='1s')

df.index  # I get this:

TimedeltaIndex(['00:00:00', '00:00:01', '00:00:02', '00:00:03', '00:00:04',
                '00:00:05', '00:00:06', '00:00:07', '00:00:08', '00:00:09'],
               dtype='timedelta64[ns]', freq='S')

df.to_hdf(""a.hdf"",key=""proba"",format=""table"")
r_df = pd.read_hdf(""a.hdf"")
r_df.index  # The read-in table gives this:

Int64Index([         0, 1000000000, 2000000000, 3000000000, 4000000000,
            5000000000, 6000000000, 7000000000, 8000000000, 9000000000],
           dtype='int64')
~~~

#### Problem description
Index datatype is changed to Int64Index i dont know why.
I tried same with format=""fixed"" However, in this case, the format of the index remains timedeltaindes, which in my code is not.

I do not know what the problem is, do I do the right thing, but certainly the to_hdf method behaves differently in relation to what is the parameter format sent when the ""fixed"" TimedeltaIndex stay TimedeltaIndex and when the ""table"" is passed then it is converted to Int64Index

Thanks a lot.


<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.4.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: AMD64 Family 16 Model 4 Stepping 3, AuthenticAMD
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.23.0
pytest: 3.5.1
pip: 10.0.1
setuptools: 28.8.0
Cython: None
numpy: 1.14.4
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 6.3.1
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: 3.4.3
numexpr: 2.6.5
feather: None
matplotlib: 2.1.1
openpyxl: 1.7.0
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
341621846,21936,Summary Method Misleading for Unsorted TimeseriesIndex Object,missing-semicolon,closed,2018-07-16T18:14:22Z,2020-04-01T03:28:53Z,"#### Code Sample

```python
import pandas as pd
import numpy as np
np.random.seed = 12345

ts = pd.date_range(start='2018-01-01', end='2018-06-30', freq='M')
ts.summary()  # Works as expected
> 'DatetimeIndex: 6 entries, 2018-01-31 to 2018-06-30\nFreq: M'

ts_scrambled = np.random.choice(ts, len(ts), replace=False)
pd.DatetimeIndex(ts_scrambled).summary()  # Not good.
> 'DatetimeIndex: 6 entries, 2018-01-31 to 2018-04-30'

```
#### Problem description
This is an issue when a DataFrame's index is set via the `set_index` method and it is applied to an unsorted column. The summary output of the resulting index is misleading.

#### Expected Output
```
>'DatetimeIndex: 6 entries, 2018-01-31 to 2018-06-30\nFreq: M'
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.4.final.0
python-bits: 64
OS: Linux
OS-release: 4.1.35-pv-ts2
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: en_US.utf8
LOCALE: en_US.UTF-8

pandas: 0.22.0
pytest: 3.2.1
pip: 9.0.1
setuptools: 36.4.0
Cython: 0.26
numpy: 1.12.1
scipy: 0.19.1
pyarrow: 0.8.0
xarray: None
IPython: 6.1.0
sphinx: 1.6.3
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.2
feather: None
matplotlib: 2.0.2
openpyxl: 2.4.8
xlrd: 1.1.0
xlwt: None
xlsxwriter: 0.9.8
lxml: None
bs4: 4.6.0
html5lib: 0.9999999
sqlalchemy: 1.1.13
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
371060624,23198,Check boundary undefined with non-default close parameter for date_range,Adirio,open,2018-10-17T12:58:02Z,2020-04-01T03:38:11Z,"#### Code Sample
```python
import pandas as pd
```
```python
print(pd.date_range(start=""2018-10-17"", periods=7, closed=""left""))
print(pd.date_range(start=""2018-10-17"", periods=7, closed=""right""))
print(pd.date_range(end=""2018-10-17"", periods=7, closed=""left""))
print(pd.date_range(end=""2018-10-17"", periods=7, closed=""right""))
```

#### Problem description

There is a check in the code that should not allow this cases. It raises a `ValueError`. The `if`condition is bugged and thus this check is not being processed. This was found while investigating #23176.

Note: this issue is not debating wether allowing or disallowing `closed` differents to `None` for only partially bounded `date_range`es (understanding partially bounded as only one of `start` and `end` provided). It is just pointing out that the current code is skipping a check.

#### Expected Output
1:
```python
ValueError(""Closed has to be None if not both of startand end are defined"")
```

2:
```python
ValueError(""Closed has to be None if not both of startand end are defined"")
```

3:
```python
ValueError(""Closed has to be None if not both of startand end are defined"")
```

4:
```python
ValueError(""Closed has to be None if not both of startand end are defined"")
```

#### Output of ``pd.show_versions()``

```
python: 3.6.4.final.0

pandas: 0.23.4
```"
591553707,33195,Added test for #5091 (Missing Periods for some DateOffsets),gorogoroumaru,closed,2020-04-01T01:39:05Z,2020-04-01T03:51:35Z,"- [x] closes #5091 

"
383822132,23869,DatetimeIndex.__getitem__ returns ndarray for nd indexer,TomAugspurger,closed,2018-11-23T13:19:19Z,2020-04-01T04:00:24Z,"I didn't know this was possible, but apparently it is (and tested).

For ndim > 1, `DatetimeIndex.__getitem__` returns an array

```python
In [13]: pd.date_range('2000', periods=12)[:, None]
Out[13]:
array([['2000-01-01T00:00:00.000000000'],
       ['2000-01-02T00:00:00.000000000'],
       ['2000-01-03T00:00:00.000000000'],
       ['2000-01-04T00:00:00.000000000'],
       ['2000-01-05T00:00:00.000000000'],
       ['2000-01-06T00:00:00.000000000'],
       ['2000-01-07T00:00:00.000000000'],
       ['2000-01-08T00:00:00.000000000'],
       ['2000-01-09T00:00:00.000000000'],
       ['2000-01-10T00:00:00.000000000'],
       ['2000-01-11T00:00:00.000000000'],
       ['2000-01-12T00:00:00.000000000']], dtype='datetime64[ns]')

```

while Index returns an Index:

```python
In [14]: pd.Index(np.arange(12))[:, None]
Out[14]: Int64Index([[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11]], dtype='int64')
```

FWIW, Categorical raises with an unfriendly error.

```python
In [17]: pd.Categorical([0, 1, 2])[:, None]
```

```pytb
Out[17]: ---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/Envs/pandas-dev/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj)
    700                 type_pprinters=self.type_printers,
    701                 deferred_pprinters=self.deferred_printers)
--> 702             printer.pretty(obj)
    703             printer.flush()
    704             return stream.getvalue()

~/Envs/pandas-dev/lib/python3.7/site-packages/IPython/lib/pretty.py in pretty(self, obj)
    398                         if cls is not object \
    399                                 and callable(cls.__dict__.get('__repr__')):
--> 400                             return _repr_pprint(obj, self, cycle)
    401
    402             return _default_pprint(obj, self, cycle)

~/Envs/pandas-dev/lib/python3.7/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)
    693     """"""A pprint that just redirects to the normal repr function.""""""
    694     # Find newlines and replace them with p.break_()
--> 695     output = repr(obj)
    696     for idx,output_line in enumerate(output.splitlines()):
    697         if idx:

~/sandbox/pandas/pandas/core/base.py in __repr__(self)
     75         Yields Bytestring in Py2, Unicode String in py3.
     76         """"""
---> 77         return str(self)
     78
     79

~/sandbox/pandas/pandas/core/base.py in __str__(self)
     54
     55         if compat.PY3:
---> 56             return self.__unicode__()
     57         return self.__bytes__()
     58

~/sandbox/pandas/pandas/core/arrays/categorical.py in __unicode__(self)
   1963             result = self._tidy_repr(_maxlen)
   1964         elif len(self._codes) > 0:
-> 1965             result = self._get_repr(length=len(self) > _maxlen)
   1966         else:
   1967             msg = self._get_repr(length=False, footer=True).replace(""\n"", "", "")

~/sandbox/pandas/pandas/core/arrays/categorical.py in _get_repr(self, length, na_rep, footer)
   1954         formatter = fmt.CategoricalFormatter(self, length=length,
   1955                                              na_rep=na_rep, footer=footer)
-> 1956         result = formatter.to_string()
   1957         return compat.text_type(result)
   1958

~/sandbox/pandas/pandas/io/formats/format.py in to_string(self)
    147                 return u('')
    148
--> 149         fmt_values = self._get_formatted_values()
    150
    151         result = [u('{i}').format(i=i) for i in fmt_values]

~/sandbox/pandas/pandas/io/formats/format.py in _get_formatted_values(self)
    135
    136     def _get_formatted_values(self):
--> 137         return format_array(self.categorical.get_values(), None,
    138                             float_format=None, na_rep=self.na_rep)
    139

~/sandbox/pandas/pandas/core/arrays/categorical.py in get_values(self)
   1455         if is_datetimelike(self.categories):
   1456             return self.categories.take(self._codes, fill_value=np.nan)
-> 1457         return np.array(self)
   1458
   1459     def check_for_ordered(self, op):

~/sandbox/pandas/pandas/core/arrays/categorical.py in __array__(self, dtype)
   1247             categorical.categories.dtype
   1248         """"""
-> 1249         ret = take_1d(self.categories.values, self._codes)
   1250         if dtype and not is_dtype_equal(dtype, self.categories.dtype):
   1251             return np.asarray(ret, dtype)

~/sandbox/pandas/pandas/core/algorithms.py in take_nd(arr, indexer, axis, out, fill_value, mask_info, allow_fill)
   1652     func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,
   1653                                  mask_info=mask_info)
-> 1654     func(arr, indexer, out, fill_value)
   1655
   1656     if flip_order:

~/sandbox/pandas/pandas/_libs/algos_take_helper.pxi in pandas._libs.algos.take_1d_int64_int64()
   3420 @cython.boundscheck(False)
   3421 def take_1d_int64_int64(ndarray[int64_t, ndim=1] values,
-> 3422                               int64_t[:] indexer,
   3423                               int64_t[:] out,
   3424                               fill_value=np.nan):

ValueError: Buffer has wrong number of dimensions (expected 1, got 2)
```


and unsurprisingly, lots of stuff seems to be broken even if the original `__getitem__` worked.

```pytb
In [36]: x
Out[36]: Int64Index([[1], [2], [4], [3]], dtype='int64')

In [37]: x.get_indexer_for(2)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-37-5c5d6de00158> in <module>()
----> 1 x.get_indexer_for(2)

~/sandbox/pandas/pandas/core/indexes/base.py in get_indexer_for(self, target, **kwargs)
   3511         This dispatches to get_indexer or get_indexer_nonunique as appropriate
   3512         """"""
-> 3513         if self.is_unique:
   3514             return self.get_indexer(target, **kwargs)
   3515         indexer, _ = self.get_indexer_non_unique(target, **kwargs)

~/sandbox/pandas/pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__()
     34             val = <object>PyDict_GetItem(cache, self.name)
     35         else:
---> 36             val = self.func(obj)
     37             PyDict_SetItem(cache, self.name, val)
     38         return val

~/sandbox/pandas/pandas/core/indexes/base.py in is_unique(self)
   1570     def is_unique(self):
   1571         """""" return if the index has unique values """"""
-> 1572         return self._engine.is_unique
   1573
   1574     @property

~/sandbox/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.is_unique.__get__()
    182     def is_unique(self):
    183         if self.need_unique_check:
--> 184             self._do_unique_check()
    185
    186         return self.unique == 1

~/sandbox/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine._do_unique_check()
    189
    190         # this de-facto the same
--> 191         self._ensure_mapping_populated()
    192
    193     @property

~/sandbox/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine._ensure_mapping_populated()
    248             values = self._get_index_values()
    249             self.mapping = self._make_hash_table(len(values))
--> 250             self._call_map_locations(values)
    251
    252             if len(self.mapping) == len(values):

~/sandbox/pandas/pandas/_libs/index_class_helper.pxi in pandas._libs.index.Int64Engine._call_map_locations()
    131         # self.mapping is of type Int64HashTable,
    132         # so convert dtype of values
--> 133         self.mapping.map_locations(algos.ensure_int64(values))
    134
    135     cdef _get_index_values(self):

~/sandbox/pandas/pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.map_locations()
    955
    956     @cython.boundscheck(False)
--> 957     def map_locations(self, ndarray[int64_t, ndim=1] values):
    958         cdef:
    959             Py_ssize_t i, n = len(values)

ValueError: Buffer has wrong number of dimensions (expected 1, got 2)

```"
386084728,24010,Clarify behavior pd.Period and pd.PeriodIndex when using timestamp information,trendelkampschroer,open,2018-11-30T08:26:31Z,2020-04-01T04:02:51Z,"#### Code Sample, a copy-pastable example if possible

```python
index = pd.date_range(""2001-01-01-08:00:00"", freq=""B"", periods=10)
DatetimeIndex(['2001-01-01 08:00:00', '2001-01-02 08:00:00',
               '2001-01-03 08:00:00', '2001-01-04 08:00:00',
               '2001-01-05 08:00:00', '2001-01-08 08:00:00',
               '2001-01-09 08:00:00', '2001-01-10 08:00:00',
               '2001-01-11 08:00:00', '2001-01-12 08:00:00'],
              dtype='datetime64[ns]', freq='B')
index.to_period()
PeriodIndex(['2001-01-01', '2001-01-02', '2001-01-03', '2001-01-04',
             '2001-01-05', '2001-01-08', '2001-01-09', '2001-01-10',
             '2001-01-11', '2001-01-12'],
            dtype='period[B]', freq='B')
pd.Period(""2001-01-01-09:00:00"", freq=pd.tseries.frequencies.to_offset(""D""))
Period('2001-01-01', 'D')
pd.Period(""2001-01-01-09:00:00"", freq=pd.DateOffset(days=1))
pandas/_libs/tslibs/period.pyx in pandas._libs.tslibs.period.Period.__new__()

pandas/_libs/tslibs/parsing.pyx in pandas._libs.tslibs.parsing.parse_time_string()

~/.miniconda3/lib/python3.6/site-packages/pandas/tseries/offsets.py in rule_code(self)
    394     @property
    395     def rule_code(self):
--> 396         return self._prefix
    397 
    398     @property

~/.miniconda3/lib/python3.6/site-packages/pandas/tseries/offsets.py in _prefix(self)
    390     @property
    391     def _prefix(self):
--> 392         raise NotImplementedError('Prefix not defined')
    393 
    394     @property

NotImplementedError: Prefix not defined

```
#### Problem description

I am struggling to understand the correct way to handle time information together with a Period/PeriodIndex. Assume my business day starts at 08:00:00 possibly in some specified time zone, then I would want to represent time periods spanning a business day starting at that time using a PeriodIndex. 

A DatetimeIndex can very well handle time information, and if it has a fixed frequency it does somehow imply the underlying periods of time, e.g. [2001-01-01-08:00:00, 2001-01-02-07:59:59].

I am not completely sure if the behaviour is correct/desired but the difference between DatetimeIndex and PeriodIndex is unintuitive/seems inconsistent to me. 

I am not sure if freq=""B"" (or ""D"") implies that start_time = 00:00:00 and end_time = 23:59:59
for all periods, but if so then this should be clarified via additional examples in the docs. Still the 
different behaviour of DatetimeIndex (where time 09:00:00 is respected) and PeriodIndex (where time is ignored / implicitly set to beginning of period implied by ""B"") seems inconsistent.

The failing example with `pd.DateOffset(days=1)` is also counterintuitive in the following sense. If `freq=""D""` implies a fixed `start_time=00:00:00` and `end_time=23:59:59` then I want to be able to describe a time period with a length of one day starting at ""2001-01-01-09:00:00"" exactly the way that is now raising above exception.

#### Expected Output
```python
index.to_period()
PeriodIndex(['2001-01-01-08:00:00', '2001-01-02-08:00:00', '2001-01-03-08:00:00', 
             '2001-01-04-08:00:00', '2001-01-05-08:00:00', '2001-01-08-08:00:00',
             '2001-01-09-08:00:00', '2001-01-10-08:00:00', '2001-01-11-08:00:00',
             '2001-01-12-08:00:00'],  dtype='period[B]', freq='B')
```

The same for single Periods. 

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None

pandas: 0.23.4
pytest: None
pip: 10.0.1
setuptools: 39.2.0
Cython: None
numpy: 1.15.0
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.5.0
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.1
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
410868797,25336,[BUG?] pd.read_json does not convert date before 1971-01-01,okomarov,open,2019-02-15T17:24:03Z,2020-04-01T04:17:59Z,"#### Code Sample

```python
from datetime import datetime
import pandas as pd
input_before_31536000 = pd.DataFrame([1],index=[datetime(1971, 1, 1)])
input_after_31536000 = pd.DataFrame([1],index=[datetime(1971, 1, 1, 0, 0, 1)])

# Compare before (including) and after 31536000
pd.read_json(input_before_31536000.to_json(date_unit='s'), date_unit='s')
pd.read_json(input_after_31536000.to_json(date_unit='s'), date_unit='s')
```

#### Problem description
Read json does not convert epoch timestamps before and including 31536000 into dates. We start with the following input:

```
            0
1971-01-01  1
```
and after a roundtrip encoding/decoding transforms into:
```
          0
31536000  1
```

Any date after `datetime(1971, 1, 1)` get converted correctly.

#### A warkaround
I am using epoch timestamps and `orient='split'` to keep the encoding as terse as possible. ISO dates would solve the issue but would take much more space. To keep epoch timestamp here is a manual workaround:

```python
import json

# Encode series
json_str = input_before_31536000[0].to_json(orient='split', date_unit='s')
data = json.loads(json_str)
# Build DataFrame manually with to_datetime() conversion
pd.DataFrame(data['data'], index=pd.to_datetime(data['index'], unit='s'), columns=[data['name']])
```
#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.2.final.0
python-bits: 64
OS: Darwin
OS-release: 18.2.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.24.0
pytest: None
pip: 19.0.1
setuptools: 40.7.3
Cython: None
numpy: 1.16.1
scipy: None
pyarrow: None
xarray: None
IPython: 7.2.0
sphinx: None
patsy: None
dateutil: 2.7.5
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: 4.3.0
bs4: None
html5lib: None
sqlalchemy: 1.2.17
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: 0.2.1
pandas_gbq: None
pandas_datareader: 0.7.0
gcsfs: None
```

</details>
"
590976404,33174,CLN: using C-API of datetime,ShaharNaveh,closed,2020-03-31T10:16:14Z,2020-04-01T09:37:46Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
591260458,33184,REF/CLN: test_get_dummies,jbrockmendel,closed,2020-03-31T16:48:35Z,2020-04-01T11:00:23Z,
589101557,33068,"df.to_feather pandas:  Failed to open local file, error: The requested operation cannot be performed on a file with a user-mapped section open.",naveenbhupathi,closed,2020-03-27T12:31:25Z,2020-04-01T11:56:45Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
df = pd.DataFrame({'Loan':[""L1"",""L2""], 'Gender':[""male"",""Female""], ""income"":[25000.324, 43422.2754]})
df

df['income'] = df['income'].round(decimals=2)
df = df.astype(dict(zip(['Gender'], ['category'])), copy = True)
df

df.to_feather(""temp"")
df = pd.read_feather(""temp"")
df.to_feather(""temp"")
```
#### Problem description
I applied round function on a column and then applied object to category conversion.
I saved the final dataframe into feather file.

When i tried to read the feather file and save the feather file again, I'm getting file is open as per the error. Now able to understand, why the file is getting locked when i read the feather file. Any help on this is appreciated

Using pandas 1.0.0 and python 3.6.5v"
587092540,32982,ImportError: Unable to import required dependencies. Most likely due to a circular import. Conflict with numpy,GrzegorzKrug,closed,2020-03-24T16:24:29Z,2020-03-24T17:19:26Z,"<!-- Please describe the issue in detail here, and fill in the fields below -->
I am working in python 3.8.1 to be specific.
This error appears when runing script in conda environment and also in docker image.


### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```
conda create -n analzyer python=3.8.1
conda activate analyzer
pip install -r requirements.txt
python db_initiator.py
```
```
# Requirements.txt
numpy==1.18.2  # This package has conflict 
pandas==1.0.1  
PyQt5==5.14.1
request==2019.4.13
requests==2.23.0
requests_oauthlib==1.3.0
pytest==5.3.5
psycopg2-binary==2.8.4
SQLAlchemy==1.3.15
```
```
#  db_initiator.py
from sqlalchemy import Column, Integer, String, ForeignKey
```

<!-- Remove these sections for a feature request -->

### Error message:
```
Traceback (most recent call last):
  File ""/home/greg/_Local/git/TwitterAnalyzer/twitter_analyzer/analyzer/db_initiator.py"", line 1, in <module>
    from sqlalchemy import Column, Integer, String, ForeignKey
  File ""/home/greg/anaconda3/envs/analyzer/lib/python3.8/site-packages/sqlalchemy/__init__.py"", line 8, in <module>
    from . import util as _util  # noqa
  File ""/home/greg/anaconda3/envs/analyzer/lib/python3.8/site-packages/sqlalchemy/util/__init__.py"", line 9, in <module>
    from collections import defaultdict  # noqa
  File ""/home/greg/anaconda3/envs/analyzer/lib/python3.8/collections/__init__.py"", line 21, in <module>
    from operator import itemgetter as _itemgetter, eq as _eq
  File ""/home/greg/_Local/git/TwitterAnalyzer/twitter_analyzer/analyzer/operator.py"", line 6, in <module>
    import pandas as pd
  File ""/home/greg/anaconda3/envs/analyzer/lib/python3.8/site-packages/pandas/__init__.py"", line 16, in <module>
    raise ImportError(
ImportError: Unable to import required dependencies:
numpy: 

IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the numpy c-extensions failed.
- Try uninstalling and reinstalling numpy.
- If you have already done that, then:
  1. Check that you expected to use Python3.8 from ""/home/greg/anaconda3/envs/analyzer/bin/python"",
     and that you have no directories in your PATH or PYTHONPATH that can
     interfere with the Python and numpy version ""1.18.2"" you're trying to use.
  2. If (1) looks fine, you can open a new issue at
     https://github.com/numpy/numpy/issues.  Please include details on:
     - how you installed Python
     - how you installed numpy
     - your operating system
     - whether or not you have multiple versions of Python installed
     - if you built from source, your compiler versions and ideally a build log

- If you're working with a numpy git repository, try `git clean -xdf`
  (removes all files not under version control) and rebuild numpy.

Note: this error has many possible causes, so please don't comment on
an existing issue about this - open a new one instead.

Original error was: cannot import name 'namedtuple' from partially initialized module 'collections' (most likely due to a circular import) (/home/greg/anaconda3/envs/analyzer/lib/python3.8/collections/__init__.py)


Process finished with exit code 1
```
### Numpy/Python version information:
```
numpy==1.18.2
pandas==1.0.1
```
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
I can not do this. Got error when importing numpy itself
```
import sys, numpy; print(numpy.__version__, sys.version)
```
```
>>> import numpy
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/greg/anaconda3/envs/analyzer/lib/python3.8/site-packages/numpy/__init__.py"", line 142, in <module>
    from . import core
  File ""/home/greg/anaconda3/envs/analyzer/lib/python3.8/site-packages/numpy/core/__init__.py"", line 24, in <module>
    from . import multiarray
  File ""/home/greg/anaconda3/envs/analyzer/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 9, in <module>
    import functools
  File ""/home/greg/anaconda3/envs/analyzer/lib/python3.8/functools.py"", line 17, in <module>
    from collections import namedtuple
  File ""/home/greg/anaconda3/envs/analyzer/lib/python3.8/collections/__init__.py"", line 21, in <module>
    from operator import itemgetter as _itemgetter, eq as _eq
  File ""/home/greg/_Local/git/TwitterAnalyzer/twitter_analyzer/analyzer/operator.py"", line 6, in <module>
    import pandas as pd
  File ""/home/greg/anaconda3/envs/analyzer/lib/python3.8/site-packages/pandas/__init__.py"", line 22, in <module>
    from pandas.compat.numpy import (
  File ""/home/greg/anaconda3/envs/analyzer/lib/python3.8/site-packages/pandas/compat/__init__.py"", line 10, in <module>
    import platform
  File ""/home/greg/anaconda3/envs/analyzer/lib/python3.8/platform.py"", line 117, in <module>
    import re
  File ""/home/greg/anaconda3/envs/analyzer/lib/python3.8/re.py"", line 313, in <module>
    @functools.lru_cache(_MAXCACHE)
AttributeError: partially initialized module 'functools' has no attribute 'lru_cache' (most likely due to a circular import)
```

From my test, I know that conflict is when I install `pandas` and `numpy` only. Error is the same when importing numpy.
```
$ pip list
Package         Version            
--------------- -------------------
certifi         2019.11.28         
numpy           1.18.2             
pandas          1.0.3              
pip             20.0.2             
python-dateutil 2.8.1              
pytz            2019.3             
setuptools      46.1.1.post20200323
six             1.14.0             
wheel           0.34.2 
```
"
586219285,32929,TST: bare pytest raises in tests/scalar,quangngd,closed,2020-03-23T13:43:27Z,2020-03-24T17:47:31Z,"- [ ] ref #30999
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

https://github.com/pandas-dev/pandas/issues/30999

"
516453516,29334,BUG: assignment to multiple columns when some column do not exist,howsiwei,closed,2019-11-02T03:19:42Z,2020-03-24T18:38:01Z,"- [x] closes #13658
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Previous PR: #26534

In particular, the following code now behaves correctly.

```python
import pandas as pd
df = pd.DataFrame({'a': [0, 1, 2], 'b': [3, 4, 5]})
df[['a', 'c']] = 1
print(df)
```
v0.22: error

master: column `'c'` is converted to index `-1`, which causes last column to be overwritten.
```
   a  b
0  1  1
1  1  1
2  1  1
```

After this PR:
```
   a  b  c
0  1  3  1
1  1  4  1
2  1  5  1
```"
586829328,32970,Correct data type misspelling,evolutics,closed,2020-03-24T09:48:21Z,2020-03-24T19:17:50Z,
587070446,32981,TST: Move _get_cython_table_params into pandas/_testing.py,SaturnFromTitan,closed,2020-03-24T15:57:22Z,2020-03-24T19:35:43Z,"Part of #30914
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
586666574,32968,CLN: move misplaced (and duplicated) Dataframe.__repr__ test,jbrockmendel,closed,2020-03-24T03:34:00Z,2020-03-24T19:49:15Z,
586366207,32938,REF: misplaced DTI.shift tests,jbrockmendel,closed,2020-03-23T17:05:26Z,2020-03-24T19:51:16Z,
586492079,32947,REF: .values -> ._values,jbrockmendel,closed,2020-03-23T20:18:05Z,2020-03-24T19:51:37Z,
586501412,32948,REF: misplaced DataFrame.where tests,jbrockmendel,closed,2020-03-23T20:34:57Z,2020-03-24T19:51:55Z,
586673421,32969,"REF: misplaces Series.where, Series.rename tests",jbrockmendel,closed,2020-03-24T03:55:42Z,2020-03-24T19:52:17Z,
586524568,32952,CLN: Remove GroupByError exception,dsaxton,closed,2020-03-23T21:18:17Z,2020-03-24T19:56:45Z,From what I can tell this exception isn't used / needed
585870113,32919,CLN: Remove unused is_datetimelike arg,dsaxton,closed,2020-03-23T01:39:39Z,2020-03-24T19:56:57Z,
585793121,32910,CLN: Split integer array tests,dsaxton,closed,2020-03-22T19:26:12Z,2020-03-24T19:57:08Z,Follow-up to https://github.com/pandas-dev/pandas/pull/32780 (as with that PR there shouldn't be any changes to testing logic here)
585789372,32909,TST: collect .insert tests,jbrockmendel,closed,2020-03-22T19:07:25Z,2020-03-24T19:58:52Z,
585796543,32912,REF: misplaced arithmetic tests,jbrockmendel,closed,2020-03-22T19:44:33Z,2020-03-24T19:59:16Z,
585742856,32906,TST: Avoid bare pytest.raises in mult files,Vlek,closed,2020-03-22T15:23:04Z,2020-03-24T20:00:53Z,"* [x]  ref #30999
 
* [x]  tests added / passed
 
* [x]  passes `black pandas`

* [x]  passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
587097073,32983,DOC: Fix orc link,RJ3,closed,2020-03-24T16:29:52Z,2020-03-24T20:10:53Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
585613790,32899,Timedeltas: Understand µs,alvaroaleman,closed,2020-03-22T00:21:26Z,2020-03-24T20:24:39Z,"- [ ] closes #xxxx
- [X] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry


These are emitted by golangs `time.Duration` printing: https://github.com/golang/go/blob/36b815edd6cd23d5aabdb488c24db2033bbdeea2/src/time/time.go#L669"
583795350,32806,ENH: Allow regex matching in `fullmatch` mode,frreiss,closed,2020-03-18T15:05:54Z,2020-03-24T21:33:02Z,"#### Problem description

`Series.str` contains methods for all the regular expression matching modes in the `re` package except for `re.fullmatch()`. `fullmatch` only returns matches that cover the entire input string, unlike `match`, which also returns matches that start at the beginning of the string but do not cover the complete string.

One can work around the lack of `fullmatch` by round-tripping to/from numpy arrays and using `np.vectorize`, i.e.

```python
>>> s = pd.Series([""foo"", ""bar"", ""foobar""])
>>> my_regex = ""foo""
>>> import re
>>> import numpy as np
>>> compiled_regex = re.compile(my_regex)
>>> regex_f = np.vectorize(lambda s: compiled_regex.fullmatch(s) is not None)
>>> matches_array = regex_f(s.values)
>>> matches_series = pd.Series(matches_array)
>>> matches_series
0     True
1    False
2    False
dtype: bool
```

but it would be more convenient for users if `fullmatch` was built in.

The `fullmatch` method was added to the `re` package in Python 3.4. I think that the reason this method wasn't in previous versions of Pandas was that older versions of Python don't have `re.fullmatch`. As of Pandas 1.0, all the supported versions of Python now have `fullmatch`.

I have a pull request ready that adds this functionality. After my changes, the `Series.str` namespace gets a new method `fullmatch` that evaluates `re.fullmatch` over the series. For example:
```python
>>> s = pd.Series([""foo"", ""bar"", ""foobar""])
>>> s.str.fullmatch(""foo"")
0     True
1    False
2    False
dtype: bool
```

[Edit: Simplified the workaround]"
583829988,32807,"[ENH] Add ""fullmatch"" matching mode to Series.str [#32806]",frreiss,closed,2020-03-18T15:54:11Z,2020-03-24T21:33:11Z,"- [X] closes #32806
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

This is my first PR against this project, so apologies if I've missed any steps in the process. I'm assuming that I'm supposed to fill in the checklist above myself.

This pull request adds the `fullmatch` regular expression matching mode to the other modes already present under the `Series.str` namespace. For example:

```python
>>> s = pd.Series([""foo"", ""bar"", ""foobar""])
>>> s.str.fullmatch(""foo"")
0     True
1    False
2    False
dtype: bool
```

The `fullmatch` matching mode restricts matches to those that only match the *entire* string. Note the differences from `match`:
```python
>>> s = pd.Series([""foo"", ""bar"", ""foobar""])
>>> s.str.fullmatch(""foo"")
0     True
1    False
2    False
dtype: bool
>>> s.str.match(""foo"")
0     True
1    False
2     True
dtype: bool
```

I've also added regression tests and a ""what's new"" entry.

I have also opened issue #32806 to cover this new feature."
485654688,28168,WEB: Home page content for the new website,datapythonista,open,2019-08-27T08:11:58Z,2020-03-24T21:42:31Z,"In #28014 we added a first draft of the new pandas website. It can be seen here:
https://datapythonista.github.io/pandas-web/ (will be at https://dev.pandas.io when #28497 is merged).

Things that are already in the home page, or that could be added:

- [X] Short sentence to summarize what pandas is about
- [X] Links to the main pages / section (feedback on things to add/remove/change welcome)
- [X] List of companies supporting pandas (do we need to formalize the criteria to be there?)
- [X] Info and links to the last version (download, docs...)
- [X] Follow us on twitter
- [X] Wes book
- [X] Links to previous versions
- [ ] Project highlights (See the home of Jupyter, Dask, Django or Spark for reference). How do we want it? What exact highlights do we want to list?
- [ ] Success stories. We have some in the current website (https://pandas.pydata.org/#what-do-our-users-have-to-say) but in my opinion we could do much better. Like some known faces/companies. I can ask one from the team of the black hole image for example.
- [ ] Summary of the ecosystem page. Like the basics, mentioning matplotlib for plotting, numpy for array computing, Jupyter as an interface, Dask for distributed...
- [ ] Anything else?"
587257501,32986,DOC: Modify validate_rst_title_capitalization.py script,cleconte987,closed,2020-03-24T20:25:36Z,2020-03-24T22:54:54Z,"Regarding issue #32550.

@datapythonista

Modify script in order to not take into account words with 2 or more capital letters, which should all be proper words."
586382046,32939,pandas/core/ops/: replace .format() with f-strings,SvoONs,closed,2020-03-23T17:20:29Z,2020-03-25T00:05:55Z,"- https://github.com/pandas-dev/pandas/issues/29547
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
586395888,32940,REF: collect casting portion of Block.setitem,jbrockmendel,closed,2020-03-23T17:40:55Z,2020-03-25T00:40:59Z,"Block.setitem is sometimes inplace and sometimes casts/copies.  The idea here is to isolate the casting cases and eventually separate them into their own method, so we can be more careful about when we call the inplace vs not-inplace setters."
586637379,32966,CLN: Assorted core/generic.py cleanup,dsaxton,closed,2020-03-24T02:09:41Z,2020-03-25T01:05:20Z,A few mostly aesthetic cleanups
587266032,32987,CLN: Fix linting,dsaxton,closed,2020-03-24T20:41:32Z,2020-03-25T02:52:30Z,I think this fixes the linting problem in the CI
587433554,32998,df.pct_change() not working as expected,bharat-ics,closed,2020-03-25T04:27:23Z,2020-03-25T05:00:33Z,"It appears that pct_change is handling signs wrongly! 

```python
df['% Change']=df['A'].pct_change().fillna(0)*100

```
#### Problem description

#### Current output from dataframe
index | A | % Change
1 | -482.411 | 0.000000
2 | -357.073 | -25.981580
3 | -506.110 | 41.738524

#### Expected Output from the dataframe
index | A | % Change
1 | -482.411 | 0.000000
2 | -357.073 | 25.981580
3 | -506.110 | -41.738524



#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.0.1
"
587477319,32999,"DOC: Fix PR01, PR02, RT03 on cumulative function",farhanreynaldo,closed,2020-03-25T06:41:20Z,2020-03-25T06:44:50Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977."
586914673,32975,CLN: Remove shebang from files that don't need it,ShaharNaveh,closed,2020-03-24T12:12:35Z,2020-03-25T07:40:23Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
569408410,32190,BUG: could be a problem related to dropna(),Conxz,closed,2020-02-22T23:29:40Z,2020-03-25T07:41:56Z,"#### Code Sample

```python
import pandas as pd
import numpy as np

df = pd.DataFrame({""name"": ['Alfred', 'Batman', 'Catwoman'],
                   ""toy"": [np.nan, 'Batmobile', 'Bullwhip'],
                   ""born"": [pd.NaT, pd.Timestamp(""1940-04-25""),
                            pd.NaT]})
#df
#        born      name        toy
#0        NaT    Alfred        NaN
#1 1940-04-25    Batman  Batmobile
#2        NaT  Catwoman   Bullwhip

df.dropna(inplace=True)
#df 
#        born    name        toy 
#1 1940-04-25  Batman  Batmobile

# A. 
dat_df = pd.DataFrame(data=np.zeros(1), columns=['Score'])
dat_df['SID'] = df['name']
#dat_df 
#   Score  SID 
#0    0.0  NaN 

# B.
dat_df = pd.DataFrame(data=np.zeros(1), columns=['Score'])
dat_df['SID'] = df['name'].values
#dat_df 
#   Score     SID
#0    0.0  Batman 

```
#### Problem description
Have a look at the code in block A and B. In A, df['name'] was used, while in B, df['name'].values was used. Apperantly, output of A was not correct, where it seems that the first term in the original df (before applying dropna()) was assign to the new dat_df. 
I tested this issue with version 0.18.1, 0.25.1, and 1.0.1. 

"
585062490,32864,Fix wrong type checking in concat,pbourguignon,closed,2020-03-20T13:16:23Z,2020-03-25T08:49:10Z,"While the documentation of ``pandas.DataFrame.concat`` specifies that
any mapping of ``Label`` to ``FrameOrSeries`` is acceptable as the first
argument, but the code actually checks for an instance of ``dict``.
This changeset fixes that restriction.

- [x] closes #32863 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
587622558,33007,min of Index does not have required axis= keyword for np.min ,SylvainGuieu,closed,2020-03-25T11:16:55Z,2020-03-25T11:28:07Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
> from pandas import DataFrame
> import bumpy as np
> df = DataFrame( [[10,20,30]], index=[0], columns=[1,2,3])
> df.columns.min()
1
> min(df.columns)
1
> np.min( df.columns )
TypeError: min() got an unexpected keyword argument 'axis'
```


#### Problem description

The np.min (or max, ... etc)  function failed probably because the .min method of Index does not have the axis keyword 
np.min should behave like min builtin 

#### Expected Output
1 

#### Output of ``pd.show_versions()``
<details>
INSTALLED VERSIONS
------------------
commit: None

pandas: 0.23.4
pytest: 4.0.2
pip: 18.1
setuptools: 40.6.3
Cython: 0.29.2
numpy: 1.15.4
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 7.2.0
sphinx: 1.8.2
patsy: 0.5.1
dateutil: 2.7.5
pytz: 2018.7
blosc: None
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.8
feather: None
matplotlib: 3.0.2
openpyxl: 3.0.0
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.2
lxml: 4.2.5
bs4: 4.6.3
html5lib: 1.0.1
sqlalchemy: 1.2.15
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None


</details>
"
587543160,33002,Timestamps with different same tzinfo but different tzinfo type (datetime.timezone vs pytz.UTC),rooom13,closed,2020-03-25T08:59:45Z,2020-03-25T18:08:58Z,"#### Code Sample, a copy-pastable example if possible

```python
import pytz
import datetime
import pandas as pd
date = pd.Timestamp(""13-12-2020"",tzinfo=datetime.timezone.utc)
date2 = pd.Timestamp(""12-12-2020"",tzinfo=pytz.UTC)
date - date2

>>> Traceback (most recent call last):
 File ""<stdin>"", line 1, in <module>
 File ""pandas/_libs/tslibs/c_timestamp.pyx"", line 300, in
 pandas._libs.tslibs.c_timestamp._Timestamp.__sub__
TypeError: Timestamp subtraction must have the same timezones or no timezones
```
#### Problem description
Current's timezone comparison is not timzone object type sensitive. Even if two dates are UTC but different tzinfo object (e.g: datetime.timezone.utc and pytz.UTC) they will be considered different timezones.
This could lead to unstabilitites since a Timestamp can have both obejct types as tzinfo

#### Expected Output

```python
import pytz
import datetime
import pandas as pd
date = pd.Timestamp(""13-12-2020"",tzinfo=datetime.timezone.utc)
date2 = pd.Timestamp(""12-12-2020"",tzinfo=pytz.UTC)
date - date2
>>> Timedelta('1 days 00:00:00')
```
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-18362-Microsoft
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 45.0.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
586435645,32943,CLN: xarray tests,ShaharNaveh,closed,2020-03-23T18:46:54Z,2020-03-25T19:06:10Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
584702455,32839,BUG: Fix read_csv IndexError crash for c engine with header=None and 2 (or more) extra columns,roberthdevries,closed,2020-03-19T21:11:33Z,2020-03-25T20:36:11Z,"- [x] closes #26218
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
587480594,33000,"DOC: Fix PR01, PR02, RT03 on cumulative function",farhanreynaldo,closed,2020-03-25T06:49:34Z,2020-03-25T23:49:35Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977."
508166943,29046,CLN: Fixing mypy errors in pandas/conftest.py,angelaambroz,closed,2019-10-17T00:30:25Z,2020-03-25T23:51:19Z,"- [x] xref #28926 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] ~whatsnew entry~

"
587600586,33005,STY: Boolean values for bint variables,ShaharNaveh,closed,2020-03-25T10:39:02Z,2020-03-25T23:54:48Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

```asv``` benchmarks: to show that performance is not changed with the stylistic changes)

```
All benchmarks:

       before           after         ratio
     [28e0f18a]       [6f1fd78d]
     <master>         <STY-bint-boolean-not-int-join>
         21.4±1ms       19.8±0.4ms     0.92  join_merge.Join.time_join_dataframe_index_multi(False)
       24.9±0.3ms       23.6±0.3ms     0.95  join_merge.Join.time_join_dataframe_index_multi(True)
       14.5±0.3ms       14.0±0.3ms     0.97  join_merge.Join.time_join_dataframe_index_shuffle_key_bigger_sort(False)
       17.6±0.6ms       16.7±0.5ms     0.95  join_merge.Join.time_join_dataframe_index_shuffle_key_bigger_sort(True)
       14.4±0.3ms       13.8±0.2ms     0.95  join_merge.Join.time_join_dataframe_index_single_key_bigger(False)
       17.8±0.7ms       16.4±0.5ms     0.92  join_merge.Join.time_join_dataframe_index_single_key_bigger(True)
       13.5±0.3ms       13.1±0.3ms     0.97  join_merge.Join.time_join_dataframe_index_single_key_small(False)
       14.5±0.1ms       13.9±0.5ms     0.96  join_merge.Join.time_join_dataframe_index_single_key_small(True)
       2.86±0.08s       2.94±0.05s     1.03  join_merge.JoinIndex.time_left_outer_join_index
         433±70μs         428±50μs     0.99  join_merge.JoinNonUnique.time_join_non_unique_equal
```"
587369649,32994,"REF: collect DataFrame.drop, Series.drop tests",jbrockmendel,closed,2020-03-25T00:56:29Z,2020-03-26T00:40:00Z,
587405734,32996,REF: collect TimedeltaIndex.delete tests ,jbrockmendel,closed,2020-03-25T02:54:46Z,2020-03-26T00:40:29Z,organize the tests left in test_indexing; i made that file a while back and had some pretty inconsistent ideas about what belonged there
587901525,33019,REF: collect .align tests,jbrockmendel,closed,2020-03-25T18:26:30Z,2020-03-26T00:40:56Z,
587884064,33018,REF: CategoricalIndex indexing tests,jbrockmendel,closed,2020-03-25T17:59:22Z,2020-03-26T00:42:57Z,Current goal is to get the tests.indexes.foo.test_indexing files into alignment (and to actually hold tests for all/only indexing methods)
583140266,32778,CLN: .values -> ._values,jbrockmendel,closed,2020-03-17T16:29:33Z,2020-03-26T01:04:31Z,
586589400,32962,"TYP: ensure Block.putmask, Block.where get arrays, not Series/DataFrame",jbrockmendel,closed,2020-03-23T23:42:34Z,2020-03-26T01:07:01Z,"Avoids the `getattr(obj, ""values"", obj)` pattern (there's an issue for that)"
586545745,32955,CLN: Remove encoding specifier,ShaharNaveh,closed,2020-03-23T22:02:10Z,2020-03-26T01:10:28Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
583654406,32801,CLN/STY: pandas/_libs/internals.pyx,ShaharNaveh,closed,2020-03-18T11:17:06Z,2020-03-26T01:11:16Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
586583737,32961,CLN/STY: Nitpicks,ShaharNaveh,closed,2020-03-23T23:25:29Z,2020-03-26T01:13:40Z,Things I find myself doing in multiple branches
586608835,32964,TST: Use indices fixture in tests/indexes/test_setops.py,SaturnFromTitan,closed,2020-03-24T00:43:26Z,2020-03-26T01:15:28Z,"part of #30914
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
586541733,32954,Fixture `non_mapping_dict_subclass` is wrongly named,pbourguignon,closed,2020-03-23T21:53:39Z,2020-03-26T01:19:31Z,"#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

In `conftest.py`, a fixture is defined with the name `non_mapping_dict_subclass`, which returns a minimal class implementing the `collections.abc.Mapping` base class by minimally wrapping a `dict`.

Obviously, the intended name was `non_dict_mapping_subclass`, which also is the name of the class returned (casing apart).

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 1d13390e26bc7b1f5fb597bc5df1591f5d160e1f
python           : 3.6.10.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.3.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_GB.UTF-8
pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0.post20200309
Cython           : 0.29.15
pytest           : 5.4.1
hypothesis       : 5.7.0
sphinx           : 2.4.4
blosc            : 1.8.3
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.3.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.2.1
fastparquet      : 0.3.3
gcsfs            : None
lxml.etree       : 4.3.1
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.0
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.8
numba            : 0.48.0
</details>
"
585061912,32863,BUG: passing a non-dict mapping to pd.concat raises a TypeError,pbourguignon,closed,2020-03-20T13:15:16Z,2020-03-26T01:19:31Z,"#### Code Sample, a copy-pastable example if possible

```python
class MyMapping(Mapping):
    def __init__(self, content=None):
        if content:
            self._content = content
        else:
            self._content = {}
    def __getitem__(self, key):
        return self._content[key]
    def __iter__(self):
        return iter(self._content)
    def __len__(self):
        return len(self._content)   

objs = MyMapping({""a"": pd.DataFrame({""_0"": [0.], ""_1"": [0.]}), ""b"": pd.DataFrame({""_0"": [0.], ""_1"": [0.]})}
concat_objs = pd.concat(objs, axis=1)
```
#### Problem description

The above code raises a TypeError with the message ""cannot concatenate object of type '<class 'str'>'; only Series and DataFrame objs are valid"".

As the documentation explicitly specifies that the argument `objs` can be a mapping of Series or DataFrame objects, the expected behaviour is to just proceed.

#### Expected Output

The concatenated data frame.


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.3.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_GB.UTF-8
pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.0.0
Cython           : None
pytest           : 5.3.2
hypothesis       : None
sphinx           : 1.8.5
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.3.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.1
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.6.1
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
587250533,32985,CLN: Remove NameResolutionError,dsaxton,closed,2020-03-24T20:12:36Z,2020-03-26T01:25:03Z,Another small cleanup (this exception was removed in 0.14.0 according to release notes)
582658172,32764,BUG: Allow list-like in DatetimeIndex.searchsorted,dsaxton,closed,2020-03-16T23:34:20Z,2020-03-26T01:25:32Z,"- [x] closes #32762
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
588118039,33031,fixed #32904,gkaku,closed,2020-03-26T02:57:27Z,2020-03-26T02:59:57Z,"- [x] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
588155169,33033,fix #32904,gkaku,closed,2020-03-26T04:55:55Z,2020-03-26T05:22:34Z,"- [x] closes #32904 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
587750917,33016,"DOC: Document axis for swaplevel, standardize elsewhere",deepyaman,closed,2020-03-25T14:47:00Z,2020-03-26T06:43:25Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
525223059,29717,Improve return description in `droplevel` docstring,deepyaman,closed,2019-11-19T19:41:10Z,2020-03-26T06:44:04Z,"Use https://pandas-docs.github.io/pandas-docs-travis/reference/api/pandas.DataFrame.dropna.html#pandas.DataFrame.dropna
docstring as a\ template.

MINOR DOCUMENTATION CHANGE ONLY (ignored below)

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
586539317,32953,BUG: passing a non-dict mapping to pd.concat raises a TypeError,pbourguignon,closed,2020-03-23T21:48:29Z,2020-03-26T08:55:54Z,"closes #32954 
closes #32863 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
244702592,17048,read_json with lines=True not using buff/cache memory,louispotok,closed,2017-07-21T15:21:41Z,2020-03-26T12:36:39Z,"I have a 3.2 GB json file that I am trying to read into pandas using pd.read_json(lines=True). When I run that, I get a MemoryError, even though my system has >12GB of available memory. This is Pandas version 0.20.2.

I'm on Ubuntu, and the `free` command shows >12GB of ""Available"" memory, most of which is ""buff/cache"".

I'm able to read the file into a dataframe by iterating over the file like so:

```python
dfs = []
with open(fp, 'r') as f:
    while True:
        lines = list(itertools.islice(f, 1000))
        
        if lines:
            lines_str = ''.join(lines)
            dfs.append(pd.read_json(StringIO(lines_str), lines=True))
        else:
            break

df = pd.concat(dfs)

```
You'll notice that at the end of this I have the original data in memory **twice** (in the list and in the final df), but no problems.

It seems that `pd.read_json` with `lines=True` doesn't use the available memory, which looks to me like a bug.
"
469710225,27453,Right merge not preserve order,jesrael,closed,2019-07-18T11:12:41Z,2020-03-26T12:45:38Z,"Base by [SO question](https://stackoverflow.com/q/57092373/2901002):

Merge with right join not preserve order of columns:

    columns = ['A', 'B', 'C']
    data_1 = [[2, 5, 3, 5], [8, 2, 4, 1], [6, 5, 9, 1]]
    data_1 = np.array(data_1).T
    df_1 = pd.DataFrame(data=data_1, columns=columns)
    print (df_1)
       A  B  C
    0  2  8  6
    1  5  2  5
    2  3  4  9
    3  5  1  1
    
    columns = ['A', 'B', 'C']
    data_2 = [[2, 5, 3, 5], [7, 1, 3, 0], [np.nan, np.nan, np.nan, np.nan]]
    data_2 = np.array(data_2).T
    df_2 = pd.DataFrame(data=data_2, columns=columns)
    print (df_2)
         A    B   C
    0  2.0  7.0 NaN
    1  5.0  1.0 NaN
    2  3.0  3.0 NaN
    3  5.0  0.0 NaN
    
    #right join NOT preserve order
    df1 = df_1.merge(df_2[['A', 'B']], on=['A', 'B'], how='right')
    print (df1)
       A  B    C
    0  5  1  1.0
    1  2  7  NaN
    2  3  3  NaN
    3  5  0  NaN
    
Correct order for right join:
        
        A   B   C
    0   2   7   NaN
    1   5   1   1
    2   3   3   NaN
    3   5   0   NaN
    
    #left join wpreserve order correct
    df2 = df_1.merge(df_2[['A', 'B']], on=['A', 'B'], how='left')
    print (df2)
       A  B  C
    0  2  8  6
    1  5  2  5
    2  3  4  9
    3  5  1  1


----

    print (pd.show_versions())
    
    INSTALLED VERSIONS
    ------------------
    commit: None
    python: 3.7.3.final.0
    python-bits: 64
    OS: Windows
    OS-release: 7
    machine: AMD64
    processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
    byteorder: little
    LC_ALL: None
    LANG: en
    LOCALE: None.None
    
    pandas: 0.24.2
    pytest: 4.3.1
    pip: 19.0.3
    setuptools: 40.8.0
    Cython: 0.29.6
    numpy: 1.16.2
    scipy: 1.2.1
    pyarrow: None
    xarray: None
    IPython: 7.4.0
    sphinx: 1.8.5
    patsy: 0.5.1
    dateutil: 2.8.0
    pytz: 2018.9
    blosc: None
    bottleneck: 1.2.1
    tables: 3.5.1
    numexpr: 2.6.9
    feather: None
    matplotlib: 3.0.3
    openpyxl: 2.6.1
    xlrd: 1.2.0
    xlwt: 1.3.0
    xlsxwriter: 1.1.5
    lxml.etree: 4.3.2
    bs4: 4.7.1
    html5lib: 1.0.1
    sqlalchemy: 1.3.1
    pymysql: None
    psycopg2: None
    jinja2: 2.10
    s3fs: None
    fastparquet: None
    pandas_gbq: None
    pandas_datareader: None
    gcsfs: None
    None"
554700859,31278,BUG: 27453 right merge order,MarcoGorelli,closed,2020-01-24T12:08:57Z,2020-03-26T12:49:31Z,"Resurrection of #27762

I fixed the NameError in the test, and also changed the example given in the whatsnew entry (I couldn't see a difference between the outputs of 0.25.x and 1.0.0 in the original)

- [x] closes #27453
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
542709271,30501,BUG: Fixed strange behaviour of pd.DataFrame.drop() with inplace argu…,rjfs,closed,2019-12-27T01:27:01Z,2020-03-26T12:53:19Z,"closes  #30484

- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This is my first PR, so pardon me if I did something wrong.

To fix this issue I started by creating a failing test based on the example given by the reporter. After, I fixed the test by removing the ""inplace"" special methods from add_special_arithmetic_methods, following what was suggested as a comment in the code."
588171162,33034,"Bug: TypeError: ufunc 'sqrt' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",gkaku,closed,2020-03-26T05:42:44Z,2020-03-26T12:54:17Z,"- [x] closes #32904 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
588402801,33038,Why pandas doesn't support python 3.6.1?,finswimmer,closed,2020-03-26T12:59:41Z,2020-03-26T13:20:34Z,"Hello,

the requirements for pandas 1 explicit says, that it needs python>=3.6.1. Why is 3.6.0 not supported?

fin swimmer"
587871083,33017,Unspecific error message when setting singular index with np dtype,JoElfner,closed,2020-03-25T17:37:55Z,2020-03-26T13:24:10Z,"#### Code Sample, a copy-pastable example if possible

```python
some_np_array = np.array([43, 56])

# unspecific error message 'TypeError: len() of unsized object':
pd.DataFrame([4324, 345], index=some_np_array[0])
pd.Series([4324, 345], index=some_np_array[0])

# specific error message 'TypeError: Index(...) must be called with a collection of some kind, 5 was passed':
pd.DataFrame([4324, 345], index=5)
pd.Series([4324, 345], index=5)
```
#### Problem description

When passing a non-collection type as an index, pandas typically raises a quite helpful error message of the following form:
`TypeError: Index(...) must be called with a collection of some kind, 5 was passed`

Whereas when passing a non-standard type non-collection item as an index, pandas seems to raise some unspecific and misleading error:
`TypeError: len() of unsized object`

#### Expected Output
Specific error message, even when using np.float64, np.int64 etc. as type of singular values.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 7
machine          : AMD64
processor        : Intel64 Family 6 Model 58 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.1.post20200323
Cython           : 0.29.15
pytest           : 5.4.1
hypothesis       : 5.5.4
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.8
numba            : 0.48.0

</details>
"
546509379,30796,BLD/CI: Require Python 3.6.0,TomAugspurger,closed,2020-01-07T21:10:51Z,2020-03-26T13:29:57Z,Closes #30794 
585060311,32862,importing pandas resulting an error,ghost,closed,2020-03-20T13:12:29Z,2020-03-26T14:03:02Z,"import pandas as pd
import dask.distributed as dd

The above code was working fine till afternoon IST and now, it has started throwing errors.
Perhaps, the version?

AttributeError: module 'pandas._libs.missing' has no attribute 'NAType'

>>> df_train = dd.read_csv(filename_train, blocksize = ""60MB"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'dd' is not defined
>>> df_test = dd.read_csv(filename_test, blocksize = ""60MB"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'dd' is not defined

>>> df_train = df_train.drop(columns = ['INTERVAL_ID'], axis=1)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'df_train' is not defined

version
pandas=1.0.2=py37h0573a6f_0"
576904052,32490,"BUG: Fix bug, where BooleanDtype columns are converted to Int64",AnnaDaglis,closed,2020-03-06T12:34:41Z,2020-03-26T14:28:41Z,"- [x] closes #32287
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
587630902,33008,STY: Boolean values for bint variables,ShaharNaveh,closed,2020-03-25T11:31:42Z,2020-03-26T15:40:16Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565749306,32024,WEB: Add greeting note to CoC,MarcoGorelli,closed,2020-02-15T13:40:06Z,2020-03-26T16:13:32Z,"I noticed something similar in the [CPython contributing docs](https://cpython-core-tutorial.readthedocs.io/en/latest/diversity.html):

> For example, don’t say “hey guys!” but “hey everyone!”.

and thought it was really nice.

Pushing directly rather than opening as good first issue to avoid invoking a s*******m"
588631748,33048,Convenient way to use offset aliases to create a DateOffset?,yohplala,closed,2020-03-26T18:23:33Z,2020-03-26T18:45:46Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

alias_1H = '1H'
alias_1M = '1M'

offset_dict = {alias_1H: ('hours',1), alias_1M: ('months',1)}

def freq_to_offset(freq):
    custom_dict = {offset_dict[freq][0] : offset_dict[freq][1]}
    return custom_dict

offset_1H = pd.tseries.offsets.DateOffset(**freq_to_offset(alias_1H))

offset_1M = pd.tseries.offsets.DateOffset(**freq_to_offset(alias_1M))
```
#### Problem description

Hello,

I would like to be able to use the same type of offset aliases as defined [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases) for date_range() and period_range() function to create DateOffset.

The way I have found is the one copied/pasted here above, but I find the code not that straight.
Please, is there any better way to achieve that?

I understand the reversed is already possible: from a DateOffset, getting its offset alias.
I ask if the round way is possible?

Thanks for your help!
Bests,"
490043464,28306,Pandas unstack() unexpected behavior with multiindex row and column,fmmirzaei,closed,2019-09-05T22:43:48Z,2020-03-26T20:08:08Z,"#### Code Sample, a copy-pastable example if possible

```python
data = {
    ('effect_size', 'cohen_d', 'mean'): {
        ('m1', 'P3', '222'): 0.52,
        ('m1', 'A5', '111'): -0.07,
        ('m2', 'P3', '222'): -0.53,
        ('m2', 'A5', '111'): 0.05,
    },
    ('wilcoxon', 'z_score', 'stouffer'): {
        ('m1', 'P3', '222'): 2.2,
        ('m1', 'A5', '111'): -0.92,
        ('m2', 'P3', '222'): -2.0,
        ('m2', 'A5', '111'): -0.52,
    }
}
df = pd.DataFrame(data)
df.index.rename(['metric', 'bar', 'foo'], inplace=True)
df.unstack(['foo', 'bar'])
```
#### Problem description

The `df` looks like this before unstacking:
```
               effect_size wilcoxon
                   cohen_d  z_score
                      mean stouffer
metric bar foo                     
m1     A5  111       -0.07    -0.92
       P3  222        0.52     2.20
m2     A5  111        0.05    -0.52
       P3  222       -0.53    -2.00
```
by unstacking `bar` and `foo`, I had expected to see them as column indices, but that's not what happens. Instead `foo` and `metric` are unstacked, and `bar` is left stacked as a row index: 

```
> df.unstack(['foo', 'bar'])

       effect_size                   wilcoxon                
           cohen_d                    z_score                
              mean                   stouffer                
foo            111         222            111        222     
metric          m1    m2    m1    m2       m1    m2   m1   m2
bar                                                          
A5           -0.07  0.05   NaN   NaN    -0.92 -0.52  NaN  NaN
P3             NaN   NaN  0.52 -0.53      NaN   NaN  2.2 -2.0
```

I got around the problem by doing the following, but I think the above behavior might be a bug.

Here's my workaround:

```
> print df.stack([0, 1, 2]).unstack(0).transpose()

bar             A5                   P3         
foo            111                  222         
       effect_size wilcoxon effect_size wilcoxon
           cohen_d  z_score     cohen_d  z_score
              mean stouffer        mean stouffer
metric                                          
m1           -0.07    -0.92        0.52      2.2
m2            0.05    -0.52       -0.53     -2.0
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.15.final.0
python-bits: 64
OS: Linux
OS-release: 4.19.37-5+deb10u1rodete2-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: en_US.UTF-8
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.24.1
pytest: None
pip: None
setuptools: unknown
Cython: None
numpy: 1.16.4
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 2.0.0
sphinx: None
patsy: 0.4.1
dateutil: 2.8.0
pytz: 2019.2
blosc: None
bottleneck: None
tables: 3.5.2
numexpr: 2.6.10dev0
feather: None
matplotlib: 1.5.2
openpyxl: None
xlrd: 1.2.0
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: 0+unknown
pandas_datareader: None
gcsfs: None
</details>
"
398387517,24729,Multiple unstack in multi level columns DataFrame,faulaire,closed,2019-01-11T17:41:22Z,2020-03-26T20:08:08Z,"#### Code Sample, a copy-pastable example if possible

```python

import pandas as pd
from six import StringIO
stats = pd.DataFrame({('B', 'C'): 
                          {
                              (10, 20, 30): 0.0,
                              (10, 20, 40): 0.0
                          },
                      ('B', 'D'): 
                          {
                              (10, 20, 30): 0.0, 
                              (10, 20, 40): 0.0
                          }
                     })

stats.index.names = ['i1', 'i2', 'i3']
stats.columns.names = ['c1', 'c2']

# Expected true,  actual false
stats.unstack(['i2', 'i1']).columns.names[-2:] == ['i2', 'i1']

# In case of single level column, expected true, actual true
stats['B'].unstack(['i2', 'i1']).columns.names[-2:] == ['i2', 'i1']

```
#### Problem description

Trying to unstack multiindex multicolumns DataFrame, yield to an unexpected behavior. 
The unstacked levels are not those expected (see the code above). 

Note: In a single level column case, behavior is the expected one.

#### Expected Output

N/A

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.13.final.0
python-bits: 64
OS: Linux
OS-release: 2.6.32-642.6.2.el6.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: en_US.UTF-8
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.23.2
pytest: 3.2.3
pip: 9.0.1
setuptools: 36.3.0
Cython: 0.27.3
numpy: 1.13.3
scipy: 0.19.1
pyarrow: None
xarray: None
IPython: 5.1.0
sphinx: 1.4.4
patsy: None
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: 3.2.1
numexpr: 2.6.5
feather: None
matplotlib: 2.1.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.9999999
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.8
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
579261065,32624,BUG in _unstack_multiple,r7sa,closed,2020-03-11T13:23:40Z,2020-03-26T20:08:09Z,"#### Code Sample
Next code fail to do unstack.

```python
d = pd.DataFrame([[1,2,3],[2,2,3],[3,2,3],[4,2,3],[5,2,3]], 
                 columns=pd.MultiIndex.from_tuples([[0, 0, 0], [0, 0, 1], [0, 0, 2]], names=['c1', 'c2', 'c3']),
                 index=pd.MultiIndex.from_tuples([[0, 0, 0, 0, 0, 0, 0],
                                                  [0, 0, 1, 0, 0, 0, 1],
                                                  [0, 1, 0, 0, 0, 1, 0],
                                                  [0, 1, 1, 0, 0, 1, 1],
                                                  [1, 0, 0, 0, 1, 0, 0]
                                                  ], 
                                                 names=['i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7']))
e = d.unstack(['i2', 'i3', 'i4', 'i5', 'i6', 'i7'])
```
It rise exception IndexError with message ""Too many levels: Index has only 2 levels, not 3""

#### Problem description
The reason semms to mistyping in https://github.com/pandas-dev/pandas/blob/master/pandas/core/reshape/reshape.py#L366 :
```python
clocs = [v if i > v else v - 1 for v in clocs]
```
I think it must be:
```python
clocs = [v if val > v else v - 1 for v in clocs]
```"
588027148,33024,Using DateTimeIndex in rolling() to define time-based windows,yohplala,closed,2020-03-25T22:23:45Z,2020-03-26T20:51:34Z,"Hi,
I would like to be able to provide a DateTimeIndex directly to rolling() so that the window is defined as [timestamp in provided DateTimeIndex that is immediately previous to timestamp of current row ; timestamp of current row[
Please, is that possible?"
497347335,28585,DOC: do we want type hints in signature on API pages?,jorisvandenbossche,closed,2019-09-23T21:52:31Z,2020-03-26T20:55:22Z,"With the type hints that are being added in a lot of places, the question comes up if we want to have them in the docs.

Currently, this actually happens. See eg https://dev.pandas.io/docs/reference/api/pandas.DataFrame.to_string.html#pandas.DataFrame.to_string

![image](https://user-images.githubusercontent.com/1020496/65465611-080e7200-de5d-11e9-8866-a43de6bfc046.png)

But personally, I am not sure the having the type hints in the signature like above is an improvement. It makes the signature very hard to read and interpret, while we (in principle) already have the type information in textual form in the Parameter listing.


"
588003452,33022,Fiy Typo in datetimes (parced),phofl,closed,2020-03-25T21:30:55Z,2020-03-26T22:46:01Z,Fixed a typo in pandas/core/tools/datetimes (parced -> parsed)
587334801,32990,BUG: Multiple unstack using row index level labels and multi level columns DataFrame,phofl,closed,2020-03-24T23:12:52Z,2020-03-26T22:46:31Z,"- [x] closes #24729
- [x] closes #28306
- [x] closes #32624
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This should fix the issue with a long list of indices and unstack. The issue appeared with 6 indices for the first time.
"
588657651,33050,REF: RangeIndex tests,jbrockmendel,closed,2020-03-26T19:03:48Z,2020-03-26T23:35:12Z,
588608038,33045,BUG: frame.lookup with non-unique axes,jbrockmendel,closed,2020-03-26T17:47:50Z,2020-03-26T23:37:05Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

xref #33041, one more PR coming up to close that."
32479145,7001,Sum of grouped bool column has inconsistent type,jkleint,closed,2014-04-29T19:56:55Z,2020-03-26T23:46:19Z,"Summing a bool column after a groupby gives a bool result until there are two or more True values, when it becomes a float64.  Seems like it should always be an (unsigned?) integer.  Straight sum without a groupby always gives an int64. This is with 0.13.1.

```
pd.DataFrame([True]).groupby(lambda x: 0).sum()
      0
0  True

pd.DataFrame([True,True]).groupby(lambda x: 0).sum()
   0
0  2

pd.DataFrame([False]).groupby(lambda x: 0).sum()
       0
0  False

pd.DataFrame([False,False]).groupby(lambda x: 0).sum()
       0
0  False

pd.DataFrame([False,False,True]).groupby(lambda x: 0).sum()
      0
0  True

pd.DataFrame([False,False,True,True]).groupby(lambda x: 0).sum()
   0
0  2

pd.DataFrame([False,False]).sum()
0    0
dtype: int64
```
"
588544671,33040,REF: test_searchorted for PeriodIndex,jbrockmendel,closed,2020-03-26T16:17:01Z,2020-03-26T23:52:08Z,
565732787,32013,pandas.index.copy remove index name information when specifying new dtype,asepwhite,closed,2020-02-15T11:04:11Z,2020-03-27T00:00:35Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import pandas as pd
>>> named_idx = pd.Index([17.3, 69.221, 33.1, 15.5, 19.3, 74.8, 10, 5.5]).set_names('old_name')
>>> named_idx
Float64Index([17.3, 69.221, 33.1, 15.5, 19.3, 74.8, 10.0, 5.5], dtype='float64', name='old_name')
>>> named_idx.copy(name='new_name', dtype='int64')
Int64Index([17, 69, 33, 15, 19, 74, 10, 5], dtype='int64')
>>> named_idx.copy(dtype='int64')
Int64Index([17, 69, 33, 15, 19, 74, 10, 5], dtype='int64')

```
#### Problem description
When set new dtype parameter in function pandas.index.copy somehow it remove name information on new object

#### Expected Output
>>> import pandas as pd
>>> named_idx = pd.Index([17.3, 69.221, 33.1, 15.5, 19.3, 74.8, 10, 5.5]).set_names('old_name')
>>> named_idx
Float64Index([17.3, 69.221, 33.1, 15.5, 19.3, 74.8, 10.0, 5.5], dtype='float64', name='old_name')
>>> named_idx.copy(name='new_name', dtype='int64')
Int64Index([17, 69, 33, 15, 19, 74, 10, 5], dtype='int64', name='new_name')
>>> named_idx.copy(dtype='int64')
Int64Index([17, 69, 33, 15, 19, 74, 10, 5], dtype='int64', name='old_name')

#### Output of ``pd.show_versions()``

<details>
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}

{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
</details>
"
565843564,32036,BUG: Preserve name in Index.astype,dsaxton,closed,2020-02-16T03:15:52Z,2020-03-27T01:06:31Z,"- [x] closes #32013
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
587565024,33004,DOC: for pandas.read_json() using orient='table' add docu about schema spec,KaiRoesnerAtSAP,closed,2020-03-25T09:39:06Z,2020-03-27T06:39:53Z,"Please provide documentation about what kind of type specifications are supported when using the `pandas.read_json()` function with `orient` = 'table'. Currently the [function documentation](https://pandas-docs.github.io/pandas-docs-travis/reference/api/pandas.read_json.html#pandas.read_json) does not even mention 'table' under ""The set of possible orients is"". There is only a small example in the ""Encoding with Table Schema"" section."
588872375,33060,xlsx cannot open on windows 10,mar-heaven,closed,2020-03-27T04:07:15Z,2020-03-27T12:43:55Z,"#### pd.ExcelWriter  xlsx cannot open on windows 10
# Code
xlsx_writer = pd.ExcelWriter(os.path.join(output_path, ""report.xlsx""), engine='openpyxl')
    df = pd.DataFrame(data=[{""val"": val} for val in summary.values()], index=summary.keys()).sort_index()
    df.to_excel(xlsx_writer, sheet_name=""summary"")
xlsx_writer.save()
```
#### Problem description

I used pd.ExcelWriter to create a report.xlsx file and on Mac system it can be open. 
But when change to Windows it cannot be open and size of file is 0.

#### df data
                                         val
STOCK                                 100000
alpha                                    NaN
annualized_returns                     0.205
beta                                     NaN
cash                                 3845.03
downside_risk                          0.257
end_date                          2015-12-31
information_ratio                      0.676
max_drawdown                           0.444
run_type                            BACKTEST
sharpe                                 0.567
sortino                                0.054
start_date                        2014-01-02
strategy_file       examples/buy_and_hold.py
strategy_name                   buy_and_hold
total_returns                          0.437
total_value                           143696
tracking_error                         0.387
unit_net_value                         1.437
units                                 100000
volatility                             0.387

#### Expected Output
![image](https://user-images.githubusercontent.com/26325127/77720517-20856200-7023-11ea-9230-1042cc8e85ea.png)

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
pandas: 0.24.2
pytest: 5.4.1
pip: 20.0.2
setuptools: 46.1.1.post20200323
Cython: 0.29.15
numpy: 1.18.2
scipy: 1.4.1
pyarrow: None
xarray: None
IPython: 7.13.0
sphinx: None
patsy: None
dateutil: 2.8.1
pytz: 2019.3
blosc: None
bottleneck: None
tables: 3.6.1
numexpr: 2.7.1
feather: None
matplotlib: 3.2.1
openpyxl: 3.0.3
xlrd: None
xlwt: None
xlsxwriter: 1.2.8
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: 1.3.15
pymysql: 0.9.3
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
555785949,31359,BUG: regression when applying groupby aggregation on categorical columns,charlesdong1991,closed,2020-01-27T19:32:45Z,2020-03-27T13:32:46Z,"- [ ] closes #31256 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
575816077,32450,Converting a StringDtype series to an Inte64Dtype not working as expected,brent-field,closed,2020-03-04T22:01:12Z,2020-03-27T14:59:58Z,"I am interested in converting a StringDtype series to an Inte64Dtype. The following code produces a TypeError:

```
x = pd.Series(['1', pd.NA, '3'], dtype=pd.StringDtype())
x.astype('Int64')
...
TypeError: data type not understood
```


If I rewrite it as follows, I get a different TypeError:

```
x = pd.Series(['1', pd.NA, '3'], dtype=pd.StringDtype())
x.astype(int)
...
TypeError: int() argument must be a string, a bytes-like object or a number, not 'NAType
```


The only way I have been able to convert from StringDtype is:

```
x = pd.Series(['1', pd.NA, '3'], dtype=pd.StringDtype())
pd.to_numeric(x, errors='coerce').convert_dtypes()
...
0       1
1    <NA>
2       3
dtype: Int64
```

This works fine, but is inelegant. I would have expect astype to be able to do the conversion directly. Is there a recommended way to convert between these types?"
588969282,33062,BUG: implement astype from string dtype to nullable int dtype,jorisvandenbossche,closed,2020-03-27T08:27:37Z,2020-03-27T15:06:57Z,"Closes #32450

This implements a conversion from nullable string dtype to nullable int dtype."
588973297,33063,CLN: remove CategoricalBlock.to_native_types,jorisvandenbossche,closed,2020-03-27T08:35:25Z,2020-03-27T15:07:20Z,"The implementation of `to_native_types` on CategoricalBlock seems duplicated with the one of ExtensionBlock (the default for `na_rep` is different, but internally we always pass a value for that and never use the default)."
588080643,33028,CLN: de-duplicate Block.should_store and related,jbrockmendel,closed,2020-03-26T00:56:50Z,2020-03-27T15:23:33Z,"FooBlock.should_store is equivalent to `is_dtype_equal(self.dtype, other.dtype)` for almost all of our Block subclasses.  This makes that the implementation for the base class, so we define it in fewer places.  xref #32878 on remaining cases."
588085349,33030,Feature request:  series.fillna('mean'),bhishanpdl,closed,2020-03-26T01:12:55Z,2020-03-27T15:31:35Z,"I would request pandas devs to implement various nans filling methods.
For example:
- mean
- median
- max
- random forest regressor
so on.

Output:
`df['A'].fillna('mean')`"
588054214,33026,ERR: Raise a better error for numpy singletons in Index,dsaxton,closed,2020-03-25T23:32:57Z,2020-03-27T16:33:55Z,"- [x] closes #33017
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
588860184,33056,REF: avoid internals in tshift,jbrockmendel,closed,2020-03-27T03:26:39Z,2020-03-27T16:36:55Z,
588650592,33049,Comparing monthly to dayly frequencies: DateOffset or Timedelta?,yohplala,closed,2020-03-26T18:51:21Z,2020-03-28T05:53:21Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

offset_1M = pd.tseries.frequencies.to_offset('1MS')
offset_1D = pd.tseries.frequencies.to_offset('1D')

>>> offset_1M > offset_1D
Traceback (most recent call last):
  File ""/.local/lib/python3.7/site-packages/pandas/tseries/offsets.py"", line 2534, in f
    return op(self.delta, other.delta)
AttributeError: 'MonthBegin' object has no attribute 'delta'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.local/lib/python3.7/site-packages/pandas/tseries/offsets.py"", line 2538, in f
    f""Invalid comparison between {type(self).__name__} ""
TypeError: Invalid comparison between Day and MonthBegin
```
#### Problem description

Hi, I would like to compare 2 frequencies, among which monthly frequency.
Two tools appear relevant for that but have each an issue:
- Timedelta that supports comparison, but not Month frequency
- DateOffset that supports Month frequency, but not comparison

Please, how should be this managed?

Thanks for your help.
Have a good day!
Bests
"
589372910,33076,Pandas styling is not rendered in Github,ghost,closed,2020-03-27T19:37:40Z,2020-03-28T07:05:50Z,"**My pandas style for background in the local jupyter notebook looks like this** I am using version 1.0

![Screenshot from 2020-03-25 13-19-55](https://user-images.githubusercontent.com/59860027/77793524-3d0bb380-7090-11ea-9173-86002e37151f.png)

**But once i push the notebook in Github the rendering does not work it looks like this in github**

![Screenshot from 2020-03-25 13-20-40](https://user-images.githubusercontent.com/59860027/77793603-62002680-7090-11ea-9641-f36c97ed9c62.png)

Can u please solve this problem"
528451405,29853,PERF: implement scalar ops blockwise,jbrockmendel,closed,2019-11-26T02:41:19Z,2020-03-28T09:25:00Z,"Similar to #28583, but going through BlockManager.apply.
"
532431301,30032,"changed ""fun !r"" -> ""repr(fun)""",souravs17031999,closed,2019-12-04T03:46:10Z,2020-03-28T14:00:02Z,"As described in the following issue
, usage of !r is currently redundant and so changing to f strings in place of it.

- [x] ref https://github.com/pandas-dev/pandas/issues/29886
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
589575785,33097,i m not able to use this .append function even after the syntax is correct,PrasannajeetBajpai,closed,2020-03-28T13:17:07Z,2020-03-28T16:39:58Z,"import pandas as pd
import xlsxwriter



cars = {'Brand': ['Honda Civic','Toyota Corolla','Ford Focus','Audi A8'],
        'Price': [22000,25000,27000,35000]}

df = pd.DataFrame(cars, columns = ['Brand', 'Price'])

def add():
    ans = input (""Want to add more data.?(y/n):"")
    if (ans=='y'):
        car_name = input (""Enter name:"")
        price = input (""Price:"")
        df2 = pd.DataFrame({'Brand' : [car_name],
                            'Price' : [price]})
        df.append(df2, ignore_index = True) 
        
        add()
        
add()

df = pd.DataFrame(cars, columns = ['Brand', 'Price'])
#### Expected Output
Want to add more data.?(y/n):y

Enter name:Bugatti

Price:789456

Want to add more data.?(y/n):n
            Brand  Price
0     Honda Civic  22000
1  Toyota Corolla  25000
2      Ford Focus  27000
3         Audi A8  35000
4          Bugatti  789456


#### Output of ``pd.show_versions()``
Want to add more data.?(y/n):y

Enter name:Bugatti

Price:789456

Want to add more data.?(y/n):n
            Brand  Price
0     Honda Civic  22000
1  Toyota Corolla  25000
2      Ford Focus  27000
3         Audi A8  35000

<details>

here i m not able to use this .append function even after the syntax is correct can anyone plz help me out here i have sucessfully installed pandas also and i m running it in spyder

</details>
"
589597116,33101,"Add test for named period index, doing group by index",sumanau7,closed,2020-03-28T15:22:17Z,2020-03-28T17:25:48Z,"- [x] closes #32108
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
581276573,32704,DOC: Add examples to Series operators (#24589),sullivanbt,closed,2020-03-14T16:33:25Z,2020-03-28T20:12:11Z,"Further work on #24589.  PR #25524 added examples for many operations but not `pandas.Series.eq`, `pandas.Series.ne`, `pandas.Series.gt`, `pandas.Series.ge`, `pandas.series.le`, and `pandas.series.lt`.  This adds examples for those.

`pandas.Series.divmod` is still missing an example as discussed in #25524.
"
583404922,32799,CLN: remove DatetimeLikeArray._add_delta,jbrockmendel,closed,2020-03-18T01:47:10Z,2020-03-19T21:51:33Z,"Ultimately we want to 1) avoid using _from_sequence (so we can make that stricter in what it accepts) and 2) be more careful about when we use `freq=""infer""`.  Splitting up the cases handled by _add_delta will make this more feasible."
581247635,32701,BUG: Fix segfault on dir of a DataFrame with a unicode surrogate character in the column name,roberthdevries,closed,2020-03-14T15:13:37Z,2020-03-19T22:02:36Z,"Return a `repr()` version if the column name string is not printable. This also means the the column name is not present in the output of `dir()`

- [x] closes #25509 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
557498635,31451,DOC: Fixed example section in pandas/core/dtypes/*.py,ShaharNaveh,closed,2020-01-30T13:36:49Z,2020-03-19T23:29:34Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
583369314,32794,BLD: Suppressing errors while compling pandas/_libs/groupby,ShaharNaveh,closed,2020-03-17T23:52:40Z,2020-03-20T00:05:35Z,"- [x] ref #32163
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This are the errors that this PR is getting rid of:

```
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_116group_last’:
pandas/_libs/groupby.c:37458:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
37458 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_118group_last’:
pandas/_libs/groupby.c:38233:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
38233 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_120group_last’:
pandas/_libs/groupby.c:39008:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
39008 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_122group_last’:
pandas/_libs/groupby.c:39781:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
39781 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_124group_last’:
pandas/_libs/groupby.c:40563:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
40563 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_128group_nth’:
pandas/_libs/groupby.c:41999:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
41999 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_130group_nth’:
pandas/_libs/groupby.c:42820:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
42820 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_132group_nth’:
pandas/_libs/groupby.c:43641:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
43641 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_134group_nth’:
pandas/_libs/groupby.c:44460:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
44460 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_136group_nth’:
pandas/_libs/groupby.c:45288:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
45288 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_152group_max’:
pandas/_libs/groupby.c:55168:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
55168 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_154group_max’:
pandas/_libs/groupby.c:55970:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
55970 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_156group_max’:
pandas/_libs/groupby.c:56772:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
56772 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_158group_max’:
pandas/_libs/groupby.c:57568:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
57568 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_162group_min’:
pandas/_libs/groupby.c:58985:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
58985 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_164group_min’:
pandas/_libs/groupby.c:59784:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
59784 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_166group_min’:
pandas/_libs/groupby.c:60583:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
60583 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
      |                              ^~
pandas/_libs/groupby.c: In function ‘__pyx_pf_6pandas_5_libs_7groupby_168group_min’:
pandas/_libs/groupby.c:61376:30: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
61376 |   __pyx_t_3 = ((!((__pyx_t_2 == __pyx_t_1) != 0)) != 0);
```
"
573303383,32369,CLN: Some code cleanups in pandas/_libs/parsers.pyx,ShaharNaveh,closed,2020-02-29T14:20:09Z,2020-03-20T00:06:43Z,"There are __a lot__ of cdef unused variables in ```pandas/_libs/parsers.pyx``` this PR is covering *some* of the unused variables.
"
584636378,32833,DOC: FutureWarning in Sphinx build when calling read_parquet,ShaharNaveh,closed,2020-03-19T19:09:43Z,2020-03-20T00:40:23Z,"- [x] closes #32832
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
577278283,32512,[BUG] add consistency to_numeric on empty list,mikekutzma,closed,2020-03-07T04:52:51Z,2020-03-20T02:04:17Z,"to_numeric should work similarly on empty lists for
downcast=unsigned/float/integer

Addresses: GH32493

- [x] closes #32493
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
576651061,32478,Made apt changes to pandas.Series.str.replace(),ashwinpn,closed,2020-03-06T02:14:16Z,2020-03-20T03:20:23Z,"- [x] closes #31225 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
574784351,32412,DEPR: _ndarray_values vs to_numpy vs __array__,jbrockmendel,closed,2020-03-03T16:21:56Z,2020-03-20T03:51:43Z,Is there a clear explanation for when to use one of these instead of another?
584774001,32847,Backport PR #32833 on branch 1.0.x (DOC: FutureWarning in Sphinx build when calling read_parquet),meeseeksmachine,closed,2020-03-20T00:22:58Z,2020-03-20T07:07:16Z,Backport PR #32833: DOC: FutureWarning in Sphinx build when calling read_parquet
583315849,32789,TST: Define sections in pandas/conftest.py,SaturnFromTitan,closed,2020-03-17T21:41:09Z,2020-03-20T07:48:21Z,"part of #31989
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
584428004,32826,PERF: skip non-consolidatable blocks when checking consolidation,jorisvandenbossche,closed,2020-03-19T13:51:36Z,2020-03-20T10:30:19Z,"This skips the non-consolidatable blocks to determine if the blocks are consolidated. So meaning that if you have multiple EA columns with the same dtype, we do not do an unnecessary consolidation.

From investigating https://github.com/pandas-dev/pandas/issues/32196#issuecomment-600824238

@rth this should give another speed-up to your benchmark"
584623553,32832,DOC: CI failure due to fsspec deprecation warning,ShaharNaveh,closed,2020-03-19T18:46:12Z,2020-03-20T11:55:08Z,"There is CI failure in the ```Web/Docs```, under ```check ipython directive errors``` on github actions, noticed on #32831 and on #32830 

The error that is being shown:

```
Run ! grep -B1 ""^<<<-------------------------------------------------------------------------$"" sphinx.log
  FutureWarning,
<<<-------------------------------------------------------------------------
##[error]Process completed with exit code 1.
```

---

The root cause of the problem, is a warning that is generated during the creation of the docs.

```
>>>-------------------------------------------------------------------------
Warning in /home/runner/work/pandas/pandas/doc/source/user_guide/scale.rst at block ending on line 254
 Specify :okwarning: as an option in the ipython:: block to suppress this message
 ----------------------------------------------------------------------------
 /home/runner/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/fsspec/implementations/local.py:33: FutureWarning: The default value of auto_mkdir=True has been deprecated and will be changed to auto_mkdir=False by default in a future release.
   FutureWarning,
<<<-------------------------------------------------------------------------
```

"
36123939,7517,Unable to import pandas,phani-vadrevu,closed,2014-06-19T22:18:08Z,2020-03-20T13:36:38Z,"I've installed pandas using pip. I got the following versions of pip and numpy:

```
pvadrevu@MacPro~$ sudo pip freeze|grep pandas
pandas==0.14.0
pvadrevu@MacPro~$ sudo pip freeze|grep numpy
numpy==1.8.1
```

I am using this on OSX 10.9.2. I get the following error when trying to import pandas

```
numpy.dtype has the wrong size, try recompiling
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Python/2.7/site-packages/pandas/__init__.py"", line 6, in <module>
    from . import hashtable, tslib, lib
  File ""numpy.pxd"", line 157, in init pandas.hashtable (pandas/hashtable.c:22331)
ValueError: numpy.dtype has the wrong size, try recompiling
```
"
584149091,32817,TST: bare pytest raises,quangngd,closed,2020-03-19T04:11:32Z,2020-03-20T14:18:28Z,"GH30999

- [ ] ref #30999
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
585122533,32868,Update 03_subset_data.rst,rogererens,closed,2020-03-20T14:53:32Z,2020-03-20T15:56:46Z,"IIUC, type() will report pandas.core.series.Series to me

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
577406261,32530,CI: Update web and docs to OVH with the right structure,datapythonista,closed,2020-03-07T21:53:36Z,2020-03-20T16:36:14Z,"- [X] closes #32303

I'm uploading all the docs from the NumFOCUS server to the OVH (manually). And I'm changing the CI in this PR to upload the website and the dev docs to the server automatically.

I'll have a look and see if we can upload a new version of the docs automatically to OVH when a new GitHub release is generated. So, we never need to upload the website manually.

After merging this PR, and when we're confident this is working well, we'll have to redirect pandas.pydata.org to the OVH server. The dev docs will be available at pandas.pydata.org/pandas-docs/dev/ (and we can stop using pandas.io and dev.pandas.io)."
585235643,32872,Modification of validate_rst_title_capitalization.py script,cleconte987,closed,2020-03-20T17:55:24Z,2020-03-20T18:00:07Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
584952722,32858,PERF: allow to skip validation/sanitization in DataFrame._from_arrays,jorisvandenbossche,closed,2020-03-20T09:48:39Z,2020-03-20T20:06:07Z,"For cases where you know to have valid data (eg you just created them yourself, or they are already validated), it can be useful to skip the validation checks when creating a DataFrame from arrays.

Use case is for example https://github.com/pandas-dev/pandas/pull/32825

From investigating https://github.com/pandas-dev/pandas/issues/32196#issuecomment-600824238

@rth this gives another 20% improvement on the dataframe creation part. Together with https://github.com/pandas-dev/pandas/pull/32856, it gives a bit more than a 2x improvement on the dataframe creation part (once the sparse arrays are created)"
584808659,32850,BUG: is_scalar_indexer,jbrockmendel,closed,2020-03-20T02:43:02Z,2020-03-20T20:40:44Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

ATM we go through the wrong path when setting a listlike entry in a single position.

The broader goal: separate out the cases where setitem is inplace vs copying."
583299885,32787,replaced one occurrence of `Appender` by `doc` decorator,smartvinnetou,closed,2020-03-17T21:08:17Z,2020-03-20T22:21:11Z,"- [X] xref #31942
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] replaces one use of `Appender` in pandas/core/generic.py with `doc`
"
565735495,32017,Fix docstring validation errors in pandas.Index.slice_indexer,giovanism,closed,2020-02-15T11:28:33Z,2020-03-20T22:26:23Z,"- [ ] closes https://github.com/pandanistas/pandanistas_sprint_jakarta2020/issues/6
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
580325788,32672,Avoid bare pytest.raises in dtypes/test_dtypes.py,Vlek,closed,2020-03-13T02:10:55Z,2020-03-21T00:41:31Z,"* [x]  ref #30999
 
* [x]  tests added / passed
 
* [x]  passes `black pandas`

* [x]  passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
578942344,32603,Avoid bare pytest.raises in dtypes/cast/test_upcast.py,Vlek,closed,2020-03-11T00:49:57Z,2020-03-21T00:41:45Z,"* [x]  ref #30999
 
* [x]  tests added / passed
 
* [x]  passes `black pandas`

* [x]  passes `git diff origin/master -u -- ""*.py"" | flake8 --diff`"
584835951,32853,pandas transforms a datetime column.,allwell997,closed,2020-03-20T04:23:09Z,2020-03-21T03:24:53Z,"There is a csv file with a column in string datetime. 
1.  I want to load this csv file with pandas and set the string datetime to index, in the mean time, I want to change the datetime year-month-day.  for example as the below. 
5:06:12
6:08:23
21:09:59
==================================
I want the right result as below
2017-01-01 5:06:12
2017-01-01 6:08:23
2017-01-01 21:09:59
"
585153597,32869,"Incorrect calculations of corrwith when ""other"" is the same table shifted on one row. ",YuriyTigiev,closed,2020-03-20T15:39:55Z,2020-03-21T03:36:04Z,"#### Code Sample, a copy-pastable example if possible

```python

import pandas as pd 

pd.show_versions()

df = pd.DataFrame([[1.0,20.0,3.0], [30.0,26.0,0.0], [32.0,0.0,86.0], [17.0, 3.0, 9.0]])

df1 = df.iloc[1:]

df2 = pd.DataFrame([[30.0,26.0,0.0], [32.0,0.0,86.0], [17.0, 3.0, 9.0]])

print(""\n*** df ***"")
print(df)
print(""\n*** df1 ***"")
print(df1)
print(""\n*** df2 ***"")
print(df2)

print(""\ndf.corr(df1)"")
print(df.corrwith(df1, axis=1, drop = True))

print(""\ndf.corr(df2)"")
print(df.corrwith(df2, axis=1, drop = True))

```
#### Problem description

The method corrwith returns incorrect values (1) when ""other"" is the same table shifted on one row. 
  
```
df.corr(df1)
1    1.0
2    1.0
3    1.0
```

#### Expected Output
```
df.corr(df2)
0    0.299889
1   -0.877555
2    0.290426
```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.1
numpy            : 1.16.5
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.2.1
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.1
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.9
tables           : 3.5.2
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.1

</details>
"
581748027,32728,CLN: Remove unncessary lambda wrapper,skasturi,closed,2020-03-15T17:06:15Z,2020-03-21T07:46:48Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

A lambda wrapper is unnecessary in these cases as the underlying methods are themselves callable. Moreover, the code does not add any logic in the wrapper to warrant the usage of lambda. "
582832402,32772,CLN: simplify MultiIndex._shallow_copy,topper-123,closed,2020-03-17T08:12:32Z,2020-03-21T11:45:21Z,Minor simplification of ``MultiIndex._shallow_copy``.
583047395,32775,TYP: PandasObject._cache,topper-123,closed,2020-03-17T14:19:12Z,2020-03-21T11:45:31Z,"Move ``_cache: Dict[str, Any]`` fragment to PandasObject, where it belongs + related changes."
584780830,32848,Backport PR #32840 on branch 1.0.x (DOC: use new pydata-sphinx-theme name),ShaharNaveh,closed,2020-03-20T00:48:31Z,2020-03-21T14:41:49Z,Manual Backport of #32840
584595209,32830,STY: Correct whitespace placement,ShaharNaveh,closed,2020-03-19T17:57:06Z,2020-03-21T15:17:51Z,"- [x] ref #30755
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
585520073,32887,TST: Using ABCMultiIndex in isinstance checks,SaturnFromTitan,closed,2020-03-21T15:23:34Z,2020-03-21T15:33:39Z,"A follow-up on [#32483 (comment)](https://github.com/pandas-dev/pandas/pull/32483#discussion_r388795102) by @MomIsBestFriend 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
584105945,32816,TST: Avoid bare pytest.raises in multiple files,Vlek,closed,2020-03-19T01:44:27Z,2020-03-21T17:23:32Z,"* [x]  ref #30999
 
* [x]  tests added / passed
 
* [x]  passes `black pandas`

* [x]  passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
172067750,14042,"numpy.split on non-UTC, tz-aware data undergoes UTC roundtrip",isacarnekvist,closed,2016-08-19T07:04:29Z,2020-03-21T20:19:05Z,"#### Code Sample, a copy-pastable example if possible

```
import pandas as pd
import numpy as np
indices = pd.date_range('2016-01-01 00:00:00+0200', freq='S', periods=10)
np.split(indices, indices_or_sections=[])
Out:
[DatetimeIndex(['2015-12-31 20:00:00+02:00', '2015-12-31 20:00:01+02:00',
               '2015-12-31 20:00:02+02:00', '2015-12-31 20:00:03+02:00',
               '2015-12-31 20:00:04+02:00', '2015-12-31 20:00:05+02:00',
               '2015-12-31 20:00:06+02:00', '2015-12-31 20:00:07+02:00',
               '2015-12-31 20:00:08+02:00', '2015-12-31 20:00:09+02:00'],
              dtype='datetime64[ns, pytz.FixedOffset(120)]', freq='S')]
```
#### Expected Output

```
[DatetimeIndex(['2016-01-01 00:00:00+02:00', '2016-01-01 00:00:01+02:00',
               '2016-01-01 00:00:02+02:00', '2016-01-01 00:00:03+02:00',
               '2016-01-01 00:00:04+02:00', '2016-01-01 00:00:05+02:00',
               '2016-01-01 00:00:06+02:00', '2016-01-01 00:00:07+02:00',
               '2016-01-01 00:00:08+02:00', '2016-01-01 00:00:09+02:00'],
              dtype='datetime64[ns, pytz.FixedOffset(120)]', freq='S')]
```
#### output of `pd.show_versions()`

```
In [16]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.6.final.0
python-bits: 64
OS: Linux
OS-release: 4.2.0-38-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.18.1
nose: 1.3.7
pip: 8.1.2
setuptools: 25.2.0
Cython: 0.24.1
numpy: 1.11.1
scipy: 0.18.0
statsmodels: None
xarray: None
IPython: 1.2.1
sphinx: None
patsy: None
dateutil: 2.5.3
pytz: 2016.6.1
blosc: None
bottleneck: None
tables: 3.1.1
numexpr: 2.6.1
matplotlib: 1.3.1
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.999
httplib2: 0.8
apiclient: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
boto: 2.42.0
pandas_datareader: None
```
"
585097351,32866,TST: add test for non UTC datetime split #14042,JDkuba,closed,2020-03-20T14:13:52Z,2020-03-21T20:19:10Z,"- [x] closes #14042
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
585500244,32883,PERF/REF: MultiIndex.copy,topper-123,closed,2020-03-21T13:33:34Z,2020-03-21T20:49:05Z,"Makes ``MultiIndex.copy`` call ``MultiIndex._shallow_copy`` rather than the other way around. This is cleaner and let's us copy the existing  ``.cache``, so may give performance boost when operating on copied MultiIndexes:

```python
>>> n = 100_000
>>> df = pd.DataFrame({'a': range(n), 'b': range(1, n+1)})
>>> mi = pd.MultiIndex.from_frame(df)
>>> mi.get_loc(mi[0])  # also sets up the cache
>>> %timeit mi.copy().get_loc(mi[0])
8.57 ms ± 157 µs per loop  # master
57.9 µs ± 798 ns per loop  # this PR
```

Also cleans kwargs from the ``MultiIndex._shallow_copy`` signature. This PR is somewhat related to #32669.
"
585551628,32891,CLN: unnecessary ABCClasses,jbrockmendel,closed,2020-03-21T18:19:01Z,2020-03-21T21:02:28Z,"
"
585232583,32871,REF: misplaced Timedelta tests,jbrockmendel,closed,2020-03-20T17:49:25Z,2020-03-21T21:02:57Z,some parametrization along the way
585363875,32876,TST: misplaced Series.get test,jbrockmendel,closed,2020-03-20T22:44:11Z,2020-03-21T21:03:30Z,small unrelated cleanup in test_timeseries
584933875,32856,PERF: faster placement creating extension blocks from arrays,jorisvandenbossche,closed,2020-03-20T09:13:09Z,2020-03-21T21:05:19Z,"When creating a DataFrame from many arrays stored in ExtensionBlocks, it seems quite some time is taken inside BlockPlacement using `np.require` on the passed list. Specifying the placement as a slice instead gives a much faster creation of the BlockPlacement. This delays the conversion to an array, though, but afterwards the conversion of the slice to an array inside BlockPlacement when neeeded is faster than an initial creation of a BlockPlacement from a list/array of 1 element.

From investigating https://github.com/pandas-dev/pandas/issues/32196#issuecomment-600824238

@rth this reduces it with another third! (only from the dataframe creation, to be clear)"
584815983,32852,CLN: avoid .setitem in tests,jbrockmendel,closed,2020-03-20T03:14:09Z,2020-03-21T21:05:19Z,"Make a few tests marginally clearer, avoid some clutter when grepping for `.getitem`"
583229230,32781,CLN: .value -> ._values outside of core/,jbrockmendel,closed,2020-03-17T18:56:15Z,2020-03-21T21:10:05Z,pandas.plotting I left alone
583195231,32780,CLN: Split up Boolean array tests,dsaxton,closed,2020-03-17T17:57:07Z,2020-03-21T21:14:27Z,"Apologies for the size of this PR, but it looks like the ExtensionArray tests sometimes have their own directories by type, and other times all tests for the type are in a single file (e.g., https://github.com/pandas-dev/pandas/tree/master/pandas/tests/arrays/categorical vs. https://github.com/pandas-dev/pandas/blob/master/pandas/tests/arrays/test_boolean.py). I think the ""whole directory for type"" structure makes it easier to find and add new tests, so I'm doing that here for the Boolean tests. There shouldn't be any changes other than moving things around."
584758985,32846,REF: pass align_keys to BlockManager.apply,jbrockmendel,closed,2020-03-19T23:33:08Z,2020-03-21T21:15:37Z,The upcoming branch that implements the last frame-with-series arithmetic ops block-wise is going to need this
585531624,32889,TST: organize tests in test_timeseries,jbrockmendel,closed,2020-03-21T16:26:33Z,2020-03-21T21:19:49Z,
585566675,32895,CLN: Fix common spelling mistakes,SaturnFromTitan,closed,2020-03-21T19:43:03Z,2020-03-21T23:00:05Z,"I applied [this awesome automatic spell fixer](https://github.com/vlajos/misspell-fixer) to our codebase. I needed to do a manual review as it was attempting to fix some names, but the false-positive rate was pretty low!

They also provide a GitHub action for automatic checks on PRs. Might be helpful, especially for the `docs`.

If this is merged, I can create a follow-up issue for a discussion on the CI hook.
"
585517105,32886,TST: Replace tm.all_index_generator with indices fixture,SaturnFromTitan,closed,2020-03-21T15:06:29Z,2020-03-22T00:12:11Z,"Inspired by #30999
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
583322940,32790,ERR: Raise on invalid na_action in Series.map,dsaxton,closed,2020-03-17T21:56:43Z,2020-03-22T00:14:03Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

`Series.map` treats any `na_action` other than `""ignore""` like `None`, but it should probably raise if someone provides an invalid option:

```python
In [1]: import pandas as pd                                                                                                                                

In [2]: pd.Series([1, 2, 3]).map(lambda x: x, na_action=""xxxxx"")                                                                                           
Out[2]: 
0    1
1    2
2    3
dtype: int64
```"
585557071,32892,"REF: simplify should_extension_dispatch, remove dispatch_to_extension_op",jbrockmendel,closed,2020-03-21T18:50:01Z,2020-03-22T00:37:14Z,"dispatch_to_extension_op was needed back before we got rid of integer-addition for DTA/TDA/Timestamp/Timedelta.  Now it reduces to a one-liner, which this inlines.

This also consolidates all of the casting/checks up-front, which in turn avoids a runtime import and simplifies the check for should_extension_dispatch."
585333839,32875,CLN: Period tests,jbrockmendel,closed,2020-03-20T21:21:23Z,2020-03-22T00:41:08Z,"Pretty much re-wrote the comparison tests to match the patterns we use elsewhere, avoided bare pytest.raises.

A bunch of tests use `Period(""NaT"", freq)` which ends up being very duplicative.  Cut that down a bit and marked a couple tests that belong elsewhere, will move in a separate pass."
583326524,32791,"CLN: remove align kwarg from Block.where, Block.putmask",jbrockmendel,closed,2020-03-17T22:04:40Z,2020-03-22T00:49:39Z,
585588246,32898,TST: test that the .copy method on indexes copy the cache,topper-123,closed,2020-03-21T21:26:36Z,2020-03-22T01:15:15Z,"In #32883 I made a PR to fix an issue that ``MultiIndex.copy`` didn't copy the ``_cache``.

This adds a test for this for all index types (there's no futher issue, but it should still be tested for)."
585416013,32879,TST: Avoid bare pytest.raises in test_series.py,Vlek,closed,2020-03-21T02:55:48Z,2020-03-22T01:57:40Z,"* [x]  ref #30999
 
* [x]  tests added / passed
 
* [x]  passes `black pandas`

* [x]  passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`


Please note that this file had an error message that was pretty ugly, so I changed
it to only include the class name as I have done for other tests."
580401450,32675,FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.   import pandas.util.testing as tm,KillerStrike17,closed,2020-03-13T06:36:39Z,2020-03-22T02:19:28Z,"#### Problem description

Why am i getting this warning? i am just creating a dataframe using DataFrame function

#### Expected Output

#### Output of ``pd.show_versions()``

version is 1.0.1

"
585641548,32900,DOC: Fix EX02 in pandas.Series.factorize,farhanreynaldo,closed,2020-03-22T04:14:41Z,2020-03-22T04:42:15Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. 

output of `python scripts/validate_docstrings.py pandas.Series.factorize`:
```
################################################################################
################################## Validation ##################################
################################################################################
```

"
565636035,31989,pandas/conftest.py refactoring,SaturnFromTitan,closed,2020-02-15T01:21:03Z,2020-03-22T15:30:41Z,"As stated in [this review comment](https://github.com/pandas-dev/pandas/pull/31701#pullrequestreview-359291503) from jreback, we should address some refactoring issues in `pandas/conftest.py`:
- [x] create the indices fixture using `tm.all_index_generator` as suggested [by jbrockmendel in this comment](https://github.com/pandas-dev/pandas/pull/31701#discussion_r378532760)
- [x] all fixtures and functions should have a docstring
- [x] add sections, i.e. ""some text markers so reading the file is easy""
- [ ] remove/avoid imports from `conftest.py`"
569451244,32196,DataFrame.sparse.from_spmatrix seems inefficient with large (but very sparse) matrices?,fedarko,closed,2020-02-23T07:26:48Z,2020-03-22T20:33:47Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
from scipy.sparse import csr_matrix
mil = 1000000
big_csr_diag_1s = csr_matrix((mil, mil), dtype=""float"")
# Following line takes around 15 seconds to run
big_csr_diag_1s.setdiag(1)
# At this point, big_csr_diag_1s is just a completely-sparse matrix with the only
# nonzero values being values of 1 on its diagonal (and there are 1 million of
# these values; I don't think this should be *too* bad to store in a sparse data
# structure).
# The following line runs for at least 5 minutes (I killed it after that point):
pd.DataFrame.sparse.from_spmatrix(big_csr_diag_1s)
```
#### Problem description

It seems like the [scipy csr matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) is being converted to dense somewhere in `pd.DataFrame.sparse.from_spmatrix()`, which results in that function taking a large amount of time (on my laptop, at least).

I _think_ this seems indicative of an efficiency problem, but if constructing the sparse DataFrame in this way really is expected to take a huge amount of time then I can close this issue. Thanks!

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-76-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : 0.29.15
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
584361272,32825,PERF: optimize DataFrame.sparse.from_spmatrix performance,rth,closed,2020-03-19T11:59:13Z,2020-03-22T20:33:51Z,"This optimizes `DataFrame.sparse.from_spmatrix` performance, using an approach proposed by @jorisvandenbossche in https://github.com/pandas-dev/pandas/issues/32196#issuecomment-600824238

Bulds on top of https://github.com/pandas-dev/pandas/pull/32821 and adds another ~4.5x speed up in addition to that PR. Benchmarks for run time (in seconds) are done by running `pd.DataFrame.sparse.from_spmatrix` on a random sparse CSR array of given n_samples, n_features with a density=0.01:
```
label                     PR   master Speed-up master/PR                                                                               
n_samples n_features                                    
100       100000      2.3624  10.1247              4.29x
10000     10000       0.2391   1.1037              4.62x
100000    100         0.0031   0.0134              4.32x
```

with the benchmarking code below,
<details>

```py
import pandas as pd
import numpy as np
import scipy.sparse

from neurtu import timeit, delayed


def bench_cases():
    for n_samples, n_features in [(100, 100000), (10000, 10000), (100000, 100)]:
        X = scipy.sparse.rand(
            n_samples, n_features, random_state=0, density=0.01, format=""csr""
        )
        tags = {""n_samples"": n_samples, ""n_features"": n_features, ""label"": ""PR""}

        yield delayed(pd.DataFrame.sparse.from_spmatrix, tags=tags.copy())(X)


res = timeit(bench_cases())

res = (
    res.reset_index()
    .set_index([""n_samples"", ""n_features"", ""label""])[""wall_time""]
    .unstack(-1)[[""PR""]]
    .round(4)
)
# res[""master""] = np.array([10.1247, 1.1037, 0.0134])

# res[""Speed-up master/PR""] = (res[""master""] / res[""PR""]).round(2).astype(""str"") + ""x""
print(res)
```

</details>

Closes https://github.com/pandas-dev/pandas/issues/32196 although further optimization might be possible. Around 90% of remaining run time happens in `DataFrame._from_arrays` which goes deeper into pandas internals. Maybe some checks could be disabled there, but that looks less straightforward."
582456884,32755,Request: Better error message for missing columns in aggregate,tsoernes,closed,2020-03-16T16:56:38Z,2020-03-22T20:34:44Z,"When attempting to aggregate a non-existing column, this error is raised, which is not very descriptive: `SpecificationError: nested renamer is not supported`
```
In [39]: pd.DataFrame([[1, 2]], columns=['A', 'B']).groupby('A').agg({'B': 'mean'})
Out[39]: 
   B
A   
1  2

In [40]: pd.DataFrame([[1, 2]], columns=['A', 'B']).groupby('A').agg({'B': 'mean', 'non-existing': 'mean'})
Out[40]:
...
SpecificationError: nested renamer is not supported
```"
557183855,31435,ENH: Added index to testing assert message when series values differ,amilbourne,closed,2020-01-29T23:44:41Z,2020-03-22T20:42:42Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This implements the suggestion made in #31190 
In short, where the values in 2 series differ, the assert exception message includes the index as well as the values.  This helps when using assert_frame_equal with check_like=True, since the index may be reordered, making the output hard to understand."
553215931,31190,ENH: assert_frame_equal can produce unhelpful output,amilbourne,closed,2020-01-21T23:43:55Z,2020-03-22T20:43:15Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

df1 = pd.DataFrame({'a': [1, 2, 3], 'b': [10, 20, 30]}, index=[0, 1, 2])
df2 = pd.DataFrame({'a': [2, 1, 3], 'b': [20, 999, 30]}, index=[1, 0, 2])

print(df1)
print(df2)

pd.testing.assert_frame_equal(df1, df2, check_like=True)
```
Generates the output:
```
   a   b
0  1  10
1  2  20
2  3  30
   a    b
1  2   20
0  1  999
2  3   30
Traceback (most recent call last):
 <removed stack trace>
    raise AssertionError(msg)
AssertionError: DataFrame.iloc[:, 1] are different

DataFrame.iloc[:, 1] values are different (33.33333 %)
[left]:  [20, 10, 30]
[right]: [20, 999, 30]
```

#### Problem description

When running assert_frame_equal with check_like=True the error message can be unhelpful if the column or index order differs (or worse, both).

#29218 has greatly helped the issue with column reordering, although it could be insufficient if check_names is false and the column names differ (but this is probably a small edge case).

However, row (index) reordering is still an issue.  As the output above demonstrates, it is not obvious that the left column has been reordered.  In most cases the user could probably work it out but it is not as clear as it could be.

#### Possible Solution

One solution would be to explicitly state the index values in the output.  This would probably have to affect the output of assert_series_equal as well.  This would be quite a significant change to the output though so it may be a sledgehammer to crack a nut.
```
DataFrame.iloc[:, 1] values are different (33.33333 %)
[index]: [1, 0, 2]
[left]:  [20, 10, 30]
[right]: [20, 999, 30]
```
A side effect of including the index would be that you could avoid having to display the entire sequence for low numbers of differences in long sequences - although I'm not suggesting that be part of this change.

Is this worth doing and is the above a sensible solution?

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.2
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.6.0.post20191030
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
585385623,32877,REF: collect to_xarray tests,jbrockmendel,closed,2020-03-21T00:01:58Z,2020-03-22T20:58:28Z,"I'm split on where to put test_to_xarray.  Since its testing one method, tests/generic/methods/ makes sense.  BUT we could try to reserve that directory for tests that are nicely parametrized over Series/DataFrame, which these are not.  Thoughts?"
582716833,32769,TYP: annotate Block/BlockManager putmask,jbrockmendel,closed,2020-03-17T02:54:58Z,2020-03-22T21:01:45Z,medium-term goal is to avoid passing Series/DataFrame objects to Block methods via BlockManager.apply
585294694,32874,BUG/TST: Add searchsorted tests,dsaxton,closed,2020-03-20T19:53:29Z,2020-03-22T22:02:33Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

xref: https://github.com/pandas-dev/pandas/issues/32845"
585241862,32873,Modification of validate_rst_title_capitalization.py script,cleconte987,closed,2020-03-20T18:07:03Z,2020-03-22T22:19:05Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
570858535,32251,pandas.tseries.offsets.CustomBusinessDay apply_index implementation,gaibo,closed,2020-02-25T21:52:23Z,2020-03-23T01:11:45Z,"#### Code Sample

Hi there, I'm trying to do something like this:
```python
BUSDAY_OFFSET = pd.offsets.CustomBusinessDay(calendar=MY_HOLIDAY_CALENDAR)
my_rollback = pd.date_range('2010-05-10', '2020-02-25') + BUSDAY_OFFSET - BUSDAY_OFFSET
```
and I am getting the following warning:
```
C:\Users\my_username\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\arrays\datetimes.py:743: PerformanceWarning: Non-vectorized DateOffset being applied to Series or DatetimeIndex
  ""or DatetimeIndex"", PerformanceWarning)
```

#### Problem description

The custom business day non-vectorized addition/subtraction works, but when performed on the order of millions of dates (ridiculous because I could take out duplicate dates, I know, but it was my original use case), it takes a non-trivial amount of time, whereas `` pd.offsets.Day()`` can be added and subtracted in trivial time.

I checked _datetimes.py_ and CustomBusinessDay's apply_index() method is not implemented, and it is not clear from any indication whether that's intended (as in, the user should override it?) or not. [The doc page is here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.tseries.offsets.CustomBusinessDay.apply_index.html#pandas.tseries.offsets.CustomBusinessDay.apply_index).

Can anyone familiar with CustomBusinessDay's structure comment on whether there can be a simple fix?

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.2
pytest: 5.0.1
pip: 19.1.1
setuptools: 41.0.1
Cython: 0.29.12
numpy: 1.16.4
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 7.6.1
sphinx: 2.1.2
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: 1.2.1
tables: 3.5.2
numexpr: 2.6.9
feather: None
matplotlib: 3.1.0
openpyxl: 2.6.2
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.8
lxml.etree: 4.3.4
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.3.5
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
585442776,32881,DOC: Fix errors in pandas.DataFrame.melt,farhanreynaldo,closed,2020-03-21T06:42:35Z,2020-03-23T02:50:58Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry


Related to #27977. 

output of `python scripts/validate_docstrings.py pandas.DataFrame.melt`:
```
################################################################################
################################## Validation ##################################
################################################################################
"
585438529,32880,DOC: Fix errors in pandas.DataFrame.sort_index,farhanreynaldo,closed,2020-03-21T06:05:14Z,2020-03-23T02:51:54Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. 

output of `python scripts/validate_docstrings.py pandas.DataFrame.sort_index`:
```
################################################################################
################################## Validation ##################################
################################################################################
```

"
406137661,25130,df.apply function is executed twice when rows == 1,aboidriss,closed,2019-02-03T22:14:16Z,2020-03-23T07:34:26Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
def func(row):
    for col in row.keys():
        row[col]=row[col]+1
    return row

# func is applied twice , not expected
df = pd.DataFrame(data = [[0,0]],columns=['a','b'])
df.apply(func,axis=1)
print(df)

# func is applied once , expected
df = pd.DataFrame(data = [[0,0]]*2,columns=['a','b'])
df.apply(func,axis=1)
print(df)

```
I get the following output : 
```
   a  b
0  2  2

   a  b
0  1  1
1  1  1
```

#### Problem description
The function func is applied twice
The problem happens when the number of rows is equal to 1

#### Expected Output
```
   a  b
0  1  1

   a  b
0  1  1
1  1  1
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.7.final.0
python-bits: 64
OS: Linux
OS-release: 4.14.65+
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: en_US.UTF-8
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: None
pip: 18.1
setuptools: 40.6.2
Cython: 0.29.2
numpy: 1.15.4
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 7.2.0
sphinx: None
patsy: None
dateutil: 2.7.5
pytz: 2018.7
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: 1.2.14
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
586079043,32924,ImportError: cannot import name 'rolling_cov,talaikis,closed,2020-03-23T09:57:44Z,2020-03-23T10:38:08Z,"#### Code Sample, a copy-pastable example if possible

```python
from pandas import read_json, to_datetime, concat, rolling_cov
```

#### Problem description

Missing export in Pandas 1.0.3

#### Expected Output

#### Output of ``pd.show_versions()``

<details>
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
586093702,32925,DOC: Fix formatting in documentation,bartbroere,closed,2020-03-23T10:19:38Z,2020-03-23T11:10:44Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
545945707,30755,CI: Unify code_checks whitespace checking,ShaharNaveh,closed,2020-01-06T21:20:51Z,2020-03-23T12:06:19Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Unify test cases of #30467 #30708 #30737"
586105974,32926,Renamed `validate_string_concatenation.py` to `validate_unwanted_patterns.py`,ShaharNaveh,closed,2020-03-23T10:38:22Z,2020-03-23T12:06:39Z,"Follow up for  #30755
"
560567596,31717,Are the contributors in the whatsnew v1.0.1 note correct?,MarcoGorelli,closed,2020-02-05T19:02:26Z,2020-03-23T13:41:35Z,"Having a look at https://github.com/pandas-dev/pandas/milestone/69?closed=1, it seems there's some contributors (e.g. dsaxton , srvanrell) who aren't listed in the whatsnew page: https://pandas.pydata.org/docs/whatsnew/v1.0.1.html

> A total of 7 people contributed patches to this release. People with a “+” by their names contributed a patch for the first time.
> 
>     Guillaume Lemaitre
> 
>     Jeff Reback
> 
>     Joris Van den Bossche
> 
>     Kaiqi Dong
> 
>     MeeseeksMachine
> 
>     Pandas Development Team
> 
>     Tom Augspurger
"
585502357,32884,fix bare pytest raises in indexes/datetimes,quangngd,closed,2020-03-21T13:47:18Z,2020-03-23T13:42:02Z,"- [ ] ref #30999
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
584454205,32827,DOC: Fixed contributors for bugfix releases,TomAugspurger,closed,2020-03-19T14:30:12Z,2020-03-23T13:43:00Z,"Previously we showed just contributors who manually backported commits
to the maintenance branch.

```
Contributors
============

A total of 8 people contributed patches to this release.  People with a
""+"" by their names contributed a patch for the first time.

* Daniel Saxton
* Joris Van den Bossche
* MeeseeksMachine
* MomIsBestFriend
* Pandas Development Team
* Simon Hawkins
* Tom Augspurger
* jbrockmendel
```

Fixed

```
Contributors
============

A total of 22 people contributed patches to this release.  People with a
""+"" by their names contributed a patch for the first time.

* Anna Daglis +
* Daniel Saxton
* Irv Lustig
* Jan Škoda +
* Joris Van den Bossche
* Justin Zheng +
* Kaiqi Dong
* Kendall Masse +
* Marco Gorelli
* Matthew Roeschke +
* Pedro Reys +
* Prakhar Pandey +
* Robert de Vries +
* Rushabh Vasani +
* Simon Hawkins +
* Stijn Van Hoey +
* Terji Petersen +
* Tom Augspurger
* William Ayd
* alimcmaster1
* gfyoung +
* jbrockmendel
```

Closes https://github.com/pandas-dev/pandas/issues/31717"
579258847,32623,Pandas 1.0.1 unable to read excels it created itself,gnudiff,closed,2020-03-11T13:20:17Z,2020-03-23T14:01:26Z,"After creating lots of XLSX files with pandas.Excelwriter(), the same pandas is unable to read some of them:

`writer=pd.ExcelWriter(outfile)`

Then in a different script, when processing the same files with `pd.read_excel(outfile)`

on some (but not all) files I get error:

`xlrd.biffh.XLRDError: ZIP file contents not a known type of workbook
`

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-72-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.17.2
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 20.0.2
setuptools       : 41.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.2 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 5.5.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.10
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.1.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
586218453,32928,Backport PR #32827 on branch 1.0.x (DOC: Fixed contributors for bugfix releases),meeseeksmachine,closed,2020-03-23T13:42:17Z,2020-03-23T14:14:06Z,Backport PR #32827: DOC: Fixed contributors for bugfix releases
585805915,32913,pd.pct_change() inconsistently rasies SettingWithCopyWarning:,RandyBetancourt,closed,2020-03-22T20:32:32Z,2020-03-23T14:56:40Z,"When calling the get_daily_adjusted() function from the Alpha Vantage web API to building a DataFrame and calling the pd.pct_change() function on the resulting DataFrame, a SettingWithCopyWarning is raised.  

Alternatively, if a similar DataFrame is built calling the DataFrame() constructor method with a call to the pd.pct_change() function, no SettingWithCopyWarning is raised.  

```python
Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from alpha_vantage.timeseries import TimeSeries
>>> import pandas as pd
>>> import datetime
>>>
>>> ts = TimeSeries(key=""XXXX"", output_format='pandas')
>>> df, meta_data = ts.get_daily_adjusted(symbol='TSLA', outputsize='full')
>>>
>>> df.columns = df.columns.str[3:]
>>> tesla = df[(df.index > '2019-05-01')]
>>>
>>> print(isinstance(tesla, pd.DataFrame))
True
>>>
>>> print(tesla.info())
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 224 entries, 2020-03-20 to 2019-05-02
Data columns (total 8 columns):
 #   Column             Non-Null Count  Dtype
---  ------             --------------  -----
 0   open               224 non-null    float64
 1   high               224 non-null    float64
 2   low                224 non-null    float64
 3   close              224 non-null    float64
 4   adjusted close     224 non-null    float64
 5   volume             224 non-null    float64
 6   dividend amount    224 non-null    float64
 7   split coefficient  224 non-null    float64
dtypes: float64(8)
memory usage: 15.8 KB
None
>>>
>>> tesla.close = tesla.close.pct_change(periods=1)
C:\Users\randy\Anaconda3\lib\site-packages\pandas\core\generic.py:5292: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  self[name] = value
>>>
>>> df2 = pd.DataFrame({
...     'open': [14.97,15.1,1.5,14.82],
...     'high': [14.97,15.15,15,15.47],
...     'low': [14.97,14.86,14.16,14.62],
...     'close': [14.97,13.08,15,15.14]},
...     index=['2019-12-31', '2020-01-02', '2020-01-03','2020-01-04'])
>>>
>>> print(isinstance(df, pd.DataFrame))
True
>>>
>>> df2.close = df2.close.pct_change(periods = 1)
>>>
>>> print(df2.info())
<class 'pandas.core.frame.DataFrame'>
Index: 4 entries, 2019-12-31 to 2020-01-04
Data columns (total 4 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   open    4 non-null      float64
 1   high    4 non-null      float64
 2   low     4 non-null      float64
 3   close   3 non-null      float64
dtypes: float64(4)
memory usage: 160.0+ bytes
None
```
#### Problem description

I can't determine why the method for DataFrame construction seems to influence the behavior of the pd.pct_change() method.  What is different between these two instances?  Chaining a copy() method call to the first example still raises the SettingWithCopyWarning.


#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 58 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.0rc0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : 0.29.15
pytest           : 5.3.5
hypothesis       : 5.5.4
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.48.0
</details>
"
565714210,31992,COMPAT: numpy dev version raises new errors during CI,AlexKirko,closed,2020-02-15T08:01:15Z,2020-03-23T15:13:00Z," ### Problem description

Looks like something has been changed in numpy, and we now get new errors when running that Numpy dev pipeline during CI. This happens regardless of PR (saw it in #31563 and #31991). Seems to be centered around cumsum and division by nan, but I don't have the time to delve deeper atm.

One of the errors below:

```python


        # axis = 0
        cummin = datetime_frame.cummin()
        expected = datetime_frame.apply(Series.cummin)
>       tm.assert_frame_equal(cummin, expected)

pandas/tests/frame/test_cumulative.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pandas/_libs/testing.pyx:65: in pandas._libs.testing.assert_almost_equal
    cpdef assert_almost_equal(a, b,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   raise_assert_detail(obj, msg, lobj, robj)
E   AssertionError: DataFrame.iloc[:, 0] (column name=""A"") are different
E   
E   DataFrame.iloc[:, 0] (column name=""A"") values are different (46.66667 %)
E   [left]:  [0.652282807224414, -0.06609607048075124, 6.9221550547283e-310, 4.6661712289503e-310, 4.6661712289503e-310, 0.22575641583650302, 1.5013182000449852, -0.035445399728511705, -0.4007619715825971, -0.4007619715825971, -1.4917178151508124, -0.14834313680366412, 0.545948106375247, 1.495247786960942, 0.2128973463640255, 0.794985190506577, -1.4214210868734076, -1.4214210868734076, -1.019792839223919, -0.28828070056157995, -0.47176522372684804, -0.4267351486193231, -2.816743984494702, 0.4308015055934936, -0.007381576327430263, -0.007381576327430263, -1.121456436411571, -0.25765070817685554, -0.7686775772512485, 1.0963456159671487]
E   [right]: [0.652282807224414, -0.06609607048075124, -0.06609607048075124, 0.48729707348930335, 0.48729707348930335, 0.22575641583650302, 0.22575641583650302, -0.035445399728511705, -0.4007619715825971, -0.4007619715825971, 0.3249965661970306, -0.14834313680366412, -0.14834313680366412, 0.545948106375247, 0.2128973463640255, 0.2128973463640255, -1.4214210868734076, -1.4214210868734076, -0.9987072139900786, -0.9987072139900786, -0.47176522372684804, -0.47176522372684804, -2.816743984494702, -2.816743984494702, -0.007381576327430263, -0.007381576327430263, -1.121456436411571, -1.121456436411571, -0.7686775772512485, -0.7686775772512485]

```

[Link](https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=28623&view=logs&j=3a03f79d-0b41-5610-1aa4-b4a014d0bc70&t=4d05ed0e-1ed3-5bff-dd63-1e957f2766a9&l=75) to one of the failed pipelines. You can also just take a look at any recent PR."
585874268,32920,"REF: misplaced to_datetime, date_range tests",jbrockmendel,closed,2020-03-23T01:57:02Z,2020-03-23T15:29:03Z,
579664894,32650,BUG: DTI/TDI/PI get_indexer_non_unique with incompatible dtype,jbrockmendel,closed,2020-03-12T03:17:52Z,2020-03-23T16:12:20Z,"I'm pretty sure we can share more code here, but PeriodEngine needs some work first."
584715029,32843,DOC: Fix capitalization among headings in documentation files (#32550),themien,closed,2020-03-19T21:37:48Z,2020-03-23T17:26:56Z,"- [x] closes #32550 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
585827574,32915,DOC: Modify validate_rst_title_capitalization.py script,cleconte987,closed,2020-03-22T22:25:19Z,2020-03-23T18:20:28Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
586421340,32941,(WIP) CI: Check that private functions are not used across modules,ShaharNaveh,closed,2020-03-23T18:22:26Z,2020-03-23T18:23:04Z,"- [x] ref https://github.com/pandas-dev/pandas/pull/32932#issuecomment-602704059
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

cc @jbrockmendel 
"
585583480,32897,CLN: Avoiding casting,ShaharNaveh,closed,2020-03-21T20:58:54Z,2020-03-23T18:36:08Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

Follow up to https://github.com/pandas-dev/pandas/pull/32794#discussion_r394045101
"
585526957,32888,CLN: pandas/_libs/tslibs/nattype.pyx,ShaharNaveh,closed,2020-03-21T16:01:03Z,2020-03-23T18:36:59Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

This PR is doing basicly two things:

* Getting rid of ```noqa: E128``` comments.

* Unifying use of```util.foo```

in some places there was a use of  ```is_integer_object(foo)``` and in some places there was a use of ```util.is_integer_object(foo)```, now ```util.is_integer_object(foo)``` is being in use instead.

---

Benchmarks:

```
In [1]: from pandas._libs.tslibs.nattype import _make_nat_func 

In [2]: %timeit _make_nat_func(""foo"", ""bar"")
120 ns ± 1.01 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each) # Master
117 ns ± 1.47 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each) # PR
```"
584788817,32849,"REF: implement _with_freq, use _from_sequence less",jbrockmendel,closed,2020-03-20T01:18:59Z,2020-03-23T20:25:07Z,"This makes method-chaining a little bit nicer.  

Moving away from using _from_sequence and towards using the constructors where feasible.  This will make it easier to stricten what we accept in these _from_sequence methods.

We also should be inferring freq _less_ in arithmetic methods, but that is a behavior change that will be done separately."
583874203,32808,ERR: Better error message,ShaharNaveh,closed,2020-03-18T17:02:44Z,2020-03-23T20:42:21Z,"When invalid value for pd.to_datetime or pd.to_timedelta is passed

- [x] closes #10720
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

Resurrection of #31118
"
565062549,31970,CLN: @doc - base.py & indexing.py,HH-MWB,closed,2020-02-14T02:20:29Z,2020-03-23T21:42:50Z,"- [x] working on #31942 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
585560825,32893,Added `const` where avaible,ShaharNaveh,closed,2020-03-21T19:10:46Z,2020-03-23T22:27:08Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
586845124,32971,import is failing in windows in 1.0.3 version,jaydave1988,closed,2020-03-24T10:14:44Z,2020-03-24T12:34:46Z,"Import is failing in 1.0.3 version of panda in windows. It was working in 0.22 version

>>> import pandas
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\pandas\__init__.py"", line 55, in <module>
    from pandas.core.api import (
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\pandas\core\api.py"", line 29, in <module>
    from pandas.core.groupby import Grouper, NamedAgg
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\pandas\core\groupby\__init__.py"", line 1, in <module>
    from pandas.core.groupby.generic import DataFrameGroupBy, NamedAgg, SeriesGroupBy
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\pandas\core\groupby\generic.py"", line 60, in <module>
    from pandas.core.frame import DataFrame
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\pandas\core\frame.py"", line 124, in <module>
    from pandas.core.series import Series
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\pandas\core\series.py"", line 4572, in <module>
    Series._add_series_or_dataframe_operations()
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\pandas\core\generic.py"", line 10349, in _add_series_or_dataframe_operations
    from pandas.core.window import EWM, Expanding, Rolling, Window
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\pandas\core\window\__init__.py"", line 1, in <module>
    from pandas.core.window.ewm import EWM  # noqa:F401
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36-32\lib\site-packages\pandas\core\window\ewm.py"", line 5, in <module>
    import pandas._libs.window.aggregations as window_aggregations
ImportError: DLL load failed: The specified module could not be found.
>>> exit()


C:\Users\Administrator>pip uninstall pandas
Found existing installation: pandas 1.0.3
Uninstalling pandas-1.0.3:
  Would remove:
    c:\users\administrator\appdata\local\programs\python\python36-32\lib\site-packages\pandas-1.0.3.dist-info\*
    c:\users\administrator\appdata\local\programs\python\python36-32\lib\site-packages\pandas\*
Proceed (y/n)? y
  Successfully uninstalled pandas-1.0.3"
586458059,32946,DOC: Fix capitalization among headings in documentation files (#32550) part 3,themien,closed,2020-03-23T19:21:37Z,2020-03-24T13:24:51Z,"Files updated:

```
doc/source/getting_started/comparison/comparison_with_sas.rst
doc/source/getting_started/comparison/comparison_with_sql.rst
doc/source/getting_started/comparison/comparison_with_stata.rst
doc/source/getting_started/intro_tutorials/01_table_oriented.rst
doc/source/getting_started/tutorials.rst
```
"
586450441,32945,DOC: Fix capitalization among headings in documentation files (#32550) part 2,themien,closed,2020-03-23T19:13:07Z,2020-03-24T13:24:51Z,"Files updated:

```
doc/source/development/policies.rst
doc/source/development/roadmap.rst
doc/source/ecosystem.rst
doc/source/getting_started/10min.rst
doc/source/getting_started/basics.rst
```
"
576879230,32489,CLN: trying isort-dev,ShaharNaveh,closed,2020-03-06T11:44:45Z,2020-03-24T13:41:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

Trying ```isort``` 5.0.0 which is still under development."
576191507,32459,CLN: Using clearer imports,ShaharNaveh,closed,2020-03-05T11:39:44Z,2020-03-24T13:58:11Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
586633969,32965,CLN: Clean NDFrame.__finalize__ signature,dsaxton,closed,2020-03-24T01:58:06Z,2020-03-24T14:41:17Z,`NDFrame.__finalize__` doesn't do anything with `method` or `kwargs` so removing them from the signature and all calls. Had to change a few tests which overrode this method with another that also included those arguments.
586555659,32958,Subsequent calls to read_csv for newly created S3 files in the same directory throws an error,mattlbeck,closed,2020-03-23T22:23:07Z,2020-03-24T15:31:38Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import s3fs

BUCKET = 'mattlbeck-mybucket'
s3path1 = f's3://{BUCKET}/key/file1.csv'
s3path2 = f's3://{BUCKET}/key/file2.csv'

with s3fs.S3FileSystem().open(s3path1, 'w') as fh:
    fh.write('a,b,c\n1,2,3')

pd.read_csv(s3path1)

with s3fs.S3FileSystem().open(s3path2, 'w') as fh:
    fh.write('a,b,c\n1,2,3')

pd.read_csv(s3path2)
```

Produces the trace summarised below:

```
Traceback (most recent call last):
  ...
FileNotFoundError: mattlbeck-mybucket/key/file2.csv

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  ...
botocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
 ...
PermissionError: Access Denied
```

#### Problem description

Subsequent calls to `read_csv` beyond the first call causes an error when the target is a file in S3 within the same directory as a file that has previously been read by `read_csv`

The problem appears to be overly aggressive caching of S3 directory listings. But from reading the code I can't immediately see how the cache is persisting between the two calls - there is no obvious reuse of `S3FileSystem` instances.

#### Expected Output

Both calls to `read_csv` should resolve without error because both files exist at the time of the call.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f20331d543c5d42e54dff29135146fc0e6798a84
python           : 3.8.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.3.0
Version          : Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.0.dev0+945.gf20331d54
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.1.post20200323
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : 0.4.0
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
550052369,31034,TST: Split test_offsets.py - Added fixtures for date offsets and business date offsets,Raalsky,closed,2020-01-15T09:03:21Z,2020-03-24T16:12:17Z,"In context of #30194 and #31031
Second part with added (bussiness-)date offsets fixtures
- [ ] closes #27085
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
585508828,32885,Provide [major|religious|national] holiday offsets,fabianegli,closed,2020-03-21T14:23:14Z,2020-03-24T16:53:39Z,"#### Motivation

`pandas` does a wonderful job when working with time series - e.g. it is easy to find periodic patterns. There are straight forward methods to find these patterns for standard time intervals such as days, weeks, months and years.

However, it is trickier to find these patterns when they occur in other periodicities or irregular but defined intervals. Often religious holidays depend on the moon or a calendar different from the western calendar ([ISO 8601](https://en.wikipedia.org/wiki/ISO_8601#Usage)). Easter is such an example and already has an offset in `pandas`. It would be very practical to get offsets to other holidays as well, like [iftar](https://en.wikipedia.org/wiki/Iftar), [Chinese New Year](https://en.wikipedia.org/wiki/Chinese_New_Year), [Passover](https://en.wikipedia.org/wiki/Passover), [Krishna Janmashtami](https://en.wikipedia.org/wiki/Krishna_Janmashtami) and [Kanamara Matsuri](https://en.wikipedia.org/wiki/Kanamara_Matsuri) to find patterns relative to their timing.
"
460639556,27043,CLN: Remove no-longer-used BlockManager.xs,jbrockmendel,closed,2019-06-25T20:59:23Z,2020-04-05T17:36:42Z,Also some unrecheable try_cast code.
404809838,25028,CLN: typo fixups,jbrockmendel,closed,2019-01-30T14:57:35Z,2020-04-05T17:36:54Z,Also edit DatetimeLikeBlockMixin.get_values to be much simpler.
399003944,24769,REF/TST: Stop using singleton fixtures,jbrockmendel,closed,2019-01-14T17:52:46Z,2020-04-05T17:37:07Z,"Found when trying to collect reduction tests that we are still using a lot of singleton fixtures (agreed undesirable in #23701).  This gets rid of singleton fixtures in tests.frame

Following this it will be easier to more usefully parametrize some of those tests."
401579412,24873,TST: Remove subset of singleton fixtures,jbrockmendel,closed,2019-01-22T03:00:08Z,2020-04-05T17:37:09Z,"Broken off of #24769 where we have learned that some test behavior depends on whether or not a fixture is being used.

I claim that this is another point in favor of not using fixtures if there is a regular-python alternative (in this case, a ""function"").  Whenever with-fixture behavior is different from without-fixture behavior, it is definitely the latter that better represents user runtime environments.  That is what we should be testing."
401431119,24864,BLD: silence npy_no_deprecated warnings with numpy>=1.16.0,jbrockmendel,closed,2019-01-21T16:36:36Z,2020-04-05T17:37:14Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
401513287,24869,REF: move methods into cdef classes,jbrockmendel,closed,2019-01-21T21:17:44Z,2020-04-05T17:37:19Z,"A bunch of methods are defined in Timestamp/Timedelta that can be defined in _Timestamp/_Timedelta.  Moving these into the cdef classes is supposedly slightly more efficient.  If we can work around the `__cinit__/__new__` rules, we might be able to get rid of _Timestamp entirely.

Fix a couple of typos while we're at it."
400922377,24833,POC: move to_offset to libfrequencies,jbrockmendel,closed,2019-01-18T23:04:42Z,2020-04-05T17:37:28Z,"The runtime non-cython import of to_offset is a sticking point in a bunch of tslibs code.  The reason we haven't moved to_offset up into cython is because it requires all of tseries.offsets, and we haven't wanted to move all of that up.

This moves to_offset up by defining appropriate dictionaries in the cython module and then filling them in the python modules.  It's a bit roundabout, but may be worthwhile (about to run asvs)

The refactor that was left out of this PR is moving tslibs.resolution.Resolution into tslibs.frequencies to avoid a circular/runtime import."
384036563,23890,WIP: implement reductions for DatetimeArray/TimedeltaArray/PeriodArray,jbrockmendel,closed,2018-11-25T01:04:30Z,2020-04-05T17:37:40Z,"Some idiosyncrasies in signatures between Series vs Index make the parametrized test cases ugly.  Do we want to adapt the Index reductions to have a `skipna` kwarg like the Series and EA methods?

Still needs tests for timedelta and period dtypes."
387534105,24104,WIP/ENH: Pass tzinfos to dateutil parser,jbrockmendel,closed,2018-12-05T00:02:48Z,2020-04-05T17:37:44Z,"Putting this up early because the `set_option` behavior deserves discussion.  The alternative is to make/let users pass `tzinfos` in Timestamp, to_datetime, DatetimeIndex, and probably others.

- [x] closes #22234, #18702
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Still need to:
- [ ] document usage, whether via set_option or passing tzinfos
- [ ] tests for array_to_datetime, to_datetime, Timestamp, DatetimeIndex
- [ ] test for the fix of #22234
"
381402721,23730,BUG: Fix+test dataframe tranpose with datetimeTZ,jbrockmendel,closed,2018-11-16T00:56:44Z,2020-04-05T17:37:49Z,"Reminder of the bug here:

```
>>> dti = pd.date_range('1977-04-15', periods=3, freq='MS', tz='US/Hawaii')
>>> df = pd.DataFrame(dti)
>>> df.T
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pandas/core/frame.py"", line 2562, in transpose
    return super(DataFrame, self).transpose(1, 0, **kwargs)
  File ""pandas/core/generic.py"", line 686, in transpose
    new_values = self.values.transpose(axes_numbers)
  File ""pandas/core/base.py"", line 672, in transpose
    nv.validate_transpose(args, kwargs)
  File ""pandas/compat/numpy/function.py"", line 56, in __call__
    self.defaults)
  File ""pandas/util/_validators.py"", line 218, in validate_args_and_kwargs
    validate_kwargs(fname, kwargs, compat_args)
  File ""pandas/util/_validators.py"", line 157, in validate_kwargs
    _check_for_default_values(fname, kwds, compat_args)
  File ""pandas/util/_validators.py"", line 69, in _check_for_default_values
    format(fname=fname, arg=key)))
ValueError: the 'axes' parameter is not supported in the pandas implementation of transpose()
```

The implementation is an unholy mess.  It can be made a little bit prettier, but to actually get a nice implementation we need to allow 2D EAs."
381434945,23734,TST: Implement maximally-specific/strict fixtures for arithmetic tests,jbrockmendel,closed,2018-11-16T03:50:06Z,2020-04-05T17:37:53Z,"gets rid of all remaining ""FIXME"" comments in tests/arithmetic.

Should go after #23681 and #23642 (will need to be rebased)"
380829528,23702,Catch exception around much smaller piece of code,jbrockmendel,closed,2018-11-14T18:28:19Z,2020-04-05T17:37:59Z,"Broken off of #23675, followed by refactor.

The refactor has a non-trivial diff, but the only actual _logic_ changed here is the `except ValueError as e`, instead of catching everything from L240-L303, now specifically only catches errors raised by `tslib.array_to_datetime`.  Everything else is just moving code outside of the try/except block and de-indenting.

Catching more specific exceptions is worthwhile in its own right, but this is also an important step for implementing `DatetimeArray._from_sequence` (@TomAugspurger, @jorisvandenbossche, @jreback) since after this we can isolate the part of `to_datetime` called by `DatetimeIndex.__new__` (xref #23675) to make the constructor non-circular (and then refactor it out into `_from_sequence`).

cc @mroeschke "
375271217,23414,REF: de-duplicate DataFrame/SparseDataFrame arithmetic code,jbrockmendel,closed,2018-10-30T00:32:01Z,2020-04-05T17:38:03Z,"Not quite all the way de-duplicated, but this is as much as I could do without it getting convoluted."
377182439,23493,REF/ENH: Constructors for DatetimeArray/TimedeltaArray,jbrockmendel,closed,2018-11-04T18:52:37Z,2020-04-05T17:38:16Z,"Big push on the constructors for DatetimeArray/TimedeltaArray, some progress de-duplicating code from their Index counterparts.

As discussed elsewhere, adds `dtype` to `PeriodArray.__init__`

@TomAugspurger can you confirm that the `_from_sequence`s implemented here handle the right cases?  It wasn't obvious if object-dtyped array/indexes were supposed to be handled there, but combining those cases was too clean to overlook.

Small fix in DatetimeArray comparison methods, just enough to make the tests work.  A separate PR forthcoming PR will do a more thorough fix/improvements of those.

One non-obvious point on which input would be especially welcome is how/when to use the `copy` kwarg in such a way as to copy at-most-once.  (related: deep_copy_if_needed and #21907)

#23491 found during this process, will be addressed in a follow-up."
377093821,23474,"STY: standardize spacing for casting, with linting",jbrockmendel,closed,2018-11-03T21:13:13Z,2020-04-05T17:38:22Z,"Casting in cython is done with the syntax `<type>obj` or `<type> obj`.  ATM the code uses both of these.  This PR standardizes on the first usage, with a lint check.

@datapythonista I'm not sure whether the grep check here belongs in the patterns section or in the non-python section.  LMK if you have a preference."
375303703,23416,REF: use asi8 instead of _data in PeriodArray,jbrockmendel,closed,2018-10-30T03:23:25Z,2020-04-05T17:38:28Z,"@TomAugspurger @jorisvandenbossche discussed on the call a couple weeks ago.  Reference `asi8` instead of `_data` or `_ndarray_values` is the most-explicit option available.

I would also be in favor of getting rid of `_data` directly and setting `asi8` in `__init__`, but holding off on that pending consensus.

Also re-implemented some simplifications that got lost in rebasing during the PeriodArray marathon."
374043384,23333,"Cleanup tests/scalar, isolate scalar-only tests from scalar-mixed tests",jbrockmendel,closed,2018-10-25T17:02:23Z,2020-04-05T17:38:33Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
373268117,23308,TST: fix _most_ remaining xfails in tests/arithmetic,jbrockmendel,closed,2018-10-24T01:32:35Z,2020-04-05T17:38:39Z,"Most of the remaining xfails in tests/arithmetic are caused by single-column DataFrame operations not being equivalent to Series/Index operations.  This PR fixes those tests by transposing both the DataFrame and the expected result in the appropriate cases.

We end up with 3 xfailed tests instead of 223.  Runtime drops from 168.9 seconds to 99.4 seconds.

Some other ancillary fixes along the way: RangeIndex correctly returns NotImplemented when operating with a DataFrame.  Series operations with Tick and non-nano timedelta64 are fixed."
351028740,22378,Fix and test scalar extension dtype op corner case,jbrockmendel,closed,2018-08-16T02:04:00Z,2020-04-05T17:38:47Z,"Fixes the following behavior in master:

```
ser = pd.Series(['a', 'b', 'c'])

>>> ser + ""category""
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pandas/core/ops.py"", line 1233, in wrapper
    return dispatch_to_extension_op(op, left, right)
  File ""pandas/core/ops.py"", line 1163, in dispatch_to_extension_op
    res_values = op(new_left, new_right)
TypeError: can only concatenate list (not ""str"") to list


>>> ser + ""Int64""
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pandas/core/ops.py"", line 1233, in wrapper
    return dispatch_to_extension_op(op, left, right)
  File ""pandas/core/ops.py"", line 1163, in dispatch_to_extension_op
    res_values = op(new_left, new_right)
TypeError: can only concatenate list (not ""str"") to list
```

and the same for the reversed ops."
366927418,22995,use tm.assert_equal instead of parametrizing assert funcs,jbrockmendel,closed,2018-10-04T19:03:07Z,2020-04-05T17:38:52Z,
366156006,22961,"CLN: Move some PI/DTI methods to EA subclasses, implement tests",jbrockmendel,closed,2018-10-03T02:58:05Z,2020-04-05T17:38:58Z,"If/when this goes through the basic template it creates for tests will be pretty straightforward to extend to the other relevant methods/properties.

@TomAugspurger this definitely overlaps with #22862, particularly the pieces touching constructors.  LMK if this causes problems and I can try to stay in my lane."
369230328,23095,"PERF: PeriodIndex.dropna, difference, take",jbrockmendel,closed,2018-10-11T17:38:40Z,2020-04-05T17:39:04Z,"Related #23083 

Besides the performance boost, this goes a long way towards making `PeriodIndex._simple_new` actually simple; avoids passing object-dtype to `_simple_new`

```
In [2]: pi_with_nas = pd.PeriodIndex(['1985Q1', 'NaT', '1985Q2'] * 1000, freq='Q')
In [3]: %timeit pi_with_nas.dropna()
The slowest run took 13.93 times longer than the fastest. This could mean that an intermediate result is being cached.
10000 loops, best of 3: 23.3 µs per loop  <-- PR
100 loops, best of 3: 4.96 ms per loop    <-- master

In [4]: pi_diff = pd.PeriodIndex(['1985Q1', 'NaT', '1985Q3'] * 1000, freq='Q')
In [5]: %timeit pi_with_nas.difference(pi_diff)
1000 loops, best of 3: 553 µs per loop  <-- PR
100 loops, best of 3: 5.29 ms per loop  <-- master

In [6]: %timeit pi_with_nas.symmetric_difference(pi_diff)
1000 loops, best of 3: 787 µs per loop  <-- PR
100 loops, best of 3: 10.3 ms per loop  <-- master
```"
365002360,22880,Use align_method in comp_method_FRAME,jbrockmendel,closed,2018-09-28T18:53:16Z,2020-04-05T17:39:10Z,"Closes #20090

<b>update</b> Since #23000 was merged, some of the discussion is out of date.  The bottom line remains unchanged: This PR makes DataFrame _comparison_ ops behave like DataFrame _arithmetic_ ops currently do.  Also fixes some bugs e.g. #20090
<b> end update</b>

This is a much nicer alternative to the implementation in #22751.  The problem is that two tests still fail with this implementation.  We need to pin down the design spec more explicitly regardless.

core.ops has three functions for defining DataFrame ops: _arith_method_FRAME, _flex_comp_method_FRAME, _comp_method_FRAME.  The first two both call `_align_method_FRAME`, with `_comp_method_FRAME` being the outlier.  This PR just adds that alignment call.

The two tests that currently fail:

```
df = DataFrame(np.arange(6).reshape((3, 2)))
b_r = np.atleast_2d([2, 2])
l = (2, 2, 2)
expected = DataFrame([[False, False], [False, True], [True, True]])
result = df > l   # <-- raises ValueError under this PR because it has the wrong shape
assert_frame_equal(result, expected)

result = df > b_r   # <-- raises ValueError under this PR because it has the wrong shape
assert_frame_equal(result, expected)
```

```
df = pd.DataFrame(np.arange(6).reshape((3, 2)))
with pytest.raises(ValueError):  # <-- doesnt raise
           df == (2, 2)
```

I understand why the behavior tested by the first test makes sense, but don't see the logic behind having `df == (2, 2)` raise (maybe #4576 holds the answer, will look at that more closely)"
370262402,23166,REF: De-duplicate pieces of datetimelike arithmetic,jbrockmendel,closed,2018-10-15T17:20:49Z,2020-04-05T17:39:15Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
372250199,23258,CLN: remove unused try_cast kwarg from ops,jbrockmendel,closed,2018-10-20T20:33:24Z,2020-04-05T17:39:19Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
372255997,23260,CLN: remove unused `mgr` kwarg,jbrockmendel,closed,2018-10-20T21:48:37Z,2020-04-05T17:39:26Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
297342654,19709,Implement maybe_cache for compat between immutable/mutable classes,jbrockmendel,closed,2018-02-15T06:35:41Z,2020-04-05T17:39:43Z,"Also fixes lookups/docstrings for class-level access to cache_readonly attributes.

- [x] closes #19700
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Related: https://github.com/pandas-dev/pandas2/issues/27"
594557358,33305,"BUG:I am using Latest python 3.8 ,the pip install pandas is working and is successful but showing an when importing it is showing an error, need help",canaraphil,closed,2020-04-05T17:02:24Z,2020-04-05T17:39:47Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here

```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
294139706,19528,WIP: parametrize giant frame flex op tests,jbrockmendel,closed,2018-02-03T19:48:34Z,2020-04-05T17:39:48Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
289477096,19291,Fix (Series|DataFrame).interpolate for datetime dtypes,jbrockmendel,closed,2018-01-18T01:53:08Z,2020-04-05T17:39:53Z,"- [x] closes #19199
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
281234155,18738,Implement missing offset comparison methods,jbrockmendel,closed,2017-12-12T02:19:26Z,2020-04-05T17:39:59Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Not sure if it closes #8386, but its most of the way there.

Fix some missing comparisons, including Timedelta with Tick and Tick with (timedelta|Timedelta|Week)

Attach correct names for comparison methods.

If we can resolve #18510, we can make `Week(weekday=None)` just return a `Tick`, avoid some special casing."
272782485,18207,ERR: raise if values passed to Categorical is a DataFrame,jbrockmendel,closed,2017-11-10T00:42:44Z,2020-04-05T17:40:04Z,"2nd of 3 to address #17112.   This raises instead of making the following mistake:

```
>>> df = pd.DataFrame(np.random.randn(3,2), columns=['A', 'B'])
>>> pd.Categorical(df)
[A, B]
Categories (2, object): [A, B]
```"
272782505,18208,"Handle unsortable Periods correctly in set_index, MultiIndex",jbrockmendel,closed,2017-11-10T00:42:48Z,2020-04-05T17:40:10Z,"3rd of 3 to address bugs in #17112

`set_index` goes through MultiIndex.from_arrays, which calls `factorize_from_iterables`... which tries to sort the inputs.  In cases like `Period`, sometimes the inputs can't be sorted.  But for the purposes of `set_index`, we don't actually _care_ about the order.  So we impose a reasonable fallback.

New tests are likely not in the correct place.  Pls advise."
272779739,18205,Catch SystemError in py3 Period.__richcmp__,jbrockmendel,closed,2017-11-10T00:30:35Z,2020-04-05T17:40:10Z,First of ~3 to fix bugs reported in #17112.  This just catches a py3-specific error in `Period.__richcmp__` and raises the correct error instead.  With a test that fails under the status quo.
350882441,22371,[CLN] remove last cython: nprofile comments,jbrockmendel,closed,2018-08-15T16:47:17Z,2020-04-05T17:40:23Z,Separates out parts of #22287 to troubleshoot persistent Travis failures.
346827130,22163,dispatch scalar DataFrame ops to Series,jbrockmendel,closed,2018-08-02T01:36:24Z,2020-04-05T17:40:29Z,"Many issues closed; will track them down and update.  Will also need whatsnew.

closes #18874
closes #20088
closes #15697
closes #13128
closes #8554
closes #8932
closes #21610 
closes #22005 
closes #22047
closes #22242

This will be less verbose after #22068 implements `ops.dispatch_to_series`.

This still only dispatches a subset of ops.  #22019 dispatches another (disjoint) subset.  After that is another easy-ish case where alignment is known.  Saved for last are cases with ambiguous alignment that is currently done in an ad-hoc best-guess way."
343216497,22001,"CLN: De-privatize core.common funcs, remove unused",jbrockmendel,closed,2018-07-20T19:05:58Z,2020-04-05T17:40:40Z,"Moves a few console-checking functions to `io.formats.console`.

A bunch of core.common functions were never used outside of tests, got rid of em.

The ones I left alone were _any_not_none, _all_not_none etc, as I'm inclined to think these should be removed in favor of python builtins."
342854905,21981,ENH: Implement subtraction for object-dtype Index,jbrockmendel,closed,2018-07-19T19:13:32Z,2020-04-05T17:40:45Z,"- [x] closes #19369
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

@jreback @jorisvandenbossche discussed briefly at the sprint.  Merits more thorough testing, but I'd like to get the go-ahead to separate out arithmetic tests that are common to EA/Index/Series/Frame[1col] that are highly duplicative first."
291683660,19398,Fix invalid relativedelta_kwds,jbrockmendel,closed,2018-01-25T19:16:02Z,2020-04-05T17:40:50Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
298709904,19795,"Simplify bool ops closures, improve tests",jbrockmendel,closed,2018-02-20T18:59:45Z,2020-04-05T17:40:57Z,"Series bool ops are not going to be possible to dispatch to their Index counterparts, so this one I'm _mostly_ going to leave alone.  This breaks up the tests, adds an xfail for op(Series, Categorical), and fixes+tests op(series, Index).  It makes some headway in simplifying the closure so the possible paths through the code are more obvious.

There is a TODO comment on line 1113 where I think there is a potential bug, but need someone who understands the _intended_ behavior better to weigh in."
303336347,20049,API: PeriodIndex subtraction to return object Index of DateOffsets,jbrockmendel,closed,2018-03-08T02:45:24Z,2020-04-05T17:41:09Z,"Implements _sub_period_array in DatetimeIndexOpsMixin.  Behavior is analogous to subtraction of Period scalar.

- [x] closes #13077
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
340461240,21870,[CLN] De-privatize commonly-used functions,jbrockmendel,closed,2018-07-12T01:46:58Z,2020-04-05T17:41:20Z,"Also updated numpy_helper to not use things from numpy's deprecated C API.  This won't get rid of the warnings since cython still causes them, but it's still nice.

Not sure how to lint for this (or if we really want to), will see if google knows."
342144044,21958,remove cnp cimports where possible,jbrockmendel,closed,2018-07-18T01:32:51Z,2020-04-05T17:41:28Z,"Not sure what it will take to get avoid the 1.7 deprecation warnings, will see if this helps.

Also curious whether cimporting from libc.stdint works on Appveyor."
342171772,21962,Trim unncessary code in datetime/np_datetime.c,jbrockmendel,closed,2018-07-18T04:23:43Z,2020-04-05T17:41:30Z,"`pydatetime_to_datetimestruct` does a ton of checking that boils down to ""is this a valid datetime object?""  Since the function only gets called after a type-check, we can assume it is a date/datetime and be a lot less verbose about it.

This also rips out an unnecessary layer of functions `convert_datetime_to_datetimestruct`, `convert_timedelta_to_timedeltastruct`.

cc @WillAyd you mentioned wanting to work on your C-foo.  There's a comment about figuring out how to import the cpython datetime C-API.  Any thoughts?"
326677771,21213,implement arith ops on pd.Categorical,jbrockmendel,closed,2018-05-25T22:13:32Z,2020-04-05T17:41:43Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Then in core.ops we dispatch to pd.Categorical instead of special-casing."
329085790,21314,Make Period - Period return DateOffset instead of int,jbrockmendel,closed,2018-06-04T14:42:47Z,2020-04-05T17:41:50Z,"Discussed briefly in #20049.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
340077139,21854,"[CLN] resolve circular Period dependency, prepare setup.py",jbrockmendel,closed,2018-07-11T03:17:59Z,2020-04-05T17:41:56Z,"For a long time there has been a comment in `_libs.__init__` saying it would be nice to import `Period` directly but that is not possible due to circular imports.  This resolves that issue.

Also does some cleanup in setup.py, motivated by the goals of a) using `cythonize` and b) implementing test coverage for cython files.  I've gotten those working locally, but they involve big diffs, so this gets some of the easy stuff out of the way."
340475054,21872,"[REF] Move comparison methods to EAMixins, share code",jbrockmendel,closed,2018-07-12T03:17:44Z,2020-04-05T17:42:01Z,"Changes an old usage numpy's C API that is deprecated.  This won't get rid of the warnings because cython hasn't changed it, but still.

Also stops making copies of offsets since they are now immutable.

Handles a handful of changes requested in the last pass: de-privatizes _quarter_to_myear (plus bonus docstring), renames _generate --> _generate_range, comments in is_list_like

renames arrays.timedelta --> arrays.timedeltas to match core.indexes

Implements comparison methods in DatetimeArray and TimedeltaArray, cleans up some Index code that is no longer needed as a result.

Makes some progress on sharing code between TDI and DTI constructors (most of which we want to move up to the array classes)"
341349059,21923,[BUG] change types to Py_ssize_t to fix #21905,jbrockmendel,closed,2018-07-15T20:16:15Z,2020-04-05T17:42:33Z,"May close #21905, will need to check with OP.
"
341755110,21942,[CLN] Un-xfail now-passing tests,jbrockmendel,closed,2018-07-17T03:44:44Z,2020-04-05T17:42:37Z,"Some of these were fixed in #21861, others sometime earlier this year, unclear.

Removes un unused util `docstring_wrapper`, closing #19676

- [x] closes #19676
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
341737448,21940,[BLD] Fix remaining compile-time warnings,jbrockmendel,closed,2018-07-17T01:56:41Z,2020-04-05T17:42:44Z,"With the exception of Numpy-Deprecated-API-1.7 warnings that _any_ cython code produces, this fixes all remaining compiler warnings (... on py27, there's still a whole mess of them in py37).

Based on a little bit of profiling it looked like `npy_isnan(x)` gives a 40% perf improvement over `x != x`.  But profiling is hard, so who knows.  In the comment where npy_nan is imported there is a link to a discussion about it."
385977932,24006,WIP: multi-timezone handling for array_to_datetime,jbrockmendel,closed,2018-11-29T23:52:59Z,2020-04-05T17:42:58Z,"cc @mroeschke 

ATM `array_to_datetime` handles strings and datetime objects very differently, with `conversion.datetime_to_datetime64` picking up (some of) the slack.  This unifies the treatment of strings/datetimes within array_to_datetime, rendering `conversion_to_datetime64` (and some ugly try/excepts in `pd.to_datetime`) unnecessary.

As of now there are still 5 tests failing locally; resolving them will involve some design decisions.

1) This PR introduces cases where dateutil's UTC or dateutil tzoffsets are returned while the test expects the equivalent pytz object.  We can either a) try to convert them within `array_to_datetime` to more consistently return pytz objects, b) change the tests to expect the dateutil versions, or c) change the tests to not care.

2) In the status quo we do (and test) something weird:

```
vals = [pd.Timestamp('2011-01-01 10:00'), pd.Timestamp('2011-01-02 10:00', tz='US/Eastern')]
# i.e. one tz-naive, one tz-aware

>>> pd.to_datetime(vals)
DatetimeIndex(['2011-01-01 05:00:00-05:00', '2011-01-02 10:00:00-05:00'], dtype='datetime64[ns, US/Eastern]', freq=None)

# for strings we do something more reasonable

>>> pd.to_datetime([str(x) for x in vals])
Index([2011-01-01 10:00:00, 2011-01-02 10:00:00-05:00], dtype='object')
```
This PR changes the first call to behave like the second.

2b) We also need to decide whether datetime64 are considered naive or UTC within `array_to_datetime`"
393177247,24376,REF/TST: Collect Straggler Arithmetic Tests,jbrockmendel,closed,2018-12-20T19:16:16Z,2020-04-05T17:43:09Z,"After this AFAICT there are only 5 files left with possibly-misplaced arithmetic tests.  Leaving those for another pass since some of them are testing type-specific behavior so will need a careful look.

Moved tests are unchanged except for contextifying pytest.raises where appropriate.

There's a hackathon in Feb I'm vaguely planning to use as a time to try to de-duplicate/parametrize the tests that have been moved to tests.arithmetic with TODO notes."
393800733,24405,implement astype portion of #24024,jbrockmendel,closed,2018-12-24T02:58:27Z,2020-04-05T17:43:34Z,"and accompanying tests

Uses _eadata like #24394

Constitutes ~10% of the diff of #24024, more after that gets rebased."
393297979,24378,implement ensure_localized in datetimelikeArrayMixin,jbrockmendel,closed,2018-12-21T03:24:02Z,2020-04-05T17:43:39Z,"This method is used by `TimelikeOps._round`, so needs to be implemented on the array mixin before we can move forward on #24064.

@TomAugspurger this places the method in a different location than you did in #24024 and made a small docstring edit, but otherwise should be compatible."
398726761,24761,REF/TST: Collect DataFrame Reduction Tests,jbrockmendel,closed,2019-01-14T02:01:43Z,2020-04-05T17:44:11Z,"Tests are mostly unchanged: avoid singleton fixtures, use context syntax for pytest.raises."
397663990,24695,remove unused kwarg,jbrockmendel,closed,2019-01-10T04:06:08Z,2020-04-05T17:44:11Z,"AFAICT `klass` is never passed to `astype`, so the kwarg can be removed.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
517927753,29416,maybe_promote: Restrict fill_value to scalar for non-object dtype,jbrockmendel,closed,2019-11-05T17:45:37Z,2020-04-05T17:44:46Z,"Partially reverts #29362 by allowing non-scalar fill_value for _object_ dtypes.  i.e. in 0.25.3 `pd.Series(range(3), dtype=object).shift(1, fill_value={})` would work, #29362 broke that, and this restores it.  Added `test_shift_object_non_scalar_fill` for this.

With the new restriction on `maybe_promote` in place, we can get rid of all the `box` tests and simplify test_promote a _ton_.  This removes about 2500 tests.  This also uncovers the fact that we were failing to run some of the non-box cases, which are now xfailed."
512773525,29232,CLN: assorted cleanups,jbrockmendel,closed,2019-10-26T01:21:52Z,2020-04-05T17:44:57Z,
502256307,28774,PERF: block-wise ops for scalar and series,jbrockmendel,closed,2019-10-03T19:34:56Z,2020-04-05T17:45:04Z,"This extends #28583 to handle `op(frame, series)` (whereas 28583 handles only `op(frame, scalar)`).

I got this passing locally, haven't gone back through to do e.g. lint fixups.  There are a few places where this surfaced bugs that can be fixed independently.

"
497288245,28583,PERF: Implement DataFrame-with-scalar ops block-wise,jbrockmendel,closed,2019-09-23T19:38:24Z,2020-04-05T17:45:11Z,"One of four cases we'll need to implement (the others being Series-align-index, Series-align-columns, and DataFrame).

~670x speedup on the fastest ops, ~8x on the slower end.
```
In [3]: arr = np.arange(10**5).reshape(100, 1000)                                                               
In [4]: df = pd.DataFrame(arr)                                                                                  
In [5]: %timeit df + 1                                              
198 ms ± 2.17 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)         # <-- master                                            
294 µs ± 3.25 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  # <-- PR

In [8]: ts = pd.Timestamp.now(""UTC"")                                                                            
In [9]: df2 = pd.DataFrame(arr.view(""timedelta64[ns]""))                                                         
In [10]: %timeit ts - df2                                                                                       
319 ms ± 2.49 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)      # <-- master
40.2 ms ± 622 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)   # <-- PR
```

The 2D DTA and TDAs that get created exist only briefly, and the changes to these classes are about as minimal as I can make them."
575915362,32452,"CLN: avoid _ndarray_values, values in MultiIndex",jbrockmendel,closed,2020-03-05T00:36:55Z,2020-04-05T17:45:35Z,"There are a couple of places in MultiIndex left after this that are a little trickier, will do in a separate pass."
574953244,32422,CLN: avoid values_from_object in NDFrame,jbrockmendel,closed,2020-03-03T21:20:54Z,2020-04-05T17:45:35Z,xref #32419
564278337,31940,REF: use iloc instead of _ixs outside of indexing code,jbrockmendel,closed,2020-02-12T21:18:37Z,2020-04-05T17:45:44Z,"Readers are much more likely to be familiar with iloc than _ixs.

This leaves us with exactly one usage of _ixs."
577436820,32538,ENH: IntegerArray.astype(dt64),jbrockmendel,closed,2020-03-08T03:09:38Z,2020-04-05T17:46:02Z,"- [x] closes #32435
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Let's us de-kludge to_datetime code, getting rid of another _ndarray_values usage."
589433791,33081,CLN: avoid internals in DataFrame.sort_values,jbrockmendel,closed,2020-03-27T21:59:51Z,2020-04-05T17:48:02Z,
588821434,33053,REF: MultiIndex Indexing tests,jbrockmendel,closed,2020-03-27T01:17:19Z,2020-04-05T17:48:12Z,Trying to make the tests.indexes.foo.test_indexing files cover a uniform collection of methods
593543099,33265,TST: add date range test for reindex case from GH-32740 ,BenjaminLiuPenrose,closed,2020-04-03T17:48:39Z,2020-04-05T18:28:26Z,"- [x] closes #32740 
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
594070057,33294,REF: BlockManager.combine -> _combine,jbrockmendel,closed,2020-04-04T20:24:10Z,2020-04-05T19:44:07Z,
594081824,33295,REF: make kwargs explicit in BlockManager methods,jbrockmendel,closed,2020-04-04T20:36:12Z,2020-04-05T19:44:39Z,
585877545,32922,DEPR: Index.is_mixed,jbrockmendel,closed,2020-03-23T02:11:01Z,2020-04-05T19:53:13Z,"We only use it in one place, and removing it there doesn't break anything.  Moreover, it behaves surprisingly:

```
>>> pd.Index(['a', np.nan, 'b']).is_mixed()
True

>>> Index([0, 'a', 1, 'b', 2, 'c']).is_mixed()
False
```"
594048504,33291,DEPR: Index.is_mixed,jbrockmendel,closed,2020-04-04T20:01:59Z,2020-04-05T19:54:26Z,"- [x] closes #32922
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
594067588,33293,CLN: remove BlockManager.__contains__,jbrockmendel,closed,2020-04-04T20:21:36Z,2020-04-05T20:08:46Z,
591303120,33187,ADMIN: Create separate issue templates for different usecases,ShaharNaveh,closed,2020-03-31T17:54:33Z,2020-04-06T08:24:06Z,"revival of #31551 originally opened by @jschendel 

----

> I recall this being mentioned on one of the core dev calls a few months back, and took inspiration from [rapidsai/cudf](https://github.com/rapidsai/cudf) for the various templates.
> 
> Added templates for the following use cases:
> 
>     * Bug Report
>       
>       * This is largely the same as the current issue template
> 
>     * Documentation Enhancement
> 
>     * Documentation Error
> 
>     * Feature Request
> 
>     * Submit Question
>       
>       * This attempts to direct users to StackOverflow for usage questions
> 
> 
> We could also add a Blank Template for opening an issue without any template provided, but opted not to do that for now, as I don't want to encourage people to bypass these templates. Could certainly add one if there's a consensus that we want this.
> 
> I've created a [local repo](https://github.com/jschendel/pandas-templates/issues) where you can see these templates in action since I couldn't figure out another way to actually display these. Click the ""New Issue"" button to see what these changes would look like.
> 
>

"
593577571,33268,TYP: Fixed type annotaions in `scripts/validate_rst_title_capitalization`,ShaharNaveh,closed,2020-04-03T18:56:51Z,2020-04-06T08:26:58Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
593594576,33271, CLN: Added static types _libs/algos,ShaharNaveh,closed,2020-04-03T19:31:02Z,2020-04-06T08:28:14Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
593921130,33285,DOC: Mention Index.equals takes into account order of arguments,jlandercy,closed,2020-04-04T15:21:05Z,2020-04-06T08:29:11Z,"- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of pandas.
- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
pd.Index(['10607', '6574', '6621', '99917']).equals(pd.Index(['99917', '10607', '6574', '6621'])) # False
set(['10607', '6574', '6621', '99917']) == set(['99917', '10607', '6574', '6621']) # True
```

#### Problem description

I have seen #13708, but according to documentation:

> Determine if two Index objects contain the same elements.

Should it return `True`  instead?

Reading how the function is implemented:

https://github.com/pandas-dev/pandas/blob/3adf3340453d6704d4a2cb47058214cc697a7d29/pandas/core/indexes/base.py#L4072-L4074

It relies on:

https://github.com/pandas-dev/pandas/blob/3adf3340453d6704d4a2cb47058214cc697a7d29/pandas/core/dtypes/missing.py#L421-L431

Which is indeed strict and do take order into account.

#### Expected Output

As I understand the equality of index is about having the same elements. It is not required the elements having the same order. I have the feeling that once pandas worked this way (maybe before `v0.23`).

What is the intended way ? Maybe the documentation should be a bit more explicit about this comparison. If you indicate me where I can update the doc, I will be glad to edit it.

#### Output of ``pd.show_versions()``

<details>

commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-91-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 9.0.1
setuptools       : 46.1.3
Cython           : 0.29.14
pytest           : 5.3.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.3.4
html5lib         : 0.999999999
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
pytest           : 5.3.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : None
tabulate         : 0.8.3
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8
numba            : None

</details>
"
594008633,33289,DOC: Improved doc for `Index.equals`,ShaharNaveh,closed,2020-04-04T19:13:10Z,2020-04-06T08:29:43Z,"- [x] closes #33285
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
593554464,33266,DOC: Fixed examples in `pandas/core/window`,ShaharNaveh,closed,2020-04-03T18:10:47Z,2020-04-06T08:34:41Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
593404350,33260,DOC: Fixed examples in `pandas/core/accessor.py`,ShaharNaveh,closed,2020-04-03T14:01:04Z,2020-04-06T08:35:11Z,"Since this changes require an interactive Ipython shell (I think), we can't run check them from pytest.

This is what the docs looks like:

Before:

![Screenshot_20200403_170159](https://user-images.githubusercontent.com/50263213/78368644-9c0b8400-75cc-11ea-9061-24652f2bdfca.png)

After:
![register_acc](https://user-images.githubusercontent.com/50263213/78368530-71b9c680-75cc-11ea-8ab5-d2ee2eadc727.png)"
589865818,33126,CLN: Added static types,ShaharNaveh,closed,2020-03-29T19:33:37Z,2020-04-06T08:41:06Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
590916016,33169,CLN: Added static types,ShaharNaveh,closed,2020-03-31T08:46:57Z,2020-04-06T08:43:09Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

I'll revert any change related to styling, I applied ""black"" formatting in some places, just so I could think straight."
592219644,33225,CI: Checking all the examples in `pandas/core/series.py`,ShaharNaveh,closed,2020-04-01T21:38:23Z,2020-04-06T08:43:47Z,"Since all the examples are passing, there is no reason not to check for all the examples (IMO).
"
558542216,31538,TST: Added regression test,ShaharNaveh,closed,2020-02-01T13:36:47Z,2020-04-06T08:46:06Z,"- [x] closes #10329
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
584711988,32840,DOC: use new pydata-sphinx-theme name,jorisvandenbossche,closed,2020-03-19T21:31:13Z,2020-04-06T09:26:08Z,"Following https://github.com/pandas-dev/pydata-sphinx-theme/issues/102, need to use the new package name

We could actually also start using the released version. But going to merge this soon when CI passes (as I think doc build in other PRs will start to fail)"
558538812,31536,TST: Added regression test case,ShaharNaveh,closed,2020-02-01T13:11:59Z,2020-04-06T09:49:03Z,"Resurrection of #28966

- [x] closes #18549
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
561722585,31784,BUG: DataFrame._item_cache not cleared on on .copy(),tommontroy,closed,2020-02-07T15:54:02Z,2020-04-06T14:35:32Z,"#### Code Sample, a copy-pastable example if possible

import copy
import pandas as pd
import platform

print('Python Version', platform.python_version())
print('Pandas Version', pd.__version__)

a = [1.]
    
d = {}
d['a'] = a 
df = pd.DataFrame(d)

print('Initial data frame')
print(df)
print('------')

\# This statement must be before the print statement
\# Removing this causes the bug to disappear
df['x'] = 0.0

\# Removing this causes the bug to disappear
print(""df['a'].values="", df['a'].values)
print('-----')

\# Removing the copy causes the bug to disappear
\# copy.deepcopy(df) or df.copy() leads to the same result
\#df1 = copy.deepcopy(df) 
df1 = df.copy()

\# set the first element of 'a' to value 
value = -1.0
df['a'].values[0] = value

print(""After setting df['a'].values[0] = %f"" % (value))
print(df)
print('-------')

print(""df['a'].values="", df['a'].values)
print('-------')

\# Removing this causes the bug to disappear
df['y'] = 0.0

print('Final')
print(df)
print('-------')

print(""df['a'].values="", df['a'].values)
    
#### Problem description

First, we build a dataframe from a dictionary containing a list. 

After that, we add  a column, do add some print statements, copy the dataframe. The addition of the column and print statement are crucial to the bug showing up. The fact that the print statement matters leads me to believe that it's memory issue.

The critical point is that we set df['a'].values[0] = -1.0 which is somehow not permanent.

Lastly we add another column. This column addition is necessary for the issue to occur.

In short the issue looks like this:

a = [1.0]

df = pd.DataFrame({'a': a})
df['x'] = 0.0 # necessary for bug
print(df['a'].values) # necessary for bug
df1 = df.copy()
df['a'].values[0] = -1.0 # this is the statement which is ignored
print(df['a'].values)
print(df)
df['y'] = 0.0 # necessary for bug
print(df['a'].values)

At this point, we'd expect df['a'].values = [-1.0], but in fact df['a'].values == 1.0 

I've tested this issue on a wide variety of versions. The last version where it works as expected is 0.16.2 and the version with the unexpected result is 0.17.1. I've tried this with python 3.6 / pandas=1.0.0 on Linux and python 3.7 / pandas 1.0.1 on OS X and get the same result. 


<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.0.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 17.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.15.1
pytz             : 2019.3
dateutil         : 2.7.3
pip              : 18.0
setuptools       : 40.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 6.5.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
None

</details>
"
584343504,32824,DOC: Updating capitalization in folder doc/source/reference,cleconte987,closed,2020-03-19T11:25:46Z,2020-04-06T14:49:43Z,"- [X] xref #32550
"
595070553,33318,Changed files permissions to be the same,ShaharNaveh,closed,2020-04-06T12:22:30Z,2020-04-06T15:05:44Z,"Not 100% sure about this change, feel free to close at anytime.
"
594993072,33316,CLN: Added static types for `pandas/_libs/reduction.pyx`,ShaharNaveh,closed,2020-04-06T10:26:24Z,2020-04-06T15:07:07Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
595190584,33326,CI: Doc build failing,TomAugspurger,closed,2020-04-06T15:01:36Z,2020-04-06T15:22:15Z,"Raw log: https://pipelines.actions.githubusercontent.com/xZyE9jtmkxWlfCAbyu1SHPJOlsa2huNFYcxohSTomy6EbdNZT9/_apis/pipelines/1/runs/25615/signedlogcontent/5?urlExpires=2020-04-06T15%3A02%3A02.2235397Z&urlSigningMethod=HMACV1&urlSignature=eEVJM7I2bg9PFf1apMIamBRrB1gG01K%2F12iKl%2Fe%2Bp5k%3D

```
2020-04-06T13:50:55.3962303Z looking for now-outdated files... none found
2020-04-06T13:50:55.6426712Z pickling environment... done
2020-04-06T13:50:55.6437496Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.BDay.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6440109Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.BMonthBegin.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6440725Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.BMonthEnd.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6441280Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.BQuarterBegin.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6441822Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.BQuarterEnd.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6442356Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.BYearBegin.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6442955Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.BYearEnd.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6443493Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.BusinessDay.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6444257Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.BusinessHour.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6444841Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.BusinessMonthBegin.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6445386Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.BusinessMonthEnd.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6445944Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.CBMonthBegin.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6448129Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.CBMonthEnd.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6448680Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.CDay.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6449227Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.CustomBusinessDay.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6449763Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.CustomBusinessHour.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6450302Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.CustomBusinessMonthBegin.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6450916Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.CustomBusinessMonthEnd.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6451574Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.DateOffset.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6453847Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.Day.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6456375Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.Easter.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6456758Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.FY5253.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6457151Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.FY5253Quarter.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6457538Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.Hour.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6465081Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.LastWeekOfMonth.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6465522Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.Micro.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6465912Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.Milli.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6466283Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.Minute.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6466680Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.MonthBegin.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6467071Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.MonthEnd.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6467648Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.MonthOffset.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6468033Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.Nano.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6468754Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.QuarterBegin.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6468876Z checking consistency... done
2020-04-06T13:50:55.6476990Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.QuarterEnd.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6477700Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.QuarterOffset.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6478266Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.Second.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6478810Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.SemiMonthBegin.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6479349Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.SemiMonthEnd.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6524416Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.SemiMonthOffset.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6525343Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.Tick.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6526474Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.Week.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6529160Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.WeekOfMonth.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6529727Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.YearBegin.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6530263Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.YearEnd.__call__.rst: WARNING: document isn't included in any toctree
2020-04-06T13:50:55.6530778Z /home/runner/work/pandas/pandas/doc/source/reference/api/pandas.tseries.offsets.YearOffset.__call__.rst: WARNING: document isn't included in any toctree
```"
595160227,33324,PERF: fix placement when slicing a Series,jorisvandenbossche,closed,2020-04-06T14:22:55Z,2020-04-06T16:11:08Z,"Closes https://github.com/pandas-dev/pandas/issues/33323
"
595157870,33323,PERF: performance regression in slicing a Series,jorisvandenbossche,closed,2020-04-06T14:19:57Z,2020-04-06T16:11:08Z,"All of the slice indexing benchmarks are showing regressions, eg https://pandas.pydata.org/speed/pandas/#indexing.NumericSeriesIndexing.time_loc_slice?p-index_dtype=%3Cclass%20'pandas.core.indexes.numeric.Int64Index'%3E&p-index_structure='unique_monotonic_inc'&commits=80d37adc-4f89c261

Replicating one of them with a small snippet confirms this:

```
In [1]: pd.__version__   
Out[1]: '1.1.0.dev0+1122.gc7c640ec7'

In [2]: N = 10 ** 6  

In [3]: data = pd.Series(np.random.rand(N), index=pd.Int64Index(range(N))) 

In [4]: %timeit data.iloc[:800000] 
103 ms ± 8.33 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [5]: %timeit data[:800000] 
97.2 ms ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

vs

```
In [1]: pd.__version__
Out[1]: '1.0.3'

In [2]: N = 10 ** 6  

In [3]: data = pd.Series(np.random.rand(N), index=pd.Int64Index(range(N))) 

In [4]: %timeit data.iloc[:800000]  
55.7 µs ± 2.72 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)

In [5]: %timeit data[:800000]   
69.6 µs ± 2.48 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```

Checking with snakeviz shows that on master, quasi all time the indexing operation takes is spent in setting the `mgr_locs` (creating the BlockPlacement object), which is certainly not as expected:

![image](https://user-images.githubusercontent.com/1020496/78568022-cefa8580-7821-11ea-9fc8-6f0e55e8d4d0.png)

Putting a breakpoint in the Block init just before creating the BlockPlacement, showed that the value being passed on master is a `range` object, while this should be a `slice` object, I assume. 
Going up the stack to see where this range object is created, points to:

https://github.com/pandas-dev/pandas/blob/a9c105a7a6dfa210f2706e2d8df6a6222964ff26/pandas/core/internals/managers.py#L1571

which according to `git blame` is last touched by this PR: https://github.com/pandas-dev/pandas/pull/32421. And indeed, a range object was added there instead of a slice (other places in the PR *did* use a slice, so this was probably kind of a typo).

"
595091592,33320,DOC/CLN: Fix docstring typo,dsaxton,closed,2020-04-06T12:54:06Z,2020-04-06T16:13:13Z,
588834195,33054,REF: rename _data->_mgr,jbrockmendel,closed,2020-03-27T01:59:24Z,2020-04-06T18:07:37Z,"We have a long-term goal of reducing the exposed surface of the internals.  One difficulty is that it is not trivial to identify all the places that access the internal, in part because grepping for `._data` will turn up a ton of non-internals Index and EA attributes.  By renaming _data -> _mgr, we'll make it easier to identify places that we can de-internalize.

If this isn't a route we want to go down (in the past when I've pitched this it hasn't caught on), I'll try to keep this up-to-date so we an use the diff as the relevant reference."
593639736,33275,Added finalize benchmark,TomAugspurger,closed,2020-04-03T21:03:52Z,2020-04-06T18:36:01Z,This adds a benchmark for finalize. It scales with the number of `attrs`.
595337916,33335,Selenium By Python--> Getting error while running program,sammy0231,closed,2020-04-06T18:37:19Z,2020-04-06T19:58:11Z,"![selenium by python](https://user-images.githubusercontent.com/63258257/78593086-b14e1000-7863-11ea-805e-7f185ae202ae.PNG)

pip latest version is also installed"
594897492,33312,DOC: do not include type hints in signature in html docs,jorisvandenbossche,closed,2020-04-06T08:11:52Z,2020-04-06T20:55:44Z,See https://github.com/pandas-dev/pandas/issues/33025. Removing the type hints in the online docs until we have a better solution to improve readability.
569315899,32173,can't create DataFrame with multiIndex by list of list,redHairGhost,closed,2020-02-22T10:36:01Z,2020-04-06T21:15:19Z,"#### Code Sample, a copy-pastable example if possible

```python
df=DataFrame([1,2,3,4],[4,5,6,7], columns= [list('abcd'), list('cdef')])

```
#### Problem description
when creating DataFrame by list of list with multiIndex, raise ValueError. I'm new to pandas, I don't know why, but  it seems like a bug...

`d=DataFrame([[1,2,3,4],[4,5,6,7]], columns= [list('abcd'), list('cdef')])`

```
Traceback (most recent call `last):

 File ""<ipython-input-70-b1022cf50f33>"", line 1, in <module>
    d=DataFrame([[1,2,3,4],[4,5,6,7]], columns= [list('abcd'), list('cdef')])

  File ""D:\Users\tangliu\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py"", line 450, in __init__
    arrays, columns = to_arrays(data, columns, dtype=dtype)

  File ""D:\Users\tangliu\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py"", line 464, in to_arrays
    return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)

  File ""D:\Users\tangliu\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py"", line 504, in _list_to_arrays
    raise ValueError(e) from e

ValueError: 2 columns passed, passed data had 4 columns`
It's ok using numpy ndarray or something else
```
```
d=DataFrame([[1,2,3,4],[4,5,6,7]])

d

Out[72]: 
   0  1  2  3
0  1  2  3  4
1  4  5  6  7

d.columns=[list('abcd'),list('cdef')]

d
Out[74]: 
   a  b  c  d
   c  d  e  f
0  1  2  3  4
1  4  5  6  7
```
it's ok when using numpy's ndarray

```
d=DataFrame(np.random.randn(6,4), columns=[list('abcd'), list('cdef')])

d
Out[81]: 
          a         b         c         d
          c         d         e         f
0 -0.066216 -1.264157 -1.199212 -0.184394
1  0.413666 -0.310750  1.138304 -0.271516
2  0.263854  0.827858  0.652344 -0.024369
3  0.041060 -2.235779  0.707691  0.319255
4 -0.118407 -1.278394  0.398972  0.597036
5  0.048947  0.697497  2.168712  0.650022
```"
569512775,32202,BUG: DataFrame fail to construct when data is list and columns is nested list for MI,charlesdong1991,closed,2020-02-23T15:35:13Z,2020-04-06T21:15:25Z,"- [ ] closes #32173 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"

id,number,title,user,state,created_at,updated_at,body
445750505,26449,Annotate to_json,WillAyd,closed,2019-05-18T20:15:32Z,2020-01-16T00:34:46Z,"Simple annotations as I was looking at the code

@gwrome and @vaibhavhrt "
445779122,26457,WIP: Add indent support in to_json,WillAyd,closed,2019-05-19T04:31:48Z,2020-01-16T00:34:47Z,"- [X] closes #12004
- [ ] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This is a WIP and requires a little more refinement (namely around how `level` is incremented in the extension module, testing arrays) but is more or less the direction I think we could head in

This is mostly inspired by changes done in https://github.com/esnme/ultrajson/commit/930dfa5525219d52c8422cf416ca308bc7382870#diff-edae06c872e58ad6edf574f4414aac9f
"
446249028,26473,Excel Test Cleanup - ReadWriteClass,WillAyd,closed,2019-05-20T18:21:47Z,2020-01-16T00:34:48Z,"The tests we have in this module need a pretty big refactor as they mix a few testing idioms together. This is slowing down contributions on items like #25092 and #25427

This is going to require quite a few PRs to get it where it needs to be. This one simply:

 - Moves tests that require both reading and writing into a dedicated class
 - Moves skip fixtures off of the SharedItems class onto the classes that actually use them

I'd eventually like to get rid of the SharedItems class and use parametrization on Reader/Writer classes to test the various combinations of engines and extensions, though again going to take a few PRs to get there"
450455945,26579,Remove SharedItems from test_excel,WillAyd,closed,2019-05-30T18:57:20Z,2020-01-16T00:34:49Z,"Pretty big step towards logically partitioning tests in this module - replaced SharedItems and instance attributes with fixtures. Should make subsequent test reorg easier

@simonjayhawkins "
451953662,26645,MyPy.ini Make Import Machinery Explicit,WillAyd,closed,2019-06-04T11:56:04Z,2020-01-16T00:34:50Z,"Now that we've wound down a lot of the mistyped annotations in the code base the next logical review point is our import machinery. Previously we used `ignore_missing_imports` for everything in the code base, but this is suggested by Mypy only as a last resort:

https://mypy.readthedocs.io/en/latest/running_mypy.html#missing-imports

This PR makes the missing imports explicit. Third party libraries I think we'll have to keep until they become available in typeshed. Internal imports may be resolved through either stubfiles or internal import machinery (would need review as follow ups)"
454685224,26782,Change Scope of xlrd Import Attempt,WillAyd,closed,2019-06-11T13:17:05Z,2020-01-16T00:34:51Z,Moved attempt to import xlrd to global scope of module. Simplifies code a bit and keeps xlrd relevant imports within one block
460947609,27048,Use mypy default follow import strategy,WillAyd,closed,2019-06-26T12:41:28Z,2020-01-16T00:34:52Z,"This was put in place as we were fixing up old annotations but isn't required at this point, so can safely remove and stick with mypy defaults (which is more strict)
"
462107925,27097,Enabled stricter type checking,WillAyd,closed,2019-06-28T16:10:16Z,2020-01-16T00:34:53Z,"Considered a ""mistake"" in the original PEP 484 using the implicit `Optional[]` on parameters with a default argument of None this PR enables a mypy flag that doesn't do that and checks types more strictly"
462236624,27116,Bump python_requires to 3.5.3,WillAyd,closed,2019-06-28T23:15:36Z,2020-01-16T00:34:54Z,"Mentioned during sprint today some of the typing constructs we use were not added until Python 3.5.2, specifically typing.Type and typing.DefaultDict and maybe at some point typing.TYPE_CHECKING

https://docs.python.org/3/library/typing.html#typing.Type

This is a bump of the setup files and docs to 3.5.2. "
468229847,27402,Easy warning fixups for mypy,WillAyd,closed,2019-07-15T16:46:40Z,2020-01-16T00:34:59Z,"xref #27396 @simonjayhawkins some of these were just easy cleanups. 

Didn't look through all but some of the others are more complicated and probably require in depth review"
462148627,27101,CLN/DEPR: Final panel removal,WillAyd,closed,2019-06-28T18:03:18Z,2020-01-16T00:35:00Z,"- [ ] closes #25632
- [ ] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
468883453,27426,Reallow usecols to reference OOB indices - reverts 25623,WillAyd,closed,2019-07-16T21:58:23Z,2020-01-16T00:35:03Z,"- [x] closes #27252
reverts #25623

@heckeop 

@gfyoung I know you asked for a FutureWarning to be raised but I didn't want to mess around with the validation_function in place here so just did a simple revert for sake of time / effort
"
462764356,27166,Removed block context from objToJSON,WillAyd,closed,2019-07-01T15:01:15Z,2020-01-16T00:35:05Z,"- [X] closes #27164
- [ ] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Getting one failure locally which will check later but pushing to see on other platforms"
472004763,27550,Revert CI changes from 27542,WillAyd,closed,2019-07-23T23:03:08Z,2020-01-16T00:35:06Z,"- [X] closes #27546
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Think this was all a perfect storm of temporary issues, so let's see what reverting yields. 

Note: didn't revert the sudo tag removal on purpose"
473131674,27603,Removed get_value benchmarks,WillAyd,closed,2019-07-26T01:10:11Z,2020-01-16T00:35:07Z,"Follow up to #27377 @jbrockmendel 
"
476292186,27719,Removed continued parametrization of FilePathOrBuffer,WillAyd,closed,2019-08-02T17:54:39Z,2020-01-16T00:35:08Z,"follow up to #27598 to simplify things a bit. I think it was a mistake to add `IO[AnyStr]` as `AnyStr` is a TypeVar and not something to actually parametrize with. The source for `IO` already does this:

https://github.com/python/cpython/blob/8990ac0ab0398bfb9c62031288030fe7c630c2c7/Lib/typing.py#L1452

 I'm not sure why mypy doesn't error on this. 

@simonjayhawkins "
488341035,28259,Fix to_json Memory Tests,WillAyd,closed,2019-09-03T00:04:39Z,2020-01-16T00:35:11Z,"Inspired by @mroeschke I noticed the JSON tests weren't actually measuring anything because you need to return something for the `mem_` tests which these weren't. In any case probably better served as `peakmem_`
"
524516421,29693,Removed compat_helper.h,WillAyd,closed,2019-11-18T17:41:11Z,2020-01-16T00:35:17Z,"ref #29666 and comments from @jbrockmendel and @gfyoung looks like the compat header can now be removed, now that we are on 3.6.1 as a minimum"
524798398,29704,Assorted io extension cleanups,WillAyd,closed,2019-11-19T06:16:01Z,2020-01-16T00:35:18Z,Just giving these a look seem to be a lot of unused definitions / functions
509737132,29124,Fixed Inconsistent GroupBy Output Shape with Duplicate Column Labels,WillAyd,closed,2019-10-21T05:23:50Z,2020-01-16T00:35:20Z,"- [X] closes #21668 (and potentially a few others)
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
526050368,29753,Less blocks in groupby,WillAyd,closed,2019-11-20T19:08:23Z,2020-01-16T00:35:22Z,"This isn't complete (2 tests fail) but this is arguably controversial, so putting up for review.

With #29124 we *could* get rid of the DataFrame aggregation methods that go by blocks. This would hopefully make our code easier to grok and move towards only ""one way of doing things"" in groupby.

This causes a definite regression for very wide DataFrames with a small number of rows. If you look at the new benchmark created, when testing Frames of N rows x 10_000 columns where N is one of 100, 1000 or 10_000 here are the results:

```sh
       before           after         ratio
     [6b3ba986]       [124a135a]
     <categorical-regression>       <easier-groupby>
+      3.94±0.2ms        2.55±0.1s   646.23  groupby.GroupByWide.time_wide(100)
+        38.3±6ms       2.49±0.02s    64.91  groupby.GroupByWide.time_wide(1000)
```

So drastically slower at 100 rows, but we've already reached a break-even point by the time we get to 10,000 rows.

The rest of the benchmarks don't cover wide data frames that well but here are the results for posterity (I think these are noise)

```sh
       before           after         ratio
     [6b3ba986]       [124a135a]
     <categorical-regression>       <easier-groupby>
+        862±30μs       1.15±0.1ms     1.34  groupby.GroupByMethods.time_dtype_as_field('int', 'cummin', 'direct')
+         412±7μs         533±50μs     1.29  groupby.GroupByMethods.time_dtype_as_group('float', 'tail', 'direct')
+        300±10μs         366±10μs     1.22  groupby.GroupByMethods.time_dtype_as_group('object', 'all', 'direct')
+        375±10ms          455±6ms     1.21  groupby.GroupByMethods.time_dtype_as_group('float', 'skew', 'transformation')
+         287±6μs         335±20μs     1.17  groupby.GroupByMethods.time_dtype_as_group('object', 'shift', 'direct')
+         252±2ms         290±10ms     1.15  groupby.GroupByMethods.time_dtype_as_group('float', 'unique', 'direct')
+         414±3μs         469±10μs     1.13  groupby.GroupByMethods.time_dtype_as_group('float', 'tail', 'transformation')
+         398±9ms          450±8ms     1.13  groupby.GroupByMethods.time_dtype_as_group('float', 'skew', 'direct')
```

@jbrockmendel "
545846834,30746,CI: Using docstring validator from numpydoc,datapythonista,closed,2020-01-06T17:32:37Z,2020-01-16T02:01:23Z,"- [X] xref #28822
- [x] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

We moved our script to numpydoc, and it already had some improvements there. The script does like 80% of our validation, so what I'm doing here is to call numpydoc validation, and then call our custom validation (things that for different reasons weren't moved to numpydoc)."
550264026,31045,TST: Disallow bare pytest.raises,gdex1,closed,2020-01-15T15:38:54Z,2020-01-16T09:12:20Z,"- [ ] ref #30999 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This pull request is to add appropriate match arguments to bare pytest.raises listed in the comment https://github.com/pandas-dev/pandas/issues/30999#issuecomment-574174389 as described by the referenced issue."
550643248,31067,pd.to_datetime() time conversion error,chunxueyu,closed,2020-01-16T08:11:40Z,2020-01-16T11:52:16Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here

```
#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
549374157,30995,BUG: Fix Timestamp constructor changes value on ambiguous DST,AlexKirko,closed,2020-01-14T07:04:46Z,2020-01-16T12:40:16Z,"- [X] closes #24329
- [X] 1 tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
550556102,31062,"TYP/CLN: Replaced ""Optional[Hashable]"" with ""Label"" from pandas._typing",ShaharNaveh,closed,2020-01-16T03:45:13Z,2020-01-16T14:58:49Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
550462017,31056,CLN: assorted cleanups,jbrockmendel,closed,2020-01-15T22:19:33Z,2020-01-16T16:30:12Z,
550573755,31065,CLN: remove redundant return value,jbrockmendel,closed,2020-01-16T04:53:51Z,2020-01-16T16:35:04Z,
550887488,31076,STY: Some stuff that triggers my OCD ;),ShaharNaveh,closed,2020-01-16T15:38:57Z,2020-01-16T18:41:33Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

 Some stuff that got the attention of my eye"
550525496,31059,CLN: Removed unused varibles from for loops,ShaharNaveh,closed,2020-01-16T01:46:28Z,2020-01-16T18:42:41Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
550990468,31081,CLN: Moved the same code in 'if' and the 'else' outside,ShaharNaveh,closed,2020-01-16T18:50:48Z,2020-01-16T19:52:14Z,"AFICT this should be ok, right?

cc (@WillAyd)"
548241582,30898,DOC: Add pandas_path to the accesor list in the documentation,pjbull,closed,2020-01-10T19:06:19Z,2020-01-16T20:00:11Z,"Added the [`pandas_path`](https://github.com/drivendataorg/pandas-path/) package that I just released to the list of third-party accessor extensions.

As a bonus, added a Description column for the existing libraries."
548516808,30936,ENH: Add engine keyword argument to expanding.apply to leverage Numba,mroeschke,closed,2020-01-12T03:45:01Z,2020-01-16T21:15:38Z,"In 1.0, we're adding a `engine` keyword argument to `rolling.apply` to run the routine using Cython or Numba.

By natural extension, we should allow `expanding.apply` to utilize Numba by adding the same `engine` keyword argument to `expanding.apply`"
548520159,30937,ENH: Add engine keyword to expanding.apply to utilize Numba,mroeschke,closed,2020-01-12T04:36:31Z,2020-01-16T21:15:52Z,"- [x] closes #30936
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
507989311,29038,REF/PERF: PeriodEngine covers narrow type,proost,closed,2019-10-16T17:22:21Z,2020-01-16T21:36:21Z,"#### Code Sample, a copy-pastable example if possible

```python
pi = pd.period_range(start=2000, end=2019, freq=""A"")

p = pd.Period(2010, freq=""A"")

In [7]: pi._engine.get_loc(p)                                                   
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-7-cd77985ba7c9> in <module>
----> 1 pi._engine.get_loc(p)

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index_class_helper.pxi in pandas._libs.index.Int64Engine._check_type()

KeyError: Period('2010', 'A-DEC')


```
#### Problem description

When i handled #28628, i found `PeriodEngine` only works for Int type. So `PeriodIndex.get_loc` compute twice because `PeriodEngine.get_loc` only accepts Int type. 


#### Suggestion
`.get_loc` on `PeriodIndex` computes once. Either expanding `PeriodEngine.get_loc` accept data type or removeing `PeriodEngine.get_loc` are ok


#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.0.0-31-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : ko_KR.UTF-8
LOCALE           : ko_KR.UTF-8

pandas           : 0.25.1
numpy            : 1.16.1
pytz             : 2018.7
dateutil         : 2.7.5
pip              : 19.3
setuptools       : 40.6.3
Cython           : 0.29.2
pytest           : 5.1.0
hypothesis       : None
sphinx           : 1.8.2
blosc            : None
feather          : None
xlsxwriter       : 1.1.2
lxml.etree       : 4.2.5
html5lib         : 1.0.1
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.2.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.5
matplotlib       : 3.0.2
numexpr          : 2.6.8
odfpy            : None
openpyxl         : 2.5.12
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : 1.2.15
tables           : 3.4.4
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.2

</details>
"
548195046,30895,CLN: remove unnecessary arg from _to_dt64,jbrockmendel,closed,2020-01-10T17:13:56Z,2020-01-16T21:46:04Z,
551002161,31083,"CLN: Remove download_wheels.py, moved to pandas-release",datapythonista,closed,2020-01-16T19:15:37Z,2020-01-16T21:46:34Z,"- [X] xref #31039

Script being moved to the pandas-release repository in https://github.com/pandas-dev/pandas-release/pull/23"
548255406,30899,CLN: simplify Float64Index.__contains__,jbrockmendel,closed,2020-01-10T19:39:59Z,2020-01-16T21:48:12Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
489686761,28293,Different behavior of read_csv on Windows with Anaconda and Ubuntu 18,jumpingfella,closed,2019-09-05T11:14:01Z,2020-01-16T22:38:33Z,"Hello, 

I'm experiencing different behavior of the following script:
https://stackoverflow.com/questions/57791115/forecast-produced-by-gluon-ts-example-is-around-0

on Windows with Anaconda and on Ubuntu 18. It works as expected on Windows, but doesn't on Ubuntu 18 (produces plot around 0)

This seems to have something to do with `df = pd.read_csv(url, header=0, index_col=0)`"
551013748,31085,DOC: automatic 'end' year of copyright,ShaharNaveh,closed,2020-01-16T19:41:07Z,2020-01-16T23:39:35Z,"#31022 Updated it after nearly 6 years, I think this will update with each new merge, so that should not be a problem from now on, (I think)
cc (@datapythonista)
"
545718349,30734,"TYP: _config/config.py && core/{apply,construction}.py",ShaharNaveh,closed,2020-01-06T13:10:35Z,2020-01-16T23:41:26Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
512794116,29234,PERF: PeriodEngine.get_loc accept narrow data type,proost,closed,2019-10-26T05:34:34Z,2020-01-17T00:23:29Z,"- [x] closes #29038
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

xref #28628. 
I want to refactor `PeriodIndex.get_loc`. Because `PeriodIndex.get_loc` compute twice `Int64index.get_loc` and `PeriodEngine.get_loc`. Problem is only `PeriodEngine.get_loc` accepts `Period.ordinal`. 
I try to change `PeriodIndex.get_loc` in two ways.
First one, I try to remove `PeriodEngine.get_loc` in `PeriodIndex.get_loc`. so just compute `.get_loc` once. I change several ways But always stuck in test cases. So I give it up this way.
Second one, I try to change `PeriodEngine.get_loc` to accept `Period` not only `Period.ordinal`. This way is success in that work successfully and pass all test cases. However Problem is most cases users don't use `Period` using `PeriodIndex.get_loc` and `PeriodEngine.get_loc` can't accept ""intolerance"" argument. Not using `Int64index.get_loc` can't be possible. 
But I send PR. Little improvement is also improvement. 

---------------------------------------------------------------------------------------------------------------------
improvement : memory usage with Period.get_loc 
"
550143379,31040,DOC: Add CZI to the sponsors page,datapythonista,closed,2020-01-15T11:59:14Z,2020-01-17T00:25:00Z,"I guess we want to add CZI to the sponsors page: https://pandas.io/about/sponsors.html

I think a section similar to Tidelift is the best option. Any other idea?"
550295005,31046,"WEB: Improving the sponsors in the web, and adding CZI",datapythonista,closed,2020-01-15T16:30:03Z,2020-01-17T00:25:01Z,"- [X] closes #31040

Things addressed here:
- Added CZI as sponsor
- Adding an arbitrary number of sponsor logos in the home (previously the style only worked for 6)
- Adding a `Become a sponsor` section to the sponsors page
- Moving all sponsors to the web yaml, and standardizing how we present them in the sponsors page

![Screenshot at 2020-01-15 16-22-57](https://user-images.githubusercontent.com/10058240/72451690-36211480-37b4-11ea-8564-ff5e421b468f.png)

![Screenshot at 2020-01-15 16-23-32](https://user-images.githubusercontent.com/10058240/72451699-39b49b80-37b4-11ea-99ad-50fabe965d9c.png)
"
551005998,31084,REF: stricter types for RangeIndex._simple_new,jbrockmendel,closed,2020-01-16T19:24:11Z,2020-01-17T01:08:23Z,xref #31055.
550964827,31078,CLN: remove unused legacy pickle compat code,jbrockmendel,closed,2020-01-16T17:57:13Z,2020-01-17T10:07:41Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
551266755,31099,Backport PR #31091 on branch 1.0.x (CI/TST: fix failing tests in py37_np_dev),ShaharNaveh,closed,2020-01-17T08:09:58Z,2020-01-17T13:21:03Z,"(Manual) Backport PR #31091 on branch 1.0.x (CI/TST: fix failing tests in py37_np_dev)

cc @WillAyd "
551261683,31098,CLN: Regular expression clean,ShaharNaveh,closed,2020-01-17T07:56:44Z,2020-01-17T13:21:58Z,"- [x] ref https://github.com/pandas-dev/pandas/pull/31091#discussion_r367732954
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

"
550302904,31047,DOC: Add missing docstrings,galuhsahid,closed,2020-01-15T16:43:31Z,2020-01-17T13:27:49Z,"Added docstrings:
```
pandas.Index.has_duplicates
pandas.Index.is_all_dates
pandas.Index.name
pandas.Index.is_boolean
pandas.Index.is_floating
pandas.Index.is_integer
pandas.Index.is_interval
pandas.Index.is_mixed
pandas.Index.is_numeric
pandas.Index.is_object
```

There are some other docstrings left - I couldn't find the docstrings below in the code:
```
None:None:GL08:pandas.Index.names:The object does not have a docstring
None:None:GL08:pandas.Index.empty:The object does not have a docstring
```

I don't really understand what the function does so I'm leaving the docstring empty for now:
```
pandas/pandas/core/indexes/base.py:639:GL08:pandas.Index.view:The object does not have a docstring
```

cc @datapythonista

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
545697515,30733,Fix SS03 issues in docstrings,galuhsahid,closed,2020-01-06T12:15:37Z,2020-01-17T13:50:21Z,"Fix the docstrings where summary does not end with a period.

Current errors found:
```
$ ./scripts/validate_docstrings.py --errors=SS03
None:None:SS03:pandas.tseries.offsets.DateOffset.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.BusinessDay.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.BusinessHour.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.CustomBusinessDay.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.CustomBusinessHour.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.MonthOffset.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.MonthEnd.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.MonthBegin.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.BusinessMonthEnd.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.BusinessMonthBegin.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.CustomBusinessMonthEnd.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.CustomBusinessMonthBegin.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.SemiMonthOffset.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.SemiMonthEnd.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.SemiMonthBegin.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.Week.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.WeekOfMonth.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.LastWeekOfMonth.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.QuarterOffset.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.BQuarterEnd.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.BQuarterBegin.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.QuarterEnd.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.QuarterBegin.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.YearOffset.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.BYearEnd.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.BYearBegin.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.YearEnd.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.YearBegin.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.FY5253.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.FY5253Quarter.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.Easter.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.Tick.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.Day.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.Hour.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.Minute.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.Second.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.Milli.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.Micro.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.Nano.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.BDay.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.BMonthEnd.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.BMonthBegin.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.CBMonthEnd.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.CBMonthBegin.normalize:Summary does not end with a period
None:None:SS03:pandas.tseries.offsets.CDay.normalize:Summary does not end with a period
None:None:SS03:pandas.Timestamp.isoweekday:Summary does not end with a period
None:None:SS03:pandas.Timestamp.weekday:Summary does not end with a period
None:None:SS03:pandas.DatetimeIndex.freqstr:Summary does not end with a period
None:None:SS03:pandas.PeriodIndex.freqstr:Summary does not end with a period
pandas/pandas/core/window/indexers.py:34:SS03:pandas.api.indexers.BaseIndexer:Summary does not end with a period
None:None:SS03:pandas.io.formats.style.Styler.loader:Summary does not end with a period
```"
551130055,31094,WEB: Styling blog,datapythonista,closed,2020-01-17T00:22:06Z,2020-01-17T16:51:21Z,"Improving how the blog section looks. Changing a bit the font size, color, and spacing. And also removing html tags from the content, since it's tricky to make titles, images, links... in the posts look good in the preview.

Not convinced that the blog looks great, but that's the best I could do, and I think it looks much better than what we've got now.

![Screenshot at 2020-01-17 00-17-27](https://user-images.githubusercontent.com/10058240/72573847-e418e580-38be-11ea-9ba8-f3b252aacd48.png)
"
551410839,31103,CLN: Index._values docstring + Block.internal/external_values,jorisvandenbossche,closed,2020-01-17T13:18:53Z,2020-01-17T17:55:46Z,"@jbrockmendel this is the ""clean"" part of https://github.com/pandas-dev/pandas/pull/31037, so if you merge this, I will update the other PR so that only the required changes are in the PR for 1.0.0."
550996353,31082,CLN: __all__ for core/index.py,ShaharNaveh,closed,2020-01-16T19:03:00Z,2020-01-17T17:59:01Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
535507185,30180,BUG: to_csv output formatting for datetimes,burkbre,closed,2019-12-10T05:42:10Z,2020-01-17T23:37:36Z,"closes #29711 

…ssue and provides rounding variable to csv formatter to handle datetimes differently. I decided to try and tackle this issue for one of the classes I am in at university. The issue seems to be stemming from the fact that BlockManager is grouping together two differently formatted datetime values in the DataFrame thus making it so the second datetime value is formatted like the first(leading to the trailing zeroes in the example). If you want to format all datetimes the same in the csv you can simply make use of the date_format attribute which the CsvFormatter takes as a variable to format the datetimes however you like. I decided to add a rounding_milliseconds attribute to the csv formatter which would remove  the milliseconds from the end of the datetime if they are all equal to '0'. 

- [x] tests added / passed
- [x] passes `black pandas`
"
274436415,18321,"pd.crosstab(s1, s2) keeps dummy MultiIndex as columns if both s1 and s2 have tuple name",toobaz,closed,2017-11-16T08:48:59Z,2020-01-17T23:57:04Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: s1 = pd.Series(range(3), name=('a', 'b'))

In [3]: s2 = pd.Series(range(3), name=('c', 'd'))

In [4]: pd.crosstab(s1, s2)
Out[4]: 
                   
('c', 'd')  0  1  2
(a, b)             
0           1  0  0
1           0  1  0
2           0  0  1

In [5]: list(pd.crosstab(s1, s2).columns)
Out[5]: [('', 0), ('', 1), ('', 2)]
```

#### Problem description

While the example is similar to that reported in #18304, the fix doesn't seem to be related. Reminder: test by expanding the test in #18309 .


#### Expected Output

``` python
In [5]: list(pd.crosstab(s1, s2).columns)
Out[5]: [0, 1, 2]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: 9c799e2c4331e5e42cfda03323eef165feb5be1a
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.0-3-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: it_IT.UTF-8
LOCALE: it_IT.UTF-8

pandas: 0.22.0.dev0+135.g9c799e2c4
pytest: 3.2.3
pip: 9.0.1
setuptools: 36.7.0
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.19.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: 1.5.6
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: 1.2.0dev
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.0.0
openpyxl: None
xlrd: 1.0.0
xlwt: 1.1.2
xlsxwriter: 0.9.6
lxml: None
bs4: 4.5.3
html5lib: 0.999999999
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.2.1


</details>
"
549121483,30978,"BUG: pd.crosstab(s1, s2) handle column index incorrectly when both series have tuple names",charlesdong1991,closed,2020-01-13T19:02:05Z,2020-01-17T23:57:10Z,"- [ ] closes #18321
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
551020793,31086,Issue generating a line plot from a Series when previous plots are already defined,RickOldenburger,closed,2020-01-16T19:55:39Z,2020-01-18T00:16:25Z,"#### Code Sample, a copy-pastable example if possible

```python
        df = self.getStats() # returns a dataFrame with a row called Rounds
        totalMatches = len(df)
        if (totalMatches == 0):
            print(""Need to play at least one match for a graph"", flush = True)
            return
        vc = df[""Rounds""].value_counts()
        x = vc.plot(legend=True, figsize=[10, 8])
        x.set_xlabel(""Number of Rounds to Win"")
        x.set_ylabel(""Occurrences"")
        x.set_title(""Value-Count Graph (Matches: "" + str(totalMatches) + "")"")
        
        Occurences = max(vc) + 1
        # for small values set y tic mark text
        if Occurences <= 20:
            yText = [] # Stores our Y-Axis text
            
            for i in range(1, Occurences):
                yText.append(f'${i}$')
            plt.yticks(np.arange(1, Occurences), yText, ha=""center"")
        
        plt.show()
```
#### The code above works fine unless the following has been executed to make ""popout"" graphs
%matplotlib qt5 

If a previous graph exists from a previous run, and is still up. Then this graph will have additions added directly to it. Please see the images for what is happening.

![Bug report for Pandas](https://user-images.githubusercontent.com/59935634/72558067-9b820d80-3867-11ea-9d69-cf424b63cc05.png)

It should be noted, I am using version 0.25.3 of pandas, and I searched and could not find this issue already logged.

1) The reason this is an issue is that it leads to potentially different results each time the program is executed. 
2) It only occurs with pandas Series objects. (I tested and it is not an issue for dataFrames.)

You can find the full python program at: 
https://github.com/RickOldenburger/War-Card-Game-Analysis

There is a work around and I have applied it in my program.
change the statement: 
x = vc.plot(legend=True, figsize=[10, 8])

to these two statments:
plt.figure(figsize=(10, 8))
x = vc.plot.line(legend=True) # no point in having the figsize defined twice


**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
496813439,28569,Bugfix/groupby datetime issue,sidharthann,closed,2019-09-22T18:14:59Z,2020-01-18T00:29:08Z,"- [x] closes #28247
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
545450713,30711,Datetime error message (unable to continue that PR),baevpetr,closed,2020-01-05T18:30:22Z,2020-01-18T12:47:41Z,"- [ ] closes #10720
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
551466958,31105,TST: Fix some bare pytest raises,ShaharNaveh,closed,2020-01-17T15:06:45Z,2020-01-18T14:50:41Z,"- [x] ref #30999
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Follow up for #30998"
548335878,30904,to_json doesn't work with Datetime.date,WillAyd,closed,2020-01-10T23:18:21Z,2020-01-18T15:34:13Z,"This is a regression between 0.25.3 and 1.0

```python
>>> import pandas as pd
>>> import datetime
>>> data = [datetime.date(year=2020, month=1, day=1), ""a""]
>>> pd.Series(data).to_json(date_format=""iso"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/williamayd/clones/pandas/pandas/core/generic.py"", line 2363, in to_json
    indent=indent,
  File ""/Users/williamayd/clones/pandas/pandas/io/json/_json.py"", line 85, in to_json
    indent=indent,
  File ""/Users/williamayd/clones/pandas/pandas/io/json/_json.py"", line 145, in write
    self.indent,
  File ""/Users/williamayd/clones/pandas/pandas/io/json/_json.py"", line 199, in _write
    indent,
  File ""/Users/williamayd/clones/pandas/pandas/io/json/_json.py"", line 167, in _write
    indent=indent,
TypeError: Expected datetime object
```

Noticed while working on #30903"
548821700,30958,pd.Series.sum() returns nan for Int64 with pd.NA,koizumihiroo,closed,2020-01-13T10:02:21Z,2020-01-18T15:35:40Z,"#### Code Sample, a copy-pastable example if possible

I'm not sure this is a bug or intended behavior. If intended, I'm glad to be pointed to how API design was discussed and decided.

On `1.0.0 rc0`, `pd.Series.sum()`returns `nan` when using `skipna=False` for `'Int64'` with `pd.NA`. 

```python
>>> pd.Series([1, 2, pd.NA]).sum(skipna=False)
<NA>
>>> pd.Series([1, 2, pd.NA], dtype='Int64').sum(skipna=False)
nan

>>> pd.Series([1, 2, np.nan]).sum(skipna=False)
nan
>>> pd.Series([1, 2, np.nan], dtype='Int64').sum(skipna=False)
nan

>>> pd.Series([1, 2, None]).sum(skipna=False)
nan
>>> pd.Series([1, 2, None], dtype='Int64').sum(skipna=False)
nan
```

#### Problem description

I understand the second and third example returns `nan` for backward compatibility, but the first example of `dtype='Int64'` seems natural to return `pd.NA`.

#### Expected Output

```python
>>> pd.Series([1, 2, pd.NA], dtype='Int64').sum(skipna=False)
<NA>

# When breaking backward compatibility is allowed, this is also expected (on and after pandas 2.0.0?)
>>> pd.Series([1, 2, np.nan], dtype='Int64').sum(skipna=False)
<NA>
>>> pd.Series([1, 2, None], dtype='Int64').sum(skipna=False)
<NA>
```


#### Output of ``pd.show_versions()``

<details>
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.9.184-linuxkit
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.0rc0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 44.0.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None


```Dockerfile
# Dockerfile
FROM python:3.7.6
WORKDIR /home
RUN pip install pandas==1.0.0rc0
CMD [""/bin/bash""]
```
</details>
"
551784215,31120,Backport PR #30977 on branch 1.0.x (JSON Date Handling 1.0 Regressions),meeseeksmachine,closed,2020-01-18T15:34:38Z,2020-01-18T16:03:34Z,Backport PR #30977: JSON Date Handling 1.0 Regressions
551489013,31106,CLN: prune unreachable code,jbrockmendel,closed,2020-01-17T15:46:28Z,2020-01-18T16:07:28Z,
551572586,31112,BUG: Index.get_value being called incorrectly,jbrockmendel,closed,2020-01-17T18:42:42Z,2020-01-18T16:09:12Z,"AFAICT there aren't user-facing consequences, just built-up kludges in `Series.__getitem__` and `Index.get_value` that we'll be able to get rid of once we fix these inconsistencies.

cc @jreback"
551705681,31115,TYP: annotations,jbrockmendel,closed,2020-01-18T01:59:25Z,2020-01-18T16:11:48Z,"Several of the Index subclasses are difficult to annotate, so I want to get the easy-ish parts to make it easier to focus on the pain points."
551120177,31089,CLN: update _simple_new usages,jbrockmendel,closed,2020-01-16T23:49:23Z,2020-01-18T16:14:58Z,"In preparation for making the _simple_new constructors stricter, xref #31084, #31055."
550567899,31064,BUG: partial-timestamp slicing near the end of year/quarter/month,jbrockmendel,closed,2020-01-16T04:33:04Z,2020-01-18T16:20:56Z,"I'm fairly confident we can refactor this to be a lot less verbose, but will do that separately from the bugfix.

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
548450445,30924,BUG: Wrong error message is raised when columns=None in df.pivot,charlesdong1991,closed,2020-01-11T16:31:15Z,2020-01-18T16:23:32Z,"From docstring, `index` and `values` can be optional in df.pivot, but `columns` is not

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot.html

```
index : string or object, optional
Column to use to make new frame’s index. If None, uses existing index.

columns : string or object
Column to use to make new frame’s columns.
```
However, the error message is confusing now, should raise `columns` is not optional.

Code example:
```python
>>> df = pd.DataFrame({""foo"": ['one', 'one', 'two', 'two'],""bar"": ['A', 'A', 'B', 'C'],""baz"": [1, 2, 3, 4]})
>>> df.pivot(columns=None)

KeyError: 'None of [None] are in the columns'
```
"
548450688,30925,BUG: correct wrong error message in df.pivot when columns=None,charlesdong1991,closed,2020-01-11T16:33:22Z,2020-01-18T16:23:36Z,"- [x] closes #30924 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
551784344,31121,Backport PR #30971 on branch 1.0.x (BUG: reductions for nullable dtypes should return pd.NA for skipna=False),meeseeksmachine,closed,2020-01-18T15:35:49Z,2020-01-18T16:23:52Z,Backport PR #30971: BUG: reductions for nullable dtypes should return pd.NA for skipna=False
550453102,31055,REF: be stricter about what we pass to _simple_new,jbrockmendel,closed,2020-01-15T21:58:35Z,2020-01-18T16:25:23Z,This will take a few passes to get rid of all the checks done in _simple_new that should be done elsewhere
549661990,31008,DOC: Moved PANDAS_TESTING_MODE tip to .travis.yml (#30694),saloni30agr,closed,2020-01-14T15:59:38Z,2020-01-18T16:57:52Z,"Added comments for travis.yml for PANDAS_TESTING_MODE from tips and tricks section of wiki

- [X] xref #30964
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
551029054,31087,DOC: replace ggpy with plotnine on the ecosystem page,jorisvandenbossche,closed,2020-01-16T20:12:08Z,2020-01-18T17:11:21Z,"We have an entry about ggpy in the visualization section of the ecosystem page (https://dev.pandas.io/docs/ecosystem.html#visualization). However, ggpy is no longer maintained (last commit is more than 3 years ago: https://github.com/yhat/ggpy). While plotnine is an actively developed alternative ggplot-python-interface (https://github.com/has2k1/plotnine/). 

So I think we can just replace the entry about ggpy with one about plotnine."
543705194,30553,TST/BUG: fix incorrectly-passing Exception in test_html,jbrockmendel,closed,2019-12-30T03:23:00Z,2020-01-18T17:24:11Z,A `raise` is indented one indent further than it should be.  Fixing this surfaces a failing test that is fixed by decoding bytes before passing it to bs4.
551797570,31124,Backport PR #30553 on branch 1.0.x (TST/BUG: fix incorrectly-passing Exception in test_html),meeseeksmachine,closed,2020-01-18T17:20:13Z,2020-01-18T18:53:15Z,Backport PR #30553: TST/BUG: fix incorrectly-passing Exception in test_html
551796523,31122,Backport PR #31097 on branch 1.0.x (DOC: Replace ggpy with plotnine in ecosystem),meeseeksmachine,closed,2020-01-18T17:11:31Z,2020-01-18T18:53:37Z,Backport PR #31097: DOC: Replace ggpy with plotnine in ecosystem
551800799,31125,REF/CLN: Index.get_value wrapping incorrectly,jbrockmendel,closed,2020-01-18T17:48:11Z,2020-01-18T19:54:03Z,"Index.get_value isn't wrapping non-scalar results correctly, as a result of which we have an ugly kludge in `Series.__getitem__` to do that wrapping.  This fixes that."
551796626,31123,CLN/BUG: Float64Index.get_loc,jbrockmendel,closed,2020-01-18T17:12:32Z,2020-01-19T00:29:29Z,"- clean up unnecessary try/except
- the ""bug"" part is that ATM we incorrectly raise KeyError instead of TypeError on non-hashable."
544438638,30614,ENH: Create a devcontainer.json env to work with VS Code in Containers,yehoshuadimarsky,closed,2020-01-02T03:56:56Z,2020-01-19T01:00:59Z,"VS Code has a great feature where you can work on a repo fully within a Docker container, either on your computer (called Remote - Containers) or remotely on Azure hosted by them (called VS Code Online). This could be particularly useful for setting up a quick dev environment to work on the pandas codebase, and not have to manually go through the setup steps in the [docs](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#working-with-the-code).

Per the VS Code docs (https://code.visualstudio.com/docs/remote/containers and https://docs.microsoft.com/en-us/visualstudio/online/reference/configuring) you can create a `.devcontainer.json` file in the root of the repo that will contain everything needed to setup the environment, similar to the conda `environment.yml` file. You can also create a DockerFile inside a `devcontainer` folder to use with the `devcontainer.json` file.

I will try to put something together for this, figured I'd first start an issue here so that if anyone else is working on this we can collaborate and not duplicate our work."
544841945,30638,ENH: Create DockerFile and devcontainer.json files to work with Docker and VS Code in Containers,yehoshuadimarsky,closed,2020-01-03T04:03:13Z,2020-01-19T03:53:33Z,"- [x] closes #30614 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The instructions aren't that well written, and should probably be improved, but here's a first shot."
551285940,31100,Updated years in LICENSE,ShaharNaveh,closed,2020-01-17T08:55:32Z,2020-01-19T06:24:48Z,"Really not sure about this change.

(Feel free to close this PR at any time, if this file should not be changed)."
499600003,28659,accept a dictionary in plot colors,Leostayner,closed,2019-09-27T18:58:27Z,2020-01-19T08:16:24Z,"- [ ] closes #8193
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Line/bar plot accepts color by dictonary.
@elisamalzoni"
551815831,31127,"Docs for `DataFrame.plot.scatter` say that `c` can be of type `int`, yet it's not explained what that means and passing one raises an error",MarcoGorelli,closed,2020-01-18T19:53:39Z,2020-01-19T08:19:14Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame(
    [[5.1, 3.5, 0], [4.9, 3.0, 0], [7.0, 3.2, 1], [6.4, 3.2, 1], [5.9, 3.0, 2]],
    columns=[""length"", ""width"", ""species""],
)
ax1 = df.plot.scatter(x=""length"", y=""width"", c=128)
```
returns
```
IndexError: index 128 is out of bounds for axis 0 with size 3
```
#### Problem description

The docstring reads

>         c : str, int or array_like, optional
>             The color of each point. Possible values are:
> 
>             - A single color string referred to by name, RGB or RGBA code,
>               for instance 'red' or '#a98d19'.
> 
>             - A sequence of color strings referred to by name, RGB or RGBA
>               code, which will be used for each point's color recursively. For
>               instance ['green','yellow'] all points will be filled in green or
>               yellow, alternatively.
> 
>             - A column name or position whose values will be used to color the
>               marker points according to a colormap.

but it doesn't say what passing a `c` of type `int` does.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 00c9b2df2dd5680eb13605f599e38a0f5950aeef
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-74-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.26.0.dev0+1837.g00c9b2df2
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.2
setuptools       : 41.0.1
Cython           : 0.29.14
pytest           : 5.0.1
hypothesis       : 4.32.2
sphinx           : 2.2.2
blosc            : 1.8.3
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.0.1
s3fs             : 0.2.1
scipy            : 1.3.1
sqlalchemy       : 1.3.7
tables           : 3.5.2
tabulate         : 0.8.6
xarray           : 0.12.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8
numba            : 0.45.1

</details>"
467626300,27372,Cython warning about unsafe nogil cast in parsers.pyx,ghost,closed,2019-07-12T22:42:03Z,2020-01-19T19:47:20Z,"```
commit 681e6a9b07271a0955e6780e476ab2d7101e549c
Author: Phil Ruffwind <rf@rufflewind.com> 

    BUG: Segfault due to float_precision='round_trip'
    
    `round_trip` calls back into Python, so the GIL must be held.  It also
    fails to silence the Python exception, leading to spurious errors.
    Closes #15140. 
```    

681e6a9b07271a0955e6 closed #15140 but introduced a cython warning (seen with 0.29.12)
```
warning: pandas/_libs/parsers.pyx:1724:34: Casting a GIL-requiring 
function into a nogil function circumvents GIL validation
```

The commit says it fixes a segfault, while the cython warning suggests this is still unsafe.

The lines in question are https://github.com/pandas-dev/pandas/blob/8913b7e84ac75b1093bf708e0d1a9a0789e8cfcd/pandas/_libs/parsers.pyx#L1723-L1727

"
549040899,30971,BUG: reductions for nullable dtypes should return pd.NA for skipna=False,jorisvandenbossche,closed,2020-01-13T16:25:27Z,2020-01-19T20:07:37Z,"Closes https://github.com/pandas-dev/pandas/issues/30958

Just realized that I should probably check boolean dtype as well"
551843244,31129,REF: share code between Int64Index and UInt64Index,jbrockmendel,closed,2020-01-19T00:05:47Z,2020-01-19T20:57:12Z,
551958293,31137,TYP: Index get_indexer_foo methods,jbrockmendel,closed,2020-01-19T17:57:56Z,2020-01-19T20:59:38Z,
551990423,31141,show dtype when using df.head(),shuai-zhou,closed,2020-01-19T21:43:30Z,2020-01-19T22:00:56Z,"I have a class that requires R, and I realized R will show the data type right below the column names when listing the first n-th rows. I was wondering if this is a feature in Pandas, like when I type `df.head()`, there will be dtype right below the column names.

"
24855398,5790,BUG: dont' always coerce reductions in a groupby always to datetimes,jreback,closed,2013-12-29T03:59:24Z,2020-01-19T23:27:51Z,"only when we have actual Timestamps in the data (GH5788,GH5789)
closes #5789

TST: tests for idxmax used in an apply 
closes #5788
"
24853099,5788,BUG: apply idxmax on one-column DataFrameGroupby generates ValueError,jorisvandenbossche,closed,2013-12-28T23:33:51Z,2020-01-19T23:46:46Z,"With this dataframe:

```
import pandas as pd
from StringIO import StringIO

s=""""""2011.05.16,00:00,1.40893
2011.05.16,01:00,1.40760
2011.05.16,02:00,1.40750
2011.05.16,03:00,1.40649
2011.05.17,02:00,1.40893
2011.05.17,03:00,1.40760
2011.05.17,04:00,1.40750
2011.05.17,05:00,1.40649
2011.05.18,02:00,1.40893
2011.05.18,03:00,1.40760
2011.05.18,04:00,1.40750
2011.05.18,05:00,1.40649""""""

df = pd.read_csv(StringIO(s), header=None, names=['date', 'time', 'value'], parse_dates=[['date', 'time']])
df = df.set_index('date_time')
```

applying an `idxmax`:

```
df.groupby(df.index.date).apply(lambda x: x.idxmax())
```

produces on master:

```
ValueError: Shape of passed values is (1, 3), indices imply (1, 3)
```

while this does work on 0.12

It has to do I think with difference between SeriesGroupby and DataFrameGroupby with one column, as `df.groupby(df.index.date)['value'].apply(lambda x: x.idxmax())` does work in master, but in 0.12 both ways work.
"
548652623,30950,REF: handle searchsorted casting within DatetimeLikeArray,jbrockmendel,closed,2020-01-13T00:26:47Z,2020-01-20T00:26:29Z,
537949007,30276,Only reading first sheet without warning ,samuelweisenthal,closed,2019-12-14T18:37:05Z,2020-01-20T01:20:57Z,"#29491 ## Problem description

When there are multiple sheets in an excel file, read_excel only returns the first.  This is fine, but I think it should also warn that there are multiple sheets"
550405884,31052,User guide HDF5 complevel and complib formatting strange,kylekeppler,closed,2020-01-15T20:14:34Z,2020-01-20T13:34:12Z,"Formatting for HDF5 `complevel` and `complib` is a bit weird on both the old docs and new docs.

https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#compression
https://pandas.io/docs/user_guide/io.html#compression

"
550407536,31053,DOC: Fix HDF5 complevel and complib formatting,kylekeppler,closed,2020-01-15T20:18:05Z,2020-01-20T13:34:23Z,"Fixes formatting of these params.

- [x] closes #31052
- [ ] tests added / passed (N/A)
- [ ] passes `black pandas` (N/A)
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff` (N/A)
- [ ] whatsnew entry (N/A)

N/A as this PR only updates an rst file for the docs."
536061745,30192,"DOC: undocumented head(n), tail(n) accept negative values. But not on GroupBy",smcinerney,closed,2019-12-10T23:56:52Z,2020-01-20T14:25:06Z,"DOC: undocumented behavior that works: **`head(n)`, `tail(n)`accept negative values** both for DataFrame, Series. But not for [`GroupBy.head(n)`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.head.html#pandas.core.groupby.GroupBy.head) ; doesn't correctly handle negative values.

This is quite useful and could be documented. 
- `head(-n)` returns all rows(/elements) other than the n first values. Similar to `df[-6::]`.
- `tail(-n)` returns all rows(/elements) other than the n last values
- There is no error if the length n specified is too long for the dataframe(/series), just silently returns an empty result.

#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame({'x': range(18,0,-1), 'y': [x % 6 for x in range(18)] })

df.head(-25)
Empty DataFrame
Columns: [x, y]
Index: []

df.head(-15)
    x  y
0  18  0
1  17  1
2  16  2

 df.tail(-15)
    x  y
15  3  3
16  2  4
17  1  5

df['x'].tail(-15)
15    3
16    2
17    1

# But GroupBy.head(n) doesn't
 df.groupby('y').head(-1)
Empty DataFrame
Columns: [x, y]
Index: []
df.groupby('y').tail(-1)
Empty DataFrame
Columns: [x, y]
Index: []

```
#### Problem description

Add to documented behavior. 

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Darwin
OS-release: 18.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 5.2.4
pip: 19.3.1
setuptools: 41.6.0.post20191030
Cython: 0.29.14
numpy: 1.17.3
scipy: 1.3.1
pyarrow: None
xarray: None
IPython: 7.9.0
sphinx: 2.2.1
patsy: 0.5.1
dateutil: 2.8.1
pytz: 2019.3
blosc: None
bottleneck: 1.3.1
tables: 3.6.1
numexpr: 2.7.0
feather: None
matplotlib: 3.1.1
openpyxl: 3.0.1
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.2.6
lxml.etree: 4.4.1
bs4: 4.8.1
html5lib: 1.0.1
sqlalchemy: 1.3.11
pymysql: None
psycopg2: 2.8.4 (dt dec pq3 ext lo64)
jinja2: 2.10.3
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
552134221,31147,Use Python 3 shebangs,rebecca-palmer,closed,2020-01-20T08:07:45Z,2020-01-20T15:16:46Z,"Plain 'python' may either not exist at all, or be Python 2."
552304306,31152,Backport PR #31095 on branch 1.0.x (DOC: Restore ExtensionIndex.dropna.__doc__),meeseeksmachine,closed,2020-01-20T13:35:47Z,2020-01-20T15:24:46Z,Backport PR #31095: DOC: Restore ExtensionIndex.dropna.__doc__
551950301,31134,"REF/BUG: Index.get_value called incorrectly, de-duplicate+simplify",jbrockmendel,closed,2020-01-19T16:50:53Z,2020-01-20T16:24:29Z,"Also engine.get_value is being called incorrectly in DTI/TDI, which this fixes."
551151884,31095,DOC: Restore ExtensionIndex.dropna.__doc__,jbrockmendel,closed,2020-01-17T01:46:15Z,2020-01-20T16:27:26Z,Per request from @jorisvandenbossche on #30717.
551126761,31091,CI/TST: fix failing tests in py37_np_dev,ShaharNaveh,closed,2020-01-17T00:11:17Z,2020-01-27T12:37:03Z,
555598649,31342,CI Failing on IPython tab complete test,TomAugspurger,closed,2020-01-27T14:09:39Z,2020-01-27T14:14:49Z,"https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=26868&view=logs&j=a3a13ea8-7cf0-5bdb-71bb-6ac8830ae35c&t=add65f64-6c25-5783-8fd6-d9aa1b63d9d4

```
##[error]2 test(s) failed, 61943 test(s) collected.
Skipping uploading of coverage data.

=================================== FAILURES ===================================
______________ TestCategoricalWarnings.test_tab_complete_warning _______________
[gw1] linux -- Python 3.7.6 /home/vsts/miniconda3/envs/pandas-dev/bin/python


self = <contextlib._GeneratorContextManager object at 0x7fec44a81210>
type = None, value = None, traceback = None

    def __exit__(self, type, value, traceback):
        if type is None:
            try:
>               next(self.gen)
E               AssertionError: Caused unexpected warning(s): [('DeprecationWarning', DeprecationWarning('Deprecated since version 0.16.0. Use get_signatures()[...].params'), '/home/vsts/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/jedi/cache.py', 111)]

../../../miniconda3/envs/pandas-dev/lib/python3.7/contextlib.py:119: AssertionError
_____________________ TestIndex.test_tab_complete_warning ______________________
[gw0] linux -- Python 3.7.6 /home/vsts/miniconda3/envs/pandas-dev/bin/python

self = <pandas.tests.indexes.test_base.TestIndex object at 0x7f6d1ff31850>
ip = <IPython.core.interactiveshell.InteractiveShell object at 0x7f6d1ff31b90>

    @async_mark()
    async def test_tab_complete_warning(self, ip):
        # ***/issues/16409
        pytest.importorskip(""IPython"", minversion=""6.0.0"")
        from IPython.core.completer import provisionalcompleter
    
        code = ""import pandas as pd; idx = pd.Index([1, 2])""
        await ip.run_code(code)
        with tm.assert_produces_warning(None):
            with provisionalcompleter(""ignore""):
>               list(ip.Completer.completions(""idx."", 4))

pandas/tests/indexes/test_base.py:2418: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7f6d1ebc7a50>
type = None, value = None, traceback = None

    def __exit__(self, type, value, traceback):
        if type is None:
            try:
>               next(self.gen)
E               AssertionError: Caused unexpected warning(s): [('DeprecationWarning', DeprecationWarning('Deprecated since version 0.16.0. Use get_signatures()[...].params'), '/home/vsts/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/jedi/cache.py', 111)]

../../../miniconda3/envs/pandas-dev/lib/python3.7/contextlib.py:119: AssertionError

```

taking a look now."
555542579,31341,Backport PR #31323 on branch 1.0.x (CI: Fix jedi upgrades causes deprecation warning),meeseeksmachine,closed,2020-01-27T12:28:59Z,2020-01-27T14:22:05Z,Backport PR #31323: CI: Fix jedi upgrades causes deprecation warning
555204374,31323,CI: Fix jedi upgrades causes deprecation warning,charlesdong1991,closed,2020-01-26T09:45:14Z,2020-01-27T15:49:41Z,"- [ ] closes #31324 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
134071023,12355,Feature Sugesstion: date_range from end to start,npezolano,closed,2016-02-16T19:13:07Z,2020-01-27T15:55:16Z,"When using `pd.date_range` we work our way forwards from start to end.  It would be nice to pass a flag or use the dates in reverse to go from end to start when creating the date_range.

``` python
#EX:
pd.date_range(end, start)
#or
pd.date_range(start, end, end_to_start=True)
```

``` python
Ex:
start = datetime.datetime(2016 ,2, 8)
end = datetime.datetime(2018 , 6, 1)
pd.date_range(start end, freq='6m', start_to_end=True)
>>> DatetimeIndex(['2016-02-08', '2016-06-01', '2016-12-01', '2017-06-01',
               '2017-12-01', '2018-06-01'])
```
"
554265103,31250,"pd.DataFrame.duplicated, drop_duplicates does not support column operation",matthewgson,closed,2020-01-23T16:26:06Z,2020-01-27T17:54:14Z,"I was trying to use .duplicated to find identical columns, and possibly dropping them, but realized it doesn't do columnar operations.
It would be great if pd.DataFrame.duplicated or pd.DataFrame.drop_duplicates supports axis= argument. 

Current version 0.25.3 not supporting axis=
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html

Some workaround by custom function
https://thispointer.com/how-to-find-drop-duplicate-columns-in-a-dataframe-python-pandas/

"
552227575,31148,DOC: replace long table of contents on home page with main links,jorisvandenbossche,closed,2020-01-20T11:01:43Z,2020-01-27T19:53:54Z,"This is a proposal to remove the long table of contents on the documentation home page and replace it with a highlight of the main parts of the documentation.

Reasons: I don't think the long table of contents resulted in a very attractive landing page of the docs. And further, it was also a ""manual"" table of contents which would therefore easily get out of date. And with the new content, we can also at the same time give some explanation on the organization of the docs.

So would like to get feedback on the general idea. The wording in the different boxes can certainly still be improved. And we could also make similar boxes to point to eg the installation guide or for the different ""getting help"" links. 

It's using some bootstrap elements (""cards"") that can be used now since the sphinx theme is based on bootstrap (and all credit to @stijnvanhoey for the html/css part, I re-used some content of what he worked on for the getting started guides, a PR for that is coming shortly). Screenshot of how it currently looks below, and a live version can be checked here: http://jorisvandenbossche.github.io/example-pandas-docs/html-doc-home/

![image](https://user-images.githubusercontent.com/1020496/72721384-35a1c880-3b7c-11ea-87af-eb26bff4f3db.png)
"
521216843,29556,Missing values proposal: concrete steps for 1.0,jorisvandenbossche,closed,2019-11-11T22:14:44Z,2020-01-27T19:59:43Z,"Updated with to do list:

* [x] Implement `pd.NA` scalar -> https://github.com/pandas-dev/pandas/pull/29597
* [x] Basic BooleanArray -> https://github.com/pandas-dev/pandas/pull/29555
* [x] Use `pd.NA` in BooleanArray -> https://github.com/pandas-dev/pandas/pull/29961
    * [x] Implement kleene-logic in logical ops on BooleanArray -> https://github.com/pandas-dev/pandas/pull/29842
    * [x] Update the behaviour of `any`/`all` reductions with `skipna=False` (https://github.com/pandas-dev/pandas/issues/29686) -> https://github.com/pandas-dev/pandas/pull/30062
* [x] Use BooleanArray in comparison ops of StringArray -> https://github.com/pandas-dev/pandas/pull/30231
* [x] Use `pd.NA` in IntegerArray -> https://github.com/pandas-dev/pandas/pull/29964
* [x] Use BooleanArray as the return value for logical ops in IntegerArray -> https://github.com/pandas-dev/pandas/pull/29964
* [x] Enable boolean indexing with BooleanArray ( https://github.com/pandas-dev/pandas/issues/28778/) -> https://github.com/pandas-dev/pandas/pull/30308
* [x] Use BooleanArray as the return value for boolean `.str` methods. -> https://github.com/pandas-dev/pandas/pull/30239
* [x] Implement `NA.__array_ufunc__` -> https://github.com/pandas-dev/pandas/pull/30245
* [x] Base class for IntegerArray & BooleanArray -> https://github.com/pandas-dev/pandas/pull/30789
* [ ] Ensure everything is properly documented

---
*Original issue:*

Issue to discuss the implementation strategy for https://github.com/pandas-dev/pandas/issues/28095. 
Opening a new issue, as the other one already has a lot of discussion in several discussion, and would propose to keep this one focused on the practical aspects of how to implement this (regardless of certain aspects of the NA proposal such as single NA vs dtype-specific NAs -> for that will post a summary of the discussion on #28095 tomorrow).

I would like to propose the following way forward:

**On the short term (ideally for 1.0)**:

* Already implement and provide the `pd.NA` scalar, and recognize it in the appropriate places as missing value (e.g. `pd.isna`). This way, it can already be used in external ExtentionArrays
* Implement a `BooleanArray` with support for missing values and appropriate NA behaviour. To start, we can just use a numpy masked array approach (similar to the existing IntegerArray), not involving any pyarrow memory optimizations.
* Start using this BooleanArray as the boolean result of comparison operations for IntegerArray/StringArray (breaking change for nullable integers)
  * Other arrays will keep using the numpy bool, this means we have two ""boolean"" dtypes side by side with different behaviour, and which one you get depends on the original data type (potentially confusing for users)
*  Start using `pd.NA` as the missing value indicator for Integer/String/BooleanArray (breaking change for nullable integers)

**On the intermediate term (after 1.0)**

* Investigate if it can be implemented optionally for other data types and ""activated"" to have users opt-in for existing dtypes (to be further thought out).

---

I think the main discussion point is if we are OK with such a breaking change for IntegerArray. 
I would personally do this: IntegerArray was only introduced recently, still regarded as experimental, and the perfect use case for those changes. But, it's certainly a clear backwards incompatible, breaking change.

cc @pandas-dev/pandas-core "
546356168,30784,API: DataFrame.take always returns a copy,jorisvandenbossche,closed,2020-01-07T15:44:56Z,2020-01-28T04:00:51Z,"Closes #27357

This adds an internal version of `take` with the behaviour of setting `_is_copy` for DataFrames that the public `take` did before, so this version can be used in the indexing code (where we want to keep track of parent dataframe with `_is_copy` for SettingWithCopyWarnings). 
I named it  `_take_with_is_copy` which is literally what it is doing, but happy to hear alternatives.

This then updates the deprecation to fully ignore the keyword and indicate in the deprecation message the keyword has no effect anymore.

I checked https://github.com/pandas-dev/pandas/pull/27349 and https://github.com/pandas-dev/pandas/pull/30615 to ensure that where previously the internal version was used or `is_copy` was specified, now the appropriate function is used. I suppose that in some of the cases where I now use the internal `_take_with_is_copy` this is not actually needed, but it's the safest thing anyway (it will do the same as it did before)."
555796223,31360,Backport PR #31148: DOC: replace long table of contents on home page with main links,jorisvandenbossche,closed,2020-01-27T19:53:16Z,2020-01-27T20:35:22Z,#31148
555423074,31337,CLN: some minor cleanups,topper-123,closed,2020-01-27T08:24:18Z,2020-01-27T21:05:08Z,
554207894,31248,REGR: CategoricalIndex and IntervalIndex are missing _index_data attribute,jschendel,closed,2020-01-23T14:53:42Z,2020-01-28T01:57:19Z,"It looks like the `_index_data` attribute wasn't added to `CategoricalIndex` or `IntervalIndex`.  This leads to some regressions with `GroupBy.apply`.

Not sure what the fix should be, as there isn't a singular array that directly backs the underlying values for `CategoricalIndex` or `IntervalIndex`, so the approach used to add `_index_data` for other types doesn't look like it will work here.

Example of broken behavior on `master`:
```python
In [1]: import pandas as pd; pd.__version__
Out[1]: '1.0.0rc0+146.g6f395ad42'

In [2]: index = pd.CategoricalIndex(list(""abc""))  
   ...: df = pd.DataFrame({""group"": [0, 0, 1], ""value"": [1, 2, 3]}, index=index)

In [3]: df.groupby(""group"").apply(lambda x: x)
---------------------------------------------------------------------------
AttributeError: 'CategoricalIndex' object has no attribute '_index_data'

```

Same behavior worked on 0.25.3:
```python
In [1]: import pandas as pd; pd.__version__
Out[1]: '0.25.3'

In [2]: index = pd.CategoricalIndex(list(""abc""))  
   ...: df = pd.DataFrame({""group"": [0, 0, 1], ""value"": [1, 2, 3]}, index=index)

In [3]: df.groupby(""group"").apply(lambda x: x)
Out[3]: 
   group  value
a      0      1
b      0      2
c      1      3
```
Swapping `CategoricalIndex` with `IntervalIndex` results in the same behavior above."
553839409,31223,GroupBy aggregation fails if DataFrame has CategoricalIndex ,frances-h,closed,2020-01-22T22:47:16Z,2020-01-28T01:57:19Z,"#### Code Sample, a copy-pastable example if possible

```python
ids = pd.Categorical([0, 1, 2])
df = pd.DataFrame({
    'id': ids,
    'groups': [1, 1, 2],
    'value': [0, 1, 0]
}).set_index('id')

df.groupby('groups').agg({'value': pd.Series.nunique})

```
#### Problem description
The above works in v. `0.25.3` but not in `1.0.0rc0`. It fails with: `TypeError: Cannot convert Categorical to numpy.ndarray` 

Full stack trace:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""~/.virtualenvs/3.6-test-pd-1.0.0/lib/python3.6/site-packages/pandas/core/groupby/generic.py"", line 940, in aggregate
    result, how = self._aggregate(func, *args, **kwargs)
  File ""~/.virtualenvs/3.6-test-pd-1.0.0/lib/python3.6/site-packages/pandas/core/base.py"", line 430, in _aggregate
    result = _agg(arg, _agg_1dim)
  File ""~/.virtualenvs/3.6-test-pd-1.0.0/lib/python3.6/site-packages/pandas/core/base.py"", line 397, in _agg
    result[fname] = func(fname, agg_how)
  File ""~/.virtualenvs/3.6-test-pd-1.0.0/lib/python3.6/site-packages/pandas/core/base.py"", line 381, in _agg_1dim
    return colg.aggregate(how)
  File ""~/.virtualenvs/3.6-test-pd-1.0.0/lib/python3.6/site-packages/pandas/core/groupby/generic.py"", line 265, in aggregate
    return self._python_agg_general(func, *args, **kwargs)
  File ""~/.virtualenvs/3.6-test-pd-1.0.0/lib/python3.6/site-packages/pandas/core/groupby/groupby.py"", line 935, in _python_agg_general
    result, counts = self.grouper.agg_series(obj, f)
  File ""~/.virtualenvs/3.6-test-pd-1.0.0/lib/python3.6/site-packages/pandas/core/groupby/ops.py"", line 624, in agg_series
    return self._aggregate_series_fast(obj, func)
  File ""~/.virtualenvs/3.6-test-pd-1.0.0/lib/python3.6/site-packages/pandas/core/groupby/ops.py"", line 648, in _aggregate_series_fast
    grouper = libreduction.SeriesGrouper(obj, func, group_index, ngroups, dummy)
  File ""pandas/_libs/reduction.pyx"", line 329, in pandas._libs.reduction.SeriesGrouper.__init__
TypeError: Cannot convert Categorical to numpy.ndarray
```
#### Expected Output
The correct result applying the aggregation function to the DataFrameGroupBy

#### Output of ``pd.show_versions()``


<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.0rc0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.1
setuptools       : 45.1.0
Cython           : None
pytest           : 5.2.0
hypothesis       : None
sphinx           : 2.0.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.2.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.0.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.2.0
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.47.0
</details>
"
548184673,30892,value assignment bug for multi-indexed dataframes in v1.0.0rc0,leo4183,closed,2020-01-10T16:52:11Z,2020-01-28T01:59:32Z,"#### Problem Description
in new pre-release 1.0.0.rc0, sliced value assignment for multi-indexed dataframe with duplicated datetime indices would fail

```python
import pandas as pd
x = pd.DataFrame([[1,2,3],['a','a','c']],index=['c1','index2'],
                 columns=pd.to_datetime(['20190101','20190101','20190102'])).T
x['c2'] = np.nan
x.index.name = 'index1'
x.set_index(['index2'],append=True,inplace=True)


In [0]: x
Out[0]:
                 c1  c2
index1   index2       
20190101 a       1  NaN
         a       2  NaN
20190102 c       3  NaN


x.loc[(pd.to_datetime('20190101'),'a'),'c2'] = 1.0     # ok
x.loc[[(pd.to_datetime('20190101'),'a')],'c2'] = 1.0   # ValueError
```"
555750304,31358,BUG/REG: multiindex nested tuples with duplicates,jbrockmendel,closed,2020-01-27T18:21:58Z,2020-01-30T09:58:54Z,"- [x] closes #30892
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

there is an underlying problem with MultiIndex.get_loc that needs to get worked out, but that's going to take a while.  This fixes the reported regression."
555927133,31373,CLN: require extracting .values before expressions calls,jbrockmendel,closed,2020-01-28T01:12:04Z,2020-01-28T02:25:36Z,
555937863,31374,Backport PR #31290 on branch 1.0.x (BUG: Handle IntegerArray in pd.cut),meeseeksmachine,closed,2020-01-28T01:52:51Z,2020-01-28T02:57:41Z,Backport PR #31290: BUG: Handle IntegerArray in pd.cut
555938236,31375,Backport PR #30860 on branch 1.0.x (REF: gradually move ExtensionIndex delegation to use inherit_names),meeseeksmachine,closed,2020-01-28T01:54:25Z,2020-01-28T02:57:56Z,Backport PR #30860: REF: gradually move ExtensionIndex delegation to use inherit_names
553905797,31238,REGR: Prevent indexes that aren't directly backed by numpy from entering libreduction code paths,jschendel,closed,2020-01-23T02:38:37Z,2020-01-30T09:59:53Z,"- [X] closes #31223
- [X] closes #31248 
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

No whatsnew since this is a regression."
555883836,31371,CLN: tighten what we pass to Index._shallow_copy,jbrockmendel,closed,2020-01-27T22:57:56Z,2020-01-28T04:20:23Z,
555801928,31362,Backport PR #30784: API: DataFrame.take always returns a copy,jorisvandenbossche,closed,2020-01-27T20:04:55Z,2020-01-28T06:00:10Z,Backport #30784
553607341,31210,COMPAT: Argsort position matches NumPy,TomAugspurger,closed,2020-01-22T15:14:38Z,2020-01-28T06:43:30Z,"Closes https://github.com/pandas-dev/pandas/issues/29884

Bit of a WIP, CI is probably going to fail here. One API question, do we want NaT to sort at the end, regardless of the NumPy version? Or should we match the behavior of the installed NumPy (NaT at the start for <1.18, at the end for >=1.18)?"
555674190,31353,CLN: remove construct_from_string from various,simonjayhawkins,closed,2020-01-27T16:05:15Z,2020-01-28T08:54:32Z,
555817192,31366,TYP: check_untyped_defs core.computation.ops,simonjayhawkins,closed,2020-01-27T20:34:54Z,2020-01-28T08:59:14Z,
555807145,31363,TYP: update setup.cfg,simonjayhawkins,closed,2020-01-27T20:15:45Z,2020-01-28T09:00:14Z,
555885905,31372,Allow non-inplace option for `ndframe.update()`,giuliobeseghi,closed,2020-01-27T23:03:27Z,2020-01-28T09:41:30Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

s = pd.Series([1, 2, 3, 4, 5])
t = pd.Series([10, 11, 12], index=range(3, 6))

print(s.update(t).reindex([1, 2, 3, 4, 5]))
```
#### Problem description

Obviously the previous doesn't work. `s.update(t)` is executed inplace, and doesn't return anything. I see no reasons why one shouldn't be able to call `update` and get a new object (for example to allow method chaining).

The easiest way of implementing this is by allowing a parameter that makes the function return the updated object:

```python
print(s.update(t, inplace=False).reindex([1, 2, 3, 4, 5]))
```

**I know there is already a plan about deprecating `inplace`, but I'm not sure it involves `update`. I guess that `update` was designed to be similar to the same method of a dict object.**

**Update**
Closed as it was solved in https://github.com/pandas-dev/pandas/issues/10730
"
219255440,15888,DTYPE: use a Categorical for bool dtypes,jreback,closed,2017-04-04T13:44:45Z,2020-01-28T10:02:10Z,"this may seem counter intutive, but this would allow us to store nulls as well (efficiently, rather than as ``object`` dtype), or casting to floats.

I am sure if this would really be possible w/o some API breaks, so will have a look."
554883153,31290,BUG: Handle IntegerArray in pd.cut,TomAugspurger,closed,2020-01-24T18:18:11Z,2020-01-28T12:41:00Z,"xref https://github.com/pandas-dev/pandas/issues/30944.
I think this doesn't close it, since only the pd.cut compoment
is fixed.

cc @jorisvandenbossche @jreback. The changes here attempt to be extremely conservative, since we're backporting stuff. I'm trying to not change behavior for anything other than IntegerArray. In particular, I'm not trying to support arbitrary EAs in `pd.cut`. This leads to some code that's fairly ugly & specific to IntegerArray. I think we should attempt to clean that up in 1.1."
556063547,31382,DOC: reduce long error tracebacks in 1.0 whatsnew + some clean-up,jorisvandenbossche,closed,2020-01-28T08:49:35Z,2020-01-28T13:07:49Z,"Made some of the ipython blocks into literal code blocks to avoid showing long error tracebacks (that only distract from the core text). Plus did a few other small formatting clean-ups, and combined 2 pyarrow related entries."
484868913,28130,Add Indent Support in to_json,WillAyd,closed,2019-08-24T22:33:50Z,2020-01-28T13:12:43Z,"- [X] closes #12004
- [X] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

After some of the recent refactor this should be relatively easy to do after vendoring a few updates"
556187531,31390,Backport PR #31382 on branch 1.0.x (DOC: reduce long error tracebacks in 1.0 whatsnew + some clean-up),meeseeksmachine,closed,2020-01-28T12:49:01Z,2020-01-28T13:33:41Z,Backport PR #31382: DOC: reduce long error tracebacks in 1.0 whatsnew + some clean-up
517032956,29393,BUG: GH25495 incorrect dtype when using .loc to set Categorical value for column in 1-row DataFrame,keechongtan,closed,2019-11-04T09:25:45Z,2020-01-28T15:29:45Z,"- [x] closes https://github.com/pandas-dev/pandas/issues/25495
- [x] 1 test added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
415947681,25495,Unexpected dtype when using .loc to set Categorical value for column in 1-row DataFrame,mbcole,closed,2019-03-01T05:27:46Z,2020-01-28T15:29:53Z,"#### Code Sample, a copy-pastable example if possible

```python
In [1]: import pandas as pd

In [2]: df = pd.DataFrame({'Alpha': [u'a'], 'Numeric': [0]})

In [3]: df.loc[:,'Alpha']
Out[3]: 
0    a
Name: Alpha, dtype: object

In [4]: codes = pd.Categorical(df['Alpha'], categories = [u'a',u'b',u'c'])

In [5]: codes
Out[5]: 
[a]
Categories (3, object): [a, b, c]

In [6]: df.loc[:,'Alpha'] = codes

In [7]: df.loc[:,'Alpha']
Out[7]: 
0    a
Name: Alpha, dtype: object
```

#### Problem description

When I try to set the column of a one-row DataFrame to a `pandas.core.arrays.categorical.Categorical`, it is returned as a `pandas.core.series.Series` of `dtype('O')` rather than a `pandas.core.series.Series` of `CategoricalDtype(categories=[u'a', u'b', u'c'], ordered=False)`. I get the latter return value when I set the column using `df['Alpha'] = codes` or `df.Alpha = codes`. I can't replicate this inconsistency with DataFrames containing more than one row.

#### Expected Output

```python
Out[7]: 
0    a
Name: Alpha, dtype: category
Categories (3, object): [a, b, c]
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.15.final.0
python-bits: 64
OS: Darwin
OS-release: 18.2.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.24.1
pytest: None
pip: 19.0.3
setuptools: 40.6.3
Cython: None
numpy: 1.15.4
scipy: 1.2.0
pyarrow: None
xarray: None
IPython: 5.8.0
sphinx: None
patsy: 0.5.1
dateutil: 2.7.5
pytz: 2018.7
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.2.3
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
556285908,31393,Backport PR #29393 on branch 1.0.x (BUG: GH25495 incorrect dtype when using .loc to set Categorical value for column in 1-row DataFrame),meeseeksmachine,closed,2020-01-28T15:29:56Z,2020-01-28T16:00:40Z,Backport PR #29393: BUG: GH25495 incorrect dtype when using .loc to set Categorical value for column in 1-row DataFrame
555843695,31369,Center align the text and icons in documentation index,TomAugspurger,closed,2020-01-27T21:27:49Z,2020-01-28T16:18:19Z,"At https://dev.pandas.io/docs/index.html, the alignment is slightly off. Especially notable for User Guide and API reference

![Screen Shot 2020-01-27 at 3 27 16 PM](https://user-images.githubusercontent.com/1312546/73215355-90f22e80-4119-11ea-9f6b-d9bedd1f332b.png)

cc @jorisvandenbossche 

"
556270816,31392,DOC: Fix image alignment,TomAugspurger,closed,2020-01-28T15:06:31Z,2020-01-28T16:18:20Z,"Closes #31369 

![Screen Shot 2020-01-28 at 9 02 54 AM](https://user-images.githubusercontent.com/1312546/73275908-74073b00-41ad-11ea-878a-0144ba558f3f.png)

"
553082645,31182,CLN: Make Series._values match Index._values,jbrockmendel,closed,2020-01-21T19:32:13Z,2020-01-28T17:11:52Z,"Discussed in #31037 

- Note this is _not_ an alternative to that, as this should not be considered for 1.0.
- This does not implement any of the simplifications that it makes available.
- There is a decent chance that the check in core.apply can be changed so that we can still use the fastpath for these dtypes.
- The PandasDType edit is not central to the change, just included for perf comparison against #31037."
556318096,31394,Backport PR #31392 on branch 1.0.x (DOC: Fix image alignment),meeseeksmachine,closed,2020-01-28T16:18:32Z,2020-01-28T17:56:58Z,Backport PR #31392: DOC: Fix image alignment
82502153,10231,Undesired UX behavior of DataFrame output in IPython Notebook,danieljl,closed,2015-05-29T16:50:51Z,2020-01-28T18:05:16Z,"From my previous line comment at https://github.com/pydata/pandas/commit/be80898920eab8f740cf61019327621a75d73947#commitcomment-11338872. cc @takluyver @jorisvandenbossche

Because DataFrame's `_repr_html_` returns hardcoded `div` tag with style `max-height:1000px`, if the height of DataFrame's output exceeds 1000px, it will generate a scroll bar.

First, it is unnecessary because IPython Notebook has ""toggle output scrolling"" feature. Second, it causes UX problem if the height of browser resolution is below 1000px (user must scroll twice: the whole page and the generated div). Third, it can't make use of ""toggle output scrolling"" feature (if user turn it on, it will generate nested scroll bars (worse UX)). For more detailed explanations and examples, see the steps to reproduce below.

Pandas version: 0.16.1.

Steps to reproduce:
1. Open an IPython Notebook and paste this code. It will generate an inner scroll bar because the output's height exceeds 1000px.
   
   ``` python
   import pandas as pd
   pd.DataFrame(list(range(100)))
   ```
2. If you want to see all rows, first you should scroll the page.
   ![a](https://cloud.githubusercontent.com/assets/1947204/7887439/3ba2e788-065b-11e5-8be1-9e9cdb4eb304.png)
3. After you reach the bottom of the generated ouput's `div`, you should scroll again the inner scroll bar.
   ![b](https://cloud.githubusercontent.com/assets/1947204/7887460/705b87f0-065b-11e5-827c-a4c7d915e485.png)
4. If you want to toggle the IPython Notebook's ""toggle output scrolling"" feature, you get this.
   ![c](https://cloud.githubusercontent.com/assets/1947204/7887471/8620d4d2-065b-11e5-9386-8b8f08a62c07.png)

Desired behaviors:
- When ""output scrolling"" feature is off
  ![d](https://cloud.githubusercontent.com/assets/1947204/7887506/c8d131b4-065b-11e5-8936-9636094ed99b.png)
- When ""output scrolling"" feature is on
  ![e](https://cloud.githubusercontent.com/assets/1947204/7887515/d85116a4-065b-11e5-9691-7110988f95f1.png)

Solution:
Remove style property from `div` tag that DataFrame's `_repr_html_` returns.
Old: `<div style=""max-height:1000px;max-width:1500px;overflow:auto;"">\n`
New: `<div>\n`

Why should `max-width` be removed as well? Because it also generates _far_ worse UX. If you have so many columns that the horizontal scroll bar generated, you cannot scroll it. You must refresh the page to be able to scroll it.
"
555943698,31376,"""Backport PR #31358 on branch 1.0.x""",jbrockmendel,closed,2020-01-28T02:15:36Z,2020-01-28T19:32:43Z,xref #31358 
555801569,31361,Avoid Index DeprecationWarning in Series getitem,TomAugspurger,closed,2020-01-27T20:04:03Z,2020-01-28T20:55:45Z,"xref https://github.com/pandas-dev/pandas/issues/30867

cc @jorisvandenbossche. The alternative is to use a warnings filter inside SingleBlockManger.get_slice to filter the warning, but I think avoiding the warning in the first place is a bit nicer."
548479946,30929,ENH: Implement convert_dtypes,Dr-Irv,closed,2020-01-11T21:03:28Z,2020-01-28T20:56:46Z,"- [x] xref #29752
- [x] tests added / passed
    - pandas/tests/series/test_dtypes.py:test_convert_dtypes
    - pandas/tests/frame/test_dtypes.py:test_convert_dtypes
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This implements `DataFrame.convert_dtypes()` and `Series.convert_dtypes()`, which will make it much easier to use the new `pd.NA` functionality.

Added documentation in the section about the new `pd.NA` functionality.

I'm sure there will be comments about how I could have done this in a more/better/different way, and I'm open to resolving them so we get this into 1.0."
551499854,31109,Return NotImpelmented for arith ops with object-dtype Index subclasses,TomAugspurger,closed,2020-01-17T16:05:52Z,2020-01-28T20:57:19Z,"This change in 1.0 impacted xarray

```python
 In [22]: import xarray as xr

In [23]: import pandas as pd

In [24]: idx = pd.timedelta_range(""1D"", periods=5, freq=""D"")

In [25]: a = xr.cftime_range(""2000"", periods=5)

In [26]: idx + a
/Users/taugspurger/sandbox/pandas/pandas/core/arrays/datetimelike.py:1204: PerformanceWarning: Adding/subtracting array of DateOffsets to TimedeltaArray not vectorized
  PerformanceWarning,
Out[26]:
Index([2000-01-02 00:00:00, 2000-01-04 00:00:00, 2000-01-06 00:00:00,
       2000-01-08 00:00:00, 2000-01-10 00:00:00],
      dtype='object')

In [27]: a + idx
Out[27]:
CFTimeIndex([2000-01-02 00:00:00, 2000-01-04 00:00:00, 2000-01-06 00:00:00,
             2000-01-08 00:00:00, 2000-01-10 00:00:00],
            dtype='object')
```

cc @jbrockmendel @dcherian. Brock suggested checking `type(other)`, and returning NotImplemented for index subclasses we don't know about, to give the subclass a chance to take over the op."
556419536,31400,read_csv error for a long entry ,dauss75,closed,2020-01-28T19:30:31Z,2020-01-28T21:10:47Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.read_csv(fileName, delimiter='\t', comment='#', skip_blank_lines=True)

```
#### Problem description
After reading a csv file, all the entries after a particular column in only one row shows as 'na', not the real value.  This is due to the character length (469 including spaces) in that particular column entry as removing some characters do load the data properly. Is there any way to load the file properly? I don't mind truncating each entry not to cause this issue.   

Thanks,
Segun
"
551957127,31136,COMPAT: Return NotImplemented for subclassing,TomAugspurger,closed,2020-01-19T17:48:06Z,2020-01-28T21:14:13Z,"This changes index ops to check the *type* of the argument in index
ops, rather than just the dtype. This lets index subclasses take control
of binary ops when they know better what the result should be.

Closes https://github.com/pandas-dev/pandas/issues/31109

cc @jbrockmendel, I've fixed this as close to the root of the problem as I could, but perhaps this case should be checked elsewhere? As written currently, it only affects add & sub between an Index and a datelike index, but perhaps it should be done more generally?"
556472400,31405,Backport PR #31136 on branch 1.0.x (COMPAT: Return NotImplemented for subclassing),meeseeksmachine,closed,2020-01-28T21:14:23Z,2020-01-28T22:17:03Z,Backport PR #31136: COMPAT: Return NotImplemented for subclassing
556463434,31403,Backport PR #31361 on branch 1.0.x (Avoid Index DeprecationWarning in Series getitem),meeseeksmachine,closed,2020-01-28T20:55:53Z,2020-01-28T22:17:23Z,Backport PR #31361: Avoid Index DeprecationWarning in Series getitem
556351903,31395,GH31391 - Fixed typos and fixed inexistant path in a SAS class comment,jeandersonbc,closed,2020-01-28T17:15:03Z,2020-01-29T01:48:35Z,Simple change: just fixed some typos and incorrect path as reported in #31391 
556024652,31379,Pandas: cannot set a Timestamp with a non-timestamp,devarshigoswami,closed,2020-01-28T07:11:51Z,2020-01-29T05:11:35Z,"
I have a data-frame of the following order: https://imgur.com/a/fp2hKZe on which i ran the following code 
```python
for i in r.index[1:]:
    if (r.at[i, 'lbl'] == r.at[i-1, 'lbl']) & (r.at[i, 'lbl'] == 'f'):
        r.at[i, 'date'] = r.at[i, 'date'] - r.at[i-1, 'date']
    elif r.at[i, 'lbl'] == 'f':
        r.at[i, 'date'] = r.at[i, 'date'] - r.at[i-1, 'date']
``` 
#### Problem description

When i run the same code for a much smaller data-set, it gives proper output
```python
data = pd.DataFrame(data=[[101,'f',datetime.date(2017,12,28)],
                         [101, 'f', datetime.date(2019,8,27)],
                         [101,'m',datetime.date(2013,4,12)],
                         [101,'m',datetime.date(2015,6,25)],
                         [101, 'f', datetime.date(2016,8,16)],
                         [102,'m', datetime.date(2019,8,15)],
                         [102,'m',datetime.date(2019,6,25)]],
                  columns=[""TC"", ""lbl"", ""date""])
```


```python
for i in data.index[1:]:
    if (data.at[i, 'lbl'] == data.at[i-1, 'lbl']) & (data.at[i, 'lbl'] == 'f'):
        data.at[i, 'date'] = data.at[i, 'date'] - data.at[i-1, 'date']
    elif data.at[i, 'lbl'] == 'f':
        data.at[i, 'date'] = data.at[i, 'date'] - data.at[i-1, 'date']
```

But when i change the  data-set, i get The **ValueError**
#### Expected Output

  | TC | lbl | date
-- | -- | -- | --
0 | 101 | f | 2017-12-28
1 | 101 | f | 607 days, 0:00:00
2 | 101 | m | 2013-04-12
3 | 101 | m | 2015-06-25
4 | 101 | f | 418 days, 0:00:00
5 | 102 | m | 2019-08-15
6 | 102 | m | 2019-06-25


#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.0.0-32-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_IN
LOCALE           : en_IN.ISO8859-1

pandas           : 0.25.1
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.2.1
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.1
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.9
tables           : 3.5.2
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.1

"
556211541,31391,SAS class comment mentions inexistant path,MarcoGorelli,closed,2020-01-28T13:31:44Z,2020-01-29T08:15:00Z,"`pandas/asv_bench/benchmarks/io/sas.py` :

> class SAS:
> 
>     params = [""sas7bdat"", ""xport""]
>     param_names = [""format""]
> 
>     def setup(self, format):
>         # Read files that are located in 'pandas/io/tests/sas/data'

However, the files are in  'pandas/tests/io/sas/data'.

EDIT:
unrelated typo: there's lots of places in the codebase that use ""rather then"" instead of ""rather than"". As this is also a minor typo that doesn't affect any code, I think it'd be OK to address both in the same pull request"
505123979,28893,whitespaces in column name handled differently than input to eval/query,EinarJohnsen,closed,2019-10-10T08:42:46Z,2020-01-29T08:15:19Z,"#### Code Sample

```python
>>> import pandas as pd
>>> d = pd.DataFrame(([1, 2], [3, 4]), columns=[""Col 1"", ""Col 2""])
>>> d.query(""`Col 1` < `Col 2`"") # This works as expected.
   Col 1  Col 2
0      1      2
1      3      4
>>>  d.query(""`Col  1` < `Col 2`"") # 2 whitespaces between ""Col"" and ""1"". Also works, but not expected?
   Col 1  Col 2
0      1      2
1      3      4

>>> d = pd.DataFrame(([1, 2], [3, 4]), columns=[""Col  1"", ""Col 2""]) # 2 whitespaces between ""Col"" and ""1"" in the column name. 
>>> d.query(""`Col  1` < `Col 2`"") # 2 whitespaces between ""Col"" and ""1"" in the query.  
pandas.core.computation.ops.UndefinedVariableError: name 'Col_1_BACKTICK_QUOTED_STRING' is not defined

>>> d.query(""`Col          1` < `Col 2`"") # 10 whitespaces between ""Col"" and ""1"".
pandas.core.computation.ops.UndefinedVariableError: name 'Col_1_BACKTICK_QUOTED_STRING' is not defined

>>> d.eval(""`Col  1` < `Col 2`"") # 2 whitespaces between ""Col"" and ""1"".
pandas.core.computation.ops.UndefinedVariableError: name 'Col_1_BACKTICK_QUOTED_STRING' is not defined
```
#### Problem description
When using backticks in the input to `DataFrame.query` or `DataFrame.eval`, it seems that the number of whitespaces between `Col` and `1`, is mapped into: `Col_1_BACKTICK_QUOTED_STRING`, regardless of how many whitespaces there are between `Col` and `1`.

Having 2 whitespaces when defining the column name in the DataFrame, it seems that the column name is defined as: `Col__1_BACKTICK_QUOTED_STRING`. 
With 10 whitespaces: `Col__________1_BACKTICK_QUOTED_STRING`, and so forth. 

This makes the number of whitespaces in the column name important, as column name will be based on the amount of them. However, the number of whitespaces in the input to `DataFrame.query` or `DataFrame.eval` does not seem to have any affect, and will always have one underscore, and not match the column name if it has more than one whitespace. 

#### Expected Output

The expected output would be to have the column name and the input to `query` or `eval` transformed with the same function, having a one to one mapping between whitespaces and underscores when definding the variable. i.e.
2 whitespaces: `Col__1_BACKTICK_QUOTED_STRING`.
10 whitespaces: `Col__________1_BACKTICK_QUOTED_STRING`.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-957.21.3.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : None
pytest           : 5.2.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : 2.7.0
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.0
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>"
545735623,30738,API: positional indexing with IntegerArray,jorisvandenbossche,closed,2020-01-06T13:47:43Z,2020-01-29T12:04:57Z,"Currently, the following does not work (but probably should):

```
In [12]: arr1 = pd.array([1, 2, 3]) 

In [13]: arr2 = pd.array([0, 2])

In [14]: arr1[arr2]  
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-14-1646c66d4d26> in <module>
----> 1 arr1[arr2]

~/scipy/pandas/pandas/core/arrays/integer.py in __getitem__(self, item)
    375             item = check_bool_array_indexer(self, item)
    376 
--> 377         return type(self)(self._data[item], self._mask[item])
    378 
    379     def _coerce_to_ndarray(self, dtype=None, na_value=lib._no_default):

IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
```

So that raises the following questions:

- This should probably simply work for the above case? (converting the IntegerArray to a numpy integer array, instead of object array, so numpy's indexing works) We might want to combine this with the boolean array checking?
- What if there are missing values? This should probably simply raise an error for now (which is what pandas 0.25 also does), although we could consider propagating an NA value as well, I think.

For Series, this seems to work partly. For `iloc` it works for the case without missing values. For `__getitem__` you get the same error as above. 
"
224314096,16137,Add NaTType to tslib public api,dhirschfeld,closed,2017-04-26T01:02:26Z,2020-01-29T12:22:27Z,"When testing the new `0.20.0` release candidate I found that the removal of `NaTType`  from the `tslib` public api breaks `odo`.

If this api breakage is actually intentional/desired I'm ok with this being closed however it should probably get a mention in the backwards incompatible API changes section of the whatsnew.

Whilst a deprecation warning is raised in https://github.com/pandas-dev/pandas/commit/648ae4f03622d8eafe1ca3b833bd6a99f56bece4#diff-d6b2c5dbe1e7fbda4ed4b87e0c49399f the removal of NaTType breaks the previous public api rendering the warning ineffective and not giving any 3rd party libraries a chance to make the necessary changes and cut a release"
32305270,6978,SeriesGroupBy and DataFrameGroupBy are missing mode() function,dov,closed,2014-04-27T06:04:11Z,2020-01-29T12:37:48Z,"The mode() function is missing from SeriesGroupBy and DataFrameGroupBy. It should be implemented with the same semanthics as as mean(), median(), max(), etc.
"
552285613,31150,API: generalized check_array_indexer for validating array-like getitem indexers,jorisvandenbossche,closed,2020-01-20T13:00:52Z,2020-01-29T14:46:57Z,"Closes https://github.com/pandas-dev/pandas/issues/30738. Also fixes the performance issue for other arrays from https://github.com/pandas-dev/pandas/issues/30744, and related to https://github.com/pandas-dev/pandas/pull/30308#issuecomment-571146305

This generalizes the `check_bool_array_indexer` helper method that we added for 1.0.0 to not be specific to boolean arrays, but any array-like input, and ensures that the output is a proper numpy array that can be used to index into numpy arrays.

I think such a more general ""check"" is useful, to avoid that all (external+internal) EAs need to do what the test EAs are already doing (checking for integer arrays as well) at https://github.com/pandas-dev/pandas/blob/master/pandas/tests/extension/decimal/array.py#L118-L126, and also to fix the performance issue in a general way (as was now only done for Categorical in https://github.com/pandas-dev/pandas/pull/30747/files)

If we agree on the general idea, I still need to clean up this PR (eg remove the existing `check_bool_array_indexer`, update the extending docs, etc)

cc @TomAugspurger "
556615739,31411,COMPAT: dont pass dtype to datetime64,jbrockmendel,closed,2020-01-29T03:34:46Z,2020-01-29T15:51:10Z,Hopefully fixes npdev build
556926977,31421,Backport PR #31411 on branch 1.0.x (COMPAT: dont pass dtype to datetime64),meeseeksmachine,closed,2020-01-29T15:13:02Z,2020-01-29T16:13:05Z,Backport PR #31411: COMPAT: dont pass dtype to datetime64
556908391,31419,Backport PR #31150: API: generalized check_array_indexer for validating array-like getitem indexers,jorisvandenbossche,closed,2020-01-29T14:44:14Z,2020-01-29T16:30:33Z,Backport https://github.com/pandas-dev/pandas/pull/31150
556925996,31420,Can't pre-define dtype when reading data,abrahamdu,closed,2020-01-29T15:11:26Z,2020-01-29T17:29:26Z,"I am reading a pipe delimited file without headings into Pandas and I am using Pandas version 0.24.2. And this is public data so no worries around confidentiality.

The data looks like:

999778247820|R|JPMORGAN CHASE BANK, NATIONAL ASSOCIATION|7.375|113000|360|02/2001|04/2001|95|95|1|52|665|Y|P|SF|1|P|IL|601|30|FRM||1|N
999783196683|R|OTHER|7.25|59000|360|01/2001|04/2001|97|97|2|43|682|Y|P|PU|1|P|HI|967|30|FRM|676|1|N
999783470376|C|BANK OF AMERICA, N.A.|7.875|110000|360|12/2000|02/2001|74|74|2|26|700|N|P|SF|1|P|NY|125||FRM|698||N
999786911479|C|BANK OF AMERICA, N.A.|7.5|57000|360|12/2000|02/2001|90|90|1|28|699|N|P|SF|1|P|TX|781|25|FRM||1|N
999786913710|R|JPMORGAN CHASE BANK, NA|7.125|114000|360|01/2001|04/2001|73|73|2|16|745|N|C|SF|1|P|WA|992||FRM|||N
999788833695|B|OTHER|9|50000|360|10/2000|12/2000|90|90|2|40|674|N|P|SF|2|I|WI|535|25|FRM|737|1|N


This is the code I am using:

```python
orig_files_fnma = glob.glob(""/...1/Acquisition*.txt"")

col_names = [""loan_id"", ""origination_channel"",""seller_name"",""original_interest_rate"",""original_upb"",""original_loan_term"",""origination_date"",""first_payment_date"",""original_ltv"",""original_cltv"",""number_of_borrowers"",""original_dti"",
            ""borrower_fico_at_origination"",""first_time_home_buyer_indicator"", ""loan_purpose"",""property_type"",""number_of_units"",""occupancy_type"",""property_state"",""zip_code_short"",""primary_mortgage_insurance_percent"",
            ""product_type"",""coborrower_fico_at_origination"",""mortgage_insurance_type"",""relocation_mortgage_indicator""]

col_type = {""loan_id"": ""object"",""origination_channel"": ""object"",""seller_name"": ""object"",""original_interest_rate"": ""float"",""original_upb"": ""float"",""original_loan_term"": ""int"",""origination_date"": ""object"",
            ""first_payment_date"": ""object"",""original_ltv"": ""object"",""original_cltv"": ""object"",""number_of_borrowers"": ""int"",""original_dti"": ""float"",""borrower_fico_at_origination"": ""int"",
            ""first_time_home_buyer_indicator"": ""object"", ""loan_purpose"": ""object"",""property_type"": ""object"",""number_of_units"": ""int"",""occupancy_type"": ""object"",""property_state"": ""object"",
            ""zip_code_short"": ""object"",""primary_mortgage_insurance_percent"": ""float"",
            ""product_type"": ""object"",""coborrower_fico_at_origination"": ""int"",""mortgage_insurance_type"": ""object"",""relocation_mortgage_indicator"": ""object""}

dfs = []
temp_df = []

for orig_files_fnma in orig_files_fnma:
    temp_df = pd.read_csv(orig_files_fnma, sep = '|', header = None, names = col_names, dtype = col_type, index_col = None, parse_dates=True, verbose = True, engine='python')
    dfs.append(temp_df)
```
Always getting the following errors:

```python
Filled 1 NA values in column original_ltv
Filled 52 NA values in column original_cltv
ValueError: Unable to convert column number_of_borrowers to type int
```

I do find out if I don't pre-define the dtype and .astype to change the data type after loading. But ask if possible that I can pre-define the data type first like the code above.

Also, I want to define the length of object for 20 length. What is the right code to do so?

Thanks a lot!"
556470381,31404,Add pandas blog to feed,TomAugspurger,closed,2020-01-28T21:10:39Z,2020-01-29T17:34:38Z,I don't think this needs to be backported.
557007974,31424,Backport PR #31423 on branch 1.0.x (PERF: postpone imports in Index constructor),meeseeksmachine,closed,2020-01-29T17:25:43Z,2020-01-29T18:23:37Z,Backport PR #31423: PERF: postpone imports in Index constructor
556953712,31423,PERF: postpone imports in Index constructor,jorisvandenbossche,closed,2020-01-29T15:54:19Z,2020-01-29T18:38:46Z,"xref https://github.com/pandas-dev/pandas/issues/30790, it's for improving this one mentioned in that issue:

```
arr_str = np.array([""foo"", ""bar"", ""baz""], dtype=object)
%timeit pd.Index(arr_str)
```

Only importing the index classes when we actually need them gives a 15-20% speedup.

Last week when profiling the Index constructor, I noticed that those imports actually take quite a bit of time (and it's a change that was introduced after 0.25, https://github.com/pandas-dev/pandas/pull/28141). I didn't get any further with profiling (this alone doesn't explain the full slowdown), but thought it's an easy one to already do anyway."
554356566,31256,Regression bugs when applying GroupBy Aggregations to Categorical columns,frances-h,closed,2020-01-23T19:25:34Z,2020-01-29T20:20:53Z,"
```python
>>> import pandas as pd

>>> ids = range(5)
>>> groups = pd.Series([0, 1, 1, 2, 2])
>>> values = pd.Categorical([0, 0, 0, 0, 1])
>>> df = pd.DataFrame({
            'id': ids,
            'groups': groups,
            'value': values
    }).set_index('id')

>>> df.groupby('groups').agg(pd.Series.nunique)
       value
groups
0          1
1          1
2        NaN

>>> df.groupby('groups').agg(pd.Series.nunique)['value'].dtype
CategoricalDtype(categories=[0, 1], ordered=False)

>>> df.groupby('groups').agg(pd.Series.count)
       value
groups
0          1
1        NaN
2        NaN

>>> df.groupby('groups').agg(pd.Series.mode)
                                                                           value
groups
0                                                                              0
1              0    0
Name: value, dtype: category
Categories (2, int64): [0, 1]
2       0    0
1    1
Name: value, dtype: category
Categories (2, int64): [0, 1]

```
#### Problem description
There's some regression bugs in `1.0.0rc0` when applying groupby aggregations to Categorical columns. All of these examples work as expected in `0.25.3`. 

Applying `pd.Series.nunique` and `pd.Series.count` both result in `NaN` for some groupings, the result has a Categorical dtype. The resulting dtype should be int64 and `NaN` should not be produced.

Applying `pd.Series.mode` returns the correct result for the first grouping but produces nonsensical output for subsequent groupings. 


#### Expected Output
Aggregations should work as expected on Categorical columns as they do in `0.25.3`:
```python
>>> df.groupby('groups').agg(pd.Series.nunique)
        value
groups
0           1
1           1
2           2

>>> df.groupby('groups').agg(pd.Series.nunique)['value'].dtype
dtype('int64')

>>> df.groupby('groups').agg(pd.Series.count)
        value
groups
0           1
1           2
2           2

>>> df.groupby('groups').agg(pd.Series.mode)
         value
groups
0      0     0
1      0     0
2      0     0
       1     1
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.0rc0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.1
setuptools       : 45.1.0
Cython           : None
pytest           : 5.2.0
hypothesis       : None
sphinx           : 2.0.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.2.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.0.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.2.0
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.47.0

</details>
"
557098578,31429,DOC: separate section with experimental features in 1.0.0 whatsnew,jorisvandenbossche,closed,2020-01-29T20:30:27Z,2020-01-30T09:55:56Z,"Just a thought I had, to put the experimental features (pd.NA, string, boolean dtypes) in a separate section, better highlighting that those are experimental. 

Since @TomAugspurger basically wants to release now, this is certainly not a blocker :)"
557118605,31431,DOC: separate section with experimental features in 1.0.0 whatsnew (#…,TomAugspurger,closed,2020-01-29T21:13:04Z,2020-01-30T09:54:02Z,"…31429)

Backport https://github.com/pandas-dev/pandas/pull/3142"
557115432,31430,release date,TomAugspurger,closed,2020-01-29T21:06:32Z,2020-01-29T21:43:38Z,
557133356,31432,Backport PR #31430 on branch 1.0.x (release date),meeseeksmachine,closed,2020-01-29T21:43:26Z,2020-01-30T09:53:27Z,Backport PR #31430: release date
548486670,30931,Attempted install of 1.0.0.0rc0  with conda fails,discdiver,closed,2020-01-11T22:13:24Z,2020-01-30T01:40:01Z,"Installing the release candidate into a new Python 3.7 conda environment throws errors.

Do I need to be doing something differently?

#### Other notes:
- Installing jupyterlab in the same conda environment works fine. 
- Creating a virtual environment with venv and using pip to install pandas 1.0.0rc0 works fine on the same machine.

#### Command
```
conda install -c conda-forge/label/rc pandas==1.0.0rc0
```

#### Error

```
Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: / 
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
failed                                                                          

UnsatisfiableError: The following specifications were found to be incompatible with each other:



Package pip conflicts for:
defaults/osx-64::python==3.7.5=h359304d_0 -> pip
Package sqlite conflicts for:
defaults/osx-64::python==3.7.5=h359304d_0 -> sqlite[version='>=3.30.1,<4.0a0']
Package openssl conflicts for:
defaults/osx-64::python==3.7.5=h359304d_0 -> openssl[version='>=1.1.1d,<1.1.2a']
Package pCollecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: / 
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
failed                                                                                             

UnsatisfiableError: The following specifications were found to be incompatible with each other:



Package tk conflicts for:
python=3.7 -> tk[version='>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']
Package libcxx conflicts for:
pandas==1.0.0rc0 -> libcxx[version='>=9.0.1']
python=3.7 -> libcxx[version='>=4.0.1']
Package openssl conflicts for:
python=3.7 -> openssl[version='>=1.0.2o,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1b,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']
Package pytz conflicts for:
pandas==1.0.0rc0 -> pytz[version='>=2017.2']
Package python-dateutil conflicts for:
pandas==1.0.0rc0 -> python-dateutil[version='>=2.6.1']
Package readline conflicts for:
python=3.7 -> readline[version='>=7.0,<8.0a0']
Package libffi conflicts for:
python=3.7 -> libffi[version='>=3.2.1,<4.0a0']
Package pip conflicts for:
python=3.7 -> pip
Package ncurses conflicts for:
python=3.7 -> ncurses[version='>=6.1,<7.0a0']
Package xz conflicts for:
python=3.7 -> xz[version='>=5.2.4,<6.0a0']
Package zlib conflicts for:
python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']
Package numpy conflicts for:
pandas==1.0.0rc0 -> numpy[version='>=1.14.6,<2.0a0']
Package sqlite conflicts for:
python=3.7 -> sqlite[version='>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.25.3,<4.0a0|>=3.26.0,<4.0a0|>=3.27.2,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']

(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Collecting package metadata (current_repodata.json): done
-bash: syntax error near unexpected token `('
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Solving environment: failed with initial frozen solve. Retrying with flexible solve.
-bash: Solving: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
-bash: Solving: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Collecting package metadata (repodata.json): done
-bash: syntax error near unexpected token `('
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Solving environment: failed with initial frozen solve. Retrying with flexible solve.
-bash: Solving: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Solving environment: / 
-bash: Solving: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Found conflicts! Looking for incompatible packages.
-bash: Found: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ This can take several minutes.  Press CTRL-C to abort.
-bash: This: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ failed                                                                          
-bash: failed: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ 
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ UnsatisfiableError: The following specifications were found to be incompatible with each other:
-bash: UnsatisfiableError:: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ 
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ 
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ 
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Package pip conflicts for:
-bash: Package: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ defaults/osx-64::python==3.7.5=h359304d_0 -> pip
-bash: defaults/osx-64::python==3.7.5=h359304d_0: No such file or directory
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Package sqlite conflicts for:
-bash: Package: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ defaults/osx-64::python==3.7.5=h359304d_0 -> sqlite[version='>=3.30.1,<4.0a0']
-bash: defaults/osx-64::python==3.7.5=h359304d_0: No such file or directory
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Package openssl conflicts for:
-bash: Package: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ defaults/osx-64::python==3.7.5=h359304d_0 -> openssl[version='>=1.1.1d,<1.1.2a']
-bash: defaults/osx-64::python==3.7.5=h359304d_0: No such file or directory
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Package pytz conflicts for:
-bash: Package: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ pandas==1.0.0rc0 -> pytz[version='>=2017.2']
-bash: -: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Package numpy conflicts for:
-bash: Package: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ pandas==1.0.0rc0 -> numpy[version='>=1.14.6,<2.0a0']
-bash: -: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Package ncurses conflicts for:
-bash: Package: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ defaults/osx-64::python==3.7.5=h359304d_0 -> ncurses[version='>=6.1,<7.0a0']
-bash: defaults/osx-64::python==3.7.5=h359304d_0: No such file or directory
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Package readline conflicts for:
-bash: Package: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ defaults/osx-64::python==3.7.5=h359304d_0 -> readline[version='>=7.0,<8.0a0']
-bash: defaults/osx-64::python==3.7.5=h359304d_0: No such file or directory
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Package libcxx conflicts for:
-bash: Package: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ pandas==1.0.0rc0 -> libcxx[version='>=9.0.1']
-bash: -: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ defaults/osx-64::python==3.7.5=h359304d_0 -> libcxx[version='>=4.0.1']
-bash: defaults/osx-64::python==3.7.5=h359304d_0: No such file or directory
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Package libffi conflicts for:
-bash: Package: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ defaults/osx-64::python==3.7.5=h359304d_0 -> libffi[version='>=3.2.1,<4.0a0']
-bash: defaults/osx-64::python==3.7.5=h359304d_0: No such file or directory
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Package xz conflicts for:
-bash: Package: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ defaults/osx-64::python==3.7.5=h359304d_0 -> xz[version='>=5.2.4,<6.0a0']
-bash: defaults/osx-64::python==3.7.5=h359304d_0: No such file or directory
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Package python-dateutil conflicts for:
-bash: Package: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ pandas==1.0.0rc0 -> python-dateutil[version='>=2.6.1']
-bash: -: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Package tk conflicts for:
-bash: Package: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ defaults/osx-64::python==3.7.5=h359304d_0 -> tk[version='>=8.6.8,<8.7.0a0']
-bash: defaults/osx-64::python==3.7.5=h359304d_0: No such file or directory
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ Package zlib conflicts for:
-bash: Package: command not found
(myenv) Jeffs-MacBook-Pro:course-info jeffhale$ defaults/osx-64::python==3.7.5=h359304d_0 -> zlib[version='>=1.2.11,<1.3.0a0']
```
"
548113711,30886,"Regression in DataFrame.sum with mixed datetime, numeric and missing values",TomAugspurger,closed,2020-01-10T14:36:13Z,2020-01-30T05:39:19Z,"On 0.25.3

```python
In [3]: df = pd.DataFrame({""A"": pd.date_range(""2000"", periods=4), ""B"": [1, 2, 3, 4]}).reindex([2, 3, 4])

In [4]: df.sum()
```

```pytb
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
TypeError: unsupported operand type(s) for +: 'Timestamp' and 'Timestamp'

The above exception was the direct cause of the following exception:

SystemError                               Traceback (most recent call last)
<ipython-input-4-7e5fdb616c56> in <module>
----> 1 df.sum()

~/sandbox/pandas/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, min_count, **kwargs)
  11035             skipna=skipna,
  11036             numeric_only=numeric_only,
> 11037             min_count=min_count,
  11038         )
  11039

~/sandbox/pandas/pandas/core/frame.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)
   7885             values = self.values
   7886             try:
-> 7887                 result = f(values)
   7888
   7889                 if filter_type == ""bool"" and is_object_dtype(values) and axis is None:

~/sandbox/pandas/pandas/core/frame.py in f(x)
   7843
   7844         def f(x):
-> 7845             return op(x, axis=axis, skipna=skipna, **kwds)
   7846
   7847         def _get_data(axis_matters):

~/sandbox/pandas/pandas/core/nanops.py in _f(*args, **kwargs)
     67             try:
     68                 with np.errstate(invalid=""ignore""):
---> 69                     return f(*args, **kwargs)
     70             except ValueError as e:
     71                 # we want to transform an object array

~/sandbox/pandas/pandas/core/nanops.py in nansum(values, axis, skipna, min_count, mask)
    491     elif is_timedelta64_dtype(dtype):
    492         dtype_sum = np.float64
--> 493     the_sum = values.sum(axis, dtype=dtype_sum)
    494     the_sum = _maybe_null_out(the_sum, axis, mask, values.shape, min_count=min_count)
    495

~/Envs/pandas-dev/lib/python3.7/site-packages/numpy/core/_methods.py in _sum(a, axis, dtype, out, keepdims, initial, where)
     36 def _sum(a, axis=None, dtype=None, out=None, keepdims=False,
     37          initial=_NoValue, where=True):
---> 38     return umr_sum(a, axis, dtype, out, keepdims, initial, where)
     39
     40 def _prod(a, axis=None, dtype=None, out=None, keepdims=False,

~/sandbox/pandas/pandas/_libs/tslibs/c_timestamp.pyx in pandas._libs.tslibs.c_timestamp._Timestamp.__add__()

~/sandbox/pandas/pandas/_libs/tslibs/c_timestamp.pyx in pandas._libs.tslibs.c_timestamp.integer_op_not_supported()

SystemError: <class 'TypeError'> returned a result with an error set
```

On 0.25.x

```python
In [1]: import pandas as pd
df
In [2]: df = pd.DataFrame({""A"": pd.date_range(""2000"", periods=4), ""B"": [1, 2, 3, 4]}).reindex([2, 3, 4])

In [3]: df.sum()
Out[3]:
B    7.0
dtype: float64
```

cc @jbrockmendel "
71950145,10019,support for 3d scatter plot if both axes are numerical values,den-run-ai,closed,2015-04-29T18:23:15Z,2020-01-30T09:38:41Z,
557096451,31428,BUG: regression when applying groupby aggregation on categorical colu…,TomAugspurger,closed,2020-01-29T20:26:12Z,2020-01-30T10:18:05Z,Backport of https://github.com/pandas-dev/pandas/pull/31359
557338522,31442,Backport PR #31438 on branch 1.0.x (DOC: Fix 1.0.0 contributors),meeseeksmachine,closed,2020-01-30T08:31:13Z,2020-01-30T10:21:46Z,Backport PR #31438: DOC: Fix 1.0.0 contributors
557338961,31443,Backport PR #30907 on branch 1.0.x (DOC: add 1.0.1 whatsnew file),meeseeksmachine,closed,2020-01-30T08:32:16Z,2020-01-30T10:22:25Z,Backport PR #30907: DOC: add 1.0.1 whatsnew file
557362577,31445,Feature request: code sugar for index.get_level_values,mhooreman,closed,2020-01-30T09:21:09Z,2020-01-30T11:20:25Z,"(Sorry, I have closed issue 31444 by mistake, hence re-creating it)
#### Code Sample

```python
data.index.get_level_values('foobar') == 42
# would like to have data.index['foobar'] == 42
```
#### Problem description

Filtering data frame row by one column of multi index is quite common.

Unless I've missed something, the current implementation need `get_index_value`. It would be nice to have a more direct way to select one MultiIndex column, like we can do with the data frame columns.

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-74-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0
Cython           : 0.29.14
pytest           : None
hypothesis       : None
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

```

</details>
"
557237772,31439,"""Tuple of Lists"" method doesn't applicable for DataFrame",GYHHAHA,closed,2020-01-30T02:59:08Z,2020-01-30T12:31:24Z,"#### Code Sample, a copy-pastable example if possible

```python
>>>df = pd.DataFrame({'col':np.ones(9)},index=pd.MultiIndex.\
                       from_product([['A','B','C'],['a','b','c']])).sort_index()
>>>df['col'].loc[(['A','B'],['b','c'])]
A  b    1.0
    c    1.0
B  b    1.0
    c    1.0
Name: col, dtype: float64
>>>df.loc[(['A','B'],['b','c'])]
KeyError: ""None of [Index(['b', 'c'], dtype='object')] are in the [columns]""
```
#### Problem description

It seems that the ""tuple of lists"" method doesn't applicable for dataframe.
Originally, I thought these two operations will return the same numerical result."
470694636,27492,RLS: 1.0.0,jreback,closed,2019-07-20T17:02:30Z,2020-01-30T12:34:23Z,"discussion for 1.0. 

We are focusing all deprecation removals and we should try to release by September 2019."
554884807,31292,CI: Updated version of macos image,ShaharNaveh,closed,2020-01-24T18:22:05Z,2020-01-30T12:42:20Z,"- [x] closes #31281
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
554941426,31295,"DOC: correct examples for ""is_iterator""",ShaharNaveh,closed,2020-01-24T20:34:49Z,2020-01-30T13:39:48Z,"- [x] closes #31291
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
556861566,31418,Sanitize column names,zahash,closed,2020-01-29T13:27:32Z,2020-01-30T13:43:27Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
557172248,31434,pandas web and docs structure,TomAugspurger,closed,2020-01-29T23:10:23Z,2020-01-30T14:22:11Z,"The site served by `web` assumes that the docs are served from a `/docs/` directory relative to the root. The content of that directory should be equivalent to what we have at https://pandas.pydata.org/pandas-docs/version/1.0.0/, so `/version/1.0.0/` or `/version/stable/`.

Should I symlink `/docs/` to `/version/stable/`? Does that mess with canonical urls?

cc @datapythonista."
548356729,30907,DOC: add 1.0.1 whatsnew file,WillAyd,closed,2020-01-11T00:57:56Z,2020-01-30T15:59:37Z,
557534949,31454,TYP: Type annotaion for test decorators,ShaharNaveh,closed,2020-01-30T14:35:35Z,2020-01-30T16:56:09Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I am not 100% sure this is something we want, or at least with this implementation.

cc @simonjayhawkins "
557211112,31436,TST: make sure fixture params are sorted list,jbrockmendel,closed,2020-01-30T01:14:52Z,2020-01-30T19:21:33Z,ATM running tests with --n 4 breaks locally because the tests that get collected dont match across processes.
555949022,31377,DEPR: Deprecate core,lithomas1,closed,2020-01-28T02:36:38Z,2020-01-31T03:08:52Z,"- [x] closes #27522 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I haven't actually performed the refactor(pandas.core -> pandas._core) yet so tests are failing because of the warnings. 
"
4297151,1135,df.groupby('key').groups printed all: problem with large arrays,jorisvandenbossche,closed,2012-04-26T07:52:16Z,2020-01-31T04:06:39Z,"When you are working with a large array, it is not printed out in its entirety in the console, but when you have grouped them with `groupby('key')`, the groups are all printed out. Would it be possible to also restrict the output of `groupby('key').groups` to eg the first and last groups?

I was working with a rather large dataframe (around 80000 rows), and first it took a long time to print it all, and second the console got stuck for a while (but that could also be an issue with spyder).
I know it is not very useful to print out the groups, but I was curious to see what it would look like, but it was not a very good idea with such a large array.


```python
>>> import numpy as np
>>> import pandas as pd
>>> df = pd.DataFrame(np.random.randn(100000, 4), columns=list('abcd'))
>>> df['g'] = np.random.randint(0, 100, 100000)
>>> df.groupby('g').groups  # this repr should be truncated
```"
556371984,31397,TYP: Annotate,jbrockmendel,closed,2020-01-28T17:53:50Z,2020-01-31T04:10:15Z,AFAICT having the more specific annotations for the Index subclasses _data attribute is a blocker for a bunch of other annotations.
556439019,31402,CLN: remove maybe_box,jbrockmendel,closed,2020-01-28T20:06:28Z,2020-01-31T04:13:19Z,Recently removed the last usaged from TDI/DTI.
555174734,31321,CLN: dont return anything from _validate_indexer; annotations,jbrockmendel,closed,2020-01-26T04:07:50Z,2020-01-31T04:15:16Z,
555269128,31329,BUG: ser.at match ser.loc with Float64Index,jbrockmendel,closed,2020-01-26T18:39:44Z,2020-01-31T04:16:09Z,xref #31163.
557063966,31426,ENH: implement _typing.DtypeObj,jbrockmendel,closed,2020-01-29T19:20:35Z,2020-01-31T04:17:40Z,"like Dtype, but doesnt include strings."
557665571,31468,REF: use inherit_names for DTI,jbrockmendel,closed,2020-01-30T18:13:38Z,2020-01-31T04:19:20Z,"xref #31427, #31433.  With all three of these in, we can then remove indexes.datetimelike.DatetimelikeDelegateMixin

Using inherit_names, we also have the option of making tz, tzinfo, dtype cache_readonly if that turns out to make a difference."
557161088,31433,REF: use inherit_names for PeriodIndex,jbrockmendel,closed,2020-01-29T22:42:03Z,2020-01-31T04:19:53Z,analogous to #31427.
554859564,31288,REF: combine all alignment into _align_method_FRAME,jbrockmendel,closed,2020-01-24T17:22:04Z,2020-01-31T04:23:10Z,"This puts us within striking distance of de-duplicating _flex_comp_method_FRAME and _comp_method_FRAME, with _arith_method_FRAME not far behind"
557593645,31460,DOC: Example of DataFrame.to_csv() is not showing properly,srvanrell,closed,2020-01-30T16:05:51Z,2020-01-31T07:19:57Z,"#### Problem description

Example of DataFrame.to_csv() is not showing properly. See the screenshoot below:

![Screenshot_20200130_130131](https://user-images.githubusercontent.com/1920463/73466588-d48aa980-4360-11ea-9432-6e52155fb00d.png)

#### Expected Output

I'll propose a PR fixing the docstring
"
557597286,31461,DOC: Fix DataFrame.to_csv example,srvanrell,closed,2020-01-30T16:11:16Z,2020-01-31T07:19:58Z,"- [x] closes #31460
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
503906945,28838,DOC: fix code-block in the reshaping docs,oktaysabak,closed,2019-10-08T08:38:48Z,2020-01-31T07:39:17Z,"- [x] closes #28819

"
555081140,31307,Add test for multiindex json,rushabh-v,closed,2020-01-25T12:42:39Z,2020-01-31T09:03:28Z,"- [x] related to #31028 and adds test for #27618 
- [x] tests added / passed
- [x] passes `black pandas`
"
557958759,31481,Backport PR #31461 on branch 1.0.x (DOC: Fix DataFrame.to_csv example),meeseeksmachine,closed,2020-01-31T07:20:16Z,2020-01-31T09:16:48Z,Backport PR #31461: DOC: Fix DataFrame.to_csv example
556134589,31387,CLN: more consistent error message for construct_from_string wrong type,simonjayhawkins,closed,2020-01-28T11:03:21Z,2020-01-31T09:23:43Z,
555812622,31365,TYP: pandas/core/computation/expr.py,simonjayhawkins,closed,2020-01-27T20:26:12Z,2020-01-31T09:24:50Z,
555831043,31367,TYP: check_untyped_defs core.reshape.reshape,simonjayhawkins,closed,2020-01-27T21:02:47Z,2020-01-31T09:25:59Z,"pandas\core\reshape\reshape.py:378: error: ""Index"" has no attribute ""levels""; maybe ""nlevels""?
pandas\core\reshape\reshape.py:381: error: ""Index"" has no attribute ""codes""
pandas\core\reshape\reshape.py:383: error: ""Index"" has no attribute ""codes""
pandas\core\reshape\reshape.py:436: error: Incompatible types in assignment (expression has type ""_Unstacker"", variable has type ""partial[_Unstacker]"")      
pandas\core\reshape\reshape.py:444: error: ""partial[_Unstacker]"" has no attribute ""get_result""
pandas\core\reshape\reshape.py:1000: error: Incompatible types in assignment (expression has type ""None"", variable has type ""Index"")
pandas\core\reshape\reshape.py:1009: error: Incompatible types in assignment (expression has type ""float"", variable has type ""int"")
pandas\core\reshape\reshape.py:1013: error: Need type annotation for 'sp_indices'"
558068154,31490,DOC: Fix the description of the 'day' field accessor in DatetimeArray,alejandrohall,closed,2020-01-31T11:11:56Z,2020-01-31T11:51:18Z,Fix a wrong description in the field accessor `day` of the DatetimeArray class
557572744,31458,"Fix to_csv and to_excel links on read_csv, read_table and read_excel See Also docs",srvanrell,closed,2020-01-30T15:33:43Z,2020-01-31T12:08:47Z,"- [x] closes #31448
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
553099696,31183,BUG: Series/Frame invert dtypes,TomAugspurger,closed,2020-01-21T20:06:08Z,2020-02-01T14:53:28Z,"Fixes NDFrame.__invert__ to preserve dtypes by applying things blockwise. Notably, this fixes `~Series[BooleanDtype]`.

This also provides a pattern for other unary ops, though I think that those are less pressing for 1.0, which should probably have a working `~Series[BooleanDtype]`

cc @jorisvandenbossche 
"
558086024,31492,Large dataframes aren't truncated in LaTeX output,mojones,closed,2020-01-31T11:50:07Z,2020-01-31T13:03:04Z,"#### Code Sample, a copy-pastable example if possible

```python
import numpy as np
pd.DataFrame(np.random.rand(300,4),columns=['a','b','c','d']).to_latex()

```
#### Problem description

I'm using Jupyter notebook to write a long document using pandas, which I eventually will nbconvert to a PDF. In the notebook with HTML output, long dataframes are truncated as expected and I can control the number of rows with `pd.set_option('max_rows', 5)`. When displaying as LaTeX, however, long dataframes are not truncated, so the above code will generate a 300 row LaTeX table. My real dataframes are much longer. This breaks PDF export. 

Obviously I can get round this by just calling `head()`. However, given that this a tutorial, it would be nice to be able to display a table in the PDF that will match the HTML output, i.e. with the rows of dots in the middle to represent data not shown. Could `to_latex()` honour the `max_rows` option? Is there any workaround here? 

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-74-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.25.3
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : None
pytest           : 5.3.4
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.3.3
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.3
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
</details>
"
558104061,31494,DOC: move whatnew entry for invert from 1.0.0 t 1.0.1,jorisvandenbossche,closed,2020-01-31T12:29:48Z,2020-01-31T14:04:57Z,"see https://github.com/pandas-dev/pandas/pull/31493, forgot to backport, so the fix will only be included in 1.0.1"
555651500,31346,TYP: core.arrays.boolean,simonjayhawkins,closed,2020-01-27T15:29:55Z,2020-01-31T14:58:25Z,
555659950,31349,TYP: pandas/core/arrays/sparse/dtype.py,simonjayhawkins,closed,2020-01-27T15:43:14Z,2020-01-31T14:59:15Z,
556115162,31386,TYP: pandas/core/arrays/string_.py,simonjayhawkins,closed,2020-01-28T10:27:52Z,2020-01-31T15:00:18Z,
555654365,31347,TYP: core.arrays.integer,simonjayhawkins,closed,2020-01-27T15:34:19Z,2020-01-31T15:01:00Z,
555657819,31348,TYP: core.arrays.numpy_,simonjayhawkins,closed,2020-01-27T15:39:56Z,2020-01-31T15:02:46Z,
557903646,31480,CLN: remove DatetimelikeDelegateMixin,jbrockmendel,closed,2020-01-31T04:21:35Z,2020-01-31T15:48:02Z,Made possibly by #31433.
556558702,31408,REF: docstrings that dont need to go into shared_docs,jbrockmendel,closed,2020-01-28T23:54:55Z,2020-01-31T15:50:08Z,
557225215,31437,BUG: PeriodIndex.is_monotonic with leading NaT,jbrockmendel,closed,2020-01-30T02:09:48Z,2020-01-31T15:53:04Z,Before long I expect to just have PeriodEngine subclass DatetimeEngine.
557071471,31427,REF: use inherit_names for TimedeltaIndex,jbrockmendel,closed,2020-01-29T19:34:52Z,2020-01-31T16:04:55Z,"ATM we have two separate ways of doing inheritance/pass-through for our EA-backed Indexes.  This moves TDI over to all-inherit_names,  Will do the same for other EA-Indexes in their own PRs."
558053419,31489,~ operator on Series with BooleanDtype casts to object and Series with object dtype gives TypeError comparing to Series with BooleanDtype,gerritholl,closed,2020-01-31T10:42:26Z,2020-01-31T16:23:46Z,"#### Code Sample, a copy-pastable example if possible

Using pandas 1.0.0:

```python
~pandas.Series([True, False], dtype=pandas.BooleanDtype())
```

Or:

```python
s1 = pandas.Series([True, False], dtype=pandas.BooleanDtype())
s2 = pandas.Series([False, True], dtype=pandas.BooleanDtype())
(~s1).equals(s2) and (~s2).equals(s1)
```

#### Problem description

Using the `pandas.BooleanDtype()` dtype, the `~` operator unexpectedly:

* results in a `Series` with dtype `object` instead of `pandas.BooleanDtype()`
* operates on the boolean values as if they were integers (resulting in `-2` for `~True` and `-1` for `~False`), rather than booleans (resulting in `False` for `~True` and `True` for `~False`).

This is unexpected and undesirable, because:

* On numpy arrays or `Series` with dtype `np.bool_`, the same operation behaves differently:
  - The dtype remains `np.bool_`
  - `~True` results in `False` and `~False` results in `True`
* When having a Series with a boolean dtype, the `~` operator is a convenient way to negate the value

As a workaround, I'm using `numpy.logical_not(s1)`, which does preserve the dtype and have the expected result.

The second example results in an exception `TypeError: data type not understood`, because (~s1) with its `object` dtype doesn't understand the comparison against `s2` with its `pandas.BooleanDtype()` dtype.

#### Expected Output

I would expect that:

* the first example results in `pandas.Series([False, True], dtype=pandas.BooleanDtype())`
* the second example results in `True`

#### Output of ``pd.show_versions()``

<details>

In [35]: pandas.show_versions()
/media/nas/x21324/miniconda3/envs/py37e/lib/python3.7/site-packages/fastparquet/dataframe.py:5: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.
  from pandas.core.index import CategoricalIndex, RangeIndex, Index, MultiIndex

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.12.14-lp150.12.82-default
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.0
numpy            : 1.17.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200119
Cython           : 0.29.14
pytest           : 5.3.5
hypothesis       : None
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.0
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : 0.14.1
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
558102625,31493,Backport PR #31183: BUG: Series/Frame invert dtypes',jorisvandenbossche,closed,2020-01-31T12:26:42Z,2020-01-31T16:24:04Z,"We forgot to backport this: https://github.com/pandas-dev/pandas/pull/31183

Closes #31489"
558151439,31496,Backport PR #31494 on branch 1.0.x (DOC: move whatnew entry for invert from 1.0.0 t 1.0.1),meeseeksmachine,closed,2020-01-31T14:04:52Z,2020-01-31T16:24:17Z,Backport PR #31494: DOC: move whatnew entry for invert from 1.0.0 t 1.0.1
557609759,31462,Some code cleanups,ShaharNaveh,closed,2020-01-30T16:30:24Z,2020-01-31T16:26:35Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
557229468,31438,DOC: Fix 1.0.0 contributors,TomAugspurger,closed,2020-01-30T02:24:22Z,2020-01-31T22:50:56Z,I've also applied this locally in the doc build env.
558234447,31500,"DOC: Replaced ""the the"" with ""the""",ShaharNaveh,closed,2020-01-31T16:29:37Z,2020-02-01T10:51:13Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
558523375,31533,Formatting of `pandas.set_option` seems broken,MarcoGorelli,closed,2020-02-01T10:56:37Z,2020-02-01T11:03:05Z,"![image](https://user-images.githubusercontent.com/33491632/73591030-7f29d600-44e1-11ea-8061-5ac027e3b398.png)

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.set_option.html"
558518085,31531,DOC issue of rendering for the docsting of pd.set_option,glemaitre,closed,2020-02-01T10:14:13Z,2020-02-01T13:46:17Z,"It seems that the `pd.set_option` docsting is not parsed properly by `sphinx`:

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.set_option.html

It was already the case in 0.25 doc but it was working in 0.23:

https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.set_option.html"
557665572,31469,REGR: __setitem__ with integer slices on Int/RangeIndex is broken (label instead of positional),amueller,closed,2020-01-30T18:13:38Z,2020-02-01T14:22:06Z,"There's an backward incompatible change in pandas 1.0 that I didn't find in the changelog. I might have just overlooked it though.

```python
import numpy as np
X = pd.DataFrame(np.zeros((100, 1)))
X[-4:] = 1
X
```
In pandas 0.25.3 or lower, this results in the last four entries of X to be 1 and all the others zero. In pandas 1.0, it results in all entries of X being 1.
I assume it's a change of indexing axis 0 or axis 1?"
558388913,31520,"REGR: to_datetime, unique with OOB values",TomAugspurger,closed,2020-01-31T21:45:02Z,2020-02-01T14:29:37Z,"Closes https://github.com/pandas-dev/pandas/issues/31491

This turned up two bugs:

1. `to_datetime(oob_values, cache=True, errors='coerce')` would raise. We had tests intended to catch this, but we didn't actually hit it because caching is disabled for inputs less than length 50. We only tested with length 3. I've updated the test to pass that threshold
2. The original report, that `to_datetime` was raising for non-ns resolution values that aren't OOB for ns resolution. This was a bug in `unique`. "
558082499,31491,"Error in pd.to_datetime, with argument np.datetime64 array of 51 elements",pajachiet,closed,2020-01-31T11:42:01Z,2020-02-01T14:29:37Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np
d = np.datetime64(""2000-01-01"", ""s"")

pd.to_datetime(np.full(50, d))
# Works

pd.to_datetime(np.full(51,  d))
# ERROR
# pandas._libs.tslibs.np_datetime.OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 29999249045-07-29 00:00:00



pd.to_datetime(pd.Series(np.full(51,  d)))

```
#### Problem description

With pandas 1.0, we get a new issue when using pd.to_datetime to convert an existing np.datetime64 array with 51 elements. 

This works with 50 elements, or when converting to a Series beforehand, so I believe the problems comes from inside implicit conversion.
"
557414538,31446,REGR: Array.__setitem__ failing with nullable boolean mask,mcitoler,closed,2020-01-30T10:53:50Z,2020-02-01T14:37:26Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
s = pd.Series([1, 2], dtype='Int64') 
s.where(s > 1)
```
#### Problem description
The code above raises

`TypeError: bad operand type for unary ~: 'BooleanArray'`

#### Expected Output
```
0    <NA>
1       2
dtype: Int64
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-76-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.0rc0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200119
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.0
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
558239893,31501,Can no longer slice DatetimeIndex with datetime.date values outside the index in 1.0.0,davidia,closed,2020-01-31T16:39:29Z,2020-02-01T14:56:13Z,"#### Code Sample
```python
import pandas as pd
import datetime as dt
s = pd.Series([0,1],pd.DatetimeIndex([dt.date(2019,1,1),dt.date(2019,1,2)]))
s[dt.date(2019,1,1):]  # works 
s[dt.date(2018,1,1):]  # error

```
#### Problem description

As of 1.0.0 you can no longer slice a DatetimeIndex with dt.date values outside the index.

#### Error


```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
pandas\_libs\index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

TypeError: an integer is required

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
   2645             try:
-> 2646                 return self._engine.get_loc(key)
   2647             except KeyError:

pandas\_libs\index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas\_libs\index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas\_libs\index.pyx in pandas._libs.index.DatetimeEngine._date_check_type()

KeyError: datetime.date(2018, 1, 1)

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
pandas\_libs\index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

TypeError: an integer is required

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\indexes\datetimes.py in get_loc(self, key, method, tolerance)
    714         try:
--> 715             return Index.get_loc(self, key, method, tolerance)
    716         except (KeyError, ValueError, TypeError):

C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
   2647             except KeyError:
-> 2648                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2649         indexer = self.get_indexer([key], method=method, tolerance=tolerance)

pandas\_libs\index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas\_libs\index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas\_libs\index.pyx in pandas._libs.index.DatetimeEngine._date_check_type()

KeyError: datetime.date(2018, 1, 1)

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
pandas\_libs\index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

KeyError: 1514764800000000000

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
   2645             try:
-> 2646                 return self._engine.get_loc(key)
   2647             except KeyError:

pandas\_libs\index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas\_libs\index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

KeyError: Timestamp('2018-01-01 00:00:00')

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
pandas\_libs\index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

KeyError: 1514764800000000000

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\indexes\datetimes.py in get_loc(self, key, method, tolerance)
    727                     stamp = stamp.tz_localize(self.tz)
--> 728                 return Index.get_loc(self, stamp, method, tolerance)
    729             except KeyError:

C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
   2647             except KeyError:
-> 2648                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2649         indexer = self.get_indexer([key], method=method, tolerance=tolerance)

pandas\_libs\index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas\_libs\index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

KeyError: Timestamp('2018-01-01 00:00:00')

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\indexes\base.py in get_slice_bound(self, label, side, kind)
   4841         try:
-> 4842             slc = self.get_loc(label)
   4843         except KeyError as err:

C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\indexes\datetimes.py in get_loc(self, key, method, tolerance)
    729             except KeyError:
--> 730                 raise KeyError(key)
    731             except ValueError as e:

KeyError: datetime.date(2018, 1, 1)

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-47-a23c04333556> in <module>
      3 s = pd.Series([0,1],pd.DatetimeIndex([dt.date(2019,1,1),dt.date(2019,1,2)]))
      4 s[dt.date(2019,1,1):]  # works
----> 5 s[dt.date(2018,1,1):]  # error

C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\series.py in __getitem__(self, key)
    908             key = check_bool_indexer(self.index, key)
    909 
--> 910         return self._get_with(key)
    911 
    912     def _get_with(self, key):

C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\series.py in _get_with(self, key)
    913         # other: fancy integer or otherwise
    914         if isinstance(key, slice):
--> 915             return self._slice(key)
    916         elif isinstance(key, ABCDataFrame):
    917             raise TypeError(

C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\series.py in _slice(self, slobj, axis, kind)
    863 
    864     def _slice(self, slobj: slice, axis: int = 0, kind=None):
--> 865         slobj = self.index._convert_slice_indexer(slobj, kind=kind or ""getitem"")
    866         return self._get_values(slobj)
    867 

C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\indexes\base.py in _convert_slice_indexer(self, key, kind)
   2961             indexer = key
   2962         else:
-> 2963             indexer = self.slice_indexer(start, stop, step, kind=kind)
   2964 
   2965         return indexer

C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\indexes\datetimes.py in slice_indexer(self, start, end, step, kind)
    806 
    807         try:
--> 808             return Index.slice_indexer(self, start, end, step, kind=kind)
    809         except KeyError:
    810             # For historical reasons DatetimeIndex by default supports

C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\indexes\base.py in slice_indexer(self, start, end, step, kind)
   4711         slice(1, 3)
   4712         """"""
-> 4713         start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind)
   4714 
   4715         # return a slice

C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\indexes\base.py in slice_locs(self, start, end, step, kind)
   4924         start_slice = None
   4925         if start is not None:
-> 4926             start_slice = self.get_slice_bound(start, ""left"", kind)
   4927         if start_slice is None:
   4928             start_slice = 0

C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\indexes\base.py in get_slice_bound(self, label, side, kind)
   4843         except KeyError as err:
   4844             try:
-> 4845                 return self._searchsorted_monotonic(label, side)
   4846             except ValueError:
   4847                 # raise the original KeyError

C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\indexes\base.py in _searchsorted_monotonic(self, label, side)
   4794     def _searchsorted_monotonic(self, label, side=""left""):
   4795         if self.is_monotonic_increasing:
-> 4796             return self.searchsorted(label, side=side)
   4797         elif self.is_monotonic_decreasing:
   4798             # np.searchsorted expects ascending sort order, have to reverse

C:\ProgramData\Miniconda3.7\lib\site-packages\pandas\core\indexes\datetimes.py in searchsorted(self, value, side, sorter)
    851         elif not isinstance(value, DatetimeArray):
    852             raise TypeError(
--> 853                 ""searchsorted requires compatible dtype or scalar, ""
    854                 f""not {type(value).__name__}""
    855             )

TypeError: searchsorted requires compatible dtype or scalar, not date
```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 7
machine          : AMD64
processor        : Intel64 Family 6 Model 79 Stepping 1, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.0
numpy            : 1.17.3
pytz             : 2018.9
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 41.6.0.post20191030
Cython           : 0.29.5
pytest           : 5.1.2
hypothesis       : None
sphinx           : None
blosc            : 1.7.0
feather          : None
xlsxwriter       : None
lxml.etree       : 4.3.2
html5lib         : None
pymysql          : None
psycopg2         : 2.7.7 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.5.0
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.2
matplotlib       : 3.0.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.14.0
pytables         : None
pytest           : 5.1.2
pyxlsb           : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : 1.2.17
tables           : None
tabulate         : None
xarray           : 0.12.1
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.45.1
</details>
"
558548598,31540,"Backport PR #31520 on branch 1.0.x (REGR: to_datetime, unique with OOB values)",meeseeksmachine,closed,2020-02-01T14:30:13Z,2020-02-01T15:07:26Z,"Backport PR #31520: REGR: to_datetime, unique with OOB values"
534608636,30147,CLN: Refactor pandas/tests/base - part3,SaturnFromTitan,closed,2019-12-08T21:55:16Z,2020-02-01T15:13:36Z,"Part of #23877

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

-----------------------
Starting to dissolve the `Ops` Mixin to drive fixturization of `pandas/tests/base/`"
557323183,31441,TypeError: copy() takes no keyword arguments,BayerSe,closed,2020-01-30T07:53:02Z,2020-02-01T15:14:52Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

df = pd.DataFrame(['A', 'A', 'B', 'B'], columns=['grps'])
df.groupby('grps').apply(lambda x: x.index.to_list())
```

```python
Traceback (most recent call last):
  File ""/home/bay2rng/miniconda3/envs/p356_rtp_production_scheduling/lib/python3.7/site-packages/pandas/core/groupby/groupby.py"", line 735, in apply
    result = self._python_apply_general(f)
  File ""/home/bay2rng/miniconda3/envs/p356_rtp_production_scheduling/lib/python3.7/site-packages/pandas/core/groupby/groupby.py"", line 751, in _python_apply_general
    keys, values, mutated = self.grouper.apply(f, self._selected_obj, self.axis)
  File ""/home/bay2rng/miniconda3/envs/p356_rtp_production_scheduling/lib/python3.7/site-packages/pandas/core/groupby/ops.py"", line 171, in apply
    result_values, mutated = splitter.fast_apply(f, group_keys)
  File ""/home/bay2rng/miniconda3/envs/p356_rtp_production_scheduling/lib/python3.7/site-packages/pandas/core/groupby/ops.py"", line 925, in fast_apply
    return libreduction.apply_frame_axis0(sdata, f, names, starts, ends)
  File ""pandas/_libs/reduction.pyx"", line 505, in pandas._libs.reduction.apply_frame_axis0
TypeError: copy() takes no keyword arguments
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/home/bay2rng/miniconda3/envs/p356_rtp_production_scheduling/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3319, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-a3e576516eed>"", line 4, in <module>
    df.groupby('grps').apply(lambda x: x.index.to_list())
  File ""/home/bay2rng/miniconda3/envs/p356_rtp_production_scheduling/lib/python3.7/site-packages/pandas/core/groupby/groupby.py"", line 746, in apply
    return self._python_apply_general(f)
  File ""/home/bay2rng/miniconda3/envs/p356_rtp_production_scheduling/lib/python3.7/site-packages/pandas/core/groupby/groupby.py"", line 751, in _python_apply_general
    keys, values, mutated = self.grouper.apply(f, self._selected_obj, self.axis)
  File ""/home/bay2rng/miniconda3/envs/p356_rtp_production_scheduling/lib/python3.7/site-packages/pandas/core/groupby/ops.py"", line 171, in apply
    result_values, mutated = splitter.fast_apply(f, group_keys)
  File ""/home/bay2rng/miniconda3/envs/p356_rtp_production_scheduling/lib/python3.7/site-packages/pandas/core/groupby/ops.py"", line 925, in fast_apply
    return libreduction.apply_frame_axis0(sdata, f, names, starts, ends)
  File ""pandas/_libs/reduction.pyx"", line 505, in pandas._libs.reduction.apply_frame_axis0
TypeError: copy() takes no keyword arguments

```
#### Problem description

This snippet is working well on version 0.25.3. It breaks in version 1.0.0.

#### Expected Output

```python
A    [0, 1]
B    [2, 3]
dtype: object
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-72-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : None
pytest           : 5.3.2
hypothesis       : None
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.3.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None


</details>
"
557432069,31447,REGR: to_csv(na_rep) truncates values,mdruiter,closed,2020-01-30T11:28:04Z,2020-02-01T15:21:12Z,"#### Code Sample

```python
>>> import pandas as pd
>>> pd.__version__
'1.0.0rc0'
>>> pd.Series(range(8, 12)).to_csv(na_rep='-')
',0\r\n0,8\r\n1,9\r\n2,1\r\n3,1\r\n'
```
#### Problem description

Despite (or due to?) #29975 and #30146, using `na_rep` still makes `to_csv()` truncate values. In this case integers.

#### Expected Output

',0\r\n0,8\r\n1,9\r\n2,10\r\n3,11\r\n'

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : Dutch_Netherlands.1252

pandas           : 1.0.0rc0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>"
558300029,31513,REGR: Fixed truncation with na_rep,TomAugspurger,closed,2020-01-31T18:31:06Z,2020-02-01T15:21:16Z,Closes https://github.com/pandas-dev/pandas/issues/31447
558528343,31535,"CLN: Replace isinstace(foo, Class) with isinstance(foo, ABCClass)",ShaharNaveh,closed,2020-02-01T11:37:51Z,2020-02-01T15:24:23Z,"- [x] ref #27353
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
557561562,31457,Timedelta multiplication crashes for large arrays,giuliobeseghi,closed,2020-01-30T15:15:57Z,2020-02-01T15:24:45Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

s = pd.Series(range(10001))
delta_t = pd.Timedelta(""30T"")

print(s * delta_t)
```

`Traceback:`
<details>

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-14-a39fa18ca12d> in <module>
      4 delta_t = pd.Timedelta(""30T"")
      5 
----> 6 print(s * delta_t)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\ops\common.py in new_method(self, other)
     62         other = item_from_zerodim(other)
     63 
---> 64         return method(self, other)
     65 
     66     return new_method

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\ops\__init__.py in wrapper(left, right)
    498         lvalues = extract_array(left, extract_numpy=True)
    499         rvalues = extract_array(right, extract_numpy=True)
--> 500         result = arithmetic_op(lvalues, rvalues, op, str_rep)
    501 
    502         return _construct_result(left, result, index=left.index, name=res_name)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\ops\array_ops.py in arithmetic_op(left, right, op, str_rep)
    194     else:
    195         with np.errstate(all=""ignore""):
--> 196             res_values = na_arithmetic_op(lvalues, rvalues, op, str_rep)
    197 
    198     return res_values

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\ops\array_ops.py in na_arithmetic_op(left, right, op, str_rep)
    147 
    148     try:
--> 149         result = expressions.evaluate(op, str_rep, left, right)
    150     except TypeError:
    151         result = masked_arith_op(left, right, op)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\computation\expressions.py in evaluate(op, op_str, a, b, use_numexpr)
    206     use_numexpr = use_numexpr and _bool_arith_check(op_str, a, b)
    207     if use_numexpr:
--> 208         return _evaluate(op, op_str, a, b)
    209     return _evaluate_standard(op, op_str, a, b)
    210 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\computation\expressions.py in _evaluate_numexpr(op, op_str, a, b)
    112             f""a_value {op_str} b_value"",
    113             local_dict={""a_value"": a_value, ""b_value"": b_value},
--> 114             casting=""safe"",
    115         )
    116 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\numexpr\necompiler.py in evaluate(ex, local_dict, global_dict, out, order, casting, **kwargs)
    820     # Create a signature
    821     signature = [(name, getType(arg)) for (name, arg) in
--> 822                  zip(names, arguments)]
    823 
    824     # Look up numexpr if possible.

~\AppData\Local\Continuum\anaconda3\lib\site-packages\numexpr\necompiler.py in <listcomp>(.0)
    819 
    820     # Create a signature
--> 821     signature = [(name, getType(arg)) for (name, arg) in
    822                  zip(names, arguments)]
    823 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\numexpr\necompiler.py in getType(a)
    701     if kind == 'S':
    702         return bytes
--> 703     raise ValueError(""unknown type %s"" % a.dtype.name)
    704 
    705 

ValueError: unknown type object
```

</details>

#### Problem description
This works if the series is initialised with ""large arrays"" (len > 10000). It seems it crashes just with timedeltas, but not with float/int values instead of `delta_t`.

I managed to get around this error by calling:

```python
t = pd.Series([delta_t] * len(s))  # create series of timedeltas

print(s * t)
```

It worked fine until pandas 0.25.3

#### Expected Output
0         0 days 00:00:00
1         0 days 00:30:00
2         0 days 01:00:00
3         0 days 01:30:00
4         0 days 02:00:00
               ...       
9996    208 days 06:00:00
9997    208 days 06:30:00
9998    208 days 07:00:00
9999    208 days 07:30:00
10000   208 days 08:00:00
Length: 10001, dtype: timedelta64[ns]

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : 0.29.14
pytest           : 5.3.4
hypothesis       : 4.54.2
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.4
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.3.2
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.47.0


</details>
"
558474352,31529,BUG: Series multiplication with timedelta scalar numexpr path,jbrockmendel,closed,2020-02-01T03:58:14Z,2020-02-01T16:10:25Z,"- [x] closes #31457 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
558359614,31518,TYP: Arraylike,jbrockmendel,closed,2020-01-31T20:36:09Z,2020-02-01T16:44:33Z,"@simonjayhawkins mypy doesn't like this

```
$ mypy pandas
pandas/core/base.py:679: error: ""ArrayLike"" has no attribute ""nbytes""
pandas/core/indexes/base.py:246: error: Type variable ""pandas._typing.ArrayLike"" is unbound
pandas/core/indexes/base.py:246: note: (Hint: Use ""Generic[ArrayLike]"" or ""Protocol[ArrayLike]"" base class to bind ""ArrayLike"" inside a class)
pandas/core/indexes/base.py:246: note: (Hint: Use ""ArrayLike"" in function signature to bind ""ArrayLike"" inside a function)
pandas/core/indexes/base.py:1152: error: ""ArrayLike"" has no attribute ""copy""
pandas/core/indexes/base.py:3881: error: ArrayLike? has no attribute ""view""
pandas/core/series.py:252: error: ""ArrayLike"" has no attribute ""copy""
pandas/io/pytables.py:2240: error: ""ExtensionDtype"" has no attribute ""itemsize""
pandas/io/pytables.py:2245: error: ""ExtensionArray"" has no attribute ""size""
pandas/io/pytables.py:2248: error: ""ExtensionArray"" has no attribute ""codes""
pandas/io/pytables.py:4985: error: ""ExtensionArray"" has no attribute ""codes""
```

Is this a product of ArrayLike being a TypeVar instead of a Union?  At least the last two look like they need to be Categorical instead of ExtensionArray, not sure about the others."
558563730,31546,Manual backport 31446,charlesdong1991,closed,2020-02-01T16:33:45Z,2020-02-01T16:54:02Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
467443499,27357,"DEPR: .take(..., is_copy=True); rename is_copy -> copy",jreback,closed,2019-07-12T14:26:09Z,2020-02-01T17:57:13Z,this is more in-line with our other signatures. ideally would do this for 0.25
550745686,31071,ENH: accept a dictionary in plot colors,MarcoGorelli,closed,2020-01-16T11:25:12Z,2020-02-01T18:19:00Z,"This is a resurrection of #28659. Have kept the original commit history, just rebased and added docstrings.

- [x] closes #8193
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
558553979,31541,Backport PR #31456 on branch 1.0.x (BUG: Groupby.apply wasn't allowing for functions which return lists),meeseeksmachine,closed,2020-02-01T15:15:25Z,2020-02-01T19:37:49Z,Backport PR #31456: BUG: Groupby.apply wasn't allowing for functions which return lists
558555180,31543,Backport PR #31529 on branch 1.0.x (BUG: Series multiplication with timedelta scalar numexpr path),meeseeksmachine,closed,2020-02-01T15:24:54Z,2020-02-01T19:38:45Z,Backport PR #31529: BUG: Series multiplication with timedelta scalar numexpr path
558554722,31542,Backport PR #31513 on branch 1.0.x (REGR: Fixed truncation with na_rep),meeseeksmachine,closed,2020-02-01T15:21:22Z,2020-02-01T19:39:15Z,Backport PR #31513: REGR: Fixed truncation with na_rep
553721622,31215,Follow-up: XLSB Support,Rik-de-Kort,closed,2020-01-22T18:38:45Z,2020-02-01T20:33:13Z,"Updated min version for `pyxlsb` to 1.0.6 and removed the now unnecessary xfail for read from url. Turns out the other xfail is actually unnecessary so I've removed it.

Question: is my name supposed to show up in the contributors list?

- [X] closes #8540
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry"
558602891,31554,REF: move _convert_to_indexer to Loc,jbrockmendel,closed,2020-02-01T21:47:35Z,2020-02-01T22:40:26Z,"It is overriden in iLoc, so by putting it directly in Loc we can simplify bits of it that check `self.name` and keep only the `self.name == ""loc""` branches.  This lets us get rid of a bit of dead code (L1638-L1642).

Most of the diff is misleading; it is only the one method that is moved, with a placeholder kept in NDFrameIndexer"
557031724,31425,REF: do all convert_tolerance casting inside Index.get_loc,jbrockmendel,closed,2020-01-29T18:13:40Z,2020-02-01T22:40:47Z,As a bonus we get better exception messages in the affected tests.
558595932,31553,"REF: collect MultiIndex indexing, set methods",jbrockmendel,closed,2020-02-01T20:53:42Z,2020-02-01T22:43:36Z,
558562117,31545,BUG&TST: df.replace fail after converting to new dtype,charlesdong1991,closed,2020-02-01T16:21:18Z,2020-02-01T23:10:57Z,"- [x] closes #31517 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

"
555513697,31340,Bug: loc assignment results in wrong dtype,krasnikov138,closed,2020-01-27T11:28:01Z,2020-02-01T23:20:37Z,"Let's see output of this code snippet
```python
df = pd.DataFrame({'id': ['A'], 'a': [1.2], 'b': [0.0], 'c': [-2.5]})
cols = ['a', 'b', 'c']
df.loc[:, cols] = df.loc[:, cols].astype('float32')
print(df.dtypes)
```
#### Problem description
In this code we expect columns `'a', 'b', 'c'` will have types `float32`. But in fact the result is:
```
>>> print(df.dtypes)
id     object
a     float32
b       int64
c     float32
dtype: object
```
Bug occurs when the value in column is close to integer. In other words, for `df = pd.DataFrame({'id': ['A'], 'a': [1.2], 'b': [1.0], 'c': [-2.5]})` behavior will be the same. However, if df has 2 rows or more the result is right:
```
>>> df = pd.DataFrame({'id': ['A', 'B'], 'a': [1.2, 0.55], 'b': [0.0, 0.0], 'c': [-2.5, 12.56]})
>>> df.loc[:, cols] = df.loc[:, cols].astype('float32')
>>> df.dtypes
id     object
a     float32
b     float32
c     float32
dtype: object
```
#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.6-200.fc28.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2018.4
dateutil         : 2.7.5
pip              : 19.2.3
setuptools       : 41.6.0
Cython           : 0.28.2
pytest           : 3.5.1
hypothesis       : None
sphinx           : 1.7.4
blosc            : None
feather          : None
xlsxwriter       : 1.0.4
lxml.etree       : 4.2.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.5 (dt dec pq3 ext lo64)
jinja2           : 2.10
IPython          : 6.4.0
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.1
matplotlib       : 3.0.1
numexpr          : 2.6.5
odfpy            : None
openpyxl         : 2.5.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : 1.2.14
tables           : 3.4.3
xarray           : None
xlrd             : 1.1.0
xlwt             : 1.3.0
xlsxwriter       : 1.0.4
</details>
"
557994257,31483,TST: preserve dtypes on assignment,prakhar987,closed,2020-01-31T08:49:46Z,2020-02-01T23:20:49Z,"- [x] closes #31340 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
558468279,31527,CLN: libperiod,jbrockmendel,closed,2020-02-01T03:04:35Z,2020-02-01T23:34:49Z,separating cleanups from non-cleanup branches
493404306,28429,BUG: close hdf store on any error,adimascio,closed,2019-09-13T16:03:15Z,2020-02-02T01:03:53Z,"NOTE: I'm opening the PR mainly to open a discussion about the current behaviour. If this is intended, please tell me. If not and you need me to write some tests, I'll do it :)

In `read_hdf`, if the `store.select()` call throws either a
`ValueError`, a `TypeError` or a `KeyError` then the store is closed.

However, if any other exception is thrown (e.g. an `AssertionError` if
there are gaps in blocks ref_loc) , the store is not closed and some
client code could end up trying to reopen the store and hit an
error like: `the file XXX is already opened. Please close it before
reopening in write mode`. Furthermore, the exception is re-raised in all
cases.

This commit just catches any exception.

Closes #28430 

- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
473560779,27617,DEPR: Deprecate NDFrame.filter,topper-123,closed,2019-07-26T22:43:26Z,2020-02-02T01:19:17Z,"- [ ] xref #26642
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I propose to deprecate (DataFrame|Series).filter. The method's functionality is already in the ``.loc``method, except the regex functionality, for which I've made a proposal adding regex functionality  to ``.loc`` in #27363.
"
551127668,31093,Pull Request Tips,WillAyd,closed,2020-01-17T00:14:18Z,2020-02-02T01:21:45Z,"I think these issues pop up on a good deal of PRs but aren't explicitly stated anywhere, so adding a few bullet points to the contributing guide might help"
556639292,31412,CLN: Replace '.format' with f-strings in some test files #29547,vandana-iyer,closed,2020-01-29T05:07:38Z,2020-02-02T03:06:03Z,"Updates the below 2 test files to use f-strings (fast and preferred) instead of '.format' (old and slow)

1) pandas/tests/reshape/test_reshape.py
2) pandas/tests/scalar/period/test_period.py
 
For more details on the issue, refer: https://github.com/pandas-dev/pandas/issues/29547

- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
558655891,31561,Try manual backport,charlesdong1991,closed,2020-02-02T07:26:29Z,2020-02-02T07:36:33Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
558545904,31539,WEB: Wrong links on web page,topper-123,closed,2020-02-01T14:10:32Z,2020-02-02T15:24:00Z,"On [this page ](https://pandas.pydata.org/ )The link at ""what's new in 1.0.0"" (upper right corner) points to the v0.25 whatsnew.

On [this page](https://pandas.pydata.org/docs/index.html) the link on the upper left side logo points to the same page. I would expect it to point to the main page (https://pandas.pydata.org/)."
558658061,31562,Backport PR: BUG: Array.__setitem__ failing with nullable boolean mask (#31484),charlesdong1991,closed,2020-02-02T07:49:56Z,2020-02-02T16:04:40Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
558698715,31573,CI: Remove warning raising after new matplotlib release,charlesdong1991,closed,2020-02-02T13:45:49Z,2020-02-02T16:42:32Z,"- [ ] xref #31562   
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

seems CI is broken after the new release of matplotlib"
557626398,31463,"Regression: objToJSON ""nonvoid function does not return a value"" error is back",toddrme2178,closed,2020-01-30T16:57:31Z,2020-02-02T17:09:19Z,"There was an issue in 2013, #5326, where `initObjToJSON` in `pandas/_libs/src/ujson/python/objToJSON.c` was causing a build error due to it lacking a return value.  Specifically, the issue was:

```
pandas/_libs/src/ujson/python/objToJSON.c: In function ‘initObjToJSON’:
pandas/_libs/src/ujson/python/objToJSON.c:181:1: error: control reaches end of non-void function [-Werror=return-type]
```

This was fixed at the time in #5334.  However, a recent commit, #30710, removed the return value again.  This has caused the warning/error to reappear, causing our builds of pandas 1.0.0 to fail.  The warning is appearing in your CI builds as well.  The difference is that we have it configured as an error.

Would it be possible to set a valid return value for the function?  Thank you."
558722470,31576,Backport PR #31573 on branch 1.0.x (CI: Remove warning raising after new matplotlib release),meeseeksmachine,closed,2020-02-02T16:42:49Z,2020-02-02T17:10:03Z,Backport PR #31573: CI: Remove warning raising after new matplotlib release
557977556,31482,BUG: objToJson.c - fix return value,alimcmaster1,closed,2020-01-31T08:08:46Z,2020-02-02T17:10:20Z,"- [x] closes #31463

https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=27217&view=logs&j=3a03f79d-0b41-5610-1aa4-b4a014d0bc70&t=fe74a338-551b-5fbb-553d-25f48b1836e8&l=687

Seems like this warning is somehow causing an error in the users builds of pandas"
558667681,31565,Fix docstring for to_markdown buf parameter,jakevdp,closed,2020-02-02T09:15:51Z,2020-02-02T17:35:56Z,"- [ ] ~~closes #xxxx~~ (N/A)
- [ ] ~~tests added / passed~~ (N/A)
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] ~~whatsnew entry~~ (N/A)
"
558629772,31557,REF: organize Series indexing tests,jbrockmendel,closed,2020-02-02T02:15:51Z,2020-02-02T17:38:48Z,We've got indexing tests pretty scattered; this is organizing the ones in tests.series.indexing by method
558722483,31577,REF: de-duplicate Period freq conversion code,jbrockmendel,closed,2020-02-02T16:42:54Z,2020-02-02T17:39:20Z,
558519290,31532,BUG display.max_colwidth do not accept -1 for unlimited width,glemaitre,closed,2020-02-01T10:22:39Z,2020-02-02T18:30:39Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
pd.set_option(""display.max_colwidth"", -1)
```
#### Problem description

There is a regression with `""display.max_colwidth""`. In the past, it was only accepting integer. The way to no limit the size was to pass `-1`. In pandas 1,0, this option becomes more consistent and not limiting the width should be set with `None`. However, the support for negative integer was removed.

Thus, one would need to either set to `-1` or `None` depending on the pandas version. It would be best to support both options. Potentially, support for negative integer could be removed with a deprecation cycle."
558726425,31579,CLN: Fix ReadMe Badges,alimcmaster1,closed,2020-02-02T17:11:58Z,2020-02-02T18:31:43Z,"Noticed when discussing https://github.com/pandas-dev/pandas/issues/31560 our badges are written in HTML + take up a lot of vertical space on our Readme.

My branch (a lot more compact vs master):
https://github.com/alimcmaster1/pandas/tree/mcmali-readme#pandas-powerful-python-data-analysis-toolkit

I've also added a NumFocus one :) 
"
558726122,31578,Backport PR #31482 on branch 1.0.x (BUG: objToJson.c - fix return value),meeseeksmachine,closed,2020-02-02T17:09:29Z,2020-02-02T18:32:03Z,Backport PR #31482: BUG: objToJson.c - fix return value
558633005,31559,REF: CategoricalIndex indexing tests,jbrockmendel,closed,2020-02-02T02:56:45Z,2020-02-02T18:41:49Z,
558631131,31558,"REF: organize DataFrame tests for indexing, by-method",jbrockmendel,closed,2020-02-02T02:32:19Z,2020-02-02T18:44:05Z,The only change that isnt just rearrangement is implementing where_frame fixture in test_where and using it to get rid of for-loops inside a bunch of tests in that file
558684446,31569,BUG accept and deprecate negative integer for max_colwidth,glemaitre,closed,2020-02-02T11:44:06Z,2020-02-03T10:38:51Z,"- [x] closes #31532
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
558612857,31555,Backport PR #31545 on branch 1.0.x (BUG&TST: df.replace fail after converting to new dtype),meeseeksmachine,closed,2020-02-01T23:10:51Z,2020-02-02T19:16:54Z,Backport PR #31545: BUG&TST: df.replace fail after converting to new dtype
558741282,31584,Backport PR #31545: BUG&TST: df.replace fail after converting to new …,jreback,closed,2020-02-02T19:07:30Z,2020-02-02T19:42:56Z,"xref #31555 
xref #31545 "
555682974,31354,BUG: Timedelta components rounded by float imprecision,pganssle,closed,2020-01-27T16:19:06Z,2020-02-02T21:08:57Z,"#### Problem description

It appears that there is some premature rounding happening in the `Timedelta` constructor that makes it so the unit-adjusted sums of the `days`, `seconds`, `microseconds` and `nanoseconds` attributes do not sum to the total number of nanoseconds. A fundamental assumption of the `datetime.timedelta` type (and breaking this assumption breaks Liskov substitutability) is that the total time difference at the precision of microseconds can be represented by summing up the unit-adjusted `days`, `seconds` and `microseconds` attributes, and it's how `datetime.total_seconds()` works.

I believe that this is the root cause of issue #31043, which was ""fixed"" with what is essentially a workaround in PR #31155, as I mentioned in [this comment](https://github.com/pandas-dev/pandas/pull/31155#discussion_r370721655).

At the moment the most obvious effect is that bug #31043 only is fixed for recent versions of `dateutil`, but presumably it will show up in other places where standard `datetime` arithmetic is being used on `pandas` timestamps.

#### Code Sample, a copy-pastable example if possible

```python
def to_ns(td):
  ns = td.days * 86400
  ns += td.seconds
  ns *= 1000000
  ns += td.microseconds
  ns *= 1000
  ns += td.nanoseconds
  return ns

td = timedelta(1552211999999999872, unit=""ns"")
print(td.value)  # 1552211999999999872
print(to_ns(td))  # 1552212000000000872
```

#### Actual output:
```
1552211999999999872
1552212000000000872
```

#### Expected output
```
1552211999999999872
1552211999999999872
```"
558739983,31583,REF: Timestamp constructor tests,jbrockmendel,closed,2020-02-02T18:57:56Z,2020-02-02T21:52:42Z,
362589525,22797,loc() does not swap two rows in multi-index pandas dataframe,eisthfroyalblue,closed,2018-09-21T12:15:52Z,2020-02-02T22:20:56Z,"```
df = pd.DataFrame(np.arange(12).reshape((4, 3)),
    index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],
    columns=[['Ohio', 'Ohio', 'Colorado'],
    ['Green', 'Red', 'Green']])

df.loc[['b','a'],:]  # does not swap

# df.reindex(index=['b', 'a'], level=0) # This works
```
#### Problem description

df.loc[['b','a'],:] does not swap the rows 'a' and 'b', nor does for a Series.

#### Expected Output
The output should be the same as the one obtained via df.reindex(index=['b', 'a'], level=0)
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.3.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.20.3
pytest: 3.2.1
pip: 10.0.1
setuptools: 36.5.0.post20170921
Cython: 0.26.1
numpy: 1.14.0
scipy: 0.19.1
xarray: None
IPython: 6.1.0
sphinx: 1.6.3
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.2
feather: None
matplotlib: 2.1.0
openpyxl: 2.4.8
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.2
lxml: 4.1.0
bs4: 4.6.0
html5lib: 0.999999999
sqlalchemy: 1.1.13
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
pandas_gbq: None
pandas_datareader: 0.7.0

</details>
"
557260636,31440,BUG: Fix qcut for nullable integers,dsaxton,closed,2020-01-30T04:34:24Z,2020-02-02T22:22:40Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] whatsnew entry

Related to #31389 but doesn't close any issues that I'm aware of. Seems the problems were indexing into a numpy array with a BooleanArray and using searchsorted in the presence of pd.NA."
558294723,31512,CLN: inherit PeriodIndex._box_func,jbrockmendel,closed,2020-01-31T18:23:53Z,2020-02-02T22:39:04Z,
558751136,31585,REF: define _convert_to_indexer in Loc,jbrockmendel,closed,2020-02-02T20:18:45Z,2020-02-02T22:39:38Z,"_convert_to_indexer is overriden by iloc, so the version currently defined in LocationIndexer is only used in Loc.  This moves it directly to Loc.

Also, since ix has been removed, we have an unnecessary level of the class hierarchy, so this merges NDFrameIndexer and LocationIndexer.

Follow-ups will move base class methods that are actually Loc-specific to Loc. "
558767540,31587,Backport PR #31440 on branch 1.0.x (BUG: Fix qcut for nullable integers),meeseeksmachine,closed,2020-02-02T22:19:37Z,2020-02-02T22:57:17Z,Backport PR #31440: BUG: Fix qcut for nullable integers
556419718,31401,BUG: preserve object dtype for Index set ops,jbrockmendel,closed,2020-01-28T19:30:54Z,2020-02-02T23:09:41Z,"I'd be fine with coercing if we did it consistently, but ATM we behave differently for monotonic-increasing vs otherwise."
556491471,31406,"REF: consolidate DTI/TDI/PI get_value in ExtensionIndex, associated cleanups",jbrockmendel,closed,2020-01-28T21:50:47Z,2020-02-02T23:12:40Z,"@jschendel does the holds_integer change for IntervalIndex make sense?

@jreback is this a correct usage of self.holds_integer() as meaning ""we only fall back to positional indexing if not self.holds_integer()"""
556596673,31409,PERF/CLN: float-to-int casting,jbrockmendel,closed,2020-01-29T02:13:52Z,2020-02-02T23:16:45Z,"using float.is_integer is apparently much faster than casting and checking equality, especially for values between int32max and int64max (anecdotal based on small sample size)

```
cast1 = com.cast_scalar_indexer_master
cast2 = com.cast_scalar_indexer_PR

def cast3(key):
    # Effectively what we have in _maybe_cast_indexer in master
    if lib.is_float(key):
        try:
            ckey = int(key)
            if ckey == key:
                key = ckey
        except (OverflowError, ValueError, TypeError):
            pass
    return key

key1 = np.float64(17179869184.0)
key2 = float(key1) * 2**30             
key3 = np.float64(np.iinfo(np.int64).max)

In [87]: for func in [cast1, cast2, cast3]: 
    ...:     for key in [key1, key2, key3]: 
    ...:                 %timeit func(key) 
    ...:                                                                                                                                                                                                                                          

647 ns ± 3.71 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
559 ns ± 27.7 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
1.98 µs ± 19.7 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)

360 ns ± 4.1 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
353 ns ± 3.11 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
377 ns ± 6.54 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)

568 ns ± 3.92 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
445 ns ± 15 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
1.91 µs ± 36.7 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
```

The middle three timeit results are the ones we get from this PR."
558274635,31506,CLN: No need to use libindex.get_value_at,jbrockmendel,closed,2020-01-31T17:48:46Z,2020-02-02T23:19:12Z,Made possible because Series._values returns DTA/TDA for datetime64/timedelta64.
558286363,31510,CLN: remove IndexEngine.set_value,jbrockmendel,closed,2020-01-31T18:11:52Z,2020-02-02T23:24:41Z,"made possible bc Series._values now returns DTA/TDA for datetime64/timedelta64

Small perf improvement

```
In [3]: dti = pd.date_range('2016-01-01', freq='D', periods=10**4)              
In [4]: idx = list('abcdefghijklmnop')                                          
In [5]: arr = np.random.random(len(idx)*len(dti)).reshape(len(dti), -1)         
In [6]: df = pd.DataFrame(arr, index=dti, columns=idx)                          

In [7]: %timeit df2 = df._set_value(""2043-05-14"", ""c"", 4)                      
381 µs ± 14.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)   # <-- master
330 µs ± 13 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  # <-- PR

```"
558788542,31593,Import libindex in series.py,dsaxton,closed,2020-02-03T00:56:28Z,2020-02-03T00:59:59Z,I think we may be missing an import in `series.py`. cc @jbrockmendel 
558798566,31595,`Series.isin` does not return all values in the column,Dpananos,closed,2020-02-03T01:52:49Z,2020-02-03T02:13:42Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np

print(pd.__version__)

df = pd.DataFrame({'t': np.arange(0.5, 2.0, 0.1)})

slxn = df.t.isin(np.arange(0.5, 2.0, 0.5))

slxn.sum()
```
#### Problem description

`df` is a data frame with a single column called `t`.  The values of this column range from 0.5 to 2.0 in increments of 0.1.

There should be three values in `slxn` which are `True`, however the expression above yields 1.


#### Expected Output

```python
>>>slxn.sum()
3
```

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_CA.UTF-8
LOCALE           : en_CA.UTF-8

pandas           : 1.0.0
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : 0.29.14
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.10.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : 0.14.1
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None



</details>
"
452785096,26683,Warning while reindexing tz aware index with method='nearest',goriccardo,closed,2019-06-06T01:13:19Z,2020-02-03T03:48:29Z,"Reindexing a tz aware dataframe using method='nearest' raise an internal warning.
```python
from pandas.util.testing import makeTimeDataFrame
df = makeTimeDataFrame(freq='1h')
df = df.tz_localize('UTC')
df.reindex(df.index[1:4], method='nearest')
```
raises a warning:
```
/lib/python3.7/site-packages/pandas/core/indexes/base.py:2820: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.
        To accept the future behavior, pass 'dtype=object'.
        To keep the old behavior, pass 'dtype=""datetime64[ns]""'.
  target = np.asarray(target)
```
in pandas 0.24.2"
558775103,31590,pip3 install pandas==1.0.0 fails to find source distribution on ARM host,wesm,closed,2020-02-02T23:20:24Z,2020-02-03T03:51:08Z,"#### Code Sample, a copy-pastable example if possible

```python
$ time pip3 install pandas==1.0.0
Collecting pandas==1.0.0
  Could not find a version that satisfies the requirement pandas==1.0.0 (from versions: 0.1, 0.2b0, 0.2b1, 0.2, 0.3.0b0, 0.3.0b2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0rc1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0rc1, 0.8.0rc2, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0rc1, 0.19.0, 0.19.1, 0.19.2, 0.20.0rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0rc1, 0.21.0, 0.21.1, 0.22.0, 0.23.0rc2, 0.23.0, 0.23.1, 0.23.2, 0.23.3, 0.23.4, 0.24.0rc1, 0.24.0, 0.24.1, 0.24.2, 0.25.0rc0, 0.25.0, 0.25.1, 0.25.2, 0.25.3)
No matching distribution found for pandas==1.0.0
```

#### Problem description

Unable to build pandas on a machine that has to build from source. 

Here's my host information:

```
$ uname -a
Linux rockpro64 4.4.180 #1 SMP Fri May 31 17:07:08 EDT 2019 aarch64 GNU/Linux
```"
553543443,31205,BUG: plotting regular tz-aware timeseries gives UserWarning ,jorisvandenbossche,closed,2020-01-22T13:31:48Z,2020-02-03T07:36:36Z,"With this example of plotting a tz-aware, regular timeseries:

```
df = pd.DataFrame(np.random.randn(100, 3), index=pd.date_range(""2012"", freq='H', periods=100, tz='UTC'), columns=['a', 'b', 'c'])
df.plot()
```
gives a warning that the user shouldn't see I think (there is nothing to do about it):

```
/home/joris/scipy/pandas/pandas/core/arrays/datetimes.py:1099: UserWarning: Converting
to PeriodArray/Index representation will drop timezone information.
  UserWarning,
```

When setting a filterwarning to error, you can see the warning is coming from a `to_period()` call in the matplotlib plotting backend included in pandas:

```
In [5]: warnings.simplefilter(""error"", UserWarning)

In [6]: df.plot()  
---------------------------------------------------------------------------
UserWarning                               Traceback (most recent call last)
<ipython-input-6-848b80e64df8> in <module>
----> 1 df.plot()

~/scipy/pandas/pandas/plotting/_core.py in __call__(self, *args, **kwargs)
    846                     data.columns = label_name
    847 
--> 848         return plot_backend.plot(data, kind=kind, **kwargs)
    849 
    850     def line(self, x=None, y=None, **kwargs):

~/scipy/pandas/pandas/plotting/_matplotlib/__init__.py in plot(data, kind, **kwargs)
     59             kwargs[""ax""] = getattr(ax, ""left_ax"", ax)
     60     plot_obj = PLOT_CLASSES[kind](data, **kwargs)
---> 61     plot_obj.generate()
     62     plot_obj.draw()
     63     return plot_obj.result

~/scipy/pandas/pandas/plotting/_matplotlib/core.py in generate(self)
    261         self._compute_plot_data()
    262         self._setup_subplots()
--> 263         self._make_plot()
    264         self._add_table()
    265         self._make_legend()

~/scipy/pandas/pandas/plotting/_matplotlib/core.py in _make_plot(self)
   1050             from pandas.plotting._matplotlib.timeseries import _maybe_convert_index
   1051 
-> 1052             data = _maybe_convert_index(self._get_ax(0), self.data)
   1053 
   1054             x = data.index  # dummy, not used

~/scipy/pandas/pandas/plotting/_matplotlib/timeseries.py in _maybe_convert_index(ax, data)
    252 
    253         if isinstance(data.index, ABCDatetimeIndex):
--> 254             data = data.to_period(freq=freq)
    255         elif isinstance(data.index, ABCPeriodIndex):
    256             data.index = data.index.asfreq(freq=freq)

~/scipy/pandas/pandas/core/frame.py in to_period(self, freq, axis, copy)
   8379         axis = self._get_axis_number(axis)
   8380         if axis == 0:
-> 8381             new_data.set_axis(1, self.index.to_period(freq=freq))
   8382         elif axis == 1:
   8383             new_data.set_axis(0, self.columns.to_period(freq=freq))

~/scipy/pandas/pandas/core/accessor.py in f(self, *args, **kwargs)
     97         def _create_delegator_method(name):
     98             def f(self, *args, **kwargs):
---> 99                 return self._delegate_method(name, *args, **kwargs)
    100 
    101             f.__name__ = name

~/scipy/pandas/pandas/core/indexes/datetimelike.py in _delegate_method(self, name, *args, **kwargs)
    986 
    987     def _delegate_method(self, name, *args, **kwargs):
--> 988         result = operator.methodcaller(name, *args, **kwargs)(self._data)
    989         if name not in self._raw_methods:
    990             result = Index(result, name=self.name)

~/scipy/pandas/pandas/core/arrays/datetimes.py in to_period(self, freq)
   1097                 ""Converting to PeriodArray/Index representation ""
   1098                 ""will drop timezone information."",
-> 1099                 UserWarning,
   1100             )
   1101 

UserWarning: Converting to PeriodArray/Index representation will drop timezone information.
```"
558931472,31602,Manual backport 31594,charlesdong1991,closed,2020-02-03T08:47:10Z,2020-02-03T08:57:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
474789773,27662,DOC: add anonymizeIp for Google analytics in docs,jorisvandenbossche,closed,2019-07-30T19:50:58Z,2020-02-03T09:29:21Z,"Using a more recent snippet (following their current docs), and adding anonymization of the IP (according to their docs, this should only ""slightly reduce the accuracy of geolocation""). 

If we do this, should do the same for main website as well."
516660784,29366,29213: Dataframe Constructor from List of List and non-iterables,matttan90,closed,2019-11-02T17:13:35Z,2020-02-03T10:12:57Z,"- [x] closes https://github.com/pandas-dev/pandas/issues/29213
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

---
## What
Whilst the original issue is from the factory method `DataFrame.from_dict(d, orient='index')`, the main issue is the *order* of elements within the list in the main constructor:

For example:
```
pd.DataFrame([[1, 2, 3], 4])  # doesn't work
# TypeError: object of type 'int' has no len()

In [2]: pd.DataFrame([4, [1, 2, 3]]) # works, creates a 1D DataFrame
Out[2]:
           0
0          4
1  [1, 2, 3]
```



## Current Constructor Logic on List Argument
- Current logic looks at the [first element](https://github.com/pandas-dev/pandas/blob/master/pandas/core/frame.py#L462-L465), and *infers* that all other elements are iterables as well.
    - if all elements in the list are iterables, it generates a 2D DataFrame.
    - if any element from index-1 onwards is a non-iterable, it doesn't have a len() method and fails.

## Proposed Solution
- Based on the first element, *try* to infer that all elements are iterables as well.
    - If not all subsequent elements are iterables, then return 1D DataFrame. This would be the same behaviour as it would have been if the first element in the list is non-iterable.
    - This should not have performance degradation as there is noneed to check if all elements in the list are iterables.


## Note
- This does not solve the issue whereby we have iterables of different types (such as lists and strings...)
```
In [2]: pd.DataFrame([[1, 2, 3], 'foobar'])
Out[2]:
   0  1  2     3     4     5
0  1  2  3  None  None  None
1  f  o  o     b     a     r
```"
549598946,31005,DOC: removed wrong value of `pd.NA ** 0`,tsvikas,closed,2020-01-14T14:20:06Z,2020-02-03T10:32:41Z,"- [x] closes #31003
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
558956156,31603,Backport PR #31569 on branch 1.0.x,glemaitre,closed,2020-02-03T09:32:56Z,2020-02-03T10:38:41Z,backport of #31569 in 1.0.x
558687817,31571,DOC fix *_option() docstring,glemaitre,closed,2020-02-02T12:12:30Z,2020-02-03T10:39:34Z,"- [x] closes #28780
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
502348458,28780,DOC: wrong formatting of the set_option docs,alimcmaster1,closed,2019-10-03T23:31:58Z,2020-02-03T10:39:49Z,"https://pandas-docs.github.io/pandas-docs-travis/reference/api/pandas.set_option.html

Notes section of the set_option docs seem to have lost some formatting/spacing on master.

PR coming to fix!

Ali"
558792158,31594,TST: troubleshoot npdev build,jbrockmendel,closed,2020-02-03T01:18:14Z,2020-02-03T14:33:00Z,
558897736,31601,Backport PR #31207 on branch 1.0.x (BUG: no longer raise user warning when plotting tz aware time series),meeseeksmachine,closed,2020-02-03T07:37:08Z,2020-02-03T10:57:46Z,Backport PR #31207: BUG: no longer raise user warning when plotting tz aware time series
558897401,31600,"How to determine the severity of skewness(nanskew) such as highly skewed,moderately skewed,normally distributed,etc by interpreting the returned numerical values from the .skew() function.",getsrikar,closed,2020-02-03T07:36:17Z,2020-02-03T11:05:36Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here

```
#### Problem description

The given documentation available below doesn't provide any specifics about the skewness results interpretation.

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.skew.html

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
545187996,30667,groupby.apply fails with ValueError: cannot reindex from a duplicate axis,harishmk,closed,2020-01-03T22:19:43Z,2020-01-20T16:28:01Z,"#### Code Sample

```python
import pandas as pd

df=pd.DataFrame([['x','p'],['x','p'],['x','q']], columns=['X','Y'], index=[1,2,2])
print(df)
df=df.groupby(['Y']).apply(lambda x: x)

df=pd.DataFrame([['x','p'],['x','p'],['x','o']], columns=['X','Y'], index=[1,2,2])
print(df)
df=df.groupby(['Y']).apply(lambda x: x)
```
#### Problem description

On dataframe:
```
   X  Y
1  x  p
2  x  p
2  x  q
```
`groupby.apply` on column `Y` works.
but throws duplicate axis exception with:
```
   X  Y
1  x  p
2  x  p
2  x  o
```
the exception does not happen if the dataframe was sorted on `Y`

```
Traceback (most recent call last):
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/groupby/groupby.py"", line 725, in apply
    result = self._python_apply_general(f)
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/groupby/groupby.py"", line 745, in _python_apply_general
    keys, values, not_indexed_same=mutated or self.mutated
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/groupby/generic.py"", line 372, in _wrap_applied_output
    return self._concat_objects(keys, values, not_indexed_same=not_indexed_same)
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/groupby/groupby.py"", line 955, in _concat_objects
    result = result.reindex(ax, axis=self.axis)
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/util/_decorators.py"", line 221, in wrapper
    return func(*args, **kwargs)
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/frame.py"", line 3976, in reindex
    return super().reindex(**kwargs)
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/generic.py"", line 4514, in reindex
    axes, level, limit, tolerance, method, fill_value, copy
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/frame.py"", line 3864, in _reindex_axes
    index, method, copy, level, fill_value, limit, tolerance
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/frame.py"", line 3886, in _reindex_index
    allow_dups=False,
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/generic.py"", line 4577, in _reindex_with_indexers
    copy=copy,
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1251, in reindex_indexer
    self.axes[axis]._can_reindex(indexer)
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3362, in _can_reindex
    raise ValueError(""cannot reindex from a duplicate axis"")
ValueError: cannot reindex from a duplicate axis

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/hm106930/ipv/web/groupby_bug.py"", line 16, in <module>
    df=df.groupby(['Y']).apply(lambda x: x)
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/groupby/groupby.py"", line 737, in apply
    return self._python_apply_general(f)
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/groupby/groupby.py"", line 745, in _python_apply_general
    keys, values, not_indexed_same=mutated or self.mutated
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/groupby/generic.py"", line 372, in _wrap_applied_output
    return self._concat_objects(keys, values, not_indexed_same=not_indexed_same)
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/groupby/groupby.py"", line 955, in _concat_objects
    result = result.reindex(ax, axis=self.axis)
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/util/_decorators.py"", line 221, in wrapper
    return func(*args, **kwargs)
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/frame.py"", line 3976, in reindex
    return super().reindex(**kwargs)
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/generic.py"", line 4514, in reindex
    axes, level, limit, tolerance, method, fill_value, copy
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/frame.py"", line 3864, in _reindex_axes
    index, method, copy, level, fill_value, limit, tolerance
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/frame.py"", line 3886, in _reindex_index
    allow_dups=False,
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/generic.py"", line 4577, in _reindex_with_indexers
    copy=copy,
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1251, in reindex_indexer
    self.axes[axis]._can_reindex(indexer)
  File ""/home/hm106930/.local/share/virtualenvs/ipv-work-62kMEXht/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3362, in _can_reindex
    raise ValueError(""cannot reindex from a duplicate axis"")
ValueError: cannot reindex from a duplicate axis

```

#### Expected Output
```
   X  Y
1  x  p
2  x  p
2  x  o

```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-957.27.2.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.0.1
Cython           : 0.29.13
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.3.3
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.3
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.7
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
</details>
"
551827005,31128,REF: require PeriodArray in PeriodIndex._simple_new,jbrockmendel,closed,2020-01-18T21:30:41Z,2020-01-20T16:30:24Z,
550316834,31049,CLN: Remove unused release scripts,datapythonista,closed,2020-01-15T17:07:35Z,2020-01-20T17:38:33Z,"- [X] ref #31039

Old scripts that are not used anymore.

CC: @TomAugspurger "
547776839,30871,Improve error message for DataFrame.append(<dict-like>),simonjayhawkins,closed,2020-01-09T23:15:59Z,2020-01-20T18:11:02Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import pandas as pd
>>>
>>> pd.__version__
'0.26.0.dev0+1731.g3ddd495e4'
>>>
>>> df = pd.DataFrame({""Name"": [""Alice""], ""Gender"": [""F""]})
>>> df
    Name Gender
0  Alice      F
>>>
>>> df.append({""Name"": ""Bob"", ""Gender"": ""M""})
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\simon\pandas\pandas\core\frame.py"", line 7013, in append
    ""Can only append a Series if ignore_index=True ""
TypeError: Can only append a Series if ignore_index=True or if the Series has a name

>>>

```
#### Problem description
dict-like is valid type for `other` parameter of DataFrame.append

#### Expected Output

ValueError: Can only append a dict-like if ignore_index=True

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
547797655,30874,CLN: fix wrong types getting passed to TDI._get_string_slice,jbrockmendel,closed,2020-01-10T00:27:19Z,2020-01-20T18:57:16Z,Removes unreachable code paths.
548363225,30908,BUG: EAs should not be hashable,jbrockmendel,closed,2020-01-11T01:44:37Z,2020-01-20T18:58:40Z,This turns out to be a blocker for some cleanup in index.pyx
548332458,30902,BUG: raise on non-hashable in __contains__,jbrockmendel,closed,2020-01-10T23:05:59Z,2020-01-20T19:00:48Z,"By checking before we get to the `Engine.__contains__` call, we can avoid a redundant call to `hash`"
547803826,30876,ENH Avoid redundant CSS in Styler.render,jnothman,closed,2020-01-10T00:51:21Z,2020-01-20T19:52:11Z,"Where multiple styled cells have the same CSS, only make one CSS declaration. This can reduce the output size substantially.

- [x] ~~closes #xxxx~~
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
550518472,31058,REF: use _get_string_slice in PeriodIndex.get_value,jbrockmendel,closed,2020-01-16T01:19:55Z,2020-01-20T19:54:42Z,"Most of _get_string_slice is duplicated inside PeriodIndex.get_value, which likely explains why get_string_slice has almost no test [coverage](https://codecov.io/gh/pandas-dev/pandas/src/master/pandas/core/indexes/period.py#L715).  This also brings PeriodIndex.get_value into closer alignment with DatetimeIndex.get_value, so we may be able to share code a few steps from here.

"
542233334,30461,TYP: Annotations in pandas/core/nanops.py,ShaharNaveh,closed,2019-12-25T01:58:33Z,2020-01-20T20:02:08Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
541450020,30403,TYP: Type annotations in pandas/io/formats/style.py,ShaharNaveh,closed,2019-12-22T13:19:42Z,2020-01-20T20:03:41Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
549806027,31016,BUG: pivot_table with multi-index columns only and margins=True gives wrong output or fails,charlesdong1991,closed,2020-01-14T20:40:53Z,2020-01-20T20:08:12Z,"Discovered in #31013 , when `pivot_table` only defines `columns` with single column and with `margins` set to `True`, it will fail.

```python
>>> df = pd.DataFrame({""A"": [""foo"", ""foo"", ""foo"", ""foo"", ""foo"",""bar"", ""bar"", ""bar"", ""bar""],
                    ""B"": [""one"", ""one"", ""one"", ""two"", ""two"",""one"", ""one"", ""two"", ""two""],
                    ""C"": [""small"", ""large"", ""large"", ""small"",""small"", ""large"", ""small"", ""small"",""large""],
                    ""D"": [1, 2, 2, 3, 3, 4, 5, 6, 7],
                    ""E"": [2, 4, 5, 5, 6, 6, 8, 9, 9]})
>>> df.pivot_table(columns=""A"", margins=True, aggfunc=np.mean)
KeyError: 'bar'
```
However, when defining two columns, it magically works, but return a wrong shape:
```python
>>> df.pivot_table(columns=[""A"", ""B""], margins=True, aggfunc=np.mean)

   A    B  
D  bar  one    4.500000
        two    6.500000
   foo  one    1.666667
        two    3.000000
   All         3.666667
E  bar  one    7.000000
        two    9.000000
   foo  one    3.666667
        two    5.500000
   All         6.000000
```
This returns a Series, but A, B should be column index, so should get a DataFrame.

"
551985519,31140,Feature Request: Add offset as head() second parameter,StashOfCode,closed,2020-01-19T21:05:16Z,2020-01-20T20:16:29Z,"It would be very cool and intuitive if we could do somethig like this:
```python
df.head(10, 100) 
```
#### Problem description
There are a lot of situations where you need to look through rows and rows of data and currently the alternatives are: 

1. play with pd.options to display more rows
2. use indexing and slicing
3. output dataframe to excel

Adding `offset` would definitely save a lot of time and effort since it would take only a second to use. It is even easier than <strong>SQL</strong>'s `LIMIT` `OFFSET` approach

#### Docs
```python
DataFrame.head(self, n=5, offset=0)

Parameters: 
n : int, default 5. Number of rows to select.
offset: int, default 0. Skips the offset rows before selecting rows.
```
#### Expected Output
```python
>>> df = pd.DataFrame({'animal':['alligator', 'bee', 'falcon', 'lion',
...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})
>>> df
      animal
0  alligator
1        bee
2     falcon
3       lion
4     monkey
5     parrot
6      shark
7      whale
8      zebra

>>> df.head(3, 2)
      animal
2     falcon
3       lion
4     monkey
```"
552020359,31143,"REF: share searchsorted between DTI/TDI/PI, insert between DTI/TDI",jbrockmendel,closed,2020-01-20T01:20:32Z,2020-01-20T21:24:34Z,"Made possible following #30950, takes the place of #30853."
552341482,31153,Cannot iterate over result of a group by multiple columns if one contains NaN,gfitas,closed,2020-01-20T14:41:23Z,2020-01-20T21:45:44Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
df = pd.DataFrame({""a"": [1, 1, None, None], ""b"": [2,3,4,5]})
r = df.groupby([""a"",""b""])
print(len(r))
for _, i in r:
    print(i)
```
#### Problem description

Hi, 
when trying to perform a group by over multiples columns and if a column contains a Nan, the composite key is ignored. We can still access to the lines by iterating over the groups property of the generic.DataFrameGroupBy by using iloc but it is unwieldy. 
```python
for i in gb.groups:
    print(df.iloc[gb.groups.get(i)]) 
```
It seems unexpected that the len of the result is different from the number of results got when iterating.

#### Expected Output
4
     a  b
0  1.0  2
     a  b
1  None  3
     a  b
1  None  4
     a  b
1  None  5
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 44.0.0.post20200106
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
None

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 44.0.0.post20200106
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

</details>
"
550646149,31068,REGR: MultiIndex level names RuntimeError in groupby.apply,jorisvandenbossche,closed,2020-01-16T08:17:55Z,2020-01-20T22:25:33Z,"```
df = pd.DataFrame({
    'A': np.arange(10), 'B': [1, 2] * 5, 
    'C': np.random.rand(10), 'D': np.random.rand(10)}
).set_index(['A', 'B'])  
df.groupby('B').apply(lambda x: x.sum())
```

On master this gives an error:
```
In [40]: df.groupby('B').apply(lambda x: x.sum())
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-40-75bc1ff12251> in <module>
----> 1 df.groupby('B').apply(lambda x: x.sum())

~/scipy/pandas/pandas/core/groupby/groupby.py in apply(self, func, *args, **kwargs)
    733         with option_context(""mode.chained_assignment"", None):
    734             try:
--> 735                 result = self._python_apply_general(f)
    736             except TypeError:
    737                 # gh-20949

~/scipy/pandas/pandas/core/groupby/groupby.py in _python_apply_general(self, f)
    752 
    753         return self._wrap_applied_output(
--> 754             keys, values, not_indexed_same=mutated or self.mutated
    755         )
    756 

~/scipy/pandas/pandas/core/groupby/generic.py in _wrap_applied_output(self, keys, values, not_indexed_same)
   1200                 if len(keys) == ping.ngroups:
   1201                     key_index = ping.group_index
-> 1202                     key_index.name = key_names[0]
   1203 
   1204                     key_lookup = Index(keys)

~/scipy/pandas/pandas/core/indexes/base.py in name(self, value)
   1171             # Used in MultiIndex.levels to avoid silently ignoring name updates.
   1172             raise RuntimeError(
-> 1173                 ""Cannot set name on a level of a MultiIndex. Use ""
   1174                 ""'MultiIndex.set_names' instead.""
   1175             )

RuntimeError: Cannot set name on a level of a MultiIndex. Use 'MultiIndex.set_names' instead.
```

On 0.25.3 this works:

```
In [10]:  df.groupby('B').apply(lambda x: x.sum()) 
Out[10]: 
          C         D
B                    
1  2.761792  3.963817
2  1.040950  3.578762
```

It seems the additional MultiIndex level that is not used to group (['A', 'B'] are index levels, but only grouping by 'B')."
551947582,31133,BUG: Break reference from grouping level to MI,TomAugspurger,closed,2020-01-19T16:29:18Z,2020-01-20T22:25:40Z,Closes https://github.com/pandas-dev/pandas/issues/31068
548109707,30885,[DOC] set klass correctly for series and dataframe set_axis,MarcoGorelli,closed,2020-01-10T14:28:37Z,2020-01-20T22:27:09Z,"- [x] closes #30881 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
548085105,30882,raise more specific error if dict is appended to frame wit…,MarcoGorelli,closed,2020-01-10T13:39:36Z,2020-01-20T22:28:04Z,"…hout ignore_index

- [x] closes #30871 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
301785480,19966,MultiIndexed unstack with tuple names fails with KeyError,TomAugspurger,closed,2018-03-02T14:33:58Z,2020-01-20T23:15:14Z,"```python
In [8]: idx = pd.MultiIndex.from_product([['a', 'b', 'c'], [1, 2, 3]], names=[('A', 'a'), ('B', 'b')])

In [9]: s = pd.Series(1, index=idx)

In [10]: s
Out[10]:
(A, a)  (B, b)
a       1         1
        2         1
        3         1
b       1         1
        2         1
        3         1
c       1         1
        2         1
        3         1
dtype: int64

In [11]: s.unstack((""A"", ""a""))
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/indexes/multi.py in _get_level_number(self, level)
    749                                  'level number' % level)
--> 750             level = self.names.index(level)
    751         except ValueError:

ValueError: 'A' is not in list

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
<ipython-input-11-1ce241b42d82> in <module>()
----> 1 s.unstack((""A"", ""a""))

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/series.py in unstack(self, level, fill_value)
   2231         """"""
   2232         from pandas.core.reshape.reshape import unstack
-> 2233         return unstack(self, level, fill_value)
   2234
   2235     # ----------------------------------------------------------------------

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/reshape/reshape.py in unstack(obj, level, fill_value)
    481             # _unstack_multiple only handles MultiIndexes,
    482             # and isn't needed for a single level
--> 483             return _unstack_multiple(obj, level, fill_value=fill_value)
    484         else:
    485             level = level[0]

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/reshape/reshape.py in _unstack_multiple(data, clocs, fill_value)
    315     index = data.index
    316
--> 317     clocs = [index._get_level_number(i) for i in clocs]
    318
    319     rlocs = [i for i in range(index.nlevels) if i not in clocs]

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/reshape/reshape.py in <listcomp>(.0)
    315     index = data.index
    316
--> 317     clocs = [index._get_level_number(i) for i in clocs]
    318
    319     rlocs = [i for i in range(index.nlevels) if i not in clocs]

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/indexes/multi.py in _get_level_number(self, level)
    751         except ValueError:
    752             if not isinstance(level, int):
--> 753                 raise KeyError('Level %s not found' % str(level))
    754             elif level < 0:
    755                 level += self.nlevels

KeyError: 'Level A not found'
```

cc @ibrahimsharaf, @toobaz does this look difficult?"
548595914,30943,BUG: Fix MutliIndexed unstack failures at tuple names,charlesdong1991,closed,2020-01-12T16:47:26Z,2020-01-20T23:15:20Z,"- [x] closes #19966
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
547635235,30856,REF: Move generic methods to aggregation.py,charlesdong1991,closed,2020-01-09T18:04:47Z,2020-01-20T23:31:25Z,"xref #29116

https://github.com/pandas-dev/pandas/pull/29116#pullrequestreview-340272465

based on @jreback comment, this PR is a precursor PR for #29116 to make PR smaller and more readable"
535184016,30164,Request: show datetime percentiles in 'describe' method,david-cortes,closed,2019-12-09T19:21:33Z,2020-01-20T23:37:56Z,"Would be nice if `pandas.Series.describe` would show percentiles of `datetime` dtypes in the same way it does for other numeric types:
```python
import pandas as pd
dt_list = [
    ""2019-01-01 00:01:00"",
    ""2019-01-01 00:02:00"",
    ""2019-01-01 00:02:10"",
    ""2019-01-01 00:10:01"",
    ""2019-01-01 00:11:00""
]
s = pd.to_datetime(pd.Series(dt_list))
s.describe()
```
```
count                       5
unique                      5
top       2019-01-01 00:02:00
freq                        1
first     2019-01-01 00:01:00
last      2019-01-01 00:11:00
dtype: object
```
```python
s.describe(percentiles = [0.25, 0.5, 0.75])
### doesn't show percentiles
```

Compare to R:
```r
dt <- c(
	""2019-01-01 00:01:00"",
	""2019-01-01 00:02:00"",
	""2019-01-01 00:02:10"",
	""2019-01-01 00:10:01"",
	""2019-01-01 00:11:00""
)
summary(as.POSIXct(dt))
```
```
                 Min.               1st Qu.                Median                  Mean               3rd Qu.                  Max. 
""2019-01-01 00:01:00"" ""2019-01-01 00:02:00"" ""2019-01-01 00:02:10"" ""2019-01-01 00:05:14"" ""2019-01-01 00:10:01"" ""2019-01-01 00:11:00""
```"
536369300,30209,ENH: show percentiles in timestamp describe (#30164),david-cortes,closed,2019-12-11T13:27:36Z,2020-01-20T23:38:33Z,"closes https://github.com/pandas-dev/pandas/issues/30164

When creating a pandas Series of timestamps and calling `describe` on it, it will not show the percentiles of the data, even if these are specified as arguments (`s.describe(percentiles = [0.25, 0.5, 0.75])`).

This PR solves the issue by introducing a new describe logic for timestamps that would:
* Add the percentiles in the same way as for numeric.
* Rename `first` and `last` to `min` and `max` in order to match numeric types.
* Add the mean of the series.
* ~~Show datetime columns alongside numerics by default in the describe method for DataFrames.~~ (just realized there were other issues in which this was deemed to be the desirable behavior)

After this, it more or less matches the functionality of R's `summary` on `POSIXct` and `Date` types.

Why I think this is a good idea: I oftentimes find myself trying to inspect tables of mixed types, and want to get a quick overview of which kind of ranges and variations are there in each column, including timestamps. Currently this is very inconvenient in pandas, and they are not displayed alongside numeric columns by default and don't match the names if passing `include = ""all""`, which is what I want to see when I call `DataFrame.describe`.

Examples:
```python
import numpy as np, pandas as pd
dt_list = [
    ""2019-01-01 00:01:00"",
    ""2019-01-01 00:02:00"",
    ""2019-01-01 00:02:10"",
    ""2019-01-01 00:10:01"",
    ""2019-01-01 00:11:00""
]
s = pd.to_datetime(pd.Series(dt_list))
s.describe()
```
```python
pd.DataFrame({
    ""col1"" : s,
    ""col2"" : np.arange(s.shape[0])
}).describe(include = ""all"")
```

Update: corrected the coding style to comply with `black` and pass the automatic test."
45558559,8540,Enhancement: XLSB support in read_excel() ,kevindavenport,closed,2014-10-11T18:28:48Z,2020-01-20T23:49:14Z,"openpyxl and xlrd do not support XLSB. I'm curious if anyone has taken a look at integrating (more like creating) the functionality into Pandas. Looks like it could be a Python package in it self. 

Spec from Microsoft:
http://msdn.microsoft.com/en-us/library/cc313133(v=office.12).aspx
"
552534021,31164,Backport PR #31133 on branch 1.0.x (BUG: Break reference from grouping level to MI),meeseeksmachine,closed,2020-01-20T22:25:45Z,2020-01-20T23:59:04Z,Backport PR #31133: BUG: Break reference from grouping level to MI
552507770,31162,CLN/STY: various code cleanups,ShaharNaveh,closed,2020-01-20T21:03:44Z,2020-01-21T00:43:01Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
187661234,14600,Concatenation of category value counts mixes up the index order,jbao,closed,2016-11-07T09:24:01Z,2020-01-21T01:06:09Z,"Let's say we have two `Series` s1 and s2, which can be the output of the `pd.value_counts()` function, and we want to combine them into one `DataFrame`

```python
s1 = pd.Series([39,6,4], index=pd.CategoricalIndex(['female','male','unknown']))
s2 = pd.Series([2,152,2,242,150], index=pd.CategoricalIndex(['f','female','m','male','unknown']))
pd.DataFrame([s1,s2])
```

The result is 

        female   male  unknown      f      m
    0     NaN   39.0      NaN    6.0    4.0
    1     2.0  152.0      2.0  242.0  150.0

where the order of categories in the first row is changed.

And the current workaround is 

    pd.DataFrame([pd.Series(s1.values,index=s1.index.astype(list)),pd.Series(s2.values,index=s2.index.astype(list))])

which gives the correct result

            f  female    m   male  unknown
    0  NaN    39.0  NaN    6.0      4.0
    1  2.0   152.0  2.0  242.0    150.0


#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Linux
OS-release: 3.19.0-31-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.19.0
nose: None
pip: 8.1.2
setuptools: 27.2.0
Cython: None
numpy: 1.11.2
scipy: 0.18.1
statsmodels: None
xarray: None
IPython: 5.1.0
sphinx: None
patsy: None
dateutil: 2.5.3
pytz: 2016.7
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
boto: None
pandas_datareader: None


</details>
"
197161349,14955,DataFrameGroupBy fillna swallows exceptions.,RobertasA,closed,2016-12-22T12:10:25Z,2020-01-21T01:06:10Z,"
#### Problem description
Doing a fillna on DataFrameGroupBy fails silently - returning an empty dataframe, rather than raising what the error is.
This often masks the actual problem (e.g. errors like 
```ValueError: Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True``` when handling tz-aware data.)

#### Code Sample, a copy-pastable example if possible

```python
>>> df = pd.DataFrame({'a':[1,2],'b':[1,1]})
>>> df.groupby('b').fillna()
Empty DataFrame
Columns: []
Index: []
```

#### Expected Output
```ValueError: must specify a fill method or value```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.10.final.0
python-bits: 64
OS: Darwin
OS-release: 15.6.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: None.None

pandas: 0.19.1
nose: 1.3.7
pip: 1.5.6
setuptools: 25.1.1
Cython: 0.23.2
numpy: 1.11.1
scipy: 0.14.0
statsmodels: 0.6.1
xarray: None
IPython: 2.3.1
sphinx: None
patsy: 0.4.1
dateutil: 2.5.3
pytz: 2016.6.1
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: 1.4.3
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.999
httplib2: 0.9
apiclient: 1.1
sqlalchemy: None
pymysql: None
psycopg2: 2.6.1 (dt dec pq3 ext lo64)
jinja2: 2.7.3
boto: 2.26.0
pandas_datareader: None

</details>
"
194730027,14849,groupby type coercion dependent on presence of datetime column in grouped data,wes-turner,closed,2016-12-10T00:34:17Z,2020-01-21T01:06:10Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

foo = pd.DataFrame.from_records(
    [ 
      (pd.datetime(2016,1,1), 'red', 'dark', 1, '8'),
      (pd.datetime(2015,1,1), 'green', 'stormy', 2, '9'),
      (pd.datetime(2014,1,1), 'blue', 'bright', 3, '10'),
      (pd.datetime(2013,1,1), 'blue', 'calm', 4, 'potato')
    ],
    columns=['observation', 'color', 'mood', 'intensity', 'score'])

# The type of 'score' changes depending on the types passed through the groupby
print(pd.concat(
    [
        foo.dtypes,
        foo.loc[:,['observation', 'color', 'mood', 'intensity', 'score']].groupby('color').apply(lambda g: g.iloc[0]).dtypes,
        foo.loc[:,[               'color', 'mood', 'intensity', 'score']].groupby('color').apply(lambda g: g.iloc[0]).dtypes
    ],
    axis=1,
    keys=['original DF', 'w/ datetime', 'w/o datetime']))
```
#### Problem description

When the results of a groupby contain a Series with a datetime and are aggregated back into a DataFrame, columns of object type are cast numeric when possible.  When that Series contains no datetime, they are not.

The presence of a datetime elsewhere in the Series should not have effects on unrelated columns.  Doing no implicit type coercion seems (to me) like the safest option (especially in a language where ""1"" != 1).  But regardless, whether or not type coercion is done for a column 'A' should not depend on the types of all the column 'B's.

Issue #14423 is a different problem over the same code.

#### Expected Output
Current:
```
                original DF     w/ datetime w/o datetime
color                object          object       object
intensity             int64           int64        int64
mood                 object          object       object
observation  datetime64[ns]  datetime64[ns]          NaN
score                object           int64       object
```

Expected:
```
                original DF     w/ datetime w/o datetime
color                object          object       object
intensity             int64           int64        int64
mood                 object          object       object
observation  datetime64[ns]  datetime64[ns]          NaN
score                object          object       object
```
**-or-**
```
                original DF     w/ datetime w/o datetime
color                object          object       object
intensity             int64           int64        int64
mood                 object          object       object
observation  datetime64[ns]  datetime64[ns]          NaN
score                object           int64        int64
```
#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.4.3.final.0
python-bits: 64
OS: Linux
OS-release: 3.13.0-101-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.19.1
nose: 1.3.1
pip: 1.5.4
setuptools: 3.3
Cython: 0.24.1
numpy: 1.11.2
scipy: 0.18.1
statsmodels: None
xarray: None
IPython: 5.1.0
sphinx: None
patsy: None
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: 1.5.3
openpyxl: 2.4.0
xlrd: 1.0.0
xlwt: None
xlsxwriter: None
lxml: 3.3.3
bs4: 4.2.1
html5lib: 0.999
httplib2: 0.8
apiclient: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
boto: None
pandas_datareader: None
</details>
"
187490514,14592,Setting DataFrame of Python objects and MultiIndex columns wth single-element NDFrame inserts list,toobaz,closed,2016-11-05T11:03:46Z,2020-01-21T01:06:10Z,"#### A small, complete example of the issue

```python

In [2]: t = pd.DataFrame('a', index=range(2),
                              columns=pd.MultiIndex.from_product([range(2), range(2)]))

In [3]: t.loc[0, [(0,1)]] = t.loc[0, [(0,1)]]

In [4]: t
Out[4]: 
   0       1   
   0    1  0  1
0  a  [a]  a  a
1  a    a  a  a

```
The same happens when providing, rather than a list of indices, a mask with only one ``True`` value.

The above line is an useless example, but this is a problem in in-place operations.

I suspect the fix should not be very complicated, given that everything works smoothly if we have numbers rather than Python objects (the bug instead arises both if we assign a cell of a numbers-only DF to a cell of a DF of objects,  _and_ if we do the opposite)

#### Expected Output

Just the original ``t``.



#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: 7a2bcb6605bacea858ec14cfac424898deb568b3
python: 3.5.2.final.0
python-bits: 64
OS: Linux
OS-release: 4.7.0-1-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: it_IT.utf8
LOCALE: it_IT.UTF-8

pandas: 0.19.0+67.g7a2bcb6.dirty
nose: 1.3.7
pip: 8.1.2
setuptools: 28.0.0
Cython: 0.23.4
numpy: 1.11.2
scipy: 0.18.1
statsmodels: 0.8.0.dev0+f80669e
xarray: None
IPython: 5.1.0.dev
sphinx: 1.4.8
patsy: 0.3.0-dev
dateutil: 2.5.3
pytz: 2015.7
blosc: None
bottleneck: 1.2.0dev
tables: 3.2.2
numexpr: 2.6.0
matplotlib: 1.5.3
openpyxl: None
xlrd: 1.0.0
xlwt: 1.1.2
xlsxwriter: 0.9.3
lxml: None
bs4: 4.5.1
html5lib: 0.999
httplib2: 0.9.1
apiclient: 1.5.2
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.8
boto: 2.40.0
pandas_datareader: 0.2.1


</details>
"
129389481,12163,Dead code breaks unpickling of timezone objects,akaihola,closed,2016-01-28T09:05:35Z,2020-01-21T01:06:10Z,"There is a problem when writing pickles with Pandas 0.14.1 and reading them with Pandas 0.17.1. The problem occurs if the pickles contain `isodate.UTC` or other timezone objects.

The problem can be reduced to this test case:

**my_tz.py**:

```
from datetime import tzinfo

class MyTz(tzinfo):
    def __init__(self):
        pass
```

**write_pickle_test.py** (run with Pandas 0.14.1):

```
import pandas as pd
import cPickle
from my_tz import MyTz

data = pd.Series(), MyTz()
with open('test.pickle', 'wb') as f:
    cPickle.dump(data, f)
```

**read_pickle_test.py** (run with Pandas 0.17.1):

```
import pandas as pd
pd.read_pickle('test.pickle')
```

Reading the pickle with `pickle.load()` would fail when trying to load the `Series` object:

```
TypeError: _reconstruct: First argument must be a sub-type of ndarray
```

which is why `pd.read_pickle()` attempts to use `pandas.compat.pickle_compat.load()`. But then we get this instead for the `MyTz` object:

```
$ python read_pickle.py
Traceback (most recent call last):
  File ""read_pickle_test.py"", line 2, in <module>
    pd.read_pickle('test.pickle')
  File ""pandas/io/pickle.py"", line 60, in read_pickle
    return try_read(path)
  File ""pandas/io/pickle.py"", line 57, in try_read
    return pc.load(fh, encoding=encoding, compat=True)
  File ""pandas/compat/pickle_compat.py"", line 116, in load
    return up.load()
  File ""/usr/lib64/python2.7/pickle.py"", line 858, in load
    dispatch[key](self)
  File ""pandas/compat/pickle_compat.py"", line 16, in load_reduce
    if type(args[0]) is type:
IndexError: tuple index out of range
```

I see the following problems in [pandas.compat.pickle_compat.load_reduce()](/pydata/pandas/blob/9bc82438268742575e501d099c6f22ce0a2dcac1/pandas/compat/pickle_compat.py#L13):

(1) It attemps to access `args[0]` while `args` can be empty.

```
if type(args[0]) is type:
    n = args[0].__name__
```

(2) The above code is dead code anyway, since `n` isn't referenced to anywhere else in the function. Removing those two lines fixes the unpickling problem completely.

Note that the `MyTz` class in the test above does implement a proper `__init__()` method as required in [Python documentation for tzinfo](https://docs.python.org/2.7/library/datetime.html#tzinfo-objects): _""Special requirement for pickling: A tzinfo subclass must have an `__init__` method that can be called with no arguments, else it can be pickled but possibly not unpickled again. This is a technical requirement that may be relaxed in the future.""_ While `isodate.UTC` violates this requirement, that doesn't seem to be the cause of this problem.

(3) Also, at the very end of `load_reduce()`:

```
stack[-1] = value
```

is code that is never reached, since all previous code branches either return or raise. Also, this line is invalid, since `value` isn't initialized anywhere in the function.

---

I originally sent this as [a comment](/pydata/pandas/issues/6871#issuecomment-176043180) to issue #6871, but since it's not exactly the same thing (only a similar traceback), I think it deserves a separate issue.
"
198766773,15058,"When running set_index on a categorical to a MultiIndex, it gets coerced to a string. ",thequackdaddy,closed,2017-01-04T17:39:51Z,2020-01-21T01:06:11Z,"Hello! 

I apologize if this expected behavior. This is relatively similar to [this StackOverflow question](http://stackoverflow.com/questions/30574740/pandas-multiindex-levels-custom-sort).

#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

x = pd.Categorical(['apples', 'dairy', 'chicken', 'beef', 'apples', 'dairy', 'chicken'], categories=['apples', 'dairy', 'beef', 'chicken'])
y = pd.Series([1, 2, 1, 2, 1, 2, 1])
z = pd.Series([3, 4, 2, 1, 3, 2, 1])

df = pd.DataFrame({'z': z, 'x': x, 'y':y})
df.set_index(['x', 'y']).sort_index()
df.sort_values('x')
```
#### Problem description

I would like to sort and group-by a column in a custom way. In the example above, I've ordered a categorical (it could be a string) in a way that makes intuitive sense. In this example, I want fruits first, followed by dairy, followed by meats. 

#### Expected Output

When the categorical is in a MultiIndex, `set_index` seems to coerce the categorical to a string before adding it to the index. It would be nicer if pandas kept the categorical ordering for the index. 

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en

pandas: 0.18.1
nose: 1.3.7
pip: 8.1.2
setuptools: 27.2.0
Cython: 0.24.1
numpy: 1.11.1
scipy: 0.18.1
statsmodels: 0.8.0.dev0+7e6b94b
xarray: None
IPython: 5.1.0
sphinx: 1.4.6
patsy: 0.4.1
dateutil: 2.5.3
pytz: 2016.6.1
blosc: None
bottleneck: 1.1.0
tables: 3.2.2
numexpr: 2.6.1
matplotlib: 1.5.3
openpyxl: 2.3.2
xlrd: 1.0.0
xlwt: 1.1.2
xlsxwriter: 0.9.3
lxml: 3.6.4
bs4: 4.5.1
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 1.0.13
pymysql: 0.7.9.None
psycopg2: None
jinja2: 2.8
boto: 2.42.0
pandas_datareader: None

</details>
"
552490078,31161,TST: Add regression tests for fixed issues,mroeschke,closed,2020-01-20T20:14:22Z,2020-01-21T01:06:15Z,"- [x] closes #14600
- [x] closes #14849
- [x] closes #14955
- [x] closes #14592
- [x] closes #12163
- [x] closes #12948
- [x] closes #15058
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
468524860,27414,read_csv memory leak,xiahaohuan001,closed,2019-07-16T08:44:51Z,2020-01-21T04:15:08Z,"#### Code Sample, a copy-pastable example if possible

```python
import sys
import pandas as pd
_dir = r'D:\Python\data'
file_name = pd.read_csv(_dir + r'\dblist.csv', index_col=0)
file_name.loc[:, 'code'] = [x.upper() for x in file_name['code']]
file_name.loc[:, 'name'] = [x.upper() for x in file_name['name']]
file_name.loc[:, 'date'] = [str(x) for x in file_name['date']]

file_name11 = pd.read_csv(_dir + r'\dblist.csv', index_col=0)
file_name11.loc[:, 'code'] = [x.upper() for x in file_name11['code']]
file_name11.loc[:, 'name'] = [x.upper() for x in file_name11['name']]
file_name11.loc[:, 'date'] = [str(x) for x in file_name11['date']]

from copy import deepcopy
open_file1 = file_name.loc[file_name['name'] == 'IC1601', ['path', 'date']]
open_file11 = open_file1.copy()


for i2, fi in enumerate(open_file11['path']):
    # fi0 = r'D:\Python\data\Full\2016\f_c201601dn\zc\20160104\IC1601_20160104.csv'
    print('file_name0:', sys.getsizeof(file_name))
    data = pd.read_csv(fi)# (fi = r'D:\Python\data\Full\2016\f_c201601dn\zc\20160104\IC1601_20160104.csv',type:str)
    print('file_name1:', sys.getsizeof(file_name))
    del open_file2
    print('file_name2:', sys.getsizeof(file_name))
del open_file1
```
#### Problem description
I found a memory leak issue when I use pd.read_csv to read a csv file.
As the code shows,when I pd.read_csv(**fi**) the **file_name** add 104 byte memory everytimes.And the **file_name**'s shape is not change.The situation is not change when I write enumerate(open_file1).But when I use a static const str fi0,same content and type with fi,just pd.read_csv(**fi0**)memory is normal.I'm not sure it's a read_csv memory leak or not.My English is poor,I hope that I have already expressed my meaning.

output:
file_name0: 141473975
file_name1: 141474079
file_name2: 141474079
file_name0: 141474079
file_name1: 141474183
file_name2: 141474183
file_name0: 141474183
file_name1: 141474287
file_name2: 141474287

#### Expected Output

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 15 Stepping 11, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None
pandas: 0.24.2
pytest: None
pip: 19.1.1
setuptools: 41.0.1
Cython: None
numpy: 1.16.4
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.3
openpyxl: None
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: None
lxml.etree: 4.3.3
bs4: 4.7.1
html5lib: None
sqlalchemy: 1.3.3
pymysql: 0.9.3
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
424473612,25846,reset_index() error after crosstab,MichaelSchroter,closed,2019-03-23T07:29:15Z,2020-01-21T04:23:19Z,"Hi All,

I have a pandas dataframe which I constructed after doing a crosstab. When I try to `reset_index()` it gives me an error. Saying `TypeError: cannot insert an item into a CategoricalIndex that is not already an existing category` this happens sometimes and does not happen sometimes. The pandas dataframe was constructed using dask `.compute()` method.

Would anyone be able to tell me what is the issue and how to rectify it. I have `pandas 0.24.2 ` running. 

Thanks

Michael"
452980885,26687,rolling_apply forces output type to be numerical,tobi-lipede-oodle,closed,2019-06-06T11:35:37Z,2020-01-21T04:27:19Z,"#### Code Sample, a copy-pastable example if possible

```python
pd.Series([1, 2, 3, 4]).rolling(2).apply(lambda n: str(n))
```
#### Problem description
When using `rolling(x),apply(func)`, an error is thrown if the return value of `func`  is non-numeric. Is this necessary? There are valid cases for a rolling apply function returning a string value.

#### Expected Output
```
0    1
1    2
2    3
3    4
dtype: object
```

#### Output of ``pd.show_versions()``
<details>

```
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.5.final.0
python-bits: 64
OS: Linux

pandas: 0.24.2
```

</details>
"
458108990,26948,to_csv header=False not working,mkagenius,closed,2019-06-19T16:25:13Z,2020-01-21T04:28:47Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
 c = a.apply(fixsub, axis=1)
 c = c.fillna(""nan"")
 c.to_csv('solution2.csv', header=False, index=False)
```
#### Problem description

Even after putting header=False, header keeps coming in the csv

#### Expected Output

no header should come in the csv file


#### Output of ``pd.show_versions()``
0.24.2

### How to fix

culprit code in pandas/io/formats/csvs.py

```python

    def _save_header(self):

        writer = self.writer
        obj = self.obj
        index_label = self.index_label
        cols = self.cols
        has_mi_columns = self.has_mi_columns
        header = self.header
        encoded_labels = []

        has_aliases = isinstance(header, (tuple, list, np.ndarray,
                                          ABCIndexClass))
        if not (has_aliases or self.header): ## BUG HERE FIX NEEDED
            return

```

[paste the output of ``pd.show_versions()`` here below this line]
0.24.2
</details>
"
466395759,27325,IndexError when trying to Fill Gaps in Time Series using pd.DataFrame.resample,thakursc1-zz,closed,2019-07-10T16:06:20Z,2020-01-21T04:29:47Z,"#### Code Sample
```
df = pd.read_hdf(file_path, key=""alarms"")
df['acqTime'] = pd.to_datetime(df['acqTime'], unit='s')
df.set_index('acqTime', inplace= True)
df.sort_index(inplace=True)
df = df.resample(""2S"").fillna(""backfill"")
```
#### Problem description
I am trying to fill gaps in my time-series by resampling data every 2 seconds. Sometimes it may be the case the number of missing data points are even or odd. I want to even it out for 2 seconds. 

But using the code above I get the following error:
```

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
BigQuery.py"", line 94, in <module>
    data = p.map(worker_partial, alarms[:2])
lib\multiprocessing\pool.py"", line 268, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File lib\multiprocessing\pool.py"", line 657, in get
    raise self._value
ValueError: cannot reindex a non-unique index with a method or limit
```

####  Output Description

## Before Resampling: 
```
[Timestamp('2018-10-16 05:13:38'),
 Timestamp('2018-10-16 05:13:40'),
 Timestamp('2018-10-16 05:13:42'),
 Timestamp('2018-10-16 05:13:44'),
 Timestamp('2018-10-16 05:13:47'),
 Timestamp('2018-10-16 05:13:55),
]
```
After Resampling Expected Output: 
```
[Timestamp('2018-10-16 05:13:38'),
 Timestamp('2018-10-16 05:13:40'),
 Timestamp('2018-10-16 05:13:42'),
 Timestamp('2018-10-16 05:13:44'),
 Timestamp('2018-10-16 05:13:46'),
 Timestamp('2018-10-16 05:13:47'),
 Timestamp('2018-10-16 05:13:49'),
 Timestamp('2018-10-16 05:13:51'),
 Timestamp('2018-10-16 05:13:53'),
 Timestamp('2018-10-16 05:13:55'),
]
```

#### Output of ``pd.show_versions()``
```
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.2
pytest: None
pip: 19.1.1
setuptools: 41.0.1
Cython: None
numpy: 1.16.4
scipy: 1.3.0
pyarrow: None
xarray: None
IPython: 7.5.0
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: None
tables: 3.5.2
numexpr: 2.6.9
feather: None
matplotlib: 3.1.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: 0.10.0
pandas_datareader: None
gcsfs: None
​
```
<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
525145798,29714,After Anaconda upgrade to 0.25.3 can't import pandas,boardtc,closed,2019-11-19T17:10:54Z,2020-01-21T05:50:30Z,"#### Code Sample, a copy-pastable example if possible

```
conda update pandas
```
#### Problem description
I did a conda update pandas at the anaconda prompt today to update from 0.23.x to 0.25.3 and now I get this error in Spyder simply importing pandas.

```
Traceback (most recent call last):

  File ""<ipython-input-4-7dd3504c366f>"", line 1, in <module>
    import pandas as pd

  File ""D:\Applications\Anaconda3\lib\site-packages\pandas\__init__.py"", line 55, in <module>
    from pandas.core.api import (

  File ""D:\Applications\Anaconda3\lib\site-packages\pandas\core\api.py"", line 5, in <module>
    from pandas.core.arrays.integer import (

  File ""D:\Applications\Anaconda3\lib\site-packages\pandas\core\arrays\__init__.py"", line 7, in <module>
    from .categorical import Categorical  # noqa: F401

  File ""D:\Applications\Anaconda3\lib\site-packages\pandas\core\arrays\categorical.py"", line 54, in <module>
    from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs

  File ""D:\Applications\Anaconda3\lib\site-packages\pandas\core\base.py"", line 36, in <module>
    import pandas.core.nanops as nanops

  File ""D:\Applications\Anaconda3\lib\site-packages\pandas\core\nanops.py"", line 38, in <module>
    bn = import_optional_dependency(""bottleneck"", raise_on_missing=False, on_version=""warn"")

  File ""D:\Applications\Anaconda3\lib\site-packages\pandas\compat\_optional.py"", line 99, in import_optional_dependency
    version = _get_version(module)

  File ""D:\Applications\Anaconda3\lib\site-packages\pandas\compat\_optional.py"", line 48, in _get_version
    raise ImportError(""Can't determine version for {}"".format(module.__name__))

ImportError: Can't determine version for bottleneck
```
[29181](https://github.com/pandas-dev/pandas/issues/29181) is similar but the iPython was already installed as part of Anaconda,

#### Expected Output

None. Should be able to import pandas as pd

#### Output of ``pd.show_versions()``

<details>
Traceback (most recent call last):

  File ""<ipython-input-5-3d232a07e144>"", line 1, in <module>
    pd.show_versions()

NameError: name 'pd' is not defined
</details>
"
552554755,31166,Backport PR #29836 on branch 1.0.x (ENH: XLSB support),meeseeksmachine,closed,2020-01-20T23:49:24Z,2020-01-21T10:13:28Z,Backport PR #29836: ENH: XLSB support
529110655,29879,pd.concat with copy=True doesn't copy columns index,mwtoews,closed,2019-11-27T04:39:54Z,2020-01-21T10:50:47Z,"#### Code Sample

```python
import pandas as pd
print(pd.__version__)  # '0.25.3'

# simple frame with a name attribute for columns
df = pd.DataFrame({'a': [1.1], 'b': [2.2]})
df.columns.name = 'x'
print(df)
# x    a    b
# 0  1.1  2.2

# tile rows to a new frame, apparently using the copy of the first frame
df2 = pd.concat([df] * 2, copy=True)
print(df2)
# x    a    b
# 0  1.1  2.2
# 0  1.1  2.2

# clear the column name for the first frame
df.columns.name = None

# and observe the name for the second
assert df2.columns.name is None
print(df2)
#      a    b
# 0  1.1  2.2
# 0  1.1  2.2

# aha, it's the same object
assert df.columns is df2.columns
```

#### Problem description

It is expected that `copy=True` should make a copy of the `columns` index. However, the demonstration shows it's the same object. Modifications, such as the `name` property, are made to the source frame and the output frame created by `pd.concat`.

#### Expected Output

It is expected that `df2.columns` is a copy of the original `df.columns`. This would prevent (e.g.) changing the `name` property for both frames.

A workaround is to manually copy this property after `pd.concat`:
```python
df2 = pd.concat([df] * 2, copy=True)
df2.columns = df2.columns.copy()
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 63 Stepping 2, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.1.post20191125
Cython           : 0.29.14
pytest           : 5.3.0
hypothesis       : None
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.6
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.11
tables           : 3.6.1
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.6
</details>"
551735953,31119,BUG: concat not copying index and columns when copy=True,fujiaxiang,closed,2020-01-18T07:48:20Z,2020-01-21T10:59:50Z,"- [x] closes #29879
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
545296534,30679,BUG: groupby apply raises ValueError when groupby axis has duplicates and applied identity function,fujiaxiang,closed,2020-01-04T15:22:01Z,2020-01-21T11:00:06Z,"This is a more of a patch than a complete solution to this groupby apply paradigm. 
When there are duplicates in the groupby axis, we restore the axis to its original order, but not guaranteeing that the order of the data with the same axis values is restored.

- [x] closes #30667
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
552850817,31174,Pip 20 ruins builds.,baevpetr,closed,2020-01-21T12:36:57Z,2020-01-21T12:49:45Z,"#### Problem description:

During testing, it was noticed that setting up the environment with version 20 pip fails.
New release was several hours ago.

#### Output
```
[Updating pip]
Collecting pip
  Downloading https://files.pythonhosted.org/packages/60/65/16487a7c4e0f95bb3fc89c2e377be331fd496b7a9b08fd3077de7f3ae2cf/pip-20.0-py2.py3-none-any.whl (1.5MB)
Requirement already up-to-date: wheel in /home/runner/miniconda3/envs/pandas-dev/lib/python3.7/site-packages (0.33.6)
Requirement already up-to-date: setuptools in /home/runner/miniconda3/envs/pandas-dev/lib/python3.7/site-packages (45.1.0.post20200119)
Installing collected packages: pip
  Found existing installation: pip 19.3.1
    Uninstalling pip-19.3.1:
      Successfully uninstalled pip-19.3.1
Successfully installed pip-20.0
[Install pandas]
Traceback (most recent call last):
  File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pip/__main__.py"", line 19, in <module>
    sys.exit(_main())
  File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pip/_internal/cli/main.py"", line 73, in main
    command = create_command(cmd_name, isolated=(""--isolated"" in cmd_args))
  File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pip/_internal/commands/__init__.py"", line 96, in create_command
    module = importlib.import_module(module_path)
  File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pip/_internal/commands/install.py"", line 24, in <module>
    from pip._internal.cli.req_command import RequirementCommand
  File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pip/_internal/cli/req_command.py"", line 20, in <module>
    from pip._internal.operations.prepare import RequirementPreparer
  File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pip/_internal/operations/prepare.py"", line 16, in <module>
    from pip._internal.distributions import (
  File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pip/_internal/distributions/__init__.py"", line 1, in <module>
    from pip._internal.distributions.source import SourceDistribution
ImportError: cannot import name 'SourceDistribution' from 'pip._internal.distributions.source' (/home/runner/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/pip/_internal/distributions/source/__init__.py)
##[error]Process completed with exit code 1.
```"
551978266,31139,BUG: in DataFrame.count not returning subclassed data types.,EmilianoJordan,closed,2020-01-19T20:20:12Z,2020-01-21T13:13:42Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Once #30945 is merged these tests will pass.

I also, didn't know if a PR without first submitting an issue was appropriate, but to cut down on workload I decided to give it a try. If I'm mistaken please let me know.

As for the test there are four logical branches in df.count() that needed to be address within the test:

1. Non-homogeneous data types (this is the partion of the test that is dependent on #30945 as it uses .sum()).
```
        df = tm.SubclassedDataFrame(
            {
                ""Person"": [""John"", ""Myla"", ""Lewis"", ""John"", ""Myla""],
                ""Age"": [24.0, np.nan, 21.0, 33, 26],
                ""Single"": [False, True, True, True, False],
            }
        )
        result = df.count()
        assert isinstance(result, tm.SubclassedSeries)
```
2. Homogeneous data.
```
        df = tm.SubclassedDataFrame({""A"": [1, 0, 3], ""B"": [0, 5, 6], ""C"": [7, 8, 0]})
        result = df.count()
        assert isinstance(result, tm.SubclassedSeries)
```
3. MultiIndex with level kwarg.
```
        df = tm.SubclassedDataFrame(
            [[10, 11, 12, 13], [20, 21, 22, 23], [30, 31, 32, 33], [40, 41, 42, 43]],
            index=MultiIndex.from_tuples(
                list(zip(list(""AABB""), list(""cdcd""))), names=[""aaa"", ""ccc""]
            ),
            columns=MultiIndex.from_tuples(
                list(zip(list(""WWXX""), list(""yzyz""))), names=[""www"", ""yyy""]
            ),
        )
        result = df.count(level=1)
        assert isinstance(result, tm.SubclassedDataFrame)
```
4. Force length of axis to be 0. 
```
        df = tm.SubclassedDataFrame()
        result = df.count()
        assert isinstance(result, tm.SubclassedSeries)
```"
551165245,31096,ENH: partial string indexing on non-monotonic PeriodIndex,jbrockmendel,closed,2020-01-17T02:40:30Z,2020-01-21T17:15:54Z,"Looks like the DatetimeIndex analogue of this was done in #2437 (https://github.com/pandas-dev/pandas/commit/3a173f19b308d2fb7d8e96dfc57f2e5ecd046138)

This does _not_ yet support two-sides slicing e.g. `ser[""2015"":""2016""]`, but im hoping to get that handled before long.

Partial overlap with #31058, nothing should actually conflict."
242861055,16916,Aggregate fails with mixed types in grouping series,nreeve17,closed,2017-07-13T23:50:20Z,2020-01-21T18:26:45Z,"#### Code Sample, a copy-pastable example if possible

```python
X = pd.DataFrame(data=np.random.rand(7, 3), columns=list('XYZ'), index=list('zxcvbnm'))
X['grouping'] = ['group 1', 'group 1', 'group 1', 2, 2 , 2, 'group 1']
X.groupby('grouping').aggregate(lambda x: x.tolist())
```

This is the exception and traceback that the code above returns:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/Users/nicolaireeve/miniconda2/envs/skbiodev/lib/python3.4/site-packages/pandas/core/groupby.py in aggregate(self, arg, *args, **kwargs)
   3482                     result = self._aggregate_multiple_funcs(
-> 3483                         [arg], _level=_level, _axis=self.axis)
   3484                     result.columns = Index(

/Users/nicolaireeve/miniconda2/envs/skbiodev/lib/python3.4/site-packages/pandas/core/base.py in _aggregate_multiple_funcs(self, arg, _level, _axis)
    690         if not len(results):
--> 691             raise ValueError(""no results"")
    692 

ValueError: no results

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
/Users/nicolaireeve/miniconda2/envs/skbiodev/lib/python3.4/site-packages/pandas/core/groupby.py in _aggregate_generic(self, func, *args, **kwargs)
   3508                 for name, data in self:
-> 3509                     result[name] = self._try_cast(func(data, *args, **kwargs),
   3510                                                   data)

<ipython-input-25-18b24604e98f> in <lambda>(x)
      2 X['grouping'] = ['group 1', 'group 1', 'group 1', 2, 2 , 2, 'group 1']
----> 3 X.groupby('grouping').aggregate(lambda x: x.tolist())

/Users/nicolaireeve/miniconda2/envs/skbiodev/lib/python3.4/site-packages/pandas/core/generic.py in __getattr__(self, name)
   3080                 return self[name]
-> 3081             return object.__getattribute__(self, name)
   3082 

AttributeError: 'DataFrame' object has no attribute 'tolist'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-25-18b24604e98f> in <module>()
      1 X = pd.DataFrame(data=np.random.rand(7, 3), columns=list('XYZ'), index=list('zxcvbnm'))
      2 X['grouping'] = ['group 1', 'group 1', 'group 1', 2, 2 , 2, 'group 1']
----> 3 X.groupby('grouping').aggregate(lambda x: x.tolist())

/Users/nicolaireeve/miniconda2/envs/skbiodev/lib/python3.4/site-packages/pandas/core/groupby.py in aggregate(self, arg, *args, **kwargs)
   4034         versionadded=''))
   4035     def aggregate(self, arg, *args, **kwargs):
-> 4036         return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)
   4037 
   4038     agg = aggregate

/Users/nicolaireeve/miniconda2/envs/skbiodev/lib/python3.4/site-packages/pandas/core/groupby.py in aggregate(self, arg, *args, **kwargs)
   3486                         name=self._selected_obj.columns.name)
   3487                 except:
-> 3488                     result = self._aggregate_generic(arg, *args, **kwargs)
   3489 
   3490         if not self.as_index:

/Users/nicolaireeve/miniconda2/envs/skbiodev/lib/python3.4/site-packages/pandas/core/groupby.py in _aggregate_generic(self, func, *args, **kwargs)
   3510                                                   data)
   3511             except Exception:
-> 3512                 return self._aggregate_item_by_item(func, *args, **kwargs)
   3513         else:
   3514             for name in self.indices:

/Users/nicolaireeve/miniconda2/envs/skbiodev/lib/python3.4/site-packages/pandas/core/groupby.py in _aggregate_item_by_item(self, func, *args, **kwargs)
   3554             # GH6337
   3555             if not len(result_columns) and errors is not None:
-> 3556                 raise errors
   3557 
   3558         return DataFrame(result, columns=result_columns)

/Users/nicolaireeve/miniconda2/envs/skbiodev/lib/python3.4/site-packages/pandas/core/groupby.py in _aggregate_item_by_item(self, func, *args, **kwargs)
   3539                                      grouper=self.grouper)
   3540                 result[item] = self._try_cast(
-> 3541                     colg.aggregate(func, *args, **kwargs), data)
   3542             except ValueError:
   3543                 cannot_agg.append(item)

/Users/nicolaireeve/miniconda2/envs/skbiodev/lib/python3.4/site-packages/pandas/core/groupby.py in aggregate(self, func_or_funcs, *args, **kwargs)
   2885                 result = self._aggregate_named(func_or_funcs, *args, **kwargs)
   2886 
-> 2887             index = Index(sorted(result), name=self.grouper.names[0])
   2888             ret = Series(result, index=index)
   2889 

TypeError: unorderable types: str() < int()
```
#### Problem description
If a grouping vector is of mixed type and aggregate is used after groupby(...), an exception will be raised. The source code will get [to this line](https://github.com/pandas-dev/pandas/blob/master/pandas/core/groupby.py?utf8=%E2%9C%93#L2886) and fails because sorted() does not support mixed types. 

#### Expected Output
This is what we would expect to see if the exception was not raised. This output was achieved by using a column in groupby that is of a single type. In this instance, 2 was changed to a string
```python
X = pd.DataFrame(data=np.random.rand(7, 3), columns=list('XYZ'), index=list('zxcvbnm'))
X['grouping'] = ['group 1', 'group 1', 'group 1', '2', '2' , '2', 'group 1']
X.groupby('grouping').aggregate(lambda x: x.tolist())

                                                          X  \
grouping                                                      
2         [0.9219120799240533, 0.6439069401684864, 0.035...   
group 1   [0.6884732212797477, 0.326906484996646, 0.6718...   

                                                          Y  \
grouping                                                      
2         [0.7796923828539405, 0.7668459596180287, 0.868...   
group 1   [0.20259205506065203, 0.9138593138141587, 0.95...   

                                                          Z  
grouping                                                     
2         [0.9863526134877422, 0.6342347501171951, 0.873...  
group 1   [0.054465751087565906, 0.9026560581041934, 0.9...  
```
#### Output of ``pd.show_versions()``

<details>


```
# Paste the output here pd.show_versions() here
INSTALLED VERSIONS
------------------
commit: None
python: 3.4.5.final.0
python-bits: 64
OS: Darwin
OS-release: 16.6.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.20.3
pytest: None
pip: 9.0.1
setuptools: 35.0.2
Cython: None
numpy: 1.13.1
scipy: 0.19.0
xarray: None
IPython: 6.0.0
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
pandas_gbq: None
pandas_datareader: None
```
</details>

cc @ElDeveloper "
242398534,16894,DataFrame._init_dict handles columns with nan incorrectly if columns passed separately,kernc,closed,2017-07-12T14:24:17Z,2020-01-21T18:26:45Z,"#### Code Sample, a copy-pastable example if possible

```py
>>> df = pd.DataFrame({np.nan: [1, 2]})
>>> df[np.nan]   # Arguably expectedly, nan matches nan
0    1
1    2
Name: nan, dtype: int64

>>> df = pd.DataFrame({np.nan: [1, 2], 2: [2, 3]}, columns=[np.nan, 2])
>>> df   # nan from dict didn't match nan from ensured Float64Index
  NaN    2.0
0  NaN     2
1  NaN     3
```
#### Problem description
When DataFrame is initialized from dict, if columns are passed, nan isn't recognized and retrieved from dict correctly. The problem is in [loops like](https://github.com/pandas-dev/pandas/blob/3e20eab7ad5639810b4824790cd559367b326b0b/pandas/core/frame.py#L427-L428):
```py
columns = _ensure_index(columns)  # Float64Index
for c in columns:  # c = np.float64(np.nan)  (is not np.nan)
    if c in data_dict:  # c is not in dict
        ....
```
If `columns` aren't passed separately, initialization works as expected.
```py
>>> pd.DataFrame({np.nan: [1, 2], 2: [2, 3]})
   NaN    2.0
0     1     2
1     2     3
```

Consistentcy would be nice.

#### Expected Output
```py
>>> df = pd.DataFrame({np.nan: [1, 2], 2: [2, 3]}, columns=[np.nan, 2])
>>> df   # nan from dict matches nan from Float64Index
  NaN    2.0
0  1     2
1  2     3
```
#### Output of ``pd.show_versions()``

<details>
pandas 0.21.0.dev+225.gb55b1a2fe
</details>
"
236013237,16699,.loc Multiindex DateTime slicing failures,attack68,closed,2017-06-14T21:19:59Z,2020-01-21T18:26:46Z,"related to #16637 and #14946

#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

dt_idx = pd.to_datetime(['2017-05-04','2017-05-05'])
m_idx = pd.MultiIndex.from_product([dt_idx,dt_idx], names=['Idx1','Idx2'])
df = pd.DataFrame(data=[[1,2],[3,4],[5,6],[7,6]], index=m_idx, columns=['C1','C2'])

level1 = df.index.get_level_values(1)
_mask = (level1 > '2017-05-04')

print(df.loc[(dt_idx[0]), 'C1'])              # WORKS OK
print(df.loc[(slice(None), _mask), 'C1'])     # WORKS OK
print(df.loc[('2017-05-04', _mask), 'C1'])    # WORKS OK
print(df.loc[(dt_idx[0], _mask), 'C1'])       # FAILS TypeError, invalid key

```
#### Problem description

Inconsistent behaviour of .loc slicer when combining with mask (or possibly other data types) leads to fail.
I believe this is either tangentially or directly related to other issues with similar headers but this may be a new variant. 

#### Expected Output

Failed line expected to be equivalent to the working line above.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Darwin
OS-release: 16.4.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: None
LOCALE: en_GB.UTF-8
pandas: 0.20.2
pytest: 3.0.5
pip: 9.0.1
setuptools: 32.1.0.post20161217
Cython: 0.25.2
numpy: 1.11.2
scipy: 0.18.1
xarray: None
IPython: 5.1.0
sphinx: 1.5
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: 1.2.0
tables: 3.3.0
numexpr: 2.6.1
feather: None
matplotlib: 1.5.3
openpyxl: 2.4.0
xlrd: 1.0.0
xlwt: 1.1.2
xlsxwriter: 0.9.4
lxml: None
bs4: 4.5.1
html5lib: None
sqlalchemy: 1.1.4
pymysql: None
psycopg2: None
jinja2: 2.8
s3fs: 0.0.7
pandas_gbq: None
pandas_datareader: None


</details>
"
230256834,16410,IntervalIndex get_indexer may return -1 for values that are in the index,nateyoder,closed,2017-05-21T23:33:30Z,2020-01-21T18:26:46Z,"#### Code Sample, a copy-pastable example if possible

```python
idx = pd.IntervalIndex.from_breaks(pd.np.arange(10))
target = pd.Interval(0, 1, closed='right')
# Indexer works in this case if index was sorted
a = idx.get_indexer([target])
assert a[0] == 0

idx = pd.Index(pd.np.roll(idx.values, 1))
a = idx.get_indexer([target])
assert tmp == idx[1]
# Even though the second value in the index is equal to the target `get_indexer` returns -1
a[0]
>>> -1

```
#### Problem description
I expected get_indexer on IntervalIndex is expected to return the index of matching values.  However it does not always seem to do so.

#### Expected Output
In the above example I expected an output of a = np.array([1])

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: 41113824289f957b61d715be5710496ecb39f6d5
python: 3.5.2.final.0
python-bits: 64
OS: Darwin
OS-release: 16.5.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.21.0.dev+67.g4111382
pytest: 3.0.7
pip: 8.1.2
setuptools: 35.0.2
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.19.0
xarray: None
IPython: 5.1.0
sphinx: 1.4.8
patsy: None
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: 1.2.0
tables: 3.3.0
numexpr: 2.6.2
feather: None
matplotlib: 2.0.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: 1.1.2
pymysql: 0.7.9.None
psycopg2: None
jinja2: 2.8
s3fs: None
pandas_gbq: None
pandas_datareader: None

</details>
"
208659710,15454,BUG: __getitem__ on MultiIndex with empty slice raises,TomAugspurger,closed,2017-02-18T18:23:10Z,2020-01-21T18:26:46Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
In [26]: df = pd.DataFrame(0, index=range(2), columns=pd.MultiIndex.from_product([[1], [2]]))
    ...:
    ...:

In [27]: df
Out[27]:
   1
   2
0  0
1  0

In [28]: df[[]]
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-28-13353c6d99d3> in <module>()
----> 1 df[[]]

/Users/tom.augspurger/Envs/py3/lib/python3.6/site-packages/pandas/pandas/core/frame.py in __getitem__(self, key)
   2014         if isinstance(key, (Series, np.ndarray, Index, list)):
   2015             # either boolean or fancy integer index
-> 2016             return self._getitem_array(key)
   2017         elif isinstance(key, DataFrame):
   2018             return self._getitem_frame(key)

/Users/tom.augspurger/Envs/py3/lib/python3.6/site-packages/pandas/pandas/core/frame.py in _getitem_array(self, key)
   2058             return self.take(indexer, axis=0, convert=False)
   2059         else:
-> 2060             indexer = self.loc._convert_to_indexer(key, axis=1)
   2061             return self.take(indexer, axis=1, convert=True)
   2062

/Users/tom.augspurger/Envs/py3/lib/python3.6/site-packages/pandas/pandas/core/indexing.py in _convert_to_indexer(self, obj, axis, is_setter)
   1216                 # this is not the most robust, but...
   1217                 if (isinstance(labels, MultiIndex) and
-> 1218                         not isinstance(objarr[0], tuple)):
   1219                     level = 0
   1220                     _, indexer = labels.reindex(objarr, level=level)

IndexError: index 0 is out of bounds for axis 0 with size 0
```
#### Problem description

This is inconsistent with our other indexers, which all return empty DataFrames / series

1. Regular index in the columns or index

```python
In [30]: df.T[[]]
Out[30]:
Empty DataFrame
Columns: []
Index: [(1, 2)]
```

```python
In [32]: df.loc[[]]
Out[32]:
Empty DataFrame
Columns: [(1, 2)]
Index: []
```

2. MultiIndex in the index with .loc

```python
In [34]: df.T.loc[[]]
Out[34]:
Empty DataFrame
Columns: [0, 1]
Index: []
```

A probably related bug: while `df.loc[:, []]` works as expected, `__setitem__` raises with the same error:

```python
In [65]: df.loc[:, []]  # OK
Out[65]:
Empty DataFrame
Columns: []
Index: [0, 1]
```

```python
In [66]: df.loc[:, []] = 10  # should be a no-op really
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-66-6cd48eddd6a3> in <module>()
----> 1 df.loc[:, []] = 10

/Users/tom.augspurger/Envs/py3/lib/python3.6/site-packages/pandas/pandas/core/indexing.py in __setitem__(self, key, value)
    146         else:
    147             key = com._apply_if_callable(key, self.obj)
--> 148         indexer = self._get_setitem_indexer(key)
    149         self._setitem_with_indexer(indexer, value)
    150

/Users/tom.augspurger/Envs/py3/lib/python3.6/site-packages/pandas/pandas/core/indexing.py in _get_setitem_indexer(self, key)
    125         if isinstance(key, tuple):
    126             try:
--> 127                 return self._convert_tuple(key, is_setter=True)
    128             except IndexingError:
    129                 pass

/Users/tom.augspurger/Envs/py3/lib/python3.6/site-packages/pandas/pandas/core/indexing.py in _convert_tuple(self, key, is_setter)
    192                 if i >= self.obj.ndim:
    193                     raise IndexingError('Too many indexers')
--> 194                 idx = self._convert_to_indexer(k, axis=i, is_setter=is_setter)
    195                 keyidx.append(idx)
    196         return tuple(keyidx)

/Users/tom.augspurger/Envs/py3/lib/python3.6/site-packages/pandas/pandas/core/indexing.py in _convert_to_indexer(self, obj, axis, is_setter)
   1216                 # this is not the most robust, but...
   1217                 if (isinstance(labels, MultiIndex) and
-> 1218                         not isinstance(objarr[0], tuple)):
   1219                     level = 0
   1220                     _, indexer = labels.reindex(objarr, level=level)

IndexError: index 0 is out of bounds for axis 0 with size 0
```

#### Expected Output

```python
In [47]: pd.DataFrame([], index=[0, 1])
Out[47]:
Empty DataFrame
Columns: []
Index: [0, 1]
```

I *think* that's right, all the columns should be dropped.

#### Output of ``pd.show_versions()``

<details>
# Paste the output here pd.show_versions() here

INSTALLED VERSIONS
------------------
commit: 0ceb40fde46ec705a6a7d0a7e53477e65ad2c891
python: 3.6.0.final.0
python-bits: 64
OS: Darwin
OS-release: 16.4.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.19.0+466.g0ceb40fde.dirty
pytest: 3.0.5
pip: 9.0.1
setuptools: 32.3.0
Cython: 0.25.2
numpy: 1.12.0
scipy: 0.18.1
xarray: None
IPython: 5.2.2
sphinx: 1.5.2
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: 1.2.0
tables: None
numexpr: 2.6.1
feather: None
matplotlib: 2.0.0
openpyxl: None
xlrd: 1.0.0
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.9999999
httplib2: None
apiclient: None
sqlalchemy: 1.1.5
pymysql: None
psycopg2: 2.6.2 (dt dec pq3 ext lo64)
jinja2: 2.9.5
s3fs: 0.0.8
pandas_datareader: None

</details>
"
226258869,16231,resample.apply flattens column index when more than 3 levels,jmarrec,closed,2017-05-04T12:03:21Z,2020-01-21T18:26:47Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np

##### Case MultiIndex levels = 2
cols = pd.MultiIndex.from_tuples([('A', 'one'), ('A', 'two')])
ind = pd.DatetimeIndex(start='2017-01-01', freq='15Min', periods=8)
df = pd.DataFrame(np.random.randn(8,2), index=ind, columns=cols)

# Creates an agg dict to map the column to different functions
agg_dict = { col:(np.sum if col[1] == 'one' else np.mean) for col in df.columns }
resampled = df.resample('H').apply(lambda x: agg_dict[x.name](x))
try:
    assert isinstance(resampled.columns, pd.MultiIndex)
except AssertionError as e:
    e.args += ('Case nlevels={}'.format(df.columns.nlevels),)
    raise

##### Case MultiIndex levels = 3
cols = pd.MultiIndex.from_tuples([('A', 'i','one'), ('A', 'ii','two')])
ind = pd.DatetimeIndex(start='2017-01-01', freq='15Min', periods=8)
df = pd.DataFrame(np.random.randn(8,2), index=ind, columns=cols)

# Creates an agg dict to map the column to different functions
agg_dict = { col:(np.sum if col[2] == 'one' else np.mean) for col in df.columns }
resampled = df.resample('H').apply(lambda x: agg_dict[x.name](x))
try:
    assert isinstance(resampled.columns, pd.MultiIndex)
except AssertionError as e:
    e.args += ('Case nlevels={}'.format(df.columns.nlevels),)
    raise

##### Case MultiIndex levels = 4
cols = pd.MultiIndex.from_tuples([('A', 'a', '', 'one'), ('B', 'b', 'i', 'two')])
ind = pd.DatetimeIndex(start='2017-01-01', freq='15Min', periods=8)
df = pd.DataFrame(np.random.randn(8,2), index=ind, columns=cols)

agg_dict = { col:(np.sum if col[3] == 'one' else np.mean) for col in df.columns }
resampled = df.resample('H').apply(lambda x: agg_dict[x.name](x))
try:
    assert isinstance(resampled.columns, pd.MultiIndex)
except AssertionError as e:
    e.args += ('Case nlevels={}'.format(df.columns.nlevels),)
    raise

```
#### Problem description

With MultiIndexed columns that have 2 or 3 levels, the resample().apply() does return the same MultiIndexed columns. If you go to 4 levels, returned is a single-level column where only the first level is kept.
In the above code, only the case nlevels=4 raises.

#### Expected Output

The above code shouldn't raise.


#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.0.final.0
python-bits: 64
OS: Darwin
OS-release: 16.5.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: en_US.UTF-8
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.19.2
nose: None
pip: 9.0.1
setuptools: 34.3.0
Cython: 0.25.2
numpy: 1.12.0
scipy: 0.18.1
statsmodels: 0.8.0
xarray: 0.9.1
IPython: 5.3.0
sphinx: None
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: 1.2.0
tables: 3.3.0
numexpr: 2.6.2
matplotlib: 2.0.0
openpyxl: None
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: 0.9.6
lxml: None
bs4: 4.5.3
html5lib: 0.9999999
httplib2: 0.10.3
apiclient: 1.6.2
sqlalchemy: 1.1.5
pymysql: None
psycopg2: None
jinja2: 2.9.5
boto: None
pandas_datareader: 0.3.0.post
</details>
"
201503636,15150,BUG: crosstab cannot normalize multiple columns for the index,hurcy,closed,2017-01-18T08:04:03Z,2020-01-21T18:26:47Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame({'A': ['one', 'one', 'two', 'three'] * 6, 'B': ['A', 'B', 'C'] * 8, 'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4, 'D': np.random.randn(24), 'E': np.random.randn(24)})

pd.crosstab([df.A, df.B], df.C, values=df.D, aggfunc=np.sum, normalize=True, margins=True)

```
#### Problem description

This gives ""ValueError: labels ['All'] not contained in axis""

Currently, crosstab only normalize single index.
I expect crosstab can normalize with multiple columns for the index, but I got ValueError: labels ['All'] not contained in axis. 

#### Expected Output
normalized result

#### Output of ``pd.show_versions()``

<details>
# Paste the output here pd.show_versions() here

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.12.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-47-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.19.1
nose: None
pip: 9.0.1
setuptools: 28.8.0
Cython: None
numpy: 1.11.2
scipy: 0.18.1
statsmodels: None
xarray: None
IPython: 5.1.0
sphinx: None
patsy: None
dateutil: 2.6.0
pytz: 2016.7
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: 1.5.3
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 1.1.4
pymysql: None
psycopg2: None
jinja2: 2.8
boto: None
pandas_datareader: None
None

</details>
"
552589914,31171,TST: Add more regression tests for fixed issues,mroeschke,closed,2020-01-21T02:29:18Z,2020-01-21T18:26:52Z,"- [x] closes #16894
- [x] closes #16916
- [x] closes #16410
- [x] closes #15454
- [x] closes #16699
- [x] closes #16231
- [x] closes #15150
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
529564609,29901,Strange behaviour of pd.offsets.CustomBusinessHour when using holidays,maxQC,closed,2019-11-27T20:43:11Z,2020-01-21T20:55:59Z,"#### Code Sample, a copy-pastable example if possible

```python
import datetime
import pandas as pd
from pandas.tseries.holiday import USFederalHolidayCalendar

bhour_us = pd.offsets.CustomBusinessHour(calendar=USFederalHolidayCalendar())
#bhour_us = pd.offsets.CustomBusinessHour(calendar=USFederalHolidayCalendar(),start='07:00')
#bhour_us = pd.offsets.CustomBusinessHour(calendar=USFederalHolidayCalendar(),holidays=['2014-01-21'])
#bhour_us = pd.offsets.CustomBusinessHour(calendar=USFederalHolidayCalendar(),holidays=['2014-01-21','2014-01-22'])
#bhour_us = pd.offsets.CustomBusinessHour(calendar=USFederalHolidayCalendar(),holidays=['2014-01-21','2014-02-06'])

dt = datetime.datetime(2014, 1, 17, 13)

for i in range(1,30):
    y = dt + bhour_us * i
    print(y)
```
>>>
2014-01-17 14:00:00
2014-01-17 15:00:00
2014-01-17 16:00:00
2014-01-21 09:00:00
2014-01-21 10:00:00
2014-01-21 11:00:00
2014-01-21 12:00:00
**_2014-01-20 13:00:00
2014-01-31 14:00:00
2014-01-31 15:00:00
2014-01-31 16:00:00
2014-02-03 09:00:00
2014-02-03 10:00:00
2014-02-03 11:00:00
2014-02-03 12:00:00_**
2014-01-21 13:00:00
2014-01-21 14:00:00
2014-01-21 15:00:00
2014-01-21 16:00:00
2014-01-22 09:00:00
2014-01-22 10:00:00
2014-01-22 11:00:00
2014-01-22 12:00:00
2014-01-22 13:00:00
2014-01-22 14:00:00
2014-01-22 15:00:00
2014-01-22 16:00:00
2014-01-23 09:00:00
2014-01-23 10:00:00

#### Problem description
CustomBusinessHour behaves strangely when I try to use a custom calendar (same behaviour when using a list of holidays directly).

In the example above, I modified the example from the official doc.
https://pandas.pydata.org/pandas-docs/version/0.18.1/timeseries.html#timeseries-custombusinesshour

Something strange happens (here at i in range(8, 16)).

Then, the rest is fine, but shifted away.

Playing with the calendar and length of the business day (start and end) affects this.
This seems related in some sense to a mod function somewhere... But I can't get my finger on it.
#### Expected Output
2014-01-17 14:00:00
2014-01-17 15:00:00
2014-01-17 16:00:00
2014-01-21 09:00:00
2014-01-21 10:00:00
2014-01-21 11:00:00
2014-01-21 12:00:00
2014-01-21 13:00:00
2014-01-21 14:00:00
2014-01-21 15:00:00
2014-01-21 16:00:00
2014-01-22 09:00:00
2014-01-22 10:00:00
2014-01-22 11:00:00
2014-01-22 12:00:00
2014-01-22 13:00:00
2014-01-22 14:00:00
2014-01-22 15:00:00
2014-01-22 16:00:00
2014-01-23 09:00:00
2014-01-23 10:00:00
2014-01-23 11:00:00
2014-01-23 12:00:00
2014-01-23 13:00:00
2014-01-23 14:00:00
2014-01-23 15:00:00
2014-01-23 16:00:00
2014-01-24 09:00:00
2014-01-24 10:00:00
#### Output of ``pd.show_versions()``
<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.1.post20191125
Cython           : 0.29.14
pytest           : 5.3.0
hypothesis       : None
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.6
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.11
tables           : 3.6.1
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.6
</details>"
539939203,30334,COMPAT: numpy test warning,nschloe,closed,2019-12-18T21:40:10Z,2020-01-21T21:11:55Z,"I'm currently testing a numpy deprecation to assess its impact on the ecosystem (https://github.com/numpy/numpy/pull/10615). To this end, I'd like to run pandas tests with `-Werror`. Doing that, I see that pandas throws a bunch of warnings at other places. Are those intentional? Perhaps pandas should test warning-free out of the box."
540212334,30345,COMPAT: numpy test warnings,nschloe,closed,2019-12-19T10:11:40Z,2020-01-21T21:12:02Z,"Fixes #30334.

Some of the warnings are coming from within pandas (like the FutureWarning on truediv), some are numpy warnings (like the nan-warning on sign). All in all changes are minor.

Tests are passing with `-Werror` for me now."
553209012,31189,REF: move misplaced tests,jbrockmendel,closed,2020-01-21T23:27:29Z,2020-01-22T01:22:48Z,
324221308,21112,"""Concatenation will no longer sort"" wording is very inconsistent",quant5,closed,2018-05-17T23:36:05Z,2020-01-22T01:35:06Z,"### The wording in the update docs is (emphasis added):
In a future version of pandas `pandas.concat()` will no longer sort the non-concatenation axis when it is not already aligned. (...)

To keep the previous behavior (sorting) and silence the warning, pass `sort=True`.
**To accept the future behavior (no sorting), pass `sort=False`**

https://pandas-docs.github.io/pandas-docs-travis/whatsnew.html#concatenation-will-no-longer-sort

### But the wording in the `FutureWarning` messages is (emphasis added):
Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

**To accept the future behavior, pass `sort=True`.**
To retain the current behavior and silence the warning, pass `sort=False`

https://github.com/pandas-dev/pandas/blob/c4da79b5b322c73d8e61d1cb98ac4ab1e2438b40/pandas/core/indexes/api.py

It needs to be clearer that current default is still `sort=True` but will revert to `sort=False` in a future version."
476608876,27748,Reference docs: Nearly empty page for pandas.DataFrame.plot,vlbrown,closed,2019-08-05T00:05:01Z,2020-01-22T08:43:23Z,"The web page
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html

is essentially empty. It should, instead, include the content from
https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.plot.html
![Screen Shot 2019-08-04 at 17 03 33](https://user-images.githubusercontent.com/703084/62431177-e05f2100-b6d9-11e9-880e-a06dbf09fe0a.png)

![Screen Shot 2019-08-04 at 17 03 51](https://user-images.githubusercontent.com/703084/62431178-e523d500-b6d9-11e9-9ede-693d17a92a88.png)

"
526306276,29762,TST: add tests for period resample sum with min_count,ganevgv,closed,2019-11-21T01:16:05Z,2020-01-22T13:25:02Z,"- [x] closes #19974
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
548905012,30962,AttributeError: 'FrameFixed' object has no attribute 'format_type' on 1.0.0rc0,TomAugspurger,closed,2020-01-13T12:41:15Z,2020-01-22T14:16:46Z,"```python
import pandas as pd


df = pd.DataFrame({""A"": [1, 2]})
df.to_hdf(""foo.h5"", ""foo"")

with pd.HDFStore(""foo.h5"") as hdf:
    for key in hdf.keys():
        storer = hdf.get_storer(key)
        print(type(storer), storer.format_type)
```

On 0.25.3, we have

```python
<class 'pandas.io.pytables.FrameFixed'> fixed
```

On 1.0.0rc0

```pytb
Traceback (most recent call last):
  File ""bug.py"", line 10, in <module>
    print(storer.format_type)
AttributeError: 'FrameFixed' object has no attribute 'format_type'
```

Didn't see anything about this in the release notes. cc @jbrockmendel (if this was part of your tables cleanup)."
549807471,31017,REG: restore format_type attr,jbrockmendel,closed,2020-01-14T20:43:43Z,2020-01-22T15:47:37Z,"- [x] closes #30962

cc @TomAugspurger "
553252721,31192,CLN: remove unused fixtures,jbrockmendel,closed,2020-01-22T01:25:44Z,2020-01-22T16:10:21Z,
553464913,31201,COMPAT: numpy object array coercion warnings,AlexKirko,closed,2020-01-22T11:00:28Z,2020-01-22T16:20:33Z,"#### Problem description

Since at least a couple hours ago, the Numpy Dev pipeline fails 39 tests:
```
##[error]39 test(s) failed, 60964 test(s) collected.
```
I checked and all 39 are like this:
```
numpy.VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
```

Full output of one of the tests below.

<details>

```

        True
        >>> a = np.array([(1.0, 2), (3.0, 4)], dtype='f4,i4').view(np.recarray)
        >>> np.asarray(a) is a
        False
        >>> np.asanyarray(a) is a
        True
    
        """"""
>       return array(a, dtype, copy=False, order=order)
E       numpy.VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
```

</details>

Thanks to @jreback for pointing out we've run into this before, but Numpy reverted the deprecation warning. The original issue was #30043"
553644513,31214,min(axis = 1) propagates NAN when encountering missing timestamps,randomgambit,closed,2020-01-22T16:13:44Z,2020-01-22T16:27:44Z,"Hello there,
Consider this:

```
dftest = pd.DataFrame({'time1' : ['2019-01-01- 10:10:10.123456Z',
                                  '2019-01-01- 10:10:10.123456Z'],
                       'time2' : ['ha',
                                 'pandas!!!'],
                       'time3' :['here we go',
                                'again']})

dftest
Out[179]: 
                          time1      time2       time3
0  2019-01-01- 10:10:10.123456Z         ha  here we go
1  2019-01-01- 10:10:10.123456Z  pandas!!!       again
```

I am trying to get the minimum timestamp between `time1`, `time2` and `time3`. Of course, in this example `time2` and `time3` are not timestamps so (after proper conversion to NaTs) I expect the minimum to be `time1` all the time. 

Surprisingly, this fails entirely.

```

dftest['time1'] = pd.to_datetime(dftest.time1, errors = 'coerce')
dftest['time2'] = pd.to_datetime(dftest.time2, errors = 'coerce')    
dftest['time3'] = pd.to_datetime(dftest.time3, errors = 'coerce') 
dftest
Out[181]: 
                             time1 time2 time3
0 2019-01-01 10:10:10.123456+00:00   NaT   NaT
1 2019-01-01 10:10:10.123456+00:00   NaT   NaT

```
 And now:
```
dftest.loc[:, ['time1', 'time2', 'time3']].min(axis = 1)
Out[182]: 
0   nan
1   nan
dtype: float64
```

This looks like a bug. It seems that the errors is coming from the timezone component. What do you think?

Thanks!"
553515477,31203,numpydev ragged array dtype warning,TomAugspurger,closed,2020-01-22T12:41:36Z,2020-01-22T17:48:55Z,"Closes #31201

There are still two failures in the Categorical constructor I'm looking into. Not sure what's best yet.

```python
In [2]: pd.Categorical(['a', ('a', 'b')])
/Users/taugspurger/sandbox/pandas/pandas/core/dtypes/cast.py:1066: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  v = np.array(v, copy=False)
Out[2]:
[a, (a, b)]
Categories (2, object): [(a, b), a]
```

I think that perhaps the user should see this warning, but have an option to pass through a `dtype` to silence it... Not sure yet.

Tagged for backport to keep the CI passing on that branch."
553634494,31213,DOC: pin statsmodels inventory,TomAugspurger,closed,2020-01-22T15:57:30Z,2020-01-22T18:18:27Z,xref https://github.com/pandas-dev/pandas/issues/31212
552435012,31157,REGR: outer merge resulting in missing datetime values converts to int,vadella,closed,2020-01-20T17:42:39Z,2020-01-22T19:36:22Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

df = pd.DataFrame(
    {
        ""x"": [0, 1],
        ""date"": pd.to_datetime([""2020-01-05T12:00"", ""2020-01-05T13:00""]),
    }
)
df2 = pd.DataFrame({""x"": [0, 2], ""y"": [0, 1]})

merged = pd.merge(df, df2, on=""x"", how=""outer"")
merged.info()
print(merged)
```

> ```
>     <class 'pandas.core.frame.DataFrame'>
>     Int64Index: 3 entries, 0 to 2
>     Data columns (total 3 columns):
>      #   Column  Non-Null Count  Dtype  
>     ---  ------  --------------  -----  
>      0   x       3 non-null      int64  
>      1   date    3 non-null      object 
>      2   y       2 non-null      float64
>     dtypes: float64(1), int64(1), object(1)
>     memory usage: 96.0+ bytes
> ```

> ```
>    x   date                 y
> 0  0   1578225600000000000  0.0
> 1  1   1578229200000000000  NaN
> 2  2  -9223372036854775808  1.0
> ```

```python
merged[""date""].map(type)
```

> ```
>     0    <class 'int'>
>     1    <class 'int'>
>     2    <class 'int'>
>     Name: date, dtype: object
> ```

```python
print(pd.merge(df, df2, on=""x"", how=""inner""))
```

> ```
>    x                date  y
> 0  0 2020-01-05 12:00:00  0
> ```

#### Problem description

Merging 2 DataFrames which result in missing Datetime values silently converts the column to integers with an `object` dtype, instead of keeping `datetime64[ns]` as dtype, which was the behaviour in pandas 0.25.3, and the behaviour I would expect.

#### Expected Output

```python
merged = pd.merge(df, df2, on=""x"", how=""outer"")
merged.info()
print(merged)
```

    <class 'pandas.core.frame.DataFrame'>
    Int64Index: 3 entries, 0 to 2
    Data columns (total 3 columns):
    x       3 non-null int64
    date    2 non-null datetime64[ns]
    y       2 non-null float64
    dtypes: datetime64[ns](1), float64(1), int64(1)
    memory usage: 96.0 bytes
       x                date    y
    0  0 2020-01-05 12:00:00  0.0
    1  1 2020-01-05 13:00:00  NaN
    2  2                 NaT  1.0


```python
merged[""date""].map(type)
```

> ```
>     0    <class 'pandas._libs.tslibs.timestamps.Timesta...
>     1    <class 'pandas._libs.tslibs.timestamps.Timesta...
>     2        <class 'pandas._libs.tslibs.nattype.NaTType'>
>     Name: date, dtype: object
> ```

This is the output from Python 0.25.3

Whether the `y`-column should be converted to float or `Int64` is open for debate, but not the issue at hand here.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 8.1
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.0rc0
numpy            : 1.17.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 45.1.0.post20200119
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
553736304,31217,DOC: fixup whatsnew,TomAugspurger,closed,2020-01-22T19:09:30Z,2020-01-22T19:38:00Z,The lack of newline below the ipython directive may be causing issues.
395426499,24576,REF: use _unbox_scalar to de-duplicate TDA/DTA/PA comparison methods,jbrockmendel,closed,2019-01-03T00:44:56Z,2020-01-22T19:39:28Z,"DatetimeArray and TimedeltaArray comparison methods share a lot of code, might be possible to de-duplicate a lot of it by using the new unbox_scalar method:

From DTA:
```
        if isinstance(other, (datetime, np.datetime64, compat.string_types)):
            if isinstance(other, (datetime, np.datetime64)):
                # GH#18435 strings get a pass from tzawareness compat
                self._assert_tzawareness_compat(other)

            try:
                other = _to_m8(other, tz=self.tz)
            except ValueError:
                # string that cannot be parsed to Timestamp
                return ops.invalid_comparison(self, other, op)

            result = op(self.asi8, other.view('i8'))
            if isna(other):
                result.fill(nat_result)
```

From TDA:
```
        if _is_convertible_to_td(other) or other is NaT:
            try:
                other = _to_m8(other)
            except ValueError:
                # failed to parse as timedelta
                return ops.invalid_comparison(self, other, op)

            result = meth(self, other)
            if isna(other):
                result.fill(nat_result)
```

Note that `_to_m8` means something different in these two files.

The _unbox_scalar methods might not be quite the right fit, but the idea behind them is really similar to `_is_convertible_to_td` and the try/except calls to to_m8."
553628061,31212,CI: Web and docs (pull_request) fails: can't rich intershipinx inventory,AlexKirko,closed,2020-01-22T15:47:23Z,2020-01-22T20:49:54Z,"#### Problem description
Seems like currently CI / Web and docs (pull_request) fails for everyone with this error message:
```
WARNING: failed to reach any of the inventories with the following issues:
intersphinx inventory 'http://www.statsmodels.org/devel/objects.inv' not readable due to ValueError: unknown or unsupported inventory version: ValueError('invalid inventory header: ')
```
All my PRs had the `CI / Web and docs (pull_request)` check break without any changes to docs, and I checked PRs by other people: other people seem to have this problem.
#### Expected behaviour
Passing the test"
553604575,31209,Backport PR #31203 on branch 1.0.x (numpydev ragged array dtype warning),meeseeksmachine,closed,2020-01-22T15:10:17Z,2020-01-23T09:41:02Z,Backport PR #31203: numpydev ragged array dtype warning
553570629,31206,Backport PR #31017 on branch 1.0.x (REG: restore format_type attr),meeseeksmachine,closed,2020-01-22T14:17:27Z,2020-01-23T09:41:34Z,Backport PR #31017: REG: restore format_type attr
553767108,31221,Backport PR #31215 on branch 1.0.x (Follow-up: XLSB Support),meeseeksmachine,closed,2020-01-22T20:11:35Z,2020-01-23T09:40:20Z,Backport PR #31215: Follow-up: XLSB Support
551882175,31132,TST/REF: Parameteratized tests,ShaharNaveh,closed,2020-01-19T07:51:57Z,2020-01-22T23:41:00Z,
552277944,31149,Lint issue with new version of flake8-comprehensions plugin,ShaharNaveh,closed,2020-01-20T12:44:58Z,2020-01-23T00:49:55Z,"[flake8-comprehensions](https://pypi.org/project/flake8-comprehensions/) have released a new version (3.2.1), and now when running ```./ci/code_checks.sh lint``` I get this output:

```
flake8 --version
3.7.9 (flake8-comprehensions: 3.2.1, mccabe: 0.6.1, pycodestyle: 2.5.0, pyflakes: 2.1.1) CPython 3.8.1 on Linux
Linting .py code
./pandas/tests/io/parser/test_network.py:208:21: C412 Unnecessary set comprehension - 'in' can take a generator.
Linting .py code DONE
```

(feel free to close this issue if this is happening because I have python version 3.8.1)

<details>

INSTALLED VERSIONS
------------------
commit           : 7c94949dc89c62cae1bc647acd87266d6c3a0468
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.13.a-1-hardened
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.0rc0+107.g7c94949dc
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 45.1.0
Cython           : 0.29.14
pytest           : 5.3.3
hypothesis       : 5.2.0
sphinx           : 2.3.1
blosc            : 1.8.3
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.3
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.12
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.47.0
</details>"
553879085,31231,Backport PR #31229 on branch 1.0.x ((Fixes CI) Replaced set comprehension with a generator),meeseeksmachine,closed,2020-01-23T00:50:26Z,2020-01-23T03:07:15Z,Backport PR #31229: (Fixes CI) Replaced set comprehension with a generator
553594966,31208,dtypes convert to object on merge on 1.0.0rc0,mdoom23,closed,2020-01-22T14:56:15Z,2020-01-23T04:26:32Z,"#### dtypes convert to object on merge
Currently on 1.0.0rc0, when doing a left merge with datetime64[ns] on the right dataframe, if any rows from the left dataframe don't have a match on the right dataframe, then the result dataframe converts datetime to be object.  If all items match, then it will remain as a datetime column.  This previously maintained dtype in 0.25.3 and 0.24.2.

It seems to no longer maintain the dtype and populate values with NaT.

With 1.0.0rc0, after this I am able to convert to datetime column and it'll properly recognize as a NaT value.

#### Example with extra value in left dataframe
```python
df1 = pd.DataFrame({'x': {0: 'a', 1: 'b', 2:'c'}, 'y': {0: '1', 1: '2', 2:'4'}})

df2 = pd.DataFrame({'y': {0: '1', 1: '2', 2:'3'}, 'z': {0: '2018-05-01', 1: '2018-05-02', 2:'2018-05-03'}})
df2['z'] = df2['z'].astype('datetime64[ns]')

result = pd.merge(df1, df2, how='left', on='y')

```

#### Output
```python
  # 0.24.2
result.dtypes
x            object
y            object
z    datetime64[ns]
dtype: object

  # 0.25.3
result.dtypes
x            object
y            object
z    datetime64[ns]
dtype: object

  # 1.0.0rc0
result.dtypes
x            object
y            object
z            object
dtype: object

```"
553929421,31240,Backport PR #31211 on branch 1.0.x (BUG: Fixed upcast dtype for datetime64 in merge),meeseeksmachine,closed,2020-01-23T04:26:44Z,2020-01-23T09:42:21Z,Backport PR #31211: BUG: Fixed upcast dtype for datetime64 in merge
553898219,31234,DOC BUG DataFrame.plot() and Series.plot() documentation missing,jpeacock29,closed,2020-01-23T02:07:43Z,2020-01-23T08:48:04Z,In the [API reference for `DataFrame.plot()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html#pandas.DataFrame.plot) and [`Series.plot()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.plot.html#pandas.Series.plot) the documentation is missing.
553817831,31222,Empty DatetimeIndex gets converted on assignment with .loc,tomasssalles,closed,2020-01-22T21:57:38Z,2020-01-23T09:52:06Z,"#### Code Sample

```python
from pandas import DataFrame, Series, DatetimeIndex
from numpy import float64

df = DataFrame(index=DatetimeIndex([]), columns=[""A"", ""B""]).astype({""A"": float64})
df.loc[:, ""A""] = Series(index=df.index, dtype=float64)
type(df.index)
```

#### Output

```python
pandas.core.indexes.range.RangeIndex
```

#### Problem description

The index of the dataframe gets converted from `DatetimeIndex` into `RangeIndex` even though the `Series` assigned to one of the columns has the exact same index as the dataframe.

We often want to check that the index is a `DatetimeIndex` when validating the data we receive as input. If this can change unexpectedly from a simple assignment operation when the `DataFrame` is empty, it becomes necessary to treat empty `DataFrame`s as special cases in our code.

This is at the very least unexpected behaviour:
- It doesn't happen if the index is not empty;
- It doesn't happen if we use `df[""A""]` (instead of `.loc`);
- It doesn't happen if there's only one column;
- It doesn't happen if we omit `.astype({""A"": float64})` (i.e. if both columns initially have dtype `object`).

#### Expected Output

```python
pandas.core.indexes.datetimes.DatetimeIndex
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-26-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.2
Cython           : None
pytest           : 5.3.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.12
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
520126362,29489,DataFrame.plot docs broken,WillAyd,closed,2019-11-08T17:01:59Z,2020-01-23T12:35:15Z,"Not showing anything useful  currently. I think the regression came between 0.24 and 0.25

0.25.0 docs:

https://pandas.pydata.org/pandas-docs/version/0.25.0/reference/api/pandas.DataFrame.plot.html

0.24.2 docs:
https://pandas.pydata.org/pandas-docs/version/0.24.2/reference/api/pandas.DataFrame.plot.html"
554130334,31244,Backport PR #31167 on branch 1.0.x (DOC: fix DataFrame.plot docs ),meeseeksmachine,closed,2020-01-23T12:35:26Z,2020-01-23T15:56:21Z,Backport PR #31167: DOC: fix DataFrame.plot docs 
553507774,31202,"IntervalIndex.map raises ""TypeError: Unexpected keyword arguments {'closed'}"" in pandas 1.0.0rc0",vadella,closed,2020-01-22T12:25:51Z,2020-01-23T14:16:00Z,"#### Code Sample, a copy-pastable example if possible

```python
pd.interval_range(1,3,2).map(str)
```

> ```
> TypeError                                 Traceback (most recent call last)
> <ipython-input-4-91424d1c664f> in <module>
> ----> 1 pd.interval_range(1,3,2).map(str)
> 
> C:\Miniconda3\envs\pandas_new\lib\site-packages\pandas\core\indexes\base.py in map(self, mapper, na_action)
>    4546             attributes[""dtype""] = self.dtype
>    4547 
> -> 4548         return Index(new_values, **attributes)
>    4549 
>    4550     def isin(self, values, level=None):
> 
> C:\Miniconda3\envs\pandas_new\lib\site-packages\pandas\core\indexes\base.py in __new__(cls, data, dtype, copy, name, tupleize_cols, **kwargs)
>     389                 if new_dtype is not None:
>     390                     return cls(
> --> 391                         new_data, dtype=new_dtype, copy=False, name=name, **kwargs
>     392                     )
>     393 
> 
> C:\Miniconda3\envs\pandas_new\lib\site-packages\pandas\core\indexes\base.py in __new__(cls, data, dtype, copy, name, tupleize_cols, **kwargs)
>     393 
>     394             if kwargs:
> --> 395                 raise TypeError(f""Unexpected keyword arguments {repr(set(kwargs))}"")
>     396             if subarr.ndim > 1:
>     397                 # GH#13601, GH#20285, GH#27125
> 
> TypeError: Unexpected keyword arguments {'closed'}
> ```

#### Problem description
When using map on an `IntervalIndex` raises a `TypeError` since testing on pandas 1.0.0rc0. In 0.25.1 it worked as expected. 

As a workaround `pd.interval_range(1,3,2).astype(object).map(str)` works, but the casting to `object` should not be necessary.


#### Expected Output

> ```
> Index(['(1, 2]', '(2, 3]'], dtype='object')
> ```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 8.1
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.0rc0
numpy            : 1.17.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 45.1.0.post20200119
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
550023862,31032,"Revert ""DEPR: is_copy arg of take (#30615)""",jorisvandenbossche,closed,2020-01-15T07:56:55Z,2020-01-23T14:41:39Z,"This reverts commit 7796be6e2c548de0be16e6adf08f5fca2ab9b4bc.

See https://github.com/pandas-dev/pandas/issues/27357#issuecomment-571639242 and https://github.com/pandas-dev/pandas/pull/30615#issuecomment-571637550"
554184562,31246,Backport PR #31232 on branch 1.0.x (REGR: Fix IntervalIndex.map when result is object dtype),meeseeksmachine,closed,2020-01-23T14:16:12Z,2020-01-23T15:56:52Z,Backport PR #31232: REGR: Fix IntervalIndex.map when result is object dtype
551405834,31102,BUG: Cannot astype from IntegerArray to BooleanArray with missing values,jorisvandenbossche,closed,2020-01-17T13:08:47Z,2020-01-23T15:26:39Z,"For the boolean -> integer array conversion, I added a special path for that. But `astype`ing integer to boolean is currently not working:

```
In [23]: a = pd.array([1, 0, pd.NA])  

In [24]: a  
Out[24]: 
<IntegerArray>
[1, 0, <NA>]
Length: 3, dtype: Int64

In [25]: a.astype(""boolean"")  
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-25-41973ed53ee3> in <module>
----> 1 a.astype(""boolean"")

~/scipy/pandas/pandas/core/arrays/integer.py in astype(self, dtype, copy)
    454             kwargs = {}
    455 
--> 456         data = self.to_numpy(dtype=dtype, **kwargs)
    457         return astype_nansafe(data, dtype, copy=False)
    458 

~/scipy/pandas/pandas/core/arrays/masked.py in to_numpy(self, dtype, copy, na_value)
    124             ):
    125                 raise ValueError(
--> 126                     f""cannot convert to '{dtype}'-dtype NumPy array ""
    127                     ""with missing values. Specify an appropriate 'na_value' ""
    128                     ""for this dtype.""

ValueError: cannot convert to 'boolean'-dtype NumPy array with missing values. Specify an appropriate 'na_value' for this dtype.

In [26]: a.astype(pd.BooleanDtype()) 
...
ValueError: cannot convert to 'boolean'-dtype NumPy array with missing values. Specify an appropriate 'na_value' for this dtype.
```

while for conversions to other nullable dtypes, this should be possible."
553840327,31224,REF/TST: collect misplaced tests,jbrockmendel,closed,2020-01-22T22:49:36Z,2020-01-23T17:20:01Z,"ATM we are mixing/matching pytest vs UnitTest idioms, which is confusing for me because I have to track down both inheritance and fixtures.  So I'm trying to clean this up, leading to some yak shaving.
"
553019969,31179,CLN: core.internals,jbrockmendel,closed,2020-01-21T17:22:22Z,2020-01-23T17:26:46Z,"some annotations, remove an unused argument, privatize where possible, avoid a runtime/circular import"
553873831,31230,"make TDI.get_value use get_loc, fix wrong-dtype NaT",jbrockmendel,closed,2020-01-23T00:30:46Z,2020-01-23T17:29:18Z,"the wrong-dtype NaT is a bug, the DTI analogue is fixed in #31163.

The refactor to make get_value is get_loc is basically what we want all of them index subclasses to do (at which point we de-duplicate and possibly get rid of get_value altogether)

cc @jreback "
553852028,31226,"CLN: unused kwarg, poorly named obj->key",jbrockmendel,closed,2020-01-22T23:21:17Z,2020-01-23T17:53:27Z,"elsehwere we use `obj = self.obj` which is either a Series or DataFrame, better to call this `key` like we do elsewhere"
548159256,30889,Series lose nullable integer dtype after call to .diff(),mwaskom,closed,2020-01-10T16:01:40Z,2020-01-23T19:00:09Z,"c.f. #28285

#### Code Sample, a copy-pastable example if possible

```python
pd.Series([1, 2, 3], dtype=""Int16"").diff()
```
```
0    NaN
1      1
2      1
dtype: object
```

#### Problem description

Output dtype becomes `object`.

#### Expected Output

Output dtype remains `Int16Dtype`.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-42-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.2
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 44.0.0.post20200102
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
xarray           : 0.14.1
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
548980286,30967,DataFrame.diff fails with nullable integers,MarcoGorelli,closed,2020-01-13T14:50:24Z,2020-01-23T19:00:10Z,"Related to #30889 (I've opened a separate issue as the error is different)

#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame({""a"": [1, 2, 3]}, dtype=""Int16"")
df.diff()
```
#### Problem description

This returns
```
IndexError: list assignment index out of range
```

#### Expected Output
Something like
```
     a
0  pd.NA
1  1
2  1
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 46c2864c34eee0cd94c8842353331e293b0f2004
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-74-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.25.0+1754.g46c2864c3
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 42.0.2.post20191201
Cython           : 0.29.14
pytest           : 5.3.2
hypothesis       : 4.55.4
sphinx           : 2.3.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.6
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : 1.3.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.3.2
s3fs             : 0.4.0
scipy            : 1.4.0
sqlalchemy       : 1.3.11
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.6
numba            : 0.46.0

</details>
"
553161747,31187,BUG: IntegerArray.astype(boolean),TomAugspurger,closed,2020-01-21T22:01:21Z,2020-01-23T19:02:02Z,Closes https://github.com/pandas-dev/pandas/issues/31102
549895528,31025,ENH: Handle extension arrays in algorithms.diff,TomAugspurger,closed,2020-01-15T00:26:03Z,2020-01-23T19:29:21Z,"Closes https://github.com/pandas-dev/pandas/issues/30889
Closes https://github.com/pandas-dev/pandas/issues/30967

This still needs a few things

1. Additional tests. It's hard to write thorough tests in `extensions/base`, since we don't know what the dtype will be after the `.diff()` (subtracting doesn't necessarily return the same dtype).
2. Decide what we want for BooleanArray.diff. Should that return an IntegerArray probably?
3. (edit) Verify that there's no perf regression for datetime, datetimetz, timedelta.

Are we comfortable doing this for 1.0.0?"
554345140,31254,Backport PR #31187 on branch 1.0.x (BUG: IntegerArray.astype(boolean)),meeseeksmachine,closed,2020-01-23T19:02:18Z,2020-01-23T19:32:25Z,Backport PR #31187: BUG: IntegerArray.astype(boolean)
554345624,31255,Backport PR #31025 on branch 1.0.x (ENH: Handle extension arrays in algorithms.diff),meeseeksmachine,closed,2020-01-23T19:03:08Z,2020-01-23T19:33:20Z,Backport PR #31025: ENH: Handle extension arrays in algorithms.diff
551857767,31131,ENH: Implement CSV reading for BooleanArray,koizumihiroo,closed,2020-01-19T03:01:14Z,2020-01-23T20:56:01Z,"#### Code Sample, a copy-pastable example if possible

In 1.0.0 rc0, `pd.read_csv` returns this error when reading NA with specifying dtype of 'boolean'

```python
# import pandas as pd
# import io

>>> txt = """"""
X1,X2,X3
1,a,True
2,b,False
NA,NA,NA
""""""
>>> df1 = pd.read_csv(io.StringIO(txt), dtype={'X3': 'boolean'}) # `pd.BooleanDtype()` also fails
raceback (most recent call last):
  File ""pandas/_libs/parsers.pyx"", line 1191, in pandas._libs.parsers.TextReader._convert_with_dtype
  File ""/usr/local/lib/python3.7/site-packages/pandas/core/arrays/base.py"", line 232, in _from_sequence_of_strings
    raise AbstractMethodError(cls)
pandas.errors.AbstractMethodError: This method must be defined in the concrete class type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 454, in _read
    data = parser.read(nrows)
  File ""/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 1133, in read
    ret = self._engine.read(nrows)
  File ""/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 2037, in read
    data = self._reader.read(nrows)
  File ""pandas/_libs/parsers.pyx"", line 859, in pandas._libs.parsers.TextReader.read
  File ""pandas/_libs/parsers.pyx"", line 874, in pandas._libs.parsers.TextReader._read_low_memory
  File ""pandas/_libs/parsers.pyx"", line 951, in pandas._libs.parsers.TextReader._read_rows
  File ""pandas/_libs/parsers.pyx"", line 1083, in pandas._libs.parsers.TextReader._convert_column_data
  File ""pandas/_libs/parsers.pyx"", line 1114, in pandas._libs.parsers.TextReader._convert_tokens
  File ""pandas/_libs/parsers.pyx"", line 1194, in pandas._libs.parsers.TextReader._convert_with_dtype
NotImplementedError: Extension Array: <class 'pandas.core.arrays.boolean.BooleanArray'> must implement _from_sequence_of_strings in order to be used in parser methods
```

while ""Int64"" and ""string"" with NA can be correctly recognized.

```python
>>> df2 = pd.read_csv(io.StringIO(txt), dtype={'X1': 'Int64', 'X2': 'string'})
>>> df2
     X1    X2     X3
0     1     a   True
1     2     b  False
2  <NA>  <NA>    NaN
>>> df2.dtypes
X1     Int64
X2    string
X3    object
dtype: object
```

#### Problem description

NA literal for boolean is not parsed by `pd.read_csv`

#### Expected Output

```python
df3 = pd.read_csv(io.StringIO(txt), dtype={'X3': 'boolean'})
>>> df3
    X1   X2     X3
0  1.0    a   True
1  2.0    b  False
2  NaN  NaN   <NA>
>>> df3.dtypes
X1    float64
X2     object
X3    boolean
dtype: object

# and
df4 = pd.read_csv(io.StringIO(txt), dtype={'X1': 'Int64', 'X2': 'string', 'X3': 'boolean'})
>>> df4
     X1    X2     X3
0     1     a   True
1     2     b  False
2  <NA>  <NA>   <NA>
>>> df4.dtypes
X1     Int64
X2    string
X3   boolean
dtype: object
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.9.184-linuxkit
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.0rc0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 44.0.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
553619879,31211,BUG: Fixed upcast dtype for datetime64 in merge,TomAugspurger,closed,2020-01-22T15:34:12Z,2020-01-23T20:57:16Z,Closes https://github.com/pandas-dev/pandas/issues/31208
554402210,31261,Backport PR #31159 on branch 1.0.x (ENH: Implement _from_sequence_of_strings for BooleanArray),meeseeksmachine,closed,2020-01-23T20:56:14Z,2020-01-23T21:31:20Z,Backport PR #31159: ENH: Implement _from_sequence_of_strings for BooleanArray
552452644,31159,ENH: Implement _from_sequence_of_strings for BooleanArray,dsaxton,closed,2020-01-20T18:30:01Z,2020-01-23T21:56:21Z,"- [x] closes #31131
- [x] tests added / passed
- [x] passes `black pandas`
- [x] whatsnew entry"
552871702,31175,test_timestamp:test_class_ops_pytz can fail due to bad luck of timing of calls,Dr-Irv,closed,2020-01-21T13:18:43Z,2020-01-23T23:10:17Z,"#### Code Sample, a copy-pastable example if possible

I had a test fail in CI and the output looks like this:

```
______________________ TestTimestamp.test_class_ops_pytz _______________________
[gw1] linux -- Python 3.6.6 /home/travis/miniconda3/envs/pandas-dev/bin/python

self = <pandas.tests.scalar.timestamp.test_timestamp.TestTimestamp object at 0x7fb560287320>

    def test_class_ops_pytz(self):
        def compare(x, y):
            assert int(Timestamp(x).value / 1e9) == int(Timestamp(y).value / 1e9)
    
        compare(Timestamp.now(), datetime.now())
        compare(Timestamp.now(""UTC""), datetime.now(timezone(""UTC"")))
        compare(Timestamp.utcnow(), datetime.utcnow())
>       compare(Timestamp.today(), datetime.today())

pandas/tests/scalar/timestamp/test_timestamp.py:759: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = Timestamp('2020-01-21 03:19:15.998086')
y = datetime.datetime(2020, 1, 21, 3, 19, 16, 5328)

    def compare(x, y):
>       assert int(Timestamp(x).value / 1e9) == int(Timestamp(y).value / 1e9)
E       AssertionError: assert 1579576755 == 1579576756
E        +  where 1579576755 = int((1579576755998086000 / 1000000000.0))
E        +    where 1579576755998086000 = Timestamp('2020-01-21 03:19:15.998086').value
E        +      where Timestamp('2020-01-21 03:19:15.998086') = Timestamp(Timestamp('2020-01-21 03:19:15.998086'))
E        +  and   1579576756 = int((1579576756005328000 / 1000000000.0))
E        +    where 1579576756005328000 = Timestamp('2020-01-21 03:19:16.005328').value
E        +      where Timestamp('2020-01-21 03:19:16.005328') = Timestamp(datetime.datetime(2020, 1, 21, 3, 19, 16, 5328))
```



#### Problem description

So due to ""bad luck"", the call to `Timestamp.today()` was just before the next second that hit when `datetime.now()` was called, making the comparison fail.  I think the comparison should be:

```
int((Timestamp(x).value - Timestamp(y).value)/1.0e9) == 0
```
which would handle this round off error better.

#### Expected Output

No failure

#### Output of ``pd.show_versions()``

<details>

>>> pd.show_versions()                                                          
C:\Anaconda3\envs\pandas-dev\lib\site-packages\fastparquet\dataframe.py:5: Futur
eWarning: pandas.core.index is deprecated and will be removed in a future versio
n.  The public classes are available in the top-level namespace.                
  from pandas.core.index import CategoricalIndex, RangeIndex, Index, MultiIndex 
                                                                                
INSTALLED VERSIONS                                                              
------------------                                                              
commit           : 8e4dfff71a0547c821e67477b65c7b1930df588e                     
python           : 3.7.6.final.0                                                
python-bits      : 64                                                           
OS               : Windows                                                      
OS-release       : 10                                                           
machine          : AMD64                                                        
processor        : Intel64 Family 6 Model 158 Stepping 13, GenuineIntel         
byteorder        : little                                                       
LC_ALL           : None                                                         
LANG             : None                                                         
LOCALE           : None.None                                                    
                                                                                
pandas           : 1.0.0rc0+156.g8e4dfff71                                      
numpy            : 1.17.3                                                       
pytz             : 2019.3                                                       
dateutil         : 2.8.1                                                        
pip              : 19.3.1                                                       
setuptools       : 42.0.2.post20191201                                          
Cython           : 0.29.14                                                      
pytest           : 5.3.2                                                        
hypothesis       : 4.57.1                                                       
sphinx           : 2.3.1                                                        
blosc            : None                                                         
feather          : None                                                         
xlsxwriter       : 1.2.7                                                        
lxml.etree       : 4.4.2                                                        
html5lib         : 1.0.1                                                        
pymysql          : None                                                         
psycopg2         : None                                                         
jinja2           : 2.10.3                                                       
IPython          : 7.10.1                                                       
pandas_datareader: None                                                         
bs4              : 4.8.2                                                        
bottleneck       : 1.3.1                                                        
fastparquet      : 0.3.2                                                        
gcsfs            : None                                                         
lxml.etree       : 4.4.2                                                        
matplotlib       : 3.1.2                                                        
numexpr          : 2.7.0                                                        
odfpy            : None                                                         
openpyxl         : 3.0.1                                                        
pandas_gbq       : None                                                         
pyarrow          : 0.15.1                                                       
pytables         : None                                                         
pytest           : 5.3.2                                                        
pyxlsb           : None                                                         
s3fs             : 0.4.0                                                        
scipy            : 1.3.1                                                        
sqlalchemy       : 1.3.12                                                       
tables           : 3.6.1                                                        
tabulate         : 0.8.6                                                        
xarray           : 0.14.1                                                       
xlrd             : 1.2.0                                                        
xlwt             : 1.3.0                                                        
xlsxwriter       : 1.2.7                                                        
numba            : 0.46.0                                                       
>>>                                                                             
</details>
"
552511270,31163,"BUG: corner cases in DTI.get_value, Float64Index.get_value",jbrockmendel,closed,2020-01-20T21:14:09Z,2020-01-23T23:12:24Z,Series lookups are affected for the Float64Index case.
554377948,31258,REF: avoid calling engine for EA values,jbrockmendel,closed,2020-01-23T20:06:07Z,2020-01-23T23:12:57Z,
554447376,31264,Cannot add pandas_profiling module into SQL Server,huyanh1511,closed,2020-01-23T22:36:35Z,2020-01-23T23:13:04Z,"#### Code Sample, a copy-pastable example if possible

```python
import sqlmlutils
connection = sqlmlutils.ConnectionInfo(server=""localhost"", database=""SQLAI"")
sqlmlutils.SQLPackageManager(connection).install(""pandas-profiling"")
```
#### Problem description

I was trying to install some module (i.e. pandas_profiling) in SQL Server so I can run the script directly from there. I was utilizing a Microsoft tool called 'sqlmlutils' to achieve this, but it returned an error specifically when I tried to install pandas_profiling (in fact another library called 'text-tools' was successfully installed.

#### Expected Output (this is the output I got by running the scripts)

Traceback (most recent call last):

  File ""<ipython-input-2-e8db1d445900>"", line 1, in <module>
    runfile('C:/Users/BusinessUser/Desktop/abc.py', wdir='C:/Users/BusinessUser/Desktop')

  File ""C:\Users\BusinessUser\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""C:\Users\BusinessUser\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/BusinessUser/Desktop/abc.py"", line 3, in <module>
    sqlmlutils.SQLPackageManager(connection).install(""pandas_profiling"")

  File ""C:\Users\BusinessUser\Anaconda3\lib\site-packages\sqlmlutils\packagemanagement\sqlpackagemanager.py"", line 74, in install
    self._install_from_pypi(package, upgrade, version, install_dependencies, scope, out_file=out_file)

  File ""C:\Users\BusinessUser\Anaconda3\lib\site-packages\sqlmlutils\packagemanagement\sqlpackagemanager.py"", line 151, in _install_from_pypi
    self._install_from_file(target_package_file, scope, upgrade, out_file=out_file)

  File ""C:\Users\BusinessUser\Anaconda3\lib\site-packages\sqlmlutils\packagemanagement\sqlpackagemanager.py"", line 171, in _install_from_file
    required_installs = resolver.get_required_installs(target_package_requirements)

  File ""C:\Users\BusinessUser\Anaconda3\lib\site-packages\sqlmlutils\packagemanagement\dependencyresolver.py"", line 34, in get_required_installs
    self._server_packages, requirement.name, spec)

  File ""C:\Users\BusinessUser\Anaconda3\lib\site-packages\sqlmlutils\packagemanagement\dependencyresolver.py"", line 60, in _check_if_installed_package_meets_spec
    return getattr(operator, operator_map[op_str])(LooseVersion(installed_version), LooseVersion(req_version))

  File ""C:\Users\BusinessUser\Anaconda3\lib\distutils\version.py"", line 46, in __eq__
    c = self._cmp(other)

  File ""C:\Users\BusinessUser\Anaconda3\lib\distutils\version.py"", line 337, in _cmp
    if self.version < other.version:

TypeError: '<' not supported between instances of 'int' and 'str'

#### Output of ``pd.show_versions()``

<details>

Not sure what this mean but it was saying 'The version installed on the server is 0.23.4'

</details>
"
554460087,31265,Backport PR #31249 on branch 1.0.x (TST: Fix timestamp comparison test),meeseeksmachine,closed,2020-01-23T23:10:32Z,2020-01-23T23:52:52Z,Backport PR #31249: TST: Fix timestamp comparison test
554372890,31257,BUG: DatetimeIndex.get_loc/get_value raise InvalidIndexError,jbrockmendel,closed,2020-01-23T19:57:39Z,2020-01-24T00:31:20Z,almost done with the get_value/get_loc methods
552578752,31169,"REF: require scalar in IntervalIndex.get_loc, get_value",jbrockmendel,closed,2020-01-21T01:38:58Z,2020-01-24T00:54:01Z,"cc @jschendel the casting of tuple->Interval on L819-820 seems sketchy, can you confirm if thats appropriate?

I'm also curious about the restriction on slice steps in _convert_slice_indexer, would be nice to have a comment there about why that is.

xref #31117."
553948291,31241,TST: add tzaware case to indices fixture,jbrockmendel,closed,2020-01-23T05:36:36Z,2020-01-24T01:07:21Z,Much easier alternative to #31236.
418445886,25596,pandas.DataFrame.sum() returns wrong type for subclassed pandas DataFrame,MaximilianHahn,closed,2019-03-07T18:12:01Z,2020-01-24T01:07:44Z,"#### Code Sample, a copy-pastable example if possible

```python

# the following code is obtained from the documentation
# https://pandas.pydata.org/pandas-docs/stable/development/extending.html

import pandas as pd

class SubclassedSeries(pd.Series):
    @property
    def _constructor(self):
        return SubclassedSeries
    @property
    def _constructor_expanddim(self):
        return SubclassedDataFrame


class SubclassedDataFrame(pd.DataFrame):
    @property
    def _constructor(self):
        return SubclassedDataFrame
    @property
    def _constructor_sliced(self):
        return SubclassedSeries

# create a class instance as in the example of the documentation

df = SubclassedDataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})
>>> df
   A  B  C
0  1  4  7
1  2  5  8
2  3  6  9

# this works just fine

>>> type(df)
<class '__main__.SubclassedDataFrame'>

# slicing also works fine

>>> sliced2 = df['A']
>>> sliced2
0    1
1    2
2    3
Name: A, dtype: int64

>>> type(sliced2)
<class '__main__.SubclassedSeries'>

# however, the sum operation returns a pandas.Series, not SubclassedSeries

>>> sliced3 = df.sum()
>>> sliced3
0    1
1    2
2    3
Name: A, dtype: int64

>>> type(sliced3)
<class 'pandas.core.series.Series'>

```
#### Problem description

In our project, we extend pandas as described in the documentation and implement our own kind of DataFrame and Series, similar to the geopandas project (if you apply sum on their DataFrame, the same problem appears). If you want to use _reduce operations like sum, it is important that the correct SubclassedSeries is returned. Otherwise, inheritance from pandas.DataFrames is not possible.

#### Expected Output
```python
>>> type(sliced3)
<class '__main__.SubclassedSeries'>

```

I think I can provide a possible fix of this problem: The relevant code is contained in core/frame.py just before the return statement of the _reduce function:

```python

# this is the code in core/frame.py:
def _reduce(...):
        # .... left out
        if constructor is not None:
            result = Series(result, index=labels)
        return result

# I suggest the following change:
def _reduce(...):
        # .... left out
        if constructor is None:
            result = Series(result, index=labels)
        else:
            result = constructor(result, index=labels)
        # alternative (since constructor will create a SubclassedDataFrame):
            result = self._constructor_sliced(result, index=labels)
        return result

```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.2.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.1
pytest: None
pip: 19.0.3
setuptools: 40.8.0
Cython: None
numpy: 1.16.2
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
550197014,31043,BUG: nonexistent Timestamp pre-summer/winter DST change with dateutil timezone,AlexKirko,closed,2020-01-15T13:48:10Z,2020-01-24T03:33:00Z,"#### Code Sample, a copy-pastable example if possible

This is fine:
```
>>> pd.__version__
'0.26.0.dev0+1790.gdd94e0db9'
>>> epoch =  1552211999999999871
>>> t = pd.Timestamp(epoch, tz='dateutil/US/Pacific')
>>> t
Timestamp('2019-03-10 01:59:59.999999871-0800', tz='dateutil/US/Pacific')
>>> t.value
1552211999999999871
>>> pd.Timestamp(t)
Timestamp('2019-03-10 01:59:59.999999871-0800', tz='dateutil/US/Pacific')
>>> pd.Timestamp(t).value
1552211999999999871
```
This is also fine:
```
>>> epoch =  1552212000000000000
>>> t = pd.Timestamp(epoch, tz='dateutil/US/Pacific')
>>> t
Timestamp('2019-03-10 03:00:00-0700', tz='dateutil/US/Pacific')
>>>
>>> t.value
1552212000000000000
>>> pd.Timestamp(t)
Timestamp('2019-03-10 03:00:00-0700', tz='dateutil/US/Pacific')
>>> pd.Timestamp(t).value
1552212000000000000
```
Meanwhile, this breaks representation and gets us nonexistent times:
```
>>> epoch =  1552211999999999872
>>> t = pd.Timestamp(epoch, tz='dateutil/US/Pacific')
>>> t
Timestamp('2019-03-10 01:59:59.999999872-0700', tz='dateutil/US/Pacific')
>>> t.value
1552211999999999872
>>> pd.Timestamp(t)
Timestamp('2019-03-10 01:59:59.999999872-0800', tz='dateutil/US/Pacific')
>>> pd.Timestamp(t).value
1552208399999999872
```
And right on the cusp, the value breaks too:
```
>>> epoch =  1552211999999999999
>>> t = pd.Timestamp(epoch, tz='dateutil/US/Pacific')
>>> t
Timestamp('2019-03-10 01:59:59.999999999-0700', tz='dateutil/US/Pacific')
>>> t.value
1552211999999999999
>>> pd.Timestamp(t)
Timestamp('2019-03-10 01:59:59.999999999-0800', tz='dateutil/US/Pacific')
>>> pd.Timestamp(t).value
1552208399999999999
```

#### Problem description

When we use `dateutil` timezones and try to create a Timestamp that is right on the cusp of the change from winter to summer time, we can get nonexistent times (the clock is supposed to jump from 2 A.M. to 3 A.M. and yet we get 2:59:59).

I've investigated this, and it appears that at 128 nanoseconds before the clock jump, DST offset and utcoffset in `dateutil` change, so we end up in a situation when the offsets are what they are supposed to be after the jump, but the time hasn't jumped yet, so the constructor returns a nonexistent time. Calling the constructor again moves the clock 1 hour back.

This can be checked out with:
```
>>> epoch =  1552211999999999872
>>> t = pd.Timestamp(epoch, tz='dateutil/US/Pacific')
>>> t.tz.dst(t)
datetime.timedelta(seconds=3600)
```

My assumption is that when we need to determine UTC offset, rounding happens at some point, and we round to `epoch=1552212000000000000`, get offset, and then use it on time pre-clock jump.

I'd like to try to fix this one.

#### Expected Output
```
>>> epoch =  1552211999999999872
>>> t = pd.Timestamp(epoch, tz='dateutil/US/Pacific')
>>> t
Timestamp('2019-03-10 01:59:59.999999872-0800', tz='dateutil/US/Pacific')
>>> t.value
1552211999999999872
>>> pd.Timestamp(t)
Timestamp('2019-03-10 01:59:59.999999872-0800', tz='dateutil/US/Pacific')
>>> pd.Timestamp(t).value
1552208399999999872
```

#### Notes

This was thought to be part of #24329 but turned to be a separate bug as I worked on closing that issue in PR #30995.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : dd94e0db9556f35a3ea91ce85714c0b1e151a770
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : ru_RU.UTF-8
LOCALE           : None.None

pandas           : 0.26.0.dev0+1790.gdd94e0db9
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 44.0.0.post20200106
Cython           : 0.29.14
pytest           : 5.3.2
hypothesis       : 5.1.5
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.3.2
s3fs             : 0.4.0
scipy            : 1.3.1
sqlalchemy       : 1.3.12
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.47.0

</details>
"
549076849,30975,AssertionError from Series.append(DataFrame) with 1.0.0rc0,TomAugspurger,closed,2020-01-13T17:29:36Z,2020-01-24T03:37:33Z,"#### Code Sample, a copy-pastable example if possible

The behavior of the following changed with 1.0.0rc0

```python
>>> import pandas as pd
>>> df = pd.DataFrame({""A"": [1,2], ""B"": [3, 4]})
>>> df.A.append(df)
```

On 0.25.3, we have

```python
     0    A    B
0  1.0  NaN  NaN
1  2.0  NaN  NaN
0  NaN  1.0  3.0
1  NaN  2.0  4.0
```

On 1.0.0rc0, we have

```pytb
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-9-fde662a5b7c4> in <module>
----> 1 df.A.append(df)

~/sandbox/pandas/pandas/core/series.py in append(self, to_append, ignore_index, verify_integrity)
   2541         return self._ensure_type(
   2542             concat(
-> 2543                 to_concat, ignore_index=ignore_index, verify_integrity=verify_integrity
   2544             )
   2545         )

~/sandbox/pandas/pandas/core/base.py in _ensure_type(self, obj)
     93         Used by type checkers.
     94         """"""
---> 95         assert isinstance(obj, type(self)), type(obj)
     96         return obj
     97

AssertionError: <class 'pandas.core.frame.DataFrame'>
```

#### Problem description

I don't think the behavior of 0.25.3 is necessarily correct. I would expect either a TypeError from passing a DataFrame to Series.append, or something like

```python
In [17]: r
Out[17]:
   A    B
0  1  3.0
1  2  4.0
0  1  NaN
1  2  NaN
```

where the name of the Series is aligned with the DataFrame. We clearly document that the elements passed to append should be Series, so IMO we should raise."
552125996,31144,Check for pyarrow not feather before pyarrow tests,rebecca-palmer,closed,2020-01-20T07:51:15Z,2020-01-24T03:54:17Z,"read_feather/to_feather now use pyarrow.feather, not top-level (feather-format) feather, but some of their tests were still looking for top-level feather.

Two of them that deliberately cause a file not found error were also looking for the wrong form of error message.  (Probably nobody noticed because the above was skipping them.)"
552129084,31145,Use https for links where available,rebecca-palmer,closed,2020-01-20T07:57:35Z,2020-01-24T03:55:13Z,"As some of our links are suggestions to install add-on software, insecure links potentially allow an attacker to replace this with their malware.

During the process of writing this, I also found and fixed some broken or semi-broken (e.g. redirected to the top level instead of the page we want) links.

The following (already) broken links remain:
http://nipunbatra.github.io/2015/06/timeseries/ + others from this site
http://collaboration.cmc.ec.gc.ca/science/rpn/biblio/ddj/Website/articles/CUJ/1992/9210/ross/ross.htm
http://mysite.verizon.net/aesir_research/date/jdalg2.htm"
539639698,30320,Read_json overflow error when json contains big number strings ,boing102,closed,2019-12-18T12:03:13Z,2020-01-24T04:25:17Z,"#### Code Sample, a copy-pastable example if possible

```python
import json
import pandas as pd

test_data = [{""col"": ""31900441201190696999""}, {""col"": ""Text""}]
test_json = json.dumps(test_data)
pd.read_json(test_json)

```
#### Problem description

The current behaviour doesn't return a dateframe for a valid JSON. Note when the number is smaller, it works fine. It also works when only big numbers are present. It would be cool to have it work with big numbers as it works for small numbers.

#### Expected Output
A dataframe with a number and string
```
       col
0  3.190044e+19
1     Text
```

#### Output of ``pd.read_json()``

<details>
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py"", line 592, in read_json
    result = json_reader.read()
  File "".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py"", line 717, in read
    obj = self._get_object_parser(self.data)
  File "".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py"", line 739, in _get_object_parser
    obj = FrameParser(json, **kwargs).parse()
  File "".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py"", line 855, in parse
    self._try_convert_types()
  File "".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py"", line 1151, in _try_convert_types
    lambda col, c: self._try_convert_data(col, c, convert_dates=False)
  File "".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py"", line 1131, in _process_converter
    new_data, result = f(col, c)
  File "".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py"", line 1151, in <lambda>
    lambda col, c: self._try_convert_data(col, c, convert_dates=False)
  File "".../.venv/lib/python3.6/site-packages/pandas/io/json/_json.py"", line 927, in _try_convert_data
    new_data = data.astype(""int64"")
  File "".../.venv/lib/python3.6/site-packages/pandas/core/generic.py"", line 5882, in astype
    dtype=dtype, copy=copy, errors=errors, **kwargs
  File "".../.venv/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 581, in astype
    return self.apply(""astype"", dtype=dtype, **kwargs)
  File "".../.venv/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 438, in apply
    applied = getattr(b, f)(**kwargs)
  File "".../.venv/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 559, in astype
    return self._astype(dtype, copy=copy, errors=errors, values=values, **kwargs)
  File "".../.venv/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 643, in _astype
    values = astype_nansafe(vals1d, dtype, copy=True, **kwargs)
  File "".../.venv/lib/python3.6/site-packages/pandas/core/dtypes/cast.py"", line 707, in astype_nansafe
    return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape)
  File ""pandas/_libs/lib.pyx"", line 547, in pandas._libs.lib.astype_intsafe
OverflowError: Python int too large to convert to C long

</details>
"
539871432,30329,BUG: Integer Overflow in read_json with big number in string,rohitkg98,closed,2019-12-18T19:24:42Z,2020-01-24T04:25:38Z,"- [x] closes #30320
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
552567985,31168,REF: simplify index.pyx,jbrockmendel,closed,2020-01-21T00:51:02Z,2020-01-24T04:27:23Z,simplification of is_definitely_invalid_key is made possible by the recent change to make EA non-hashable.
547607266,30854,CLN: remove pydt_to_i8,jbrockmendel,closed,2020-01-09T17:05:39Z,2020-01-24T04:32:18Z,"It's only used in one non-test place, and all supported python versions now support datetime.timestamp, so we can use that directly.  Its a little bit less performant, but not used often."
554506175,31266,REF: do alignment before _combine_series_frame,jbrockmendel,closed,2020-01-24T01:55:49Z,2020-01-24T04:32:56Z,This will allow us to de-duplicate _construct_result calls (this does so for _comp_method_FRAME) and ultimately move all of the alignment into _align_method_FRAME
554528327,31267,Backport PR #31036 on branch 1.0.x (BUG: AssertionError on Series.append(DataFrame) fix  #30975 ),meeseeksmachine,closed,2020-01-24T03:38:02Z,2020-01-24T04:34:47Z,Backport PR #31036: BUG: AssertionError on Series.append(DataFrame) fix  #30975 
543287023,30531,Performance of maybe_box_datetimelike #30520,ivan-vasilev,closed,2019-12-28T22:18:54Z,2020-01-24T04:38:08Z,"- [x] closes #30520
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] `maybe_box_datetimelike` avoids unnecessary conversion of `value` to `tslibs.Timestamp` or `tslibs.Timedelta` if `value` is already an instance of `tslibs.Timestamp` or `tslibs.Timedelta`.
"
543018426,30520,Performance issue with pandas/core/common.py -> maybe_box_datetimelike,ivan-vasilev,closed,2019-12-27T23:24:01Z,2020-01-24T04:38:29Z,"#### Code Sample, a copy-pastable example if possible

```python
# The existing implementation is:
def maybe_box_datetimelike(value):
    # turn a datetime like into a Timestamp/timedelta as needed

    if isinstance(value, (np.datetime64, datetime)):
        value = tslibs.Timestamp(value)
    elif isinstance(value, (np.timedelta64, timedelta)):
        value = tslibs.Timedelta(value)

    return value

# Proposed improvement:
def maybe_box_datetimelike(value):
    # turn a datetime like into a Timestamp/timedelta as needed

    if isinstance(value, (np.datetime64, datetime)) and not isinstance(value, tslibs.Timestamp):
        value = tslibs.Timestamp(value)
    elif isinstance(value, (np.timedelta64, timedelta)):
        value = tslibs.Timedelta(value)

    return value
```
#### Problem description

This function determines whether `value` is of type `(np.datetime64, datetime)` and if so, converts it into `tslibs.Timestamp`. However, the class `tslibs.Timestamp` is already a subclass of `datetime`. Therefore, even if the object `value` is already of type `tslibs.Timestamp`, it will be needlessly converted one more time. This issue has large performance, when working with large dataframes, which contain datet time objects. This issue could be fixed by changing the condition
`if isinstance(value, (np.datetime64, datetime)):`
to:
`if isinstance(value, (np.datetime64, datetime)) and not isinstance(value, tslibs.Timestamp):`"
553031642,31180,BUG/CLN: use more-correct isinstance check,jbrockmendel,closed,2020-01-21T17:45:21Z,2020-01-24T04:39:30Z,"this is only kind of a bug, since the isinstance check still returns True on the `type(obj)`"
553273512,31193,REF: combine IndexEngine test files,jbrockmendel,closed,2020-01-22T02:40:46Z,2020-01-24T04:42:30Z,indexing tests are pretty scattered.  One step at a time...
553078837,31181,TYP: annotations in indexes,jbrockmendel,closed,2020-01-21T19:24:52Z,2020-01-24T04:42:47Z,
554564283,31272,Dropped columns still available for filtering,Dobiasd,closed,2020-01-24T06:17:57Z,2020-01-24T06:21:27Z,"#### Code Sample
```python3
import pandas as pd
df = pd.DataFrame(data={'foo': [0, 1], 'bar': [""a"", ""b""]})
df.drop(columns=[""bar""])[df[""bar""] == ""a""]
```

#### Problem description
The code sample above should result in an error like the following:

```
KeyError: 'bar'
```

However, it does not but instead works without error, as if the column ""bar"" would exist.

#### Output of ``pd.show_versions()``
```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.13.0-38-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2018.5
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.6.0
Cython           : 0.29.6
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.5 (dt dec pq3 ext lo64)
jinja2           : 2.10
IPython          : 7.5.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.0.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.2.10
tables           : None
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
```
"
359061580,22661,Bug: pd.read_csv does not read lines with data containing leading quotes but not matching close quotes,dhruvsakalley,closed,2018-09-11T14:00:56Z,2020-01-24T07:02:52Z,"#### Code Sample, a copy-pastable example if possible
pd.read_csv(""pandas_bug.tsv"", sep=""\t"", index_col=None, header=None, encoding='utf-8', skip_blank_lines=True, quotechar='""')
#### Problem description

The pandas_bug.tsv looks like this
3	abc	5.6
4	""abc""	4.3
5	""error	3.3

This code line results in an error
ParserError: Error tokenizing data. C error: EOF inside string starting at line 2

for another case in a larger file, this error does not occur and pandas silently skips some lines, however, when converting the same tsv file to json via file io, the pandas read_json function handles this gracefully and adds an escape character in front of the string. e.g. \""error

#### Expected Output
	0	1	2
0	3	abc	5.6
1	4	abc	4.3
2	5	\""error	3.3

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.5.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.22.0
pytest: 3.7.0
pip: 10.0.1
setuptools: 36.2.7
Cython: 0.28.2
numpy: 1.14.5
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 6.4.0
sphinx: 1.7.4
patsy: 0.5.0
dateutil: 2.6.1
pytz: 2017.3
blosc: None
bottleneck: 1.2.1
tables: 3.4.3
numexpr: 2.6.5
feather: None
matplotlib: 2.2.2
openpyxl: 2.5.3
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.4
lxml: 4.2.4
bs4: 4.6.0
html5lib: 0.9999999
sqlalchemy: 1.2.7
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
385120302,23965,DataFrame: damage data integrity in conversion and iterations,ZHANGGU,closed,2018-11-28T06:27:41Z,2020-01-24T07:04:55Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
import pandas as pd
d = {'col1': [1, 2, 3], 'col2': [4, 5+1j, 6]}
dftemp = pd.DataFrame(d)
dftemp['col2']
dstemp = pd.Series({'col2': [4, 5+1j, 6]})
```
#### Problem description

dftemp['col2'] shows all vaues as below are complex, but there was only one complex value originally

0    (4+0j)
1    (5+1j)
2    (6+0j)

dstemp shows consitent values as those in origin as below:

col2    [4, (5+1j), 6]

NB. once iterating the rows, the values in 'col1' is also converted to complex!

#### Expected Output
0    (4)
1    (5+1j)
2    (6)
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.2.final.0
python-bits: 32
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.23.4
pytest: None
pip: 18.0
setuptools: 40.5.0
Cython: None
numpy: 1.13.1
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: 1.2.9
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None"
386291364,24022,type integrity failure astype vs read_csv,MGDMedia,closed,2018-11-30T18:00:47Z,2020-01-24T07:11:20Z,"I'm having a problem with pandas where dataframes loaded from csv seem to revert to their origional dtypes when I set their indexes after processing. Look at this:

```
#Both these dicts contain columns and column types.  They are the same
# except for two records - dtypesA lists columns ('ga:date', 'ga:minute')
# as dtype('O') and dtypesB lists columns ('ga:date', 'ga:minute') as 
# dtype('int64')
dtypesA = {'ga:source': dtype('O'), 'ga:deviceCategory': dtype('O'),
    'ga:country': dtype('O'), 'ga:metro': dtype('O'), 'ga:date': dtype('O'), 
    'ga:minute': dtype('O'), 'ga:channelGrouping': dtype('O'), 'ga:sessions': 
    dtype('int64'), 'ga:transactions': dtype('int64')}

dtypesB = {'ga:source': dtype('O'), 'ga:deviceCategory': dtype('O'),
    'ga:country': dtype('O'), 'ga:metro': dtype('O'), 'ga:date': dtype('int64'), 
    'ga:minute': dtype('int64'), 'ga:channelGrouping': dtype('O'), 'ga:sessions': 
    dtype('int64'), 'ga:transactions': dtype('int64')}

dfA = pd.read_csv(""my.csv"", dtype=dtypesA)
dfB = pd.read_csv(""my.csv"", dtype=dtypesB)

#So the only difference between dfA and dfB now is that
# dfA has columns ('ga:date', 'ga:minute') typed as strings
# and dfB has columns ('ga:date', 'ga:minute') typed as
# ints.

dfB = dfB.astype(dtypesA)
dfB.dtypes
>ga:source              object
>ga:deviceCategory      object
>ga:country             object
>ga:metro               object
>ga:date                object
>ga:minute              object
>ga:channelGrouping     object
>ga:sessions             int64
>ga:transactions         int64
>dtype: object

dfA.dtypes
>ga:source              object
>ga:deviceCategory      object
>ga:country             object
>ga:metro               object
>ga:date                object
>ga:minute              object
>ga:channelGrouping     object
>ga:sessions             int64
>ga:transactions         int64
>dtype: object

#now both df's have the same dtypes.

indexCols = ['ga:source', 'ga:deviceCategory', 'ga:country', 'ga:metro', 
    'ga:date', 'ga:minute', 'ga:channelGrouping']

dfA.set_index(indexCols, inplace=True)
dfB.set_index(indexCols, inplace=True)

dfA.reset_index().dtypes
>ga:source              object
>ga:deviceCategory      object
>ga:country             object
>ga:metro               object
>ga:date                object
>ga:minute              object
>ga:channelGrouping     object
>ga:sessions             int64
>ga:transactions         int64
>dtype: object

dfB.reset_index().dtypes
>ga:source              object
>ga:deviceCategory      object
>ga:country             object
>ga:metro               object
>ga:date                int64
>ga:minute              int64
>ga:channelGrouping     object
>ga:sessions             int64
>ga:transactions         int64
>dtype: object

#Aditionally if I do a simple left join, you'll see that
# the indexs of both files don't join:

dfB[len(dfB.columns)] = 1
t = dfA.merge(dfB, how=""left"", left_index=True, 
right_index=True, suffixes = ('','_y'))
	
t.iloc[:,-1].notnull().any()
>False

#my merge ended up with no '1' values in the column I
# added, mean no indexes matched.

#lastly looking through the pandas docs for 
# DataFrame.reset_index i can use the type detection 
# method there in to determine that the values in 
# my index columns have reverted.

list(enumerate(dfA.index))[1][1][4]
>20180110
list(enumerate(dfA.index))[1][1][5]
>20

list(enumerate(dfB.index))[1][1][4]
>'20180110'
list(enumerate(dfB.index))[1][1][5]
>'20'

#So you can see that the detection of the types
# is different even though i've set both to 
# dtype('0') prior to the .set_index() method.
```

#### Problem description

You can see from the above, that after the I reset the
index my index columns go back to their original
data type instead of the data type I set them to.
Obviously the inability to depend on type consistency is
a big issue.

#### Expected Output

Types remain as declared (astype('O')) when setting and 
resetting index.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.0.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 158 Stepping 9, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.23.4
pytest: None
pip: 18.1
setuptools: 39.0.1
Cython: None
numpy: 1.15.1
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
>>> 

</details>
"
550124126,31037,PERF: improve access of .array,jorisvandenbossche,closed,2020-01-15T11:17:29Z,2020-01-24T08:33:02Z,"xref https://github.com/pandas-dev/pandas/issues/30790

This improves the performance of `.array` access considerably (and thus of `extract_array`, which eg caused the slowdown in the indexing benchmark). The speed-up depends on the dtype (the numpy dtypes are still quite slow ..).

This touches quite some code, but I *think* this is OK for 1.0.0 (although I mixed a bit of clean-up into it, I can also remove that again). 
@jbrockmendel I went the way of adding ""yet another array/values like thing"", but only on the Block (`Block.array_values()`). I think it might be possible to let `_values` return a DatetimeArray instead of datetime64 ndarray for datetime-like data, but that's something that needs more investigation, and also, even if possible, not something I would like to do on 1.0.0, but only for 1.1.

On master:
```
In [1]: s = pd.Series(pd.date_range(""2012-01-01"", periods=3, tz='UTC')) 

In [3]: %timeit s.array   
17.2 µs ± 219 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

In [4]: s = pd.Series(pd.date_range(""2012-01-01"", periods=3)) 

In [6]: %timeit s.array   
4.3 µs ± 205 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

In [7]: s = pd.Series([1, 2, 3]) 

In [9]: %timeit s.array 
6.76 µs ± 208 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
```

With this PR:

```
In [1]: s = pd.Series(pd.date_range(""2012-01-01"", periods=3, tz='UTC'))  

In [3]: %timeit s.array 
290 ns ± 6.26 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)

In [4]: s = pd.Series(pd.date_range(""2012-01-01"", periods=3))   

In [6]: %timeit s.array 
809 ns ± 7.05 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)

In [7]: s = pd.Series([1, 2, 3])   

In [9]: %timeit s.array 
2.34 µs ± 93.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
```"
554605457,31275,Backport PR #31037: PERF: improve access of .array,jorisvandenbossche,closed,2020-01-24T08:32:43Z,2020-01-24T09:06:39Z,https://github.com/pandas-dev/pandas/pull/31037
553869159,31229,(Fixes CI) Replaced set comprehension with a generator,ShaharNaveh,closed,2020-01-23T00:14:58Z,2020-01-24T12:18:37Z,"- [x] closes #31149
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
552540715,31165,TST: Replaced try-catch blocks with pytest.raises,ShaharNaveh,closed,2020-01-20T22:50:55Z,2020-01-24T12:19:25Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
553865284,31228,TST: parametrize tests,ShaharNaveh,closed,2020-01-23T00:03:11Z,2020-01-24T12:19:57Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
540428398,30350,[ENH] Add to_markdown method,MarcoGorelli,closed,2019-12-19T16:41:57Z,2020-01-24T13:10:30Z,"- [ ] closes #11052 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
554621253,31277,ASV: add benchmarks for Series.array and extract_array,jorisvandenbossche,closed,2020-01-24T09:14:35Z,2020-01-24T13:11:28Z,Just thought it would be useful to have a benchmark for `.array` (now the performance regression of https://github.com/pandas-dev/pandas/pull/31037 was caught indirectly through an indexing benchmark)
548606297,30945,BUG: Use self._constructor_sliced in df._reduce to respect subclassed…,EmilianoJordan,closed,2020-01-12T18:13:58Z,2020-01-24T13:15:42Z,"… types. (GH25596)

- [x] closes #25596
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
554799955,31284,Can't replace tz-aware with None,MarcoGorelli,closed,2020-01-24T15:27:30Z,2020-01-24T15:29:31Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
value_a = pd.Timestamp(""20130102"", tz=""US/Eastern"")
value_b = pd.Timestamp(""20130102"", tz=""US/Eastern"")
df = pd.DataFrame({""A"": [value_a], ""B"": [value_b]})
df.replace(to_replace={value_a: None}).dtypes.drop(""A"")

```
#### Problem description

Throws `*** TypeError: data type not understood`. On the other hand, this works:
```python
import pandas as pd
value_a = pd.Timestamp(""20130102"")
value_b = pd.Timestamp(""20130102"")
df = pd.DataFrame({""A"": [value_a], ""B"": [value_b]})
df.replace(to_replace={value_a: None}).dtypes.drop(""A"")

```

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : 7adbc4d0516159cb21fa85c5a476fc67a28af917
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-74-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.25.0+1928.g7adbc4d05.dirty
numpy            : 1.17.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 45.1.0.post20200119
Cython           : 0.29.14
pytest           : 5.3.4
hypothesis       : 5.3.0
sphinx           : 2.3.1
blosc            : 1.8.3
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.3.4
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.12
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.47.0

</details>
"
554771921,31282,Backport PR #30929: ENH: Implement convert_dtypes',jorisvandenbossche,closed,2020-01-24T14:36:45Z,2020-01-24T15:34:18Z,
554744163,31279,DOC: move convert_dtypes whatsnew to 1.0,Dr-Irv,closed,2020-01-24T13:45:32Z,2020-01-24T16:00:05Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
   - moved whatsnew entry from 1.1 to 1.0

Per comment here from @jreback: https://github.com/pandas-dev/pandas/pull/30929#issuecomment-577989924
"
554244545,31249,TST: Fix timestamp comparison test,Dr-Irv,closed,2020-01-23T15:52:37Z,2020-01-24T16:00:07Z,"- [x] closes #31175
- [x] tests added / passed
    - tests/scalar/timestamp/test_timestamp.py
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
    - Not needed for fixing a test
"
552365999,31155,BUG: nonexistent Timestamp pre-summer/winter DST w/dateutil timezone,AlexKirko,closed,2020-01-20T15:23:12Z,2020-01-24T16:31:54Z,"- [X] closes #31043
- [X] tests added 1 / passed 1
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

This implements rounding down to microseconds into Timedelta.total_seconds(). Lack of this rounding led to `dateutil.tz.tzinfo.utcoffset` and `dst` breaking less than 128 nanoseconds before winter/summer DST switch. This happened due to an unintended cast to float in `Timedelta.total_seconds()` compounded with `Timedelta.value` being np.int64 type which doesn't support long arithmetic. The loss of precision led to rounding up, which meant that `total_seconds` since epoch time implied DST time while the `Timedelta.value` hasn't yet reached DST.

Details and code examples below.

<details>

This was quite a journey, but I found out what's going on. Let's say we have a `dateutil.tz.tzinfo` object named `du_tz` and want to find out the DST-aware UTC offset.

1. We call `du_tz.utcoffset(dt)` which calls `du_tz._find_ttinfo(dt)` which calls `du_tz._resolve_ambiguous_time(dt)` to find the index of the last transition time that it uses to return the correct offset.
2. `du_tz._resolve_ambiguous_time(dt)` calls `du_tz._find_last_transition(dt)` which calls the `_datetime_to_timestamp(dt)` dateutil function.
3. This is what this function does:

```
def _datetime_to_timestamp(dt):
    """"""
    Convert a :class:`datetime.datetime` object to an epoch timestamp in
    seconds since January 1, 1970, ignoring the time zone.
    """"""
    return (dt.replace(tzinfo=None) - EPOCH).total_seconds()
```

The problem is dateutil's reliance on `Timedelta.total_seconds` which casts to float:

```
    def total_seconds(self):
        """"""
        Total duration of timedelta in seconds (to ns precision).
        """"""
        return self.value / 1e9
```

Demonstration:

```
IN:
import datetime
import pandas as pd

epoch =  1552211999999999872
ts = pd.Timestamp(epoch, tz='dateutil/US/Pacific')

EPOCH = datetime.datetime.utcfromtimestamp(0)

delta = ts.replace(tzinfo=None) - EPOCH
print(delta.value)
OUT:
1552183199999999872
IN:
print(delta.total_seconds())
OUT:
1552183200.0
IN:
print(ts.tz.dst(ts))
OUT:
datetime.timedelta(seconds=3600)
```
The same thing happens with a `pytz` timezone, only pytz relies on something else to check for DST transitions:
```
IN:
import datetime
import pandas as pd

epoch =  1552211999999999872
ts = pd.Timestamp(epoch, tz='US/Pacific')

EPOCH = datetime.datetime.utcfromtimestamp(0)

delta = ts.replace(tzinfo=None) - EPOCH
print(delta.value)
OUT:
1552183199999999872
IN:
print(delta.total_seconds())
OUT:
1552183200.0
IN:
print(ts.tz.dst(ts))
OUT:
datetime.timedelta(0)
```
So `timedelta` value is okay, but `timedelta.total_seconds` hits the precision limit of floats, and this leads to rounding and an incorrect DST offset.

</details>
"
477014565,27762,BUG: right merge not preserve row order (#27453),bongolegend,closed,2019-08-05T19:13:16Z,2020-01-24T17:15:10Z,"- [ ] closes #27453 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
554386953,31259,"Please help me to understand TypeError: Expected tuple, got str",jason-hernandez-73,closed,2020-01-23T20:24:25Z,2020-01-24T17:19:09Z,"#### Code Sample, a copy-pastable example if possible

```python
# Summarize data by schools
schools_group = complete_data_pd.groupby([""school_name""])
sum_students = schools_group[""student_name""].value_counts()
budgets_df = schools_group[""budget""].mean()
per_student = budgets_df / sum_students
math_mavens_group = complete_data_pd[complete_data_pd['math_score'] >= 70].groupby([""school_name""])
rad_readers_group = complete_data_pd[complete_data_pd['reading_score'] >= 70].groupby([""school_name""])
math_per_school = schools_group[""math_score""].mean()
reading_per_school = schools_group[""reading_score""].mean()
mavens_per_school = math_mavens_group.count()[""student_name""] / sum_students
readers_per_school = rad_readers_group.count()[""student_name""] / sum_students
rate_passing = ((mavens_per_school + readers_per_school) / 2)

# Create new dataframe
summary_df = pd.DataFrame({
                           ""School Type"" : type,
                          ""Total Budget"" : budgets_df,
                          ""Budget Per Student"" : per_student,
                          ""Average Math Score"" : math_per_school, 
                          ""Average Reading Score"" : reading_per_school, 
                          ""% Passing Math"" : mavens_per_school, 
                          ""% Passing Reading"" : readers_per_school, 
                            ""% Overall Passing Rate"" : rate_passing
                          })

```
#### Problem description

If this has been answered in an earlier issue, I couldn't translate it into anything I understand

The above code is my attempt to create a new dataframe by using a groupby object from an earlier dataframe. When I try to run the code, I get a whole lot of traceback that I do not understand, ending with the line, TypeError: Expected tuple, got str.

The error message suggests that the problem is in the line, ""% Overall Passing Rate"" : rate_passing; however, when I remove that line, the same error message throws, but now points to the new last line. Putting brackets around the variables did nothing. What exactly is going on here with tuples and strings?"
554445329,31263,BUG: na_logical_op with 2D inputs,jbrockmendel,closed,2020-01-23T22:31:18Z,2020-01-24T17:44:19Z,"Broken off from a branch implementing block-wise ops for op(dataframe, dataframe)"
549205157,30986,Cannot write MultiIndex DataFrame to excel (to_excel) which contains Peroid IntervalIndex,kurtforrester,closed,2020-01-13T21:56:01Z,2020-01-24T19:04:55Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np

midx = pd.MultiIndex.from_arrays(
    [
        np.repeat([1, 2], 2),
        pd.interval_range(start=pd.Timestamp(""2020-01-01""), periods=4, freq=""6M""),
    ]
)

df = pd.DataFrame(data=np.random.randint(0, 2, size=(4,)), index=midx)
print(df)

df.to_excel(""tmp.xlsx"")
```
#### Problem description
I receive a TypeError and the DataFrame does not write to file.

```python
...
  File ""pandas/_libs/tslibs/timestamps.pyx"", line 412, in pandas._libs.tslibs.timestamps.Timestamp.__new__

  File ""pandas/_libs/tslibs/conversion.pyx"", line 329, in pandas._libs.tslibs.conversion.convert_to_tsobject

TypeError: Cannot convert input [True] of type <class 'bool'> to Timestamp
```

#### Expected Output
Expect the DataFrame to write to file. A DataFrame with a Period/IntervalIndex alone can be written to an excel file. Seems to be an issue with MultiIndex.
#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.7-200.fc31.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.25.3
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.2.1
hypothesis       : 4.54.2
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.1
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : 1.3.9
tables           : 3.6.1
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.1
</details>
"
301901079,19974,min_count is ignored by sum/prod when resampling a PeriodIndex,roedema,closed,2018-03-02T20:51:22Z,2020-01-24T19:04:55Z,"#### Code Sample
Setup:
```python
import numpy as np
import pandas as pd
​
index = pd.date_range(start='2018', freq='M', periods=6)
data = np.ones(6)
data[3:6] = np.nan

datetime = pd.Series(data, index)
period = datetime.to_period()
```

Resampling and summing (`min_count=1`) the Series with a DatetimeIndex:

```python
datetime
datetime.resample('Q').sum(min_count=1)
```
```
2018-01-31   1.0
2018-02-28   1.0
2018-03-31   1.0
2018-04-30   NaN
2018-05-31   NaN
2018-06-30   NaN
Freq: M, dtype: float64
```
```
2018-03-31    3.0
2018-06-30    NaN
Freq: Q-DEC, dtype: float64
```

Resampling and summing (`min_count=1`) the Series with a PeriodIndex:

```python
period
period.resample('Q').sum(min_count=1)
```
```
2018-01    1.0
2018-02    1.0
2018-03    1.0
2018-04    NaN
2018-05    NaN
2018-06    NaN
Freq: M, dtype: float64
```
```
2018Q1    3.0
2018Q2    0.0
Freq: Q-DEC, dtype: float64
```

#### Problem description
`sum()` and `prod()` seem to ignore the `min_count` argument when used on a resampled series or dataframe with a PeriodIndex.

#### Expected Output
I would expect the same result whether using a DatetimeIndex or PeriodIndex. Specifically, in the example above, 2018Q2 should be NaN.

#### Output of ``pd.show_versions()``
<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.4.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 61 Stepping 4, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.22.0
pytest: 3.2.1
pip: 9.0.1
setuptools: 36.5.0.post20170921
Cython: 0.26.1
numpy: 1.13.3
scipy: 0.19.1
pyarrow: None
xarray: None
IPython: 6.1.0
sphinx: 1.6.3
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.2
feather: None
matplotlib: 2.1.0
openpyxl: 2.4.8
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.2
lxml: 4.1.0
bs4: 4.6.0
html5lib: 0.999999999
sqlalchemy: 1.1.13
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
285275561,19020,json_normalize fails when input is a Series without a 0 in the index,colinhiggins,closed,2017-12-31T20:43:41Z,2020-01-24T19:04:55Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
#construct a toy example
data = {0:{'id':1, 'name': 'Foo', 'elements': {'a':1}},1:{'id':2, 'name': 'Bar', 'elements': {'b':2}},2:{'id':3, 'name': 'Baz', 'elements': {'c':3}}}
passing_series = pd.Series(data)

#testing the passing case
pd.io.json.json_normalize(passing_series)
```
Output:
```python
Out[5]: 
   elements.a  elements.b  elements.c  id name
0         1.0         NaN         NaN   1  Foo
1         NaN         2.0         NaN   2  Bar
2         NaN         NaN         3.0   3  Baz
```
```python
#construct the failing case
failing_series = passing_series.copy()
failing_series.index = [1,2,3]

#testing the failing case
pd.io.json.json_normalize(failing_series)
```
Output:
```python
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-7-08797bc7d807> in <module>()
----> 1 pd.io.json.json_normalize(failing_series)

~/miniconda3/envs/uptodatepandas/lib/python3.5/site-packages/pandas/io/json/normalize.py in json_normalize(data, record_path, meta, meta_prefix, record_prefix, errors, sep)
    190 
    191     if record_path is None:
--> 192         if any([isinstance(x, dict) for x in compat.itervalues(data[0])]):
    193             # naive normalization, this is idempotent for flat records
    194             # and potentially will inflate the data considerably for

~/miniconda3/envs/uptodatepandas/lib/python3.5/site-packages/pandas/core/series.py in __getitem__(self, key)
    621         key = com._apply_if_callable(key, self)
    622         try:
--> 623             result = self.index.get_value(self, key)
    624 
    625             if not is_scalar(result):

~/miniconda3/envs/uptodatepandas/lib/python3.5/site-packages/pandas/core/indexes/base.py in get_value(self, series, key)
   2558         try:
   2559             return self._engine.get_value(s, k,
-> 2560                                           tz=getattr(series.dtype, 'tz', None))
   2561         except KeyError as e1:
   2562             if len(self) > 0 and self.inferred_type in ['integer', 'boolean']:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_value()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_value()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

KeyError: 0
```
#### Problem description

This is a bit of a stretch for this function, which calls for a dict or list of dicts, but generally `json_normalize` works just fine for Pandas Series. The exception, of course, being Series without a record whose index value is 0. The reason for this error is pretty obvious--line 192 of `pandas/io/json/normalize.py` attempts to get the first item from the passed `data` parameter with `[]` indexing. (i.e. `if any([isinstance(x, dict) for x in compat.itervalues(data[0])]):`)

#### Expected Output

Either an error for passing an invalid input type (not ideal), or just work correctly if passed a Series by checking the first record in a way that is compatible with Series as well as the standard (list). It seems like an improvement that would be pretty trivial to implement.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.3.final.0
python-bits: 64
OS: Darwin
OS-release: 17.3.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.22.0
pytest: 3.2.1
pip: 9.0.1
setuptools: 36.4.0
Cython: 0.26
numpy: 1.13.1
scipy: 0.19.1
pyarrow: None
xarray: None
IPython: 6.1.0
sphinx: None
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.9999999
sqlalchemy: 1.1.5
pymysql: 0.7.9.None
psycopg2: 2.7.1 (dt dec pq3 ext lo64)
jinja2: 2.9.6
s3fs: 0.1.2
fastparquet: 0.1.0
pandas_gbq: None
pandas_datareader: None
</details>
"
273533842,18265,BUG: Calling DataFrame.stack on an out-of-order column MultiIndex leads to swapped values,tudorprodan,closed,2017-11-13T18:38:40Z,2020-01-24T19:04:56Z,"Please run the code below.  
Notice how the column values are swapped to the wrong labels.  
This is due to `stack()` failing to preserve the order in the MultiIndex.

#### Code Sample, a copy-pastable example if possible

```python
import numpy as np
import pandas as pd

values = np.arange(5)
data = np.vstack([['b{}'.format(x) for x in values],   # b0, b1, ..
                  ['a{}'.format(x) for x in values]])  # a0, a1, ..
df = pd.DataFrame(data.T, columns=['b', 'a'])
df.columns.name = 'first'

# Call pd.concat to get the 2-level MultiIndex *unsorted* columns.
# The bug seems to happen when having one of these unsorted MultiIndexes.
second_level_dict = {'x': df}
multi_level_df = pd.concat(second_level_dict, axis=1)
multi_level_df.columns.names = ['second', 'first']

# Sort the columns, i.e. [a, b] instead of [b, a].
sorted_cols_df = multi_level_df.reindex(sorted(multi_level_df.columns), axis=1)

print('Before the restack:')
print(sorted_cols_df)

# Stack and unstack, should be the same.
# This is what causes the bug. sorted_cols_df.stack() also exposes the problem
restacked = sorted_cols_df.stack(['first', 'second']).unstack(['first', 'second'])

print()
print('Restacked:')
print(restacked)
print('(Notice the swapped column values)')
```
#### Output

```
$ python pandas_bug.py
Before the restack:
second   x
first    a   b
0       a0  b0
1       a1  b1
2       a2  b2
3       a3  b3
4       a4  b4

Restacked:
first    a   b
second   x   x
0       b0  a0  <-- notice the swapped values
1       b1  a1
2       b2  a2
3       b3  a3
4       b4  a4
```


#### Output of ``pd.show_versions()``

I've reproduced this on both 0.21 and 0.20.

<details>
In [2]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-97-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.21.0
pytest: 3.2.1
pip: 9.0.1
setuptools: 36.6.0
Cython: 0.26.1
numpy: 1.13.3
scipy: 0.19.1
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: 1.6.3
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.3
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.2
feather: None
matplotlib: 2.1.0
openpyxl: 2.4.8
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.2
lxml: 4.1.0
bs4: 4.6.0
html5lib: 0.999999999
sqlalchemy: 1.1.13
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
553315645,31196,TST: More regression tests,mroeschke,closed,2020-01-22T05:20:27Z,2020-01-24T19:12:23Z,"- [x] closes #30986
- [x] closes #19020
- [x] closes #19974
- [x] closes #18265
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
548423318,30918,Unexpected behaviour of groupby.transform when using 'fillna',lfiedler,closed,2020-01-11T12:20:47Z,2020-01-24T19:16:01Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np

df = pd.DataFrame(
    {
        'A': ['foo', 'foo', 'foo', 'foo', 'bar', 'bar', 'baz'],
        'B': [1, 2, np.nan, 3, 3, np.nan, 4],
        'C': [np.nan]*7,
        'D': [0,1,2,3,4,5,6],
        'E': [np.nan] + [datetime.datetime(2020,1,1)]*3 + [datetime.datetime(2020,1,2)]*2 +[datetime.datetime(2020,1,3)],
        'F': list('abcdefg'),
        'G': list('abc') + [np.nan] + list('efg'),
        'id': range(0,7),
    }
).set_index('id')
df.groupby('A').transform('fillna', value=9999)
```
**Output**

B|C|D|E|F|G
---|---|---|---|---|---
9999.0|9999.0|2|2020-01-01 00:00:00|c|c
9999.0|9999.0|2|2020-01-01 00:00:00|c|c
9999.0|9999.0|2|2020-01-01 00:00:00|c|c
9999.0|9999.0|2|2020-01-01 00:00:00|c|c
1.0|9999.0|0|9999|a|a
1.0|9999.0|0|9999|a|a
2.0|9999.0|1|2020-01-01 00:00:00|b|b

#### Problem description
When using `GroupBy.transform` together with 'fillna' I expected it to work like `GroupBy.transform` together with `lambda x: x.fillna()`. Instead, it seems to also change values that are not NaN. Even worse, it seems to shuffle contents between groups.

Is this how it is expected to work?

#### Expected Output
```python
df.groupby('A').transform(lambda x: x.fillna(9999))
```
B|C|D|E|F|G
---|---|---|---|---|---
1.0|9999.0|0|9999|a|a
2.0|9999.0|1|2020-01-01 00:00:00|b|b
9999.0|9999.0|2|2020-01-01 00:00:00|c|c
3.0|9999.0|3|2020-01-01 00:00:00|d|9999
3.0|9999.0|4|2020-01-02 00:00:00|e|e
9999.0|9999.0|5|2020-01-02 00:00:00|f|f
4.0|9999.0|6|2020-01-03 00:00:00|g|g


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.0.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : de_DE.UTF-8
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 44.0.0.post20200106
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None


</details>
"
554393316,31260,"DOC fix truncated documentation for Series.unstack(), Series. ",anjalis2112,closed,2020-01-23T20:37:29Z,2020-01-24T19:30:59Z,"fixes https://github.com/pandas-dev/pandas/issues/31235

Changed ""e.g. "" to ""e.g., ""
Changed ""a.k.a. "" to ""also known as""
This helps avoid the truncation caused in the docstring"
462135533,27100,API: Implement new indexing behavior for intervals,jschendel,closed,2019-06-28T17:26:18Z,2020-01-24T19:47:30Z,"- [X] closes #16316, closes #23705, closes #25087, closes #25860, closes #24931
- [X] tests added / passed
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

I left the `test_interval.py` and `test_interval_new.py` files separate for now, in order to make it more obvious what is being changed.  Will condense these files in a follow-up."
549511773,30998,TST: insert 'match' to bare pytest raises in  pandas/tests/internals/,ShaharNaveh,closed,2020-01-14T11:40:54Z,2020-01-24T20:13:08Z,"- [x] ref #30999
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
554791745,31283,TYP: Type annotations in pandas/io/sas/sas_constants.py,ShaharNaveh,closed,2020-01-24T15:12:41Z,2020-01-24T20:13:44Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
551805101,31126,TYP: DataFrame.(index|columns) and Series.index,topper-123,closed,2020-01-18T18:23:05Z,2020-01-24T20:20:14Z,"Makes mypy discover the ``.index`` and ``.columns`` attributes and that they're ``Index`` (sub-)classes.

This is done by manually adding the ``properties.AxisProperty`` to DataFrame/Series instead of doing it programatically, as mypy doesn't like adding attributes programatically very much."
550077895,31036,BUG: AssertionError on Series.append(DataFrame) fix  #30975 ,hvardhan20,closed,2020-01-15T09:51:29Z,2020-01-24T20:46:20Z,"- [x] closes #30975 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Series.append() does not throw AssertionError anymore. This fix performs a precheck before proceeding with concatenation. Tests have passed. Whatsnew entry added.
Thanks!"
554852757,31287,Allow .dt.month_name() and .dt.day_name() to return ordered categorical ,richierocks,closed,2020-01-24T17:06:52Z,2020-01-24T21:42:15Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np

# A data frame with a date column
dates = pd.date_range(""2010-01-01"", ""2020-01-01"", freq=""1D"").tolist()
groups = [""first"", ""second"", ""third""]
df = pd.DataFrame({
    ""date"": dates * len(groups),
    ""group"": np.repeat(groups, len(dates)).tolist(),
    ""value"": np.random.normal(0.0, 1.0, len(groups) * len(dates))
})

# This pivot table has months alphabetically, which isn't helpful
df[""month""] = df[""date""].dt.month_name()
df.pivot_table(""value"", ""month"", ""group"")

## group         first    second     third
## month                                  
## April      0.100473  0.038078 -0.044501
## August     0.036322  0.092148  0.076523
## December   0.007469 -0.008177 -0.001926
## February  -0.177943 -0.050950  0.079945
## January    0.034046 -0.019727  0.035488
## July      -0.095040  0.123110 -0.079442
## June      -0.023626  0.003913 -0.028143
## March     -0.057740  0.050930 -0.059962
## May       -0.110125 -0.053679  0.009097
## November  -0.097954 -0.001033  0.002073
## October   -0.023615 -0.008795  0.033985
## September -0.030819 -0.024159 -0.046538

# To get months chronologically, it's much more effort
month_names = ['January', 'February', 'March', 'April', 'May', 'June', 'July',
          'August', 'September', 'October', 'November', 'December']
df[""month2""] = pd.Categorical(
    df[""date""].dt.month_name(), 
    categories=month_names, 
    ordered=True)
df.pivot_table(""value"", ""month2"", ""group"")

## group         first    second     third
## month2                                 
## January    0.034046 -0.019727  0.035488
## February  -0.177943 -0.050950  0.079945
## March     -0.057740  0.050930 -0.059962
## April      0.100473  0.038078 -0.044501
## May       -0.110125 -0.053679  0.009097
## June      -0.023626  0.003913 -0.028143
## July      -0.095040  0.123110 -0.079442
## August     0.036322  0.092148  0.076523
## September -0.030819 -0.024159 -0.046538
## October   -0.023615 -0.008795  0.033985
## November  -0.097954 -0.001033  0.002073
## December   0.007469 -0.008177 -0.001926

# What I wish I could have typed
df[""month""] = df[""date""].dt.month_name(ordered=True)
df.pivot_table(""value"", ""month"", ""group"")
```
#### Problem description

[`pandas.Series.dt.month_name()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.month_name.html) returns the month names as strings, which means that they get sorted alphabetically. In most cases, that's less useful than them begin sorted from January to December. (See the pivot table generated by the code snippet as an example.)

It is possible to generate month names as an ordered categorical variable, but it feels like a lot of extra code for such a routine task.

I'd like to be able to pass an argument to `.dt.month_name()` to specify that I want ordered months, and have pandas do the work for me. 

There are a few options for the user interface,; I'm not sure which is best. `ordered=True` is one possible interface. Another would be having a `sort_order` argument that could be `alphabetical` or `chronological`. A third possibility would would be `return_type` as `string` or `categorical`.

The situation is similar for [`pandas.Series.dt.day_name()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.day_name.html), though that has the added complication that sometimes you want an order of Sunday -> Saturday, and sometimes you want an order of Monday -> Sunday.

#### Expected Output

Months are ordered alphabetically. This is expected, but having them ordered from January to December is more useful.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.13.3
pytz             : 2017.2
dateutil         : 2.7.3
pip              : 19.2.3
setuptools       : 44.0.0.post20200106
Cython           : 0.25.2
pytest           : 3.0.7
hypothesis       : None
sphinx           : 1.5.6
blosc            : None
feather          : None
xlsxwriter       : 0.9.6
lxml.etree       : 3.7.3
html5lib         : 0.999
pymysql          : 0.9.3
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 5.3.0
pandas_datareader: None
bs4              : 4.6.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 3.7.3
matplotlib       : 2.0.2
numexpr          : 2.6.2
odfpy            : None
openpyxl         : 2.4.7
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 0.19.1
sqlalchemy       : 1.3.8
tables           : 3.3.0
xarray           : None
xlrd             : 1.0.0
xlwt             : 1.2.0
xlsxwriter       : 0.9.6

</details>
"
526386620,29768,CI: Action to assign issues fails when quotes are present,ivirshup,closed,2019-11-21T06:12:16Z,2020-01-25T03:17:39Z,"So, this is a little weird. Apparently I left such a bad comment on an issue, it failed CI 😄. 

I'm not sure how it got triggered, but looks like it's from a recently added workflow. The CI error has the contents of my comment, and I got an email that looks like this:

<details>
<summary> Notification email </summary>

------------

Run failed for master (6d11aa8)

Repository: pandas-dev/pandas
Workflow: Assign
Duration: 16.0 seconds
Finished: 2019-11-21 05:41:26 UTC

[View results](https://github.com/pandas-dev/pandas/commit/6d11aa89fc06ebc55519507f6652dcfcba35a302/checks)

Jobs:
[cb4db6dd-e658-55b7-b337-8761517bc1a3](https://github.com/pandas-dev/pandas/runs/313172123) failed (1 annotation)
—
You are receiving this because this workflow ran on your branch.
Manage your GitHub Actions notifications here.

-------------------

</details>

* [CI failure](https://github.com/pandas-dev/pandas/runs/313172123) - [Raw log](https://pipelines.actions.githubusercontent.com/xZyE9jtmkxWlfCAbyu1SHPJOlsa2huNFYcxohSTomy6EbdNZT9/_apis/pipelines/1/runs/857/signedlogcontent/3?urlExpires=2019-11-21T06%3A00%3A03.3232502Z&urlSigningMethod=HMACV1&urlSignature=vlq2JBJLcoJ3pFQnugH8M9AcGnopXrvuooUOaeU2eAA%3D)
* [The comment](https://github.com/pandas-dev/pandas/issues/14177)

Looks like this might be related to #29648, @datapythonista?"
553903246,31237,CI: Fix Assign CI not working with quotes,lithomas1,closed,2020-01-23T02:28:50Z,2020-01-25T03:19:51Z,"- [x] closes #29768 
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Examples(all three passed)
![image](https://user-images.githubusercontent.com/47963215/72952081-a6481100-3d45-11ea-8845-338f2a2f17a4.png)

"
554750291,31280,Fixes non-existent doc/source/api.rst issue ,ArunimSamudra,closed,2020-01-24T13:57:14Z,2020-01-25T06:00:03Z,Modified the path doc/source/api.rst
551954998,31135,Docs mention non-existent doc/source/api.rst,MarcoGorelli,closed,2020-01-19T17:30:13Z,2020-01-25T12:17:55Z,"E.g. in doc/source/development/contributing.rst it says:
> * Our API documentation in ``doc/source/api.rst`` houses the auto-generated
>   documentation from the docstrings. For classes, there are a few subtleties
>   around controlling which methods and attributes have pages auto-generated.
> 

There's a few other mentions of it elsewhere
"
554752089,31281,CI: update azure VM image,TomAugspurger,closed,2020-01-24T14:00:44Z,2020-01-25T15:37:59Z,"Email from Azure support

> On March 23, 2020, we’ll be removing the following Azure Pipelines hosted images:
>
> * Windows Server 2012R2 with Visual Studio 2015, a.k.a vs2015-win2012r2
> * macOS X High Sierra 10.13, a.k.a macOS-10.13
> * Windows Server Core 1803, a.k.a win1803

> If you use YAML pipelines, update your pipeline by editing its YAML file.
> 
>  
> 
>   jobs:
> 
>     - job: Windows
> 
>       pool:
> 
>         vmImage: 'vs2017-win2016'
> 
>       steps:
> 
>       - script: echo hello from Windows"
555032242,31301,CLN: remove _set_subtyp,jbrockmendel,closed,2020-01-25T02:37:01Z,2020-01-25T16:04:55Z,"its not used anywhere, apparently legacy leftover"
555099170,31309,Backport PR #31292 on branch 1.0.x (CI: Updated version of macos image),meeseeksmachine,closed,2020-01-25T15:38:35Z,2020-01-25T16:08:09Z,Backport PR #31292: CI: Updated version of macos image
555028724,31300,PERF: avoid copies if possible in fill_binop,jbrockmendel,closed,2020-01-25T02:01:57Z,2020-01-25T16:28:10Z,"This has a small perf bump, but more importantly is necessary for the blockwise frame-with-frame PR that is coming up.

```
import pandas as pd
from pandas.core.ops import *

arr = np.arange(10**6)
df = pd.DataFrame({""A"": arr})
ser = df[""A""]

%timeit result = df.add(df, fill_value=4)
7.77 ms ± 20.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  <-- master
5.46 ms ± 36.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  <-- PR

%timeit result = ser.add(ser, fill_value=1)
6.79 ms ± 56.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  <-- master
4.65 ms ± 82.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  <-- PR
```
"
554544192,31268,BUG: passing TDA and wrong freq to TimedeltaIndex,jbrockmendel,closed,2020-01-24T04:56:03Z,2020-01-25T16:31:07Z,"Caught while working towards making TimedeltaArray._simple_new more strict about its inputs.

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
552593535,31172,BUG: inconsistency between PeriodIndex.get_value vs get_loc,jbrockmendel,closed,2020-01-21T02:44:42Z,2020-01-25T16:34:19Z,"ATM we have inconsistent treatment for indexing with strings where the string has a higher resolution than the PeriodIndex.  Luckily only one test checks for the inconsistent `PeriodIndex.__contains__` behavior, so fixing it isn't _too_ invasive.

```
pi = pd.period_range(""2017-09-01"", freq=""D"", periods=3)
ser = pd.Series(range(5, 8), index=pi)
key = ""2017-09-01 00:00:01""

# master
>>> key in pi
True
>>> pi.get_loc(key)
0
>>> pi.get_value(ser, key)
KeyError: '2017-09-01 00:00:01'

# PR
>>> key in pi
False
>>> pi.get_loc(key)
KeyError: '2017-09-01 00:00:01'
>>> pi.get_value(ser, key)
KeyError: '2017-09-01 00:00:01'
```

Note: the diff is fairly big because we're mirroring the code in `get_value`.  Once this is fixed, we'll have get_value dispatch to get_loc and remove basically the same amount of code as we're adding here."
551967332,31138,TST:Disallow bare pytest raises issue 30999,shubchat,closed,2020-01-19T18:57:46Z,2020-01-25T21:06:46Z,"- [x] refers #30999 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This pull request is to add appropriate match arguments to bare pytest.raises listed in #30999 [Comment](https://github.com/pandas-dev/pandas/issues/30999#issuecomment-574339599)"
555145864,31313,Series logical operations with nullable Boolean data type,dsaxton,closed,2020-01-25T22:14:51Z,2020-01-25T22:26:00Z,"Should the `Series` logical operations also follow three-valued logic when using the nullable boolean data type? For example I would expect the two cases below to return `pd.NA`:

```python
import pandas as pd

pd.Series([True, None], dtype=""boolean"").all()
# True

pd.Series([False, None], dtype=""boolean"").any()
# False

pd.__version__
# '1.0.0rc0+198.g5ecd94c72'
```"
555103636,31310,CI: xfail sparse warning,jreback,closed,2020-01-25T16:17:07Z,2020-01-26T00:26:04Z,"this is failing builds occasionally. not sure if its a deeper problem. can just xfail for now and investigate.

https://travis-ci.org/pandas-dev/pandas/jobs/641752516

```
=================================== FAILURES ===================================
__________________________ test_legacy_sparse_warning __________________________
[gw0] linux -- Python 3.6.10 /home/travis/miniconda3/envs/pandas-dev/bin/python
datapath = <function datapath.<locals>.deco at 0x7fd87cf76268>
    def test_legacy_sparse_warning(datapath):
        """"""
    
        Generated with
    
        >>> df = pd.DataFrame({""A"": [1, 2, 3, 4], ""B"": [0, 0, 1, 1]}).to_sparse()
        >>> df.to_pickle(""pandas/tests/io/data/pickle/sparseframe-0.20.3.pickle.gz"",
        ...              compression=""gzip"")
    
        >>> s = df['B']
        >>> s.to_pickle(""pandas/tests/io/data/pickle/sparseseries-0.20.3.pickle.gz"",
        ...             compression=""gzip"")
        """"""
        with tm.assert_produces_warning(FutureWarning):
            simplefilter(""ignore"", DeprecationWarning)  # from boto
            pd.read_pickle(
                datapath(""io"", ""data"", ""pickle"", ""sparseseries-0.20.3.pickle.gz""),
>               compression=""gzip"",
            )
pandas/tests/io/test_pickle.py:218: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
self = <contextlib._GeneratorContextManager object at 0x7fd87ceed6a0>
type = None, value = None, traceback = None
    def __exit__(self, type, value, traceback):
        if type is None:
            try:
>               next(self.gen)
E               AssertionError: Caused unexpected warning(s): [('ResourceWarning', ResourceWarning(""unclosed <socket.socket fd=18, family=AddressFamily.AF_INET, type=2049, proto=6, laddr=('10.20.0.67', 37224), raddr=('74.125.202.84', 443)>"",), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.6/inspect.py', 732), ('ResourceWarning', ResourceWarning(""unclosed <socket.socket fd=19, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.67', 46940), raddr=('172.217.214.95', 443)>"",), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.6/inspect.py', 732)]
../../../miniconda3/envs/pandas-dev/lib/python3.6/contextlib.py:88: AssertionError
=============================== warnings summary ===============================
/home/travis/miniconda3/envs/pandas-dev/lib/python3.6/site-packages/fastparquet/dataframe.py:5
/home/travis/miniconda3/envs/pandas-dev/lib/python3.6/site-packages/fastparquet/dataframe.py:5
  /home/travis/miniconda3/envs/pandas-dev/lib/python3.6/site-packages/fastparquet/dataframe.py:5: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.
    from pandas.core.index import CategoricalIndex, RangeIndex, Index, MultiIndex
/home/travis/miniconda3/envs/pandas-dev/lib/python3.6/site-packages/statsmodels/tools/_testing.py:19
  /home/travis/miniconda3/envs/pandas-dev/lib/python3.6/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.
    import pandas.util.testing as tm
```"
555113010,31311,xfail sparse warning; closes #31310,bfgray3,closed,2020-01-25T17:36:58Z,2020-01-26T00:26:20Z,"xref #31310 
"
459437870,26996,BUG: Series.rolling min_period is ignored and NA behaves strangely,ghost,closed,2019-06-22T05:08:59Z,2020-01-26T00:29:40Z,"Using current d39a6de668

This may be two separate issues, I can't tell.

### Case 1.

```python
import pandas as pd
ser=pd.Series(range(10))
ser[:5]=None
print(ser)
pd.Series(ser).rolling(3, min_periods=3).count()
```
returns 
```
0    0.0
1    0.0
2    0.0
3    0.0
4    0.0
5    1.0
6    2.0
7    3.0
8    3.0
9    3.0
dtype: float64
```

I expected the first few results to be NA. perhaps related:
```
pd.Series(ser).rolling(3, min_periods=999).count()
```
doesn't raise an exception.

### Case 2.

for series with DatetimeIndex and rolling with an offset, the behavior is also strange, but different.

```python
import pandas as pd
import random
from itertools import chain
dates = [""2001-01-01""]*2 + [""2001-01-02""]*2 + [""2001-01-03""]*2 + [""2001-01-04""]*2 + [""2001-01-05""]*2+ [""2001-01-06""]*2
ser=pd.Series(index=pd.DatetimeIndex(dates))
ser[0]=111
ser[2]=222
ser[6]=333
res=ser.rolling(""2D"", min_periods=1).count()
df=pd.DataFrame(dict(data=ser,count=res))
print(df)
```

result
```
             data  count
2001-01-01  111.0    1.0   # OK. The day has one non-NA value, 
2001-01-01    NaN    1.0  # and min_period is satisfied
2001-01-02  222.0    2.0  # OK
2001-01-02    NaN    2.0  # OK
2001-01-03    NaN    1.0  # why is this 1.0? this date has no values in it.
2001-01-03    NaN    1.0 # again
2001-01-04  333.0    1.0  # OK
2001-01-04    NaN    1.0 # OK
2001-01-05    NaN    1.0 # why is this 1.0? this date has no values in it.
2001-01-05    NaN    1.0 # again
2001-01-06    NaN    NaN # why is this NaN? min_periods is satisfied
2001-01-06    NaN    NaN # and non-NA count should 0.
```

encountered in the course of #26959"
555153007,31316,CLN: internals.managers,jbrockmendel,closed,2020-01-25T23:27:06Z,2020-01-26T00:32:50Z,"Mostly annotations, use fastpath for SingleBlockManager constructor where possible."
555147771,31314,REF: DatetimeIndex.get_value wrap DTI.get_loc,jbrockmendel,closed,2020-01-25T22:33:42Z,2020-01-26T00:38:36Z,"After this, DTI.get_value is identical to TDI.get_value.  Separate PR to get PeriodIndex to match too, then de-duplicate."
555041554,31304,REF: define _get_slice_axis in correct classes,jbrockmendel,closed,2020-01-25T04:36:10Z,2020-01-26T00:46:49Z,"Since ix has been removed, a bunch of _NDFrameIndexer methods are defined in now-weird places.  This fixes the one that confused me today."
554552565,31271,"BUG: DataFrame.floordiv(ser, axis=0) not matching column-wise bheavior",jbrockmendel,closed,2020-01-24T05:30:30Z,2020-01-26T00:55:47Z,We're pretty close to being able to combine _combine_frame and _combine_match_index (and hopefully get rid of them both before long)
543517753,30543,PERF: Timestamp/Timedelta constructors when passed a Timestamp/Timedelta,jschendel,closed,2019-12-29T18:45:17Z,2020-01-26T01:03:37Z,"The `Timestamp` constructor's performance could be improved when an existing `Timestamp` object is passed to it via an `isinstance`-like check:
```python
In [1]: import pandas as pd; pd.__version__
Out[1]: '0.26.0.dev0+1469.ge817ffff3'

In [2]: ts = pd.Timestamp('2020')

In [3]: def timestamp_isinstance_shortcircuit(ts): 
   ...:     if isinstance(ts, pd.Timestamp): 
   ...:         return ts 
   ...:     return pd.Timestamp(ts) 
   ...:

In [4]: %timeit pd.Timestamp(ts)
849 ns ± 13.2 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)

In [5]: %timeit timestamp_isinstance_shortcircuit(ts)
121 ns ± 0.279 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)
```
Some care is needed in the constructor to check if other arguments have been passed, e.g. `tz`, where we wouldn't be able to directly return the `Timestamp` object.

Similar story for the `Timedelta` constructor (should be done in a separate PR):
```python
In [6]: td = pd.Timedelta('1 day')

In [7]: def timedelta_isinstance_shortcircuit(td): 
   ...:     if isinstance(td, pd.Timedelta): 
   ...:         return td 
   ...:     return pd.Timedelta(td) 
   ...:

In [8]: %timeit pd.Timedelta(td)
800 ns ± 1.35 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)

In [9]: %timeit timedelta_isinstance_shortcircuit(td)
120 ns ± 0.269 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)
```

xref #30520"
42098740,8193,Passing a dictionary of columns names and colors ,DataSwede,closed,2014-09-05T22:47:23Z,2020-01-26T01:04:27Z,"Related to this question:

http://stackoverflow.com/questions/25689558/pandas-bar-plot-specify-bar-color-by-column

Having the ability to natively pass in a dictionary of the column names and the desired color could be a nice addition to simplify specifying what color you want each line/bar to be.
"
553896892,31233,"DOC ENH: Move DataFrame.info from ""Serialization / IO / conversion"" to ""Attributes and underlying data""",jpeacock29,closed,2020-01-23T02:01:47Z,2020-01-26T01:15:23Z,"In the [API reference for DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html), why is `DataFrame.info()` in the ""Serialization / IO / conversion"" section? It would make more sense to me in the ""Attributes and underlying data"" section, which has similar methods and attributes like `DataFrame.memory_usage()`, `DataFrame.dtypes`and `DataFrame.shape`.

Glad to make a pull request if this makes sense to others."
554884324,31291,is_iterator doctest incorrect for list,jbrockmendel,closed,2020-01-24T18:20:56Z,2020-01-26T01:38:02Z,"The docstring and doctest say that lists are considered iterators, but `isiterator([1, 2, 3])` (one of the docstring examples) returns False (which I think is a more useful behavior.

Is this doctest not run?"
553194660,31188,BUG: DatetimeIndex.snap incorrectly setting freq,jbrockmendel,closed,2020-01-21T22:57:15Z,2020-01-26T01:42:51Z,
387990742,24119,API: add return_inverse to pd.unique,h-vetinari,closed,2018-12-05T23:27:05Z,2020-01-26T01:46:33Z,"- [x] splits off first chunk of #24108; progress towards #4087 / #21357 / #21720 / #22824
- [x] tests expanded / parametrized / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This is the first part I'm splitting off of #24108, but now with full test coverage. For the moment, I've added `return_inverse` to `pd.unique` and to `Categorical.unique`, but it's not trivial because of inconsistencies like the following:
```
>>> import pandas as pd
>>> idx = pd.Index([0, 1, 1, 0])
>>> pd.unique(idx)
array([0, 1], dtype=int64)
>>>
>>> # So pd.unique(Index) yields an array, except if the Index is categorical...?
>>> idx = idx.astype('category')
>>> pd.unique(idx)
CategoricalIndex([0, 1], categories=[0, 1], ordered=False, dtype='category')
```

I'd be open to further split off the change for `Categorical.unique`, or just return `NotImplemented` for all `ExtensionArray` types. As mentioned in #24108 already, I believe that the possibility for `return_inverse` (or maybe even kwargs in general??) is something that should be added to the EA interface. @TomAugspurger @jreback @jbrockmendel 

"
555032684,31303,applymap fails in presence of nan values and function returning None,andrew222651,closed,2020-01-25T02:41:59Z,2020-01-26T03:44:45Z,"Is this the proper behavior?

```python
df = pd.DataFrame([{'a': np.nan}, {'a': 3.5}])
def convert_cell(x): 
    if isinstance(x, float): 
        if math.isnan(x): 
            return None 
        else: 
            return round(x) 
    else: 
        return x

df.applymap(convert_cell)
# gives
#     a
#0  NaN
#1  4.0

[ convert_cell(x) for x in [np.nan, 3.5] ]
# gives
# [None, 4]
```
If we replace `None` with say, -1, in `convert_cell` it works as expected. If all values in the column are nan, it works. If all values are non-nan floats it works. If `convert_cell = lambda x: None`, that works.

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.0.0-37-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_CA.UTF-8
LOCALE           : en_CA.UTF-8

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 9.0.1
setuptools       : 39.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.3.2
html5lib         : 0.999999999
pymysql          : 0.8.0
psycopg2         : None
jinja2           : 2.10
IPython          : 7.3.0
pandas_datareader: None
bs4              : 4.6.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.2
matplotlib       : 3.0.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : 1.1.11
tables           : None
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : None
```

"
555169315,31320,Backport PR: Series rolling count ignores min_periods,fujiaxiang,closed,2020-01-26T02:46:14Z,2020-01-26T04:44:44Z,This is a backport for #30923
548449964,30923,BUG: Series rolling count ignores min_periods,fujiaxiang,closed,2020-01-11T16:27:36Z,2020-01-26T04:45:07Z,"- [x] closes #26996
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
555164427,31319,BUG Series rolling count ignores min periods,fujiaxiang,closed,2020-01-26T01:38:28Z,2020-01-26T04:45:08Z,"This PR targets 1.0.x branch and is a manual backport for #30923
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
555174776,31322,Update testing.py,spchandgude,closed,2020-01-26T04:08:47Z,2020-01-26T05:36:44Z,Comment added
456847573,26899,Usability: TAB completion no longer works with accessors,jorisvandenbossche,closed,2019-06-17T10:00:23Z,2020-01-26T06:16:48Z,"When doing the following in the IPython console:

```
In [1]: pd.Series.str.<TAB>
```

or 

```
In [2]: s = pd.Series(['a', 'b']) 

In [3]: s.str.<TAB>
```

this used to give the list of available methods on the `str` accessor (the same for eg `dt`). But with latest master / latest IPython this no longer works. This is quite a usability regression, I typically used it when giving workshows to show a way to interactively check what methods are available.

The non working case is with pandas 0.24.2 + IPython 7.5.0. I checked it with another environment with pandas 0.22.0 + IPython 5.8.0 where it is working. 
So it could be either due to a change in pandas or a change in IPython (or one of its dependencies for this)."
553422507,31198,DOC Remove Python 2 specific comments from documentation,rth,closed,2020-01-22T09:45:56Z,2020-01-26T09:42:56Z,"A few minor fixes to remove Python 2 specific comments from the documentation, since 1.0 does not support Python 2.
"
552563269,31167,DOC: fix DataFrame.plot docs ,lithomas1,closed,2020-01-21T00:27:07Z,2020-01-26T15:55:17Z,"- [x] fixes #29489 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
 "
555251801,31327,Consider moving tests/ outside the pandas module,joaoe,closed,2020-01-26T16:23:22Z,2020-01-26T19:03:04Z,"
#### Problem description

Hi.
After a `pip install pandas` the `lib/site-packages/pands/tests/` includes a lot of testing code which is definitely not relevant for me and many other end users of pandas.
This bloats the installation and makes installation slower.
I'm working on packaging a python environment to distribute with a preinstalled set of modules and application and there are too many popular 3rd-party modules which include unneeded test code, like `numpy`, `IPython` `jupyterlab`, etc, which needs to be striped to keep the package size down. I'll be reporting issues to these projects as well.

#### Suggestion

Therefore, my suggestion is to keep the `pandas` module streamlined, and move the tests out. Perhaps create a `pandas-unittests` module if people are interested in it, or just expect users to checkout the code. Another possibility would be to skip packaging the `tests` folder and `conftest.py` when creating packages to upload to `pypi.org`.

Since you are now almost releasing 1.0 it might be a bit short notice to include this is such a big release. But for the next major release, it could work.

Thank you very much for your attention."
550967186,31079,TST: Remove bare pytest.raises ,gdex1,closed,2020-01-16T18:02:21Z,2020-01-27T04:45:27Z,"References #30999 

Adds match argument to pytest.raises found in the following files. https://github.com/pandas-dev/pandas/issues/30999#issuecomment-574174389

**ToDo:**
- [x] pandas/tests/arithmetic/test_numeric
- [X] pandas/tests/arithmetic/test_object
- [x] pandas/tests/arithmetic/test_timedelta64
- [x] pandas/tests/arrays/interval/test_interval
- [ ] add whatsnew entry"
550696553,31070,PERF: add shortcut to Timedelta constructor,AlexKirko,closed,2020-01-16T09:57:08Z,2020-01-27T06:54:59Z,"WIP until #30676 gets merged.
- [ ] second half of #30543
- [X] tests added 1 / passed 1
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

This implements a shortcut in the Timedelta constructor to cut down on processing if Timedelta is passed. We still need to check if any other args are passed. Then, if a Timedelta with no other kwargs was passed, we just return that same Timedelta.
A test is added to check that the Timedelta is still the same object."
545252700,30676,PERF: add shortcut to Timestamp constructor,AlexKirko,closed,2020-01-04T07:19:38Z,2020-01-27T06:55:10Z,"- [X] closes #30543 
- [X] tests added 1 / passed 1
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

This implements a shortcut in the Timestamp constructor to cut down on processing if Timestamp is passed. We still need to check if the timezone was passed correctly. Then, if a Timestamp was passed, and there is no timezone, we just return that same Timestamp.
A test is added to check that the Timestamp is still the same object.

PR for timedelta to be added once I confirm that this is the approach we want to go with.
"
499948157,28672,Pandas tries to do something with indexes while only supposed to work with values.,PMykhailo,closed,2019-09-29T17:52:03Z,2020-01-27T07:23:00Z,"#### Code Sample, a copy-pastable example if possible

```python
ra = pd.read_excel(""Edata.xlsx"", sheet_name=""r-a"").astype(float)

```
```python
d[i].min().min()

```
#### Problem description
Traceback (most recent call last):
  File ""E:/загрузки/учеба/лабы/H2lab/Search.py"", line 4, in <module>
    ra = pd.read_excel(""Edata.xlsx"", sheet_name=""r-a"").astype(float)
  File ""E:\python\lib\site-packages\pandas\core\generic.py"", line 5882, in astype
    dtype=dtype, copy=copy, errors=errors, **kwargs
  File ""E:\python\lib\site-packages\pandas\core\internals\managers.py"", line 581, in astype
    return self.apply(""astype"", dtype=dtype, **kwargs)
  File ""E:\python\lib\site-packages\pandas\core\internals\managers.py"", line 438, in apply
    applied = getattr(b, f)(**kwargs)
  File ""E:\python\lib\site-packages\pandas\core\internals\blocks.py"", line 559, in astype
    return self._astype(dtype, copy=copy, errors=errors, values=values, **kwargs)
  File ""E:\python\lib\site-packages\pandas\core\internals\blocks.py"", line 643, in _astype
    values = astype_nansafe(vals1d, dtype, copy=True, **kwargs)
  File ""E:\python\lib\site-packages\pandas\core\dtypes\cast.py"", line 729, in astype_nansafe
    return arr.astype(dtype, copy=True)
ValueError: could not convert string to float: '4→0'

---------------

Traceback (most recent call last):
  File ""E:/загрузки/учеба/лабы/H2lab/Search.py"", line 23, in <module>
    mins[i] = d[i].min().min()
  File ""E:\python\lib\site-packages\pandas\core\generic.py"", line 11620, in stat_func
    f, name, axis=axis, skipna=skipna, numeric_only=numeric_only
  File ""E:\python\lib\site-packages\pandas\core\series.py"", line 4083, in _reduce
    return op(delegate, skipna=skipna, **kwds)
  File ""E:\python\lib\site-packages\pandas\core\nanops.py"", line 123, in f
    result = alt(values, axis=axis, skipna=skipna, **kwds)
  File ""E:\python\lib\site-packages\pandas\core\nanops.py"", line 843, in reduction
    result = getattr(values, meth)(axis)
  File ""E:\python\lib\site-packages\numpy\core\_methods.py"", line 32, in _amin
    return umr_minimum(a, axis, None, out, keepdims, initial)
TypeError: '<=' not supported between instances of 'str' and 'int'
I encountered this problem i new version of pandas. It seems like pandas when coverting values to float or searching for min, tries to operate with indexes(which strings) as well.  I know it because ""4→0"" is a name of row or column. I can provide any fether info if you need it. It works ok with pandas 0.23.4

#### Expected Output
everything working without mistakes
#### Output of ``pd.show_versions()``

<details>
This convig works prety fine:
[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.0.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None
pandas: 0.23.4
pytest: None
pip: 19.1.1
setuptools: 39.0.1
Cython: 0.29.7
numpy: 1.15.1
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 7.0.1
sphinx: 1.8.1
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.0
openpyxl: 2.6.3
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: 4.3.3
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

------------------------------------------

This doesn't:
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.0.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 0.25.1
numpy            : 1.15.1
pytz             : 2018.5
dateutil         : 2.7.3
pip              : 19.1.1
setuptools       : 39.0.1
Cython           : 0.29.7
pytest           : None
hypothesis       : None
sphinx           : 1.8.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.3.3
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 7.0.1
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.3
matplotlib       : 3.0.0
numexpr          : None
odfpy            : None
openpyxl         : 2.6.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.1.0
xlwt             : None
xlsxwriter       : None

</details>
"
555499512,31339,Sometimes one day of when reading date fields,matthiasleopold,closed,2020-01-27T11:00:16Z,2020-01-27T11:51:38Z,"#### Code Sample

```python
import datetime
import pandas

df = pandas.read_excel('28422_LZ.xlsx', sheet_name='Laufzettel')
stamp = df[df.columns[3]][16].timestamp()
print('first date =', datetime.datetime.utcfromtimestamp(stamp).strftime('%Y-%m-%d'), '(one day of)')

df = pandas.read_excel('test.xlsx', sheet_name='Tabelle1')
stamp = df[df.columns[0]][0
[28422_LZ.xlsx](https://github.com/pandas-dev/pandas/files/4116748/28422_LZ.xlsx)
[test.xlsx](https://github.com/pandas-dev/pandas/files/4116749/test.xlsx)

].timestamp()
print('seconde date =', datetime.datetime.utcfromtimestamp(stamp).strftime('%Y-%m-%d'), '(correct)')
```
#### Problem description

I read two date fields from two excel sheets. First date is one day off, second date is okay. I cannot see any reason for the difference in behavior. First date ('recalculated time') should be  2015-09-18.

#### Expected Output

```
first date = 2015-09-18 (!!!)
seconde date = 1978-03-16 (correct)
```


#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.0-6-amd64
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : de_DE.UTF-8
LOCALE           : de_DE.UTF-8

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 18.1
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
None
[28422_LZ.xlsx](https://github.com/pandas-dev/pandas/files/4116717/28422_LZ.xlsx)
[test.xlsx](https://github.com/pandas-dev/pandas/files/4116718/test.xlsx)

"
555204560,31324,CI: seems jedi caused ci to fail in certain versions,charlesdong1991,closed,2020-01-26T09:47:19Z,2020-01-27T12:28:10Z,"```
self = <pandas.tests.indexes.test_base.TestIndex object at 0x7f666b28c990>
ip = <IPython.core.interactiveshell.InteractiveShell object at 0x7f666b28ccd0>

    @async_mark()
    async def test_tab_complete_warning(self, ip):
        # ***/issues/16409
        pytest.importorskip(""IPython"", minversion=""6.0.0"")
        from IPython.core.completer import provisionalcompleter
    
        code = ""import pandas as pd; idx = pd.Index([1, 2])""
        await ip.run_code(code)
        with tm.assert_produces_warning(None):
            with provisionalcompleter(""ignore""):
>               list(ip.Completer.completions(""idx."", 4))

pandas/tests/indexes/test_base.py:2418: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7f666b2a0a50>
type = None, value = None, traceback = None

    def __exit__(self, type, value, traceback):
        if type is None:
            try:
>               next(self.gen)
E               AssertionError: Caused unexpected warning(s): [('DeprecationWarning', DeprecationWarning('Deprecated since version 0.16.0. Use get_signatures()[...].params'), '/home/vsts/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/jedi/cache.py', 111)]
```
"

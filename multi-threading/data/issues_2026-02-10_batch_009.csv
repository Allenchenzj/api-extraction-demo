id,number,title,user,state,created_at,updated_at,body
647866421,35059,REF: Rename NonFixedVariableWindowIndexer to VariableOffsetWindowIndexer,mroeschke,closed,2020-06-30T05:13:03Z,2020-06-30T16:33:31Z,"xref https://github.com/pandas-dev/pandas/pull/34994#discussion_r447311723
"
647674155,35056,DOC: Alligned docstring for ignore_index in append,erfannariman,closed,2020-06-29T21:13:46Z,2020-06-30T16:59:31Z,"- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
645121426,34983,BUG: HDFStore unable to create colindex w/o error thrown,arw2019,closed,2020-06-25T03:36:26Z,2020-06-30T17:46:22Z,"- [x] closes #28156 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I found this discussion of the `data_columns` argument to `pd.HDFStore.create_table_index` really useful:
https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#query-via-data-columns"
638130683,34741,REGR: merge no longer accepts a list for suffixes keyword,jorisvandenbossche,closed,2020-06-13T09:34:01Z,2020-06-30T20:58:43Z,"see https://github.com/pandas-dev/pandas/pull/34208#issuecomment-643598091

The linked PR changed the `merge` routines to now only accept tuples for the `suffixes` keyword, and no longer general sequence.

This is a breaking change (eg it breaks some GeoPandas functions), but actually also changing documented behaviour: the `merge` docstring indeed says ""tuple"", but eg `merge_asof` still says ""tuple or list"". And in our user guide, we were ourselves using a list in one example, and it explicitly says ""tuple or list"" (see https://pandas.pydata.org/pandas-docs/version/1.0/user_guide/merging.html#overlapping-value-columns)

In general, I am certainly fine with restricting the set of accepted types (eg to avoid the confusing issue with sets, the original report), but this is 1) breaking a documented behaviour and 2) a change that could also easily be done with a deprecation warning. 
And given our versioning policy, if it is easy to do a change with a deprecation, then we should try to do that, IMO.
"
648424683,35065,CLN: type get_resolution tz as tzinfo,jbrockmendel,closed,2020-06-30T19:07:11Z,2020-06-30T21:23:41Z,
648464470,35067,CLN: type tz kwarg in create_timestamp_from_ts,jbrockmendel,closed,2020-06-30T20:17:05Z,2020-06-30T22:09:05Z,Perf is indistinguishable
648505998,35069,ENH: Add option to keep index in `pd.melt`,erfannariman,closed,2020-06-30T21:26:07Z,2020-06-30T22:32:41Z,"When melting a dataframe, there's no option to keep the original index. This can be beneficial for example when users want to do different kind `ravel` than the  deault `F` (Fortran style) right now. To make this more clear, see example:

```python
df = pd.DataFrame({'A': [1, 4],
                   'B': [2, 5],
                   'C': [3, 6]})
print(df)
   A  B  C
0  1  2  3
1  4  5  6

print(df.melt())
  variable  value
0        A      1
1        A      4
2        B      2
3        B      5
4        C      3
5        C      6
```
But the expected output can be row wise instead:
```python
  variable  value
0        A      1
1        B      2
2        C      3
3        A      4
4        B      5
5        C      6
```

**Solution proposal**
This could be solved by adding an `ignore_index` argument which is `True` by default, but if it is set to `False`, the result would come out like:
```python
  variable  value
0        A      1
1        A      4
0        B      2
1        B      5
0        C      3
1        C      6
```
This way the user can sort the index themself and achieve the same result:
```python
print(df.sort_index())
  variable  value
0        A      1
0        B      2
0        C      3
1        A      4
1        B      5
1        C      6
```

Also in `pandas/core/reshape/melt` there is an `# TODO: what about the existing index?` in the `melt` function. This might solve that TODO if I'm not mistaken.
"
648527974,35071,CLN: remove unnecessary get_timezone calls,jbrockmendel,closed,2020-06-30T22:10:02Z,2020-07-01T00:50:32Z,
648535777,35073,REF: de-duplicate month_offset in tslibs.fields,jbrockmendel,closed,2020-06-30T22:27:50Z,2020-07-01T00:51:43Z,
648510647,35070,PERF: _maybe_convert_value_to_local,jbrockmendel,closed,2020-06-30T21:34:39Z,2020-07-01T00:52:20Z,"Also fixes an incorrect asv.

```
In [2]: ts = pd.Timestamp.now(""US/Pacific"")                                                                                                                                                                        

In [3]: %timeit ts.day_name()                                                                                                                                                                                      
9.13 µs ± 309 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)  # <-- PR
26.4 µs ± 1.23 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)   # <-- master
```"
648215073,35062,CLN: collect Timestamp methods,jbrockmendel,closed,2020-06-30T14:17:22Z,2020-07-01T00:57:22Z,
588057308,33027,concat with Misaligned Column Labels and Extension Types loses Dtype,WillAyd,closed,2020-03-25T23:42:54Z,2020-07-01T01:43:26Z,"Was a little surprised by this behavior when combining frames with the string dtype and misaligned labels

```python
In [2]: one = pd.DataFrame([[""a""]], dtype=""string"", columns=[""a""])
In [3]: two = pd.DataFrame([[""b""]], dtype=""string"", columns=[""b""])
In [4]: pd.concat([one, two]).dtypes
Out[4]:
a    object
b    object
dtype: object
```

A similar issue is exhibited with the integer types:

```python
In [13]: one = pd.DataFrame([[1]], dtype=""uint16"", columns=[""a""])
In [14]: two = pd.DataFrame([[2]], dtype=""uint16"", columns=[""b""])
In [15]: pd.concat([one, two]).dtypes
Out[15]:
a    float64
b    float64
dtype: object
```

Occurs on master"
475667618,27692,BUG: Data types are not preserved while concatenating DataFrames with nullable integers,vss888,closed,2019-08-01T12:57:07Z,2020-07-01T01:43:26Z,"#### Copy-pastable example

``` python
# input data
import pandas as pd
t1 = pd.DataFrame(index=[0], data={'x':[1]}, dtype='UInt8')
t2 = pd.DataFrame(index=[1], data={'y':[1]}, dtype='UInt8')
t3 = pd.concat([t1,t2], join='outer', sort=False)

'''actual result'''
print(t3.dtypes)
# x    object
# y    object
# dtype: object
```

#### Problem description

Data types are not preserved data type while concatenating DataFrames with nullable integers. Instead, the result of concatenation has mixed data types and so the column types are `object`:

``` python
>>> type(t3.at[0,'x'])
<class 'int'>
>>> type(t3.at[1,'x'])
<class 'float'>
```

#### Expected Output

``` python
'''expected result'''
print(t3.dtypes)
# x    UInt8
# y    UInt8
# dtype: object
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-862.11.6.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.utf-8
LANG             : en_US.utf-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.0
numpy            : 1.16.3
pytz             : 2018.4
dateutil         : 2.7.3
pip              : 19.1.1
setuptools       : 39.0.1
Cython           : 0.29.12
pytest           : 3.3.2
hypothesis       : None
sphinx           : 1.6.6
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.3.4
html5lib         : 0.9999999
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 6.2.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.0.2
numexpr          : 2.6.4
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.11.1
pytables         : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : None
tables           : 3.5.2
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
648468585,35068,PERF: Timestamp.normalize,jbrockmendel,closed,2020-06-30T20:24:44Z,2020-07-01T01:47:16Z,"The perf is actually a wash here, verging on slightly negative.  The really worthwhile thing is the improved asvs.

The big perf gain is being split into a separate PR that optimizes _maybe_convert_value_to_local to the tune of 40%."
648388450,35064,Assorted ujson Cleanups,WillAyd,closed,2020-06-30T18:05:02Z,2020-07-01T15:23:26Z,"I think it would be great if we could move the block iteration out of JSON as it has generic functionality that we could use down in C. This isn't it, but a small set of cleanups I noticed while looking at that"
648641980,35079,TYP: stronger typing in libtimezones,jbrockmendel,closed,2020-07-01T03:26:34Z,2020-07-01T15:42:29Z,
525126946,29712,CI: Fix clipboard problems,datapythonista,closed,2019-11-19T16:39:20Z,2020-07-01T16:06:59Z,"- [X] closes #29676

Fixes the clipboard problems in the CI. With this PR we're sure they are being run, and they work as expected."
572766056,32331,Local test failure plotting.test_datetimelike.test_ts_plot_with_tz,ShaharNaveh,closed,2020-02-28T13:47:36Z,2020-07-01T16:59:44Z,"After running:
```
(venv-pandas) $ pytest pandas/tests/plotting/test_datetimelike.py -k ""test_ts_plot_with_tz""
```

I got a test failure.

Test log:

<details> 

```
================================================== FAILURES ==================================================
___________________________________ TestTSPlot.test_ts_plot_with_tz['UTC'] ___________________________________

self = <pandas.tests.plotting.test_datetimelike.TestTSPlot object at 0x63f623915bb0>, tz_aware_fixture = 'UTC'

    @pytest.mark.slow
    def test_ts_plot_with_tz(self, tz_aware_fixture):
        # GH2877, GH17173, GH31205, GH31580
        tz = tz_aware_fixture
        index = date_range(""1/1/2011"", periods=2, freq=""H"", tz=tz)
        ts = Series([188.5, 328.25], index=index)
        with tm.assert_produces_warning(None):
            _check_plot_works(ts.plot)
            ax = ts.plot()
            xdata = list(ax.get_lines())[0].get_xdata()
            # Check first and last points' labels are correct
>           assert (xdata[0].hour, xdata[0].minute) == (0, 0)
E           AttributeError: 'numpy.datetime64' object has no attribute 'hour'

pandas/tests/plotting/test_datetimelike.py:57: AttributeError
```

</details>

---

Output of ```pd.show_versions()```:

<details>

INSTALLED VERSIONS
------------------
commit           : 4e5e73e2318a1221eb56ae7e68688278ca87ca6f
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.19.a-1-hardened
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0.dev0+635.g4e5e73e23
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : 0.29.15
pytest           : 5.3.5
hypothesis       : 5.5.4
sphinx           : 2.4.3
blosc            : 1.8.3
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : 0.3.3
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : 0.15.0
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.8
numba            : 0.48.0


</details>

"
649038626,35082,CLN: stronger typing in libtimezones,jbrockmendel,closed,2020-07-01T14:41:49Z,2020-07-01T17:14:03Z,
507988014,29037,Categorical column fails in groupby + transform.,stefansimik,closed,2019-10-16T17:19:41Z,2020-07-01T18:25:40Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

# Create demo data for package delivery
df = pd.DataFrame({'package_id': [1, 1, 1, 2, 2, 3],
                   'status': ['Waiting', 'OnTheWay', 'Delivered', 'Waiting', 'OnTheWay', 'Waiting']})

# Status column: Make ordinal
delivery_status_type = pd.CategoricalDtype(categories=['Waiting', 'OnTheWay', 'Delivered'], ordered=True)
df['status'] = df['status'].astype(delivery_status_type)


# CALCULATE LAST-STATUS FOR EACH PACKAGE

# Way 1: (works OK)
# df['last_status'] = df.groupby('package_id')['status'].transform(lambda x: x.max())

# Way 2: Fails. Let's fix it.
df['last_status'] = df.groupby('package_id')['status'].transform(max)
df
```
#### Problem description
Problem is, that code above fails with error:
```
AttributeError                            Traceback (most recent call last)
<ipython-input-26-174c91615d45> in <module>
----> 1 df['last_status'] = df.groupby('package_id')['delivery_status'].transform(max)
      2 df

~\Miniconda3\lib\site-packages\pandas\core\groupby\generic.py in transform(self, func, *args, **kwargs)
   1015                 # cythonized aggregation and merge
   1016                 return self._transform_fast(
-> 1017                     lambda: getattr(self, func)(*args, **kwargs), func
   1018                 )
   1019 

~\Miniconda3\lib\site-packages\pandas\core\groupby\generic.py in _transform_fast(self, func, func_nm)
   1062         ids, _, ngroup = self.grouper.group_info
   1063         cast = self._transform_should_cast(func_nm)
-> 1064         out = algorithms.take_1d(func()._values, ids)
   1065         if cast:
   1066             out = self._try_cast(out, self.obj)

AttributeError: 'Categorical' object has no attribute '_values'
```

#### Expected Output

Code should work without error (especially Way 2)
and produce the same result as Way 1

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.1
numpy            : 1.16.5
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : None
pytest           : 5.0.1
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

</details>
"
648537441,35074,TST: Test for groupby transform on categorical column,erfannariman,closed,2020-06-30T22:31:58Z,2020-07-01T18:25:49Z,"Added test for groupby transform on categorical column

- [x] closes #29037 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
626494574,34431,BUG: pandas.to_sql raises unexpected column error if data contains -numpy.inf,PiotrBenedysiuk,closed,2020-05-28T13:07:58Z,2020-07-01T18:28:31Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
#### Code Sample, a copy-pastable example

```python
import pandas
import numpy
pandas.DataFrame({'foo': [numpy.inf]}).to_sql('foobar1', engine) #ProgrammingError: inf can not be used with MySQL
pandas.DataFrame({'foo': [-numpy.inf]}).to_sql('foobar2', engine) # OperationalError: (1054, ""Unknown column 'infe0' in 'field list'"")
pandas.DataFrame({'foo': [-numpy.inf], 'infe0':['bar']}).to_sql('foobar3', engine) # inserts (foo:Null, infe0: 'bar') into db

```

#### Problem description

I'd expect consistent behaviour between `numpy.inf` and `-numpy.inf`. Raising ProgrammingError in both cases is clear for the user. Furthermore, the query send should **not** interpret -inf as field `infe0`.

#### Expected Output
`ProgrammingError: inf can not be used with MySQL` in all 3 cases.
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.1.final.0
python-bits: 64
OS: Linux
OS-release: 4.19.76-linuxkit
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: C.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: None
pip: 20.0.2
setuptools: 40.6.2
Cython: None
numpy: 1.15.4
scipy: None
pyarrow: 0.11.1
xarray: None
IPython: 7.13.0
sphinx: None
patsy: None
dateutil: 2.8.1
pytz: 2019.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: 4.4.1
bs4: None
html5lib: None
sqlalchemy: 1.3.11
pymysql: None
psycopg2: None
jinja2: 2.11.2
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
627883445,34493,Test for pd.to_sql column error if data contains -np.inf,arw2019,closed,2020-05-31T03:31:31Z,2020-07-01T18:28:50Z,"- [x] closes #34431 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
648532106,35072,Add test apply dtype,grahamwetzler,closed,2020-06-30T22:19:14Z,2020-07-01T21:11:57Z,"- [x] closes #31466
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
649091948,35084,"TYP: annotations, typing for infer_tzinfo",jbrockmendel,closed,2020-07-01T15:54:22Z,2020-07-02T00:43:11Z,Keeping these orthogonal
598840503,33522,BUG: Fixed concat with reindex and extension types,TomAugspurger,closed,2020-04-13T11:35:47Z,2020-07-02T08:12:21Z,"Closes https://github.com/pandas-dev/pandas/issues/27692
Closes https://github.com/pandas-dev/pandas/issues/33027

I have a larger cleanup planned, but this fixes the linked issues for now."
648993851,35081,BUG: idxmax returns erroneous index from grouped DataFrame,spluque,closed,2020-07-01T13:43:27Z,2020-07-02T14:20:01Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

The following code demonstrates the erroneous output:

```python
import numpy as np
import pandas as pd

N = 12
np.random.seed(1234)
price = np.random.uniform(0, 100, N)
color = np.repeat(list(""ABCD""), 3)
df = pd.DataFrame(dict(price=price, color=color),
                  index=pd.date_range(""2020-01-01"", periods=N,
                                      freq=""100ms"", tz=""UTC""))
df[""color""] = df[""color""].astype(""category"")
```

```python
>>> print(df)
                                      price color
2020-01-01 00:00:00+00:00         19.151945     A
2020-01-01 00:00:00.100000+00:00  62.210877     A
2020-01-01 00:00:00.200000+00:00  43.772774     A
2020-01-01 00:00:00.300000+00:00  78.535858     B
2020-01-01 00:00:00.400000+00:00  77.997581     B
2020-01-01 00:00:00.500000+00:00  27.259261     B
2020-01-01 00:00:00.600000+00:00  27.646426     C
2020-01-01 00:00:00.700000+00:00  80.187218     C
2020-01-01 00:00:00.800000+00:00  95.813935     C
2020-01-01 00:00:00.900000+00:00  87.593263     D
2020-01-01 00:00:01+00:00         35.781727     D
2020-01-01 00:00:01.100000+00:00  50.099513     D
```

```python
color_grp = df.groupby(""color"")
color_idxmax = color_grp.idxmax()
```

```python
>>> print(color_idxmax)
                                 price
color                                 
A     2020-01-01 00:00:00.100000+00:00
B     2020-01-01 00:00:00.300000+00:00
C     2020-01-01 00:00:00.800000+00:00
D     2020-01-01 00:00:00.900000+00:00
```

#### Problem description

Notice the erroneous value printed for the first row in column ""price"" of `color_idxmax`.

#### Expected Output

It should be: `2020-01-01 00:00:00.200000+00:00`

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.6.0-2-amd64
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_CA.UTF-8
LOCALE           : en_CA.UTF-8

pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 44.0.0
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
639526615,34820,BUG: Slicing on non-monotonic DatetimeIndex inconsistencies ,erezinman,closed,2020-06-16T09:46:09Z,2020-07-02T14:39:50Z,"Looked for this bug in the opened issues, but couldn't find it. I work with v.0.25.3, but checked that this also occurs in v.1.0.4.

```python
ser = pd.Series([1, 2, 3], pd.to_datetime(['2010-01-01', '2009-01-01', '2011-01-01']))

# Case 1:
ser['2009-01-01':]
# Out:
# 2009-01-01    2
# 2011-01-01    3
# dtype: int64

# Case 2:
ser['2008-01-01':]
# Out: 
# 2010-01-01    1
# 2009-01-01    2
# 2011-01-01    3
# dtype: int64

# Case 3:
ser[pd.Timestamp('2008-01-01'):]
# Out: 
# KeyError: Timestamp('2008-01-01 00:00:00')
```

#### Problem description

As you can see in the above examples, there are multiple inconsistencies and errors here:
1. The inconsistency in output between cases 2 & 3 (that's perhaps the worst).
2. The irrelevant (and so-very frustrating) error-message in case 3
3. The strange behavior in case 1 (it should've also included the first item).

#### Expected Output

For all three cases:
```python
# 2010-01-01    1
# 2009-01-01    2
# 2011-01-01    3
# dtype: int64
```

If not, at the very least you should do the following:
1. Treat string-timestamps and `pd.Timestamp`s in the same way (perhaps by overriding the slicing in the `DatetimeIndex` to cast strings to `pd.Timestamp`s right away).
2. Give an indicative error message. Could be something like ``""KeyError: Can not perform inexact slicing on an unordered `DatetimeIndex`""``.
3. Show a warning upon slicing that results may be inconsistent due to lack of monotonicity

I also want to add that you could implement an unordered `DatetimeIndex` that will return the expected output I wrote above by holding and ordered index in the background that holds the relative indices (e.g. in the above example ``OrderedDict({'2009-01-01': 1, '2010-01-01': 0, '2011-01-01': 2})``), and all timestamp-related-slicing will be performed on it.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-53-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 0.25.3
numpy            : 1.18.2
pytz             : 2018.3
dateutil         : 2.6.1
pip              : 20.1.1
setuptools       : 45.2.0
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.6.1
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

</details>
"
640997601,34859,BUG: ValueError: 1.0 and 0.0 cannot be cast to bool when using pd.read_csv(),aauss,closed,2020-06-18T08:03:35Z,2020-07-02T15:01:40Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample

```python
import io
import pandas as pd

example_csv = """"""
boolean_column, string_column
1.0, a
0.0, b
,c""""""

csv = io.StringIO(example_csv)
df = pd.read_csv(csv, dtype={""boolean_column"": ""boolean""})
```
```python-traceback
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-2-c698a4bcbc2d> in <module>
      1 csv = io.StringIO(example_csv)
----> 2 df = pd.read_csv(csv, dtype={""boolean_column"": ""boolean""})

c:\users\abbooda\appdata\local\continuum\miniconda3\envs\divi\lib\site-packages\pandas\io\parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)
    674         )
    675 
--> 676         return _read(filepath_or_buffer, kwds)
    677 
    678     parser_f.__name__ = name

c:\users\abbooda\appdata\local\continuum\miniconda3\envs\divi\lib\site-packages\pandas\io\parsers.py in _read(filepath_or_buffer, kwds)
    452 
    453     try:
--> 454         data = parser.read(nrows)
    455     finally:
    456         parser.close()

c:\users\abbooda\appdata\local\continuum\miniconda3\envs\divi\lib\site-packages\pandas\io\parsers.py in read(self, nrows)
   1131     def read(self, nrows=None):
   1132         nrows = _validate_integer(""nrows"", nrows)
-> 1133         ret = self._engine.read(nrows)
   1134 
   1135         # May alter columns / col_dict

c:\users\abbooda\appdata\local\continuum\miniconda3\envs\divi\lib\site-packages\pandas\io\parsers.py in read(self, nrows)
   2035     def read(self, nrows=None):
   2036         try:
-> 2037             data = self._reader.read(nrows)
   2038         except StopIteration:
   2039             if self._first_chunk:

pandas\_libs\parsers.pyx in pandas._libs.parsers.TextReader.read()

pandas\_libs\parsers.pyx in pandas._libs.parsers.TextReader._read_low_memory()

pandas\_libs\parsers.pyx in pandas._libs.parsers.TextReader._read_rows()

pandas\_libs\parsers.pyx in pandas._libs.parsers.TextReader._convert_column_data()

pandas\_libs\parsers.pyx in pandas._libs.parsers.TextReader._convert_tokens()

pandas\_libs\parsers.pyx in pandas._libs.parsers.TextReader._convert_with_dtype()

c:\users\abbooda\appdata\local\continuum\miniconda3\envs\divi\lib\site-packages\pandas\core\arrays\boolean.py in _from_sequence_of_strings(cls, strings, dtype, copy)
    302                 raise ValueError(f""{s} cannot be cast to bool"")
    303 
--> 304         scalars = [map_string(x) for x in strings]
    305         return cls._from_sequence(scalars, dtype, copy)
    306 

c:\users\abbooda\appdata\local\continuum\miniconda3\envs\divi\lib\site-packages\pandas\core\arrays\boolean.py in <listcomp>(.0)
    302                 raise ValueError(f""{s} cannot be cast to bool"")
    303 
--> 304         scalars = [map_string(x) for x in strings]
    305         return cls._from_sequence(scalars, dtype, copy)
    306 

c:\users\abbooda\appdata\local\continuum\miniconda3\envs\divi\lib\site-packages\pandas\core\arrays\boolean.py in map_string(s)
    300                 return False
    301             else:
--> 302                 raise ValueError(f""{s} cannot be cast to bool"")
    303 
    304         scalars = [map_string(x) for x in strings]

ValueError: 1.0 cannot be cast to bool
```

#### Problem description

I would expect that 1.0, 0.0 and None can be cast to a nullable boolean. Using ``pd.read_csv()``  without dtype assignment assigns float64 dtype. Why does ""boolean"" not work?

#### Expected Output

```python
print(df)
>>>   boolean_column  string_column
0            True              a
1           False              b
2            <NA>              c
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : None.None

pandas           : 1.0.4
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.0.post20200616
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
637738006,34731,"BUG: DataFrame.melt gives unexpected result when column ""value"" already exists",erfannariman,closed,2020-06-12T13:08:17Z,2020-07-02T15:26:44Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame({'col':list('ABC'),
                   'value':range(10,16,2)})
print(df, '\n')

dfm = df.melt(id_vars='value')

print(dfm)

  col  value
0   A     10
1   B     12
2   C     14 

  value variable value
0     A      col     A
1     B      col     B
2     C      col     C
```

#### Problem description

When the column `value` already exists,  and we set it as index with `id_vars` it copies the `value` column of the melted dataframe and does not set the ""old"" `value` column of the original dataframe as `id_vars`
#### Expected Output
```python
# note change of column name 'val' instead of 'value'
df = pd.DataFrame({'col':list('ABC'),
                   'val':range(10,16,2)})

dfm = df.melt(id_vars='val')

print(dfm)
   val variable value
0   10      col     A
1   12      col     B
2   14      col     C
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.5.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 47.1.1.post20200604
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
648538467,35075,BENCH: implement asvs for get_resolution,jbrockmendel,closed,2020-06-30T22:34:26Z,2020-07-02T16:22:48Z,
649566470,35093,TYP: types for tz_compare,jbrockmendel,closed,2020-07-02T02:54:43Z,2020-07-02T16:25:05Z,
649205139,35088,CLN: tighter typing in libconversion,jbrockmendel,closed,2020-07-01T18:42:24Z,2020-07-02T16:27:03Z,
649337805,35091,BENCH: implement asvs for ints_to_pydatetime,jbrockmendel,closed,2020-07-01T21:37:44Z,2020-07-02T16:28:10Z,
649135811,35087,PERF: tz_convert/tz_convert_single,jbrockmendel,closed,2020-07-01T17:04:08Z,2020-07-02T17:17:01Z,"Making these follow the same pattern we use elsewhere, we get a perf bump:

```
In [2]: dti = pd.date_range(""2016-01-01"", periods=10000, tz=""US/Pacific"")        
In [3]: %timeit dti.tz_localize(None)                                                                                                                                                                              
91.4 µs ± 3.06 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- PR
102 µs ± 6.43 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- master

In [4]: ts = pd.Timestamp.now(""US/Pacific"")                                                                                                                                                                        
In [5]: %timeit ts.tz_localize(None)                                                                                                                                                                               
17.2 µs ± 307 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)   # <-- PR
19.6 µs ± 233 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)   # <-- PR
```

Next up is making sure we have full asv coverage for tz_convert/tz_convert_single, analogous to #35075.  That can either be separate or added to this PR."
650077460,35096,TYP: type for get_timezone,jbrockmendel,closed,2020-07-02T16:55:11Z,2020-07-02T17:41:02Z,Nearly done with these
610753950,33918,Performance regression in timeseries.DatetimeIndex.time_get,TomAugspurger,closed,2020-05-01T13:35:09Z,2020-07-02T21:05:05Z,"```python
import pandas as pd
import numpy as np

N = 100000
index = pd.date_range(start=""2000"", periods=N, freq=""s"", tz=""US/Eastern"")
%timeit index[0]
```

```
# pandas 1.0.2
12 µs ± 292 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

# master
25.9 µs ± 935 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```

https://pandas.pydata.org/speed/pandas/index.html#timeseries.DatetimeIndex.time_get?p-index_type=%27tz_aware%27&commits=428791c5e01453ff6979b43d37c39c7315c0aaa2-1e5ff233c559ecb2f14fdb9c8d9537db3a02ce1d points to somewhere in `428791c5..1e5ff233`

```
* 1e5ff233c5 - ENH: allow passing freq=None to DatetimeIndex/TimedeltaIndex (#33635) (6 days ago) <jbrockmendel>
* 0907d9ece2 - BUG: DTI/TDI/PI.where accepting incorrectly-typed NaTs (#33715) (6 days ago) <jbrockmendel>
* ee1736a325 - REF: use cached inferred_type when calling lib.infer_dtype(index) (#33537) (6 days ago) <jbrockmendel>
* 23e91a7f3f - TST: groupby-reindex on DTI (#33638) (6 days ago) <CloseChoice>
* cb71376385 - BUG: support skew function for custom BaseIndexer rolling windows (#33745) (6 days ago) <Alex Kirko>
* e008a0a7ac - DOC: Remove ambiguity in fill_value documentation (#33788) (6 days ago) <Bharat Raghunathan>
* e82c0c69fd - CLN: Pass numpy args as kwargs (#33789) (6 days ago) <Daniel Saxton>
* a7741e3ff8 - PERF: op(frame, series) when series is not EA (#33600) (6 days ago) <jbrockmendel>
* ad4465edbd - CLN: remove unused PeriodEngine methods (#33796) (6 days ago) <jbrockmendel>
* 3c09d22f7f - BUG: arg validation in DTA/TDA/PA.take, DTI/TDI/PI.where (#33685) (6 days ago) <jbrockmendel>
* c6a1638bcd - BUG: Series[listlike_of_ints] incorrect on MultiIndex (#33539) (6 days ago) <jbrockmendel>
* 17dc6b0959 - REF: implement test_astype (#33734) (6 days ago) <jbrockmendel>
* c4ebf21e7e - REF: Implement NDArrayBackedExtensionArray (#33660) (6 days ago) <jbrockmendel>
* c60882429e - REF: remove need to override get_indexer_non_unique in DatetimeIndexOpsMixin (#33792) (6 days ago) <jbrockmendel>
* 0163b7941f - CLN: avoid getattr(obj, ""values"", obj) (#33776) (6 days ago) <jbrockmendel>
* 8c1df8d1b4 - BUG: Setting DTI/TDI freq affecting other indexes viewing the same data (#33552) (6 days ago) <jbrockmendel>
* 77a0f19c53 - ENH: Implement IntegerArray.sum (#33538) (6 days ago) <Daniel Saxton>
* f49269f5bf - ENH: Implement StringArray.min / max (#33351) (6 days ago) <Daniel Saxton>
* 31875eb3d8 - BUG: MonthOffset.name (#33757) (7 days ago) <jbrockmendel>
* 92afd5c2c0 - BUG: freq not retained on apply_index (#33779) (7 days ago) <jbrockmendel>
* ea09d504c9 - TST: prepare for freq-checking in tests.io (#33711) (7 days ago) <jbrockmendel>
* 2b6b89280b - TST: pd.NA TypeError in drop_duplicates with object dtype (#33751) (7 days ago) <Simon Hawkins>
* a585b63394 - TST: more accurate freq attr in `expected`s (#33773) (7 days ago) <jbrockmendel>
* f1fe03134d - CLN: simplify DTA.__getitem__ (#33714) (7 days ago) <jbrockmendel>
* 68132f3e0a - REGR: disallow mean of period column again (#33758) (7 days ago) <Joris Van den Bossche>
* 3c8593d0f3 - REF: simplify try_cast (#33764) (7 days ago) <jbrockmendel>
* 70175beb5b - REF: misplaced sort_index tests (#33774) (7 days ago) <jbrockmendel>
* 3df51fcc72 - DOC: data_columns can take bool (#33768) (7 days ago) <Kilian Lieret>
* 930a7f8373 - TYP/CLN: rogue type comment not caught by code checks (#33763) (7 days ago) <Simon Hawkins>
* 1a82659ec6 - TYP: construction (#33725) (7 days ago) <jbrockmendel>
```

(cc @jbrockmendel)"
650256438,35105,Extension base test,tomaszps,closed,2020-07-02T23:40:19Z,2020-07-02T23:43:04Z,"- [x] closes #28488 
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The style is not consistent with guidelines etc, and not at all ready to be merged. 
"
648136009,35061,Fixed #34859: Added support for '0' and '1' in BooleanArray._from_sequence_of_strings method,SanthoshBala18,closed,2020-06-30T12:34:39Z,2020-07-03T05:38:12Z,"- [ ] closes #34859 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
458762085,26970,pprint_thing for sequences,jamesmyatt,closed,2019-06-20T16:42:24Z,2020-07-03T08:50:52Z,"I am using a custom dataclass as in the index of a DataFrame, which works fine. However, since it implements the Sequence protocol, pprint_thing is rendering every element, rather than the object itself. I would rather use the __str__ method that I have written. I know I can avoid this by changing ""pprint_nest_depth"" option to 0, but it would be nice to have a direct way of achieving this.

My main question is why the presence of a ""__next__"" attribute is used as the indicator that the correct output is the result of ""str"" (
https://github.com/pandas-dev/pandas/blob/cfd65e98e694b2ad40e97d06ffdd9096a3dea909/pandas/io/formats/printing.py#L207)

This is related to #17695, #18843 and #22333."
647055191,35049,Performance regression in frame_methods.Quantile.time_frame_quantile,TomAugspurger,closed,2020-06-29T01:45:44Z,2020-07-03T15:07:32Z,"https://pandas.pydata.org/speed/pandas/index.html#frame_methods.Quantile.time_frame_quantile?p-axis=0&commits=ce292485c781ec0df0a765869848b97ac3c88a54 

https://github.com/pandas-dev/pandas/commit/ce292485c781ec0df0a765869848b97ac3c88a54 (an f-string PR). Most likely the formatting is somewhat expensive."
650194052,35101,PERF: Fix quantile perf regression,TomAugspurger,closed,2020-07-02T20:44:16Z,2020-07-03T15:07:43Z,"Closes https://github.com/pandas-dev/pandas/issues/35049

```pyhon
591 µs ± 29.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

440 µs ± 22.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```"
554419057,31262,PLT: Color attributes of medianprops etc are lost in df.boxplot and df.plot.boxplot,charlesdong1991,closed,2020-01-23T21:32:22Z,2020-07-03T16:15:11Z,"- [x] closes #30346 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Found out that not only `medianprops` info is lost, so is `whiskerprops`, `capprops` and `boxprops`, so I also added them alongside. Since this is a visualization PR, so add a screenshot to show what it looks like after the change

<img width=""587"" alt=""Screen Shot 2020-01-24 at 4 16 26 PM"" src=""https://user-images.githubusercontent.com/9269816/73079678-eb468180-3ec4-11ea-9628-5ab13e73f15c.png"">
"
650542958,35110,DOC: Fix code formatting and typos in Series.tz_localize,StefRe,closed,2020-07-03T11:41:52Z,2020-07-03T16:21:10Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The missing line feed prevented the example from being formatted as code sample."
609041512,33868,BUG: RGBA-transparency issue in pandas boxplot,MatthiVH,closed,2020-04-29T13:05:17Z,2020-07-03T17:01:58Z,"When setting color and transparency using rgba-values in pandas.boxplot(), the code doesn't seem to recognize the alpha-value. I raised a question on StackOverflow already (<https://stackoverflow.com/questions/61501795/set-boxplot-boxes-facecolor-transparency-in-python-matplotlib>) but this might be something to fix in the github. I have used rgba-values elsewhere and never had issues with it, only here it doesn't seem to work properly.
"
650772841,35117,CLN: convert lambda to function,topper-123,closed,2020-07-03T21:34:26Z,2020-07-04T15:30:21Z,Minor cleanup: conversion to proper function.
650744025,35115,CI: suppress external warning,jbrockmendel,closed,2020-07-03T19:40:30Z,2020-07-04T17:06:43Z,
361895384,22773,Expose Formatters via API and Improve Documentation,WillAyd,open,2018-09-19T19:31:47Z,2020-07-04T20:38:27Z,"See the discussion in https://github.com/pandas-dev/pandas/pull/22759#discussion_r218821031 - we could potentially expose the formatters via the public API and allow users to subclass them or even assign properties to them which give finer control over what gets exported.

I've had an actual use case for this for at least the `ExcelFormatter` to change the exported header formatting. Haven't had as much of a use case for other Formatters but would love input"
649134167,35086,CLN: Removed unreached else block GH33478,FollowTheProcess,closed,2020-07-01T17:01:14Z,2020-07-05T06:46:06Z,"Unreached else block in method _wrap_applied_output was removed.

Local tests passed before and after change

- [ ] xref #33478 (Not fully closed, this addresses 1 part)
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
628454557,34512,"Update : In DataFrame Class, Columns should not contain duplicate val…",MohakGangwani,closed,2020-06-01T13:52:48Z,2020-07-05T07:53:10Z,Also updated the Docstring.(#12991)
52217764,9096,"Groupby ""negative dimensions are not allowed"" error and bad key behaviour when there are NaNs values.",jordeu,closed,2014-12-17T09:26:06Z,2020-07-06T11:30:42Z,"On a groupby with a composed key if the product of all possible values is bigger than 2^63 we get a `ValueError ""negative dimensions are not allowed""` when we call `len(grouped_data)`.

A simple version to reproduce it:

``` python
values = range(55109)
data = pd.DataFrame.from_dict({'a': values, 'b': values, 'c': values, 'd': values})
grouped = data.groupby(['a', 'b', 'c', 'd'])
len(grouped)
```

A side effect of this error is that if there are NaN values as possible keys it won't ignore them, it will replace the NaN values with some other values present in the index.

Here there is a complete IPython notebook example to reproduce it:
http://nbviewer.ipython.org/gist/jordeu/cd86fc99f5f89451cf93
"
639157546,34810,API: Allow non-tuples in pandas.merge,TomAugspurger,closed,2020-06-15T21:03:27Z,2020-07-06T13:14:06Z,"Closes https://github.com/pandas-dev/pandas/issues/34741, while
retaining the spirit of the spirit of
https://github.com/pandas-dev/pandas/pull/34208."
651565914,35139,CI: pin sphinx <= 3.1.1 for autodoc failure,jorisvandenbossche,closed,2020-07-06T14:03:02Z,2020-07-06T14:35:16Z,xref https://github.com/pandas-dev/pandas/issues/35138
649116625,35085,Fix numpy warning,TomAugspurger,closed,2020-07-01T16:31:59Z,2020-07-06T15:19:43Z,
640480144,34852,CI: Fail on warning for 3.9,TomAugspurger,closed,2020-06-17T14:17:14Z,2020-07-06T15:32:05Z,This should fail till https://github.com/pandas-dev/pandas/pull/34835 is in.
644160069,34961,"REF/INT: never set `df._mgr` after `__init__`, never hold a Block in more than 1 BlockManager",jbrockmendel,closed,2020-06-23T21:31:28Z,2020-07-06T15:39:25Z,"ATM there are a few places where we will do `self._mgr = mgr.reindex_or_whatever(...)`.  I think the code would be easier to reason about if we avoided this pattern.  To do that, we will need the relevant `mgr.reindex_or_whatever` methods to be in-place at the `BlockManager` level.

Similarly I don't think we have strictly-enforced policies describing the relationship between Block objects and BlockManagers.

Potential tightening of rules we could implement:
1) A DataFrame will only ever have a single BlockManager attached to it.
2) A BlockManager will only ever be attached to a single DataFrame.
3) A Block will only ever be attached to a single BlockManager

Thoughts?  Others I'm missing?"
456662526,26887,CI: read_clipboard / to_clipboard not working in Azure,datapythonista,closed,2019-06-16T17:15:32Z,2020-07-06T15:40:56Z,"The clipboard system does not work in Azure Pipelines. I tried the next things:

Installing `xsel` and `xclip` (separately) as suggested in the documentation

Setting up a step in the azure settings based on the travis docs:
```
  - script: |
      set -x
      sudo apt-get install -y xsel xvfb
      export DISPLAY="":99.0""
      sh -e /etc/init.d/xvfb start
      sleep 3
    displayName: 'Setup clipboard'
```

But we still get the same paperclyp error. This is affecting the docs (see #26852) and also the tests (see #26428)."
651705686,35145,AttributeError: 'DataFrame' object has no attribute 'compare' ,RandyBetancourt,closed,2020-07-06T17:27:32Z,2020-07-06T17:42:08Z,"pandas .compare() from the [documentation ](https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.compare.html)returns:

```AttributeError: 'DataFrame' object has no attribute 'compare'```

```
Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas as pd
>>> import numpy as np
>>>
>>> print(pd.__name__, pd.__version__)
pandas 1.0.3
>>>
>>> df = pd.DataFrame(
...     {
...         ""col1"": [""a"", ""a"", ""b"", ""b"", ""a""],
...         ""col2"": [1.0, 2.0, 3.0, np.nan, 5.0],
...         ""col3"": [1.0, 2.0, 3.0, 4.0, 5.0]
...     },
...     columns=[""col1"", ""col2"", ""col3""],
... )
>>> df
  col1  col2  col3
0    a   1.0   1.0
1    a   2.0   2.0
2    b   3.0   3.0
3    b   NaN   4.0
4    a   5.0   5.0
>>>
>>> df2 = df.copy()
>>> df2.loc[0, 'col1'] = 'c'
>>> df2.loc[2, 'col3'] = 4.0
>>> df2
  col1  col2  col3
0    c   1.0   1.0
1    a   2.0   2.0
2    b   3.0   4.0
3    b   NaN   4.0
4    a   5.0   5.0
>>>
>>> df.compare(df2)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\randy\Anaconda3\lib\site-packages\pandas\core\generic.py"", line 5274, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'DataFrame' object has no attribute 'compare'
```"
650118004,35099,TYP: type unit as str,jbrockmendel,closed,2020-07-02T18:10:29Z,2020-07-06T18:33:03Z,
649997808,35095,BUG: reset_index is passing a bad dtype to NumPy,bashtage,closed,2020-07-02T14:58:09Z,2020-07-06T20:55:03Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug **DOES NOT ** exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
index = pd.period_range(""12-1-2000"", periods=2, freq=""Q-DEC"")
df = pd.DataFrame([[1],[2]],index=index)
df = df.reset_index()
```

#### Problem description

Should reset the index.

This is happening on the nightly:

pandas: 1.1.0.dev0+2004.g8d10bfb6f

Full Travis CI log with 4 identical failures:

https://travis-ci.org/github/statsmodels/statsmodels/jobs/704310975

The relevant part of the traceback:

```
statsmodels/tsa/statespace/tests/test_news.py:617: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
statsmodels/tsa/statespace/tests/test_news.py:242: in check_news
    details_by_update = news.details_by_update
statsmodels/base/wrapper.py:36: in __getattribute__
    obj = getattr(results, attr)
statsmodels/tsa/statespace/news.py:392: in details_by_update
    df = df.reset_index()
venv/lib/python3.8/site-packages/pandas/core/frame.py:4800: in reset_index
    level_values = _maybe_casted_values(lev, lab)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
index = PeriodIndex(['2009Q2', '2009Q3', '2009Q4', '2010Q1'], dtype='period[Q-DEC]', name='impact date', freq='Q-DEC')
labels = array([], dtype=int8)
    def _maybe_casted_values(index, labels=None):
        values = index._values
        if not isinstance(index, (PeriodIndex, DatetimeIndex)):
            if values.dtype == np.object_:
                values = lib.maybe_convert_objects(values)
    
        # if we have the labels, extract the values with a mask
        if labels is not None:
            mask = labels == -1
    
            # we can have situations where the whole mask is -1,
            # meaning there is nothing found in labels, so make all nan's
            if mask.all():
>               values = np.empty(len(mask), dtype=index.dtype)
E               TypeError: Cannot interpret 'period[Q-DEC]' as a data type
```


#### Expected Output

```
    index  0
0  2000Q4  1
1  2001Q1  2
```

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
Python: 3.8.1.final.0
OS: Linux 5.0.0-1031-gcp #32-Ubuntu SMP Tue Feb 11 03:55:48 UTC 2020 x86_64
byteorder: little
LC_ALL: en_US.UTF-8
LANG: en_US.UTF-8
statsmodels
===========
Installed: v0.12.0.dev0+519.g502132468 (/home/travis/build/statsmodels/statsmodels/statsmodels)
Required Dependencies
=====================
cython: 3.0a5 (/home/travis/build/statsmodels/statsmodels/venv/lib/python3.8/site-packages/Cython)
numpy: 1.20.0.dev0+4d5b255 (/home/travis/build/statsmodels/statsmodels/venv/lib/python3.8/site-packages/numpy)
scipy: 1.6.0.dev0+fd6f6d6 (/home/travis/build/statsmodels/statsmodels/venv/lib/python3.8/site-packages/scipy)
pandas: 1.1.0.dev0+2004.g8d10bfb6f (/home/travis/build/statsmodels/statsmodels/venv/lib/python3.8/site-packages/pandas)
    dateutil: 2.8.1 (/home/travis/build/statsmodels/statsmodels/venv/lib/python3.8/site-packages/dateutil)
patsy: 0.5.1 (/home/travis/build/statsmodels/statsmodels/venv/lib/python3.8/site-packages/patsy)
Optional Dependencies
=====================
matplotlib: 3.3.0rc1+128.g5e11a2312 (/home/travis/build/statsmodels/statsmodels/venv/lib/python3.8/site-packages/matplotlib)
    backend: TkAgg 
cvxopt: 1.2.5 (/home/travis/build/statsmodels/statsmodels/venv/lib/python3.8/site-packages/cvxopt)
joblib: 0.16.0 (/home/travis/build/statsmodels/statsmodels/venv/lib/python3.8/site-packages/joblib)
Developer Tools
================
IPython: Not installed
    jinja2: Not installed
sphinx: Not installed
    pygments: Not installed
pytest: 5.4.3 (/home/travis/build/statsmodels/statsmodels/venv/lib/python3.8/site-packages/pytest)
virtualenv: Not installed
```
</details>
"
327222495,21239,Wrong behaviour of None/np.nan in 'category' type columns,topolskib,closed,2018-05-29T08:42:11Z,2020-07-06T21:28:08Z,"#### Code Sample

```python
import pandas as pd

df = pd.DataFrame({'a': ['asd', None, 12, 'asd', 'cde']}, dtype='category')
print(df['a'].apply(lambda x: x=='cde'))
```
```
0    False
1     True
2    False
3    False
4     True
Name: a, dtype: object
```
#### Problem description

`None` (or `np.nan`) is not properly transformed by function used in `apply` method. From what I understand, this is caused by the fact that `apply` transforms levels into list of new values (in this case, boolean value indicating if the value is equal 'cde'), and then uses each categorical code as an index to that list. However, in `df['a'].cat.codes` you can see that `None`'s code is `-1`, so it returns last element of this new list - which in this case is `True`.

#### Expected Output

```
0    False
1     None
2    False
3    False
4     True
Name: a, dtype: object
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.12.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-124-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.23.0
pytest: 3.4.1
pip: 9.0.1
setuptools: 38.5.1
Cython: None
numpy: 1.14.3
scipy: 0.18.1
pyarrow: None
xarray: None
IPython: 5.5.0
sphinx: 1.7.1
patsy: None
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.1.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: 4.6.0
html5lib: 0.9999999
sqlalchemy: 1.1.6
pymysql: None
psycopg2: 2.6 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
650954172,35125,TST: Add test for category equalness on applies (#21239),gabrielNT,closed,2020-07-04T20:51:02Z,2020-07-06T21:28:17Z,"- [x] closes #21239
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The test checks if equalness comparisons through `df.apply()` works correctly for the categorical `dtype`, especially with `np.NaN` and `None` values."
651200889,35133,CLN: unused imports in tslibs,jbrockmendel,closed,2020-07-06T02:15:29Z,2020-07-06T23:15:31Z,
650265346,35107,TYP: get_utcoffset,jbrockmendel,closed,2020-07-03T00:15:24Z,2020-07-06T23:20:04Z,
478849657,27837,"API: what should a 2D indexing operation into a 1D Index do? (eg idx[:, None])",jorisvandenbossche,closed,2019-08-09T07:53:50Z,2020-07-06T23:25:28Z,"Follow-up on https://github.com/pandas-dev/pandas/issues/27775 and https://github.com/pandas-dev/pandas/pull/27818.

Short recap of what those issues were about:

Currently, indexing into an Index with a 2D (or multiple D) indexer results in an ""invalid"" Index with an underlying ndarray:

```
In [1]: idx = pd.Index([1, 2, 3])  

In [2]: idx2 = idx[:, None] 

In [3]: idx2
Out[3]: Int64Index([1, 2, 3], dtype='int64')

In [4]: idx2.values
Out[4]: 
array([[1],
       [2],
       [3]])
```
So from the repr it *looks* like a proper index, but the underlying values of an Index should always be 1D (such an invalid index will also lead to errors once you do operations on them).

Before pandas 0.25.0, the `shape` attribute of the index ""correctly"" returned the shape of the underlying values: `(3, 1)`, but in 0.25.0 this was changed to `(3,)` (only checking the length). This caused a regression matplotlib (https://github.com/pandas-dev/pandas/issues/27775), and will be ""fixed"" in 0.25.1 returning again the 2D shape of the underlying values (https://github.com/pandas-dev/pandas/pull/27818). Of course, this is only about the `shape` attribute, while the root cause is this invalid Index.

I think it is clear that we should not allow such invalid Index object to exist. 
I currently know of two ways to end up such situation:

* Passing a multidimensional array to the Index constructor (e.g. `pd.Index(np.random.randn(5, 5, 5))`. I think this is something we can deprecate and raise for later, and there is already an issue for this: https://github.com/pandas-dev/pandas/issues/27125
* Indexing into an Index (e.g. `idx[:, None] `) -> this issue

So let's use **this issue** to discuss what to do for this second way: **a 2D indexing operation on a 1D object.**

This is relevant for the Index, but we should probably try to have it consistent with Series as well."
651619399,35141,DEPR: Deprecate n-dim indexing for Series,TomAugspurger,closed,2020-07-06T15:14:46Z,2020-07-06T23:25:32Z,"Closes https://github.com/pandas-dev/pandas/issues/27837
"
492551448,28397,Alignment of CategoricalIndex will convert the index to int type?,bingtangben,closed,2019-09-12T02:29:36Z,2020-07-06T23:27:27Z,"#### Code Sample 

```python
import pandas as pd

idx1 = pd.Categorical(['A1', 'A2', 'A2'], categories=['A1', 'A2'])
idx2 = pd.Categorical(['A2', 'A1', 'A1'], categories=['A1', 'A2'])

tmp1 = pd.DataFrame(np.random.randn(6).reshape(3, 2), index=idx1)
tmp2 = pd.DataFrame(np.random.randn(6).reshape(3, 2), index=idx2)

tmp1 * tmp2
```
The result:
```python
In [1]: tmp1
Out[1]: 
           0         1
A1 -0.880693  0.794644
A2 -0.671629  1.145027
A2  0.305152  0.379116

In [2]: tmp2
Out[2]: 
           0         1
A2  0.434122 -0.966894
A1 -0.162195  0.499271
A1  0.840569  0.043832

In [3]: tmp1 * tmp2
Out[3]: 
          0         1
0  0.142844  0.396742
0 -0.740283  0.034831
1 -0.291569 -1.107120
1  0.132473 -0.366565

In [4]: pd.__version__
Out[4]: '0.24.2'
```
#### Problem description

When getting multiplication of two DataFrames with CategoricalIndex based on the same categories, the result will convert the CategoricalIndex to the Int one. The problem may be from dataframe alignment. 
```python
In [7]: tmp1_align, tmp2_align =tmp1.align(tmp2)

In [8]: tmp1_align
Out[8]: 
          0         1
0  -0.880693  0.794644
0  -0.880693  0.794644
1 -0.671629  1.145027
1  0.305152  0.379116
```
I think the simple binary operations should preserve the index type for categorical index with the same categories. However, it doesn't. Then I find sometimes the operation preserve the categorical index type but convert to object plain-text index which cost large memory. So I am confused about it and haven't found any explaination yet.

#### Expected Output
The result with categorical index like this:
```python
         0         1
A1  0.142844  0.396742
A1 -0.740283  0.034831
A2 -0.291569 -1.107120
A2  0.132473 -0.366565
```
#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en
LOCALE: None.None

pandas: 0.24.2
pytest: 5.0.1
pip: 19.1.1
setuptools: 41.0.1
Cython: 0.29.12
numpy: 1.16.4
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 7.6.1
sphinx: 2.1.2
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: 1.2.1
tables: 3.5.2
numexpr: 2.6.9
feather: None
matplotlib: 3.1.0
openpyxl: 2.6.2
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.8
lxml.etree: 4.3.4
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.3.5
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
642338852,34880,TST: Add test to verify align behaviour on CategoricalIndex,MBrouns,closed,2020-06-20T08:53:23Z,2020-07-06T23:27:32Z,"verify that aligning two dataframes with a `CategoricalIndex` does not change the type of the index.

- [x] closes #28397
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry (not applicable?)
"
650703828,35113,PERF: ints_to_pydatetime,jbrockmendel,closed,2020-07-03T17:18:38Z,2020-07-07T00:15:07Z,"This refactors ints_to_pydatetime to use a much more concise pattern (similar to #35077 but without the helper function refactored out) that I intend to move all of the ~7 functions using get_dst_info to use.  In the process, we are slower on very small arrays and faster on bigger arrays, which I think is a good trade:

```
% asv continuous -E virtualenv -f 1.01 master HEAD -b time_ints_to_pydatetime
[...]
       before           after         ratio
     [a4e19fa5]       [2492aeb4]
     <master>         <ref-ints_to_pydatetime-3>
+     3.95±0.09μs       8.22±0.5μs     2.08  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('timestamp', 0, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
+      4.80±0.2μs       9.53±0.5μs     1.99  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('timestamp', 0, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
+      5.45±0.3μs       9.01±0.2μs     1.65  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('time', 0, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
+      5.31±0.6μs       8.31±0.3μs     1.57  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('datetime', 0, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
+        6.08±1μs       8.84±0.2μs     1.45  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('datetime', 0, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
+      7.00±0.3μs       10.0±0.3μs     1.44  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('timestamp', 1, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
+      7.17±0.1μs       10.3±0.3μs     1.43  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('timestamp', 1, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
+      7.47±0.1μs       9.77±0.2μs     1.31  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('time', 1, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
+      7.08±0.2μs       8.46±0.5μs     1.20  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('time', 1, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
-      10.7±0.6μs       9.48±0.3μs     0.88  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('time', 0, datetime.timezone(datetime.timedelta(seconds=3600)))
-     3.38±0.02μs      2.89±0.06μs     0.85  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('date', 0, None)
-        74.3±1μs         56.3±1μs     0.76  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('timestamp', 100, datetime.timezone.utc)
-        73.4±4μs         54.7±2μs     0.74  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('timestamp', 100, None)
-        20.7±3ms         9.31±2ms     0.45  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('timestamp', 10000, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
-      15.4±0.2ms         6.23±1ms     0.40  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('datetime', 10000, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
-      1.75±0.03s          699±5ms     0.40  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('timestamp', 1000000, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
-      1.56±0.05s         614±20ms     0.39  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('datetime', 1000000, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
-      1.65±0.01s         650±10ms     0.39  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('timestamp', 1000000, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
-        156±10μs         57.3±9μs     0.37  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('datetime', 100, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
-        16.8±1ms       5.95±0.2ms     0.36  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('timestamp', 10000, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
-         164±7μs       58.3±0.8μs     0.36  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('datetime', 100, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
-      14.8±0.1ms       5.14±0.5ms     0.35  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('datetime', 10000, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
-      1.42±0.02s         491±20ms     0.34  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('datetime', 1000000, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
-         169±6μs         56.0±3μs     0.33  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('time', 100, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
-        193±30μs       63.1±0.9μs     0.33  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('timestamp', 100, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
-        229±30μs         73.7±3μs     0.32  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('timestamp', 100, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
-      1.65±0.06s         505±20ms     0.31  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('time', 1000000, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
-         159±3μs         46.1±2μs     0.29  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('time', 100, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
-        16.0±1ms       4.64±0.1ms     0.29  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('time', 10000, <DstTzInfo 'US/Pacific' LMT-1 day, 16:07:00 STD>)
-      15.2±0.2ms       3.78±0.2ms     0.25  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('time', 10000, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
-       1.65±0.1s         408±20ms     0.25  tslibs.tslib.TimeIntsToPydatetime.time_ints_to_pydatetime('time', 1000000, tzfile('/usr/share/zoneinfo/Asia/Tokyo'))
```"
640150072,34839,ENH: Add support for calculating EWMA with a time component,mroeschke,closed,2020-06-17T05:21:43Z,2020-07-07T00:31:46Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry


This enhancement allows EWMA to be calculated relative to the timestamp in which an observation occurs instead of assuming each observation is equally spaced."
650816559,35119,REF: make ccalendar self-contained,jbrockmendel,closed,2020-07-04T03:03:43Z,2020-07-07T01:04:23Z,
650240307,35104,CLN: tz_convert is always from UTC,jbrockmendel,closed,2020-07-02T22:42:52Z,2020-07-07T01:05:26Z,"Follow-up can remove the unnecessary arg from tz_convert, possibly rename.
"
650209566,35102,REF: standardize tz_convert_single usage,jbrockmendel,closed,2020-07-02T21:19:33Z,2020-07-07T01:06:17Z,"I'm increasingly convinced that we can get all tz_convert/tz_convert_single usages to have tz1=UTC, and everything else should go through tz_localize_to_utc."
650264566,35106,"PERF: tz_localize(None) from dst, asvs",jbrockmendel,closed,2020-07-03T00:12:40Z,2020-07-07T01:06:58Z,"This makes tz_convert_dst follow the same pattern we use elsewhere, and brings a perf improvement along with it.


```
In [2]: dti = pd.date_range(""2016-01-01"", periods=10000, freq=""S"", tz=""US/Pacific"")                                                                                                                          

In [3]: %timeit dti.tz_localize(None)                                                                                                                                                                        
89.3 µs ± 627 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- PR
100 µs ± 6.36 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- master
```

The boost comes from not doing a copy, so makes a bigger difference in the scalar case than the array case.

```
In [5]: ts = dti[0]                                                                                                                                                                                          

In [6]: %timeit ts.tz_localize(None)                                                                                                                                                                         
12.4 µs ± 188 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)   # <-- PR
16.1 µs ± 215 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)   # <-- master
```

The improvement is much bigger for fixed offsets:

```
In [9]: tz = pytz.FixedOffset(-60)                                                                                                                                                                           
In [10]: dti2 = dti.tz_convert(tz)                                                                                                                                                                           

In [11]: %timeit dti2.tz_localize(None)                                                                                                                                                                      
34.8 µs ± 1.09 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- PR
71.8 µs ± 2.99 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- master
```"
650326330,35108,REF: implement tz_localize_to_utc_single,jbrockmendel,closed,2020-07-03T03:59:31Z,2020-07-07T01:08:21Z,"In addition to implementing `tz_localize_to_utc_single`, this replaces all (but one, xref #35102) usages of tz_convert_single that have UTC as the target timezone.  In conjunction with 35102, this will allow us to remove the second kwarg altogether, similar to #35104."
651066501,35128,CLN: remove the circular import in NDFrame.dtypes,MarcoGorelli,closed,2020-07-05T13:04:40Z,2020-07-07T07:24:17Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
650591539,35111,BUG: reset_index is passing a bad dtype to NumPy,simonjayhawkins,closed,2020-07-03T13:16:44Z,2020-07-07T08:26:07Z,"- [ ] closes #35095
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
642372790,34896,BUG: Fixes plotting with nullable integers (#32073),cvanweelden,closed,2020-06-20T13:06:32Z,2020-07-07T09:32:47Z,"- [x] closes #32073
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Similar to #32410 this tries to fix plotting with nullable integer extension data types by casting them to float before passing on to matplotlib."
505469024,28903,groupby boxplot example does not work,ingrid88,closed,2019-10-10T19:35:31Z,2020-07-07T11:09:10Z,"#### Problem description
Code found here: 
https://github.com/pandas-dev/pandas/blob/v0.25.1/pandas/plotting/_core.py#L424

Example from https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.boxplot.html does not work properly. 

#### Code Sample

```python
import itertools
tuples = [t for t in itertools.product(range(1000), range(4))]
index = pd.MultiIndex.from_tuples(tuples, names=['lvl0', 'lvl1'])
data = np.random.randn(len(index),4)
df = pd.DataFrame(data, columns=list('ABCD'), index=index)
grouped = df.groupby(level='lvl1')
boxplot_frame_groupby(grouped)
grouped = df.unstack(level='lvl1').groupby(level=0, axis=1)
boxplot_frame_groupby(grouped, subplots=False)
```

#### Resulting Error 
```python
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-49-1a28c2f7125d> in <module>
      5 df = pd.DataFrame(data, columns=list('ABCD'), index=index)
      6 grouped = df.groupby(level='lvl1')
----> 7 boxplot_frame_groupby(grouped)
      8 grouped = df.unstack(level='lvl1').groupby(level=0, axis=1)
      9 boxplot_frame_groupby(grouped, subplots=False)

NameError: name 'boxplot_frame_groupby' is not defined
```"
623690131,34343,Added missing import in boxplot_frame_groupby example,gsyqax,closed,2020-05-23T16:05:51Z,2020-07-07T11:09:15Z,"- [X] closes #28903 
- [x] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
641542046,34865,DOC: Move API breaking to appropriate sections,TomAugspurger,closed,2020-06-18T20:52:33Z,2020-07-07T11:50:51Z,"As discussed in https://github.com/pandas-dev/pandas/issues/34801, this moves some items from our ""API breaking changes"" section to either bug fixes or enhancements. I've only moved the ones we can hopefully get consensus on as bug fix / enhancement. But let me know if any of these need further discussion on whether they're an API breaking change.
"
638428318,34776,"CI: move 3.6,slow or 3.6 locale builds from travis to azure",jreback,closed,2020-06-14T20:52:16Z,2020-07-07T13:10:07Z,we have capacity on azure (we are using 8/10) and slightly overloaded on travis.
647431526,35053,Performance regression in replace.Convert.time_replace,TomAugspurger,closed,2020-06-29T14:38:01Z,2020-07-07T13:37:52Z,"https://github.com/pandas-dev/pandas/pull/34999 caused a 2x slowdown in replace.Convert.time_replace (https://pandas.pydata.org/speed/pandas/index.html#replace.Convert.time_replace?p-constructor=%27Series%27&p-replace_data=%27Timestamp%27&commits=dbfbef7eaea7f23a42d9a65344b5745fc055f5b4)

"
638393222,34771,PERF: DataFrameGroupBy var and std,rhshadrach,closed,2020-06-14T17:18:24Z,2020-07-07T13:39:22Z,"Performance on wide and short frames decreased due to #34372, timings are in the OP there. std is computed by calling var."
650726495,35114,BUG: get_loc with time object matching NaT micros,jbrockmendel,closed,2020-07-03T18:37:22Z,2020-07-07T14:30:16Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
637583429,34728,PERF: interpolate_1d returns function to apply columnwise,simonjayhawkins,closed,2020-06-12T08:32:14Z,2020-07-07T14:41:52Z,"This doesn't provide a significant improvement for the existing asv due to the bulk of the time creating python sets which is in the function applied columnwise. see #34727

even without #34727 this could provide perf improvements for other index types ( and for unsorted indexes with creating a column-wise function for the numpy call.

will look at adding asvs for these cases."
612557156,33997,BUG:,kevinksyTRD,closed,2020-05-05T12:15:50Z,2020-07-07T17:52:48Z,"- [O ] I have checked that this issue has not already been reported.

- [O ] I have confirmed this bug exists on the latest version of pandas.

- [O ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas
```

#### Problem description
I cannot import pandas because of the Attribute error. AttributeError: partially initialized module 'pandas' has no attribute 'plotting' (most likely due to a circular import)

#### Expected Output
No error.
#### Output of ``pd.show_versions()``
Couldn't do it because it could not be imported. O a,
<details>
I didn't get the error when I installed version 1.0.1.
Below is the error message.

 import pandas
  File ""C:\Users\user\app\venv\lib\site-packages\pandas\__init__.py"", line 55, in <module>
    from pandas.core.api import (
  File ""C:\Users\user\app\venv\lib\site-packages\pandas\core\api.py"", line 29, in <module>
    from pandas.core.groupby import Grouper, NamedAgg
  File ""C:\Users\user\app\venv\lib\site-packages\pandas\core\groupby\__init__.py"", line 1, in <module>
    from pandas.core.groupby.generic import DataFrameGroupBy, NamedAgg, SeriesGroupBy
  File ""C:\Users\user\app\venv\lib\site-packages\pandas\core\groupby\generic.py"", line 60, in <module>
    from pandas.core.frame import DataFrame
  File ""C:\Users\user\app\venv\lib\site-packages\pandas\core\frame.py"", line 124, in <module>
    from pandas.core.series import Series
  File ""C:\Users\user\app\venv\lib\site-packages\pandas\core\series.py"", line 122, in <module>
    class Series(base.IndexOpsMixin, generic.NDFrame):
  File ""C:\Users\user\app\venv\lib\site-packages\pandas\core\series.py"", line 4562, in Series
    plot = CachedAccessor(""plot"", pandas.plotting.PlotAccessor)
AttributeError: partially initialized module 'pandas' has no attribute 'plotting' (most likely due to a circular import)
</details>
"
610811967,33923,Performance regression in Index.union,TomAugspurger,closed,2020-05-01T15:33:25Z,2020-07-07T20:15:44Z,"Setup

```python
import numpy as np
import pandas as pd

dtype = ""datetime""
method = ""union""

N = 10 ** 5
left = pd.date_range(""1/1/2000"", periods=N, freq=""T"")
right = left[:-1]

%timeit left.union(right)
```

```
# 1.0.2
161 µs ± 7.46 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)

# master
296 µs ± 5.58 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```


https://pandas.pydata.org/speed/pandas/index.html#index_object.SetOperations.time_operation?p-dtype=%27datetime%27&p-method=%27union%27&commits=428791c5e01453ff6979b43d37c39c7315c0aaa2-1e5ff233c559ecb2f14fdb9c8d9537db3a02ce1d points to somewhere in https://github.com/pandas-dev/pandas/compare/428791c5e01453ff6979b43d37c39c7315c0aaa2...1e5ff233c559ecb2f14fdb9c8d9537db3a02ce1d

Maybe c60882429e? (cc @jbrockmendel) Haven't looked closely yet."
650873398,35120,"TYP, DOC, CLN:SeriesGroupBy._wrap_applied_output",MarcoGorelli,closed,2020-07-04T11:14:26Z,2020-07-07T20:18:13Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
646492006,35021,CI: add validate_unwanted_patterns to known_third_parties,MarcoGorelli,closed,2020-06-26T20:22:22Z,2020-07-07T20:18:39Z,"- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

By using `seed-isort-config`, the `known_third_party` setting is automatically populated and alphabetically sorted. This is a cleaner solution which doesn't require manually adding third parties to `setup.cfg`.

Furthermore, it catches an existing inconsistency: currently, `validate_docstrings` is set as a known third party, while `validate_unwanted_patterns` isn't. Neither should be, IMO."
611451661,33955,BUG: Not properly checking if post_processing function is callable or not in _get_cythonized_result() function in class GroupBy,KenilMehta,closed,2020-05-03T16:30:59Z,2020-07-07T20:55:43Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.



#### Problem description
While looking at the code, I found this line which I think is a bug. In the current master branch, pandas/core/groupby/groupby.py file, in the _get_cythonized_result() function :

```
        if post_processing:
            if not callable(pre_processing):
                raise ValueError(""'post_processing' must be a callable!"")
```

While checking if post_processing is callable or not, we are actually passing pre_processing object instead of post_processing. This looks like a bug.

#### Expected Code
```
        if post_processing:
            if not callable(post_processing):
                raise ValueError(""'post_processing' must be a callable!"")
```
"
651852619,35151,CI: move py36 slow to azure (#34776),fangchenli,closed,2020-07-06T22:18:18Z,2020-07-07T22:27:19Z,"- [x] closes #34776
- [x] 0 tests added / 0 passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
356556765,22582,Update ecosystem.rst to include Pint,znicholls,closed,2018-09-03T16:13:15Z,2020-07-07T22:42:02Z,"We are working on upgrading pint to be compatible with pandas, see https://github.com/hgrecco/pint/pull/684

I am guessing that the line in the docs,

> If you’re building a library that implements the interface, please publicize it on Extension Data Types.

meant something like this pull request. If that's completely wrong, apologies.

- [x] closes #xxxx (N/A as not directly related to an issue, but makes progress towards #10349 )
- [x] (N/A) tests added / passed 
- [x] (N/A) passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff` *
- [x] whatsnew entry
"
511279183,29181,ImportError: Can't determine version for bottleneck,mlangiu,closed,2019-10-23T12:08:32Z,2020-07-08T06:10:58Z,"```
conda create -n PANDAS pandas
activate PANDAS
python
```
```python
import pandas
```
Gives me
```python
ImportError: Can't determine version for bottleneck
```

#### Output of conda list
<details>
# Name                    Version                   Build  Channel
blas                      1.0                         mkl

ca-certificates           2019.10.16                    0

certifi                   2019.9.11                py37_0

icc_rt                    2019.0.0             h0cc432a_1

intel-openmp              2019.4                      245

mkl                       2019.4                      245

mkl-service               2.3.0            py37hb782905_0

mkl_fft                   1.0.14           py37h14836fe_0

mkl_random                1.1.0            py37h675688f_0

numpy                     1.16.5           py37h19fb1c0_0

numpy-base                1.16.5           py37hc3f5095_0

openssl                   1.1.1d               he774522_3

pandas                    0.25.2           py37ha925a31_0

pip                       19.3.1                   py37_0

python                    3.7.4                h5263a28_0

python-dateutil           2.8.0                    py37_0

pytz                      2019.3                     py_0

setuptools                41.4.0                   py37_0

six                       1.12.0                   py37_0

sqlite                    3.30.0               he774522_0

vc                        14.1                 h0510ff6_4

vs2015_runtime            14.16.27012          hf0eaf9b_0

wheel                     0.33.6                   py37_0

wincertstore              0.2                      py37_0

</details>
"
628990791,34526,BUG: Select an all-zero sparse series by indexer results in NaN,HYChou0515,closed,2020-06-02T08:03:46Z,2020-07-08T11:47:08Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import scipy as sp
import scipy.sparse

spmatrix = sp.sparse.csr_matrix((2, 2))
spmatrix[0, 0] = 1

df = pd.DataFrame.sparse.from_spmatrix(spmatrix)

# work as expected, cool
df.loc[1]
# 0    0.0
# 1    0.0
# Name: 0, dtype: Sparse[float64, 0.0]

# Indexing by index list. I expect 1,0, 0.0, not cool
df.loc[[1]]
#      0   1
# 1  0.0 NaN

# whenever indexing an all-zero series, it returns Nan.
df.loc[[1]].loc[[1]] 
#     0   1
# 1 NaN NaN

# dtype is good
df.loc[[1]].dtypes
# 0    Sparse[float64, 0.0]
# 1    Sparse[float64, 0.0]
# dtype: object

# indexing by bool list also not cool
df.loc[[True, True]]
#      0   1
# 0  1.0 NaN
# 1  0.0 NaN

# Slicing is cool
df.loc[1:2]
#      0    1
# 1  0.0  0.0


```

#### Problem description

Index slice an all-zero sparse series by indexer (whether by bool array or index array) results in NaN. It should be the `fill_value` of the series.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-101-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```

</details>
"
629651215,34540,BUG: Index slice an all-zero float sparse series by indexer does not preserve dtypes,HYChou0515,closed,2020-06-03T02:51:33Z,2020-07-08T11:47:09Z,"#### Code Sample, a copy-pastable example

```python
import pandas as pd
import scipy as sp
import scipy.sparse

spmatrix = sp.sparse.csr_matrix((2, 2))
spmatrix[0, 0] = 1

df = pd.DataFrame.sparse.from_spmatrix(spmatrix)

# work as expected, cool
df.loc[1]
# 0    0.0
# 1    0.0
# Name: 0, dtype: Sparse[float64, 0.0]

# Indexing by index list. I expect 1,0, 0.0, not cool
df.loc[[1]]
#      0   1
# 1  0.0 0

# whenever indexing an all-zero series, it returns int type.
df.loc[[1]].loc[[1]] 
#    0  1
# 1  0  0

# checking the dtype, it has been turned into int64 instead of float64
df.loc[[1]].dtypes
# 0    Sparse[float64, 0]
# 1      Sparse[int64, 0]
# dtype: object

# indexing by bool list also not cool
df.loc[[True, True]]
#      0   1
# 0  1.0 0
# 1  0.0 0

# Slicing is cool
df.loc[0:2]
#      0    1
# 0  1.0  0.0
# 1  0.0  0.0

# If we explicitly specify its dtype, the type still not preserved.
import numpy as np
df.astype(np.float32).dtypes
# 0    Sparse[float32, 0.0]
# 1    Sparse[float32, 0.0]
df.astype(np.float32).loc[[1]].dtypes
# 0    Sparse[float64, 0.0]
# 1    Sparse[float64, 0.0]

```

#### Problem description

Index slice an all-zero sparse series by indexer (whether by bool array or index array) results in type int64. It should preserve type.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : 57034774bc16958b085ad6ca6d5379b9ac0289b3
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-101-generic
Version          : #102-Ubuntu SMP Mon May 11 10:07:26 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0.dev0+1742.g5703477
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 41.2.0
Cython           : 0.29.19
pytest           : 5.4.3
hypothesis       : 5.16.0
sphinx           : 3.0.4
blosc            : 1.9.1
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fastparquet      : 0.4.0
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.49.1
```

</details>
"
642401911,34908,BUG: incorrect type when indexing sparse dataframe with iterable,suvayu,closed,2020-06-20T16:04:08Z,2020-07-08T11:47:20Z,"closes #34526
closes #34540 
- [x] 2 tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The problem arose when indexing with an iterable.  If the result consisted of columns which originally only had sparse values, the `dtype` was solely inferred from the `fill_value`, which defaults to `0`.  Hence, the resulting columns have the dtype ` Sparse[int64, 0]`.   This commit changes the type inference logic to use the numpy type promotion rules between the underlying subtype of the `SparseArray.dtype` and the type of the fill value."
647053740,35047,Performance regression in frame_methods.Apply.time_apply_ref_by_name,TomAugspurger,closed,2020-06-29T01:39:29Z,2020-07-08T12:40:46Z,"https://pandas.pydata.org/speed/pandas/index.html#frame_methods.Apply.time_apply_ref_by_name?commits=91802a9ae400830f9eaadd395f6a9b40cdd92ee5

https://github.com/pandas-dev/pandas/commit/91802a9ae400830f9eaadd395f6a9b40cdd92ee5 (cc @jbrockmendel)

Also affects other benchmarks that probably use apply: https://pandas.pydata.org/speed/pandas/index.html#frame_methods.Nunique.time_frame_nunique?commits=91802a9ae400830f9eaadd395f6a9b40cdd92ee5.
"
652579520,35166,Fixed Series.apply performance regression,TomAugspurger,closed,2020-07-07T19:38:58Z,2020-07-08T12:40:56Z,"Set the option once, rather than in the loop.

Closes https://github.com/pandas-dev/pandas/issues/35047


```python
import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(1000, 3), columns=list(""ABC""))
%timeit df.apply(lambda x: x[""A""] + x[""B""], axis=1)
```

```
# 1.0.4
22.5 ms ± 4.73 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

# PR
10.4 ms ± 714 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```"
646287614,35015,BUG: MultiIndex columns containing datetime cannot be used (regression from 1.0.5 to master),Dr-Irv,closed,2020-06-26T14:06:11Z,2020-07-08T12:43:19Z,"- [x] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
In [1]: import pandas as pd

In [2]: pd.__version__
Out[2]: '1.1.0.dev0+1967.gf5b2e5a9a'

In [3]: expected_column_index = pd.MultiIndex.from_tuples(
   ...:     [(pd.to_datetime(""02/29/2020""), pd.to_datetime(""03/01/2020""))],
   ...:     names=[
   ...:         'a', 'b'])

In [4]: pd.DataFrame([], columns=expected_column_index)
---------------------------------------------------------------------------
InvalidIndexError
```
#### Problem description

This works in 1.0.5, but not in master.

See discussion in #34748 .  This new issue should be resolved before that PR moves forward.

#### Expected Output

```python
In [1]: import pandas as pd

In [2]: pd.__version__
Out[2]: '1.0.5'

In [3]:  expected_column_index = pd.MultiIndex.from_tuples(
   ...:        [(pd.to_datetime(""02/29/2020""), pd.to_datetime(""03/01/2020""))],
   ...:        names=[
   ...:            'a', 'b'])

In [4]: pd.DataFrame([], columns=expected_column_index)
Out[4]:
Empty DataFrame
Columns: [(2020-02-29 00:00:00, 2020-03-01 00:00:00)]
Index: []
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : f5b2e5a9a2431165b1e8002219c5d3962d7bc920
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 13, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.0.dev0+1967.gf5b2e5a9a
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 42.0.2.post20191201
Cython           : 0.29.20
pytest           : 5.4.3
hypothesis       : 5.18.0
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : 0.4.0
gcsfs            : None
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pyxlsb           : 1.0.6
s3fs             : 0.4.2
scipy            : 1.3.1
sqlalchemy       : 1.3.17
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.46.0
</details>
"
655493097,35255,REF: make tz_convert match pattern elsewhere,jbrockmendel,closed,2020-07-12T23:00:59Z,2020-07-13T20:13:24Z,asvs for tslibs.tz_convert show this as perf-neutral
655206636,35230,TST verify return none inplace in tests/indexing,r-toroxel,closed,2020-07-11T13:14:55Z,2020-07-13T20:36:08Z,"verify we return none for all inplace calls in tests/indexing

related: https://github.com/pandas-dev/pandas/pull/35210
"
643884681,34951,BUG: ValueError on groupby with categoricals,LukasGelbmann,closed,2020-06-23T14:21:39Z,2020-07-13T22:22:45Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

In specific situations involving categorical columns, a groupby() on two or more columns runs into an error:

#### Code Sample

```python
import pandas as pd
col = pd.Categorical([0, 1])
df = pd.DataFrame({'A': col, 'B': col, 'C': col})
grouped = df.groupby(['A', 'B']).first()
# ValueError: Shape of passed values is (4, 1), indices imply (2, 1)
```

#### Expected Output

Expected output would be something like:

```
>>> grouped
       C
A B     
0 0    0
  1  NaN
1 0  NaN
  1    1
```
Instead an exception is thrown, with the error message shown above."
655489044,35253,BUG: ValueError on groupby with categoricals,smithto1,closed,2020-07-12T22:31:33Z,2020-07-13T22:22:54Z,"- [x] closes #34951 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Within `DataFrameGroupBy._cython_agg_blocks`, if it is aggregating a one-column DataFrame, it creates a `SeriresGroupBy`, calls the function on that and takes the returned values. But the `SeriesGroupBy` also does the missing-categories reindexing. The `DataFrameGroupBy` ends up with values that contain the missing categories, and an index that does not. When they are passed into a BlockManager it raises a `ValueError` stating that their lengths don't match. 

Solutions is to have `_cython_agg_blocks` create a `SeriesGroupBy` with `observed=True` so it doesn't do any reindexing. The reindexing is left to the calling `DataFrameGroupBy`

This also explains why error only occurred in `DataFrameGroupBy` but not `SeriesGroupBy`."
655292039,35241,BUG: GroupBy.count() and GroupBy.sum() incorreclty return NaN instead of 0 for missing categories (Version 2),smithto1,closed,2020-07-11T22:42:16Z,2020-07-14T08:20:53Z,"- [x] closes #31422 
- [x] closes #35028 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

_Behavioural Changes_
Fixing two related bugs: when grouping on multiple categoricals, .sum() and .count() would return NaN for the missing categories, but they are expected to return 0 for the missing categories. Both these bugs are fixed.

_Tests_
Tests were added in PR #35022 when these bugs were discovered and the tests were marked with an xfail. For this PR the xfails are removed and the tests are passing normally. As well, a few other existing tests were expecting `sum()` to return `NaN`; these have been updated so that the tests now expect to get `0` (which is the desired behaviour).

One new test is added to ensure that the exception handling of the new `try-except-finally` block behaves as expected. 

_df.pivot_table_
The changes to `.sum()` & `.count()` also impacts the `df.pivot_table()` if it is called with `aggfunc=sum/count` and is pivoted on a Categorical column with observed=False. This is not explicitly mentioned in either of the bugs, but it does make the behaviour consistent (i.e. the sum of a missing category is zero, not NaN). Two tests on test_pivot.py was updated to reflect this change."
655801212,35260,CI: Ignore setuptools distutils warning,TomAugspurger,closed,2020-07-13T12:12:36Z,2020-07-14T13:15:47Z,xref https://github.com/pandas-dev/pandas/issues/35252. Just ignoring in the test. Can discuss a proper solution in the issue.
656599600,35274,CI: pin pytest in minimum versions,simonjayhawkins,closed,2020-07-14T13:09:29Z,2020-07-14T13:56:57Z,xref https://github.com/pandas-dev/pandas/pull/35260#discussion_r454293041
312804151,20647,API/BUG: center=True in expanding operations returns non-sensical results,jorisvandenbossche,closed,2018-04-10T07:32:12Z,2020-07-14T17:08:45Z,"What is `center=True` supposed to do in `df.expanding()...` operations?

Using the example from the docstring of `.expanding()`:

```
In [103]: df = DataFrame({'B': [0, 1, 2, np.nan, 4]})

In [105]: df.expanding(2).sum()
Out[105]: 
     B
0  NaN
1  1.0
2  3.0
3  3.0
4  7.0
```

Now adding `center=True`:

```
In [106]: df.expanding(2, center=True).sum()
Out[106]: 
     B
0  3.0
1  3.0
2  7.0
3  7.0
4  6.0
```

Two observations: it seems it just shifted the first rows (which is not the same as centering) and I don't know where the last values are coming from (and they should certainly not decrease again in an expanding sum with only positive values). 

Further, I also cannot think of what it actually *should* do. If we really want to center it, that would mean returning with a new index (as the real center would only increase with 0.5 and be something like [0, 0.5, 1, 1.5, 2] for those data).

So maybe we should rather remove this keyword for `expanding` ?"
642354700,34887,Deprecate `center` on `df.expanding`,MBrouns,closed,2020-06-20T10:52:14Z,2020-07-14T17:08:50Z,"`df.expanding(center=True)` currently returns non-sensical results and it is unclear what results would be expected. It was previously removed in #7934 for that same reason,but was reintroduced for unclear reasons in a later refactoring

- [x] closes #20647
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
656578579,35273,Move API changes to appropriate sections,TomAugspurger,closed,2020-07-14T12:35:38Z,2020-07-14T17:10:00Z,xref https://github.com/pandas-dev/pandas/issues/34801
653935559,35190,Be specific about more indexes or values,Lewiscowles1986,closed,2020-07-09T10:04:56Z,2020-07-14T17:21:47Z,"The generic ""are not equal"" really frustrated me just now when someone was conveying their issue.

This specific two errors lets me know which way around, although I believe having more indexes than data shows a place in the api for a default value (possibly per-index) which would allow this to later be reduced to the case I feel I fully understand of more data than indexes, which I think probably is unsolvable generally.

I can see that both have problems, but I still think it's more important to be specific about which side is mismatched.

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
655312487,35244,ASV: dt64arr_to_periodarr,jbrockmendel,closed,2020-07-12T01:47:05Z,2020-07-14T18:43:27Z,"
"
655493598,35256,PERF: array_equivalent,jbrockmendel,closed,2020-07-12T23:04:16Z,2020-07-14T20:15:34Z,"xref #35249, this gives about a 3x speedup in the asv reported there

cc @jorisvandenbossche EA.equals doesn't support a strict_nan arg, is there anything we need to do to account for that here?"
610372767,33903,API: Revert changes to describe,TomAugspurger,closed,2020-04-30T20:43:11Z,2020-07-14T20:21:57Z,"https://github.com/pandas-dev/pandas/pull/30209/ has some changes to describe for datelike data. I think the changes count as API breaking and should wait until a 2.0, possibly with a deprecation warning.

cc @david-cortes, @jreback.

master:

```python
In [1]: import pandas as pd

In [2]: pd.Series(pd.date_range('2000', periods=10)).describe()
Out[2]:
count                     10
mean     2000-01-05 12:00:00
min      2000-01-01 00:00:00
25%      2000-01-03 06:00:00
50%      2000-01-05 12:00:00
75%      2000-01-07 18:00:00
max      2000-01-10 00:00:00
dtype: object

In [3]: pd.__version__
Out[3]: '1.1.0.dev0+1417.g12c9c626d'
```

1.0.2:

```python
In [1]: import pandas as pd

In [2]: pd.Series(pd.date_range('2000', periods=10)).describe()
Out[2]:
count                      10
unique                     10
top       2000-01-08 00:00:00
freq                        1
first     2000-01-01 00:00:00
last      2000-01-10 00:00:00
dtype: object

In [3]: pd.__version__
Out[3]: '1.0.2'
```


"
638868213,34798,API: Make describe changes backwards compatible,TomAugspurger,closed,2020-06-15T13:46:51Z,2020-07-14T20:30:06Z,"Adds the new behavior as a feature flag / deprecation.

Closes https://github.com/pandas-dev/pandas/issues/33903

(Do we have a list of issues for deprecations introduced in 1.x?)"
547981001,30880,Multiple aggregations with the same name get overwritten,MarcoGorelli,closed,2020-01-10T10:00:41Z,2020-07-14T20:34:56Z,"#### Code Sample, a copy-pastable example if possible

```
import pandas as pd
import numpy as np
from functools import partial

df = pd.DataFrame(
    np.random.randn(1000, 3), 
    index=pd.date_range('1/1/2012', freq='S', periods=1000), 
    columns=['A', 'B', 'C']
)
dfg = df.resample('3T').agg(
    {'A': [
        partial(np.quantile, q=.9999), 
        partial(np.quantile, q=.1111),
    ]}
)
```
returns
```
                            A          
                     quantile  quantile
2012-01-01 00:00:00 -1.035997 -1.035997
2012-01-01 00:03:00 -1.326052 -1.326052
2012-01-01 00:06:00 -1.267661 -1.267661
2012-01-01 00:09:00 -1.321961 -1.321961
2012-01-01 00:12:00 -1.103027 -1.103027
2012-01-01 00:15:00 -0.987614 -0.987614
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : 5e9fe4e0843266cb1c64bff4af70b3665a4d6a8d
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-74-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.25.0+1719.g5e9fe4e08.dirty
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 42.0.2.post20191201
Cython           : 0.29.14
pytest           : 5.3.2
hypothesis       : 4.55.4
sphinx           : 2.3.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.6
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : 1.3.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.3.2
s3fs             : 0.4.0
scipy            : 1.4.0
sqlalchemy       : 1.3.11
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.6
numba            : 0.46.0
</details>
"
656980251,35279,pandas/tests/io/json/test_pandas.py::TestPandasContainer::test_read_json_large_numbers failing for 32-bit system,TomAugspurger,open,2020-07-15T00:40:28Z,2020-07-15T00:43:22Z,"```
    @pytest.mark.parametrize(""bigNum"", [sys.maxsize + 1, -(sys.maxsize + 2)])
    # @pytest.mark.xfail(sys.maxsize == 2**32, reason="""")
    def test_read_json_large_numbers(self, bigNum):
        # GH20599

        series = Series(bigNum, dtype=object, index=[""articleId""])
        json = '{""articleId"":' + str(bigNum) + ""}""
        with pytest.raises(ValueError):
            json = StringIO(json)
            result = read_json(json)
            tm.assert_series_equal(series, result)

        df = DataFrame(bigNum, dtype=object, index=[""articleId""], columns=[0])
        json = '{""0"":{""articleId"":' + str(bigNum) + ""}}""
        with pytest.raises(ValueError):
            json = StringIO(json)
            result = read_json(json)
>           tm.assert_frame_equal(df, result)
E           AssertionError: Attributes of DataFrame.iloc[:, 0] (column name=""0"") are different
E
E           Attribute ""dtype"" are different
E           [left]:  object
E           [right]: int64
```

We have

```
-> tm.assert_frame_equal(df, result)
(Pdb) result
           0
articleId  1
(Pdb) df
                              0
articleId  18446744073709551617
```"
654406546,35201,BUG: GroupBy.count() and GroupBy.sum() incorreclty return NaN instead of 0 for missing categories,smithto1,closed,2020-07-09T23:23:32Z,2020-07-15T01:33:42Z,"- [x] closes #31422 
- [x] closes #35028 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

*Behavioural Changes*
Fixing two related bugs: when grouping on multiple categoricals, `.sum()` and `.count()` would return `NaN` for the missing categories, but they are expected to return `0` for the missing categories. Both these bugs are fixed.

*Tests*
Tests were added in PR #35022 when these bugs were discovered and the tests were marked with an `xfail`. For this PR the `xfails` are removed and the tests are passing normally. As well, a few other existing tests were expecting `sum()` to return `NaN`; these have been updated so that the tests now expect to get `0` (which is the desired behaviour).

*Pivot*
The change to `.sum()` also impacts the `df.pivot_table()` if it is called with `aggfunc=sum` and is pivoted on a Categorical column with `observed=False`. This is not explicitly mentioned in either of the bugs, but it does make the behaviour consistent (i.e. the sum of a missing category is zero, not `NaN`). One test on test_pivot.py was updated to reflect this change. 

*Default Behaviour*
Because `df.groupby()` and `df.pivot_table()` have `observed=False` as the default, the default behaviour will change for a user calling `df.groupby().sum()` or `df.pivot_table(..., aggfunc='sum')` if they are grouping/pivoting on a categorical with missing categories. Previously the default would give them `NaN` for the missing categories, now the default will give them `0`.

What is the appropriate to highlight/document this change to the default behaviour?"
423360362,25805,Using custom format when outputting data to html to support column styles,bbibentyo,closed,2019-03-20T16:47:09Z,2020-07-15T03:40:46Z,"#### Code Sample

```python
df = pd.DataFrame([{'Key': 'Blue', 'Value': 101.25}, {'Key': 'Green', 'Value': 99.12}])
df.to_html()
```
#### Problem description

The code above, `to_html` has no direct way of specifying column styles. When I dump the output, it produces the table below. 

<table border=""1"" class=""dataframe"">
<thead>
    <tr style=""text-align: right;""><th></th><th>Key</th> <th>Value</th></tr>  </thead>
<tbody>
    <tr><th>0</th><td>Blue</td><td>101.25</td></tr>
    <tr><th>1</th><td>Green</td><td>99.12</td></tr>
</tbody>
</table>

I would like to pass in custom styles (which could be tags based on column index) for specific columns, such `Value` is right-aligned. `to_html` references `DataFrameFormatter` which reference `HTMLFormatter`. the `write_tr` method in `HTMLFormatter` supports `tags` which can be used to specify cell styles or `align` which can be used to specify row text alignment.

#### Expected Output
It would be nice to pass in align & tags values when calling `DataFrame.to_html`

#### Output of ``pd.show_versions()``

<details>

pandas: 0.24.1
pytest: 4.0.2
pip: 19.0.1
setuptools: 40.7.3
Cython: None
numpy: 1.15.4
scipy: None
pyarrow: None
xarray: None
IPython: 7.2.0
sphinx: None
patsy: None
dateutil: 2.7.5
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: 2.5.14
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.2
lxml.etree: None
bs4: None
html5lib: 1.0.1
sqlalchemy: 1.2.17
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
527526922,29814,Pandas wrongly read Scipy sparse matrix,m7142yosuke,closed,2019-11-23T08:10:29Z,2020-07-15T06:18:18Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> from scipy.sparse import coo_matrix
>>> sparse_data = coo_matrix((
...     [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
...     ([0, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 2, 0, 1])
... ))
>>> sparse_data.todense()
matrix([[0., 0., 0.],
        [0., 1., 1.],
        [1., 0., 1.],
        [1., 1., 0.]])
>>> pd.DataFrame.sparse.from_spmatrix(sparse_data)
     0    1    2
0  0.0  0.0  0.0
1  0.0  1.0  1.0
2  0.0  0.0  1.0
3  1.0  1.0  0.0
>>> pd.DataFrame.sparse.from_spmatrix(sparse_data.tocsr())
     0    1    2
0  0.0  0.0  0.0
1  0.0  1.0  1.0
2  0.0  0.0  1.0
3  1.0  1.0  0.0
```

#### Problem description

`sparse_data.todense()` and the other matrixes should be same, but not.
 (i.e. the [2, 0] value in bottom 2 matrixes should be 1.)

#### Expected Output

When converting sparse matrix to Pandas DataFrame values of the sparse array should stay the same.

#### Output of ``pd.show_versions()``

<details>

python           : 3.7.5.final.0
pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.6.0
Cython           : None
pytest           : 5.3.0
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
547643287,30858,BUG: aggregations were getting overwritten if they had the same name,MarcoGorelli,closed,2020-01-09T18:22:21Z,2020-07-15T07:15:33Z,"- [x] closes #30880 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

xref #30092 
"
655255925,35238,REGR: concat on index with duplicate labels fails,jorisvandenbossche,closed,2020-07-11T18:22:34Z,2020-07-15T12:24:40Z,"Noticed from failing geopandas tests:

```python
df1 = pd.DataFrame({'a': [0, 1]}) 
df2 = pd.DataFrame({'b': [0, 1, 2]}, index=[0, 0, 1]) 
pd.concat([df1, df2], axis=1)
```

this started failing recently on master:

```
In [3]: pd.concat([df1, df2], axis=1) 
...
~/scipy/pandas/pandas/core/internals/managers.py in _verify_integrity(self)
    314         for block in self.blocks:
    315             if block._verify_integrity and block.shape[1:] != mgr_shape[1:]:
--> 316                 raise construction_error(tot_items, block.shape[1:], self.axes)
    317         if len(self.items) != tot_items:
    318             raise AssertionError(

ValueError: Shape of passed values is (3, 2), indices imply (2, 2)
```

While the expected result is (with automatic reindex of `df1` because concat aligns on the index):

```
In [2]: pd.concat([df1, df2], axis=1)
Out[2]: 
   a  b
0  0  0
0  0  1
1  1  2
```
"
631077795,34580,DEPR: DateOffset.apply and DateOffset.apply_index,jreback,closed,2020-06-04T19:06:03Z,2020-07-15T12:25:34Z,"```pytb
In [1]: import pandas as pd

In [2]: rng = pd.date_range(start=""1/1/2000"", periods=10, freq=""T"")

In [3]: pd.offsets.YearEnd()
Out[3]: <YearEnd: month=12>

In [4]: pd.offsets.YearEnd().apply_index(rng)
Out[4]:
DatetimeIndex(['2000-12-31 00:00:00', '2000-12-31 00:01:00',
               '2000-12-31 00:02:00', '2000-12-31 00:03:00',
               '2000-12-31 00:04:00', '2000-12-31 00:05:00',
               '2000-12-31 00:06:00', '2000-12-31 00:07:00',
               '2000-12-31 00:08:00', '2000-12-31 00:09:00'],
              dtype='datetime64[ns]', freq='T')

In [5]: pd.__version__
Out[5]: '1.0.4'

```

vs

```python
In [1]: import pandas as pd

In [2]: rng = pd.date_range(start=""1/1/2000"", periods=10, freq=""T"")

In [3]: pd.offsets.YearEnd().apply_index(rng)
Out[3]:
array([978220800000000000, 978220860000000000, 978220920000000000,
       978220980000000000, 978221040000000000, 978221100000000000,
       978221160000000000, 978221220000000000, 978221280000000000,
       978221340000000000])

In [4]: pd.__version__
Out[4]: '1.1.0.dev0+2021.g74538104bc'

```

---


see https://github.com/pandas-dev/pandas/pull/34579#discussion_r435465450

errors are here: https://github.com/pandas-dev/pandas/pull/33962#issuecomment-638834572

these don't need to be exposed to users and we should deprecate; it seems that in the offsets reorg some of the offsets Month? are no longer exposing these methods as well (so need to test).

```[  0.65%] ···· For parameters: <BusinessDay>
               Traceback (most recent call last):
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 1184, in main_run_server
                   main_run(run_args)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 1058, in main_run
                   result = benchmark.do_run()
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 537, in do_run
                   return self.run(*self._current_params)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 627, in run
                   samples, number = self.benchmark_timing(timer, min_repeat, max_repeat,
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 694, in benchmark_timing
                   timing = timer.timeit(number)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/timeit.py"", line 177, in timeit
                   timing = self.inner(it, self.timer)
                 File ""<timeit-src>"", line 6, in inner
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 599, in <lambda>
                   func = lambda: self.func(*param)
                 File ""/home/runner/work/pandas/pandas/asv_bench/benchmarks/arithmetic.py"", line 469, in time_apply_index
                   offset.apply_index(self.rng)
                 File ""pandas/_libs/tslibs/offsets.pyx"", line 87, in pandas._libs.tslibs.offsets.apply_index_wraps.wrapper
                 File ""pandas/_libs/tslibs/offsets.pyx"", line 1397, in pandas._libs.tslibs.offsets.BusinessDay.apply_index
               AttributeError: 'PeriodIndex' object has no attribute '_addsub_int_array'
               
               For parameters: <SemiMonthEnd: day_of_month=15>
               Traceback (most recent call last):
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 1184, in main_run_server
                   main_run(run_args)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 1058, in main_run
                   result = benchmark.do_run()
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 537, in do_run
                   return self.run(*self._current_params)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 627, in run
                   samples, number = self.benchmark_timing(timer, min_repeat, max_repeat,
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 694, in benchmark_timing
                   timing = timer.timeit(number)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/timeit.py"", line 177, in timeit
                   timing = self.inner(it, self.timer)
                 File ""<timeit-src>"", line 6, in inner
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 599, in <lambda>
                   func = lambda: self.func(*param)
                 File ""/home/runner/work/pandas/pandas/asv_bench/benchmarks/arithmetic.py"", line 469, in time_apply_index
                   offset.apply_index(self.rng)
                 File ""pandas/_libs/tslibs/offsets.pyx"", line 87, in pandas._libs.tslibs.offsets.apply_index_wraps.wrapper
                 File ""pandas/_libs/tslibs/offsets.pyx"", line 2319, in pandas._libs.tslibs.offsets.SemiMonthOffset.apply_index
               AttributeError: 'PeriodIndex' object has no attribute '_addsub_int_array'
               
               For parameters: <SemiMonthBegin: day_of_month=15>
               Traceback (most recent call last):
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 1184, in main_run_server
                   main_run(run_args)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 1058, in main_run
                   result = benchmark.do_run()
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 537, in do_run
                   return self.run(*self._current_params)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 627, in run
                   samples, number = self.benchmark_timing(timer, min_repeat, max_repeat,
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 694, in benchmark_timing
                   timing = timer.timeit(number)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/timeit.py"", line 177, in timeit
                   timing = self.inner(it, self.timer)
                 File ""<timeit-src>"", line 6, in inner
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 599, in <lambda>
                   func = lambda: self.func(*param)
                 File ""/home/runner/work/pandas/pandas/asv_bench/benchmarks/arithmetic.py"", line 469, in time_apply_index
                   offset.apply_index(self.rng)
                 File ""pandas/_libs/tslibs/offsets.pyx"", line 87, in pandas._libs.tslibs.offsets.apply_index_wraps.wrapper
                 File ""pandas/_libs/tslibs/offsets.pyx"", line 2319, in pandas._libs.tslibs.offsets.SemiMonthOffset.apply_index
               AttributeError: 'PeriodIndex' object has no attribute '_addsub_int_array'
```"
652547640,35165,Fixed apply_index,TomAugspurger,closed,2020-07-07T19:00:14Z,2020-07-15T12:25:37Z,Closes https://github.com/pandas-dev/pandas/issues/34580
610800542,33920,Performance regression in replace.ReplaceDict.time_replace_series,TomAugspurger,closed,2020-05-01T15:11:41Z,2020-07-15T12:27:22Z,"Setup

```python
import pandas as pd
import numpy as np

inplace = False

N = 10 ** 5
start_value = 10 ** 5
to_rep = dict(enumerate(np.arange(N) + start_value))
s = pd.Series(np.random.randint(N, size=10 ** 3))

%timeit s.replace(to_rep, inplace=inplace)
```


```
# 1.0.2
2.48 s ± 53.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

# master
5.77 s ± 117 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```


https://pandas.pydata.org/speed/pandas/index.html#replace.ReplaceDict.time_replace_series?p-inplace=False&commits=acb525a79fd3496a57b93fcfdb86be3de28a1815-aa8e869d76878f07dff065f947c99b5663342087 points to 

```
aa8e869d76 BLD: recursive inclusion of DLLs in package data (#33246)
610568c146 REF: remove replace_list kludge (#33445)
df68cceeb3 REF: .dot tests (#33214)
8d299e4ce1 REF: call pandas_dtype up-front in Index.__new__ (#33407)
e7cbe6dc1a BUG: df.iloc[:, :1] with EA column (#32959)
31ea45f8a3 BUG: Add test to ensure, that bug will not occur again. #33058 (#33072)
f334fcc681 Properly handle missing attributes in query/eval strings (#32408)
12ffb23a72 DOC: Change doc template to fix SA04 errors in docstrings #28792 (#32972)
7f276c8ba1 DOC: Fixed examples in `pandas/core/groupby/` (#33230)
982b4aadef CI: fix lint issue (#33461)
6bc8a49f25 DOC: Fix heading capitalization in doc/source/whatsnew - part2 (#32550) (#33403)
47de449681 API/TST: Call __finalize__ in more places (#33379)
916d1f3786 DOC: Fix EX01 in DataFrame.drop_duplicates (#33283)
4fc8c2515e DOC: Fix heading capitalization in doc/source/whatsnew - part3 (#32550) (#33429)
ef9b9387c8 BUG: Fix bug when concatenating Index of strings (#33436)
40fd73ab8f Update citation webpage (#33311)
3cca07c8a5 BUG: Fix replacing in `string` series with NA (pandas-dev#32621) (#32890)
a2cdd50427 INT: provide helpers for accessing the values of DataFrame columns (#33252)
efa85af76d PERF: improve IntegerArray fast constructor (#33359)
716689a9bf REGR: Fix construction of PeriodIndex from strings (#33304)
4a74463d02 Updated headers for files in doc/source/whatsnew (#33376)
542ef40bb4 Updated headers for files in doc/source/whatsnew (#33378)
12b0d4523a TYP: F used in decorators to _typing (#33456)
d72116b0e7 BUG/PLT: Multiple plots with different cmap, colorbars legends use first cmap (#33392)
2a68c12509 BUG: `weights` is not working for multiple columns in df.plot.hist (#33440)
b5f6e59cb4 BUG: Fix ValueError when grouping by read-only category (#33410) (#33446)
0c69615ad5 DEP: Bump min version of dateutil to 2.7.3 (#33363)
be86b6583f CLN: remove unnecessary Series._convert calls (#32949)
4334482c34 BUG/API: prohibit dtype-changing IntervalArray.__setitem__ (#32782)
6e3537dbab CLN: Static types in `pandas/_lib/lib.pyx` (#33329)
54f9b03cf6 BUG/REF: unstack with EA dtypes (#33356)
496c982b4e REF: reshape.concat operate on arrays, not SingleBlockManagers (#33125)
d72dc24e62 CLN: avoid accessing private functions (#33427)
a142ad7bb2 BUG: DataFrame.diff(axis=1) with mixed (or EA) dtypes (#32995)
185a654e3d BUG: scalar indexing on 2D DTA/TDA/PA (#33342)
5f2cdf8e1d REF: call _block_shape from EABlock.make_block (#33308)
5d0faa8ca5 BUG: Series.__getitem__ with MultiIndex and leading integer level (#33404)
fe42954e08 BUG: Timedelta == ndarray[td64] (#33441)
991f784a72 STY: Using __all__; removed ""noqa"" comment (#33143)
fbae09e11a CLN/TYP: redundant casts and unused ignores (#33453)
78d8af1fe3 Bump cython for asv environment (#33454)
```

perhaps one of based on the commit message. Not sure.

* 610568c146 REF: remove replace_list kludge (#33445)
* 3cca07c8a5 BUG: Fix replacing in `string` series with NA (pandas-dev#32621) (#32890)
"
611799810,33966,Feature Request: Create a Global Setting for Enabling numba engine,jtelleriar,closed,2020-05-04T11:25:42Z,2020-07-15T12:28:04Z,"Would it be possible to create a default pandas global setting to enable numba engine whenever possible?

In pandas.DataFrame.apply, .transform, etc.

Thanks!"
580176154,32667,Add index=False option to_markdown() ,chrisjcameron,closed,2020-03-12T19:40:32Z,2020-07-15T12:29:54Z,"#### Problem description

This is a feature request to add an optional `index=False` argument to the new `to_markdown()` dataframe method. This would produce a markdown table without the initial unnamed index column. 


"
656876088,35277,"Revert ""BUG: fix union_indexes not supporting sort=False for Index subclasses""",TomAugspurger,closed,2020-07-14T20:23:45Z,2020-07-15T12:31:17Z,"Reverts pandas-dev/pandas#35098

Closes https://github.com/pandas-dev/pandas/issues/35238 I'll also push a test from https://github.com/pandas-dev/pandas/issues/35238 here, and add the xfailing tests from #35098."
589499766,33091,ENH: Add index option to_markdown(),quangngd,closed,2020-03-28T03:29:13Z,2020-07-15T12:52:29Z,"- [ ] closes #32667
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
651727150,35147,CI: MacPython failing TestPandasContainer.test_to_json_large_numbers ,TomAugspurger,closed,2020-07-06T18:08:13Z,2020-07-15T13:43:45Z,"```
_________ TestPandasContainer.test_to_json_large_numbers[2147483648] __________
[gw0] win32 -- Python 3.7.7 D:\a\1\s\test_venv\Scripts\python.exe

self = <pandas.tests.io.json.test_pandas.TestPandasContainer object at 0x2C6E2A70>
bigNum = 2147483648

    @pytest.mark.parametrize(""bigNum"", [sys.maxsize + 1, -(sys.maxsize + 2)])
    def test_to_json_large_numbers(self, bigNum):
        # GH34473
        series = Series(bigNum, dtype=object, index=[""articleId""])
        json = series.to_json()
        expected = '{""articleId"":' + str(bigNum) + ""}""
        assert json == expected
        # GH 20599
        with pytest.raises(ValueError):
            json = StringIO(json)
            result = read_json(json)
            tm.assert_series_equal(series, result)
    
        df = DataFrame(bigNum, dtype=object, index=[""articleId""], columns=[0])
        json = df.to_json()
        expected = '{""0"":{""articleId"":' + str(bigNum) + ""}}""
        assert json == expected
        # GH 20599
        with pytest.raises(ValueError):
            json = StringIO(json)
            result = read_json(json)
>           tm.assert_frame_equal(df, result)
E           AssertionError: Attributes of DataFrame.iloc[:, 0] (column name=""0"") are different
E           
E           Attribute ""dtype"" are different
E           [left]:  object
E           [right]: int64

```

https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=38672&view=logs&j=c0130b29-789d-5a3c-6978-10796a508a7f&t=e120bc6c-1f5e-5a41-8f0a-1d992cd2fbfb

xref https://github.com/pandas-dev/pandas/pull/34473.

cc @arw2019."
655134580,35228,COMPAT: Ensure rolling indexers return intp,mroeschke,closed,2020-07-11T04:50:25Z,2020-07-15T15:24:15Z,"- [x] closes #35148
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Unable to test this locally for the MacPython build, but I am pretty sure this is the main culprit of that failing job."
653469379,35182,ENH: Add compute.use_numba configuration for automatically using numba,mroeschke,closed,2020-07-08T17:25:12Z,2020-07-15T16:32:26Z,"- [x] closes #33966
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
649041401,35083,No fastcall attribute in POWER platform,ayappanec,closed,2020-07-01T14:45:30Z,2020-07-15T18:05:17Z,"Compiling the development branch of pandas fail in AIX. 

pandas/_libs/src/ujson/lib/ultrajsonenc.c:397:1: error: ‘fastcall’ attribute directive ignored [-Werror=attributes]
 Buffer_AppendShortHexUnchecked(char *outputOffset, unsigned short value) {
 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
pandas/_libs/src/ujson/lib/ultrajsonenc.c:726:59: error: ‘fastcall’ attribute directive ignored [-Werror=attributes]
                                                           char *end) {
                                                           ^~~~
cc1: all warnings being treated as errors


The fastcall attribute is not available in POWER platform also. 

"
633142870,34626,BUG: s3 reads from public buckets not working,ayushdg,closed,2020-06-07T07:22:00Z,2020-07-15T19:07:52Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
#### Code Sample

```python
# Your code here
import pandas as pd
df = pd.read_csv(""s3://nyc-tlc/trip data/yellow_tripdata_2019-01.csv"")
```
<details>
<summary> Error stack trace </summary>
<pre>
Traceback (most recent call last):
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/pandas/io/s3.py"", line 33, in get_file_and_filesystem
    file = fs.open(_strip_schema(filepath_or_buffer), mode)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/fsspec/spec.py"", line 775, in open
    **kwargs
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/s3fs/core.py"", line 378, in _open
    autocommit=autocommit, requester_pays=requester_pays)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/s3fs/core.py"", line 1097, in __init__
    cache_type=cache_type)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/fsspec/spec.py"", line 1065, in __init__
    self.details = fs.info(path)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/s3fs/core.py"", line 530, in info
    Key=key, **version_id_kw(version_id), **self.req_kw)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/s3fs/core.py"", line 200, in _call_s3
    return method(**additional_kwargs)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/client.py"", line 316, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/client.py"", line 622, in _make_api_call
    operation_model, request_dict, request_context)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/client.py"", line 641, in _make_request
    return self._endpoint.make_request(operation_model, request_dict)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/endpoint.py"", line 102, in make_request
    return self._send_request(request_dict, operation_model)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/endpoint.py"", line 132, in _send_request
    request = self.create_request(request_dict, operation_model)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/endpoint.py"", line 116, in create_request
    operation_name=operation_model.name)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/hooks.py"", line 356, in emit
    return self._emitter.emit(aliased_event_name, **kwargs)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/hooks.py"", line 228, in emit
    return self._emit(event_name, kwargs)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/hooks.py"", line 211, in _emit
    response = handler(**kwargs)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/signers.py"", line 90, in handler
    return self.sign(operation_name, request)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/signers.py"", line 160, in sign
    auth.add_auth(request)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/auth.py"", line 357, in add_auth
    raise NoCredentialsError
botocore.exceptions.NoCredentialsError: Unable to locate credentials

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/pandas/io/parsers.py"", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/pandas/io/parsers.py"", line 431, in _read
    filepath_or_buffer, encoding, compression
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/pandas/io/common.py"", line 212, in get_filepath_or_buffer
    filepath_or_buffer, encoding=encoding, compression=compression, mode=mode
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/pandas/io/s3.py"", line 52, in get_filepath_or_buffer
    file, _fs = get_file_and_filesystem(filepath_or_buffer, mode=mode)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/pandas/io/s3.py"", line 42, in get_file_and_filesystem
    file = fs.open(_strip_schema(filepath_or_buffer), mode)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/fsspec/spec.py"", line 775, in open
    **kwargs
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/s3fs/core.py"", line 378, in _open
    autocommit=autocommit, requester_pays=requester_pays)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/s3fs/core.py"", line 1097, in __init__
    cache_type=cache_type)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/fsspec/spec.py"", line 1065, in __init__
    self.details = fs.info(path)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/s3fs/core.py"", line 530, in info
    Key=key, **version_id_kw(version_id), **self.req_kw)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/s3fs/core.py"", line 200, in _call_s3
    return method(**additional_kwargs)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/client.py"", line 316, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/client.py"", line 622, in _make_api_call
    operation_model, request_dict, request_context)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/client.py"", line 641, in _make_request
    return self._endpoint.make_request(operation_model, request_dict)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/endpoint.py"", line 102, in make_request
    return self._send_request(request_dict, operation_model)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/endpoint.py"", line 132, in _send_request
    request = self.create_request(request_dict, operation_model)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/endpoint.py"", line 116, in create_request
    operation_name=operation_model.name)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/hooks.py"", line 356, in emit
    return self._emitter.emit(aliased_event_name, **kwargs)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/hooks.py"", line 228, in emit
    return self._emit(event_name, kwargs)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/hooks.py"", line 211, in _emit
    response = handler(**kwargs)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/signers.py"", line 90, in handler
    return self.sign(operation_name, request)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/signers.py"", line 160, in sign
    auth.add_auth(request)
  File ""/home/conda/envs/pandas-test/lib/python3.7/site-packages/botocore/auth.py"", line 357, in add_auth
    raise NoCredentialsError
</pre>
</details>

#### Problem description

Reading directly from s3 public buckets (without manually configuring the `anon` parameter via s3fs) is broken with pandas 1.0.4 (worked with 1.0.3).

Looks like reading from public buckets requires `anon=True` while creating the filesystem. This 22cf0f5dfcfbddd5506fdaf260e485bff1b88ef1 seems to have introduced the issue, where `anon=False` is passed when the `noCredentialsError` is encountered.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-55-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.4
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 47.1.1.post20200604
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
642256112,34877,BUG/TST: Read from Public s3 Bucket Without Creds,alimcmaster1,closed,2020-06-19T23:07:29Z,2020-07-15T19:07:56Z,"- [x] Closes #34626

We should potentially merge this before https://github.com/pandas-dev/pandas/pull/34266 to confirm reading from s3 without credentials works.

Ref discussion here: https://github.com/pandas-dev/pandas/pull/34793#issuecomment-645023794

cc. @jorisvandenbossche "
136814457,12473,Pandas datetime64 series no longer has map function when localized,AlJohri,closed,2016-02-26T21:04:14Z,2020-07-15T19:58:08Z,"Create test DF.

```
df = pd.DataFrame({""uuid"": [0,1,2,3,4], ""publication_timestamp"": [""2015-07-28 00:10:05.852"", ""2015-10-03 00:17:43.000"", ""2015-08-20 01:15:52.693"", ""2015-09-09 00:02:03.083"", ""2015-12-08 00:02:41.390""], ""timezone"": [""US/Central"", ""US/Eastern"", ""US/Eastern"", ""US/Pacific"", ""US/Mountain""]}).set_index('uuid')
```

```
                publication_timestamp     timezone
uuid
0    2015-07-28 04:10:05.852000+00:00   US/Central
1           2015-10-03 04:17:43+00:00   US/Eastern
2    2015-08-20 05:15:52.693000+00:00   US/Eastern
3    2015-09-09 04:02:03.083000+00:00   US/Pacific
4    2015-12-08 05:02:41.390000+00:00  US/Mountain
```

This call to `map` works fine:

```
df.publication_timestamp.map(lambda x: x) # works fine
```

**Localizing the datetime64 causes it to no longer have the map function**

```
df['publication_timestamp'] = df.publication_timestamp.astype(""datetime64[ms]"").dt.tz_localize(""UTC"")
```

Doesn't work:

```
df.publication_timestamp.map(lambda x: x) # no longer works
```
#### Error Message

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-13-ba941613799a> in <module>()
----> 1 df.publication_timestamp.map(lambda x: x)

/Users/johria/.pyenv/versions/3.5.1/lib/python3.5/site-packages/pandas/core/series.py in map(self, arg, na_action)
   2052                                      index=self.index).__finalize__(self)
   2053         else:
-> 2054             mapped = map_f(values, arg)
   2055             return self._constructor(mapped,
   2056                                      index=self.index).__finalize__(self)

TypeError: Argument 'arr' has incorrect type (expected numpy.ndarray, got DatetimeIndex)
```
#### output of `pd.show_versions()`

```

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.1.final.0
python-bits: 64
OS: Darwin
OS-release: 15.0.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.17.1
nose: 1.3.7
pip: 8.0.3
setuptools: 19.4
Cython: None
numpy: 1.10.4
scipy: 0.17.0
statsmodels: 0.6.1
IPython: 4.1.1
sphinx: 1.3.5
patsy: 0.4.1
dateutil: 2.2
pytz: 2015.7
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: 1.5.1
openpyxl: 2.2.0-b1
xlrd: 0.9.4
xlwt: None
xlsxwriter: None
lxml: 3.5.0
bs4: 4.4.1
html5lib: None
httplib2: 0.9.2
apiclient: 1.4.2
sqlalchemy: 1.0.11
pymysql: None
psycopg2: None
Jinja2: 2.8
```
"
657447920,35289,xfail failing 32-bit tests,TomAugspurger,closed,2020-07-15T15:34:14Z,2020-07-15T20:27:02Z,We need MacPython to be passing for the wheels to be built.
657606466,35295,CI: xfail failing 32-bit tests,TomAugspurger,closed,2020-07-15T19:44:33Z,2020-07-15T21:09:19Z,https://github.com/pandas-dev/pandas/issues/35294
628751568,34520,BUG: Mixed DataFrame with Extension Array incorrect aggregation,WillAyd,closed,2020-06-01T21:51:17Z,2020-07-15T22:17:58Z,"Surprised to not get an aggregation for ""b"" with the Int64 dtype

```python
>>> df = pd.DataFrame([[""a"", 1]], columns=list(""ab""))
>>> df.sum()
a    a
b    1
dtype: object
>>> df.astype({""b"": ""Int64""}).sum()
a    a
dtype: object
```"
570671758,32240,NamedAgg with as_index= False drops aggregated values,RaghavaDhanya,closed,2020-02-25T15:56:42Z,2020-07-15T22:19:13Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

file_name = ""https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv""
df = pd.read_csv(file_name)

df[['species','petal_width']].groupby('species', as_index=False).agg(min_width=pd.NamedAgg(column='petal_width', aggfunc='min'))

```
#### Problem description
When I try to use `as_index=False`with named aggregations it seems to be dropping values and replacing them with index. This is the output of the above code.
|    | min_width   |
|---:|:------------|
|  0 | setosa      |
|  1 | versicolor  |
|  2 | virginica   |


#### Expected Output
|    | species    |   min_width |
|---:|:-----------|--------------:|
|  0 | setosa     |           0.1 |
|  1 | versicolor |           1   |
|  2 | virginica  |           1.4 |
#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.138+
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1
setuptools       : 41.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : 1.3.4
tables           : None
tabulate         : None
xarray           : 0.12.2
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```
</details>
"
357286011,22609,DataFrame.where method results in ValueError excepted when at least one of the columns is a categorical,DmitriyLeybel,closed,2018-09-05T15:33:36Z,2020-07-15T22:24:06Z," ```pandas: 0.23.4```

I experienced the error in a different workflow but was able to narrow it down to a simple scenario.

This works as expected:
#### Working example

```python
import pandas as pd

iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')

# iris.loc[:,'species'] = iris.species.astype('category')

iris.where(iris.sepal_width > 2)

```
![image](https://user-images.githubusercontent.com/6779278/45103439-bf1a3d00-b0e4-11e8-8685-1581254845d2.png)

However, once we transform one of the columns to a category, it breaks.

#### Broken example
```python
import pandas as pd

iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')

iris.loc[:,'species'] = iris.species.astype('category')

iris.where(iris.sepal_width > 2)
```
```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-13-aa56ba330a63> in <module>()
      5 iris.loc[:,'species'] = iris.species.astype('category')
      6 
----> 7 iris.where(iris.sepal_width > 2)

C:\Anaconda3\lib\site-packages\pandas\core\generic.py in where(self, cond, other, inplace, axis, level, errors, try_cast, raise_on_error)
   7770         other = com._apply_if_callable(other, self)
   7771         return self._where(cond, other, inplace, axis, level,
-> 7772                            errors=errors, try_cast=try_cast)
   7773 
   7774     @Appender(_shared_docs['where'] % dict(_shared_doc_kwargs, cond=""False"",

C:\Anaconda3\lib\site-packages\pandas\core\generic.py in _where(self, cond, other, inplace, axis, level, errors, try_cast)
   7630                                         errors=errors,
   7631                                         try_cast=try_cast, axis=block_axis,
-> 7632                                         transpose=self._AXIS_REVERSED)
   7633 
   7634             return self._constructor(new_data).__finalize__(self)

C:\Anaconda3\lib\site-packages\pandas\core\internals.py in where(self, **kwargs)
   3682 
   3683     def where(self, **kwargs):
-> 3684         return self.apply('where', **kwargs)
   3685 
   3686     def eval(self, **kwargs):

C:\Anaconda3\lib\site-packages\pandas\core\internals.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)
   3579 
   3580             kwargs['mgr'] = self
-> 3581             applied = getattr(b, f)(**kwargs)
   3582             result_blocks = _extend_blocks(applied, result_blocks)
   3583 

C:\Anaconda3\lib\site-packages\pandas\core\internals.py in where(self, other, cond, align, errors, try_cast, axis, transpose, mgr)
   1536                 result = self._try_cast_result(result)
   1537 
-> 1538             return self.make_block(result)
   1539 
   1540         # might need to separate out blocks

C:\Anaconda3\lib\site-packages\pandas\core\internals.py in make_block(self, values, placement, ndim)
    259             ndim = self.ndim
    260 
--> 261         return make_block(values, placement=placement, ndim=ndim)
    262 
    263     def make_block_scalar(self, values):

C:\Anaconda3\lib\site-packages\pandas\core\internals.py in make_block(values, placement, klass, ndim, dtype, fastpath)
   3203                      placement=placement, dtype=dtype)
   3204 
-> 3205     return klass(values, ndim=ndim, placement=placement)
   3206 
   3207 # TODO: flexible with index=None and/or items=None

C:\Anaconda3\lib\site-packages\pandas\core\internals.py in __init__(self, values, placement, ndim)
   2301 
   2302         super(ObjectBlock, self).__init__(values, ndim=ndim,
-> 2303                                           placement=placement)
   2304 
   2305     @property

C:\Anaconda3\lib\site-packages\pandas\core\internals.py in __init__(self, values, placement, ndim)
    123             raise ValueError(
    124                 'Wrong number of items passed {val}, placement implies '
--> 125                 '{mgr}'.format(val=len(self.values), mgr=len(self.mgr_locs)))
    126 
    127     def _check_ndim(self, values, ndim):

ValueError: Wrong number of items passed 150, placement implies 1

```

Same error when a function is passed instead of a boolean Series.



"
655236913,35233,TST: Verify filtering operations on DataFrames with categorical Series,avinashpancham,closed,2020-07-11T16:20:29Z,2020-07-15T22:24:10Z,"- [ x ] closes #22609
- [ x ] tests added / passed
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ x ] whatsnew entry
"
625467210,34402,BUG/API: other object type check in Series/DataFrame.equals,jorisvandenbossche,closed,2020-05-27T07:56:20Z,2020-07-15T22:25:27Z,"xref https://github.com/geopandas/geopandas/issues/1420

First, this PR is fixing the ""bug"" that we shouldn't rely on `_constructor` being a class that can be used in `isinstance` (see https://github.com/pandas-dev/pandas/issues/32638 for the general discussion about this, this is the only place in our code where `_constructor` is used like this, AFAIK). 
And even if `_constructor` would be a class, it wouldn't necessarily be the correct class to check with (or not more correct than `type(self)`)

But, so this also brings up the API question: what are the ""requirements"" we put on the type of `other` ? Should it be the same type? (as then could also change it to `if not type(self) is type(other): ...`) 
Or is a subclass sufficient (with `isinstance`) ? 
The problem with an isinstance checks with subclasses is that then the order matters (eg `subdf.equals(df)` would not necessarily give the same answer as `df.equals(subdf)`)

"
655491381,35254,BUG: Use correct ExtensionArray reductions in DataFrame reductions,jbrockmendel,closed,2020-07-12T22:48:34Z,2020-07-15T22:44:19Z,"- [x] closes #34520
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
657514136,35291,[QST]  What should `ExtensionDtype.type` return?,shwina,closed,2020-07-15T17:07:16Z,2020-07-16T00:16:13Z,"Greetings, Pandas devs! cuDF is building out additional dtypes such as `cudf.CategoricalDtype` and `cudf.ListDtype` based on `pd.ExtensionDtype`, and this is one question that came up.

The [documentation](https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.ExtensionDtype.type.html#pandas.api.extensions.ExtensionDtype.type) states:

  > It’s expected ExtensionArray[item] returns an instance of ExtensionDtype.type for scalar item, assuming that value is valid (not NA). NA values do not need to be instances of type.

However, I note that `pd.CategoricalDtype` for instance does not adhere to this:

```python
In [47]: import pandas as pd

In [48]: a = pd.Series(['a', 'b'], dtype='category')

In [49]: type(a[0])
Out[49]: str

In [50]: type(a.array[0])
Out[50]: str

In [51]: isinstance(a.array, pd.api.extensions.ExtensionArray)
Out[51]: True

In [52]: isinstance(a.dtype, pd.api.extensions.ExtensionDtype)
Out[52]: True
```

On the other hand, [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.dtype.type.html#numpy.dtype.type) defines `dtype.type` somewhat differently:

> The type object used to instantiate a scalar of this data-type.

Would love any insights as to the appropriate return value of `.type`."
395699850,24600,BUG: DataFrame[Sparse] quantile fails because SparseArray has no reshape ,jbrockmendel,closed,2019-01-03T19:41:01Z,2020-07-16T01:31:45Z,"Tried to simplify Block.quantile by arranging for it to only have to handle 2D case by having Series.quantile dispatch to DataFrame implementation.  Ended up getting failures in pandas/tests/series/test_quantile.py test_quantile_sparse

```
ser = pd.Series([0., None, 1., 2.], dtype='Sparse[float]')
df = pd.DataFrame(ser)

>>> ser.quantile(0.5)
1.0
>>> ser.quantile([0.5])
0.5    1.0
dtype: float64
>>> df.quantile(0.5)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pandas/core/frame.py"", line 7760, in quantile
    transposed=is_transposed)
  File ""pandas/core/internals/managers.py"", line 500, in quantile
    return self.reduction('quantile', **kwargs)
  File ""pandas/core/internals/managers.py"", line 432, in reduction
    axe, block = getattr(b, f)(axis=axis, axes=self.axes, **kwargs)
  File ""pandas/core/internals/blocks.py"", line 1530, in quantile
    result = _nanpercentile(values, qs * 100, axis=axis, **kw)
  File ""pandas/core/internals/blocks.py"", line 1484, in _nanpercentile
    mask = mask.reshape(values.shape)
AttributeError: 'SparseArray' object has no attribute 'reshape'
>>> df.quantile([0.5])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pandas/core/frame.py"", line 7760, in quantile
    transposed=is_transposed)
  File ""pandas/core/internals/managers.py"", line 500, in quantile
    return self.reduction('quantile', **kwargs)
  File ""pandas/core/internals/managers.py"", line 432, in reduction
    axe, block = getattr(b, f)(axis=axis, axes=self.axes, **kwargs)
  File ""pandas/core/internals/blocks.py"", line 1511, in quantile
    axis=axis, **kw)
  File ""pandas/core/internals/blocks.py"", line 1484, in _nanpercentile
    mask = mask.reshape(values.shape)
AttributeError: 'SparseArray' object has no attribute 'reshape'
```

`datetime64[ns, tz]` breaks in a slightly different way (presumably all ExtensionBlocks will fail):

```
dti = pd.date_range('2016-01-01', periods=3, tz='US/Pacific')

ser = pd.Series(dti)
df = pd.DataFrame(ser)

>>> df.quantile(0.5)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pandas/core/frame.py"", line 7760, in quantile
    transposed=is_transposed)
  File ""pandas/core/internals/managers.py"", line 500, in quantile
    return self.reduction('quantile', **kwargs)
  File ""pandas/core/internals/managers.py"", line 473, in reduction
    values = _concat._concat_compat([b.values for b in blocks])
  File ""pandas/core/dtypes/concat.py"", line 174, in _concat_compat
    return np.concatenate(to_concat, axis=axis)
ValueError: need at least one array to concatenate
>>> df.quantile([0.5])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pandas/core/frame.py"", line 7760, in quantile
    transposed=is_transposed)
  File ""pandas/core/internals/managers.py"", line 500, in quantile
    return self.reduction('quantile', **kwargs)
  File ""pandas/core/internals/managers.py"", line 473, in reduction
    values = _concat._concat_compat([b.values for b in blocks])
  File ""pandas/core/dtypes/concat.py"", line 174, in _concat_compat
    return np.concatenate(to_concat, axis=axis)
ValueError: need at least one array to concatenate
```

xref #24583 
"
586661246,32967,Empty error message in tz_localize,quangngd,closed,2020-03-24T03:17:31Z,2020-07-16T01:51:19Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
from pandas import Timestamp
Timestamp.min.tz_localize(""Asia/Tokyo"")
```
The example is picked directly from 
```pandas/tests/scalar/timestamp/test_timezones.py::TestTimestampTZOperations::test_tz_localize_pushes_out_of_bounds```
#### Problem description
Error message is empty.

> yep, out of scope. but if you could raise an issue, that'll be great.

_Originally posted by @simonjayhawkins in https://github.com/pandas-dev/pandas/pull/32929_

#### Expected Output
Meaningful error message

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : bbfecaffa1887a86a801213fd3e1d6188f7d3456
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-40-generic
Version          : #32-Ubuntu SMP Fri Jan 31 20:24:34 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0.dev0+943.gbbfecaffa
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0.post20200311
Cython           : 0.29.15
pytest           : 5.4.1
hypothesis       : 5.6.0
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : 0.3.3
gcsfs            : None
matplotlib       : 3.2.0
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pyxlsb           : 1.0.6
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : 0.15.0
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0
</details>
"
653712895,35187,Add date overflow message to tz_localize (#32967),gabrielNT,closed,2020-07-09T02:25:32Z,2020-07-16T01:51:29Z,"- [x] closes #32967
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Added an error message to tz_localize when the timestamp overflows. I got a little confused by the history in #32979 as some changes were lost on the force pushes, but I tried to handle all review comments."
655205395,35229,Place the calculation of mask prior to the calls of comp in replace_list to improve performance,chrispe,closed,2020-07-11T13:06:40Z,2020-07-16T07:08:44Z,"- [X] closes #33920
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
579666617,32651,DataFrame with Int64 columns casts to float64 with .max()/.min(),qwhelan,closed,2020-03-12T03:23:23Z,2020-07-16T09:32:38Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np

int64_info = np.iinfo(""int64"")
s = pd.Series([int64_info.max, None, int64_info.min], dtype=pd.Int64Dtype())
df = pd.DataFrame({""Int64"": s})

df.max()
Int64    9.223372e+18
dtype: float64
```
#### Problem description

`pd.Int64` data is converted to `np.float64` in certain reduction operations on `pd.DataFrame`. This causes data corruption, as `pd.Int64` is intended to avoid this exact issue.

#### Expected Output
`df.max()` should probably return a `pd.Series` of `dtype='object'` wrapping a `pd.Int64` value.

#### Output of ``pd.show_versions()``

<details>
```
INSTALLED VERSIONS
------------------
commit           : 27ad779713f1d54b83547db5465bec4245d279cd
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-29-generic
Version          : #31-Ubuntu SMP Fri Jan 17 17:27:26 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0.dev0+779.g27ad77971
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 42.0.2.post20191203
Cython           : 0.29.14
pytest           : 5.3.5
hypothesis       : 5.4.1
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.4.0.dev0+62.g8ac3a4c8
fastparquet      : 0.3.2
gcsfs            : None
matplotlib       : None
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.11.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : 0.14.1
xlrd             : None
xlwt             : None
numba            : 0.48.0
```
</details>"
655208134,35231,BUG: Inconsistent behavior in Index.difference,simonjayhawkins,closed,2020-07-11T13:23:39Z,2020-07-16T09:57:00Z,"- [ ] closes #35217
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
657338605,35286,REGR: reindex with sparse data,jorisvandenbossche,closed,2020-07-15T13:11:34Z,2020-07-16T11:17:06Z,"See discussion at https://github.com/pandas-dev/pandas/pull/34158#issuecomment-636641583 and below. There was some discussion on the PR, but I think we didn't yet create an issue to track this regression.

The PR introduced a regression in `reindex` when having sparse data. Based on the linked discussion, there doesn't seem to direct / easy solution (but I didn't look into it again). So we might want to revert the PR for 1.1 until we figure out a correct solution.

cc @TomAugspurger "
657382346,35287,"Fix indexing, reindex on all-sparse SparseArray.",TomAugspurger,closed,2020-07-15T14:11:23Z,2020-07-16T11:17:10Z,"Closes https://github.com/pandas-dev/pandas/issues/35286.

Also added a regression tests for the issue reported there."
658112198,35304,TST: xfail more 32-bits,TomAugspurger,closed,2020-07-16T11:29:48Z,2020-07-16T13:29:02Z,xref #35294
657221706,35283,ENH: Implement PandasArray.equals,kuraga,closed,2020-07-15T09:54:58Z,2020-07-16T14:21:09Z,Feature request: implement `PandasArray.equals` for consistency (in addition to `Series.equals` and `DataFrame.equals`).
658214171,35306,Revert  BUG: Ensure same index is returned for slow and fast path in …,TomAugspurger,closed,2020-07-16T13:44:49Z,2020-07-16T15:06:59Z,"…groupby.apply #31613

xref https://github.com/pandas-dev/pandas/pull/34998."
399796541,24798,Assigning values breaks in different ways when duplicate column names,user347,closed,2019-01-16T12:56:08Z,2020-07-16T15:19:08Z,"#### Code Sample, a copy-pastable example if possible

Our primary data with two columns with identical name:

```python
df = pd.DataFrame(np.arange(12).reshape(4, 3).T)
df.columns = list('AABC')
print(df)
""""""
   A  A  B   C
0  0  3  6   9
1  1  4  7  10
2  2  5  8  11
""""""
```
Issue 1a: `Series.replace` throws `ValueError` when assigning:

```python
print(df['B'].replace(6, np.nan))  # will work as expected with int as well
""""""
0    NaN
1    7.0
2    8.0
Name: B, dtype: float64
""""""
# ValueError: Buffer has wrong number of dimensions (expected 1, got 0):
df['B'] = df['B'].replace(6, np.nan)  # inplace=True does not raise error, but no change
df['B'] = df['B'].replace(6, 5)
```
Issue 1b: Same `ValueError` as above thrown when assigning `np.nan` with `loc`:
```python
# ValueError: Buffer has wrong number of dimensions (expected 1, got 0):
df.loc[df['B'] == 6, 'B'] = np.nan

# Assigning int with loc will however work: 
df.loc[df['B'] == 6, 'B'] = 5
```
Issue 2a: assigning `np.nan` with `iloc` on column with a duplicate will apply on both columns:

```python
# Assigning np.nan with iloc on column with a duplicate will apply on both columns:
df.iloc[0, 0] = np.nan
print(df)
""""""
     A    A  B   C
0  NaN  NaN  5   9
1  1.0  4.0  7  10
2  2.0  5.0  8  11
""""""
```` 
Issue 2b: assigning `int` with `iloc` will work int `v0.22.0` but not `v0.23.4`

```python
df.iloc[0, 0] = 10
print(df)
""""""
0.22.0:
    A  A  B   C
0  10  3  5   9
1   1  4  7  10
2   2  5  8  11

0.23.4:
      A     A  B   C
0  10.0  10.0  5   9
1   1.0   4.0  7  10
2   2.0   5.0  8  11
""""""
# Assigning with iloc will not break if BOTH columns contain a nan:
x = pd.DataFrame({'a': np.array([np.nan, 1, 2])})
y = pd.DataFrame({'a': np.array([0, np.nan, 2])})

df = pd.concat([x, y], axis=1)

df.iloc[0, 0] = 10
print(df)
""""""
      a    a 
0  10.0  0.0
1   1.0  NaN
2   2.0  2.0
""""""
```
#### Problem description

The main topic for this issue is assigning different values to a `DataFrame` that contains duplicate column names. List of issues reported:

- [x] Issue 1a: `Series.replace` throws `ValueError` when assigning (#32477)
- [x] Issue 1b: Same `ValueError` as `Issue 1a` thrown when assigning np.nan with loc (#34302)
- [x] Issue 2a: assigning `np.nan` with `iloc` on column with a duplicate will apply on both columns (#22036, #15686 closed by #32477)
- [x] Issue 2b: assigning `int` with `iloc` will work in `v0.22.0` but not `v0.23.4` (#22036, #15686 closed by #32477)

he issue with `iloc` and `np.nan` (above called `Issue 2a`) was reported and closed as fixed here: https://github.com/pandas-dev/pandas/issues/13423 per  `0.18.0` but I'm able to recreate that issue with `v0.23.4`.

#### Expected Output
Either the same output as we would expect if we had only unique names in our columns or a `DuplicateColumnWarning`/`DuplicateColumnException` when `DataFrame` contains duplicate columns.

#### Output of ``pd.show_versions()``

<details>
pandas: 0.23.4
pytest: None
pip: 18.0
setuptools: 39.0.1
Cython: None
numpy: 1.16.0
scipy: 1.1.0T
pyarrow: None
xarray: None
IPython: 6.3.1
sphinx: None
patsy: None
dateutil: 2.7.2
pytz: 2018.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.1.0
openpyxl: 2.5.1
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: 4.2.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.5
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
655243183,35236,TST: added tests for sparse and date range quantiles,sanders41,closed,2020-07-11T17:00:02Z,2020-07-16T15:29:24Z,"- [x] closes #24600
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Added tests for sparse and date range quantiles"
442032769,26321,Bug: Groupby with as_index=True causes incorrect summarization,cdknox,closed,2019-05-09T03:28:12Z,2020-07-16T16:08:55Z,"#### Code Sample, a copy-pastable example:

```python
import pandas as pd
import numpy as np

pd.show_versions()

dates = ['2019-05-09', '2019-05-09', '2019-05-09']
date_series = pd.Series(dates)
# it appears to happen on object dtype aggregations
# where there is another object dtype in the frame with missing values
# hence column 'a' being populated with strings
# and missing and the dt.date accessor in 'c'
date_series_parsed = pd.to_datetime(date_series, format='%Y-%m-%d').dt.date

df = pd.DataFrame({
    'a': [np.nan, '1', np.nan],
    'b': [0, 1, 1],
    'c': date_series_parsed,
})  

# as expected the series values returned are each populated with non missing data
# I'm ignoring that this is returning a Series
# and the other method returns a DataFrame
# the logic is my real concern
print(df.groupby('b')['c'].min())
# output (as is and as expected):
# b
# 0    2019-05-09
# 1    2019-05-09
# Name: c, dtype: object
 

# looks like it's dependent on column 'a' being fully populated for some reason
# as the group df['b'] == 1 min value is not returned appropriately here
# but the group df['b'] == 0 min value does appear to be working
print(df.groupby('b', as_index=False)['c'].min())
# output (as is):
#    b           c
# 0  0  2019-05-09
# 1  1         NaN
# 
# output (as expected):
#    b           c 
# 0  0  2019-05-09
# 1  1  2019-05-09
```
#### Problem description

As there is no missing value in column 'c', I would expect that every subgroup of the series would have the minimum as being well defined and non-missing.  This is consistent with the behavior shown when as_index in the groupby method is left as it's default parameter of False.  However it can clearly be seen that the minimum of column 'c' depends on column 'a' in some capacity.  It appears as though at some point in time the indexing is looking to column 'a' instead of column 'c' though I'm not sure why or when.

I have tested this both on the version of pandas available on PyPI (0.24.2 as of the time of writing) and through the pandas-dev master branch.  The issue persists regardless of versions.  Furthermore the issue has been experienced on a variety of machines.


#### Expected Output

I would expect the results to be consistent with the calculation performed when as_index is left as it's default value of False, as it seems that the minimum of the values which comprise the group where column b is equal to 1 is a well defined result (explicitly 2019-05-09).

Thank you for the consideration!

#### Output of ``pd.show_versions()``
Note: I ran this on the master branch, and 0.24.2, the below is the output from the master branch.  If this is insufficient please let me know and I will supplement with the PyPI version of 0.24.2 when able.

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.2.final.0
python-bits: 64
OS: Linux
OS-release: 5.0.4-arch1-1-ARCH
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: C
LOCALE: None.None

pandas: 0+unknown
pytest: None
pip: 18.1
setuptools: 40.8.0
Cython: 0.29.7
numpy: 1.16.1
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 7.2.0
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.3
openpyxl: 2.6.1
xlrd: 1.2.0
xlwt: None
xlsxwriter: None
lxml.etree: 4.3.2
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
642398339,34906,BUG: Groupby with as_index=True causes incorrect summarization,vivikelapoutre,closed,2020-06-20T15:40:14Z,2020-07-16T18:02:54Z,"- [x] closes #26321 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
653490749,35184,CI: MacPython failing TestPandasContainer.test_to_json_large_numbers,arw2019,closed,2020-07-08T17:59:53Z,2020-07-16T18:55:14Z,"- [x] closes #35147
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
657624935,35298,TST: Remove deprecated use of apply_index,TomAugspurger,closed,2020-07-15T20:15:40Z,2020-07-16T19:39:05Z,"Noticed this warning. apply_index is deprecated, so not likely to get attention anyway. Removing."
373162678,23305,Columns lose category dtype after calling replace on the dataframe,somiandras,closed,2018-10-23T19:26:12Z,2020-07-16T22:45:58Z,"xref https://github.com/pandas-dev/pandas/pull/25521


#### Code Sample, a copy-pastable example if possible

```python
>>> import pandas as pd
>>> pd.__version__
'0.23.4'

# Sample dataframe with two categorical and one int column
>>> df = pd.DataFrame(
...     [[1., 'A', 'x'], [2, 'B', 'y'], [3, 'C', 'z']],
...     columns=['first', 'second', 'third']
... ).astype({'second': 'category', 'third': 'category'})

# Replace int values
>>> df = df.replace(1, 10)

# Both categorical columns turned into object...
>>> df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3 entries, 0 to 2
Data columns (total 3 columns):
first     3 non-null float64
second    3 non-null object
third     3 non-null object
dtypes: float64(1), object(2)
memory usage: 152.0+ bytes

# Sample dataframe with two categorical columns
>>> df = pd.DataFrame(
...     [['A', 'x'], ['B', 'y'], ['C', 'z']],
...     columns=['first', 'second',]
... ).astype({'first': 'category', 'second': 'category'})

>>> df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3 entries, 0 to 2
Data columns (total 2 columns):
first     3 non-null category
second    3 non-null category
dtypes: category(2)
memory usage: 294.0 bytes

# Replace values in column `first`
>>> df = df.replace('A', 'B')

# Dtype of column `second` becomes object
>>> df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3 entries, 0 to 2
Data columns (total 2 columns):
first     3 non-null category
second    3 non-null object
dtypes: category(1), object(1)
memory usage: 211.0+ bytes
```
#### Problem description

Calling `replace()` on a dataframe changes `category` columns' dtype to `object` in an apparently inconsisent manner:

1. When `replace()` contains non-categorical values that are not present in either categorical column, all categorical dtype turn into `object`.
2. When `replace()` contains categories from a `category` dtype column, then that column keeps its dtype, but other categorical columns turn into `object`.

See the examples above for both. Even if it is somehow intentional I find it quite confusing.

#### Expected Output

I would expect the categorical columns to keep their `category` dtype after `replace` is called on the dataframe (at least for those categorical columns that are unaffected by `replace`).

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None

pandas: 0.23.4
pytest: None
pip: 18.0
setuptools: 39.0.1
Cython: None
numpy: 1.15.0
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.5.0
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.2.3
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
655238819,35234,TST add test for dtype consistency with pd replace #23305,mathurk1,closed,2020-07-11T16:32:19Z,2020-07-16T22:46:02Z,"- [x] closes #23305 
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
479928745,27892,DataFrameGroupBy.quantile raises for non-numeric dtypes rather than dropping columns,TomAugspurger,closed,2019-08-13T02:22:42Z,2020-07-16T22:48:30Z,"In pandas 0.24.x, we had

```python
In [1]: import pandas as pd

In [2]: pd.DataFrame({""A"": ['a', 'b']}).groupby([0, 0]).quantile()
Out[2]:
Empty DataFrame
Columns: []
Index: []
``` 

In 0.25.0, we have

```python
In [3]: pd.DataFrame({""A"": ['a', 'b']}).groupby([0, 0]).quantile()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-3-8152ffc0932b> in <module>
----> 1 pd.DataFrame({""A"": ['a', 'b']}).groupby([0, 0]).quantile()

~/sandbox/pandas/pandas/core/groupby/groupby.py in quantile(self, q, interpolation)
   1908             post_processing=post_processor,
   1909             q=q,
-> 1910             interpolation=interpolation,
   1911         )
   1912

~/sandbox/pandas/pandas/core/groupby/groupby.py in _get_cythonized_result(self, how, grouper, aggregate, cython_dtype, needs_values, needs_mask, needs_ngroups, result_is_index, pre_processing, post_processing, **kwargs)
   2236                 vals = obj.values
   2237                 if pre_processing:
-> 2238                     vals, inferences = pre_processing(vals)
   2239                 func = partial(func, vals)
   2240

~/sandbox/pandas/pandas/core/groupby/groupby.py in pre_processor(vals)
   1875             if is_object_dtype(vals):
   1876                 raise TypeError(
-> 1877                     ""'quantile' cannot be performed against 'object' dtypes!""
   1878                 )
   1879

TypeError: 'quantile' cannot be performed against 'object' dtypes!
```

This is most relevant for mixed dataframes

```python
In [6]: df = pd.DataFrame({""A"": [0, 1], 'B': ['a', 'b']})

In [7]: df.groupby([0, 1]).quantile(0.5)

...

TypeError: 'quantile' cannot be performed against 'object' dtypes!

```"
654290873,35194,BUG: Adding two DataFrames together with duplicate column names in one results in infinite loop,tdpetrou,closed,2020-07-09T19:18:44Z,2020-07-16T22:49:13Z,"#### Code Sample, a copy-pastable example

```python
df1 = pd.DataFrame(data=[[0]], columns=['second'])
df2 = pd.DataFrame(data=[[0, 0, 0]], columns=['first', 'second', 'second'])
```

<img width=""191"" alt=""Screen Shot 2020-07-09 at 3 15 10 PM"" src=""https://user-images.githubusercontent.com/7226751/87081253-01d46100-c1f7-11ea-9767-5265010c62f7.png"">

Adding them together never completes (infinite loop?)

```python
df1 + df2
```

#### Problem description

When adding two DataFrame where one has duplicate column names and the other has one (or more) of the duplicated columns, the operation never completes

#### Expected Output

Expected to follow rules of automatic alignment of the index.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.4.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.1.1.post20200529
Cython           : None
pytest           : 5.4.2
hypothesis       : None
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.1
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.2
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : None
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
658105281,35303,Fixed reindexing arith with duplicates,TomAugspurger,closed,2020-07-16T11:20:33Z,2020-07-16T22:49:17Z,"Closes https://github.com/pandas-dev/pandas/issues/35194

Still need to run ASV on this. This was a regression from 0.25.x to 1.0. It doesn't have to go in 1.1, but probably better for 1.1.0.rc0 than a 1.1.1 release."
658954825,35319,Update multi.py,zky001,closed,2020-07-17T07:19:33Z,2020-07-17T07:21:12Z,"fix bugs be metioned on issue #35301

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
658301395,35308,CLN: consistent EA._reduce signatures,simonjayhawkins,closed,2020-07-16T15:23:52Z,2020-07-17T07:58:50Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
654774462,35212,Fix strange behavior when precision display option is zero (#20359),gimseng,closed,2020-07-10T13:15:20Z,2020-07-17T09:29:04Z,"- [x] closes #20359
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Notes:
- [x] I added a unit test for the example given in #20359
- [x] As far as the context of this code is concerned, I replacde '.' with decimal string (passed from self when called) in case of non-standard decimal use
"
658732134,35316,DOC: extra closing parens make example invalid.,Carreau,closed,2020-07-17T01:29:39Z,2020-07-17T10:25:52Z,"No attached issues or need for what's new / test... unless you want to actually get a linter the parse the docstrings and make the the examples are syntactically correct ?

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
583739949,32805,RecursionError with pd.DataFrame.from_records,kdebrab,closed,2020-03-18T13:45:59Z,2020-07-17T10:37:14Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
index = pd.CategoricalIndex([pd.Interval(-20, -10), pd.Interval(-10, 0), pd.Interval(0, 10)])
series_of_dicts = pd.Series([{'a': 1}, {'a': 2}, {'b': 3}], index=index)
frame = pd.DataFrame.from_records(series_of_dicts, index=index)
```
raises:
```
RecursionError: maximum recursion depth exceeded in comparison
```

#### Problem description

Though using `pd.DataFrame.from_records` with `pd.Series` objects is not explicitly supported, it does work in most of the cases. Only in case the index of the Series is a CategoricalIndex of Intervals, it suddenly doesn't work anymore.

E.g. the following works fine:

```python
series_of_dicts2 = pd.Series([{'a': 1}, {'a': 2}, {'b': 3}])
frame = pd.DataFrame.from_records(series_of_dicts2, index=index)
```
And it even works when `series_of_dicts2.index` is a CategoricalIndex _or_ a list of Intervals, but not when it is CategoricalIndex _of_ Intervals. It is especially strange because the index of `series_of_dicts2` is not even used for the output.

#### Expected Output

The same as when the following workaround is used:
```python
frame = pd.DataFrame.from_records(series_of_dicts.values, index=index)
Out[2]: 
              a    b
(-20, -10]  1.0  NaN
(-10, 0]    2.0  NaN
(0, 10]     NaN  3.0
```



#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.0.2
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 40.6.2
Cython           : 0.29.4
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.6.1
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.0
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : 0.2.1
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : None
tabulate         : 0.8.6
xarray           : 0.15.0
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.8
numba            : None

</details>
"
647652714,35055,TST: Adds CategoricalIndex DataFrame from_records test (GH32805),jgulian,closed,2020-06-29T20:40:19Z,2020-07-17T10:37:23Z,"- [x] closes #32805
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

The test checks to make sure `from_records` loads a `DataFrame` with a `CategorigalIndex` as shown in #32805."
652516745,35164,Infer compression if file extension is uppercase,willbowditch,closed,2020-07-07T18:06:03Z,2020-07-17T10:58:39Z,"Inferring compression fails for files with uppercase extensions (e.g. `x.zip` works but `y.ZIP` does not)

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
655241642,35235,CLN/DOC: DataFrame.to_parquet supports file-like objects,rhshadrach,closed,2020-07-11T16:50:07Z,2020-07-17T13:08:25Z,"Adds documentation and type-hints for supporting file-like objects when `engine == 'pyarrow'`; relevant to #30081. Tests for this behavior currently exist in `io.test_parquet.py`:

````
@td.skip_if_no(""pyarrow"")
    def test_read_file_like_obj_support(self, df_compat):
        buffer = BytesIO()
        df_compat.to_parquet(buffer)
        df_from_buf = pd.read_parquet(buffer)
        tm.assert_frame_equal(df_compat, df_from_buf)
````

Perhaps the restrictions on the arguments when path is not a string:

* partition_cols must be None; and
* engine must end up being pyarrow

should be checked directly in `DataFrame.to_parquet`, but I'm leaving this out as that is an API change that could be made in a subsequent PR. The latter gives the clear error message `TypeError: expected str, bytes or os.PathLike object, not _io.BytesIO` but the former raises `AttributeError: 'NoneType' object has no attribute '_isfilestore'` which is slightly confusing.

Another API change that could be made subsequently is changing the `path` argument to `path_or_buf`, consistent with `DataFrame.to_csv`."
658589125,35312,pin numpy<1.19 in doc build,TomAugspurger,closed,2020-07-16T21:32:11Z,2020-07-17T15:13:38Z,"Doc build is failing on master with a warning from NumPy via matplotlib. Waiting on a release with https://github.com/matplotlib/matplotlib/pull/17289 (3.3.0), so pinning numpy<1.19 for now."
658534306,35310,CI: Skip test for 3.7.0 exactly,TomAugspurger,closed,2020-07-16T20:20:03Z,2020-07-17T15:13:58Z,"xref https://github.com/pandas-dev/pandas/issues/35309

This test segfaults for python 3.7.0 exactly, but seems to pass for other versions. Just skipping in, to get MacPython all passing again."
571974705,32288,[Regression] Index matching is wrong for dataframe operations such as div and mul,harmbuisman,closed,2020-02-27T10:33:19Z,2020-07-17T16:06:20Z,"#### Code Sample, a copy-pastable example if possible
```python
import pandas as pd
categories = ['B', 'A', 'C']

a = pd.DataFrame({'x':categories})
a['y'] = 1
a['value'] = 1
a['x'] = a['x'].astype('category')

# the sort_values is important here to trigger the bug
b = a.copy().sort_values('x')
a['x'] = a['x'].cat.set_categories(categories)
print(a['x'])
print(b['x'])

a = a.set_index(['x', 'y'], )['value']
b = b.set_index(['x', 'y'], )['value']
a.mul(b)
```
#### Problem description

In pandas 1.01+ the result of running the above code is wrong, where it returns the correct result in 0.25.3

The output of the above in 1.0.5 gives two NaN values with duplicate index entry:
![image](https://user-images.githubusercontent.com/7702207/87791831-3d98a780-c843-11ea-8ad1-e5fc6e7d6bd6.png)

It is triggered with the following conditions:
- having a multi-index
- a difference in the order of the categories in the index (using the set_categories)
- a difference in order of the index itself (using the sort_values)

#### Expected Output
The expected output is returned in 0.25.3:
![image](https://user-images.githubusercontent.com/7702207/87791732-12ae5380-c843-11ea-9d86-cd47384b1ab0.png)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 46.1.3.post20200325
Cython           : 0.29.16
pytest           : 5.4.1
hypothesis       : 5.19.3
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.2
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.9
numba            : 0.50.1
</details>
"
58637925,9536,Asymmetric error bars are not supported for series (only dataframes),mizzao,closed,2015-02-23T19:47:50Z,2020-07-17T16:26:47Z,"According to [the documentation](http://pandas-docs.github.io/pandas-docs-travis/visualization.html#plotting-with-error-bars), asymmetric error bars should be able to be passed in to `plot` as an Mx2 array for a `Series` object or a Mx2xN array for a `DataFrame` object.

However, the code only seems to parse this correctly for a `DataFrame`: https://github.com/pydata/pandas/blob/master/pandas/tools/plotting.py#L1338

When passing raw error values as `yerr`, it seems that the first values are used as two-sided error differences, instead of being plotted as raw upper and lower values. Note the symmetric error bars below:

![image](https://cloud.githubusercontent.com/assets/2080084/6335609/b0ad6f62-bb6a-11e4-941e-71154ca48eca.png)

Using the underlying matplotlib function directly yields the expected results, but without the pandas-generated decorations:

![image](https://cloud.githubusercontent.com/assets/2080084/6335591/8a3bd170-bb6a-11e4-8e36-400f86cf6e91.png)

This was originally noted in [this StackOverflow question](http://stackoverflow.com/q/26793758/586086) and I corroborated the other user's experience.
"
638907428,34801,Check API changes in whatsnew,TomAugspurger,closed,2020-06-15T14:39:15Z,2020-07-17T16:30:02Z,"Our whatsnew has several sections detailing API changes:

- https://pandas.pydata.org/pandas-docs/dev/whatsnew/v1.1.0.html#other-api-changes
- https://pandas.pydata.org/pandas-docs/dev/whatsnew/v1.1.0.html#backwards-incompatible-api-changes. According to our version policy we shouldn't have any API-breaking changes until 2.0

Many of these, like adding `DataFrame.value_counts` are fine and should just be moved to a different section.

Others like https://github.com/pandas-dev/pandas/pull/31905 will need to be looked at closely, and we'll need to determine if they're API breaking or bug fixes."
659333919,35327,Doc fixups,TomAugspurger,closed,2020-07-17T15:08:57Z,2020-07-17T16:39:31Z,"
"
628492988,34514,BUG: asymmetric error bars for series (GH9536),Steffen911,closed,2020-06-01T14:46:29Z,2020-07-17T16:46:15Z,"closes #9536

This fix enables asymmetric errors bars for pd.Series plots. In the current implementation the bars are symmetric, even if two sets of errors are provided.

Took inspirations from #12046"
656765698,35276,DOC: remove Bug Fix / Other section in whatsnew in favor of correct placement of issues,jreback,closed,2020-07-14T17:07:00Z,2020-07-17T17:32:56Z,*very* few of the current whatsnew notes in Bug Fixes / Other actually belong. Should move everything out.
659410453,35329,More whatsnew cleanup.,TomAugspurger,closed,2020-07-17T16:38:08Z,2020-07-17T17:32:59Z,Closes https://github.com/pandas-dev/pandas/issues/35276
655426034,35249,Performance Regression in frame_methods.Equals.time_frame_float_equal,TomAugspurger,closed,2020-07-12T15:54:15Z,2020-07-17T18:49:28Z,"https://pandas.pydata.org/speed/pandas/index.html#frame_methods.Equals.time_frame_float_equal?commits=77f4a4d673e2d2b3f7396a94818771d332bd913c

Some discussion starting at https://github.com/pandas-dev/pandas/pull/34962#issuecomment-655456601

@jbrockmendel what needs to be done for 1.1, just optimize `array_equivalent` or something larger? Is this a blocker (if so can you  add the blocker tag)?"
655332504,35246,BUG: AttributeError when doing groupby with as_index=False on Empty DataFrame,salem3358,closed,2020-07-12T05:08:24Z,2020-07-17T18:49:54Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd
pd.DataFrame(columns=[""a"", ""b""]).groupby(by=""a"", as_index=False)[""b""].sum()
```

#### Problem description
in version 0.25.3, this code return an empty DataFrame with columns=[""a"", ""b""]

#### Expected Output
an empty DataFrame with columns=[""a"", ""b""]

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.2.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 85 Stepping 4, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.0.3
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None"
659270336,35324,Fix AttributeError when groupby as_index=False on empty DataFrame,salem3358,closed,2020-07-17T13:46:10Z,2020-07-17T18:49:55Z,"- [x] closes #35246
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
659368611,35328,Optimize array_equivalent for NDFrame.equals,TomAugspurger,closed,2020-07-17T15:51:14Z,2020-07-17T18:58:31Z,"Closes #35249

```python
In [1]: import numpy as np
In [2]: import pandas as pd
In [3]: N = 10**3
In [4]: float_df = pd.DataFrame(np.random.randn(N, N))
# 1.0.5
1.52 ms ± 94 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

# master
83.6 ms ± 1.15 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

# branch
31.2 ms ± 2.06 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

And proof of this being better for short-circuiting:

```python
In [4]: N = 10**3

In [5]: a = pd.DataFrame(np.random.randn(N, N))

In [6]: b = pd.DataFrame(np.random.randn(N, N))

In [7]: %timeit a.equals(b)
# 1.0.5
1.97 ms ± 127 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

In [10]: %timeit a.equals(b)
56.4 µs ± 1.67 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```

Discussion: the key thing here is that `BlockManager.equals` requires that the dtypes of `left` and `right` match. If they don't then we've already returned False. That lets us do a few things

1. Avoid many `is_foo_dtype` checks.
2. Use optimized checks like `np.isnan`
3. Avoid the slow case of considering `np.array([1, 2])` equivalent to `np.array([1.0, 2.0])`."
636382134,34699,BUG: GH34677 Roll forward asv benchmark environment to py3.8,matteosantama,closed,2020-06-10T16:18:40Z,2020-07-17T20:14:27Z,Partially closes #34677 
658281141,35307,TST: xfail 32-bit test,erfannariman,closed,2020-07-16T15:00:56Z,2020-07-17T21:25:36Z,ref #35294
550974675,31080,Pandas 0.25.0 breaks np.isin,lamourj,open,2020-01-16T18:18:57Z,2020-07-18T13:35:45Z,"Observed with numpy 1.17.4 as well as (latest) 1.18.1:

```python
# pandas 0.24.2:
l = [pd.Timestamp]
pd.Timestamp == pd.Timestamp
>>> True
np.isin(l, l)
>>> array([ True])
```

```python
# pandas 0.25.0
l = [pd.Timestamp]
pd.Timestamp == pd.Timestamp
>>> True
np.isin(l, l)
>>> array([ False])
```


#### Problem description
Since 0.25.0, the `==` of `pd.Timestamp` is preserved but it doesn't go through `np.isin`.
This is observed as well under pandas 0.25.3."
660202247,35336,DOC: Unable to install pre-release,ginward,closed,2020-07-18T15:09:49Z,2020-07-19T14:32:23Z,"#### Location of the documentation

[""https://github.com/pandas-dev/pandas/releases/tag/v1.1.0rc0""]

#### Documentation problem

It seems that I am not able to install the v1.1.0 version of the Pandas package using the following commands: 

```
python -m pip install --upgrade --pre pandas==1.1.0rc0

conda install -c conda-forge/label/rc pandas==1.1.0rc0

```

It says that: 

PackagesNotFoundError: The following packages are not available from current channels:

  - pandas==1.1.0rc0

Current channels:

```
  - https://conda.anaconda.org/conda-forge/label/rc/osx-64
  - https://conda.anaconda.org/conda-forge/label/rc/noarch
  - https://repo.anaconda.com/pkgs/main/osx-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/r/osx-64
  - https://repo.anaconda.com/pkgs/r/noarch
```"
615506760,34112,"BUG: unstack() sorts automatically, default is preferably unsorted?",MatthiVH,closed,2020-05-11T00:07:32Z,2020-07-19T18:27:16Z,"The unstack() function is automatically sorting my labels in my boxplot-code. Isn't it nice to have a sort = False option (probably the default?)

My labels are perfectly ordered, it's just after unstacking they get messed-up.

my post on stack-overflow: https://stackoverflow.com/questions/61720090/issues-with-boxplot-figure-x-label-are-alphabetically-and-extra-nan-label-is-cr

another example: https://stackoverflow.com/questions/32163063/pandas-plotting-incorrectly-sorts-the-binned-values-on-the-graph
"
642366081,34892,DOC: Contributing guide docker expansion,fdanieluk,closed,2020-06-20T12:18:45Z,2020-07-19T19:35:32Z,"Added instructions for creating development environment with docker on windows, mac os and unix.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
639894163,34829,Add support for gitpod,jrderuiter,closed,2020-06-16T18:43:09Z,2020-07-19T19:41:20Z,"This PR proposes to add support for Gitpod, which can provide (new) contributors with one-click environment to use for working on the pandas code base. 

The PR includes two main additions:
- .gitpod.yml - Config file for gitpod that specifies which image/Dockerfile to use for gitpod and which init command(s) to run when booting the pod.
- .gitpod.Dockerfile - Dockerfile used to build a base image for the pod. Partially adapted from the existing Dockerfile in the repo.

Some things maybe still to consider:
- Gitpod has some options for providing pre-built workspaces (https://www.gitpod.io/docs/prebuilds/). This may be interesting to enable.
- There is some duplication between the two dockerfiles, but unfortunately the VS Code Dockerfile is not entirely compatible with the gitpod one. As such, the duplication may not be a bad thing. 

- [x] closes #xxxx (N/A)
- [x] tests added / passed (N/A)
- [x] passes `black pandas` (N/A)
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff` (N/A)
- [x] whatsnew entry (N/A)
"
642989738,34933,ENH: add ignore_index option in DataFrame.explode,erfannariman,closed,2020-06-22T11:28:39Z,2020-07-19T21:20:05Z,"- [x] closes #34932 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
658827097,35318,Update multi.py,zky001,closed,2020-07-17T04:03:11Z,2020-07-20T06:04:31Z,"fix bugs be metioned on issue #35301

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
660858124,35343,ENH: change sort behavior in stack() so it's user-directed,pmberkeley,closed,2020-07-19T14:09:36Z,2020-07-20T12:23:56Z,"https://github.com/pandas-dev/pandas/blob/bfac13628394ac7317bb94833319bd6bf87603af/pandas/core/reshape/reshape.py#L623

**Is your feature request related to a problem?**
I need a multiindex dataframe to stack in a specific order. I also need the columns to fail the `this.columns.is_lexsorted()` test (the duplicated column names are how I'm merging the data while stacking it, as a workaround to not being able to get a `linspace` result out of the `rolling` method). Currently, the `sort_index` method is causing the dataframe to reorder alphabetically, which is *not* the order I need it to stack in. 

**Describe the solution you'd like**
One of the following (preferably not the last):
1. be able to choose the index/column order by passing in a list or set of list
2. be able to choose to keep the input order of the stacking indexes/columns (or to have it default to keeping the input order; this seems the easiest)
3. receive a warning or be provided with documentation that indicates all indexes/columns will be reordered alphabetically if they have any duplicated labels, and to name their secondary sorting indexes/columns accordingly.

**API breaking implications**
Unsure about option 1, but the default suggested in option 2 should be fine.

**Describe alternatives you've considered**
This is already a workaround for the `rolling` method not working with non-scalar outputs. I'll be renaming the columns in the short term, but this seems hacky. 

**Additional context**
This issues is closely related to the functionality of `rolling`. The `stack` method is being used/suggested as a workaround for the inability to use `rolling` to output linear results. `MultiIndex` is the pandas way of dealing with additional dimensionality of data; when the rolling method doesn't play nice with adding a dimension to the data set, the user then ends up trying to recreate a `rolling` method equivalent via `stack` (and other means); in order for `stack` and other methods to work well in this context, sorting behavior has to be explicit. "
565026811,31966,sluggish `read_pickle` on pandas 1.0 ?,elikesprogramming,closed,2020-02-14T00:17:45Z,2020-07-20T16:09:46Z,"#### Code Sample, a copy-pastable example if possible

```python
auths = pd.read_pickle('auths.pkl')
```
#### Problem description

Suddenly it takes 14-18 minutes to read a mid-size (2.1 GB) pickle file after upgrading to pandas 1.0. 
Pandas 0.25 can read the very same file in 90 seconds (the file was created using pd.to_pickle a couple of months ago using pandas 0.25).

To try to make sure it has to do with Pandas 1.0, I created a couple of minimal virtual environments, one with latest pandas and one with 0.25. (I am using conda)

```python
conda create -n min_env_101 -y
conda activate min_env_101
conda install -y pandas
```

```python
conda create -n min_env_025 -y
conda activate min_env_025
conda install -y pandas=0.25.1
```

You can see the output of `pd.show_versions()` below. I noticed the latest pandas was installed along python 3.8.1, and pandas 0.25 with python 3.7.6, so I created another virtual environment with latest pandas, but making sure it uses python 3.7.6.

```python
conda create -n min_env_101_py376 python=3.7.6 -y
conda activate min_env_101_py376
conda install -y pandas
```

Then, reading the pickle file takes more than 14 minutes in the virtual environments with pandas 1.0, and just 93 seconds with pandas 0.25.

I checked the issues already reported and did not find anything similar. At some point I thought it was related to [this one](https://github.com/pandas-dev/pandas/issues/29542), but it cannot be since that one was using pandas 0.25.

So I am not sure what is going on (I did not see in pandas 1.0 release notes major changes related to pickle, but maybe I missed something?) and what to do about it.

EDIT: I cannot share the file and I guess you would not accept it either, 'cause reading pickle files may not be safe.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

**virtual environment with pandas 0.25.1**

--- 93.72341346740723 seconds ---
>>>
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>

<details>

[paste the output of ``pd.show_versions()`` here below this line]

**virtual environment with pandas 1.0.1**

--- 854.9136216640472 seconds ---
>>>
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>

<details>

[paste the output of ``pd.show_versions()`` here below this line]

**virtual environment with pandas 1.0.1, making sure it uses python 3.7.6**

--- 1087.5447969436646 seconds ---
>>>
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None


</details>
"
360423260,22712,Unhandled warning from NumPy dev in median,TomAugspurger,closed,2018-09-14T19:16:46Z,2020-07-20T19:31:16Z,"```python
In [1]: import pandas as pd

In [2]: import numpy as np

In [3]: x = np.random.randn(30, 4)

In [4]: x[5:10] = np.nan

In [5]: pd.DataFrame(x).median(1).head()
/Users/taugspurger/miniconda3/envs/travis-37-numpydev/lib/python3.7/site-packages/numpy/lib/function_base.py:3267: RuntimeWarning: All-NaN slice encountered
  r = func(a, **kwargs)
Out[5]:
0   -0.602415
1   -0.118699
2    0.744368
3   -0.993376
4   -0.304693
dtype: float64

In [6]: np.__version__
Out[6]: '1.16.0.dev0+88cbd3d'
```

Do we want to silence these? 

Working around this in https://github.com/pandas-dev/pandas/pull/22699"
393461007,24382,"Implement min, argmin, max, argmax on ExtensionArrays?",TomAugspurger,closed,2018-12-21T13:35:09Z,2020-07-08T14:38:16Z,"We already have `ExtensionArray.argsort` as part of the EA interface, so we should be able to to do min, argmin, max, and argmax as a composition of `argsort` and `__getitem__`. Do we want to do this?"
477974243,27801,ENH: Add argmax and argmin to ExtensionArray,makbigc,closed,2019-08-07T14:45:47Z,2020-07-08T14:47:45Z,"-  closes #24382
-  tests added
-  passes `black pandas`
-  passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
-  whatsnew entry

The methods added in this PR ignore `nan` ,i.e., `skipna=True`. The existing `categorical.min` return `nan` if `categorical` contain any `nan`. This behavior is expected in `test_min_max` (tests/arrays/categorical/test_analytics.py). "
383725160,23865,Inconsistant behaviour of empty groups when grouping with one vs. many ,mojones,closed,2018-11-23T07:35:04Z,2020-07-08T15:35:26Z,"Not sure if this is a bug or expected behaviour, but it's something that catches me out constantly. 

```python
import pandas as pd
# Dummy data. Each row is a day and I'm recording where I worked, 
# what percentage of my time was spent coding, 
# and how many cups of coffee I drank
df = pd.DataFrame({
    'place' : ['office','cafe','office','cafe','office','cafe','cafe','cafe','office','office'],
    'percentage':[5, 15, 72,24, 34, 35, 37, 47, 78, 67],
    'coffee': [1,2,0,0,1,2,0,0,1,1] })

# this includes zero counts
df.groupby(pd.cut(df['percentage'], range(0, 100, 10))).size()

# this doesn't 
df.groupby(
    [pd.cut(df['percentage'], range(0, 100, 10)),
    'place']
     ).size()

```
#### Problem description

I want to look at the distribution of my `percentage` columns, so I do `groupby` so that I can use sensible bins. This is my first `gropuby` in the code example. The output is as expected:

```python
percentage
(0, 10]     1
(10, 20]    1
(20, 30]    1
(30, 40]    3
(40, 50]    1
(50, 60]    0
(60, 70]    1
(70, 80]    2
(80, 90]    0
dtype: int64
```
Including the bins for which there were zero observations (50-60% and 80-90%). 

Now I want to also group by place. This is my second `groupby`. Now my empty groups disappear:

```python
percentage  place 
(0, 10]     office    1
(10, 20]    cafe      1
(20, 30]    cafe      1
(30, 40]    cafe      2
                 office    1
(40, 50]    cafe      1
(60, 70]    office    1
(70, 80]    office    2
dtype: int64
```
When I `unstack` this to make a summary table the empty percentage bins are missing:

```python
place       cafe  office
percentage              
(0, 10]      NaN     1.0
(10, 20]     1.0     NaN
(20, 30]     1.0     NaN
(30, 40]     2.0     1.0
(40, 50]     1.0     NaN
(60, 70]     NaN     1.0
(70, 80]     NaN     2.0
```
This will cause a problem when I go to plot this data, as the spacing on my 'percentage' axis will not be consistent. If I'm just counting the size of the bins then I can get round the problem by reindexing with the original categories:

```python
df.groupby(
    [pd.cut(df['percentage'], range(0, 100, 10)),
    'place']
     ).size().unstack().reindex(pd.cut(df['percentage'], range(0, 100, 10)).cat.categories).fillna(0).stack()

percentage  place 
(0, 10]     cafe      0.0
            office    1.0
(10, 20]    cafe      1.0
            office    0.0
(20, 30]    cafe      1.0
            office    0.0
(30, 40]    cafe      2.0
            office    1.0
(40, 50]    cafe      1.0
            office    0.0
(50, 60]    cafe      0.0
            office    0.0
(60, 70]    cafe      0.0
            office    1.0
(70, 80]    cafe      0.0
            office    2.0
(80, 90]    cafe      0.0
            office    0.0
dtype: float64
```
but this seems like a complicated solution for what should be a relatively common problem. And if I want to do some other type of aggregation, e.g. calculate the mean number of cups of coffee for each group, I can't figure it out:

```python
df.groupby(
    [pd.cut(df['percentage'], range(0, 100, 10)),
    'place']
     )['coffee'].mean()

percentage  place 
(0, 10]     office    1.0
(10, 20]    cafe      2.0
(20, 30]    cafe      0.0
(30, 40]    cafe      1.0
            office    1.0
(40, 50]    cafe      0.0
(60, 70]    office    1.0
(70, 80]    office    0.5
```

I have missing groups, but I can't fix it with `stack/fillna/unstack` as I don't want to fill in a value - I want to leave it as missing data, but still  have the group appear. 

Reading through previous issues, it sounds like I am describing this:

https://github.com/pandas-dev/pandas/issues/8138

but the thread says it's fixed, so I can't figure out what is different in my situation. 


#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.5.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-38-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.23.4
pytest: None
pip: 10.0.1
setuptools: 40.0.0
Cython: None
numpy: 1.15.1
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.4.0
sphinx: 1.8.1
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.2.3
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None


</details>
"
650880985,35121,TST add test case for drop_duplicates,r-toroxel,closed,2020-07-04T12:13:06Z,2020-07-08T15:40:09Z,Add a test case to drop_duplicates for inplace=True.
651584395,35140,Fix regression on datetime in MultiIndex,Dr-Irv,closed,2020-07-06T14:27:12Z,2020-07-08T15:40:22Z,"- [x] closes #35015
- [x] tests added / passed
  - `tests.indexing.multiindex.test_datetime:test_multiindex_datetime_columns`
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
   - Not needed - this was a regression
"
642405318,34909,PERF: avoid creating many Series in apply_standard,jbrockmendel,closed,2020-06-20T16:25:52Z,2020-07-08T15:42:54Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This avoids going through perennial-problem-causing libreduction code (xref #34014, #34080) and instead does the same trick in python-space by re-assigning block.values instead of creating new Series objects.

If we avoid libreduction but _dont_ do this optimization, the most-affected asv is `time_apply_ref_by_name` that clocks in at 6.92x slower.  This achieves parity on that asv.

<s>ATM I'm still getting 4 test failures locally, need to troubleshoot.</s> Update: passing

"
624400336,34366,DOC - Moving Tips and Trick from wiki to Style Guide - added Reading …,vijaysaimutyala,closed,2020-05-25T16:50:26Z,2020-07-08T16:05:44Z,"This changes referring to https://github.com/pandas-dev/pandas/issues/30964 is for removing tips and tricks section page from the [wiki](https://github.com/pandas-dev/pandas/wiki/Tips-&-Tricks-for-pandas-dev) to the [Code Style guide](https://pandas.io/docs/development/code_style.html). 

Since this is my first contribution, I've tried to keep this as simple as possible. @simonjayhawkins mentioned that the conventions for urlopen and ZipFile should be captured under our contributing style guide, testing section. I've seen that there is no Testing section and have created a new section and added these two sections from the Wiki post here. Enabling the Travis-CI and Azure CI pipeline build seemed a bit tough for me. I will try to do the build from my side in my next PR. 

Let me know if any corrections are needed.

"
615253970,34095,ENH: enable concat for nullable Int with other dtypes,jorisvandenbossche,closed,2020-05-09T20:27:49Z,2020-07-08T16:15:56Z,"Following-up on https://github.com/pandas-dev/pandas/pull/33607 which added the `ExtensionDtype._get_common_dtype` method to the EA interface to determine concat/append behaviour, we can now use this to refine the concat behaviour for nullable integer and boolean dtypes.

See comment here: https://github.com/pandas-dev/pandas/pull/33607#discussion_r410127707

For now, the PR only enabled concat of all IntegerDtype objects (any other combination will result in object dtype). But, we can expand this to the combination of IntegerDtype with numpy int dtype, Int with boolean, Int with float, etc

"
645278335,34985,ENH: concat of nullable int + bool preserves int dtype,jorisvandenbossche,closed,2020-06-25T07:47:18Z,2020-07-08T16:20:00Z,Closes #34095
627931094,34495,DOC: Clarify behaviour of pd.merge,bharatr21,closed,2020-05-31T10:00:51Z,2020-07-08T16:21:41Z,"- [x] Closes #34412 (also attempts to clarify #34273)
- [x] tests added / passed (passes `scripts/validate_docstrings.py`)
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry (Not needed)
"
644259319,34962,REF: dont consolidate in BlockManager.equals,jbrockmendel,closed,2020-06-24T01:57:30Z,2020-07-08T16:43:31Z,"Avoid side-effects.

Side-note: with EA.equals recently added to the interface, should we update array_equivalent to use it?  ATM array_equivalent starts by casting both inputs to ndarray."
653397530,35180,Tst verify dropna returns None inplace (#35179),r-toroxel,closed,2020-07-08T15:37:01Z,2020-07-08T16:44:21Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
651525561,35137,BUG:sort_values in groupby make some value lost,leohazy,closed,2020-07-06T13:10:12Z,2020-07-08T21:19:06Z,"- [ .] I have checked that this issue has not already been reported.

- [ .] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example
- source data refer to [b.csv](https://raw.githubusercontent.com/leohazy/bug/master/b.csv)
```python
import pandas as pd

a=pd.read_csv('https://raw.githubusercontent.com/leohazy/bug/master/b.csv')
def holding0(x):
    x[""holding""] = x[""shares""].sum()
    print(x[['close_max','close_min']])   
    return x

def holding(x):
    x[""holding""] = x[""shares""].sum()
    x = x.sort_values(by=[""hold_start""])
    print(x[['close_max','close_min']])   
    return x

def holding1(x):
    x[""holding""] = x[""shares""].sum()
    x['hold_start']=x['hold_start'].apply(lambda x:pd.to_datetime(x,format=""%Y-%m-%d""))
    x = x.sort_values(by=[""hold_start""])
    print(x[['close_max','close_min']])
    return x

def holding2(x):
    x[""holding""] = int(x[""shares""].sum())
    x = x.sort_values(by=[""hold_start""])
    print(x[['close_max','close_min']])  
    return x

def holding3(x):
    x[""holding""] = float(x[""shares""].sum())
    x = x.sort_values(by=[""hold_start""])
    print(x[['close_max','close_min']])  
    return x

def holding4(x):
    x[""holding""] = x[""shares""].sum()
    x.sort_values(by=[""hold_start""],inplace=True)
    print(x[['close_max','close_min']])  
    return x

def holding5(x):
    x[""holding""] = x[""shares""].sum()
    x=x.sort_values(by=[""hold_start""]).copy()
    print(x[['close_max','close_min']])  
    return x

b = a.groupby(by=""companycode"").apply(holding)
```

#### Problem description
- after sort in groupby, several columns got only repeated values, such as ratio,shares,close_max,close_min(**columns after 'ratio'**)
0. apply with hoding(x) function,the 'close_max' and 'close_min'
<details>
<summary>result</summary>
 close_max  close_min

33    16.9286    14.5889
43    16.9286    14.5889
13    16.8893    11.9444
23    16.8893    11.9444
53    16.8893    11.9444
    close_max  close_min
36    16.9286    14.5889
46    16.9286    14.5889
16    16.8893    11.9444
26    16.8893    11.9444
56    16.8893    11.9444
   close_max  close_min
6    16.8893    11.9444
   close_max  close_min
4    16.8893    11.9444
    close_max  close_min
39    16.9286    14.5889
49    16.9286    14.5889
19    16.8893    11.9444
29    16.8893    11.9444
59    16.8893    11.9444
    close_max  close_min
38    16.9286    14.5889
48    16.9286    14.5889
17    16.8893    11.9444
27    16.8893    11.9444
58    16.8893    11.9444
   close_max  close_min
5    16.8893    11.9444
    close_max  close_min
34    16.9286    14.5889
44    16.9286    14.5889
14    16.8893    11.9444
24    16.8893    11.9444
54    16.8893    11.9444
   close_max  close_min
7    16.8893    11.9444
    close_max  close_min
32    16.9286    14.5889
42    16.9286    14.5889
12    16.8893    11.9444
22    16.8893    11.9444
52    16.8893    11.9444
   close_max  close_min
0    16.8893    11.9444
    close_max  close_min
30    16.9286    14.5889
40    16.9286    14.5889
11    16.8893    11.9444
21    16.8893    11.9444
50    16.8893    11.9444
    close_max  close_min
31    16.9286    14.5889
41    16.9286    14.5889
10    16.8893    11.9444
20    16.8893    11.9444
51    16.8893    11.9444
    close_max  close_min
35    16.9286    14.5889
45    16.9286    14.5889
15    16.8893    11.9444
25    16.8893    11.9444
55    16.8893    11.9444
   close_max  close_min
3    16.8893    11.9444
   close_max  close_min
8    16.8893    11.9444
   close_max  close_min
9    16.8893    11.9444
   close_max  close_min
1    16.8893    11.9444
    close_max  close_min
37    16.9286    14.5889
47    16.9286    14.5889
18    16.8893    11.9444
28    16.8893    11.9444
57    16.8893    11.9444
   close_max  close_min
2    16.8893    11.9444
</details>

1. change 'hold_start' into datetime,got expected output
<details>
<summary>result</summary>

close_max  close_min
13    16.8893    11.9444
23    16.8893    11.9444
33    16.9286    14.5889
43    16.9286    14.5889
53    16.8893    11.9444
    close_max  close_min
16      32.56      24.55
26      32.56      24.55
36      30.45      25.90
46      30.45      25.90
56      32.56      24.55
   close_max  close_min
6    36.6211    24.5361
   close_max  close_min
4    76.3976    61.7438
    close_max  close_min
19    38.9398    28.2549
29    38.9398    28.2549
39    25.9433    16.4589
49    25.9433    16.4589
59    38.9398    28.2549
    close_max  close_min
17    22.4004    14.0188
27    22.4004    14.0188
38    14.8609    10.2045
48    14.8609    10.2045
58    22.4004    14.0188
   close_max  close_min
5    30.7038    23.6078
    close_max  close_min
14     120.50       80.3
24     120.50       80.3
34      93.12       70.0
44      93.12       70.0
54     120.50       80.3
   close_max  close_min
7      39.37      27.12
    close_max  close_min
12    77.2825    65.6768
22    77.2825    65.6768
32    79.5022    65.4191
42    79.5022    65.4191
52    77.2825    65.6768
   close_max  close_min
0    34.8514    25.0396
    close_max  close_min
11    135.105   108.1080
21    135.105   108.1080
30    113.939    98.7243
40    113.939    98.7243
50    135.105   108.1080
    close_max  close_min
10    1157.48    984.503
20    1157.48    984.503
31    1219.51   1105.090
41    1219.51   1105.090
51    1157.48    984.503
    close_max  close_min
15    58.1580    43.2304
25    58.1580    43.2304
35    47.4252    39.7685
45    47.4252    39.7685
55    58.1580    43.2304
   close_max  close_min
3     113.34      91.48
   close_max  close_min
8    56.0291    40.9621
   close_max  close_min
9    52.1465    42.1393
   close_max  close_min
1    31.8664    21.9908
    close_max  close_min
18    61.8531    48.5440
28    61.8531    48.5440
37    61.8132    47.8151
47    61.8132    47.8151
57    61.8531    48.5440
   close_max  close_min
2    66.2883    50.5529

</details>

2.  change x[""shares""].sum() into int,got expected output
3.  change x[""shares""].sum() into float,got error
4.  use inplace=true in sort ,got expected output
5.  use copy in sort,got error
6.  use parse_dates=['hold_start'] in read_csv ,got error(against case 1,function holding1(x))

#### Expected Output
got correct values after sort in groupby apply
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.5.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.0.5
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 46.4.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.14.0
pandas_datareader: 0.8.1
bs4              : None
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.1
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
646519087,35022,TST: add test to ensure that df.groupby() returns the missing categories when grouping on 2 pd.Categoricals,smithto1,closed,2020-06-26T21:24:55Z,2020-07-08T21:42:32Z,"- [x] closes #23865
- [x] closes #27075 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

1) _New Tests Addeded_
#23865 #27075 both reported that when df.groupby was called and `by` was set to more than one pd.Categorical column, that any missing categories were not returned, even when observed=False. This issue was fixed in #29690. This Pull Request adds tests to make sure that this correct behaviour is enforced by tests. 

2) _New Bug Found_
Testing did reveal one further issue: DataFrameGroupBy.count() returns NaN for missing Categories, when it should return a count of 0.  SeriesGroupBy.count() does return 0, which is the expected behaviour. I have raised an issue for this bug (#35028 ) and marked the test with an xfail. When the bug is fixed, the xfail will cause the tests to fail, and the xfail can be removed. 

3) _Existing Test Changed as it had the Wrong Expected Result_
A similar issue was reported for .sum() in #31422: missing categories return a sum of NaN when they should return a sum of 0. There was a mistake on the existing test for SeriesGroupBy.sum(), as it said the expected output was NaN (see below) when it should have been 0.
https://github.com/pandas-dev/pandas/blob/0159cba6eb14983ab7eaf38ff138c3c397a6fe3b/pandas/tests/groupby/test_categorical.py#L1315

I have changed this so that the expected output is 0 (this is inline with the comment here: https://github.com/pandas-dev/pandas/issues/31422#issuecomment-579826193 ) and marked the tests for .sum() with xfail. When the bug is addressed, the xfail will cause the tests to fail, and the xfails can be removed. "
461537792,27075,Groupby ignores unobserved combinations when passing more than one categorical column even if observed=True,harmbuisman,closed,2019-06-27T13:43:34Z,2020-07-08T21:47:42Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np

df_test = pd.DataFrame()
df_test['A'] = pd.Series(np.arange(0,2), dtype='category').cat.set_categories(list(range(0,3)))
df_test['B'] = pd.Series(np.arange(10,12), dtype='category').cat.set_categories(list(range(10,13)))

print(""Test DF:"")
print(df_test)

print(""\nThe following are as expected, unobserved categories have size = 0:"")
print(df_test.groupby('A').size())
print(df_test.groupby('B').size())

print(""\nThe following does not consider categories, I would expect 9 result lines here:"")
print(df_test.groupby(['A','B']).size())

print(""\nExpected:"")
print(pd.DataFrame({'A':list(range(0,3))*3, 'B':list(range(10,13))*3, '':[1]*2+[0]*7 }).set_index(['A','B']))
```
![image](https://user-images.githubusercontent.com/7702207/60271106-07b01a80-98f2-11e9-9ec2-f6bc07e15069.png)

#### Problem description
groupby([cols]) gives back a result for all categories if only one column that is categorical is provided (e.g. ['A']), but it only shows the observed combinations if multiple categorical columns are provided ['A', 'B'], regardless of the setting of observed. I would expect that I would get a result for all combinations of the categorical columns.

#### Expected Output
A result for all combinations of the categorical categories of the groupby columns. For the example above:
A B    
0 10  1
1 11  1
2 12  0
0 10  0
1 11  0
2 12  0
0 10  0
1 11  0
2 12  0

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.2
pytest: 4.5.0
pip: 19.1.1
setuptools: 41.0.1
Cython: 0.29.7
numpy: 1.16.3
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 7.5.0
sphinx: 2.0.1
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: 1.2.1
tables: 3.5.1
numexpr: 2.6.9
feather: None
matplotlib: 3.0.3
openpyxl: 2.6.2
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.8
lxml.etree: 4.3.3
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.3.3
pymysql: None
psycopg2: 2.7.6.1 (dt dec pq3 ext lo64)
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.7.0
gcsfs: None
</details>

"
651775586,35149,ASV: tslibs.fields,jbrockmendel,closed,2020-07-06T19:41:32Z,2020-07-08T21:57:08Z,
638166868,34748,"BUG: read_excel issue - 2 (when ""column name"" is a date/time)",kuraga,closed,2020-06-13T13:59:23Z,2020-07-08T22:03:48Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
pd.read_excel('test.xlsx', header=[0, 1], index_col=0, engine='openpyxl')
```
(`engine` could be `xlrd` or `openpyxl`, doesn't matter.)

[test.xlsx](https://github.com/pandas-dev/pandas/files/4774723/test.xlsx)

#### Problem description

Exception:
```
Traceback (most recent call last):
  File ""test.py"", line 3, in <module>
    pd.read_excel('test.xlsx', header=[0, 1], index_col=0, engine='xlrd')
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 334, in read_excel
    **kwds,
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 888, in parse
    **kwds,
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 512, in parse
    **kwds,
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py"", line 2201, in TextParser
    return TextFileReader(*args, **kwds)
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py"", line 880, in __init__
    self._make_engine(self.engine)
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py"", line 1126, in _make_engine
    self._engine = klass(self.f, **self.options)
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py"", line 2298, in __init__
    self.columns, self.index_names, self.col_names
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py"", line 1508, in _extract_multi_indexer_columns
    for r in header
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py"", line 1508, in <listcomp>
    for r in header
TypeError: object of type 'datetime.datetime' has no len()
```

#### Expected Output

No exceptions.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.42-calculate
machine          : x86_64
processor        : Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz
byteorder        : little
LC_ALL           : None
LANG             : ru_RU.utf8
LOCALE           : ru_RU.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.4.0.post20200518
Cython           : 0.29.17
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.8
numba            : None
</details>
"
652612369,35168,REF: collect get_dst_info-using functions in tslibs.vectorized,jbrockmendel,closed,2020-07-07T20:22:31Z,2020-07-08T22:06:58Z,"I'm open to better names for the new module.

With these all in the same place, we can separate out helper functions and de-duplicate a whole bunch of verbose code."
650235334,35103,TYP: maybe_get_tz,jbrockmendel,closed,2020-07-02T22:27:09Z,2020-07-08T22:07:15Z,
652976794,35173,BUG: groupby / resample / aggregate produces wrong results,roy2006,closed,2020-07-08T05:25:04Z,2020-07-09T11:06:29Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
data = """"""        date  revenue name
0 2020-02-24        9  n_1
1 2020-05-12        8  n_2
2 2020-03-28        9  n_2
3 2020-01-14        2  n_0""""""

df = pd.read_csv(StringIO(data), sep = ""\s+"", engine = ""python"")
df.date = pd.to_datetime(df.date)     

gb = df.groupby(""name"").resample(""M"", on=""date"")

res1 = gb.sum()[['revenue']]

# ==>
#                  revenue
# name date               
# n_0  2020-01-31        2
# n_1  2020-02-29        9
# n_2  2020-03-31        9
#      2020-04-30        0
#      2020-05-31        8

res2 = gb.aggregate({'revenue':'sum'})

# ==>
#                  revenue
# name date               
# n_0  2020-05-31        2
# n_1  2020-01-31        9
# n_2  2020-02-29        8
#      2020-03-31        9

```

#### Problem description

As the code sample above shows, there are two issues: 

* `agg(sum)` produces different results from just `sum`. 
* The results of `agg(sum)` are plain wrong. The name ""n_2"" has no data in Feb, and yet - the aggregation shows that it does. 

#### Expected Output

The correct output is the one produced by `sum`. 

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.5.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.6.0
Cython           : None
pytest           : 5.4.3
hypothesis       : 5.18.0
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.10.1
pandas_datareader: 0.8.1
bs4              : 4.9.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.16
tables           : None
tabulate         : None
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.9
numba            : None

</details>
"
652239793,35158,BUG: datetime total_seconds inaccurate,frndrs,closed,2020-07-07T11:19:12Z,2020-07-09T13:56:33Z,"- [x] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame({'x':['2020-07-07T10:33:14.971260986',
       '2020-07-07T10:17:14.971260986', '2020-07-07T10:11:14.971260986',
       '2020-07-07T09:59:13.971260986'],'y':['2020-07-07T10:35:14.971260986',
       '2020-07-07T10:24:14.971260986', '2020-07-07T10:15:14.971260986',
       '2020-07-07T10:04:14.971260986']})

df.x = pd.to_datetime(df.x)
df.y = pd.to_datetime(df.y)

print((df.y-df.x).dt.total_seconds()[0]) # 120.00000000000001
print((df.y-df.x).apply(lambda x: x.total_seconds())[0]) #120.0

```

#### Problem description

There seems to be a loss of precision when doing date substractions and using total_seconds method.

As a reference, using an apply with the standard library total_seconds yields the correct value.

#### Expected Output

Value should be 120.0
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.0.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.0.5
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.14.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.11
tables           : None
tabulate         : 0.8.6
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
650115592,35098,BUG: fix union_indexes not supporting sort=False for Index subclasses,AlexKirko,closed,2020-07-02T18:05:31Z,2020-07-09T15:11:45Z,"- [X] closes #35092
- [X] 1 tests added / 1 passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

# Scope of PR
The reason for `DataFrame.append` unnecessarily sorting columns turned out to be that when merging indexes we were not passing the `sort` argument for an `Index` subclass. This PR fixes this bug.

# Details
At some point down the call stack, `DataFrame.append` calls `union_indexes` in `core.indexes.api`. In it we detect the type of our indexes: 
```python
indexes, kind = _sanitize_and_check(indexes)
```
We don't pass `sort` for `kind == 'special'` in `union_indexes`.

I had some other ideas, but what I ended up doing is to simply pass the `sort` argument to `Index.union` for `kind == 'special'`. We do have to ignore the `sort` argument for `[MultiIndex, RangeIndex, DatetimeIndex, CategoricalIndex]` to avoid breaking tests and backward compatibility, but it doesn't make sense for these ones to not be sorted when joined anyway, in my opinion.

# Performance
There are no changes to algorithms called. I still did some benchmarking to compare with my previous approach. The current solution didn't have any negative performance impact.
"
651521911,35136,CI: pin isort version,AlexKirko,closed,2020-07-06T13:04:58Z,2020-07-09T15:12:19Z,"- [X] xref #35134
- [X] 0 tests added / 0 passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry. __this is a CI PR. I don't think we touch whatsnew for those.__

### Scope
<s>Remove the `--recursive` keyword from `code_checks.sh`, so that the newest version of isort can run without errors (it does recursive sorting by default).</s>
Pin the version instead, because version 5 has changed the way isort sorts imports. Needs more work to switch safely.
"
606958457,33804,BUG: support corr and cov functions for custom BaseIndexer rolling windows,AlexKirko,closed,2020-04-26T08:51:23Z,2020-07-09T15:12:53Z,"- [X] closes #32865
- [X] reverts #33057
- [X] 2 tests added / 2 passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

## Scope of PR
This is the final PR needed to solve #32865. It fixes `cov` and `corr` by passing the correct argument to `rolling` constructors if the functions are called with a custom `BaseIndexer` subclass. A separate test is added for `cov` and `corr` as the previous tests were single-variable and wouldn't work with two columns of data. I also propose we revert #33057.

## Details
The reason `cov` and `corr` didn't work was that we were passing `_get_window` results (usually, window width), instead of the `BaseIndexer` subclass to `rolling` constructors called inside the function. Passing `self.window` (when necessary) fixes the issue.

The new test asserts only against an explicit array, because shoehorning a comparison against `np.cov` an `np.corrcoef` is ugly (I've tried). Our `rolling.apply` isn't really suited to be called with bivariate statistics functions, and the necessary gymnastics didn't look good to me. I don't believe the minor benefit would be worth, but please say if you think coomparing against numpy is necessary.

Thanks to @mroeschke for implementing the `NotImplemented` error-raising behavior in #33057 . Now that all the functions are fixed, I propose we revert that commit.

## Background on the wider issue
We currently don't support several rolling window functions when building a rolling window object using a custom class descended from `pandas.api.indexers.Baseindexer`. The implementations were written with backward-looking windows in mind, and this led to these functions breaking.
Currently, using these functions returns a `NotImplemented` error thanks to #33057, but ideally we want to update the implementations, so that they will work without a performance hit. This is what I aim to do over a series of PRs.

## Perf notes
No changes to algorithms."
646000188,35004,BUG: Avoids b' prefix for bytes in to_csv() (#9712),sidhant007,closed,2020-06-26T04:33:51Z,2020-07-09T15:56:41Z,"Avoids b' prefix written for bytes in the `to_csv()` method (in accordance with this [proposal](https://github.com/pandas-dev/pandas/issues/9712#issuecomment-635803657))

The `encoding` parameter passing in `to_csv()` method is used as the encoding scheme to decode the bytes.

Example:
```python
import pandas as pd
import sys
df = pd.DataFrame({
    b'hello': [b'abc', b'def'],
    b'world': ['pqr', 'xyz']
})
df.to_csv(sys.stdout, encoding='utf-8')
```

After the bug fix will print:
```
,hello,world
0,abc,pqr
1,def,xyz
```

Currently the `to_csv()` method prints:
```
,b'hello',b'world'
0,b'abc',pqr
1,b'def',xyz
```

- [x] closes #9712
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
654325311,35196,PERF: lookups on Timestamp attributes,jbrockmendel,closed,2020-07-09T20:23:12Z,2020-07-09T21:37:32Z,"This is pretty ugly, but it avoids python-space attribute lookups e.g. `__Pyx_PyObject_GetAttrStr(((PyObject *)__pyx_v_self), __pyx_n_s_microsecond)`

cc @scoder is there a way to make this unnecessary?  I'm imagining a cdef property that aliased these attributes appropriately."
654087870,35192,Create sayan2,osayanhu,closed,2020-07-09T14:05:02Z,2020-07-09T22:03:18Z,"code to extract row from database and replace values within row

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
651929473,35154,"REF: remove always-UTC arg from tz_convert, tz_convert_single",jbrockmendel,closed,2020-07-07T01:53:44Z,2020-07-09T22:04:13Z,
653367023,35179,TST verify inplace ops return None,r-toroxel,closed,2020-07-08T14:56:54Z,2020-07-09T22:05:34Z,"test if pandas methods, when called inplace, return None

drop_duplicates https://github.com/pandas-dev/pandas/pull/35121. **merged**
dropna https://github.com/pandas-dev/pandas/pull/35180
"
653136998,35175,DOC: fix code snippets for generic indexing,maximesong,closed,2020-07-08T09:24:03Z,2020-07-09T22:06:43Z,"The code example for generic indexing is the same as positional/label indexing above. 

I think generic indexing here intended to be referred to something like `df[0:3]`  and `df['bar':'kar']`"
610759575,33919,Performance regression in gil.ParallelDatetimeFields.time_period_to_datetime,TomAugspurger,closed,2020-05-01T13:47:36Z,2020-07-09T22:08:17Z,"```python
import pandas as pd
import numpy as np
from pandas._testing import test_parallel

N = 10 ** 6
dti = pd.date_range(""1900-01-01"", periods=N, freq=""T"")
period = dti.to_period(""D"")



@test_parallel(num_threads=2)
def run(period):
    period.to_timestamp()

%timeit run(period)
```

```
# 1.0.2
96.8 ms ± 1.27 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

# master
129 ms ± 1.28 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

https://pandas.pydata.org/speed/pandas/index.html#gil.ParallelDatetimeFields.time_period_to_datetime?commits=d106b81ce532bc71ec6cced944ddb751a4b0e5a3-577de1c6b5cda7f5ae0e4832c2bc3f97ca186e9b points to https://github.com/pandas-dev/pandas/compare/d106b81ce532bc71ec6cced944ddb751a4b0e5a3...577de1c6b5cda7f5ae0e4832c2bc3f97ca186e9b, perhaps https://github.com/pandas-dev/pandas/pull/33491 or https://github.com/pandas-dev/pandas/pull/33047 (cc @jbrockmendel)"
652841770,35171,PERF: periodarr_to_dt64arr,jbrockmendel,closed,2020-07-08T02:17:05Z,2020-07-09T23:21:34Z,"- [x] closes #33919
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Re-use `ensure_datetime64ns` for the subset of cases where that is applicable.  I'm at a loss for why we get such a slowdown for the other cases.

```
       before           after         ratio
     [42a6d445]       [a0ece778]
     <master>         <perf-periodarr_to_dt64arr>
+      48.7±0.8ms          133±6ms     2.72  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1000000, 4000)
+        48.0±1ms        131±0.8ms     2.72  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1000000, 5000)
+        493±10μs      1.33±0.05ms     2.70  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(10000, 4000)
+      47.8±0.4ms          125±3ms     2.62  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1000000, 4006)
+        599±20μs      1.47±0.07ms     2.46  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(10000, 2000)
+      60.6±0.9ms          149±7ms     2.45  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1000000, 3000)
+        524±40μs      1.27±0.04ms     2.42  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(10000, 4006)
+        587±10μs      1.42±0.04ms     2.41  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(10000, 1000)
+      59.9±0.8ms        144±0.9ms     2.40  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1000000, 1000)
+        526±40μs      1.26±0.05ms     2.39  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(10000, 5000)
+        61.0±1ms          144±2ms     2.35  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1000000, 2011)
+      60.8±0.5ms        143±0.9ms     2.35  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1000000, 2000)
+      7.72±0.3μs         17.7±3μs     2.30  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(100, 4000)
+      63.7±0.6ms          145±3ms     2.28  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1000000, 1011)
+     3.04±0.06μs         6.94±1μs     2.28  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1, 6000)
+        611±10μs      1.38±0.06ms     2.27  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(10000, 2011)
+        653±20μs      1.44±0.03ms     2.20  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(10000, 1011)
+     2.82±0.07μs       6.12±0.2μs     2.17  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(0, 6000)
+     2.81±0.06μs       6.06±0.2μs     2.16  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1, 9000)
+     2.99±0.06μs       6.16±0.2μs     2.06  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(0, 8000)
+     2.80±0.06μs       5.75±0.2μs     2.05  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(0, 11000)
+     2.95±0.09μs       6.02±0.4μs     2.04  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(0, 9000)
+     2.94±0.06μs       5.97±0.2μs     2.03  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(0, 7000)
+        701±50μs      1.41±0.01ms     2.01  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(10000, 3000)
+      3.09±0.1μs       6.18±0.1μs     2.00  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1, 7000)
+      3.02±0.2μs       6.02±0.3μs     1.99  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1, 8000)
+      3.05±0.1μs       5.95±0.2μs     1.95  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1, 10000)
+      7.98±0.3μs       15.4±0.3μs     1.93  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(100, 4006)
+      9.01±0.4μs       17.3±0.8μs     1.92  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(100, 2011)
+      2.99±0.1μs       5.66±0.2μs     1.89  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1, 11000)
+     3.09±0.09μs       5.82±0.1μs     1.88  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(0, 10000)
+      8.20±0.4μs       15.3±0.5μs     1.87  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(100, 5000)
+      9.37±0.6μs       17.4±0.5μs     1.86  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(100, 2000)
+      9.28±0.6μs       17.2±0.7μs     1.85  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(100, 3000)
+      9.61±0.3μs       17.6±0.6μs     1.83  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(100, 1011)
+      10.1±0.7μs       17.5±0.7μs     1.73  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(100, 1000)
-      2.93±0.1μs       2.48±0.1μs     0.85  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1, 4000)
-     2.90±0.04μs      2.44±0.08μs     0.84  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1, 2011)
-     2.92±0.06μs       2.44±0.1μs     0.84  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1, 1011)
-      3.08±0.1μs       2.56±0.1μs     0.83  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(0, 2011)
-      3.03±0.2μs      2.52±0.09μs     0.83  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1, 4006)
-      9.41±0.4μs       7.79±0.2μs     0.83  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(100, 8000)
-      2.97±0.1μs       2.42±0.1μs     0.81  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1, 2000)
-      2.99±0.1μs      2.40±0.07μs     0.80  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(0, 4006)
-      3.18±0.1μs       2.55±0.1μs     0.80  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(0, 1000)
-      3.21±0.2μs      2.48±0.03μs     0.77  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1, 5000)
-      3.07±0.2μs      2.34±0.05μs     0.76  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1, 3000)
-      3.08±0.1μs      2.34±0.04μs     0.76  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(0, 5000)
-      3.23±0.1μs       2.43±0.1μs     0.75  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1, 1000)
-      3.16±0.1μs      2.37±0.05μs     0.75  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(0, 2000)
-      3.28±0.3μs      2.33±0.08μs     0.71  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(0, 4000)
-      3.44±0.3μs       2.36±0.1μs     0.69  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(0, 1011)
-        398±20μs          242±7μs     0.61  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(10000, 6000)
-        39.0±1ms       23.7±0.4ms     0.61  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1000000, 6000)
-      62.4±0.5ms       28.9±0.5ms     0.46  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1000000, 11000)
-      64.4±0.2ms       28.6±0.4ms     0.44  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1000000, 10000)
-        657±20μs         290±10μs     0.44  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(10000, 11000)
-      64.7±0.4ms       27.9±0.4ms     0.43  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1000000, 9000)
-        659±20μs          279±8μs     0.42  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(10000, 7000)
-      64.1±0.6ms       27.1±0.4ms     0.42  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1000000, 8000)
-      64.6±0.9ms       26.7±0.3ms     0.41  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1000000, 7000)
-        661±20μs          272±8μs     0.41  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(10000, 8000)
-        661±10μs         269±10μs     0.41  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(10000, 9000)
-        698±50μs          282±7μs     0.40  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(10000, 10000)
-      3.02±0.1μs         344±10ns     0.11  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(0, 12000)
-      3.21±0.2μs         342±10ns     0.11  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1, 12000)
-      10.6±0.5μs         339±10ns     0.03  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(100, 12000)
-        661±20μs          346±5ns     0.00  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(10000, 12000)
-      62.3±0.6ms         368±10ns     0.00  tslibs.period.TimePeriodArrToDT64Arr.time_periodarray_to_dt64arr(1000000, 12000)
```"
654402672,35200,Strange dtype behavior for numeric strings when using to_csv and read_csv,rjboczar,closed,2020-07-09T23:13:21Z,2020-07-09T23:24:28Z,"Version 0.25.2

The following code writes out a column containing strings of the numerals 0 to N-1 plus a string 'None', and then reads it back in.
```python
for N in [10,100,1000,10000,100000,1000000]:
    A = pd.DataFrame(list(range(N)) + list(range(N)), columns=['a'])
    A['a'] = A['a'].map(str)
    A.loc[N-1,'a'] = 'None'
    # Test
    A.to_csv('./test.csv', index=False)
    B = pd.read_csv('./test.csv')
    print(len(A['a'].unique()), len(B['a'].unique()))
```
However, the output is
```
11 11
101 101
1001 1001
10001 10001
100001 100001
1000001 1524288
```
_Only_ at the N=1000000 case, the values read back in get cast to a mix of int and string, and a warning is thrown

```
.../interactiveshell.py:3049: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.
  interactivity=interactivity, compiler=compiler, result=result)
```"
255270013,17440,Index gets lost when DataFrame melt method is used,NiklasKeck,closed,2017-09-05T13:03:41Z,2020-07-09T23:34:45Z,"#### Index gets lost when DataFrame melt method is used

```python
import pandas as pd
import numpy as np
df = pd.DataFrame({""Numbers_1"":range(0,3),
                   ""Numbers_2"":range(3,6),
                   ""Letters"":[""A"",""B"",""C""]})
df.set_index(""Letters"",inplace=True)
print(df)
```
         
Letters | Numbers_1 |  Numbers_2             
| ------| ------| ------| 
A        |        0      |    3
B        |        1      |    4
C        |        2      |    5
```python

df_melted = df.melt()
print(df_melted)
```
 .     |variable | value
| ------| ------| ------| 
0 | Numbers_1   |   0
1 | Numbers_1   |   1
2 | Numbers_1   |   2
3 | Numbers_2   |   3
4 | Numbers_2   |   4
5 | Numbers_2   |   5

#### Problem description
When melting a dataframe, I expected the original index to be reused. However, the original index is lost in the melt method. This is probably meant by wesm's comment (# TODO: what about the existing index?)
https://github.com/pandas-dev/pandas/blob/133a2087d038da035a57ab90aad557a328b3d60b/pandas/core/reshape/reshape.py#L715 

#### Expected Output
I would expect something like
```python
n_row,n_col = df.shape
index_melted = list(df.index.get_values())*n_col
melt_id = list(np.arange(n_col).repeat(n_row))
temp = list(zip(*[index_melted,melt_id]))

index_melted_uniq = pd.MultiIndex.from_tuples(temp,names=[df.index.names[0], 'melt_id'])
index_numbers = list(range(df.shape[1]))*n_row

data = {'variable':df.columns.repeat(n_row),
        ""value"":df.values.ravel('F')}

df_expected = pd.DataFrame(data,columns = [""variable"",""value""], index=index_melted_uniq)
print(df_expected)
```     
Letters | melt_id | variable | value
|------|------|------|------|
A    |   0    |    Numbers_1   |   0
B    |   0     |   Numbers_1   |   1
C   |    0    |    Numbers_1   |   2
A   |    1    |    Numbers_2   |   3
B   |    1    |    Numbers_2   |   4
C   |    1   |     Numbers_2   |   5

Where Letters and melt_id are two multiindex levels and variable and value are actual columns.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: d0f62c2816ada96a991f5a624a52c9a4f09617f7
python: 3.6.2.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en
LOCALE: None.None

pandas: 0.21.0.dev+420.gd0f62c2
pytest: 3.2.1
pip: 9.0.1
setuptools: 36.2.2.post20170724
Cython: 0.26
numpy: 1.13.1
scipy: None
pyarrow: None
xarray: None
IPython: 6.1.0
sphinx: 1.6.3
patsy: None
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.9999999
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
641479743,34863,PERF: to speed up rendering of styler,jihwans,closed,2020-06-18T18:59:20Z,2020-07-09T23:36:05Z,"see https://github.com/pandas-dev/pandas/issues/19917#issuecomment-646209895

- [x] closes #19917
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
649286148,35090,TST: update gbq service account key,tswast,closed,2020-07-01T20:06:54Z,2020-07-09T23:41:00Z,"Re-enable gbq integration tests.

- [ ] closes #34779
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
638458586,34779,CI: gbq tests failing,jreback,closed,2020-06-15T00:02:51Z,2020-07-09T23:41:00Z,"https://travis-ci.org/github/pandas-dev/pandas/builds/698328374
and specificallly these 2 builds: https://travis-ci.org/github/pandas-dev/pandas/jobs/698328378

cc @alimcmaster1 @tswast 

if you have any insights"
645745540,34996,"TST: df.loc[:, 'col'] returning a view, but df.loc[df.index, 'col'] returning a copy",arw2019,closed,2020-06-25T17:51:05Z,2020-07-09T23:42:01Z,"- [x] closes #15631
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
654385942,35199,"REF: re-use get_firstbday, get_lastbday in fields.pyx",jbrockmendel,closed,2020-07-09T22:27:50Z,2020-07-09T23:46:48Z,
621022573,34256,BUG: DataFrame.isin fails when other is a categorical series,brandon-b-miller,closed,2020-05-19T14:27:23Z,2020-07-10T00:08:27Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
import pandas as pd
print(pd.__version__)

x = pd.DataFrame.from_dict({'a':[1,2,3], 'b':[4,5,6]})
y = pd.DataFrame({'a':[1,2,3]}, dtype='category')
print(x.isin(y))

y = pd.Series([1,2,3]).astype('category')
print(x.isin(y))

1.0.3
      a      b
0  True  False
1  True  False
2  True  False
Traceback (most recent call last):
  File ""/home/brmiller/repro.py"", line 9, in <module>
    print(x.isin(y))
  File ""/home/brmiller/anaconda3/envs/pandas1/lib/python3.7/site-packages/pandas/core/frame.py"", line 8423, in isin
    return self.eq(values.reindex_like(self), axis=""index"")
  File ""/home/brmiller/anaconda3/envs/pandas1/lib/python3.7/site-packages/pandas/core/ops/__init__.py"", line 814, in f
    self, other, op, fill_value=None, axis=axis, level=level
  File ""/home/brmiller/anaconda3/envs/pandas1/lib/python3.7/site-packages/pandas/core/ops/__init__.py"", line 618, in _combine_series_frame
    new_data = left._combine_match_index(right, func)
  File ""/home/brmiller/anaconda3/envs/pandas1/lib/python3.7/site-packages/pandas/core/frame.py"", line 5317, in _combine_match_index
    new_data = func(self.values.T, other.values).T
  File ""/home/brmiller/anaconda3/envs/pandas1/lib/python3.7/site-packages/pandas/core/ops/common.py"", line 64, in new_method
    return method(self, other)
  File ""/home/brmiller/anaconda3/envs/pandas1/lib/python3.7/site-packages/pandas/core/arrays/categorical.py"", line 72, in func
    raise ValueError(""Lengths must match."")
ValueError: Lengths must match.

```

#### Problem description

This operation previously worked in pandas 0.25.3 and gave the same result as the case when other is a single column DataFrame. 

#### Expected Output
```
      a      b
0  True  False
1  True  False
2  True  False
      a      b
0  True  False
1  True  False
2  True  False
```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-76-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1
setuptools       : 46.4.0.post20200518
Cython           : 0.29.17
pytest           : 5.4.2
hypothesis       : 5.14.0
sphinx           : 3.0.3
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.14.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.0
pytables         : None
pytest           : 5.4.2
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.49.1
</details>
"
624143218,34363,TST: category isin on frame,vampypandya,closed,2020-05-25T08:42:07Z,2020-07-10T00:08:31Z,"- [x] closes #34256
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
602816759,33659,ENH: Add optional argument index to pd.melt to maintain index values,Rik-de-Kort,closed,2020-04-19T20:49:20Z,2020-07-10T07:44:49Z,"Finishing up a stale PR idea: #28859 and #17459

 Has some tests and better code.
I think it's fair to duplicate the index values and not bend over backwards to maintain uniqueness like in previous iterations.

Apologies for the mess, it was a quick job and I didn't want to spend an hour fiddling with the commits.

Finally, I deleted some ignore type comments for mypy because the commits weren't going on my system. Is there some other fix for that? Other than that I think it's good to go.

- [X] closes #17440
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
- [x] Reconsider API design
- [x] Add usage examples in whatsnew, docstring, and reshaping.rst.
"
650643634,35112,BUG: Mixed DataFrame with Extension Array incorrect aggregation,simonjayhawkins,closed,2020-07-03T14:51:57Z,2020-07-10T09:14:03Z,"- [ ] closes #34520 by reverting #32950
"
619499931,34210,BUG: DataFrame with Int64 columns casts to float64 with .max()/.min(),simonjayhawkins,closed,2020-05-16T15:35:35Z,2020-07-10T09:14:27Z,"- [ ] closes #32651
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
653459826,35181,TST Verifiy that dropna returns none when called inplace (#35179),r-toroxel,closed,2020-07-08T17:09:01Z,2020-07-10T10:20:35Z,"Closes https://github.com/pandas-dev/pandas/issues/35179
"
647054556,35048,Performance regression in arithmetic.ApplyIndex.time_apply_index,TomAugspurger,closed,2020-06-29T01:42:56Z,2020-07-10T12:13:20Z,"https://pandas.pydata.org/speed/pandas/index.html#arithmetic.ApplyIndex.time_apply_index?p-offset=%3CMonthBegin%3E&commits=55adcb8688582e3a3be66b1036d3604327ae5071-b05e6b1402124a062608b97ca02af0cfb13568d1, https://github.com/pandas-dev/pandas/compare/55adcb8688582e3a3be66b1036d3604327ae5071...b05e6b1402124a062608b97ca02af0cfb13568d1 (30 commits in there)."
561716539,31783,read_excel error for header=None and index_col as list,lcmcninch,closed,2020-02-07T15:44:24Z,2020-07-10T13:54:00Z,"#### Code Sample

```python
import pandas as pd
df = pd.read_excel('Book1.xlsx', header=None, index_col = [0, 1])

```
#### Problem description

Above leads to the following error. This seems to be related to the combination of header=None and index_col as a list. If I give an integer index_col, or leave out header=None (or specify a header) there is no error.

The only other reference to this issue I found was [this stack overflow question](https://stackoverflow.com/questions/48334347/read-excel-in-pandas-giving-error-for-no-header-and-multiple-index-cols).

I am seeing this in the latest release (1.0.1) as well as an older one (0.25.3). I haven't been able to install from master because I'm working on windows and don't have a compiler set up.

Example exel file (although the contents doesn't matter, it could be workbook with an empty sheet): [Book1.xlsx](https://github.com/pandas-dev/pandas/files/4171589/Book1.xlsx)


```
<redact>\lib\site-packages\pandas\io\excel\_base.py in read_excel(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)
    332         convert_float=convert_float,
    333         mangle_dupe_cols=mangle_dupe_cols,
--> 334         **kwds,
    335     )
    336

<redact>\lib\site-packages\pandas\io\excel\_base.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)
    883             convert_float=convert_float,
    884             mangle_dupe_cols=mangle_dupe_cols,
--> 885             **kwds,
    886         )
    887

<redact>\lib\site-packages\pandas\io\excel\_base.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)
    467                 # Forward fill values for MultiIndex index.
    468                 if not is_list_like(header):
--> 469                     offset = 1 + header
    470                 else:
    471                     offset = 1 + max(header)

TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'
```


#### Expected Output

A DataFrame with two column indexes.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 13, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
646766159,35035,BUG: fix read_excel error for header=None and index_col as list #31783,rjfs,closed,2020-06-27T21:11:10Z,2020-07-10T13:54:04Z,"- [x] closes #31783
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
654495510,35205,CLN: tighten types to get_rule_month,jbrockmendel,closed,2020-07-10T04:11:10Z,2020-07-10T14:12:09Z,We're a bit inconsistent about whether we are extracting freq.freqstr vs freq.rule_code; I'd like to get DateOffsets out of libparsing altogether before long.
654318468,35195,PERF: MonthOffset.apply_index,jbrockmendel,closed,2020-07-09T20:10:39Z,2020-07-10T14:12:55Z,"- [x] closes #35048
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

```
import pandas as pd
import numpy as np

N = 10000
rng = pd.date_range(start=""1/1/2000"", periods=N, freq=""T"")
offset = pd.offsets.MonthBegin()

In [6]: %timeit offset + rng
463 µs ± 19 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)   # <-- PR
508 µs ± 18.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  # <-- 1.0.4
736 µs ± 15 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  # <-- master
```"
654384858,35198,REF: remove libresolution,jbrockmendel,closed,2020-07-09T22:25:16Z,2020-07-10T14:15:20Z,"It only has one function left, and that fits fine in fields.pyx"
654819368,35213,Add xarray copyright notice to comply with reuse under Apache License,jthielen,closed,2020-07-10T14:24:58Z,2020-07-10T14:55:40Z,"In https://github.com/xarray-contrib/pint-xarray/pull/11, @keewis and I noticed that pandas's (pandas'?...possessive forms are weird) reuse with modification of xarray code added in https://github.com/pandas-dev/pandas/commit/eee83e23ba0c5b32e27db3faca931ddb4c9619aa did not include xarray's copyright notice as required by the Apache License, Version 2.0 (which is not included in the license text itself, unlike MIT or BSD). I'm not sure how big of a deal this is, but given that pint-xarray is going ahead and modeling its code citation practice after that of pandas, I wanted to see what the thoughts are here on it. This PR takes a simple fix and just adds the copyright notice to top of the `LICENSES/XARRAY_LICENSE` file.

Also, I'm not sure if this requires a whatsnew entry or not.

- ~~closes #xxxx~~
- ~~tests added / passed~~
- ~~passes `black pandas`~~
- ~~passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`~~
- [ ] whatsnew entry
"
654461564,35204,QST: value assignment by using '.loc' with boolean and label indexing,jaydu1,closed,2020-07-10T02:21:36Z,2020-07-10T15:26:11Z,"- [Yes] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [No] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

To assign values from one column of a dataframe to another column with boolean and label indexing, I found the following code works. But I didn't find any document about it, so I am wondering if this method is proper/safe to use? 
Thanks.

```python
df = pd.DataFrame({'A':np.arange(5),
                 'B':[0,1,0,1,0],
                 'C':-np.ones(5)})
print(df)
df.loc[df['B']==1, 'A'] = df['C']
# Value error if we use 
# df.loc[df['B']==1, 'A'] = -np.ones(5)
print(df)
'''
Output:
   A  B    C
0  0  0 -1.0
1  1  1 -1.0
2  2  0 -1.0
3  3  1 -1.0
4  4  0 -1.0
     A  B    C
0  0.0  0 -1.0
1 -1.0  1 -1.0
2  2.0  0 -1.0
3 -1.0  1 -1.0
4  4.0  0 -1.0
'''
```
"
654417181,35203,BUG: indexing with -1 doesn't work anymore in expanding function,CloseChoice,closed,2020-07-09T23:55:48Z,2020-07-10T16:24:57Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd
import numpy as np
# setup
df = pd.DataFrame([5,7,4,12,3,4,1], columns=['Value'])
# calculate countif
df['Count'] = df.Value.expanding(1).apply(lambda x: np.sum(np.where(x > x[-1], 1, 0))).astype('int')
```
raises `Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/home/tobias/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/core/window/expanding.py"", line 152, in apply
    return super().apply(func, raw=raw, args=args, kwargs=kwargs)
  File ""/home/tobias/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/core/window/rolling.py"", line 1300, in apply
    return self._apply(
  File ""/home/tobias/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/core/window/rolling.py"", line 507, in _apply
    result = calc(values)
  File ""/home/tobias/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/core/window/rolling.py"", line 495, in calc
    return func(x, start, end, min_periods)
  File ""/home/tobias/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/core/window/rolling.py"", line 1326, in apply_func
    return window_func(values, begin, end, min_periods)
  File ""pandas/_libs/window/aggregations.pyx"", line 1375, in pandas._libs.window.aggregations.roll_generic_fixed
  File ""<stdin>"", line 2, in <lambda>
  File ""/home/tobias/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/core/series.py"", line 871, in __getitem__
    result = self.index.get_value(self, key)
  File ""/home/tobias/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 4405, in get_value
    return self._engine.get_value(s, k, tz=getattr(series.dtype, ""tz"", None))
  File ""pandas/_libs/index.pyx"", line 80, in pandas._libs.index.IndexEngine.get_value
  File ""pandas/_libs/index.pyx"", line 90, in pandas._libs.index.IndexEngine.get_value
  File ""pandas/_libs/index.pyx"", line 138, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 998, in pandas._libs.hashtable.Int64HashTable.get_item
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1005, in pandas._libs.hashtable.Int64HashTable.get_item
KeyError: -1
`

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output

On 0.25.2 this resulted in

```
    Value   Count
0   5        0
1   7        0
2   4        2
3   12       0
4   3        4
5   4        3
6   1        6
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.0.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.10.0-35-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : de_DE.UTF-8
LOCALE           : de_DE.UTF-8

pandas           : 1.0.5
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.2.post20191203
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.10.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.3.3
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None


</details>
"
652385979,35161,BUG: KeyError on MultiIndex when filtering by only two indexes,fabiorangel,closed,2020-07-07T14:47:59Z,2020-07-10T16:48:05Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

#### Code

```python
import pandas as pd
df = pd.DataFrame({'col1':[1,2,1,2],'col2':['a','a','b','b'],'col3':['US','US','US','BR'],'values':[1.0,2.0,3.0,4.5]})
df.set_index(['col1','col2','col3'],inplace = True)
df.loc[([1],['a'])]
```
```python-traceback
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/fabio.rangel/projects/audience_topic_viewership/.direnv/python-3.7.7/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1762, in __getitem__
    return self._getitem_tuple(key)
  File ""/home/fabio.rangel/projects/audience_topic_viewership/.direnv/python-3.7.7/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1272, in _getitem_tuple
    return self._getitem_lowerdim(tup)
  File ""/home/fabio.rangel/projects/audience_topic_viewership/.direnv/python-3.7.7/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1373, in _getitem_lowerdim
    return self._getitem_nested_tuple(tup)
  File ""/home/fabio.rangel/projects/audience_topic_viewership/.direnv/python-3.7.7/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1453, in _getitem_nested_tuple
    obj = getattr(obj, self.name)._getitem_axis(key, axis=axis)
  File ""/home/fabio.rangel/projects/audience_topic_viewership/.direnv/python-3.7.7/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1954, in _getitem_axis
    return self._getitem_iterable(key, axis=axis)
  File ""/home/fabio.rangel/projects/audience_topic_viewership/.direnv/python-3.7.7/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1595, in _getitem_iterable
    keyarr, indexer = self._get_listlike_indexer(key, axis, raise_missing=False)
  File ""/home/fabio.rangel/projects/audience_topic_viewership/.direnv/python-3.7.7/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1553, in _get_listlike_indexer
    keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing
  File ""/home/fabio.rangel/projects/audience_topic_viewership/.direnv/python-3.7.7/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1640, in _validate_read_indexer
    raise KeyError(f""None of [{key}] are in the [{axis_name}]"")
KeyError: ""None of [Index(['a'], dtype='object')] are in the [columns]""
```
#### Problem description

The problem occurs only when filtering using MultiIndex with a two-sized tuple. When using one, three or more it does not occurs. I tested using up to four indexes. 

#### Expected Output
```sh
                values
col1 col2 col3        
1    a    US       1.0
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-174-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : pt_BR.UTF-8
LOCALE           : pt_BR.UTF-8

pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None


</details>
"
448322369,26513,Allow Keyword Aggregation in DataFrame.agg and Series.agg,TomAugspurger,closed,2019-05-24T19:28:27Z,2020-07-10T19:16:28Z,"Followup to https://github.com/pandas-dev/pandas/pull/26399

```python
In [2]: df = pd.DataFrame({""A"": [1, 2, 1, 2], ""B"": [1, 2, 3, 4]})

In [3]: df
Out[3]:
   A  B
0  1  1
1  2  2
2  1  3
3  2  4

In [4]: df.agg(foo=(""B"", ""sum""))
```

Expected Output

```python
In [13]: df.agg({""B"": {""foo"": ""sum""}})
/Users/taugspurger/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/frame.py:6284: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version
  result, how = self._aggregate(func, axis=axis, *args, **kwargs)
Out[13]:
      B
foo  10
```


without the warning. Similar for `Series.agg`

```python
In [16]: df.B.agg({""foo"": ""sum""})  # allow foo=""sum""
Out[16]:
foo    10
Name: B, dtype: int64
```
"
654674672,35210,Tst return none inplace series,r-toroxel,closed,2020-07-10T10:13:39Z,2020-07-10T19:32:38Z,"verify we return None for all inplace calls in /series

related: https://github.com/pandas-dev/pandas/pull/35181#event-3531612625
"
509624719,29116,ENH: Implement Keyword Aggregation for DataFrame.agg and Series.agg,charlesdong1991,closed,2019-10-20T17:27:21Z,2020-07-10T19:32:45Z,"- [x] closes #26513, related to #28380 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This PR basically deals with three issue:
1. implement keyword aggregation for DataFrame.agg
2.  implement keyword aggregation for Series.agg
"
305446844,20359,Strange behaviour of the precision display option,dexter2206,closed,2018-03-15T07:50:52Z,2020-07-10T19:33:14Z,"Consider the following example:

```python
import pandas as pd
s = pd.Series([840.0, 4200.0])
pd.set_option('precision', 0)
print(s)
pd.set_option('precision',2)
print(s)

```
The first print produces 84, 420, while the second one produces correctly 840.0 and 4200.0.

#### Problem description

If I understand docs correctly, the precision option should affect only the number of digits after the decimal point, not the digits before it.

#### Expected Output

First print shows 840 and 4200 (or 840.0, 4200.0).

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.14.8-300.fc27.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: pl_PL.UTF-8
LOCALE: pl_PL.UTF-8

pandas: 0.22.0
pytest: 3.3.1
pip: 9.0.1
setuptools: 36.5.0
Cython: None
numpy: 1.14.2
scipy: 0.19.1
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: None
patsy: 0.4.1
dateutil: 2.7.0
pytz: 2018.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.1.2
openpyxl: 2.4.9
xlrd: 1.0.0
xlwt: None
xlsxwriter: 1.0.2
lxml: None
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
616166277,34124,Fix #23055 Change pd.to_datetime and pd.to_timedelta to always convert null-like argument into pd.NaT,ghost,closed,2020-05-11T20:31:48Z,2020-07-10T19:43:17Z,"I've tried to address the bug in `to_datetime` and `to_timedelta` functions. Please let me know if there're some other places worth to be changed.

- [x] closes #23055
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
652707130,35170,DOC: Add pint pandas ecosystem docs,znicholls,closed,2020-07-07T22:41:49Z,2020-07-10T20:33:25Z,"- [ ] closes #xxxx (N/A)
- [ ] tests added / passed (N/A)
- [ ] passes `black pandas` (N/A)
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff` (N/A)
- [ ] whatsnew entry
"
655058961,35222,Fixing a confused method name in 02_read_write.rst,evank28,closed,2020-07-10T22:01:17Z,2020-07-10T22:22:55Z,"Fixed the confused method name. It should be `read_excel` based on the context, but `to_excel` was provided instead.
Very simple PR for a simple fix to the documentation."
650526766,35109,BUG: Groupby.transform on datetime with nunique giving incorrect result,joelowj,closed,2020-07-03T11:09:44Z,2020-07-10T22:46:42Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
>>> import pandas as pd
>>> bdates = pd.bdate_range('2008-12-31', '2011-12-30')
>>> df = pd.DataFrame(bdates, columns=['date'])
>>> df['year'] = df['date'].dt.year
>>> df.groupby('year')['date'].transform('nunique')
0     1970-01-01 00:00:00.000000001
1     1970-01-01 00:00:00.000000261
2     1970-01-01 00:00:00.000000261
3     1970-01-01 00:00:00.000000261
4     1970-01-01 00:00:00.000000261
                   ...             
778   1970-01-01 00:00:00.000000260
779   1970-01-01 00:00:00.000000260
780   1970-01-01 00:00:00.000000260
781   1970-01-01 00:00:00.000000260
782   1970-01-01 00:00:00.000000260
Name: date, Length: 783, dtype: datetime64[ns]
>>> 

```

#### Problem description

The current behaviour returns the sum of default timestamp with the number of unique count of dates in each year. The expected output should have been the number of unique counts for the date in each year which is consistent with the behavior of other Groupby.transform functions.

#### Expected Output

```python
# Your code here
>>> import pandas as pd
>>> bdates = pd.bdate_range('2008-12-31', '2011-12-30')
>>> df = pd.DataFrame(bdates, columns=['date'])
>>> df['year'] = df['date'].dt.year
>>> df.groupby('year')['date'].transform('nunique')
0     1
1     261
2     261
3     261
4     261
                   ...             
778   260
779   260
780   260
781   260
782   260
Name: date, Length: 783, dtype: int
>>> 

```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.181-140.257.amzn2.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.4
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 47.1.1.post20200604
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
651860903,35152,BUG: transform with nunique should have dtype int64,rhshadrach,closed,2020-07-06T22:40:18Z,2020-07-10T22:47:43Z,"- [x] closes #35109
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

cc @WillAyd 

Removes casting on transformations which go through `_transform_fast`. The result is a reduction that is broadcast to the original index, so casting isn't necessary once special care is taken for categoricals when `observed=False`

If this PR is accepted, I'll close #35130 which is for the same issue."
626922814,34448,BUG: ensure_timedelta64ns overflows,jbrockmendel,closed,2020-05-29T00:50:28Z,2020-07-10T23:36:58Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

"
652597087,35167,CLN: remove unused freq kwarg in libparsing,jbrockmendel,closed,2020-07-07T20:02:15Z,2020-07-10T23:37:58Z,"
"
655048271,35221,ASV: asvs for normalize functions,jbrockmendel,closed,2020-07-10T21:34:33Z,2020-07-10T23:38:37Z,"With this, we have pretty good coverage for everything in tslibs.vectorized"
639972902,34830,"REF: move registry, Registry to dtypes.base",jbrockmendel,closed,2020-06-16T21:07:39Z,2020-07-11T00:27:07Z,"These are more closely related to ExtensionDtype than to our internal EADtypes.  More concretely, they are low-dependency, while dtypes.dtypes has some unfortunate circular dependencies.

I'd also like to move pandas_dtype and is_dtype_equal from dtypes.common so that those can be stricty ""above"" dtypes.dtypes in the dependency structure.  Saving that for another pass."
654999290,35220,TYPING/DOC: Move custom type to _typing and add whatsnew,charlesdong1991,closed,2020-07-10T19:46:06Z,2020-07-11T02:10:28Z,"followup of #29116 

details see : https://github.com/pandas-dev/pandas/pull/29116#discussion_r453033204"
654890795,35216,TST add corner cases in test_constructors,r-toroxel,closed,2020-07-10T16:18:31Z,2020-07-11T10:45:09Z,"add two more corner cases to frame/test_constructors.
"
655112418,35225,TST: added test for groupby/apply timezone-aware with copy,sanders41,closed,2020-07-11T01:57:18Z,2020-07-11T11:11:31Z,"- [x] closes #27212
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

added test for groupby/apply timezone-aware with copy
"
642385496,34903,Replace old string formatting syntax with f-strings #29547,ynorouzz,closed,2020-06-20T14:20:47Z,2020-07-11T14:33:36Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
651146570,35130,BUG: transform with nunique should have dtype int64,rhshadrach,closed,2020-07-05T21:11:37Z,2020-07-11T16:00:46Z,"- [x] closes #35109
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

"
643430145,34942,BUG: reset_index doesn't preserve dtype on empty frame with MultiIndex,rhshadrach,closed,2020-06-22T23:53:56Z,2020-07-11T16:01:50Z,"- [x] closes #19602
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
624502748,34372,CLN: Unify signatures in _libs.groupby,rhshadrach,closed,2020-05-25T22:11:25Z,2020-07-11T16:01:52Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref https://github.com/pandas-dev/pandas/pull/33630#discussion_r428737327

The main goal is to allow _get_cythonized_result to be used with the cython implementation of var. In order to do this, it was best to modify the signatures in _libs.groupby of 

 - group_any_all;
 - _group_var; and
 - group_quantile

so that the order of the arguments matches other cython functions (e.g. those used in ops.BaseGrouper.aggregate or transform).

Note this PR has the fix for #33955 that is also in #33988 - that PR should be merged first.

Tests using %timeit on dataframes of various shapes (code for generating is at the bottom):

![image](https://user-images.githubusercontent.com/45562402/83825639-e3cf8a00-a6a7-11ea-84c7-c224870cae8b.png)

I looked into the last row. 26.7% of the time is spent in just iterating over the columns, 12.8% in the cython call, and 35.7% in the call to ``_wrap_aggregated_output``.

While working on this, it seemed best to implement ``ddof != 1`` in cython for var. The following code takes 26.5ms on master and 3.88ms in this PR.

````
ncol, order = 1, 6
df = pd.DataFrame(np.random.uniform(low=0.0, high=10.0, size=(10**order, ncol)))
df['key'] = np.random.randint(0, 50, (10**order, 1))
g = df.groupby('key')
t = %timeit -o g.std(ddof=4)
````"
638347761,34767,BUG: Groupby with as_index=False raises error when type is Category,rhshadrach,closed,2020-06-14T12:58:48Z,2020-07-11T16:02:03Z,"- [x] closes #32599
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This regression was a symptom of `DataFrameGroupBy.size` ignoring `as_index=False`. Fixing this now makes the result either a frame or series, so where the internals use `size()` they now use `grouper.size()` which always returns a series.

I also implemented @dsaxton's suggestion to make the column named ""size"" when `as_index=False` (https://github.com/pandas-dev/pandas/issues/5755#issuecomment-522321259)."
598490464,33493,BUG: groupby.hist legend should use group keys,rhshadrach,closed,2020-04-12T13:55:43Z,2020-07-11T16:02:04Z,"- [x] closes #6279
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

cc @TomAugspurger 

Added argument ""legend"" to histogram backend. This adds the ability to display a legend even when not using a groupby. 

````
df = pd.DataFrame(np.random.randn(30, 2), columns=['A', 'B'])
df['C'] = 15 * ['a'] + 15 * ['b']
df = df.set_index('C')
df.groupby('C')['A'].hist(legend=True)
````

produces

![image](https://user-images.githubusercontent.com/45562402/79070334-25911500-7ca3-11ea-8303-d44ded966137.png)

I went off the tests that already existed in test_hist_method and test_groupby, but perhaps more stringent tests should be added. I've been manually checking with the following code:

````
import matplotlib.pyplot as plt
import pandas as pd

df = pd.DataFrame(np.random.randn(30, 2), columns=['A', 'B'])
df['C'] = 15 * ['a'] + 15 * ['b']
df = df.set_index('C')

df.groupby('C')['A'].hist(legend=False)
plt.show()
df.groupby('C')['A'].hist(legend=True)
plt.show()
df.A.hist(by='C', legend=False)
plt.show()
df.A.hist(by='C', legend=True)
plt.show()
plt.show()
df.hist(by='C', legend=False)
plt.show()
df.hist(by='C', legend=True)
plt.show()
df.hist(by='C', column='B', legend=False)
plt.show()
df.hist(by='C', column='B', legend=True)
plt.show()

df.groupby('C')['A'].hist(label='D', legend=False)
plt.show()
df.groupby('C')['A'].hist(label='D', legend=True)
plt.show()
df.A.hist(by='C', label='D', legend=False)
plt.show()
df.A.hist(by='C', label='D', legend=True)
plt.show()
df.hist(by='C', label='D', legend=False)
plt.show()
df.hist(by='C', label='D', legend=True)
plt.show()
df.hist(by='C', column='B', label='D', legend=False)
plt.show()
df.hist(by='C', column='B', label='D', legend=True)
plt.show()
````"
612901860,34012,BUG: groupby with as_index=False shouldn't modify grouping columns,rhshadrach,closed,2020-05-05T21:19:29Z,2020-07-11T16:02:05Z,"- [x] closes #32579
- [x] closes #21090
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This makes any groupby function that uses _GroupBy._make_wrapper to act on self._obj_with_exclusions rather than self._selected_obj, in parallel with the cython paths. Also similar to the cython paths, the grouping columns are added back onto the result in _wrap_applied_output. Similar remarks apply to nunique, which does not go through the _make_wrapper.

After finishing this, I found PR #29131. This PR does not change the behavior of calling apply() itself, only the previous code paths that would internally go through apply. Also, the change made there does not have any impact when as_index is False."
577369512,32520,BUG: Multiindexed series .at fix,rhshadrach,closed,2020-03-07T17:00:57Z,2020-07-11T16:02:07Z,"- [x] closes #26989
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
602483790,33630,BUG: DataFrameGroupby std/sem modify grouped column when as_index=False,rhshadrach,closed,2020-04-18T15:18:20Z,2020-07-11T16:02:09Z,"- [x] closes #10355
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

When working on this, I noticed an unrelated line of code that could be moved inside an if-block and made the change. Should unrelated cleanups like this be left to a separate PR? Can revert if that's the case."
595463809,33343,CLN: Replace first_not_none function with default argument to next,rhshadrach,closed,2020-04-06T22:19:09Z,2020-07-11T16:02:13Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
593942177,33286,CLN: Add/refine type hints to some functions in core.dtypes.cast,rhshadrach,closed,2020-04-04T16:18:53Z,2020-07-11T16:02:14Z,"
- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Some cleanup resulting from PR #32894. In order to get SeriesGroupBy._transform_fast to realize self.obj is a Series, it seemed I needed to add a generic type to GroupBy and _GroupBy. I don't know if there is a better way to do this. Here, one cannot use FrameOrSeries as a Series may return a DataFrame and vice-versa, so I added _GroupByT."
585563091,32894,[BUG] Sum of grouped bool has inconsistent dtype,rhshadrach,closed,2020-03-21T19:22:49Z,2020-07-11T16:02:15Z,"- [x] closes #7001
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Would appreciate any feedback on this attempt.

The strategy is to modify the dtype after the aggregation is computed in certain cases when casting. In order for this to work, the cast functions need to be made aware of how the data was aggregated. I've added an optional ""how"" argument to maybe_downcast_numeric and _try_cast. Because this dtype change is needed in two places, I've added the function groupby_result_dtype to dtypes/common.py to handle the logic.

I wasn't sure where the mapping information needed by groupby_result_dtype should be stored. Currently it is in the function itself, but maybe there is a better place for it.

If this is a good approach, it could potentially be expanded for other aggregations and datatypes. One thought is that perhaps groupby(-).mean() should always return a float for numeric types."
642939470,34931,BUG: fix IntegerArray astype with copy=True/False,jorisvandenbossche,closed,2020-06-22T10:11:47Z,2020-07-11T17:12:43Z,"xref https://github.com/pandas-dev/pandas/pull/34307#discussion_r443370993

Right now, we were not copying the mask correctly with `copy=True`, and also not when the data was actually copied (then the mask also needs to be copied, even with `copy=False`)"
617850956,34169,"TST: Added test for json containing the key ""last_status_change_at""",rivera-fernando,closed,2020-05-14T01:08:47Z,2020-07-12T09:17:36Z,"
- [x] closes #32409
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565890778,32040,BUG: GroupBy aggregation of DataFrame with MultiIndex columns breaks with custom function,MarcoGorelli,closed,2020-02-16T11:32:31Z,2020-07-12T09:24:28Z,"- [x] closes #31777 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
443410956,26367,Pandas import breaks when setting filterwarnings,jorisvandenbossche,closed,2019-05-13T13:52:12Z,2020-07-12T11:45:35Z,"With released pandas (tested with 0.23.4), you get the following:

```
>>> import warnings
>>> warnings.filterwarnings('error')
>>> import pandas
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/joris/miniconda3/lib/python3.7/site-packages/pandas/__init__.py"", line 26, in <module>
    from pandas._libs import (hashtable as _hashtable,
  File ""/home/joris/miniconda3/lib/python3.7/site-packages/pandas/_libs/__init__.py"", line 4, in <module>
    from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime
  File ""pandas/_libs/tslibs/conversion.pxd"", line 11, in init pandas._libs.tslib
  File ""pandas/_libs/tslibs/conversion.pyx"", line 1, in init pandas._libs.tslibs.conversion
  File ""pandas/_libs/tslibs/nattype.pyx"", line 14, in init pandas._libs.tslibs.nattype
ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__
```

(you also get a long output with `warnings.filterwarnings('always')`).

This is not happening any more on master, so the reason seems to be fixed. 
But I was wondering, do we want to guarantee that this does not happen? (to the extent possible of course, meaning that we try to test this)"
655233110,35232,Tst verify return none in tests/frame,r-toroxel,closed,2020-07-11T15:57:46Z,2020-07-12T12:05:15Z,"verify we return None for all inplace calls in tests/frame
related: https://github.com/pandas-dev/pandas/pull/35230"
655277506,35239,TST: GH#26367 Verify whether Pandas import succeeds when setting filt…,avinashpancham,closed,2020-07-11T20:52:38Z,2020-07-12T16:40:57Z,"…erwarnings

- [x] closes #26367
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
655305880,35243,Update asv default to 3.8,jbrockmendel,closed,2020-07-12T00:43:46Z,2020-07-12T19:54:30Z,
366926757,22994,Incorrect behavior when concatenating multiple ExtensionBlocks with different dtypes,TomAugspurger,closed,2018-10-04T19:01:19Z,2020-07-13T12:50:31Z,"In https://github.com/pandas-dev/pandas/blob/d430195ffaa37df2d8fc3ecf26306bb5e06b4ad0/pandas/core/internals/managers.py#L1638

we check that we have one type of block.

For ExtensionBlocks, that's insufficient. If you try to concatenate two series with different EA dtypes, it'll calling the first EA's `_concat_same_type` with incorrect types.

```python
In [13]: from pandas.tests.extension.decimal.test_decimal import *

In [14]: import pandas as pd

In [15]: a = pd.Series(pd.core.arrays.integer_array([1, 2]))

In [16]: b = pd.Series(DecimalArray([decimal.Decimal(1), decimal.Decimal(2)]))

In [17]: pd.concat([a, b])
```

```pytb
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-17-714da278d09e> in <module>
----> 1 pd.concat([a, b])

~/sandbox/pandas/pandas/core/reshape/concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    225                        verify_integrity=verify_integrity,
    226                        copy=copy, sort=sort)
--> 227     return op.get_result()
    228
    229

~/sandbox/pandas/pandas/core/reshape/concat.py in get_result(self)
    389
    390                 mgr = self.objs[0]._data.concat([x._data for x in self.objs],
--> 391                                                 self.new_axes)
    392                 cons = _concat._get_series_result_type(mgr, self.objs)
    393                 return cons(mgr, name=name).__finalize__(self, method='concat')

~/sandbox/pandas/pandas/core/internals/managers.py in concat(self, to_concat, new_axis)
   1637
   1638             if all(type(b) is type(blocks[0]) for b in blocks[1:]):  # noqa
-> 1639                 new_block = blocks[0].concat_same_type(blocks)
   1640             else:
   1641                 values = [x.values for x in blocks]

~/sandbox/pandas/pandas/core/internals/blocks.py in concat_same_type(self, to_concat, placement)
   2047         """"""
   2048         values = self._holder._concat_same_type(
-> 2049             [blk.values for blk in to_concat])
   2050         placement = placement or slice(0, len(values), 1)
   2051         return self.make_block_same_class(values, ndim=self.ndim,

~/sandbox/pandas/pandas/core/arrays/integer.py in _concat_same_type(cls, to_concat)
    386     def _concat_same_type(cls, to_concat):
    387         data = np.concatenate([x._data for x in to_concat])
--> 388         mask = np.concatenate([x._mask for x in to_concat])
    389         return cls(data, mask)
    390

~/sandbox/pandas/pandas/core/arrays/integer.py in <listcomp>(.0)
    386     def _concat_same_type(cls, to_concat):
    387         data = np.concatenate([x._data for x in to_concat])
--> 388         mask = np.concatenate([x._mask for x in to_concat])
    389         return cls(data, mask)
    390

AttributeError: 'DecimalArray' object has no attribute '_mask'
```

For EA blocks, we need to ensure that they're the same dtype. When they differ, we should fall back to object.

Checking the dtypes actually solves a secondary problem. On master, we allow `concat([ Series[Period[D]], Series[Period[M]] ])`, i.e. concatenating series of periods with different frequencies. If we want to allow that still, we need to bail out before we get down to `PeriodArray._concat_same_type`."
408506195,25257,"concat(..., copy=False) with datetime tz-aware data raises ValueError: cannot create a DatetimeTZBlock without a tz",karldw,closed,2019-02-10T06:29:24Z,2020-07-13T12:55:12Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
df = pd.DataFrame({
    'timestamp': [pd.Timestamp('2017-08-27 01:00:00.709949+0000', tz='UTC')],
})

pd.concat([df])      # works
pd.concat([df, df])  # works
pd.concat([df], copy=False)  # fails
# Edit: added full traceback
# Traceback (most recent call last):
#   File ""<stdin>"", line 1, in <module>
#   File "".../pandas/core/reshape/concat.py"", line 229, in concat
#     return op.get_result()
#   File "".../pandas/core/reshape/concat.py"", line 426, in get_result
#     copy=self.copy)
#   File "".../pandas/core/internals/managers.py"", line 2055, in concatenate_block_managers
#     b = b.make_block_same_class(values, placement=placement)
#   File "".../pandas/core/internals/blocks.py"", line 235, in make_block_same_class
#     klass=self.__class__, dtype=dtype)
#   File "".../pandas/core/internals/blocks.py"", line 3095, in make_block
#     return klass(values, ndim=ndim, placement=placement)
#   File "".../pandas/core/internals/blocks.py"", line 1680, in __init__
#     values = self._maybe_coerce_values(values)
#   File "".../pandas/core/internals/blocks.py"", line 2266, in _maybe_coerce_values
#     raise ValueError(""cannot create a DatetimeTZBlock without a tz"")
# ValueError: cannot create a DatetimeTZBlock without a tz

```
#### Problem description
Running `pd.concat` with a single dataframe and `copy=False` fails, giving an error about  `cannot create a DatetimeTZBlock without a tz`.

Closely related:
- https://github.com/pandas-dev/pandas/issues/19420
- https://github.com/pandas-dev/pandas/issues?q=is%3Aopen+is%3Aissue+label%3AReshaping+label%3ATimezones

#### Expected Output

The original dataframe, since I'm only `concat`-ing a single dataframe.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.8.final.0
python-bits: 64
OS: Linux
OS-release: 4.18.0-14-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.1
pytest: 4.2.0
pip: 19.0.1
setuptools: 40.6.3
Cython: 0.29.4
numpy: 1.15.4
scipy: 1.2.0
pyarrow: 0.11.1
xarray: None
IPython: 7.2.0
sphinx: None
patsy: None
dateutil: 2.7.5
pytz: 2018.7
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.2
openpyxl: 2.4.10
xlrd: 1.2.0
xlwt: None
xlsxwriter: 1.1.2
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: 1.2.17
pymysql: None
psycopg2: 2.7.6.1 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: 0.2.1
pandas_gbq: None
pandas_datareader: None
gcsfs: None


</details>
"
597947610,33458,"TST: concat(..., copy=False) with datetime tz-aware data raises ValueError: cannot create a DatetimeTZBlock without a tz",SurajH1,closed,2020-04-10T15:30:26Z,2020-07-13T12:55:29Z,"…n version 0.24.1 but passes on current version

- [x] closes #25257
- [x] tests added / passed
"
642163793,34875,DF.__setitem__ creates extension column when given extension scalar,justinessert,closed,2020-06-19T18:54:51Z,2020-07-13T13:01:58Z,"This PR is in response to [Issue 34832]( https://github.com/pandas-dev/pandas/issues/34832).
These changes follow the suggestions from reviewers on the PR to fix an issue where `df['a']=pd.Period('2020-01')` would create an object column instead of a `period[M]` column.

One potential issue that I see with these changes is that this code requires the `shape` parameter passed into `cast_scalar_to_array` to be an int, even though the functions docstring claims that it accepts a tuple.

I'm happy to work on resolving that issue, but first wanted to clarify that it is necessary. Based on my perspective, there would be no reason to call this function to create a multi-dimensional array, but I surely don't understand the full interconnectivity of this package.

- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
571226261,32266,Pandas 1.0.1 - .rolling().min() and .rolling().max() create memory leak at <__array_function__ internals>:6,regmeg,closed,2020-02-26T10:24:20Z,2020-07-13T13:08:58Z,"#### Code Sample, a copy-pastable example if possible

```python
import tracemalloc, linecache
import sys, os
import pandas as pd

def display_top_mem(snapshot, key_type='lineno', limit=10):
    """"""function for displaying lines of code taking most memory""""""
    snapshot = snapshot.filter_traces((
        tracemalloc.Filter(False, ""<frozen importlib._bootstrap>""),
        tracemalloc.Filter(False, ""<unknown>""),
    ))
    top_stats = snapshot.statistics(key_type)

    print(""Top %s lines"" % limit)
    for index, stat in enumerate(top_stats[:limit], 1):
        frame = stat.traceback[0]
        # replace ""/path/to/module/file.py"" with ""module/file.py""
        filename = os.sep.join(frame.filename.split(os.sep)[-2:])
        print(""#%s: %s:%s: %.1f KiB""
              % (index, filename, frame.lineno, stat.size / 1024))
        line = linecache.getline(frame.filename, frame.lineno).strip()
        if line:
            print('    %s' % line)

    other = top_stats[limit:]
    if other:
        size = sum(stat.size for stat in other)
        print(""%s other: %.1f KiB"" % (len(other), size / 1024))
    total = sum(stat.size for stat in top_stats)
    print(""Total allocated size: %.1f KiB"" % (total / 1024))


def main():
    tracemalloc.start()
    periods = 745
    df_init = pd.read_csv('./mem_debug_data.csv', index_col=0)

    for i in range(100):
        df = df_init.copy()

        df['l:c:B'] = df['c:B'].rolling(periods).min()
        df['h:c:B'] = df['c:B'].rolling(periods).max()

        #df['l:c:B'] = df['c:B'].rolling(periods).mean()
        #df['h:c:B'] = df['c:B'].rolling(periods).median()

        snapshot = tracemalloc.take_snapshot()
        display_top_mem(snapshot, limit=3)
        print(f'df size {sys.getsizeof(df)/1024} KiB')
        print(f'{i} ##################')


if __name__ == '__main__':
    main()
```
#### Problem description
Pandas `rolling().min()` and `rolling().max()` functions create memory leaks. I've run a tracemalloc line based memory profiling and  `<__array_function__ internals>:6` seems to always grow in size for every loop iteration in the script above with both of these functions present. For 1000 itereations it will consume around 650MB or RAM, whereas for example if `rolling().min()` and `rolling().max()` is changed to `rolling().mean()`and `rolling().median()` an run for 1000 iterations, RAM consumption will stay constant at around 4MB. Therefore `rolling().min()` and `rolling().max()`  seem to be the problem.

The output of this script running for 100 iterations with `<__array_function__ internals>:6` constantly increasing in size can be found here: https://pastebin.com/nvGKgmPq

CSV file `mem_debug_data.csv` used in the script can be found here: http://www.sharecsv.com/s/ad8485d8a0a24a5e12c62957de9b13bd/mem_debug_data.csv

#### Expected Output
Running `rolling().min()` and `rolling().max()`  constantly over time should not grow RAM consumption.

#### Output of ``pd.show_versions()``

<details>

> INSTALLED VERSIONS
> ------------------
> commit           : None
> python           : 3.7.6.final.0
> python-bits      : 64
> OS               : Linux
> OS-release       : 4.15.0-88-generic
> machine          : x86_64
> processor        : x86_64
> byteorder        : little
> LC_ALL           : None
> LANG             : en_GB.UTF-8
> LOCALE           : en_GB.UTF-8
> 
> pandas           : 1.0.1
> numpy            : 1.18.1
> pytz             : 2019.3
> dateutil         : 2.8.1
> pip              : 19.2.3
> setuptools       : 41.2.0
> Cython           : None
> pytest           : None
> hypothesis       : None
> sphinx           : None
> blosc            : None
> feather          : None
> xlsxwriter       : None
> lxml.etree       : None
> html5lib         : None
> pymysql          : None
> psycopg2         : None
> jinja2           : 2.11.1
> IPython          : 7.12.0
> pandas_datareader: None
> bs4              : None
> bottleneck       : None
> fastparquet      : None
> gcsfs            : None
> lxml.etree       : None
> matplotlib       : 3.1.3
> numexpr          : None
> odfpy            : None
> openpyxl         : None
> pandas_gbq       : None
> pyarrow          : None
> pytables         : None
> pytest           : None
> pyxlsb           : None
> s3fs             : None
> scipy            : 1.4.1
> sqlalchemy       : None
> tables           : None
> tabulate         : None
> xarray           : None
> xlrd             : None
> xlwt             : None
> xlsxwriter       : None
> numba            : None

</details>
"
655856100,35262,Failures with pytest 6.0.0rc1,TomAugspurger,closed,2020-07-13T13:33:12Z,2020-07-13T13:39:24Z,"https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=39140&view=logs&j=a3a13ea8-7cf0-5bdb-71bb-6ac8830ae35c&t=add65f64-6c25-5783-8fd6-d9aa1b63d9d4


```
=================================== FAILURES ===================================
____ TestStringMethods.test_api_per_method[index-empty0-get_dummies-object] ____
[gw0] linux -- Python 3.7.6 /home/vsts/miniconda3/envs/pandas-dev/bin/python

        copy=False,
        name=None,
        verify_integrity: bool = True,
        _set_identity: bool = True,
    ):
    
        # compat with Index
        if name is not None:
            names = name
        if levels is None or codes is None:
            raise TypeError(""Must pass both levels and codes"")
        if len(levels) != len(codes):
            raise ValueError(""Length of levels and codes must be the same."")
        if len(levels) == 0:
>           raise ValueError(""Must pass non-zero number of levels/codes"")
E           ValueError: Must pass non-zero number of levels/codes

pandas/core/indexes/multi.py:271: ValueError
```

In the failing cases we dynamically add a an xfail marker: https://github.com/pandas-dev/pandas/blob/f477a0e8146e3f249d2a5c08031020c15c1ecd95/pandas/tests/test_strings.py#L266-L268. Reported at https://github.com/pytest-dev/pytest/issues/7486.

For now we can pin pytest."
655299818,35242,CLN: annotate,jbrockmendel,closed,2020-07-11T23:51:52Z,2020-07-13T14:43:33Z,
313941053,20676,pd.Series.equals() fails for Series containing iterable objects,bfollinprm,closed,2018-04-13T01:38:28Z,2020-07-13T16:36:11Z,"#### Code Sample, a copy-pastable example if possible

```python
pd.Series(
    [np.array([1, 2]), np.array([1, 2])]
).equals(pd.Series(
    [np.array([1, 2]), np.array([1, 2])]
))

# Evaluates to False
```
#### Problem description

For objects that overload equality with elementwise equality (this includes `np.array` and `pd.Series` objects), `pd.Series.equals()` catches

```python
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
```

and returns `False`, behavior inherited from `np.equal` applied to object arrays. While you can argue this is a numpy bug, in numpy equality is purposefully strict for `dtype=='object'`. In pandas, it is a common enough procedure to tabulate arrays and lists under a single column (which may contain a named (time)series, image array, or an unpacked NLP document), and this behavior of equality is unintuitive.

#### Expected Output
`True`
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.13.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-1048-aws
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.22.0
pytest: 3.2.2
pip: 9.0.1
setuptools: 27.2.0
Cython: None
numpy: 1.13.3
scipy: 0.19.1
pyarrow: None
xarray: None
IPython: 5.3.0
sphinx: 1.5.4
patsy: None
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: 4.6.0
html5lib: 0.9999999
sqlalchemy: 1.1.9
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
655249076,35237,TST: GH20676 Verify equals operator for list of Numpy arrays,avinashpancham,closed,2020-07-11T17:38:42Z,2020-07-13T16:36:15Z,"- [x] closes #20676
- [ x ] tests added / passed
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ x ] whatsnew entry
"
655952894,35263,nan + nan = 0 when  skipna=True or when groupby().sum(),raholler,closed,2020-07-13T15:43:19Z,2020-07-13T19:33:54Z,"**Disclaimer**: Not sure whether this is expected behavior. I would have not expected it though and it is always bad in my use cases (survey data).

Example:
```python
data = pd.DataFrame(
    data = {
        ""var1"": [np.nan, 1000, 2000],
        ""var2"": [np.nan, 100, np.nan],
        ""hh_id"" : [1, 2, 2] 
    }
)
```

**Normal sum**
```
data[[""var1"", ""var2""]].sum(axis=1)
```

Expected output:
```
0      np.nan
1    1100.0
2    2000.0
dtype: float64
```

Output:

```
0       0.0
1    1100.0
2    2000.0
dtype: float64
```

**Groupby sum**
```python
data.groupby(""hh_id"")[""var2""].sum()
```

Expected output:
```

hh_id
1      np.nan
2    100.0
Name: var2, dtype: float64


```

Output
```

hh_id
1      0.0
2    100.0
Name: var2, dtype: float64


```
"
662241105,35358,CI: pin matplotlib for doc build,TomAugspurger,closed,2020-07-20T20:39:45Z,2020-07-21T11:16:24Z,Should fix the doc build. We have https://github.com/pandas-dev/pandas/issues/34850 to fix this.
661887193,35351,BUG: multiindex slice 1.1.0rc0,leo4183,closed,2020-07-20T14:26:22Z,2020-07-21T11:20:01Z,"mulitindex slicing returns different (from pandas<=1.0.x) results in pandas 1.1.0rc0

```
df = pd.DataFrame( data=[[0,1,'a',1],[1,0,'a',0],[1,1,'b',1],[1,1,'c',2]], columns=['index','date','author','price'] )

# to keep the most recent date for every index (could happen in practice)
idx = df.drop_duplicates('index',keep='last').set_index(['index','date']).index 

df.set_index(['index','date']).loc[idx,:].reset_index()
```

df

 index | date  | author | price
-- | -- | -- | --
0|1|a | 1
1|0|a | 0
1|1|b | 1
1|1|c | 2






Result

  | | author | price
 -- | -- | --
(0, 1) | a | 1
(1, 1) | b | 1
(1, 1) | c | 2
"
662009125,35353,REGR: MultiIndex Indexing,simonjayhawkins,closed,2020-07-20T16:22:31Z,2020-07-21T11:30:10Z,"- [ ] closes #35351
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
467038311,27343,DataFrameGroupby.resample with the `on` keyword does not produce the same output as on DateTimeIndex,philippegr,open,2019-07-11T18:18:58Z,2020-07-21T12:53:46Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame.from_records({'ref':['a','a','a','b','b'], 
                           'time':[dt.datetime(2014,12,31), dt.datetime(2015,12,31), dt.datetime(2016,12,31), 
                                   dt.datetime(2012,12,31), dt.datetime(2014,12,31)],
                          'value':5*[1]})

# These frames differs
pd.DataFrame.equals(df.groupby(""ref"").resample(rule='M', on='time')['value'].sum(), 
                    df.set_index('time').groupby(""ref"").resample(rule='M')['value'].sum())

# Similarly to .set_index() .apply produces the correct output 
pd.DataFrame.equals(df.groupby(""ref"").apply(lambda f :f.resample(rule='M', on='time')['value'].sum()), 
                    df.set_index('time').groupby(""ref"").resample(rule='M')['value'].sum())

#This is the incorrect frame
df.groupby(""ref"").resample(rule='5T', on='time')['value'].sum().to_frame()
```
#### Problem description
DataFrameGroupby.resample with the `on` keyword (last line) produces an incorrect output: it differs from the output produce with a DateTimeIndex and has incorrect values. It seems that it does not handle the group properly:
  - the result has an entry for `('a', 2012-12-31 00:00:00):1` that is incorrect `ref =='a'` only starts in 2014 so not only should there be no entry but value of 1 is incorrect
 - the result has entries for 'b' starting in 2015 when 
  - `ref == 'b'` has a min time entry in 2015 so there should not be any entry before that regardless of its value 

#### Expected Output
`True` for `pd.DataFrame.equals(df.groupby(""ref"").resample(rule='M', on='time')['value'].sum(), 
                    df.set_index('time').groupby(""ref"").resample(rule='M')['value'].sum())` 
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Darwin
OS-release: 18.6.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_CA.UTF-8
LOCALE: en_CA.UTF-8

pandas: 0.24.2
pytest: None
pip: 19.1.1
setuptools: 41.0.1
Cython: None
numpy: 1.16.4
scipy: None
pyarrow: None
xarray: None
IPython: 7.6.1
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
602708019,33653,DOC: Fix heading capitalization in doc/source/whatsnew - part6 (#32550),cleconte987,closed,2020-04-19T12:54:28Z,2020-07-21T20:15:17Z,"- [ ] Modify files v0.22.0.rst, v0.23.1.rst, v0.19.0.rst, v0.24.0.rst, v0.24.2.rst

-[ ] Add exceptions in 'validate_rst_title_capitalization.py'"
659902541,35333,Fix MPL dt conversion (epoch2num deprecation) #34850,misantroop,closed,2020-07-18T04:52:53Z,2020-07-22T02:30:02Z,"Fix datetime conversion after matplotlib (3.3.0 update) deprecating epoch2num.

- [ ] closes #34850
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
610718900,33914,ENH: print DataFrame columns names at the end,JDkuba,closed,2020-05-01T12:04:59Z,2020-07-22T09:20:46Z,"- [ ] closes #32296
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I've added the possibility of printing columns names again at the end of DataFrame. I'm not sure whether the number of rows should be checked again (max number of rows could be exceeded with that extra row).
"
652505386,35163,CI: update to isort 5 (#35134),fangchenli,closed,2020-07-07T17:46:29Z,2020-07-22T14:52:48Z,"- [x] closes #35134
- [x] 0 tests added / 0 passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
663490595,35374,Change defaults for rolling/expanding.apply engine kwargs to None,mroeschke,closed,2020-07-22T05:42:56Z,2020-07-22T16:34:10Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This change was supposed to be apart of https://github.com/pandas-dev/pandas/pull/35182 but was overlooked. Since 1.1 isn't out, this doesn't need to be backported correct?
"
653485480,35183,Json Visual Clutter cleanup,WillAyd,closed,2020-07-08T17:50:58Z,2020-07-22T18:56:47Z,"Removed PRINTMARK symbols and ran `clang-format -sort-includes=0 -i -style=""{IndentWidth: 4}"" pandas/_libs/src/ujson/python/objToJSON.c`"
663378609,35371,BUG: to_parquet doesn't reliably support file:// URI,impredicative,closed,2020-07-21T23:55:12Z,2020-07-23T10:55:31Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
pd.__version__
'1.0.5'
df = pd.DataFrame()  # Any dataframe is fine for this issue.

df.to_parquet('file:///tmp/df.parquet')  # DOESN'T WORK! Raises urllib.error.URLError: <urlopen error [Errno 2] No such file or directory: '/tmp/df.parquet'>
df.to_parquet('/tmp/df.parquet')  # Works fine.
df.to_parquet('file:///tmp/df.parquet')  # Works given that the file now exists.
```

#### Problem description

It is important to correctly support file URIs so users can interchangeably write to S3 or local files, both of which I represent as a URI. With Pandas 1.0.5, `to_parquet` works with a file URI only if the file already exists.

#### Expected Output

`to_parquet` should work with a file URI independent of whether the file already exists.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.3.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_US.UTF-8
pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1.post20200622
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fastparquet      : 0.4.1
gcsfs            : None
lxml.etree       : 4.5.1
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : 0.4.2
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.50.1

</details>
"
664449098,35391,BUG: Recursion error when querying empty df,klieret,closed,2020-07-23T12:52:50Z,2020-07-23T13:38:34Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
df = pd.DataFrame(columns=[""a"", ""b""])
df.query(""(a+b)==1"")
```

**NOTE**: Under some circumstances the same might happen when the colums only contain ``np.nan``s but I couldn't get a MWE yet.

#### Problem description

Throws recursion error.

#### Expected Output

Empty df.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-62-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.18.1
pytz             : 2019.2
dateutil         : 2.7.3
pip              : 18.1
setuptools       : 41.1.0
Cython           : None
pytest           : 5.3.2
hypothesis       : None
sphinx           : 3.0.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 5.8.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.16
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
368550615,23072,Fixing memory leaks in read_csv,zhezherun,closed,2018-10-10T08:33:17Z,2020-07-23T13:45:25Z,"This PR fixes a memory leak in parsers.pyx detected by valgrind, and also adds some further cleanup that should avoid memory leaks on exceptions,

closes #21353

- Moved the allocation of na_hashset further down, closer to where it is used. Otherwise it will not be freed if `continue` is executed,
- Delete `na_hashset` if there is an exception,
- Also clean up the allocation inside `kset_from_list` before raising an exception."
662318921,35361,FIX: remove dependence on matplotlib.dates.epoch2num,jklymak,closed,2020-07-20T22:14:23Z,2020-07-23T15:54:32Z,"- [ ] closes #34850
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

`converter._dt_to_float_ordinal(dt)` special cased `datetime64` objects, and used `matplotlib.dates.epoch2num`.  However, matplotlib has supported `datetime64` natively since Matplotlib 2.2 (https://github.com/matplotlib/matplotlib/pull/9779), so this special casing shouldn't be necessary (unless there is a subtlety I'm missing).  Matplotlib would also prefer to deprecate `epoch2num`, so this would help us streamline.  

I didn't check your support policy - if you need to support pre MPL 2.2, then I guess some try/except logic will need to go in here.  

ping @TomAugspurger @dstansby "
660191467,35335,BUG: Inconsistent resampling behaviour,LazyTrader,closed,2020-07-18T14:48:37Z,2020-07-23T16:36:07Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

Attached below is my code for extracting processing financial tick data from the MetaTrader5 application:

```python
def CollectTicks(Year, Month, Day, Hour, Minute, CurrencyPair, Digits, ResampleOffset = None):

    StartTime = datetime.datetime(Year, Month, Day, Hour, Minute).timestamp() #Convert the start time to epoch time

    EndTime = StartTime + 604800 #Add 1 week to the start time and set the result as the end time

    Ticks = mt5.copy_ticks_range(CurrencyPair, StartTime, EndTime, mt5.COPY_TICKS_ALL) #Retrieve the ticks from the start time until the end time

    BreakFlag = False #Set the break flag to false

    while(True):

        StartTime = EndTime #Set the prevous end time as the new start time

        EndTime = StartTime + 604800 #Add 1 week to the new start time and set the result as the new end time

        if(EndTime > time.time()): #If the end time is more than the current time

            EndTime = time.time() #Set the end time as the current time

            BreakFlag = True #Set the break flag to true

        NewTicks = mt5.copy_ticks_range(CurrencyPair, StartTime, EndTime, mt5.COPY_TICKS_ALL) #Retrive the new ticks with the new start time and new end time

        Ticks = np.hstack((Ticks, NewTicks)) #Cocatenate the newly retrieved ticks with the previously retrieved ticks

        if(BreakFlag == True): #If the break flag is set to true

            break #Exit the loop

    TickDataframe = pd.DataFrame(Ticks) #Covert the ticks array to a dataframe

    TickDataframe.drop([""time"", ""last"", ""volume"", ""flags"", ""volume_real""], axis = 1, inplace = True) #Drop the unwanted columns from the dataframe

    TickDataframe[""time_msc""] = pd.to_datetime(TickDataframe[""time_msc""], unit = ""ms"") #Convert the milisecond timestap to the epoch date time

    TickDataframe.set_index(""time_msc"", inplace = True) #Set the milisecond time column as the dataframe's index

    TickDataframe.round(Digits) #Round all values in the dataframe to the desired number of digits

    TickDataframe[""mid""] = round(((TickDataframe[""bid""] + TickDataframe[""ask""]) / 2.0), Digits) #Generate the mid dataframe and round it to (Digits) figures

    if(ResampleOffset != None): #If the ResampleOffset input is not None

        CandleCloses = {} #Instantiate a dictionary

        CandleCloses[""Bid Closes""] = TickDataframe[""bid""].resample(ResampleOffset).ohlc().close.dropna().to_numpy() #Resample the bid ticks to create bid candles based on the resample offset and convert it into a numpy array

        CandleCloses[""Ask Closes""] = TickDataframe[""ask""].resample(ResampleOffset).ohlc().close.dropna().to_numpy() #Resample the ask ticks to create ask candles based on the resample offset and convert it into a numpy array

        CandleCloses[""Mid Closes""] = TickDataframe[""mid""].resample(ResampleOffset).ohlc().close.dropna().to_numpy() #Resample the mid ticks to create mid candles based on the resample offset and convert it into a numpy array

        return CandleCloses #Return the dictionary containing candle closes

    else: #If the ResampleOffset input is None

        return TickDataframe #Return the dataframe of ticks

```

#### Problem description

After resampling the collected tick data with the ""1Min"" offset, the resampled data for 2 different instruments have a different number of candles even when their time indexes cover the same date range.

#### Additional Observations

The other offsets do not have this inconsistent behaviour, the issue only happens with the ""1Min"" resample offset.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en
LOCALE           : English_Singapore.1252

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.50.1

</details>
"
664663221,35396,PKG: Set max pin for Cython,TomAugspurger,closed,2020-07-23T17:56:06Z,2020-07-23T19:41:13Z,"We know that pandas doesn't work with Cython 3.0
(https://github.com/pandas-dev/pandas/issues/34213,
https://github.com/pandas-dev/pandas/issues/34014)

This sets the maximum supported version of Cython in our pyproject.toml
to ensure that pandas 1.1.0 can continue to be built from source without
Cython pre-installed after Cython 3.0 is released."
640415178,34850,BUG: epoch2num matplotlib deprecation warning,dstansby,closed,2020-06-17T12:47:58Z,2020-07-23T22:06:03Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Problem description
Using the current matplotlib master branch, pandas raises the following deprecation warning:
```
/Users/dstansby/miniconda3/envs/dev/lib/python3.8/site-packages/pandas/plotting/_matplotlib/converter.py:256: MatplotlibDeprecationWarning: 
The epoch2num function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.
```
It looks like this line is present on pandas master, so should probably be changed (I'm not sure how) to avoid raising this deprecation warning.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.4.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.4
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 47.1.1.post20200604
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : 3.0.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.1
matplotlib       : 3.2.1.post2858+gc3bfeb9c3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
661777212,35350,BUG: register_matplotlib_converters leads to wrong datetime interpretation with matplotlib 3.3,ZahlGraf,closed,2020-07-20T12:42:00Z,2020-07-23T22:06:04Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import datetime
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.dates import DateFormatter

from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()

t = [
    datetime.datetime(2022, 12, 31, 0, 0), 
    datetime.datetime(2022, 11, 30, 0, 0), 
    datetime.datetime(2022, 10, 31, 0, 0),
]
s = [0.0, 0.1, 0.2]

fig, ax = plt.subplots()
ax.plot_date(x=t, y=s)
ax.xaxis.set_major_formatter(DateFormatter(""%b '%y""))

plt.xticks(rotation=90)
plt.show()

```

#### Problem description

The years in the plot are now 91 and 92. 

![grafik](https://user-images.githubusercontent.com/10481491/87938044-387f6680-ca96-11ea-9e33-b43be7af9bd0.png)

**Note**: With `matplotlib 3.2.2` (the last stable release) the error is not there. So I guess this is a bug due to an api change inside `matplotlib 3.3.0`.

#### Expected Output

I expects the year 22 and 23 like in this image:

![grafik](https://user-images.githubusercontent.com/10481491/87938153-6664ab00-ca96-11ea-9948-9863545e8a0d.png)

The second image can be produced by deleting the call to `register_matplotlib_converters()`

#### Output of ``pd.show_versions()``

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```
"
664550156,35393,Matplotlib 3.3 compatibility fixups,TomAugspurger,closed,2020-07-23T15:00:41Z,2020-07-23T22:06:10Z,"Closes #34850 
Closes https://github.com/pandas-dev/pandas/issues/35350

Haven't tested with matplotlib's older than 3.2 yet. We'll see what CI says."
659315534,35326,BUG: multiindex reindex doesn't work properly,chloe-wang,closed,2020-07-17T14:45:50Z,2020-07-24T11:09:24Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
a = pd.DataFrame([[1,2,3,4],[5,6,7,8]], index=[['a','a'], ['x', 'y']])
a.reindex(['x','z'], level=1)
#       0  1  2  3
#a x  1  2  3  4

```

#### Problem description

pandas doesn't work properly for multiindex reindexing. It drops the index that didn't appear in the original dataset directly.

#### Expected Output
we expect pandas will reindex the second level of the multiindex to ['x' , 'z']. But it drops 'z' directly.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-109-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 45.2.0.post20200210
Cython           : 0.29.15
pytest           : 5.3.5
hypothesis       : 5.5.4
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None

</details>
"
623645384,34337,don't need len around shape,SilasK,closed,2020-05-23T12:02:59Z,2020-07-24T12:15:42Z,"#Fix #34274

- [x] closes #34274
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
638082027,34739,DOC: updated files for SS06 errors,willpeppo,closed,2020-06-13T02:46:48Z,2020-07-24T12:38:48Z,
618524059,34182,TST: add explicit csr explicit zero tests for GH28992,bakitybacon,closed,2020-05-14T20:44:35Z,2020-07-24T12:43:37Z,"simple test case to check that explicitly setting a value to zero in a matrix with csr representation does not cause other values to change, specifically to be set to zero erroneously

- [ ] closes #xxxx
- [*] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
664781580,35397,CLN: Remove leftover partial-result code from apply,alonme,closed,2020-07-23T21:38:32Z,2020-07-24T12:47:03Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
637556654,34727,PERF: remove use of Python sets for interpolate,simonjayhawkins,closed,2020-06-12T07:46:31Z,2020-07-24T16:22:43Z,"have been investigating avoiding internals for interpolation. xref #34628


This PR address the issue of using Python sets, which is responsible for the bulk of the time in our current asv.

There are other improvements (will raise other PRs), but would need new benchmarks to show a benefit such as different index types and unsorted indexes. so this PR is targeting our current benchmark first.

prelim results (will post asv results if tests pass)
```
N = 10000
# this is the worst case, where every column has NaNs.
df = pd.DataFrame(np.random.randn(N, 100))
df.values[::2] = np.nan
df
%timeit df.interpolate()
# 189 ms ± 24.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)  <-- master
# 65.1 ms ± 635 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)  <-- PR
```
"
664806386,35398,DOC: Fixed formatting and errors in whatsnew v1.1.0,rhshadrach,closed,2020-07-23T22:35:25Z,2020-07-25T14:49:01Z,"cc @TomAugspurger 

Minor fixes for formatting, spelling, and grammar. Please let me know if any of this is too nit-picky, I can revert to just the definite spelling/syntax errors."
653533645,35186,Performance regression in stat_ops.FrameMultiIndexOps.time_op,simonjayhawkins,closed,2020-07-08T19:15:05Z,2020-07-25T15:02:04Z,"- [ ] closes #35050
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

```
import pandas as pd
import numpy as np

levels = [np.arange(10), np.arange(100), np.arange(100)]
codes = [
    np.arange(10).repeat(10000),
    np.tile(np.arange(100).repeat(100), 10),
    np.tile(np.tile(np.arange(100), 100), 10),
]
index = pd.MultiIndex(levels=levels, codes=codes)
df = pd.DataFrame(np.random.randn(len(index), 4), index=index)
%timeit df.std(level=1)
# 9.09 ms ± 59.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) -> master
# 7.39 ms ± 71.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) -> 0.25.3
# 7.21 ms ± 113 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) -> PR
```"
665135293,35402,BUG: DateTime Index format bug when importing from .csv file,filipporemonato,closed,2020-07-24T12:28:23Z,2020-07-27T05:29:12Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
data = pd.read_csv(file_path, engine='python', sep=';')
data = data.set_index('timestamp_utc')
data.index = pd.to_datetime(data.index, utc=True)

```

#### Problem description
I am sorry but I cannot provide a mwe because time is against me, but I still want to report the problem.
I have loaded a .csv file which has a column called ""timestamp_utc"" containing the full timestamp, date+time.
I have then set the timestamp_utc column as my index, and converted it to datetime object.
My code was not working because some dates were apparently missing while, by hand inspectyion of the .csv file, they were present.

The .csv file has all dates in dd/mm/yyyy format, from 02/12/2019 to 03/06/2020, but after running the code above and checking the index in the dataframe, this was shown (see screenshot):
![image](https://user-images.githubusercontent.com/6236646/88389825-85757c80-cdb7-11ea-8c66-0dca1e691b35.png)

so the index jumps from 2019/12/02 (which is yyyy/dd/mm format) to 2019/02/13 (which is yyyy/mm/dd format)!

#### Expected Output
I do not mind much if Pandas choses to import something as yyyy/dd/mm or yyyy/mm/dd, I know that I can use the ""dayfirst"" option in case, but at least Pandas should be consistent. This simply breaks the logic of indexes, which should always be increasing values. I find it quite surprising that no warning was given during the set_index operation.

These kind of issues happen on a daily basis when working with Pandas and in my group several tens of hours are spent debugging what should be a straightforward thing. My suggestion to avoid these problems in the future is that Pandas should expect and treat dates as if they were dd/mm/yyyy or yyyy/mm/dd format, throwing (at least) a warning when a date in mm/dd/yyyy format is found, so that the user can correct the import.
 
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : None.None

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
666135725,35422,Using loc[] or at[] to add rows doesn't work with an IntervalIndex doesn't work,konstantinmiller,closed,2020-07-27T09:31:18Z,2020-07-27T13:13:01Z,"It seems that a data frame with an `IntervalIndex` won't let you use `loc[]` or `at[]` to add rows.

I would expect the following to work:
```
df = pd.DataFrame(index=pd.IntervalIndex(closed='left', dtype='interval[int64]',
                                         data=[pd.Interval(0, 1, closed='left')]),
                  data={'a': [42]})
df.at[pd.Interval(2, 3, closed='left'), 'a'] = 43
```

However, it throws a confusing error
```
AttributeError: 'pandas._libs.interval.IntervalTree' object has no attribute 'set_value'
```"
663298844,35369,BUG: df reassignment following reorder_categories changed behavior in 1.1.0rc0,MaozGelbart,closed,2020-07-21T20:48:11Z,2020-07-27T14:12:36Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

h = pd.Series(list(""mn"")).astype(""category"")
df = pd.DataFrame({""h"":h})

df.h = df.h.cat.reorder_categories([""n"", ""m""])
print(df.h.cat.categories)

h = h.cat.reorder_categories([""n"",""m""])
print(h.cat.categories)
```

#### Problem description
Output with 1.1.0rc0:
```
Index(['m', 'n'], dtype='object')
Index(['n', 'm'], dtype='object')
```
The assignment of the reordered series back into the dataframe seems to remove the new order. This has changed from 1.0.5 but I could not find a note on this under the release notes.

#### Expected Output
As with 1.0.5:
```
Index(['n', 'm'], dtype='object')
Index(['n', 'm'], dtype='object')
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : bfac13628394ac7317bb94833319bd6bf87603af
python           : 3.8.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.5.0
Version          : Darwin Kernel Version 19.5.0: Tue May 26 20:41:44 PDT 2020; root:xnu-6153.121.2~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.1.0rc0
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200712
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
667013846,35435,DOC: 1.1.0 release date,TomAugspurger,closed,2020-07-28T11:56:29Z,2020-07-28T13:11:31Z,
584947304,32857,ImportError: DLL load failed with Windows wheel for 1.0.2 and 1.0.3,J-Postma,closed,2020-03-20T09:39:09Z,2020-07-28T13:21:55Z,"When trying to install `pandas 1.0.2` or newer versions on a docker, I run into the following import error:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Python\lib\site-packages\pandas\__init__.py"", line 55, in <module>
    from pandas.core.api import (
  File ""C:\Python\lib\site-packages\pandas\core\api.py"", line 29, in <module>
    from pandas.core.groupby import Grouper, NamedAgg
  File ""C:\Python\lib\site-packages\pandas\core\groupby\__init__.py"", line 1, in <module>
    from pandas.core.groupby.generic import DataFrameGroupBy, NamedAgg, SeriesGroupBy
  File ""C:\Python\lib\site-packages\pandas\core\groupby\generic.py"", line 60, in <module>
    from pandas.core.frame import DataFrame
  File ""C:\Python\lib\site-packages\pandas\core\frame.py"", line 124, in <module>
    from pandas.core.series import Series
  File ""C:\Python\lib\site-packages\pandas\core\series.py"", line 4572, in <module>
    Series._add_series_or_dataframe_operations()
  File ""C:\Python\lib\site-packages\pandas\core\generic.py"", line 10349, in _add_series_or_dataframe_operations
    from pandas.core.window import EWM, Expanding, Rolling, Window
  File ""C:\Python\lib\site-packages\pandas\core\window\__init__.py"", line 1, in <module>
    from pandas.core.window.ewm import EWM  # noqa:F401
  File ""C:\Python\lib\site-packages\pandas\core\window\ewm.py"", line 5, in <module>
    import pandas._libs.window.aggregations as window_aggregations
ImportError: DLL load failed while importing aggregations: The specified module could not be found.
```

I've reproduced this error on a separate pc by running:

```cmd
docker run -it python cmd.exe
pip install pandas
python
import pandas
```

This error only occurs when trying to import pandas from the docker. Python and pip versions:

```
python 3.8.2
pip 20.0.2
```"
369918530,23151,BLD: wheel building ,jreback,closed,2018-10-14T16:26:37Z,2020-07-28T13:22:29Z,"https://github.com/explosion/wheelwright

maybe we should investigate this for building multi platform wheels

though i think conda forge is also starting an effort to do this"
615959462,34116,MAINT: Clarify supported Stata dta versions,bashtage,closed,2020-05-11T15:07:53Z,2020-07-28T14:41:35Z,"Test against old Stata versions and remove text indicating support
for versions which do not work reliably

closes #26667

- [x] closes #26667
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
612927666,34013,ENH: Add compression to stata exporters,bashtage,closed,2020-05-05T22:13:55Z,2020-07-28T14:41:35Z,"Add standard compression optons to stata exporters

closes #26599

- [X] closes #26599
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
538133355,30285,ENH: Add StataWriter 118 for unicode support,bashtage,closed,2019-12-16T00:46:04Z,2020-07-28T14:41:35Z,"Add StataWriter with unicode support

- [X] closes #23573
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
562671722,31853,REF: Use nonzero in place of argwhere,bashtage,closed,2020-02-10T16:20:16Z,2020-07-28T14:41:36Z,"Use nonzero to preserve 1d of Series

xref #31813

"
550767320,31072,CLN/MAINT: Clean and annotate stata reader and writers,bashtage,closed,2020-01-16T12:09:02Z,2020-07-28T14:41:36Z,"- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
548861057,30959,ENH: Add Stata 119 writer,bashtage,closed,2020-01-13T11:13:48Z,2020-07-28T14:41:36Z,"Add support for writing Stata 119 format files
Rename new writer to StataWriterUTF8 since no longer version-specific
Improve exception message for unsupported files
Fix small issues in to_stata missed in 118

- [X] xref #23573 (already closed)
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
637713516,34730,RLS: 1.1,TomAugspurger,closed,2020-06-12T12:25:07Z,2020-07-28T19:38:31Z,"Tracking issue for the 1.1 release. I've set the target for July 15th. https://github.com/pandas-dev/pandas/milestone/68

Planning a release candidate for sometime in early July, 1-2 weeks before the final release.

List of open regressions: https://github.com/pandas-dev/pandas/issues?q=is%3Aopen+is%3Aissue+label%3ARegression
List of open blockers: https://github.com/pandas-dev/pandas/issues?q=is%3Aopen+is%3Aissue+label%3Ablocker+"
351719013,22402,"ENH: implement pd.Series.corr(method=""distance"")",dsaxton,closed,2018-08-17T19:53:58Z,2020-07-28T21:50:50Z,"Distance correlation (https://en.wikipedia.org/wiki/Distance_correlation) is a powerful yet underused technique for comparing two distributions that I think would make a very nice addition to the existing correlation methods in `pandas`.  For one, these measures have the unique property that two random variables $X$ and $Y$ are independent if and only if their distance correlation is zero, which cannot be said of Pearson, Spearman or Kendall.

The below code is an implementation in pure `numpy` (which could certainly be optimized / more elegantly written) that could be part of the `Series` class and then called within `corr`.  Later it could be integrated seamlessly with `corrwith`, and if this feature were available I know personally it would be one of the first things I would look at when approaching a regression problem.

```python
# self and other can be assumed to be aligned already
def nandistcorr(self, other):
    n = len(self)
    a = np.zeros(shape=(n, n))
    b = np.zeros(shape=(n, n))

    for i in range(n):
        for j in range(i+1, n):
            a[i, j] = abs(self[i] - self[j])
            b[i, j] = abs(other[i] - other[j])

    a = a + a.T
    b = b + b.T

    a_bar = np.vstack([np.nanmean(a, axis=0)] * n)
    b_bar = np.vstack([np.nanmean(b, axis=0)] * n)

    A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())
    B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())

    cov_ab = np.sqrt(np.nansum(A * B)) / n
    std_a = np.sqrt(np.sqrt(np.nansum(A**2)) / n)
    std_b = np.sqrt(np.sqrt(np.nansum(B**2)) / n)

    return cov_ab / std_a / std_b
```

Here's an example that shows how distance correlation can detect relationships that the other common correlation methods miss:

```python
import numpy as np
import pandas as pd
np.random.seed(2357)

s1 = pd.Series(np.random.randn(1000))
s2 = s1**2

s1.corr(s2, method=""pearson"")
s1.corr(s2, method=""spearman"")
s1.corr(s2, method=""kendall"")
nandistcorr(s1.values, s2.values)
```"
667462327,35442,BUG: 'something' in pd.Series does not return the True when the object is contained.,sorenwacker,closed,2020-07-29T00:25:27Z,2020-07-29T00:33:01Z,"```python
import pandas as pd
df = pd.DataFrame(['Test', '123'])
'test' in df[0]
> False
```
I am not sure if this made on purpose, but I think this should return True. False seems a bit strange. Is there a good reason for this? "
666750053,35431,BUG: index_col in read_csv ignores dtype,metazoic,closed,2020-07-28T04:28:49Z,2020-07-29T03:38:10Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

If `index.csv` contains:
```
id,key
00,11
22,33
```
then
```python
df = pandas.read_csv('index.csv', dtype=str, index_col='id')
df.index.dtype
```
results in
```
dtype('int64')
```

#### Problem description

This does not result in an index of strings.

#### Expected Output

By contrast,
```python
df = pandas.read_csv('index.csv', dtype=str)
df.set_index('id', inplace=True)
df.index.dtype
```
results in
```
dtype('O')
```
as intended.

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8
 
pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : None
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.2
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : 3.4.4
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.9
numba            : 0.50.1
```

</details>
"
662090298,35355,DOC: Fix small spelling mistake in style docs,flrs,closed,2020-07-20T17:49:30Z,2020-07-29T09:12:42Z,"This pull request fixes a small spelling mistake in the pandas style user guide.

I believe this change does not justify a whatsnew entry, since it is extremely small. It does not refer to a particular issue on GitHub. I have not run `black` or `flake8`, since I believe the change is out of scope for these tools.

- [ ] ~~closes #xxxx~~
- [ ] ~~tests added / passed~~
- [ ] ~~passes `black pandas`~~
- [ ] ~~passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`~~
- [ ] ~~whatsnew entry~~
"
666743357,35430,BUG: parse_dates in read_csv fails silently,metazoic,closed,2020-07-28T04:16:43Z,2020-07-29T09:43:57Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

If `timestamp.csv` contains the following:
```
timestamp
11:22:33.44
11:22:33:44
```
then
```python
df = pandas.read_csv('timestamp.csv', parse_dates=['timestamp'])
df.timestamp.dtype
```
results in
```
dtype('O')
```
because of the malformed timestamp.

#### Problem description

This is a problem because the silent fail does not alert to the malformed timestamp.

#### Expected Output

By contrast,
```python
pandas.to_datetime(df.timestamp)
```
results in an explicit error
```python
ParserError: Unknown string format: 11:22:33:44
```

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8
 
pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : None
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.2
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : 3.4.4
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.9
numba            : 0.50.1
```

</details>
"
660875767,35344,TYP: remove # type: ignore for unpacking compression_args,simonjayhawkins,closed,2020-07-19T14:43:11Z,2020-07-29T10:42:52Z,"eg. compresslevel for GzipFile is an int and compression in get_handle is Optional[Union[str, Mapping[str, Any]]] = None

once we adopt TypedDict, then maybe we could be more explicit here, but for now Any is appropriate.

the ignored message for GZipFile is `error: Argument 2 to ""GzipFile"" has incompatible type **Dict[str, str]""; expected ""int""`"
667782182,35452,DOC: Start 1.1.1,TomAugspurger,closed,2020-07-29T11:48:02Z,2020-07-29T13:45:52Z,
667783787,35453,DOC: Add 1.1.1 whatsnew,simonjayhawkins,closed,2020-07-29T11:50:48Z,2020-07-29T14:11:54Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
667858509,35458,Backport PR #35452 on branch 1.1.x (DOC: Start 1.1.1),meeseeksmachine,closed,2020-07-29T13:46:11Z,2020-07-29T14:46:57Z,Backport PR #35452: DOC: Start 1.1.1
660072845,35334,REGR: np.argwhere on pd.Series raises ValueError,simonjayhawkins,closed,2020-07-18T10:47:19Z,2020-07-29T15:48:31Z,"- [ ] closes #35331 

not sure what behaviour we want (will add tests after discussion). We should be thinking of removing \_\_array_wrap__ in favour of \_\_array_ufunc__ and \_\_array_function__ anyhow.

from https://numpy.org/doc/stable/user/basics.subclassing.html?highlight=__array_wrap__#array-wrap-for-ufuncs-and-other-functions

> It is hoped to eventually deprecate these, but __array_wrap__ is also used by other numpy functions and methods, such as squeeze, so at the present time is still needed for full functionality.

in 0.25.3 we returned the NumPy array (albeit with a warning)

```
>>> pd.__version__
'0.25.3'
>>>
>>> s = pd.Series(np.random.randn(5), index=[""a"", ""b"", ""c"", ""d"", ""e""])
>>> s
a    1.640847
b    0.316918
c   -0.193584
d    1.473205
e    0.357223
dtype: float64
>>>
>>> np.argwhere(s < 0)
C:\Users\simon\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py:61: FutureWarning: Series.nonzero() is deprecated and will be removed in a
 future version.Use Series.to_numpy().nonzero() instead
  return bound(*args, **kwds)
array([[2]], dtype=int64)
>>>
```
"
655856099,35261,Failures with pytest 6.0.0rc1,TomAugspurger,closed,2020-07-13T13:33:12Z,2020-07-29T15:59:29Z,"https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=39140&view=logs&j=a3a13ea8-7cf0-5bdb-71bb-6ac8830ae35c&t=add65f64-6c25-5783-8fd6-d9aa1b63d9d4


```
=================================== FAILURES ===================================
____ TestStringMethods.test_api_per_method[index-empty0-get_dummies-object] ____
[gw0] linux -- Python 3.7.6 /home/vsts/miniconda3/envs/pandas-dev/bin/python

        copy=False,
        name=None,
        verify_integrity: bool = True,
        _set_identity: bool = True,
    ):
    
        # compat with Index
        if name is not None:
            names = name
        if levels is None or codes is None:
            raise TypeError(""Must pass both levels and codes"")
        if len(levels) != len(codes):
            raise ValueError(""Length of levels and codes must be the same."")
        if len(levels) == 0:
>           raise ValueError(""Must pass non-zero number of levels/codes"")
E           ValueError: Must pass non-zero number of levels/codes

pandas/core/indexes/multi.py:271: ValueError
```

In the failing cases we dynamically add a an xfail marker: https://github.com/pandas-dev/pandas/blob/f477a0e8146e3f249d2a5c08031020c15c1ecd95/pandas/tests/test_strings.py#L266-L268. Reported at https://github.com/pytest-dev/pytest/issues/7486.

For now we can pin pytest."
660366334,35339,CLN: resolve isort mypy import confilict test_algos,fangchenli,closed,2020-07-18T20:36:07Z,2020-07-29T16:45:22Z,"Part of #35134
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
663839651,35380,CLN: resolve isort mypy import confilict test_join,fangchenli,closed,2020-07-22T15:12:29Z,2020-07-29T16:45:38Z,"Part of #35134
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
664065531,35386,CLN: resolve isort mypy import confilict test_transform,fangchenli,closed,2020-07-22T21:37:04Z,2020-07-29T16:46:03Z,"Part of #35134
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
667806384,35454,DOC: update Python support policy,simonjayhawkins,closed,2020-07-29T12:28:46Z,2020-07-29T18:06:02Z,xref https://github.com/pandas-dev/pandas/issues/34472#issuecomment-660695759
667909684,35459,CI: activate github actions on 1.1.x (PR only) - DO NOT MERGE,simonjayhawkins,closed,2020-07-29T14:54:36Z,2020-07-29T18:16:00Z,"IIUC without changes to other scripts, activating GitHub actions for merge would push the docs. so this is only for PRs

If works. will open PR against master instead. and backport.

xref https://github.com/pandas-dev/pandas/pull/34800#issuecomment-644669496"
667930090,35461,CI: activate azure pipelines on 1.1.x - DO NOT MERGE,simonjayhawkins,closed,2020-07-29T15:21:32Z,2020-07-29T18:19:27Z,"Azure pipelines worked for 1.0.x since #32706 which specified which branches to run on was not backported

If works. will open PR against master instead. and backport."
658679696,35315,DOC: whatsnew for 1.2,arw2019,closed,2020-07-17T00:00:29Z,2020-07-29T19:07:43Z,"@mroeschke  re: #35302 
"
667139532,35437,BUG: tuple-of-tuples indexing results in NumPy VisibleDeprecationWarning,simonjayhawkins,closed,2020-07-28T14:53:24Z,2020-07-29T19:35:07Z,"- [ ] closes #35434
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
406360137,25141,ENH: Added max_gap keyword for series.interpolate,cchwala,closed,2019-02-04T14:41:41Z,2020-07-29T20:45:02Z,"- [x] closes #12187 and #26796 
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This PR introduces the new keyword `max_gap` for `interpolate`. For all NaN-gaps which are wider than `max_gap` no interpolation is carried out and all NaNs of the gap are preserved. This is in contrast to using the `limit` kwarg which does not prevent interpolating values in a longer NaN gap.

I added numpy-based implementation that searches for NaN-gaps wider than `max_gap`. In line with the current implementations for NaN handling in `series.interpolate`, a set of NaN-indices that has
to be preserved is generated. This is used in the end, after a full interpolation of all NaN is done, to restore the NaNs gaps that shall not be interpolated.

Test and documentation were also added.

It will need some small PEP8-cleanup and maybe tests using other interpolation methods then `linear` (edit: Done). But before I continue, I would like to get feedback if my approach is in general okay.

This PR might also be extended to close #16457 which is on interpolation directly after resampling.

Example usage:

```python
In [1]: import numpy as np                                                                                                                                                                                                         

In [2]: import pandas as pd                                                                                                                                                                                                        

In [3]: s = pd.Series([np.nan, 1., np.nan, 2., np.nan, np.nan,  
   ...:                5., np.nan, np.nan, np.nan, -1., np.nan, np.nan])                                                                                                                                                           

# Using the new `max_gap` kwarg
In [4]: s.interpolate(max_gap=2, limit_area='inside')                                                                                                                                                                               
Out[4]: 
0     NaN
1     1.0
2     1.5
3     2.0
4     3.0
5     4.0
6     5.0
7     NaN
8     NaN
9     NaN
10   -1.0
11    NaN
12    NaN
dtype: float64

# Compare to the result when using the existing `limit` kwarg
In [5]: s.interpolate(limit=2, limit_area='inside')                                                                                                                                                                                
Out[5]: 
0     NaN
1     1.0
2     1.5
3     2.0
4     3.0
5     4.0
6     5.0
7     3.5
8     2.0
9     NaN
10   -1.0
11    NaN
12    NaN
dtype: float64
```

Timing:
```python
In [6]: %timeit s.interpolate(max_gap=2, limit_area='inside')                                                                                                                                                                       
708 µs ± 14.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

In [7]: %timeit s.interpolate(limit=2, limit_area='inside')                                                                                                                                                                        
631 µs ± 5.85 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```
 
The relative speed difference is similar for larger Series."
668050906,35469,Backport PR #35272 on branch 1.1.x (CI: Unpin pytest),meeseeksmachine,closed,2020-07-29T18:29:47Z,2020-07-30T11:56:38Z,Backport PR #35272: CI: Unpin pytest
656435169,35272,CI: Unpin pytest,simonjayhawkins,closed,2020-07-14T08:35:44Z,2020-07-30T12:20:14Z,"- [ ] closes #35261

have made a start working through the failures. we should be able to get mypy to green here and then when pytest 6.0.0 is released the upstream fixes should cause ci to go red since we have `warn_unused_ignores = True` in setup.cfg."
581229535,32700,Track times,rbenes,closed,2020-03-14T14:24:32Z,2020-07-30T14:01:59Z,"I implement feature request https://github.com/pandas-dev/pandas/issues/32682 but if someone has more experience with pytables consider these changes.

The h5 files generated with track_times=False has the same md5 hashes, that is covered in the unit test as well as the fact that the hashes are not the same with track_times=True

- [x] closes #32682
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
667943015,35463,"Question/bug: How to get rid of ""0 days"" in timedelta64[s]",pllim,closed,2020-07-29T15:40:00Z,2020-07-30T16:22:12Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas. -- I can't tell if this is a feature or a bug.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
>>> import pandas as pd
>>> import numpy as np
>>> dt = pd.Series(np.array([1, 300], dtype='timedelta64[s]')) 
>>> df = pd.DataFrame({'dt': dt})
```
```python
>>> pd.__version__
'1.0.5'
>>> df
        dt
0 00:00:01
1 00:05:00
```
```python
>>> pd.__version__
'1.1.0'
>>> df
               dt
0 0 days 00:00:01
1 0 days 00:05:00
```

#### Problem description

""0 days"" started to appear when DataFrame contains `timedelta64[s]`.

#### Expected Output

See ""Code Sample"" for `pandas` 1.0.5.

#### Output of ``pd.show_versions()``

See ""Code Sample"" for `pandas` 1.1.0.

#### Further readings

astropy/astropy#10608 and astropy/astropy#10609"
660331350,35337,DOC: Improve DataFrame.dropna subset example,TyMick,closed,2020-07-18T19:25:08Z,2020-07-31T15:58:44Z,"In the examples for [DataFrame.dropna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html), which begin like so:

```pycon
>>> df = pd.DataFrame({""name"": ['Alfred', 'Batman', 'Catwoman'],
...                    ""toy"": [np.nan, 'Batmobile', 'Bullwhip'],
...                    ""born"": [pd.NaT, pd.Timestamp(""1940-04-25""),
...                             pd.NaT]})
>>> df
       name        toy       born
0    Alfred        NaN        NaT
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
```

the example that uses the `subset` parameter,

```pycon
>>> df.dropna(subset=['name', 'born'])
       name        toy       born
1    Batman  Batmobile 1940-04-25
```

yields the same output as the example that doesn't pass in any arguments,

```pycon
>>> df.dropna()
     name        toy       born
1  Batman  Batmobile 1940-04-25
```

I think changing the `subset` example to

```pycon
>>> df.dropna(subset=['name', 'toy'])
       name        toy       born
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
```

would better illustrate the concept, as its result contains a row with one missing value, just in a column that wasn't specified in `subset`.

-----

I'm not able to test the docstring locally at the moment, but I ran the code in my console up to the new example, and it worked as expected.

```pycon
>>> import pandas as pd
>>> import numpy as np
>>> df = pd.DataFrame({""name"": ['Alfred', 'Batman', 'Catwoman'],
...                    ""toy"": [np.nan, 'Batmobile', 'Bullwhip'],
...                    ""born"": [pd.NaT, pd.Timestamp(""1940-04-25""),
...                             pd.NaT]})
>>> df
       name        toy       born
0    Alfred        NaN        NaT
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
>>> df.dropna()
     name        toy       born
1  Batman  Batmobile 1940-04-25
>>> df.dropna(axis='columns')
       name
0    Alfred
1    Batman
2  Catwoman
>>> df.dropna(how='all')
       name        toy       born
0    Alfred        NaN        NaT
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
>>> df.dropna(thresh=2)
       name        toy       born
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
>>> df.dropna(subset=['name', 'toy'])
       name        toy       born
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
```"
668766204,35477,MAINT: Use float arange when required or intended,bashtage,closed,2020-07-30T14:05:09Z,2020-08-01T08:46:01Z,"Ensure arange is float when intened or required by NumPy

- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
670671180,35500,Backport PR #35477 on branch 1.1.x (MAINT: Use float arange when required or intended),meeseeksmachine,closed,2020-08-01T08:46:48Z,2020-08-01T09:23:36Z,Backport PR #35477: MAINT: Use float arange when required or intended
666183589,35426,CI/COMPAT: read_stata failing in numpy dev pipeline,AlexKirko,closed,2020-07-27T10:34:35Z,2020-08-01T09:34:51Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Problem description

Looks like a change in numpy broke our `read_stata` code, so now we have a bunch of `read_stata` test errors popping up during CI in the numpy_dev pipeline. Maybe this [Numpy PR](https://github.com/numpy/numpy/pull/16943) somehow broke things? I don't think any other PR merged during the last 24 hours could have possibly done so, although I might be missing something.

Error example:
```
pandas/tests/io/test_stata.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pandas/io/stata.py:1928: in read_stata
    data = reader.read()
pandas/io/stata.py:1646: in read
    cols_ = np.where(self.dtyplist)[0]
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ([<class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.float32'>, <class 'numpy.float64'>, <class 'numpy.float32'>, ...],)
kwargs = {}
relevant_args = ([<class 'numpy.int8'>, <class 'numpy.int16'>, <class 'numpy.int32'>, <class 'numpy.float32'>, <class 'numpy.float64'>, <class 'numpy.float32'>, ...], None, None)

>   ???
E   ValueError: invalid __array_struct__
```
"
670671334,35501,Backport PR #35470 on branch 1.1.x (CI: unpin isort 5 (#35134)),meeseeksmachine,closed,2020-08-01T08:47:01Z,2020-08-01T09:39:28Z,Backport PR #35470: CI: unpin isort 5 (#35134)
661025447,35345,"DOC: expand ""using a Docker container"" section",MarcoGorelli,closed,2020-07-19T19:38:12Z,2020-08-01T13:11:02Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#using-a-docker-container
#### Documentation problem

Someone unfamiliar with Docker would be confused reading this.

#### Suggested fix for documentation

The expansion needed is quite small, just:

1. link to the Docker installation instructions
2. link to how to develop in a container using PyCharm (current docs only link to vs code instructions)
3. mention that you might still need to rebuild the C extensions if/when you merge with upstream/master with
   ```
   python setup.py build_ext --inplace -j 4
   ```"
668101515,35470,CI: unpin isort 5 (#35134),fangchenli,closed,2020-07-29T19:49:12Z,2020-08-01T13:24:54Z,"- [x] closes #35134
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
631171850,34582,TST: series offset artithmetic tests,ghost,closed,2020-06-04T21:53:57Z,2020-08-01T14:07:20Z,"…racting a series of DateOffsets to / from a series of Timestamps (test_series_add_daytime_offset, test_series_add_daytime_offset)

- [x] closes #19211
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry (omitted, test only)
"
663798839,35379,DOC: Expanded Using a Docker Container section,alexhlim,closed,2020-07-22T14:18:12Z,2020-08-01T16:27:54Z,"- [x] closes #35345
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Added Docker installation link, Docker commands, and PyCharm sections."
670844871,35503,DOC: add note on str cons to read_sql,AdamSpannbauer,closed,2020-08-01T12:50:57Z,2020-08-01T17:05:55Z,"- [x] closes #35495
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
657969933,35302,BUG: date_range doesn't propagate ambigous=False to tz_localize,arw2019,closed,2020-07-16T08:20:58Z,2020-08-01T17:18:24Z,"- [x] closes #35297 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
671325737,35512,BUG: Unexpected/undocumented behaviour of sum and mean aggregations on object dtypes,MJafarMashhadi,closed,2020-08-02T00:15:02Z,2020-08-02T07:51:25Z,"- [ ] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

# Sum, looks kinda OK
>>> pd.Series('a b c d'.split()).sum()
'abcd'
>>> pd.Series('1 2 3 4'.split()).sum()
'1234'

# Mean, inconsistent behaviour
>>> pd.Series('a b c d'.split()).mean()
...
TypeError: Could not convert abcd to numeric
>>> pd.Series('1 2 3 4'.split()).mean()
308.5
```

I saw the last example in a #26927 comment, scanned the code to see what's wrong and eventually decided to open this issue.

#### Problem description

IMO When calculating the mean, data type conversion should either happen on each element of the series before applying sum or should happen in the end. Here, to calculate the mean of `['1', '2', '3', '4']`, first a `sum` concatenates all the elements keeping the dtype, then it's converted to float (1234) and then divided by 4 which results in 308.5. 


#### Expected Output
I have 3 options in mind:

1. an element-wise type conversion, which means:
```python
>>> pd.Series('a b c d'.split()).sum()  # and mean
Type Error
>>> pd.Series('1 2 3 4'.split()).sum()
10
>>> pd.Series('1 2 3 4'.split()).mean()
2.5
```

2. throwing an error in these cases or adding a note in docs for consequences of performing these aggregations on an object dtype. For example, considering that `np.sum(np.array('a b c d'.split()))` fails with a TypeError it's not intuitive that the same function in pandas performs a concatenation.

https://github.com/pandas-dev/pandas/blob/d9fff2792bf16178d4e450fe7384244e50635733/pandas/core/generic.py#L10410

just like what happens when dtype is explicitly set as 'string':
```python
>>> pd.Series('1 2 3 4'.split(), dtype='string').mean()
...
TypeError: Cannot perform reduction 'mean' with string dtype
```

3. No type conversions in the middle of calculating the mean: (It's not a good option IMHO for all the side effects it will have)
```python
>>> pd.Series('a b c d'.split()).sum()
'abcd'  
>>> pd.Series('a b c d'.split()).mean()
TypeError while trying to compute 'abcd' / 4, but not because 'abcd' cannot be converted to a number, but bc a string cannot be divided by a number
>>> pd.Series('1 2 3 4'.split()).sum()
'1234'
>>> pd.Series('1 2 3 4'.split()).mean()
TypeError while trying to '1234' / 4
```



#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 4014a60352b21e36ec390f361815e66f96ee783c
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-40-generic
Version          : #32~18.04.1-Ubuntu SMP Mon Feb 3 14:05:59 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_CA.UTF-8
LOCALE           : en_CA.UTF-8
pandas           : 1.1.0.dev0+2171.g4014a6035
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1.post20200616
Cython           : 0.29.20
pytest           : 5.4.3
hypothesis       : 5.16.1
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : 0.4.0
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0

</details>
"
550429054,31054,DOC: intended series.loc behavior undocumented for boolean series,hickmanw,closed,2020-01-15T21:05:39Z,2020-08-02T10:18:02Z,"#### series.loc when passed a boolean series
How will series.loc behave when passed a boolean series whose index contains the same values, but in a different order, than the focal series' index?

```python
s1 = pd.Series(list('ab'))
mask = [True, False]
s2 = pd.Series(mask, pd.Index([1,0]))

print(s1.loc[mask])
print(s1.loc[s2])
```
0    a
dtype: object
1    b
dtype: object

#### Problem description

I expected the index of `s2` to be ignored, in effect treating `s2` as a boolean array input, but I was wrong.

The [documentation on master](https://pandas-docs.github.io/pandas-docs-travis/) doesn't list a (boolean) series as an allowed input, hence doesn't provide guidance on which behavior should be expected/is intended. 

What is the intended behavior? If given guidance, I'll update the documentation.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.152-127.182.amzn2.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.7.4
pip              : 19.3.1
setuptools       : 40.4.3
Cython           : 0.29.14
pytest           : 5.3.2
hypothesis       : None
sphinx           : 2.3.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 7.0.1
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.4.0.dev0+4dcf606
sqlalchemy       : 1.3.11
tables           : None
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

</details>

"
671010949,35506,Added alignable boolean series and its example to `.loc` docs.,MJafarMashhadi,closed,2020-08-01T16:37:53Z,2020-08-02T10:18:22Z,"- [x] closes #31054
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I'm just not sure if I needed to add `IndexingError` to raises section. Added it to be safe, will remove if it's unnecessary.

Also, when someone uses `df.loc[df['col'] > 0]` it really is the same under the hood, `df['col'] > 0` is a boolean series with an alignable index. So there might be some overlap?"
671311234,35511,BUG: Unexpected behavior of sort_values,bhagyakjain,closed,2020-08-01T23:53:04Z,2020-08-02T15:31:20Z,"df.sort_values(by=[col])  -
If column(=col) has some values occurring multiple times then it orders them randomly. Instead, it should inherit the order in the DataFrame for the rows having same value for column."
561626051,31775,Feature Request: Sample method for Groupby objects,yadunathg,closed,2020-02-07T13:05:50Z,2020-08-02T17:39:24Z,"Many times it is desirable to be able to iterate over a small number of groups of a group by object. Currently there is no direct way to get a random sample of groups. 

Similar to DataFrame.sample method, can we have Groupby.sample method which gives us a Dictionary{'group_name'->'group_labels'} containing n randomly selected groups?

#### Expected behaviour
```python
import pandas as pd
df = pd.DataFrame([['Male', 1], ['Female', 3], ['Female', 2], ['Other', 1]],
                                 columns=['gender', 'feature'])
grouped_df = df.groupby('gender')

# Desired feature
grouped_df.sample(n=2)
# {'Female': Int64Index([1, 2], dtype='int64'),
#  'Male': Int64Index([0], dtype='int64')}
```"
565878951,32038,HDFStore dropna=True,xushengun,open,2020-02-16T09:51:52Z,2020-08-03T05:45:38Z,"#### Code Sample, a copy-pastable example if possible

```python
df_with_missing = pd.DataFrame({'col1': [0, np.nan, 2],'col2': [1, np.nan, np.nan]})
df_with_missing.to_hdf('file.h5', 'df_with_missing', format='table', mode='w')
pd.read_hdf('file.h5', 'df_with_missing')

df_with_missing.to_hdf('file1.h5', 'df_with_missing', format='table', mode='w', dropna=True)
pd.read_hdf('file1.h5', 'df_with_missing')

```
#### Problem description
'dropna=True' can't work .

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : 0.29.15
pytest           : 5.3.5
hypothesis       : 5.4.1
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.48.0
</details>
"
50405763,8918,Wishlist: make get_dummies() usable for train / test framework,chrish42,closed,2014-11-28T22:06:20Z,2020-08-03T11:44:05Z,"Having get_dummies() in Pandas is really nice, but to be useful for machine learning, it would need to be usable in a train / test framework (or ""fit_transform"" and ""transform"", with the sklearn terminology). Let me know if this needs more explanations.

So, I guess this is a wishlist bug report to add that functionality to Pandas. I can even create a pull request, if people agree this would be something useful to have in Pandas (and are willing to coach a bit and do code review for what would be my first contribution to this project).
"
669250276,35485,MAINT: Fix broadcasting of arrays,bashtage,closed,2020-07-30T22:29:36Z,2020-08-03T15:12:40Z,"Prevent empty object arrays from being broadcast

- [X] closes #35481
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
629156820,34527,Getting Attribute error while merging framed,datascientistpur,closed,2020-06-02T12:18:03Z,2020-08-03T21:12:09Z,"When merging frames,I am getting AttributeError:Dataframe object has no attribute _cols.
It shows the error in 5067 line of generic.py

Current pandas version:0.24.2

"
666144923,35423,TST: Add test for `GroupBy.describe()` with duplicate columns,smithto1,closed,2020-07-27T09:44:59Z,2020-08-03T23:13:38Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

While working on #35314 I made a change that broke `GroupBy.apply` if you called it on a DataFrame with duplicate columns; however, this break as not caught by any test. Was just caught on inpsection: https://github.com/pandas-dev/pandas/pull/35314#discussion_r460000612

This issue is just to add a test for this case to ensure it works and doesn't break again in the future. "
666146406,35424,TST: adding test for .describe() with duplicate columns,smithto1,closed,2020-07-27T09:47:09Z,2020-08-03T23:13:45Z,"- [x] closes #35423 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

While working on #35314 I made a change that broke `GroupBy.apply` if you called it on a DataFrame with duplicate columns; however, this break was not caught by any test. Was just caught on inpsection: https://github.com/pandas-dev/pandas/pull/35314#discussion_r460000612

This PR is just to add a test for this case to ensure it works and doesn't break again in the future. "
522914887,29617,BUG: groupby apply with head(1) raises keyerror with datetime grouper,endremborza,closed,2019-11-14T15:01:34Z,2020-08-03T23:19:27Z,"This is a very strange error with a long traceback. I found #15680 and #11324 mentioning similar things, but neither seem to cover the behavior here

#### Code Sample, a copy-pastable example:

```python
import datetime
import pandas as pd

recs = [{'LIVE': 1,
         'ITEM': '001',
         'DATE': datetime.date(2019, 10, 1)},
        {'LIVE': 2,
         'ITEM': '002',
         'DATE': datetime.date(2019, 10, 2)},
        {'LIVE': 3,
         'ITEM': '003',
         'DATE': datetime.date(2019, 10, 1)}]

pd.DataFrame(recs).groupby(['ITEM', 'DATE']).apply(lambda df: df.head(1))
```

on 0.25.3 this raises this convoluted KeyError:

<details>

```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2896             try:
-> 2897                 return self._engine.get_loc(key)
   2898             except KeyError:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: Timestamp('2019-10-01 00:00:00')

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
~/.local/lib/python3.7/site-packages/pandas/core/reshape/concat.py in _make_concat_multiindex(indexes, keys, levels, names)
    631                 try:
--> 632                     i = level.get_loc(key)
    633                 except KeyError:

~/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2898             except KeyError:
-> 2899                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2900         indexer = self.get_indexer([key], method=method, tolerance=tolerance)

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: Timestamp('2019-10-01 00:00:00')

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
~/.local/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in apply(self, func, *args, **kwargs)
    724             try:
--> 725                 result = self._python_apply_general(f)
    726             except Exception:

~/.local/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in _python_apply_general(self, f)
    744         return self._wrap_applied_output(
--> 745             keys, values, not_indexed_same=mutated or self.mutated
    746         )

~/.local/lib/python3.7/site-packages/pandas/core/groupby/generic.py in _wrap_applied_output(self, keys, values, not_indexed_same)
    371         elif isinstance(v, DataFrame):
--> 372             return self._concat_objects(keys, values, not_indexed_same=not_indexed_same)
    373         elif self.grouper.groupings is not None:

~/.local/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in _concat_objects(self, keys, values, not_indexed_same)
    972                     names=group_names,
--> 973                     sort=False,
    974                 )

~/.local/lib/python3.7/site-packages/pandas/core/reshape/concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    254         copy=copy,
--> 255         sort=sort,
    256     )

~/.local/lib/python3.7/site-packages/pandas/core/reshape/concat.py in __init__(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)
    427 
--> 428         self.new_axes = self._get_new_axes()
    429 

~/.local/lib/python3.7/site-packages/pandas/core/reshape/concat.py in _get_new_axes(self)
    521 
--> 522         new_axes[self.axis] = self._get_concat_axis()
    523         return new_axes

~/.local/lib/python3.7/site-packages/pandas/core/reshape/concat.py in _get_concat_axis(self)
    577             concat_axis = _make_concat_multiindex(
--> 578                 indexes, self.keys, self.levels, self.names
    579             )

~/.local/lib/python3.7/site-packages/pandas/core/reshape/concat.py in _make_concat_multiindex(indexes, keys, levels, names)
    635                         ""Key {key!s} not in level {level!s}"".format(
--> 636                             key=key, level=level
    637                         )

ValueError: Key 2019-10-01 00:00:00 not in level Index([2019-10-01, 2019-10-02], dtype='object', name='DATE')

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
~/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2896             try:
-> 2897                 return self._engine.get_loc(key)
   2898             except KeyError:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: Timestamp('2019-10-01 00:00:00')

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
~/.local/lib/python3.7/site-packages/pandas/core/reshape/concat.py in _make_concat_multiindex(indexes, keys, levels, names)
    631                 try:
--> 632                     i = level.get_loc(key)
    633                 except KeyError:

~/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2898             except KeyError:
-> 2899                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2900         indexer = self.get_indexer([key], method=method, tolerance=tolerance)

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: Timestamp('2019-10-01 00:00:00')

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-32-cf7f934e7c7f> in <module>
     12          'DATE': datetime.date(2019, 10, 1)}]
     13 
---> 14 pd.DataFrame(recs).groupby(['ITEM', 'DATE']).apply(lambda df: df.head(1))

~/.local/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in apply(self, func, *args, **kwargs)
    735 
    736                 with _group_selection_context(self):
--> 737                     return self._python_apply_general(f)
    738 
    739         return result

~/.local/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in _python_apply_general(self, f)
    743 
    744         return self._wrap_applied_output(
--> 745             keys, values, not_indexed_same=mutated or self.mutated
    746         )
    747 

~/.local/lib/python3.7/site-packages/pandas/core/groupby/generic.py in _wrap_applied_output(self, keys, values, not_indexed_same)
    370             return DataFrame()
    371         elif isinstance(v, DataFrame):
--> 372             return self._concat_objects(keys, values, not_indexed_same=not_indexed_same)
    373         elif self.grouper.groupings is not None:
    374             if len(self.grouper.groupings) > 1:

~/.local/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in _concat_objects(self, keys, values, not_indexed_same)
    971                     levels=group_levels,
    972                     names=group_names,
--> 973                     sort=False,
    974                 )
    975             else:

~/.local/lib/python3.7/site-packages/pandas/core/reshape/concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    253         verify_integrity=verify_integrity,
    254         copy=copy,
--> 255         sort=sort,
    256     )
    257 

~/.local/lib/python3.7/site-packages/pandas/core/reshape/concat.py in __init__(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)
    426         self.copy = copy
    427 
--> 428         self.new_axes = self._get_new_axes()
    429 
    430     def get_result(self):

~/.local/lib/python3.7/site-packages/pandas/core/reshape/concat.py in _get_new_axes(self)
    520                 new_axes[i] = ax
    521 
--> 522         new_axes[self.axis] = self._get_concat_axis()
    523         return new_axes
    524 

~/.local/lib/python3.7/site-packages/pandas/core/reshape/concat.py in _get_concat_axis(self)
    576         else:
    577             concat_axis = _make_concat_multiindex(
--> 578                 indexes, self.keys, self.levels, self.names
    579             )
    580 

~/.local/lib/python3.7/site-packages/pandas/core/reshape/concat.py in _make_concat_multiindex(indexes, keys, levels, names)
    634                     raise ValueError(
    635                         ""Key {key!s} not in level {level!s}"".format(
--> 636                             key=key, level=level
    637                         )
    638                     )

ValueError: Key 2019-10-01 00:00:00 not in level Index([2019-10-01, 2019-10-02], dtype='object', name='DATE')
```

</details>

oddly, both of these work

```python
pd.DataFrame(recs).groupby(['ITEM']).apply(lambda df: df.head(1))
pd.DataFrame(recs).groupby(['DATE']).apply(lambda df: df.head(1))
```

also, the behavior is the same if I modify recs so that only 1 distinct date is present

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.2.1-1.el7.elrepo.x86_64
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : 1.3.10
tables           : 3.6.1
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
670852264,35504,TST: ensure that DataFrameGroupBy.apply does not convert datetime.date to pd.Timestamp,smithto1,closed,2020-08-01T13:01:04Z,2020-08-03T23:19:31Z,"- [x] closes #29617 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Previously, there was a bug in `DataFrameGroupBy.apply` where a column of `datetime.date` would raise a `ValueError` if it was included as column in a multi-column grouping. Somewhere in the grouping the values would be converted to `pd.Timestamp` and then when the `pd.Timestamp` couldn't be found in the index of `datetime.date`s it would throw an error. 

This bug persisted until 1.0.5 but was fixed in 1.1.0 (not sure which change fixed it). In 1.1.0 the `datetime.date` are not converted to `pd.Timestamp`, so they are treated like any other dtype=object and left unchanged. 

This PR adds a test to enforce this behaviour to make sure the bug does not arise again. "
669640174,35493,BUG: use_inf_as_na options raises value error for read_csv,mhaselsteiner,closed,2020-07-31T09:57:30Z,2020-08-03T23:29:32Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---



#### Code Sample to Reproduce Error

```python
import pandas as pd
import numpy as np
pd.set_option('use_inf_as_na', True)
pd.DataFrame({'test_data':[1,3,4,np.nan]}).to_csv('test_data.csv', na_rep='NaN')
pd.read_csv('test_data.csv',sep=',' ,na_values='NaN')
```
Causes `ValueError`:
```python-traceback
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/lena/anaconda3/envs/pandas_pip2/lib/python3.8/site-packages/pandas/io/parsers.py"", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File ""/home/lena/anaconda3/envs/pandas_pip2/lib/python3.8/site-packages/pandas/io/parsers.py"", line 458, in _read
    data = parser.read(nrows)
  File ""/home/lena/anaconda3/envs/pandas_pip2/lib/python3.8/site-packages/pandas/io/parsers.py"", line 1201, in read
    df = DataFrame(col_dict, columns=columns, index=index)
  File ""/home/lena/anaconda3/envs/pandas_pip2/lib/python3.8/site-packages/pandas/core/frame.py"", line 467, in __init__
    mgr = init_dict(data, index, columns, dtype=dtype)
  File ""/home/lena/anaconda3/envs/pandas_pip2/lib/python3.8/site-packages/pandas/core/internals/construction.py"", line 250, in init_dict
    missing = arrays.isna()
  File ""/home/lena/anaconda3/envs/pandas_pip2/lib/python3.8/site-packages/pandas/core/series.py"", line 4795, in isna
    return super().isna()
  File ""/home/lena/anaconda3/envs/pandas_pip2/lib/python3.8/site-packages/pandas/core/generic.py"", line 7109, in isna
    return isna(self).__finalize__(self, method=""isna"")
  File ""/home/lena/anaconda3/envs/pandas_pip2/lib/python3.8/site-packages/pandas/core/dtypes/missing.py"", line 124, in isna
    return _isna(obj)
  File ""/home/lena/anaconda3/envs/pandas_pip2/lib/python3.8/site-packages/pandas/core/dtypes/missing.py"", line 157, in _isna
    return _isna_ndarraylike(obj, inf_as_na=inf_as_na)
  File ""/home/lena/anaconda3/envs/pandas_pip2/lib/python3.8/site-packages/pandas/core/dtypes/missing.py"", line 218, in _isna_ndarraylike
    result = _isna_string_dtype(values, dtype, inf_as_na=inf_as_na)
  File ""/home/lena/anaconda3/envs/pandas_pip2/lib/python3.8/site-packages/pandas/core/dtypes/missing.py"", line 246, in _isna_string_dtype
    vec = libmissing.isnaobj_old(values.ravel())
  File ""pandas/_libs/missing.pyx"", line 160, in pandas._libs.missing.isnaobj_old
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
```

#### Problem description
Nans are not recognized as expected anymore if pandas option `use_inf_as_na`is set to True. Occurred first after upgrading to pandas 1.1.0

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-42-generic
Version          : #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2
setuptools       : 49.2.0.post20200712
Cython           : None
pytest           : 6.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None


</details>
"
276412798,18451,BUG: Pandas groupby indices behaving diferrently with 2 and 3 rows,mcdallas,closed,2017-11-23T15:31:31Z,2020-08-03T23:46:23Z,"#### Code Sample, a copy-pastable example if possible

```python
df1 = pd.DataFrame({
    'Company': ['Foo Inc.', 'Foo Inc.', 'Foo Inc.'],
    'ID': ['123456', '123456', '123456'],
    'Employee': ['John Doe', 'Richard Roe', 'Jane Doe'],
    'Position': ['Executive Director', 'Director', 'Company Secretary']
})
    
df2 = pd.DataFrame({
    'Company': ['Bar Inc.', 'Bar Inc.'],
    'ID': ['56789', '56789'],
    'Employee': ['Mark Moe', 'Larry Loe'],
    'Position': ['Tax Consultant', 'Company Secretary']
})

print(df1)
    Company     Employee      ID            Position
0  Foo Inc.     John Doe  123456  Executive Director
1  Foo Inc.  Richard Roe  123456            Director
2  Foo Inc.     Jane Doe  123456   Company Secretary

print(df2)
    Company   Employee     ID           Position
0  Bar Inc.   Mark Moe  56789     Tax Consultant
1  Bar Inc.  Larry Loe  56789  Company Secretary

gb1 = df1.set_index(['Company', 'ID', 'Employee']).groupby(['Company', 'ID'])
gb2 = df2.set_index(['Company', 'ID', 'Employee']).groupby(['Company', 'ID'])
    
for (name, id), new_df in gb1:
    print(name)
    print(id)
    
for (name, id), new_df in gb2:
    print(name)
    print(id)

Foo Inc.
123456

      3     print(id)
      4
----> 5 for (name, id), new_df in gb2:
      6     print(name)
      7     print(id)

ValueError: too many values to unpack (expected 2)
```
#### Problem description

I have 2 dataframes df1 and df2. Their format is the same with the only difference that the first has 3 rows and the second 2.

When I try to groupby and run the loop above it works for the first but not for the second.
This is because their indices are different

```python
gb1.indices
>>> {('Foo Inc.', '123456'): array([0, 1, 2], dtype=int64)}

gb2.indices
>>> {'Company': array([0], dtype=int64), 'ID': array([1], dtype=int64)}
```
the code above works if I replace the groupby line with
```python
gb2 = df2.set_index(['Company', 'ID', 'Employee']).groupby(level=['Company', 'ID'])
```

#### Expected Output

The output should be consistent in both cases.

#### Output of ``pd.show_versions()``

<details>

pandas: 0.20.1
pytest: 3.2.3
pip: 9.0.1
setuptools: 36.7.2
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.19.0
xarray: None
IPython: 5.3.0
sphinx: 1.5.6
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.2.2
numexpr: 2.6.2
feather: None
matplotlib: 2.0.2
openpyxl: 2.4.7
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: 0.9.6
lxml: 3.7.3
bs4: 4.6.0
html5lib: 0.9999999
sqlalchemy: 1.1.9
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
pandas_gbq: None
pandas_datareader: None


</details>
"
669679587,35494,TST: GroupBy on 2 rows of MultiIndex returns correct group indices,smithto1,closed,2020-07-31T10:42:08Z,2020-08-03T23:46:31Z,"- [x] closes #18451 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

There was previously a bug where grouping on precisely two rows of a MultiIndex would return a wrongly formatted index. This PR is adding a test to enforce this correct behaviour. The new test fails on 0.20.3 but passes on >=1.0.5.
"
667401325,35439,BUG: output of df.to_string depends on whether columns is a CategoricalIndex or not,topper-123,closed,2020-07-28T21:44:34Z,2020-08-03T23:48:55Z,"The output of ``df.to_string`` is different if the columns is a ``CategoricalIndex`` rather than a normal ``Index``:

```python
>>> data = [[4, 2], [3, 2], [4, 3]]
>>> cols = [""aaaaaaaaa"", ""b""]
>>>  pd.DataFrame(data, columns=cols)
   aaaaaaaaa  b
0          4  2
1          3  2
2          4  3
>>> pd.DataFrame(data, columns=pd.CategoricalIndex(cols))
   aaaaaaaaa  b
0          4          2
1          3          2
2          4          3
```

We can see that the width of the ""b"" column depends on the width of the ""a"" column, if the columns are a ``CategoricalIndex``.

These two examples should return the same repr."
655329265,35245,REF: de-duplicate get_resolution,jbrockmendel,closed,2020-07-12T04:33:31Z,2020-08-04T02:01:27Z,"This changes get_resolution to use the same less-verbose pattern as ints_to_pydatetime; hopefully we'll be able to refactor out a couple of helper functions. 

asv run says this is perf-neutral"
668046119,35468,CI: activate azure pipelines on 1.1.x,simonjayhawkins,closed,2020-07-29T18:22:16Z,2020-08-04T08:26:58Z,"same as #35461 against master. so should be OK when backported.
"
668040604,35467,CI: activate github actions on 1.1.x (PR only),simonjayhawkins,closed,2020-07-29T18:13:16Z,2020-08-04T08:28:34Z,same as #35459 against master. so should be OK when backported.
659426633,35330,REF: remove special casing from Index.equals (always dispatch to subclass),simonjayhawkins,closed,2020-07-17T16:56:27Z,2020-08-04T08:55:57Z,
672604354,35535,Backport PR #35467 on branch 1.1.x (CI: activate github actions on 1.1.x (PR only)),meeseeksmachine,closed,2020-08-04T08:22:31Z,2020-08-04T09:00:50Z,Backport PR #35467: CI: activate github actions on 1.1.x (PR only)
672604711,35536,Backport PR #35468 on branch 1.1.x (CI: activate azure pipelines on 1.1.x),meeseeksmachine,closed,2020-08-04T08:23:08Z,2020-08-04T09:01:47Z,Backport PR #35468: CI: activate azure pipelines on 1.1.x
670792006,35502,CI: xfail numpy-dev,simonjayhawkins,closed,2020-08-01T11:39:06Z,2020-08-04T09:03:32Z,xref #35481
672631168,35537,Backport PR #35502 on branch 1.1.x (CI: xfail numpy-dev),meeseeksmachine,closed,2020-08-04T09:03:21Z,2020-08-04T09:53:42Z,Backport PR #35502: CI: xfail numpy-dev
641107897,34860,BUG: indexing regression with datetime index,0x0L,closed,2020-06-18T10:50:14Z,2020-06-24T16:00:50Z,"Sample code:
```python
import pandas as pd

idx = pd.date_range('2008', '2009')
idx.to_series()['2008']
```

in 0.25.3 we get
````
2008-01-01   2008-01-01
2008-01-02   2008-01-02
2008-01-03   2008-01-03
2008-01-04   2008-01-04
2008-01-05   2008-01-05
                ...    
2008-12-27   2008-12-27
2008-12-28   2008-12-28
2008-12-29   2008-12-29
2008-12-30   2008-12-30
2008-12-31   2008-12-31
Freq: D, Length: 366, dtype: datetime64[ns]
````

in >= 1.0.0 (up to 1.0.4 at least) we get
```
2008   2008-01-01
2008   2008-01-02
2008   2008-01-03
2008   2008-01-04
2008   2008-01-05
          ...    
2008   2008-12-27
2008   2008-12-28
2008   2008-12-29
2008   2008-12-30
2008   2008-12-31
Length: 366, dtype: datetime64[ns]
```

The index has dtype obj

Variantations like
```
idx.to_series().astype(str)['2008']
idx.to_series().loc['2008']
```
look unaffected
"
642489656,34917,BUG: indexing regression with datetime index,ketanarlulkar,closed,2020-06-21T03:58:14Z,2020-06-24T16:00:55Z,"The bug is already fixed on master. I have just added a unit test.

- [x] closes #34860
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
643986329,34956,"ENH: GH34946 Check type of names argument to `read_csv`, `read_table`…",MJafarMashhadi,closed,2020-06-23T16:37:24Z,2020-06-24T16:13:43Z,"
- [x] closes #34946 
- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

- ~~One test failed (TestTSPlot.test_ts_plot_with_tz) but it's not related to what I changed.~~
- ~~Does it need a whatsnew entry?~~"
642439193,34914,CLN: GH29547 format with f-strings,DanBasson,closed,2020-06-20T20:17:18Z,2020-06-24T17:28:36Z,"xref #29547 
replace .format() for f-strings in the following:

- pandas/tests/series/indexing/test_numeric.py
"
637371357,34723,QST:Need to stop converting my regular values to date formats,amansani,closed,2020-06-11T23:01:58Z,2020-06-24T17:55:06Z,"- [ ] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

**Note**: If you'd still like to submit a question, please read [this guide](
https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your question.

```python
# Your code here, if applicable

I have values in my tables as 5-7, 3-5, 5-6, these values are converting into date formats like 07-May, 05-Mar, 06-May. But those values are related to length, width and height, so i want to leave the values the way it is and to convert into date format only to date values. This is happening only when saving the data in 'to_csv' method and working properly in 'to_excel' method.

Also hyphen(—) values in csv are converted to â€” and working properly in excel.

Any help in any of the issues are much appreciated.

Thanks.
```
"
84005251,10250,ENH: Conditional HTML Formatting,TomAugspurger,closed,2015-06-02T12:41:28Z,2020-06-24T18:30:57Z,"closes https://github.com/pydata/pandas/issues/3190
closes #4315

Not close to being done, but I wanted to put this here before the meeting. Maybe someone will have a chance to check it out.

~~http://nbviewer.ipython.org/github/TomAugspurger/pandas/blob/638bd3e361633a4c446ee02534e07b8a9332258a/style.ipynb~~

~~https://github.com/TomAugspurger/pandas/blob/stylely/style.ipynb~~

[latest example notebook](http://nbviewer.ipython.org/gist/TomAugspurger/82a53e11993469cb29a1)
"
644264418,34963,REF: add Tick and BaseOffset to tslibs namespace,jbrockmendel,closed,2020-06-24T02:13:38Z,2020-06-24T22:25:21Z,Update imports appropriately.
158438877,13357,assert_almost_equal,jaysw,closed,2016-06-03T19:34:19Z,2020-06-24T22:25:21Z,"Hi, I'm wondering if the following behavior of pandas._testing.assert_almost_equal is expected:
#### Code Sample, a copy-pastable example if possible

```
from pandas import _testing
_testing.assert_almost_equal(0.000011, 0.000012, check_less_precise=True)
```
#### Expected Output

Expect no output / no AssertionError
#### Actual Output

```
AssertionError                            Traceback (most recent call last)
<ipython-input-199-ace78e82c603> in <module>()
      1 from pandas import _testing
----> 2 _testing.assert_almost_equal(0.000011, 0.000012, check_less_precise=True)

pandas/src/testing.pyx in pandas._testing.assert_almost_equal (pandas/src/testing.c:3887)()

pandas/src/testing.pyx in pandas._testing.assert_almost_equal (pandas/src/testing.c:3653)()

AssertionError: expected 0.00001 but got 0.00001, with decimal 3
```

Note that the numbers differ at decimal 6 and the output suggests they are different at position 3. 
#### output of `pd.show_versions()`
## INSTALLED VERSIONS

commit: None
python: 2.7.11.final.0
python-bits: 64
OS: Darwin
OS-release: 15.5.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.18.1
nose: None
pip: 8.0.2
setuptools: 21.2.1
Cython: None
numpy: 1.11.0
scipy: 0.17.1
statsmodels: None
xarray: None
IPython: 4.2.0
sphinx: None
patsy: None
dateutil: 2.5.3
pytz: 2016.4
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: 1.5.1
openpyxl: 2.3.3
xlrd: 0.9.4
xlwt: 1.0.0
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 1.0.12
pymysql: None
psycopg2: 2.6.1 (dt dec pq3 ext lo64)
jinja2: 2.8
boto: 2.39.0
pandas_datareader: None
"
57194889,9457,assert_almost_equal / equals should allow access to np.allclose,rockg,closed,2015-02-10T16:06:04Z,2020-06-24T22:25:21Z,"I am testing the equivalence of two large DataFrames (9084x367).  The two are the same up to 1x10-13 but when `np.array_equal` fails there is a much slower code path (comparing these two frames takes upwards of 20 seconds).  If I'm not mistaken, if the arrays aren't equivalent it does a more complicated version of `np.allclose`.  I think a good intermediate step would be to check for array equivalence and then as a the second step call `np.allclose`--or maybe just do this on the outset.  If that fails, which it will if there are any NaNs or if the tolerance is not met, then it will use the current logic.  Or we could use `np.isclose` to consider NaNs as equivalent.

https://github.com/pydata/pandas/blob/master/pandas/src/testing.pyx#L85
"
567641991,32108,Series with NAMED period index raise error on groupby index.month (pandas 1.0 specific),daxid,closed,2020-02-19T15:21:04Z,2020-06-24T22:35:59Z,"*edit from @TomAugspurger*: this is fixed on master, but the example below needs to be added as a unit test. The test can probably go in `groupby/test_groupby.py`.

#### Description

With the pandas 1.0.1 (full version with dependencies at the end), series with NAMED period index  raise error on groupby index.month

There is no error if the index is not named.

There was no error wit pandas 0.25.3

#### Code Sample
```python
import pandas as pd

index = pd.period_range(start='2018-01', periods=24, freq='M')
periodSerie = pd.Series(range(24),index=index)
periodSerie.index.name = 'Month'
periodSerie.groupby(periodSerie.index.month).sum()
```

#### Error
It seems to me that pandas tries to interpret the index name as if it were part of the index itself.

```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/.virtualenvs/dev/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_value(self, series, key)
   4410             try:
-> 4411                 return libindex.get_value_at(s, key)
   4412             except IndexError:

pandas/_libs/index.pyx in pandas._libs.index.get_value_at()

pandas/_libs/index.pyx in pandas._libs.index.get_value_at()

pandas/_libs/util.pxd in pandas._libs.util.get_value_at()

pandas/_libs/util.pxd in pandas._libs.util.validate_indexer()

TypeError: 'str' object cannot be interpreted as an integer

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
~/.virtualenvs/dev/lib/python3.8/site-packages/pandas/core/indexes/period.py in get_value(self, series, key)
    516         try:
--> 517             value = super().get_value(s, key)
    518         except (KeyError, IndexError):

~/.virtualenvs/dev/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_value(self, series, key)
   4418                 else:
-> 4419                     raise e1
   4420             except Exception:

~/.virtualenvs/dev/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_value(self, series, key)
   4404         try:
-> 4405             return self._engine.get_value(s, k, tz=getattr(series.dtype, ""tz"", None))
   4406         except KeyError as e1:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_value()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_value()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index_class_helper.pxi in pandas._libs.index.Int64Engine._check_type()

KeyError: 'Month'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
pandas/_libs/tslibs/parsing.pyx in pandas._libs.tslibs.parsing.parse_datetime_string_with_reso()

pandas/_libs/tslibs/parsing.pyx in pandas._libs.tslibs.parsing.dateutil_parse()

ValueError: Unknown datetime string format, unable to parse: Month

During handling of the above exception, another exception occurred:

DateParseError                            Traceback (most recent call last)
<ipython-input-6-a3e948d22d88> in <module>
      5 periodSerie = pd.Series(range(24),index=index)
      6 periodSerie.index.name = 'Month'
----> 7 periodSerie.groupby(periodSerie.index.month).sum()

~/.virtualenvs/dev/lib/python3.8/site-packages/pandas/core/series.py in groupby(self, by, axis, level, as_index, sort, group_keys, squeeze, observed)
   1676         axis = self._get_axis_number(axis)
   1677 
-> 1678         return groupby_generic.SeriesGroupBy(
   1679             obj=self,
   1680             keys=by,

~/.virtualenvs/dev/lib/python3.8/site-packages/pandas/core/groupby/groupby.py in __init__(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated)
    400             from pandas.core.groupby.grouper import get_grouper
    401 
--> 402             grouper, exclusions, obj = get_grouper(
    403                 obj,
    404                 keys,

~/.virtualenvs/dev/lib/python3.8/site-packages/pandas/core/groupby/grouper.py in get_grouper(obj, key, axis, level, sort, observed, mutated, validate)
    583     for i, (gpr, level) in enumerate(zip(keys, levels)):
    584 
--> 585         if is_in_obj(gpr):  # df.groupby(df['name'])
    586             in_axis, name = True, gpr.name
    587             exclusions.append(name)

~/.virtualenvs/dev/lib/python3.8/site-packages/pandas/core/groupby/grouper.py in is_in_obj(gpr)
    577             return False
    578         try:
--> 579             return gpr is obj[gpr.name]
    580         except (KeyError, IndexError):
    581             return False

~/.virtualenvs/dev/lib/python3.8/site-packages/pandas/core/series.py in __getitem__(self, key)
    869         key = com.apply_if_callable(key, self)
    870         try:
--> 871             result = self.index.get_value(self, key)
    872 
    873             if not is_scalar(result):

~/.virtualenvs/dev/lib/python3.8/site-packages/pandas/core/indexes/period.py in get_value(self, series, key)
    518         except (KeyError, IndexError):
    519             if isinstance(key, str):
--> 520                 asdt, parsed, reso = parse_time_string(key, self.freq)
    521                 grp = resolution.Resolution.get_freq_group(reso)
    522                 freqn = resolution.get_freq_group(self.freq)

pandas/_libs/tslibs/parsing.pyx in pandas._libs.tslibs.parsing.parse_time_string()

pandas/_libs/tslibs/parsing.pyx in pandas._libs.tslibs.parsing.parse_datetime_string_with_reso()

DateParseError: Unknown datetime string format, unable to parse: Month
```

#### Expected Output
With pandas 0.25.3, the following expected output is produced :

```python
Month
1     12
2     14
3     16
4     18
5     20
6     22
7     24
8     26
9     28
10    30
11    32
12    34
dtype: int64
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.18-1-MANJARO
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : fr_FR.UTF-8
LOCALE           : fr_FR.UTF-8

pandas           : 1.0.1
numpy            : 1.18.0
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 44.0.0
Cython           : 0.29.15
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
589652435,33105,Add test for #32108 (error with groupby on series with period index),daxid,closed,2020-03-28T20:22:26Z,2020-06-24T22:36:04Z,"- [ ] closes #32108
"
610805370,33921,Performance regression in reshape.Cut.time_qcut_timedelta,TomAugspurger,closed,2020-05-01T15:21:00Z,2020-06-24T22:37:00Z,"Setup

```python
import numpy as np
import pandas as pd

N = 10 ** 5
bins = 1000
timedelta_series = pd.Series(
    np.random.randint(N, size=N), dtype=""timedelta64[ns]""
)


%timeit pd.qcut(timedelta_series, bins)
```

```
# 1.0.2
57.7 ms ± 1.13 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

# master
139 ms ± 2.97 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```


https://pandas.pydata.org/speed/pandas/index.html#rolling.Methods.time_rolling?p-constructor=%27Series%27&p-window=10&p-dtype=%27float%27&p-method=%27sum%27&commits=265b8420121a66ed18329c7a90d5381aeda5454f-ad4ad22c6804e6925c4eb82f51b974c03c3036a8 points to https://github.com/pandas-dev/pandas/commit/d04b965f98fa53f22cc41f7e4d081d763d2856a1 (cc @mabelvj)"
603303464,33676,BUG: repr of Categorical does not distinguish int and str.,simonjayhawkins,closed,2020-04-20T14:57:13Z,2020-06-24T22:40:21Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
>>> pd.Categorical([1, ""2"", 3, 4])
[1, 2, 3, 4]
Categories (4, object): [1, 3, 4, 2]
```

#### Problem description

does not show the string elements quoted. This is inconsistent with numpy.

```python
>>> np.array([1, ""2"", 3, 4], dtype=""object"")
array([1, '2', 3, 4], dtype=object)
```
#### Expected Output

```python
>>> pd.Categorical([1, ""2"", 3, 4])
[1, '2', 3, 4]
Categories (4, object): [1, 3, 4, '2']
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
300394224,19917,Styler extremely slow,N2ITN,closed,2018-02-26T21:01:02Z,2020-06-24T23:16:53Z,"#### Code Sample

```python
def highlighter(col):
    '''
    Highlights rows in `highlight_map` keys where df[`highlight map` values]==1
    '''

    highlight_map = {
        'Service Type': 'ServiceType_Added',
        'Store #': 'Store_Added',
    }
    if col.name in highlight_map:
        return ['background-color: yellow' if final[highlight_map[col.name]][v] else '' for v in col.index.tolist()]
    else:
        return [''] * len(col)
    
styled = final.style.apply(highlighter)
styled.to_excel('highlights.xlsx', engine='openpyxl') 

```
#### Problem description

Here I have some conditional highlighting on a df with 18k rows. The issue is that despite preceding complex operations on the df (such as conditional merges and `df.apply` by row) taking ~300ms at the most, the `Styler.apply` part takes over two minutes. I realize this feature is in development but I am wondering if there is a way to make it faster or if this is a known issue. 



"
625214790,34395,BUG: OverflowError on to_json with numbers larger than sys.maxsize,kinghuang,closed,2020-05-26T21:36:29Z,2020-06-24T23:44:41Z,"- [X] I have checked that this issue has not already been reported.
- [X] I have confirmed this bug exists on the latest version of pandas.
- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import sys
from pandas.io.json import dumps

dumps(sys.maxsize)
dumps(sys.maxsize + 1)
```

#### Problem description

The Pandas JSON dumper doesn't seem to handle number values larger than `sys.maxsize` (a word). I have a dataframe that I'm trying to write to_json, but it's failing with `OverflowError: int too big to convert`. There are some numbers larger than `9223372036854775807` in it.

Passing a `default_handler` doesn't help. It doesn't get called for the error.

```python
>>> dumps(sys.maxsize)
'9223372036854775807'
>>> dumps(sys.maxsize + 1, default_handler=str)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
OverflowError: int too big to convert
```

#### Expected Output

Python's built-in json module handles large numbers without issues.

```python
>>> import json
>>> json.dumps(sys.maxsize)
'9223372036854775807'
>>> json.dumps(sys.maxsize+1)
'9223372036854775808'
```

I expect Pandas to be able to output large numbers to JSON. An option to use the built-in `json` module instead of `ujson` would be fine.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.76-linuxkit
machine          : x86_64
processor        :
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 46.4.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
640089205,34837,REF: implement _shared_docs to de-circularize dependencies,jbrockmendel,closed,2020-06-17T02:20:16Z,2020-06-24T23:51:38Z,"A side effect of this is that a bunch of pd.concat imports can be done at import-time instead of at runtime, will do this in a separate pass."
643327516,34937,BUG: clear cache on DataFrame._is_homogeneous_dtype,jbrockmendel,closed,2020-06-22T20:04:18Z,2020-06-24T23:52:23Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
619668826,34222,BUG: repr of Categorical does not distinguish int and str.,MarcoGorelli,closed,2020-05-17T10:42:59Z,2020-06-25T07:03:57Z,"- [x] closes #33676
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
643902685,34952,"PERF: Fixed cut regression, improve Categorical",TomAugspurger,closed,2020-06-23T14:43:50Z,2020-06-25T11:26:09Z,"This has two changes to address a performance regression in
cut / qcut.

~1. Avoid an unnecessary `set` conversion in cut.~ (reverted for now)
2. Aviod a costly conversion to object in the Categoriacl constructor
   for dtypes we don't have a hashtable for.

```python
In [2]: idx = pd.interval_range(0, 1, periods=10000)
In [3]: %timeit pd.Categorical(idx, idx)
```

```
# 1.0.4
10.4 ms ± 351 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

# master
256 ms ± 5.85 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

# HEAD
53.2 µs ± 1.26 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```

And for the qcut ASV

```
# 1.0.4
58.5 ms ± 3.13 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

# master
134 ms ± 9.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

# HEAD
53.6 ms ± 1.06 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

Closes https://github.com/pandas-dev/pandas/issues/33921"
645550440,34989,DOC: typo in release notes,simonjayhawkins,closed,2020-06-25T13:10:34Z,2020-06-25T14:41:35Z,
645014729,34978,PERF: optimize Block.getitem_block,jbrockmendel,closed,2020-06-24T22:20:54Z,2020-06-25T15:25:21Z,"Performance comparison is based on the the asv `groupby.Apply.time_scalar_function_single_col`, which is the one in which disabling the libreduction path has the biggest impact.

```
import pandas as pd
import numpy as np

N = 10 ** 4
labels = np.random.randint(0, 2000, size=N)
labels2 = np.random.randint(0, 3, size=N)
df = pd.DataFrame(
    {
        ""key"": labels,
        ""key2"": labels2,
        ""value1"": np.random.randn(N),
        ""value2"": [""foo"", ""bar"", ""baz"", ""qux""] * (N // 4),
    }
)

In [4]: %prun -s cumtime df.groupby(""key"").apply(lambda x: 1) 
```

master-but-with-fast_apply-disabled:
```
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.081    0.081 groupby.py:822(apply)
        1    0.000    0.000    0.081    0.081 groupby.py:871(_python_apply_general)
        1    0.006    0.006    0.080    0.080 ops.py:157(apply)
     1989    0.002    0.000    0.069    0.000 ops.py:933(__iter__)
     1988    0.003    0.000    0.066    0.000 ops.py:966(_chop)
     1988    0.004    0.000    0.059    0.000 managers.py:724(get_slice)
     1988    0.002    0.000    0.035    0.000 managers.py:730(<listcomp>)
     5964    0.009    0.000    0.033    0.000 blocks.py:283(getitem_block)
     5971    0.005    0.000    0.021    0.000 blocks.py:247(make_block_same_class)
     5974    0.007    0.000    0.013    0.000 blocks.py:115(__init__)
     1988    0.003    0.000    0.011    0.000 base.py:4064(__getitem__)
     1990    0.003    0.000    0.008    0.000 managers.py:120(__init__)
     1988    0.002    0.000    0.007    0.000 numeric.py:105(_shallow_copy)
     1992    0.002    0.000    0.007    0.000 blocks.py:2379(__init__)
     1988    0.002    0.000    0.005    0.000 base.py:485(_shallow_copy)
     1990    0.002    0.000    0.004    0.000 frame.py:432(__init__)
     1992    0.002    0.000    0.003    0.000 base.py:450(_simple_new)
```

PR-but-with-fast_apply-disabled
```
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.061    0.061 groupby.py:822(apply)
        1    0.000    0.000    0.061    0.061 groupby.py:871(_python_apply_general)
        1    0.005    0.005    0.058    0.058 ops.py:157(apply)
     1991    0.002    0.000    0.048    0.000 ops.py:933(__iter__)
     1990    0.003    0.000    0.046    0.000 ops.py:966(_chop)
     1990    0.004    0.000    0.039    0.000 managers.py:724(get_slice)
     1990    0.002    0.000    0.017    0.000 managers.py:730(<listcomp>)
     5970    0.009    0.000    0.015    0.000 blocks.py:297(getitem_block)
     1990    0.002    0.000    0.011    0.000 base.py:4064(__getitem__)
     1992    0.003    0.000    0.007    0.000 managers.py:120(__init__)
     1990    0.002    0.000    0.007    0.000 numeric.py:105(_shallow_copy)
     1990    0.002    0.000    0.005    0.000 base.py:485(_shallow_copy)
     1992    0.002    0.000    0.004    0.000 frame.py:432(__init__)
     5970    0.002    0.000    0.003    0.000 blocks.py:116(_simple_new)
     1994    0.002    0.000    0.003    0.000 base.py:450(_simple_new)
     1992    0.001    0.000    0.002    0.000 managers.py:126(<listcomp>)
    22438    0.002    0.000    0.002    0.000 {built-in method builtins.isinstance}
```

master
```
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.009    0.009 groupby.py:822(apply)
        1    0.000    0.000    0.009    0.009 groupby.py:871(_python_apply_general)
        1    0.000    0.000    0.008    0.008 ops.py:157(apply)
        1    0.000    0.000    0.005    0.005 ops.py:961(fast_apply)
        1    0.003    0.003    0.005    0.005 {pandas._libs.reduction.apply_frame_axis0}
     1994    0.001    0.000    0.002    0.000 base.py:4064(__getitem__)
        1    0.000    0.000    0.001    0.001 ops.py:135(_get_splitter)
        1    0.000    0.000    0.001    0.001 ops.py:268(group_info)
        1    0.000    0.000    0.001    0.001 generic.py:1206(_wrap_applied_output)
        1    0.000    0.000    0.001    0.001 ops.py:285(_get_compressed_codes)
```"
645049802,34982,REF: simplify advance/move/set_length in libreduction,jbrockmendel,closed,2020-06-24T23:55:37Z,2020-06-25T15:26:51Z,"Make it so that we set `buf.data` in fewer places.  I think from here its pretty straightforward to avoid setting buf.data at all.

cc @WillAyd."
643559711,34947,DOC: Demonstrate custom rolling indexer with Businessday,mroeschke,closed,2020-06-23T06:07:23Z,2020-06-25T16:48:29Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
419280679,25648,ERR: read_csv exposes an internal function when bad argument is specified,dsaxton,closed,2019-03-11T03:35:00Z,2020-06-25T17:38:09Z,"When a user tries to specify a non-existent argument in `read_csv` the error message pertains to what seems to be an internal function (`parser_f`).  This behavior might be a bit surprising / confusing to users, who would expect the error message to refer to the function actually being used.

```python
import pandas as pd
df = pd.read_csv(""some_file.csv"", bad_arg=True)
```

yields

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-d4606a60902f> in <module>()
----> 1 df = pd.read_csv(""some_file.csv"", bad_arg=True)

TypeError: parser_f() got an unexpected keyword argument 'bad_arg'
```"
644834530,34976,TYP: make the type annotations of read_csv & read_table discoverable,topper-123,closed,2020-06-24T18:42:37Z,2020-06-25T17:43:19Z,"- [x] closes #25648
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

In master the type of ``read_csv`` and ``read_table`` is 'Any'.  This PR makes the functions signatures discoverable by mypy. Also fixes the error message when wrong kwarg is passed."
645740697,34995,"TST: df.loc[:, 'col'] returning a view, but df.loc[df.index, 'col'] returning a copy",arw2019,closed,2020-06-25T17:42:16Z,2020-06-25T17:46:03Z,"- [x] closes #15631
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
645602106,34990,TYP: remove inappropriate use of cast,simonjayhawkins,closed,2020-06-25T14:18:12Z,2020-06-25T18:40:03Z,"the cast was silencing `error: Argument 2 to ""zip"" has incompatible type ""Union[Sequence[Union[str, int]], Mapping[Optional[Hashable], Union[str, int]]]""; expected 
""Iterable[Union[str, int]]""`

This error is because col_space is defined as `Union[str, int, Sequence[Union[str, int]], Mapping[Label, Union[str, int]]]` and the `elif isinstance(col_space, dict):` would not catch non-dict mappings, which would go though the else.

so the else is non-dict mappings and sequences and the cast to sequence is not safe.

```
>>> df = pd.DataFrame(np.random.random(size=(3, 3)), columns=[""a"", ""b"", ""c""])
>>>
>>> print(df.to_string(col_space={""b"": 20}))
          a                    b         c
0  0.382920             0.057121  0.862742
1  0.579339             0.391014  0.907678
2  0.340584             0.889387  0.922690
>>>
>>> from collections import UserDict
>>>
>>> print(df.to_string(col_space=UserDict(b=20)))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\simon\pandas\pandas\core\frame.py"", line 838, in to_string
    formatter = fmt.DataFrameFormatter(
  File ""C:\Users\simon\pandas\pandas\io\formats\format.py"", line 601, in __init__
    raise ValueError(
ValueError: Col_space length(1) should match DataFrame number of columns(3)
>>>
```
"
365681268,22933,API: add a setattr method to NDFrame to assist with piping,topper-123,closed,2018-10-01T23:24:46Z,2020-06-25T20:15:08Z,"I frequently come into situations where I have to break up piping, because piping doesn't work on attributes:

```python
>>> new_df = (df.groupby(...)
...             .pipe(...)
>>> new_df.index.name = 'index_name'
>>> new_df.columns = pd.CategoricalIndex(new_df.columns)
>>> new_df = new_df.pipe(...)  # and so on...
```

I think it would be cleaner if NDFrame had a ``setattr`` method that returns ``self``. Then the above would become:

```python
>>> new_df = (df.groupby(...)
...             .pipe(...)
...             .setattr('index.name', 'index_name')
...             .setattr('columns', lambda x: pd.CategoricalIndex(x.columns))
...             .pipe(...)  # and so on...
...             )
```

The first argument to ``.setattr`` is a string and can be dot-seperated, and the second parameter is called with ``self`` as its first argument if a callable.

I think this would make piping cleaner in many cases.

Opinions?"
109705560,11238,column-specific min_itemsize doesn't work with append_to_multiple ,BrenBarn,closed,2015-10-04T21:27:09Z,2020-06-25T22:56:46Z,"The `HDFStore.append_to_multiple` passes on its entire `min_itemsize` argument to every sub-append.  Because not all columns are in every append, it fails when it tries to set a min_itemsize for a certain column when appending to a table that doesn't use that column.  

Simple example:

```
>>> store.append_to_multiple({
...     'index': [""IX""],
...     'nums': [""Num"", ""BigNum"", ""RandNum""],
...     ""strs"": [""Str"", ""LongStr""]
... }, d.iloc[[0]], 'index', min_itemsize={""Str"": 10, ""LongStr"": 100})
Traceback (most recent call last):
  File ""<pyshell#52>"", line 5, in <module>
    }, d.iloc[[0]], 'index', min_itemsize={""Str"": 10, ""LongStr"": 100})
  File ""c:\users\brenbarn\documents\python\extensions\pandas\pandas\io\pytables.py"", line 1002, in append_to_multiple
    self.append(k, val, data_columns=dc, **kwargs)
  File ""c:\users\brenbarn\documents\python\extensions\pandas\pandas\io\pytables.py"", line 920, in append
    **kwargs)
  File ""c:\users\brenbarn\documents\python\extensions\pandas\pandas\io\pytables.py"", line 1265, in _write_to_group
    s.write(obj=value, append=append, complib=complib, **kwargs)
  File ""c:\users\brenbarn\documents\python\extensions\pandas\pandas\io\pytables.py"", line 3773, in write
    **kwargs)
  File ""c:\users\brenbarn\documents\python\extensions\pandas\pandas\io\pytables.py"", line 3460, in create_axes
    self.validate_min_itemsize(min_itemsize)
  File ""c:\users\brenbarn\documents\python\extensions\pandas\pandas\io\pytables.py"", line 3101, in validate_min_itemsize
    ""data_column"" % k)
ValueError: min_itemsize has the key [LongStr] which is not an axis or data_column
```

This apparently means that you can't use `min_itemsize` without manually creating and appending to each separate table beforehand, which is the kind of thing `append_to_multiple` is supposed to shield you from.

I think `append_to_multiple` should special-case `min_itemsize` and split it into separate dicts for each sub-append.  I don't know if there are other potential kwargs that need to be ""allocated"" separately to sub-appends, but if there are it might be good to split them too.
"
643357703,34939,HDFStore append_to_multiple with min_itemsize,fangchenli,closed,2020-06-22T21:00:12Z,2020-06-25T23:07:14Z,"- [x] closes #11238
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
589747032,33113,".astype(SparseDtype(float)) on empty dataframe leads to ""ValueError: No objects to concatenate""",choucavalier,closed,2020-03-29T08:45:14Z,2020-06-25T23:13:00Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
pd.DataFrame().astype(pd.SparseDtype(float))
```
#### Problem description

Converting an empty dataframe to a sparse representation leads to a `ValueError`.

```
ValueError: No objects to concatenate
```

#### Expected Output

I expected this to work even though the dataframe is empty.

#### Output of ``pd.show_versions()``

```
>>> pandas.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.0.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 45.0.0
Cython           : None
pytest           : 5.2.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : 2.7.0
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.2.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.6.0
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.46.0
```

</details>
"
589797451,33118,BUG: conversion of empty DataFrame to SparseDtype (#33113),choucavalier,closed,2020-03-29T13:54:40Z,2020-06-25T23:13:05Z,"- [x] closes #33113
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
52084453,9093,"[Feature Request] Add concise xlabel=, ylabel=, option in plot() method",xgdgsc,closed,2014-12-16T08:15:59Z,2020-06-26T00:37:09Z,"The current [plot()](http://pandas.pydata.org/pandas-docs/version/0.15.1/generated/pandas.DataFrame.plot.html#pandas.DataFrame.plot) method for Dataframe cannot set the x/y label in it. [Result](http://stackoverflow.com/questions/21487329/add-x-and-y-labels-to-a-pandas-plot?lq=1) is more lines of code for just doing this. 

As someone commented in that post:

> Is there a particular reason why x and y labels can't be added as arguments to pd.plot()? Given the additional concision of pd.plot() over plt.plot() it seems it would make sense to make it even more succinct instead of having to call ax.set_ylabel()
"
619680651,34223,ENH: Implement xlabel and ylabel options in Series.plot and DataFrame.plot,charlesdong1991,closed,2020-05-17T11:42:08Z,2020-06-26T00:37:20Z,"- [x] closes #9093
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
645927894,35000,ENH: Add command line support like pandashells,ghost,closed,2020-06-26T00:28:19Z,2020-06-26T01:15:57Z,"#### Is your feature request related to a problem?

I wish I could use pandas to directly process the data in the command line.

#### Describe the solution you'd like

[pandashells](https://github.com/robdmc/pandashells) does the job pretty well, but it is not under active development and doesn't use the full power of pandas.

#### Additional context

For example, pandashells can process the data in the terminal with:

```bash
$ p.df 'df[""c""] = 2 * df.b' 'df.groupby(by=""a"").c.count()' 'df.reset_index()'
```

More examples can be found [here](https://github.com/robdmc/pandashells#dataframe-manipulations).
"
642390798,34904,TST:add test for df replace GH34871,nwweber,closed,2020-06-20T14:53:44Z,2020-06-26T06:06:33Z,"- [x] improves upon #34871
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry <---- is that helpful for this specific regression test? happy to write this if you think this helps! :)

add tiny regression test for not failing on calling df.replace() with Period column

working on this revealed other quirks of df.replace(): it now does not raise an exception anymore, and the dataframe returned by replace() has the right values. However, the dtype of a Period column changes to Object, which is surprising. 

I've added the observations to GH34871"
612218491,33982,BUG: Handling columns from index_col in _is_potential_multi_index,mproszewska,closed,2020-05-04T22:51:16Z,2020-06-26T07:19:51Z,"- [x] closes #33476
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [] whatsnew entry
"
644743181,34973,Fix - BUG: AssertionError when slicing MultiIndex and setting value o…,luckydenis,closed,2020-06-24T16:18:41Z,2020-06-26T10:13:56Z,"- [x] closes #34870 
- [x] tests added / passed (1 / 1)


"
646131779,35007,CI: lint failure on master,simonjayhawkins,closed,2020-06-26T09:23:04Z,2020-06-26T12:40:24Z,
646152300,35008,CLN: remove redundant code in IndexOpsMixin.item,simonjayhawkins,closed,2020-06-26T10:00:30Z,2020-06-26T12:40:48Z,"the code removed in this PR was added (with tests) in #30175. 

in a latter PR, #31506, IndexOpsMixin.\_\_iter__  was modified, making the special case in IndexOpsMixin.item redundant.

cc @jbrockmendel "
558595255,31552,BUG: to_json not allowing uploads to S3 (#28375),YagoGG,closed,2020-02-01T20:48:08Z,2020-06-26T12:55:47Z,"- [x] closes #28375
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
646216248,35012,Fix issue #35010: Double requirement given for fsspec,SanthoshBala18,closed,2020-06-26T12:04:47Z,2020-06-26T13:12:43Z,"- [ ] closes #35010 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
646193967,35010,BUG: Double requirement given for fsspec in requirements-dev.txt,SanthoshBala18,closed,2020-06-26T11:20:10Z,2020-06-26T13:12:43Z,"Following error is found, while trying to set up local development environment:

ERROR: Double requirement given: fsspec>=0.7.4 (from -r requirements-dev.txt (line 69)) (already in fsspec>=0.5.1 (from -r requirements-dev.txt (line 25)), name='fsspec')

In requirements-dev.txt, two different versions of fsspec modules are specified.
fsspec>=0.5.1
fsspec>=0.7.4"
113483661,11436,DataFrame.duplicated detects duplicates when none exist,welchr,closed,2015-10-27T01:13:04Z,2020-06-26T13:29:17Z,"Hello, 

I'm running into what I think is a bug in DataFrame.duplicated where it detects duplicates, but the data frame does not actually have any duplicated rows. It seems to only happen with integer columns, and somewhat large datasets (>600,000 rows). 

I created a test data set to show the issue: 

```
df = pd.read_table(
  ""https://www.dropbox.com/s/vkw8bzxp290jitz/test.tab?raw=1"",
  dtype = {""chrom"" : ""int64"",""pos"" : ""int64""}
)
```

If you ask for duplicates, it will detect them: 

```
df.duplicated().any() # returns True
```

However, there are no duplicates: 

```
In [5]: from collections import Counter

In [6]: counter = Counter(zip(df.chrom,df.pos))

In [7]: counter.most_common(5)
Out[7]:
[((0, 13704091), 1),
 ((0, 201539008), 1),
 ((0, 8573433), 1),
 ((0, 127434927), 1),
 ((0, 247829766), 1)]
```

If I convert one of the columns to float, and then ask for duplicates, it is correct: 

```
df.loc[:,""pos""] = df.pos.astype(""float64"")
df.duplicated().any() # returns False
```

Strangely, converting the first column `chrom` to float or string does not seem to matter. 

I had a difficult time in constructing this data frame to illustrate the example. It seems to only occur: 
- With at least two columns
- One column (integer, string, float) that has a small number of unique values
- One column (must be integer) that has a wide range of values, but in many instances, the values are close to each other 
- The dataset must be somewhat large, but it's hard to pin down. Somewhere on the order of > 500,000 rows, it seems. 

From looking quickly at the `DataFrame.duplicated` code, it looks like it is using a hash table of some kind, and using integer columns differently than other columns - perhaps it's ending up with collisions? 

Apologies if I'm missing something obvious here. Please let me know if I can be of any help in investigating further. My pandas version information is below. 

```
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.10.final.0
python-bits: 64
OS: Darwin
OS-release: 14.5.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.17.0
nose: 1.3.7
pip: 7.1.2
setuptools: 18.4
Cython: 0.22.1
numpy: 1.10.1
scipy: 0.16.0
statsmodels: 0.6.1
IPython: 3.2.0
sphinx: 1.3.1
patsy: 0.3.0
dateutil: 2.4.2
pytz: 2015.6
blosc: None
bottleneck: None
tables: 3.2.0
numexpr: 2.4.3
matplotlib: 1.4.3
openpyxl: 2.0.2
xlrd: 0.9.4
xlwt: 1.0.0
xlsxwriter: 0.7.3
lxml: 3.4.4
bs4: 4.3.2
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 1.0.7
pymysql: None
psycopg2: None
```
"
642373916,34898,Refactor pandas.core.generic.NDFrame.to_replace() to appease static type analysis,oguzhanogreden,open,2020-06-20T13:14:52Z,2020-06-26T14:30:44Z,"Current implementation of `pandas.core.to_replace()` method won't allow type hints, as noted in #32542 . Contributor guidelines [suggest](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#style-guidelines) a refactor in this case."
645025637,34980,ENH: add re.fullmatch,rchurt,closed,2020-06-24T22:48:46Z,2020-06-26T15:39:57Z,"#### Is your feature request related to a problem?

I want to find only strings that match my full regex expression, not just the first part of it.

For example, if I am searching for the string `'panda'`, and my series contains the strings `'panda'` and `'pandas'`, I only want it to return `'panda'`. Currently, using `str.match('panda')` returns both.

#### Describe the solution you'd like

Add support for `re.fullmatch` (https://docs.python.org/3.4/library/re.html#re.fullmatch) so that it can be called with `str.fullmatch`.

"
558579817,31547,BUG: GH31142 Fix for combine.Series,jamesharrop,closed,2020-02-01T18:41:35Z,2020-06-26T15:43:06Z,"- [ ] closes #31142
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
646255400,35013,CLN: GH29547 format with f-strings,DanBasson,closed,2020-06-26T13:14:52Z,2020-06-26T15:57:22Z,"xref #29547,#34914
replace .format() for f-strings in the following:

pandas/tests/series/indexing/test_numeric.py"
627612175,34473,fix to_json for numbers larger than sys.maxsize,arw2019,closed,2020-05-30T00:40:35Z,2020-06-26T17:51:37Z,"- [x] closes #34395 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] passes performance benchmarks
- [x] whatsnew entry

Currently this patch causes a significant reduction for a number of the JSON performance benchmarks. A printout of my results is included below.

[json_benchmarks_results.txt](https://github.com/pandas-dev/pandas/files/4704831/json_benchmarks_results.txt)

I'd love to keep working on this if anybody has ideas for making the solution more efficient!"
643503008,34944,DOC: improve explanation of con argument DataFrame.to_sql,arw2019,closed,2020-06-23T03:32:56Z,2020-06-26T17:52:11Z,"- [x] closes #34824 
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
643434863,34943,BUG: exponential moving window covariance fails for multiIndexed DataFrame,arw2019,closed,2020-06-23T00:08:38Z,2020-06-26T17:52:56Z,"- [x] closes #34440 
- [x] tests added\tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
594607031,33306,ENH: Named aggregations with multiple columns,fpunny,closed,2020-04-05T19:28:13Z,2020-06-26T18:40:06Z,"- [ ] closes #29268
- [x] tests added
  - [ ] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
621633319,34272,ENH: specify missing keys in KeyError when passing list-like to .loc,arredond,closed,2020-05-20T10:04:10Z,2020-06-26T20:21:17Z,"I'm a big fan of the strong limitation when passing list-likes to `.loc[]`. However, when the a key is missing and the KeyError is raised, it'd be helpful to know _which_ key/s is/are missing.

I understand this could have performance implications since each key of the list-like object should be searched, but also think the UX would be improved.

At least, the first missing key could be reported, without hindering performance. I'm happy to provide a PR for this if provided with some guidelines (I suppose this affects both Series and DataFrames, as well as maybe some other parts of the code I'm not aware of).

```python
pd.__version__
# '1.0.3'

s = pd.Series({'a': 1, 'b': 2, 'c': 3})
s.loc['d'] # Raises KeyError but specifies 'd' is missing
s.loc[['a', 'b', 'c', 'd']] # Raises KeyError without specifying which key is missing
```
"
642951445,34932,ENH: add ignore_index argument to DataFrame.explode / Series.explode,erfannariman,closed,2020-06-22T10:29:00Z,2020-06-26T20:22:24Z,"When we use `DataFrame.explode`  right now, it will repeat the index for each element in the iterator. To keep it consistent with the methods like `DataFrame.sort_values`, `DataFrame.append` and `pd.concat`, we can add an argument `ignore_index`, which will reset the index.


```python
df = pd.DataFrame({'id':range(0,30,10),
                   'values':[list('abc'), list('def'), list('ghi')]})
print(df)

   id     values
0   0  [a, b, c]
1  10  [d, e, f]
2  20  [g, h, i]

print(df.explode('values'))
   id values
0   0      a
0   0      b
0   0      c
1  10      d
1  10      e
1  10      f
2  20      g
2  20      h
2  20      i
```
Expected behaviour with addition of the argument:
```python
df.explode('values', ignore_index=True)

   id values
0   0      a
1   0      b
2   0      c
3  10      d
4  10      e
5  10      f
6  20      g
7  20      h
8  20      i
```
"
642426646,34912,ENH: specificy missing labels in loc calls GH34272,timhunderwood,closed,2020-06-20T18:48:43Z,2020-06-26T20:23:40Z,"- [ ] closes #34272
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
645934137,35001,CLN: remove libreduction.Reducer,jbrockmendel,closed,2020-06-26T00:50:21Z,2020-06-26T20:31:18Z,
645033193,34981,ERR: Fix to_timedelta error message,dsaxton,closed,2020-06-24T23:08:03Z,2020-06-26T22:12:09Z,"Error messages were missing a case (unit of ""y"")"
646542605,35024,TST: rename fixtures named 'indices' to 'index',topper-123,closed,2020-06-26T22:28:27Z,2020-06-27T00:16:04Z,The singular name makes more sense than the plural.
635763172,34679,CLN: dont consolidate in indexing,jbrockmendel,closed,2020-06-09T21:14:10Z,2020-06-27T00:21:15Z,
545692458,30731,KeyError: 0 error on groupby apply,venatir,closed,2020-01-06T12:02:25Z,2020-06-27T03:54:55Z,"#### Code Sample, a copy-pastable example if possible

```python
def aggfunc(df):
  # operation that rely on df having the grouping column present.
  # Goes in again here without the grouping key and if my operation would rely on this, it would fail.
  return pd.Series([0.2,0.2], index=[12,13])

mydf=pd.DataFrame({""a"":[datetime.datetime.today(),datetime.datetime.today()],""b"":[1,2],""c"":[5,6]})

mydf.groupby(""a"").apply(aggfunc)

```

Looks like groupby.apply crashes when using datetime aggregation and returning non-datetime data.

The problem is here: `pandas.core.groupby.generic._recast_datetimelike_result`
`/pandas/core/groupby/generic.py:1857`

```
obj_cols = [
        idx for idx in range(len(result.columns)) if is_object_dtype(result.dtypes[idx])
    ]
```

E.g. My result columns are 12,13 and this is trying to iterate through the 0,1 which is the range.

The code in `/pandas/core/groupby/generic.py:1857` will fail with the above and an exception will be caught here: `pandas/core/groupby/groupby.py:726`. because of gh-20949 it is trying again without the grouping key. It should have worked from the beggining and this exception is not there to catch this kind of error.

The work around for this is to return a Series or DataFrame with the index reset, however this should not be a requirement.

The right way is to not use range in the `_recast_datetimelike_result` function.

Thank you


#### Output of ``pd.show_versions()``

<details>

>>> pd.show_versions()
```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.2.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.25.1
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.1.1
setuptools       : 42.0.2.post20191201
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : 2.6.9
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
```
</details>






"
586907200,32974,TST: Rename indices fixture,SaturnFromTitan,closed,2020-03-24T11:58:36Z,2020-06-27T05:53:06Z,"Following up on https://github.com/pandas-dev/pandas/pull/28865#discussion_r333052051 by @simonjayhawkins we should discuss if we want to rename the indices fixture.

A singular name seems more reasonable to its use cases. The most obvious contender is `index`.

Does any1 have objections?

**EDIT:** In [another comment](https://github.com/pandas-dev/pandas/pull/32963#discussion_r397373641) jreback suggested to use `index_fixture` instead to make things more obvious. I agree that it would be a better fit"
585796188,32911,ENH: Add ods writer,roberthdevries,closed,2020-03-22T19:42:30Z,2020-06-27T14:01:15Z,"- [x] closes #27222
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
- [x] add support for startrow and startcol parameters"
646178525,35009,BUG:to_numeric with leading zeros is not working consistently,abedhammoud,closed,2020-06-26T10:50:23Z,2020-06-27T15:44:08Z,"- [X ] I have checked that this issue has not already been reported.

- [X ] I have confirmed this bug exists on the latest version of pandas.

I using version: '1.0.5'

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python

s = pd.Series(['apple', '001', '002', -3])
pd.to_numeric(s, errors='ignore')

[out]:
0    apple
1      001
2      002
3       -3
dtype: object


but if I do the following: 
s2 = pd.Series([1, 1.0, '1', '1.0', ' 1', ' 1.0', '01', '002'])
pd.to_numeric(s2, errors='ignore')

[out]:
0    1.0
1    1.0
2    1.0
3    1.0
4    1.0
5    1.0
6    1.0
7    2.0
dtype: float64

Then I try:
s2 = pd.Series([1, 1.0, 'hello', '1.0', ' 1', ' 1.0', '01', '002'])
pd.to_numeric(s2, errors='ignore')

[out]:
0        1
1        1
2    hello
3      1.0
4        1
5      1.0
6       01
7      002
dtype: object

notice the 002 is not converted to 2 as before.
```

#### Problem description

It seems that the behavior of to_numeric in handling int with leading zeros is not consistent.

#### Expected Output

handling 00n should be consistent (converted to n)

INSTALLED VERSIONS
------------------

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1.post20200616
Cython           : 0.29.20
pytest           : 5.4.3
hypothesis       : 5.18.1
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.1
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : None
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.9
numba            : 0.49.1

</details>
"
646749537,35033,Bug 31783,rjfs,closed,2020-06-27T19:53:02Z,2020-06-27T20:01:56Z,"- [ ] closes #31783
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
313357505,20658,Confusing API for constructing Series from DataFrame,e-pet,open,2018-04-11T14:47:31Z,2020-06-28T03:17:57Z,"The current API for constructing a Series from a DataFrame containing just a single column currently is quite confusing in my opinion. The following illustrates my experience trying to transform a Dataframe containing just a single column into a Series object. This may or may not be exemplary for other users.

1. I know there's a `to_frame` (why not `to_dataframe`?) method for Series, so I guess there's also a `to_series` method for dataframes...? [There isn't.]
2. Well, then I'll just call the Series constructor using the DataFrame: `pd.Series(df)`. From a user perspective, the only sensible thing that this could possibly do is return the Series corresponding to the (single) column. I'm not exactly sure what it does, but it definitely doesn't do what I expected it to do and the result doesn't look very useful in any case:
```
pd.Series(pd.DataFrame({'a':[1,2,3]}))
Out[20]: 
0    (a,)
1    (a,)
2    (a,)
dtype: object
```
3. Some googling reveals that `squeeze` appears to be the almost-inverse of the `to_frame` method, except that it returns an `int` instead of a `Series` in the 1x1 case:
```
type(pd.DataFrame({'a':[1]}).squeeze())
Out[22]: numpy.int64
```
4. Further googling yields a solution that technically does exactly what I want but to my eyes somewhat obscures what's happening: `df.iloc[:, 0]`.

Specific questions:
1. Why is it `to_frame` and not `to_dataframe`? Is there any reason not to introduce the latter as an alias to the former?
2. Could there be a `to_series` method on dataframes that raises an exception if there's more than one column and otherwise works like `squeeze` except that it always returns a `Series`?
3. Why does the first example above using the `Series` constructor do what it does, and can't this be changed so that it does something useful for this use case?

(I'm not sure if this has been discussed before; I couldn't find any related issue. Sorry if this is a duplicate post!)

On a sidenote, this issue is quite representative of my experiences with pandas so far: it is almost always capable of doing what I'd like it to do, but it often takes me longer than expected to get things to work due to (for me, at least) unintuitive and partly inconsistent API design. That being said, it's still immensely useful and I use it on an almost daily basis. Thanks a lot to everyone involved!"
360126349,22705,"DataFrame.from_dict(OrderedDict(items),...) not a proper replacement for from_items(items,...)",PatrickDRusk,closed,2018-09-14T01:45:19Z,2020-06-28T03:28:53Z,"#### Code Sample
```python
import pandas
from collections import OrderedDict
def test_from_dict_replacing_from_items_with_duplicates():
    rows = [(1, (2,)), (1, (2,))]
    df1 = pandas.DataFrame.from_items(rows, columns=('a', ), orient='index')
    df2 = pandas.DataFrame.from_dict(OrderedDict(rows), columns=('a', ), orient='index')
    pandas.testing.assert_frame_equal(df1, df2)
```
#### Problem description

The deprecation warning in 0.23.4 indicates that `from_items(items,...)` should be changed to  `from_dict(OrderedDict(items)...`. But that doesn't work in cases where there would be duplicates in the index. The above is a test that would fail.

This would make it burdensome for developers to test all the scenarios in which they have used `from_items()` to change them to `from_dict`.

It might be worth noting that the `columns` parameter was not allowed on `from_dict()` prior to 0.23, and that would further complicate adapting code away from `from_items()`.

I recommend against this deprecation.

#### Expected Output

The above test would be expected to succeed.

#### Output of ``pd.show_versions()``
<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.0.final.0
python-bits: 64
OS: Darwin
OS-release: 17.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: 3.8.0
pip: 18.0
setuptools: 40.2.0
Cython: 0.28.5
numpy: 1.14.5
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.5.0
sphinx: None
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: 3.4.4
numexpr: 2.6.8
feather: None
matplotlib: 2.2.3
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0.1
sqlalchemy: 1.2.11
pymysql: 0.9.2
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
363705031,22832,[Bug] v0.23.4 is not able to handle tuples as keys of pd.Series like v0.22.0 | TypeError: 'values' is not ordered...,jolespin,closed,2018-09-25T18:12:57Z,2020-06-28T03:40:30Z,"#### Code Sample, a copy-pastable example if possible
I've posted this bug on [issue 15457](https://github.com/pandas-dev/pandas/issues/15457) and the issue was closed but it still occurs in the current version (0.23.4)


```python
# =====
# v0.23.4
# =====

>>> import sys; sys.version
'3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 12:04:33) \n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]'
>>> import pandas as pd; pd.__version__
'0.23.4'
>>> import numpy as np; from numpy import array; from collections import *
>>> param_index = OrderedDict([((('criterion', 'gini'), ('max_features', 'log2'), ('min_samples_leaf', 1)), array([  0,  40,  80, 120, 160, 200])), ((('criterion', 'gini'), ('max_features', 'log2'), ('min_samples_leaf', 2)), array([  1,  41,  81, 121, 161, 201])), ((('criterion', 'gini'), ('max_features', 'log2'), ('min_samples_leaf', 3)), array([  2,  42,  82, 122, 162, 202])), ((('criterion', 'gini'), ('max_features', 'log2'), ('min_samples_leaf', 5)), array([  3,  43,  83, 123, 163, 203])), ((('criterion', 'gini'), ('max_features', 'log2'), ('min_samples_leaf', 8)), array([  4,  44,  84, 124, 164, 204])), ((('criterion', 'gini'), ('max_features', 'sqrt'), ('min_samples_leaf', 1)), array([  5,  45,  85, 125, 165, 205])), ((('criterion', 'gini'), ('max_features', 'sqrt'), ('min_samples_leaf', 2)), array([  6,  46,  86, 126, 166, 206])), ((('criterion', 'gini'), ('max_features', 'sqrt'), ('min_samples_leaf', 3)), array([  7,  47,  87, 127, 167, 207])), ((('criterion', 'gini'), ('max_features', 'sqrt'), ('min_samples_leaf', 5)), array([  8,  48,  88, 128, 168, 208])), ((('criterion', 'gini'), ('max_features', 'sqrt'), ('min_samples_leaf', 8)), array([  9,  49,  89, 129, 169, 209])), ((('criterion', 'gini'), ('max_features', None), ('min_samples_leaf', 1)), array([ 10,  50,  90, 130, 170, 210])), ((('criterion', 'gini'), ('max_features', None), ('min_samples_leaf', 2)), array([ 11,  51,  91, 131, 171, 211])), ((('criterion', 'gini'), ('max_features', None), ('min_samples_leaf', 3)), array([ 12,  52,  92, 132, 172, 212])), ((('criterion', 'gini'), ('max_features', None), ('min_samples_leaf', 5)), array([ 13,  53,  93, 133, 173, 213])), ((('criterion', 'gini'), ('max_features', None), ('min_samples_leaf', 8)), array([ 14,  54,  94, 134, 174, 214])), ((('criterion', 'gini'), ('max_features', 0.382), ('min_samples_leaf', 1)), array([ 15,  55,  95, 135, 175, 215])), ((('criterion', 'gini'), ('max_features', 0.382), ('min_samples_leaf', 2)), array([ 16,  56,  96, 136, 176, 216])), ((('criterion', 'gini'), ('max_features', 0.382), ('min_samples_leaf', 3)), array([ 17,  57,  97, 137, 177, 217])), ((('criterion', 'gini'), ('max_features', 0.382), ('min_samples_leaf', 5)), array([ 18,  58,  98, 138, 178, 218])), ((('criterion', 'gini'), ('max_features', 0.382), ('min_samples_leaf', 8)), array([ 19,  59,  99, 139, 179, 219])), ((('criterion', 'entropy'), ('max_features', 'log2'), ('min_samples_leaf', 1)), array([ 20,  60, 100, 140, 180, 220])), ((('criterion', 'entropy'), ('max_features', 'log2'), ('min_samples_leaf', 2)), array([ 21,  61, 101, 141, 181, 221])), ((('criterion', 'entropy'), ('max_features', 'log2'), ('min_samples_leaf', 3)), array([ 22,  62, 102, 142, 182, 222])), ((('criterion', 'entropy'), ('max_features', 'log2'), ('min_samples_leaf', 5)), array([ 23,  63, 103, 143, 183, 223])), ((('criterion', 'entropy'), ('max_features', 'log2'), ('min_samples_leaf', 8)), array([ 24,  64, 104, 144, 184, 224])), ((('criterion', 'entropy'), ('max_features', 'sqrt'), ('min_samples_leaf', 1)), array([ 25,  65, 105, 145, 185, 225])), ((('criterion', 'entropy'), ('max_features', 'sqrt'), ('min_samples_leaf', 2)), array([ 26,  66, 106, 146, 186, 226])), ((('criterion', 'entropy'), ('max_features', 'sqrt'), ('min_samples_leaf', 3)), array([ 27,  67, 107, 147, 187, 227])), ((('criterion', 'entropy'), ('max_features', 'sqrt'), ('min_samples_leaf', 5)), array([ 28,  68, 108, 148, 188, 228])), ((('criterion', 'entropy'), ('max_features', 'sqrt'), ('min_samples_leaf', 8)), array([ 29,  69, 109, 149, 189, 229])), ((('criterion', 'entropy'), ('max_features', None), ('min_samples_leaf', 1)), array([ 30,  70, 110, 150, 190, 230])), ((('criterion', 'entropy'), ('max_features', None), ('min_samples_leaf', 2)), array([ 31,  71, 111, 151, 191, 231])), ((('criterion', 'entropy'), ('max_features', None), ('min_samples_leaf', 3)), array([ 32,  72, 112, 152, 192, 232])), ((('criterion', 'entropy'), ('max_features', None), ('min_samples_leaf', 5)), array([ 33,  73, 113, 153, 193, 233])), ((('criterion', 'entropy'), ('max_features', None), ('min_samples_leaf', 8)), array([ 34,  74, 114, 154, 194, 234])), ((('criterion', 'entropy'), ('max_features', 0.382), ('min_samples_leaf', 1)), array([ 35,  75, 115, 155, 195, 235])), ((('criterion', 'entropy'), ('max_features', 0.382), ('min_samples_leaf', 2)), array([ 36,  76, 116, 156, 196, 236])), ((('criterion', 'entropy'), ('max_features', 0.382), ('min_samples_leaf', 3)), array([ 37,  77, 117, 157, 197, 237])), ((('criterion', 'entropy'), ('max_features', 0.382), ('min_samples_leaf', 5)), array([ 38,  78, 118, 158, 198, 238])), ((('criterion', 'entropy'), ('max_features', 0.382), ('min_samples_leaf', 8)), array([ 39,  79, 119, 159, 199, 239]))])
>>> pd.Series(list(param_index.values()), index=param_index.keys())
Traceback (most recent call last):
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/core/algorithms.py"", line 635, in factorize
    order = uniques.argsort()
TypeError: '<' not supported between instances of 'NoneType' and 'str'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/core/sorting.py"", line 451, in safe_sort
    sorter = values.argsort()
TypeError: '<' not supported between instances of 'NoneType' and 'str'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/core/arrays/categorical.py"", line 345, in __init__
    codes, categories = factorize(values, sort=True)
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/util/_decorators.py"", line 178, in wrapper
    return func(*args, **kwargs)
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/core/algorithms.py"", line 643, in factorize
    assume_unique=True)
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/core/sorting.py"", line 455, in safe_sort
    ordered = sort_mixed(values)
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/core/sorting.py"", line 441, in sort_mixed
    nums = np.sort(values[~str_pos])
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/numpy/core/fromnumeric.py"", line 847, in sort
    a.sort(axis=axis, kind=kind, order=order)
TypeError: '<' not supported between instances of 'NoneType' and 'str'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/core/series.py"", line 183, in __init__
    index = _ensure_index(index)
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 4974, in _ensure_index
    return Index(index_like)
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 449, in __new__
    data, names=name or kwargs.get('names'))
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/core/indexes/multi.py"", line 1330, in from_tuples
    return MultiIndex.from_arrays(arrays, sortorder=sortorder, names=names)
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/core/indexes/multi.py"", line 1274, in from_arrays
    labels, levels = _factorize_from_iterables(arrays)
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/core/arrays/categorical.py"", line 2543, in _factorize_from_iterables
    return map(list, lzip(*[_factorize_from_iterable(it) for it in iterables]))
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/core/arrays/categorical.py"", line 2543, in <listcomp>
    return map(list, lzip(*[_factorize_from_iterable(it) for it in iterables]))
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/core/arrays/categorical.py"", line 2515, in _factorize_from_iterable
    cat = Categorical(values, ordered=True)
  File ""/Users/jespinoz/anaconda/envs/py3_testing/lib/python3.6/site-packages/pandas/core/arrays/categorical.py"", line 351, in __init__
    raise TypeError(""'values' is not ordered, please ""
TypeError: 'values' is not ordered, please explicitly specify the categories order by passing in a categories argument.
>>>

```


#### Problem description

I'm unable to use tuples as index keys in the newest version `0.23.4` like I was able to in `0.22.0`.  I have some software that is dependent on this functionality.  Is there any way to add this functionality back into the coming versions?

#### Expected Output
I was expecting to be able to create a `pd.Series` object from my `param_index` dictionary object.

```python
# =====
# v0.22.0
# =====
Python 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 17:14:51)
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import sys; sys.version
'3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 17:14:51) \n[GCC 7.2.0]'
>>> import pandas as pd; pd.__version__
'0.22.0'
>>> import numpy as np; from numpy import array; from collections import *
>>> param_index = OrderedDict([((('criterion', 'gini'), ('max_features', 'log2'), ('min_samples_leaf', 1)), array([  0,  40,  80, 120, 160, 200])), ((('criterion', 'gini'), ('max_features', 'log2'), ('min_samples_leaf', 2)), array([  1,  41,  81, 121, 161, 201])), ((('criterion', 'gini'), ('max_features', 'log2'), ('min_samples_leaf', 3)), array([  2,  42,  82, 122, 162, 202])), ((('criterion', 'gini'), ('max_features', 'log2'), ('min_samples_leaf', 5)), array([  3,  43,  83, 123, 163, 203])), ((('criterion', 'gini'), ('max_features', 'log2'), ('min_samples_leaf', 8)), array([  4,  44,  84, 124, 164, 204])), ((('criterion', 'gini'), ('max_features', 'sqrt'), ('min_samples_leaf', 1)), array([  5,  45,  85, 125, 165, 205])), ((('criterion', 'gini'), ('max_features', 'sqrt'), ('min_samples_leaf', 2)), array([  6,  46,  86, 126, 166, 206])), ((('criterion', 'gini'), ('max_features', 'sqrt'), ('min_samples_leaf', 3)), array([  7,  47,  87, 127, 167, 207])), ((('criterion', 'gini'), ('max_features', 'sqrt'), ('min_samples_leaf', 5)), array([  8,  48,  88, 128, 168, 208])), ((('criterion', 'gini'), ('max_features', 'sqrt'), ('min_samples_leaf', 8)), array([  9,  49,  89, 129, 169, 209])), ((('criterion', 'gini'), ('max_features', None), ('min_samples_leaf', 1)), array([ 10,  50,  90, 130, 170, 210])), ((('criterion', 'gini'), ('max_features', None), ('min_samples_leaf', 2)), array([ 11,  51,  91, 131, 171, 211])), ((('criterion', 'gini'), ('max_features', None), ('min_samples_leaf', 3)), array([ 12,  52,  92, 132, 172, 212])), ((('criterion', 'gini'), ('max_features', None), ('min_samples_leaf', 5)), array([ 13,  53,  93, 133, 173, 213])), ((('criterion', 'gini'), ('max_features', None), ('min_samples_leaf', 8)), array([ 14,  54,  94, 134, 174, 214])), ((('criterion', 'gini'), ('max_features', 0.382), ('min_samples_leaf', 1)), array([ 15,  55,  95, 135, 175, 215])), ((('criterion', 'gini'), ('max_features', 0.382), ('min_samples_leaf', 2)), array([ 16,  56,  96, 136, 176, 216])), ((('criterion', 'gini'), ('max_features', 0.382), ('min_samples_leaf', 3)), array([ 17,  57,  97, 137, 177, 217])), ((('criterion', 'gini'), ('max_features', 0.382), ('min_samples_leaf', 5)), array([ 18,  58,  98, 138, 178, 218])), ((('criterion', 'gini'), ('max_features', 0.382), ('min_samples_leaf', 8)), array([ 19,  59,  99, 139, 179, 219])), ((('criterion', 'entropy'), ('max_features', 'log2'), ('min_samples_leaf', 1)), array([ 20,  60, 100, 140, 180, 220])), ((('criterion', 'entropy'), ('max_features', 'log2'), ('min_samples_leaf', 2)), array([ 21,  61, 101, 141, 181, 221])), ((('criterion', 'entropy'), ('max_features', 'log2'), ('min_samples_leaf', 3)), array([ 22,  62, 102, 142, 182, 222])), ((('criterion', 'entropy'), ('max_features', 'log2'), ('min_samples_leaf', 5)), array([ 23,  63, 103, 143, 183, 223])), ((('criterion', 'entropy'), ('max_features', 'log2'), ('min_samples_leaf', 8)), array([ 24,  64, 104, 144, 184, 224])), ((('criterion', 'entropy'), ('max_features', 'sqrt'), ('min_samples_leaf', 1)), array([ 25,  65, 105, 145, 185, 225])), ((('criterion', 'entropy'), ('max_features', 'sqrt'), ('min_samples_leaf', 2)), array([ 26,  66, 106, 146, 186, 226])), ((('criterion', 'entropy'), ('max_features', 'sqrt'), ('min_samples_leaf', 3)), array([ 27,  67, 107, 147, 187, 227])), ((('criterion', 'entropy'), ('max_features', 'sqrt'), ('min_samples_leaf', 5)), array([ 28,  68, 108, 148, 188, 228])), ((('criterion', 'entropy'), ('max_features', 'sqrt'), ('min_samples_leaf', 8)), array([ 29,  69, 109, 149, 189, 229])), ((('criterion', 'entropy'), ('max_features', None), ('min_samples_leaf', 1)), array([ 30,  70, 110, 150, 190, 230])), ((('criterion', 'entropy'), ('max_features', None), ('min_samples_leaf', 2)), array([ 31,  71, 111, 151, 191, 231])), ((('criterion', 'entropy'), ('max_features', None), ('min_samples_leaf', 3)), array([ 32,  72, 112, 152, 192, 232])), ((('criterion', 'entropy'), ('max_features', None), ('min_samples_leaf', 5)), array([ 33,  73, 113, 153, 193, 233])), ((('criterion', 'entropy'), ('max_features', None), ('min_samples_leaf', 8)), array([ 34,  74, 114, 154, 194, 234])), ((('criterion', 'entropy'), ('max_features', 0.382), ('min_samples_leaf', 1)), array([ 35,  75, 115, 155, 195, 235])), ((('criterion', 'entropy'), ('max_features', 0.382), ('min_samples_leaf', 2)), array([ 36,  76, 116, 156, 196, 236])), ((('criterion', 'entropy'), ('max_features', 0.382), ('min_samples_leaf', 3)), array([ 37,  77, 117, 157, 197, 237])), ((('criterion', 'entropy'), ('max_features', 0.382), ('min_samples_leaf', 5)), array([ 38,  78, 118, 158, 198, 238])), ((('criterion', 'entropy'), ('max_features', 0.382), ('min_samples_leaf', 8)), array([ 39,  79, 119, 159, 199, 239]))])
>>> pd.Series(list(param_index.values()), index=param_index.keys())
((criterion, gini), (max_features, log2), (min_samples_leaf, 1))          [0, 40, 80, 120, 160, 200]
((criterion, gini), (max_features, log2), (min_samples_leaf, 2))          [1, 41, 81, 121, 161, 201]
((criterion, gini), (max_features, log2), (min_samples_leaf, 3))          [2, 42, 82, 122, 162, 202]
((criterion, gini), (max_features, log2), (min_samples_leaf, 5))          [3, 43, 83, 123, 163, 203]
((criterion, gini), (max_features, log2), (min_samples_leaf, 8))          [4, 44, 84, 124, 164, 204]
((criterion, gini), (max_features, sqrt), (min_samples_leaf, 1))          [5, 45, 85, 125, 165, 205]
((criterion, gini), (max_features, sqrt), (min_samples_leaf, 2))          [6, 46, 86, 126, 166, 206]
((criterion, gini), (max_features, sqrt), (min_samples_leaf, 3))          [7, 47, 87, 127, 167, 207]
((criterion, gini), (max_features, sqrt), (min_samples_leaf, 5))          [8, 48, 88, 128, 168, 208]
((criterion, gini), (max_features, sqrt), (min_samples_leaf, 8))          [9, 49, 89, 129, 169, 209]
((criterion, gini), (max_features, None), (min_samples_leaf, 1))         [10, 50, 90, 130, 170, 210]
((criterion, gini), (max_features, None), (min_samples_leaf, 2))         [11, 51, 91, 131, 171, 211]
((criterion, gini), (max_features, None), (min_samples_leaf, 3))         [12, 52, 92, 132, 172, 212]
((criterion, gini), (max_features, None), (min_samples_leaf, 5))         [13, 53, 93, 133, 173, 213]
((criterion, gini), (max_features, None), (min_samples_leaf, 8))         [14, 54, 94, 134, 174, 214]
((criterion, gini), (max_features, 0.382), (min_samples_leaf, 1))        [15, 55, 95, 135, 175, 215]
((criterion, gini), (max_features, 0.382), (min_samples_leaf, 2))        [16, 56, 96, 136, 176, 216]
((criterion, gini), (max_features, 0.382), (min_samples_leaf, 3))        [17, 57, 97, 137, 177, 217]
((criterion, gini), (max_features, 0.382), (min_samples_leaf, 5))        [18, 58, 98, 138, 178, 218]
((criterion, gini), (max_features, 0.382), (min_samples_leaf, 8))        [19, 59, 99, 139, 179, 219]
((criterion, entropy), (max_features, log2), (min_samples_leaf, 1))     [20, 60, 100, 140, 180, 220]
((criterion, entropy), (max_features, log2), (min_samples_leaf, 2))     [21, 61, 101, 141, 181, 221]
((criterion, entropy), (max_features, log2), (min_samples_leaf, 3))     [22, 62, 102, 142, 182, 222]
((criterion, entropy), (max_features, log2), (min_samples_leaf, 5))     [23, 63, 103, 143, 183, 223]
((criterion, entropy), (max_features, log2), (min_samples_leaf, 8))     [24, 64, 104, 144, 184, 224]
((criterion, entropy), (max_features, sqrt), (min_samples_leaf, 1))     [25, 65, 105, 145, 185, 225]
((criterion, entropy), (max_features, sqrt), (min_samples_leaf, 2))     [26, 66, 106, 146, 186, 226]
((criterion, entropy), (max_features, sqrt), (min_samples_leaf, 3))     [27, 67, 107, 147, 187, 227]
((criterion, entropy), (max_features, sqrt), (min_samples_leaf, 5))     [28, 68, 108, 148, 188, 228]
((criterion, entropy), (max_features, sqrt), (min_samples_leaf, 8))     [29, 69, 109, 149, 189, 229]
((criterion, entropy), (max_features, None), (min_samples_leaf, 1))     [30, 70, 110, 150, 190, 230]
((criterion, entropy), (max_features, None), (min_samples_leaf, 2))     [31, 71, 111, 151, 191, 231]
((criterion, entropy), (max_features, None), (min_samples_leaf, 3))     [32, 72, 112, 152, 192, 232]
((criterion, entropy), (max_features, None), (min_samples_leaf, 5))     [33, 73, 113, 153, 193, 233]
((criterion, entropy), (max_features, None), (min_samples_leaf, 8))     [34, 74, 114, 154, 194, 234]
((criterion, entropy), (max_features, 0.382), (min_samples_leaf, 1))    [35, 75, 115, 155, 195, 235]
((criterion, entropy), (max_features, 0.382), (min_samples_leaf, 2))    [36, 76, 116, 156, 196, 236]
((criterion, entropy), (max_features, 0.382), (min_samples_leaf, 3))    [37, 77, 117, 157, 197, 237]
((criterion, entropy), (max_features, 0.382), (min_samples_leaf, 5))    [38, 78, 118, 158, 198, 238]
((criterion, entropy), (max_features, 0.382), (min_samples_leaf, 8))    [39, 79, 119, 159, 199, 239]
dtype: object
```

#### Output of ``pd.show_versions()``

<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.4.final.0
python-bits: 64
OS: Darwin
OS-release: 15.6.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: 3.4.0
pip: 9.0.1
setuptools: 39.0.1
Cython: 0.27.3
numpy: 1.14.3
scipy: 1.0.0
pyarrow: 0.8.0
xarray: 0.10.3
IPython: 6.4.0
sphinx: 1.7.4
patsy: 0.5.0
dateutil: 2.6.1
pytz: 2018.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: 0.4.0
matplotlib: 2.2.2
openpyxl: 2.5.0
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: None
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
520918297,29541,pd.Dataframe.from_dict does not create MultiIndex from tuple keys,rubenvereecken,open,2019-11-11T12:01:57Z,2020-06-28T04:04:47Z,"#### Code Sample, a copy-pastable example if possible
I've got two samples. The first one is *successful*, ie it creates a MultiIndex as expected. The 
second one I would expect to work, but it doesn't.

##### Working example
```python
d = {('first', 'second'): {'count': 1}}
pd.DataFrame.from_dict(d, orient='index')
```
Results in:
```
              count
first second      1
```

##### Unexpected behaviour example
```python
d = {('first', 'second'): 1}
pd.DataFrame.from_dict(d, orient='index', columns=['count'])
```
Results in:

```
                 count
(first, second)      1
```
The first example creates a MultiIndex with two levels, the second one creates a single-level index with tuples, which is pretty unexpected. The second example is a more likely format to find data in if you only have a single column. There seems little motivation for the behaviour in the second example.

#### Output of ``pd.show_versions()``

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-53-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.25.2
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : None
setuptools       : 41.6.0.post20191030
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : 0.9.3
psycopg2         : 2.7.7 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.1.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.0
numexpr          : 2.7.0
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : None
tables           : 3.5.2
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
```
"
550167966,31041,BUG: something wrong at pd.Dataframe.from_dict(),zhenghao0379,open,2020-01-15T12:52:35Z,2020-06-28T04:05:35Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
A = {""a"":[1, 2, 3 ,4], ""b"":""b"", ""c"":""c""}
pd.DataFrame.from_dict(A,orient='index').T

B = { ""b"":""b"", ""c"":""c"", ""a"":[1, 2, 3 ,4],}
pd.DataFrame.from_dict(B,orient='index').T
```
#### Problem description
the output A ，B  are diffrent 
"
298133467,19756,DataFrame.agg: strange behavior,Airatvi,open,2018-02-19T00:22:34Z,2020-06-28T06:12:52Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
df=pd.DataFrame([1, 2], columns=['a'])
ser=pd.Series([1, 3], name='a')
print(df)
print(df.agg({'a': lambda x: x.cumsum()}))
print(df.agg({'a': lambda x: ser}))
#################################################output
   a
0  1
1  2
   a
0  1
1  3
   a
   0  1
0  1  3
1  1  3

```
#### Problem description

Functions (lambda ...) which return the same Series, give different results. It seems that  somehow second agg was called with axis=1.

#### Expected Output
```python
  a
0  1
1  2
   a
0  1
1  3
   a
0  1
1  3
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.2.final.0
python-bits: 64
OS: Linux
OS-release: 4.13.5-1-ARCH
machine: x86_64
processor:
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.22.0
pytest: 3.2.2
pip: 9.0.1
setuptools: 36.5.0
Cython: None
numpy: 1.14.0
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 5.3.0
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2018.3
blosc: None
bottleneck: None
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 2.1.2
openpyxl: 2.4.5
xlrd: 1.0.0
xlwt: None
xlsxwriter: None
lxml: 4.1.1
bs4: 4.6.0
html5lib: 0.999999999
sqlalchemy: 1.1.6
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
None

</details>
"
345452911,22102,Expanded `df.apply` returns empty `DataFrame` if `df` has no columns,madman-bob,open,2018-07-28T14:33:42Z,2020-06-28T06:20:52Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import pandas as pd
>>> df = pd.DataFrame([[],[],[]])
>>> df.apply(lambda x: [1, 2, 3], axis=1, result_type='expand')
Empty DataFrame
Columns: []
Index: [0, 1, 2]
```
#### Problem description

Applying with a function that doesn't depend on the columns of the input should always give the same output.

#### Expected Output

```
>>> df.apply(lambda x: [1, 2, 3], axis=1, result_type='expand')
   0  1  2
0  1  2  3
1  1  2  3
2  1  2  3
```

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.2.final.0
python-bits: 32
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 158 Stepping 9, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.23.3
pytest: None
pip: 18.0
setuptools: 28.8.0
Cython: None
numpy: 1.14.5
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
```

</details>

#### Workaround

For anyone else experiencing this issue, the following can be used as a temporary workaround:

```
>>> from pandas.core.apply import frame_apply
>>> frame_apply(df, lambda x: [1, 2, 3], axis=1, result_type='expand').apply_standard()
   0  1  2
0  1  2  3
1  1  2  3
2  1  2  3
```"
414782513,25454,Dataframe agg method passing as list,scottboston,open,2019-02-26T19:36:37Z,2020-06-28T06:26:33Z,"#### Code Sample, a copy-pastable example if possible

```python
import numpy as np
import pandas as pd
df = pd.DataFrame(np.arange(20).reshape(10,-1), columns=[*'AB'])
def f(x):
    print(type(x))

print(df.agg([f]))
```
Output:
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
<class 'int'>
      A     B
      f     f
0  None  None
1  None  None
2  None  None
3  None  None
4  None  None
5  None  None
6  None  None
7  None  None
8  None  None
9  None  None

#### Problem description

When passing a function as a list in dataframe.agg, the behavior is different than when you pass the function directly into dataframe agg.  

#### Expected Output

```pyton
def f(x):
    print(type(x))

print(df.agg(f))
```
Output:
<class 'pandas.core.series.Series'>
<class 'pandas.core.series.Series'>
A    None
B    None
dtype: object
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.6.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 26 Stepping 5, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.1
pytest: 3.7.3
pip: 10.0.1
setuptools: 40.0.0
Cython: 0.28.5
numpy: 1.15.4
scipy: 1.0.0
pyarrow: None
xarray: 0.11.3
IPython: 6.5.0
sphinx: 1.7.7
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 2.1.2
openpyxl: 2.5.5
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.8
lxml.etree: 4.2.4
bs4: 4.6.3
html5lib: 1.0.1
sqlalchemy: 1.2.11
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.7.0
gcsfs: None


</details>
"
561251755,31759,DEPR: disallow mutating operations in groupby.apply,jbrockmendel,closed,2020-02-06T20:28:43Z,2020-06-28T06:53:45Z,"A common issue [citation needed] comes up in groupby.apply when a user passes a function that modifies its argument in-place.  This is because we call the function on the first group first to see if we can use a fastpath, and then again when we apply it to all groups.

We could add a kwarg to specify if a function may mutate its data, e.g. `gb.apply(myfunc, may_mutate=True)` in which case we could avoid re-calling the function (probably means no fast-path).  With `may_mutate=False` we could use a context in which we set the data's flags to be immutable before calling anything on it.
"
646928144,35040,Doc: Updated aggregate docstring,gurukiran07,closed,2020-06-28T14:03:08Z,2020-06-28T14:04:00Z,"Link to current `Series.agg`: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.agg.html

Original question asked in gitter:
Does `pd.Series.agg` with `func` parameter set to `'median'` uses `np.nanmedian`(Not only `median` including `mean`, `mode`)?
```python
s= pd.Series([np.nan, np.nan,1,1,1])
s.agg('median') 
# 1
s.agg(np.median)
# 1
np.median(s.to_numpy())
# nan
np.nanmedian(s.to_numpy())
# 1
```
Whenever `NaN` is present does it fallback to using `np.nanmedian`?

---

Reply from @MarcoGorelli 
if you look at `pandas/core/base.py` you'll see
```
    np.median: ""median"",
    np.nanmedian: ""median"",
```
in `_cython_table`. So, both resolve to the same internal cython function.

---

IMO it's better to mention this in the docs under `Note:` section.

Under note section saying:
*Some NumPy functions such as `np.mean`, `np.nanmean`, `np.median` etc. resolve to their corresponding internal cython function.*

<details>

```
################################################################################
######################## Docstring (pandas.Series.agg)  ########################
################################################################################

Aggregate using one or more operations over the specified axis.

.. versionadded:: 0.20.0

Parameters
----------
func : function, str, list or dict
    Function to use for aggregating the data. If a function, must either
    work when passed a Series or when passed to Series.apply.

    Accepted combinations are:

    - function
    - string function name
    - list of functions and/or function names, e.g. ``[np.sum, 'mean']``
    - dict of axis labels -> functions, function names or list of such.
axis : {0 or 'index'}
        Parameter needed for compatibility with DataFrame.
*args
    Positional arguments to pass to `func`.
**kwargs
    Keyword arguments to pass to `func`.

Returns
-------
scalar, Series or DataFrame

    The return can be:

    * scalar : when Series.agg is called with single function
    * Series : when DataFrame.agg is called with a single function
    * DataFrame : when DataFrame.agg is called with several functions

    Return scalar, Series or DataFrame.

See Also
--------
Series.apply : Invoke function on a Series.
Series.transform : Transform function producing a Series with like indexes.

Notes
-----
`agg` is an alias for `aggregate`. Use the alias.
Some NumPy functions such as `np.mean`, `np.nanmean`, `np.median` etc.
resolve to their corresponding internal cython function.

A passed user-defined-function will be passed a Series for evaluation.

Examples
--------
>>> s = pd.Series([1, 2, 3, 4])
>>> s
0    1
1    2
2    3
3    4
dtype: int64

>>> s.agg('min')
1

>>> s.agg(['min', 'max'])
min   1
max   4
dtype: int64

################################################################################
################################## Validation ##################################
################################################################################

Docstring for ""pandas.Series.agg"" correct. :)
```
</details>

[Image to updated docs of `Series.agg`](https://i.stack.imgur.com/yx7N4.png)"
214127301,15685,Feature request: Add annual offset that is not anchored,rs2,open,2017-03-14T16:22:21Z,2020-06-28T20:04:12Z,"#### Code Sample, a copy-pastable example if possible

```python
pd.bdate_range(end='today', periods=2, freq='1A').freq.name
# Prints 'A-DEC'
pd.bdate_range(end='today', periods=2, freq='1A')[0].date()
# prints datetime.date(2015, 12, 31)
```
#### Problem description

`Pandas` divides offsets into `regular` and `anchored` (see [Offset aliases](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases)).

There are `regular` aliases for any frequency but annual. A typical use case - give me a `bdate_range` with dates spaced 1 year apart ending today. `12M` and `52W` are only proxies, but won't give the exact expected result.

#### Expected Output
```python
pd.bdate_range(end='2017-03-12', periods=2, freq='1A-actual-for-real')[0].date() # Saturday
# prints datetime.date(2016, 3, 11) # Friday

```


#### Output of ``pd.show_versions()``

<details>
pandas: 0.19.2
</details>
"
273203499,18235,Quarter.onOffset looks fishy,jbrockmendel,open,2017-11-12T05:10:14Z,2020-06-28T20:05:30Z,"```
qe = pd.offsets.QuarterEnd(startingMonth=2)
qs = pd.offsets.QuarterBegin(startingMonth=2)
bqe = pd.offsets.BQuarterEnd(startingMonth=2)
bqs = pd.offsets.BQuarterBegin(startingMonth=2)

feb1 = pd.Timestamp('2017-02-01')
feb28 = pd.Timestamp('2017-02-28')
apr30 = pd.Timestamp('2017-04-30')

>>> qs.onOffset(feb1)
True
>>> qe.onOffset(feb28)
True
>>> qe.onOffset(apr30)
False

>>> bqs.onOffset(feb1)
True
>>> bqe.onOffset(feb28)
True
>>> bqe.onOffset(apr30)
False
```

The QuarterStart behavior makes sense to me; the QuarterEnd does not.  If Feb1 is the start of a quarter, shouldn't the end of that same quarter be Apr30?"
553123519,31186,Adding and subtracting DateOffsets results in different behaviour,MarcoGorelli,open,2020-01-21T20:54:32Z,2020-06-28T20:26:05Z,"```
>>> import datetime as dt
>>> import pandas as pd

>>> dt.datetime(2019, 3, 6, 16, 0) + pd.offsets.BusinessHour()                                                                                                                                              
Timestamp('2019-03-07 09:00:00')

>>> dt.datetime(2019, 3, 7, 10, 0) - pd.offsets.BusinessHour()                                                                                                                                              
Timestamp('2019-03-06 17:00:00')
```
Would have expected the same answer from these two commands.

Originally noticed in #29901 "
588713202,33051,A bug of CustomBusinessDay,entron,closed,2020-03-26T20:39:05Z,2020-06-28T20:42:31Z,"#### Code Sample, a copy-pastable example if possible

```python
from pandas.tseries.holiday import USFederalHolidayCalendar
from pandas.tseries.offsets import CustomBusinessDay

US_BD = CustomBusinessDay(calendar=USFederalHolidayCalendar())


from pandas.tseries.holiday import AbstractHolidayCalendar, Holiday, nearest_workday, \
    USMartinLutherKingJr, USPresidentsDay, GoodFriday, USMemorialDay, \
    USLaborDay, USThanksgivingDay

class USTradingCalendar(AbstractHolidayCalendar):
    rules = [
        Holiday('NewYearsDay', month=1, day=1, observance=nearest_workday),
        USMartinLutherKingJr,
        USPresidentsDay,
        GoodFriday,
        USMemorialDay,
        Holiday('USIndependenceDay', month=7, day=4, observance=nearest_workday),
        USLaborDay,
        USThanksgivingDay,
        Holiday('Christmas', month=12, day=25, observance=nearest_workday)
    ]
    
US_TD = CustomBusinessDay(calendar=USTradingCalendar())

print(pd.to_datetime(""1982-12-30"") + US_BD)  
print(pd.to_datetime(""1982-12-30"") + US_TD)
```
#### Problem description

Both give the result of 1983-01-03. 

#### Expected Output

The correct answer should be 1982-12-31 as it is Friday and not a public holiday.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 17.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0.post20200309
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: 0.8.1
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
323590147,21080,What to do with future Ambiguity Error about overlapping names when sorting?,TomAugspurger,closed,2018-05-16T12:01:33Z,2020-06-28T22:58:21Z,"xref https://github.com/pandas-dev/pandas/pull/17361

```
In [24]: df = pd.DataFrame({""a"": [1, 2], ""b"": [3, 4]}, index=pd.Index([1, 2], name='a'))

In [25]: df.sort_values(['a', 'b'])
/Users/taugspurger/.virtualenvs/pandas-dev/bin/ipython:1: FutureWarning: 'a' is both an index level and a column label.
Defaulting to column, but this will raise an ambiguity error in a future version
  #!/Users/taugspurger/Envs/pandas-dev/bin/python3
Out[25]:
   a  b
a
1  1  3
2  2  4
```

What should the user do in this situation? Should we provide a keyword to disambiguate? A literal like `pd.ColumnLabel('a')` or `pd.IndexName('a')`? Or do we require that they rename an index or column? 
Right now, they're essentially stuck with the last one. If we want to discourage that, then I suppose that's OK. But it's somewhat common to end up with overlapping names, from e.g. a groupby.

cc @jmmease "
387344839,24093,Automatically keep categorical type on merge,Gerenuk,open,2018-12-04T15:36:45Z,2020-06-28T23:14:28Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
dd1=pd.DataFrame({""a"":pd.Series([""x""], dtype=""category"")})
dd2=pd.DataFrame({""a"":pd.Series([""x"", ""y""], dtype=""category"")})
dd3=dd1.merge(dd2)
dd3.dtypes  # categorical turns into object type
```
#### Problem description

It would be nice if when merging on categorical columns, their categorical nature would remain by an automatic union_categoricals. Currently only categoricals with identical values remain categorical and merge on non-identical categoricals becomes object type."
547419828,30846,Cannot `remove_unused_categories` for CategoricalIndex in a MultiIndex,fujiisoup,open,2020-01-09T11:26:55Z,2020-06-28T23:34:39Z,"I'm working to add a `CategoricalIndex` support in xarray (pydata/xarray#3647, pydata/xarray#3670), but have a trouble.

#### Code Sample, a copy-pastable example if possible

```python
In [39]: cat = pd.CategoricalDtype(categories=['foo', 'bar', 'baz']) 
    ...: i1 = pd.Series(['foo', 'bar'], dtype=cat) 
    ...: i2 = pd.Series(['bar', 'bar'], dtype=cat) 
    ...: df = pd.DataFrame({'i1': i1, 'i2': i2, 'values': [1, 2]}) 

In [40]: df1 = df.set_index('i1')                                               
In [42]: df1.index                                                              
Out[42]: CategoricalIndex(['foo', 'bar'], 
        categories=['foo', 'bar', 'baz'], ordered=False, name='i1', dtype='category')

# remove_unused_categories() is working
In [43]: df1.index.remove_unused_categories()                                   
Out[43]: CategoricalIndex(['foo', 'bar'], 
        categories=['foo', 'bar'], ordered=False, name='i1', dtype='category')

In [44]: df2 = df.set_index(['i1', 'i2'])                                       
In [45]: df2.index                                                              
Out[45]: 
MultiIndex([('foo', 'bar'),
            ('bar', 'bar')],
           names=['i1', 'i2'])

# remove_unused_categories() is NOT working
In [47]: df2.index.levels[0].remove_unused_categories()                         
Out[47]: CategoricalIndex(['foo', 'bar', 'baz'], 
        categories=['foo', 'bar', 'baz'], ordered=False, name='i1', dtype='category')  # <-- `baz` is not removed.
```
#### Problem description

When `CategoricalIndex` is a level variable of MultiIndex, `remove_unused_categories` does not work.

#### Expected Output
```python
Out[43]: CategoricalIndex(['foo', 'bar'], categories=['foo', 'bar'], ordered=False, name='i1', dtype='category')
```

#### Output of ``pd.show_versions()``

<details>
In [48]: pd.show_versions()                                                    

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-1066-oem
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.2.post20191203
Cython           : None
pytest           : 5.3.2
hypothesis       : 4.54.2
sphinx           : 2.3.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.10.2
pandas_datareader: None
bs4              : None
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : None
tables           : None
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

</details>
"
646931317,35041,"CI: Linux py37_np_dev Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead.",simonjayhawkins,closed,2020-06-28T14:17:48Z,2020-06-29T00:36:32Z,"https://github.com/numpy/numpy/commit/8ac7ac94ab868473e2276902be656946745c641f

https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=38318&view=logs&j=3a03f79d-0b41-5610-1aa4-b4a014d0bc70&t=4d05ed0e-1ed3-5bff-dd63-1e957f2766a9"
528235995,29837,groupby() drops categorical columns when aggregating with isna(),xujiboy,closed,2019-11-25T17:37:52Z,2020-06-29T07:26:52Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame({'A': [1, 1, 1, 1],
                   'B': [1, 2, 1, 2],
                   'numerical_col': [.1, .2, np.nan, .3],
                   'object_col': ['foo','bar','foo','fee'],
                   'categorical_col': ['foo','bar','foo','fee']
                  })

df = df.astype({'categorical_col':'category'})

df.groupby(['A','B']).agg(lambda df: df.isna().sum())

#		numerical_col	object_col
# A	B		
# 1	1	1.0                   0
#       2	0.0	              0

```
#### Problem description

The categorical column ""categorical_col"" is expected to survive the aggregation, however, it gets dropped.

#### Expected Output
``` python
#		numerical_col	object_col categorical_col
# A	B		
# 1	1	1.0                   0                 0
#       2	0.0	              0                 0
```
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.1.final.0
python-bits: 64
OS: Linux
OS-release: 3.10.0-693.11.6.el7.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: 4.3.1
pip: 19.3.1
setuptools: 40.6.3
Cython: None
numpy: 1.15.4
scipy: 1.1.0
pyarrow: 0.11.1
xarray: None
IPython: 7.1.1
sphinx: None
patsy: 0.5.1
dateutil: 2.7.5
pytz: 2018.7
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.2
openpyxl: 2.6.1
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.5
lxml: 4.3.0
bs4: None
html5lib: None
sqlalchemy: 1.2.13
pymysql: 0.9.3
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
646987566,35044,CI: troubleshoot,simonjayhawkins,closed,2020-06-28T19:14:08Z,2020-06-29T07:28:58Z,"- [ ] closes #35041 

I don't think the message raised by Numpy is appropriate since AFAICT we are getting the message when using operator.add and not operator.concat

maybe better to skip tests and discuss with Numpy"
63795241,9712,to_csv and bytes on Python 3.,jseabold,open,2015-03-23T18:21:21Z,2020-06-29T15:36:49Z,"Is this desired behavior and something I need to work around or a bug? Notice the byte type marker is written to disk so you can't round-trip the data. This works fine in Python 2 with unicode AFAICT.

```
In [1]: pd.__version__
Out[1]: '0.15.2-252-g0d35dd4'

In [2]: pd.DataFrame.from_dict({'a': ['a', 'b', 'c']}).a.str.encode(""utf-8"").to_csv(""tmp.csv"")

In [3]: !cat tmp.csv
0,b'a'
1,b'b'
2,b'c'
```
"
529534086,29896,BUG: incorrect fills when reindexing multi-indexed DataFrames,ChrisRobo,closed,2019-11-27T19:25:28Z,2020-06-29T17:38:56Z,"```

Assignment to multiple columns of a :class:`DataFrame` when some of the columns do not exist would previously assign the values to the last column. Now, new columns would be constructed with the right values. 

.. ipython:: python

   df = pd.DataFrame({'a': [0, 1, 2], 'b': [3, 4, 5]})
   df

*Previous behavior*:

.. code-block:: ipython

   In [3]: df[['a', 'c']] = 1
   In [4]: df
   Out[4]:
      a  b
   0  1  1
   1  1  1
   2  1  1

*New behavior*:

.. ipython:: python

   df[['a', 'c']] = 1
   df

```

#### Code Sample, a copy-pastable example if possible

```python
>>> 
>>> df = pd.DataFrame({
...     'a': [0, 0, 0, 0],
...     'b': [0, 2, 3, 4],
...     'c': ['A', 'B', 'C', 'D']
... }).set_index(['a', 'b'])
>>> 
>>> df
     c
a b   
0 0  A
  2  B
  3  C
  4  D
>>> df.index
MultiIndex([(0, 0),
            (0, 2),
            (0, 3),
            (0, 4)],
           names=['a', 'b'])
>>> mi_2 = pd.MultiIndex.from_product([[0], [-1, 0, 1, 3, 4, 5]])
>>> 
>>> df.reindex(mi_2, method='backfill')
      c
0 -1  A
   0  A
   1  D
   3  A
   4  A
   5  C
>>> 
>>> 
>>> df.reindex(mi_2, method='pad')
        c
0 -1  NaN
   0  NaN
   1    D
   3  NaN
   4    A
   5    C
>>> 
```
#### Problem description

As far as I understand, the ordering logic for reindexing here should be tuple ordering with the constituent tuples of the MultiIndex -- i.e. `df`'s index has tuples:
```
(0,  0),
(0,  2),
(0,  3),
(0,  4)
```
and `mi_2`'s tuples are:
```
(0, -1),
(0,  0),
(0,  1),
(0,  3),
(0,  4),
(0,  5)
```
where tuple ordering properties are, for tuples `x = (x_1, ..., x_n)`, `y = (y_1, ..., y_n)` that `x > y` iff there exists some i in 1, ..., n such that `x_i > y_i` and `x_j >= y_j` for all j in 1, ..., i-1.  Moreover, `x = y` <=> `x_i = y_i` for all i.

As such, the reindexing of the DataFrame with backfilling should:
- ""match"" (0, -1) and (0, 0) from `mi_2` both to (0, 0) in `df.index`
- match (0, 1) from `mi_2` to (0, 2) in `df.index`
- match (0, 3) from `mi_2` to (0, 3) in `df.index`
- match (0, 4) from `mi_2` to (0, 4) in `df.index`
- not match (0, 5) from `mi_2`

Similarly, the reindexing of the DataFrame with forward-filling, aka padding, should:
- not match (0, -1) from `mi_2`
- match (0, 0) and (0, 1) from `mi_2` to (0, 0) in `df.index`
- match (0, 3) from `mi_2` to (0, 3) in `df.index`
- match (0, 4) and (0, 5) from `mi_2` to (0, 5) in `df.index`

In summary, as far as I can tell, this is simply a bug which was introduced with the new implementation of the multi-indexing backend in 0.23, since I couldn't find anything in the docs about changing the semantics of reindexing.  I have a diff locally (will prepare a PR shortly if contributors here are in agreement that this warrants fixing) which addresses these and does not break any existing tests (and which also adds tests which pass on 0.22 but fail on versions >= 0.23), which also suggests to me that this is a bug.

#### Expected Output

This is with python2.7, numpy 1.16.5, and pandas 0.22.0 -- as far as I can tell, the issue was introduced in 0.23.0.
```python
>>> df.reindex(mi_2, method='backfill')
        c
0 -1    A
   0    A
   1    B
   3    C
   4    D
   5  NaN
>>> 
>>> df.reindex(mi_2, method='pad')
        c
0 -1  NaN
   0    A
   1    A
   3    C
   4    D
   5    D
```

#### Output of ``pd.show_versions()``

<details>
>>> pd.show_versions()
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1062.4.1.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.0
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>"
546982864,30815,like old issue #2936:  df.apply(foo) will run twice at the first row,kerwin6182828,closed,2020-01-08T16:55:32Z,2020-06-29T17:40:03Z,"```
- :meth:`DataFrame.apply` and :meth:`DataFrame.applymap` now evaluates first row/column only once (:issue:`31620`, :issue:`30815`, :issue:`33879`).

.. ipython:: python

    df = pd.DataFrame({'a': [1, 2], 'b': [3, 6]})

    def func(row):
        print(row)
        return row

*Previous behavior*:

.. code-block:: ipython

    In [4]: df.apply(func, axis=1)
    a    1
    b    3
    Name: 0, dtype: int64
    a    1
    b    3
    Name: 0, dtype: int64
    a    2
    b    6
    Name: 1, dtype: int64
    Out[4]:
       a  b
    0  1  3
    1  2  6

*New behavior*:

.. ipython:: python

    df.apply(func, axis=1)

```

#### some example....

```python
def foo(row):
    status = row.get(""status"")
    print(status)
    status = True if status==1 else False
    row[""status""] = status
    return row
    
df = pd.DataFrame([{""name"":""bob"", ""status"":1}])
df.apply(foo, axis=1)

>>>>>>>> output:
1
True
```
#### Problem description
today, I have a problem  that df.apply(foo) will run twice at the first row like old issue #2936.
I read a lot of explanations, but still can't find some help. As I wrote above, when df just have 1 row and then apply function will run twice at this row lead to print diffierent values.
 (It just modified my original value ""1"" to ""True"",  but it confused me a lot...)

I am wandering how to handle this issue, and is this a pandas bug?

thanks a lot~~


"
646759370,35034,PERF: avoid duplicate is_single_block check,jbrockmendel,closed,2020-06-27T20:39:31Z,2020-06-29T18:12:50Z,"Small bump, make up some of the ground lost in #34999.

```
In [2]: df = pd.DataFrame([1])                                                                                                                                                                                     

In [3]: %timeit df.values                                                                                                                                                                                          
3.65 µs ± 75.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)  # <-- PR
3.84 µs ± 139 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)  # <-- master
```
"
646589641,35026,"DOC: correction to ""size"" in the plotting.bootstrap_plot docstring",ericgrosz,closed,2020-06-27T02:20:57Z,2020-06-29T21:38:27Z,"The documentation for plotting.bootstrap_plot says that the size argument must be greater than or equal to the length of the series. Bootstrapping typically uses subsamples of a size less than or equal to the size of the data set, so I corrected the docs accordingly. Note that making size greater than the length of the series throws the following ValueError: Sample larger than population or is negative"
462341487,27135,read_json different behaviour when file is os path or file url,gtmaskall,closed,2019-06-29T20:55:19Z,2020-06-29T21:42:19Z,"#### Code Sample, a copy-pastable example if possible
```bash
pwd
/home/guy/tmp
echo -e '{""a"": 1, ""b"": 2}\n{""a"": 3, ""b"": 4}' > test.json
cat test.json 
{""a"": 1, ""b"": 2}
{""a"": 3, ""b"": 4}
```

```python
# Your code here
[ins] In [74]: ospath = '/home/guy/tmp/test.json'                                                                       

[ins] In [75]: fileurl = 'file://localhost/home/guy/tmp/test.json'                                                      

[nav] In [76]: import pandas as pd                                                                                      

[ins] In [77]: pd.read_json(ospath, lines=True)                                                                         
Out[77]: 
   a  b
0  1  2
1  3  4

[ins] In [78]: pd.read_json(fileurl, lines=True)                                                                        
Out[78]: 
   a  b
0  1  2
1  3  4

[ins] In [79]: reader = pd.read_json(ospath, lines=True, chunksize=1)                                                   

[ins] In [80]: for chunk in reader: 
          ...:     print(chunk) 
          ...:                                                                                                          
   a  b
0  1  2
   a  b
1  3  4

[ins] In [81]: reader = pd.read_json(fileurl, lines=True, chunksize=1)
```
#### Problem description
Create a very simple two-line JSON file. Specify the location of the file two ways - using an OS path, and using a file URL. Both allow read_json() to read the JSON when read in one go. If using chunksize to create a reader, only the OS path specifier works. Trying to use the file path specifier produces a TypeError: sequence item 0: expected str instance, bytes found

The read_json doc says:
path_or_buf : a valid JSON string or file-like, default: None
    The string could be a URL. Valid URL schemes include http, ftp, s3,
    gcs, and file. For file URLs, a host is expected. For instance, a local
    file could be ``file://localhost/path/to/table.json``

This leads to the expectation that a file URL should behave the same as a simple OS path under all use cases.

I think this is the root cause of the problem described under issue   #27022
```
# output when using fileurl:
TypeError                                 Traceback (most recent call last)
<ipython-input-82-605ab8a466fd> in <module>
----> 1 for chunk in reader:
      2     print(chunk)
      3 

~/anaconda3/envs/test_latest_pandas_json/lib/python3.7/site-packages/pandas/io/json/json.py in __next__(self)
    579         lines = list(islice(self.data, self.chunksize))
    580         if lines:
--> 581             lines_json = self._combine_lines(lines)
    582             obj = self._get_object_parser(lines_json)
    583 

~/anaconda3/envs/test_latest_pandas_json/lib/python3.7/site-packages/pandas/io/json/json.py in _combine_lines(self, lines)
    520         """"""
    521         lines = filter(None, map(lambda x: x.strip(), lines))
--> 522         return '[' + ','.join(lines) + ']'
    523 
    524     def read(self):

TypeError: sequence item 0: expected str instance, bytes found
```

[ins] In [83]: pd.__version__                                                                                           
Out[83]: '0.24.2'


#### Expected Output
Behaviour of read_json to be the same regardless of the type of file-like.
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
                                                                            

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Linux
OS-release: 5.0.0-20-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.24.2
pytest: None
pip: 19.1.1
setuptools: 41.0.1
Cython: None
numpy: 1.16.4
scipy: None
pyarrow: None
xarray: None
IPython: 7.5.0
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.1.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
639159574,34811,BUG: reading line-format JSON from file url #27135,fangchenli,closed,2020-06-15T21:05:52Z,2020-06-29T21:51:40Z,"- [x] closes #27135
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew note entry
"
647017173,35045,"CLN: assorted tslibs cleanups, annotations",jbrockmendel,closed,2020-06-28T22:16:43Z,2020-06-29T23:21:14Z,
645728897,34994,DOC: Add example of NonFixedVariableWindowIndexer usage,mroeschke,closed,2020-06-25T17:20:35Z,2020-06-30T05:12:50Z,"xref https://github.com/pandas-dev/pandas/pull/34947#issuecomment-649638742

"
647415001,35052,CLN: move categorical tests from test_aggregate to test_categorical,simonjayhawkins,closed,2020-06-29T14:16:27Z,2020-06-30T08:13:17Z,xref https://github.com/pandas-dev/pandas/pull/35039#discussion_r446948974
572026922,32289,CI Failing - Linux py37_np_dev - test_constructor_list_frames,simonjayhawkins,closed,2020-02-27T11:45:18Z,2020-06-30T11:54:57Z,https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=29531&view=logs&j=3a03f79d-0b41-5610-1aa4-b4a014d0bc70&t=4d05ed0e-1ed3-5bff-dd63-1e957f2766a9
645605505,34991,DOC/TST: DataFrame constructor with a list of DataFrames,TomAugspurger,closed,2020-06-25T14:22:35Z,2020-06-30T11:55:02Z,"Closes #32289 by removing the failing tests and asserting that we raise For NumPy>=1.19. See https://github.com/pandas-dev/pandas/issues/32289#issuecomment-649492024 for details, but the tldr is that DataFrames are now treated identically to 2D ndarrays. 

One thing I wasn't sure of: where to document this. IMO this is too minor of an edge case to warrant anything in the docstring / user guide."
485478948,28156,"HDFStore: unable to create index, no error message",adamjstewart,closed,2019-08-26T22:12:39Z,2020-06-30T13:01:24Z,"I was trying to follow the documentation at https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#indexing but ran into an unintuitive bug with HDFStore index creation. I thought I would report it in case someone else runs across this problem.

First, I create 2 dataframes and an HDFStore:
```python
>>> import pandas as pd
>>> import numpy as np
>>> df_1 = pd.DataFrame(np.random.randn(10, 2), columns=list('AB'))
>>> df_2 = pd.DataFrame(np.random.randn(10, 2), columns=list('AB'))
>>> st = pd.HDFStore('appends.h5', mode='w')
```
Now, when I append, if I do:
```python
>>> st.append('df', df_1, data_columns=['B'], index=False)
>>> st.append('df', df_2, data_columns=['B'], index=False)
```
I can successfully create an index:
```python
>>> st.create_table_index('df', columns=['B'], optlevel=9, kind='full')
>>> st.get_storer('df').table
/df/table (Table(20,)) ''
  description := {
  ""index"": Int64Col(shape=(), dflt=0, pos=0),
  ""values_block_0"": Float64Col(shape=(1,), dflt=0.0, pos=1),
  ""B"": Float64Col(shape=(), dflt=0.0, pos=2)}
  byteorder := 'little'
  chunkshape := (2730,)
  autoindex := True
  colindexes := {
    ""B"": Index(9, full, shuffle, zlib(1)).is_csi=True}
```
But if I instead leave out the `data_columns`:
```python
>>> st.append('df', df_1, index=False)
>>> st.append('df', df_2, index=False)
```
no index is created:
```python
>>> st.create_table_index('df', columns=['B'], optlevel=9, kind='full')
>>> st.get_storer('df').table
/df/table (Table(20,)) ''
  description := {
  ""index"": Int64Col(shape=(), dflt=0, pos=0),
  ""values_block_0"": Float64Col(shape=(2,), dflt=0.0, pos=1)}
  byteorder := 'little'
  chunkshape := (2730,)
```
This is unintuitive for 2 reasons:

1. Why does HDFStore need to know the indexable columns during `append` _and_ during `create_table_index`?
2. Why doesn't `create_table_index` raise an error message when it isn't able to create an index?

I think fixing either 1 or 2 would make things much more intuitive."
214974927,15716,to_json float precision bug,amirshavit,closed,2017-03-17T11:18:26Z,2020-06-30T13:16:17Z,"xref #15864 for more tests


#### Code Sample, a copy-pastable example if possible

```python
>>> pd.DataFrame([dict(a_float=0.99999999999999944)]).to_json(double_precision=14)
'{""a_float"":{""0"":1.0}}'
>>> pd.DataFrame([dict(a_float=0.99999999999999944)]).to_json(double_precision=15)
'{""a_float"":{""0"":0.1}}'
```
#### Problem description

...

#### Expected Output
```python
'{""a_float"":{""0"":1.0}}'
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.12.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-66-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: None.None

pandas: 0.19.2
nose: 1.3.7
pip: 9.0.1
setuptools: 34.3.2
Cython: 0.24.1
numpy: 1.12.0
scipy: 0.18.1
statsmodelsdouble_precision: 0.6.1
xarray: None
IPython: 5.1.0
sphinx: 1.4.6
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: 1.1.0
tables: 3.2.3.1
numexpr: 2.6.1
matplotlib: 1.5.3
openpyxl: 2.3.2
xlrd: 1.0.0
xlwt: 1.1.2
xlsxwriter: 0.9.3
lxml: 3.6.4
bs4: 4.5.1
html5lib: None
httplib2: 0.10.3
apiclient: 1.5.3
sqlalchemy: 1.1.2
pymysql: None
psycopg2: None
jinja2: 2.7.3
boto: 2.42.0
pandas_datareader: None
</details>
"
646799399,35036,PERF: put some Timetamp methods in _Timestamp,jbrockmendel,closed,2020-06-28T00:28:32Z,2020-06-30T13:49:40Z,"```
In [2]: ts = pd.Timestamp.now()                                                                                                                                                                                    

In [3]: %timeit ts.is_month_start                                                                                                                                                                                  
58.3 ns ± 1.5 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR
82.5 ns ± 2.16 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- master
```"
640041071,34835,Replaced np.bool refs; fix CI failure,WillAyd,closed,2020-06-16T23:48:06Z,2020-06-30T16:05:52Z,"I mostly just replace `np.bool` with `np.bool_` though there are some other cases where I just used bool. I don't know if it matters...so happy to conform one way or another

For now just seeing if this fixes CI

Closes https://github.com/pandas-dev/pandas/issues/34848."

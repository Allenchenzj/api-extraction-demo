id,number,title,user,state,created_at,updated_at,body
562233844,31835,1.0.1 on conda - doc change or pushing to main conda channel?,discdiver,closed,2020-02-09T20:26:53Z,2020-09-01T23:49:55Z,"To install 1.0.1 with conda I needed to use conda forge channel. 

The install instructions in the docs say to just use `conda install pandas`, but that installs 1.0.0. 

I'm not sure if the solution is to push 1.0.1 to the main conda channel (if you control that) or if it's worth updating the install instructions in the docs so they are in sync with how to install the latest pandas version."
672961466,35550,DOC: add additional numba example,raybellwaves,closed,2020-08-04T17:37:02Z,2020-09-01T23:52:09Z,"#### Location of the documentation
 
https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html#using-numba

#### Documentation problem

Provide a simpler copy-paste example.

#### Suggested fix for documentation

@TomAugspurger has a nice example of using numba to speed up a `rolling.apply()` operation here https://github.com/TomAugspurger/acon-2020-pandas/blob/master/Numba%20Acceleration.ipynb

Could this or a variant of this be added to the docs? Perhaps to show that you don't need to do `import numba` but can do `engine='numba'`
"
690480912,36053,CLN: _wrap_applied_output,rhshadrach,closed,2020-09-01T22:05:45Z,2020-09-01T23:56:26Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

2nd step toward #35412. In this, the order of the largest if-else is switched with the condition negated. We can then drop the else entirely, resulting in much of the function having one less level of nesting."
689733908,36020,REF: implement Block._replace_list,jbrockmendel,closed,2020-09-01T01:30:38Z,2020-09-02T00:55:52Z,So we can re-use BlockManager.apply for the block iteration.
689529826,36015,BUG: PeriodIndex.get_loc incorrectly raising ValueError instead of KeyError,jbrockmendel,closed,2020-08-31T21:51:19Z,2020-09-02T01:02:52Z,"- [x] closes #34240
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
689765283,36022,"TYP/CLN: cleanup `_openpyxl.py`, add type annotation #36021",fangchenli,closed,2020-09-01T02:58:11Z,2020-09-02T01:04:42Z,"- [x] closes #36021
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
689823732,36028,TYP: add type annotation to expr.py #36027,fangchenli,closed,2020-09-01T05:27:12Z,2020-09-02T01:05:48Z,"- [x] closes #36027
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
689810901,36025,TYP: add type annotation to `_xlwt.py` #36024,fangchenli,closed,2020-09-01T05:01:30Z,2020-09-02T01:06:26Z,"- [x] closes #36024
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

pandas\io\excel\_xlwt.py:52: error: Need type annotation for 'style_dict' (hint: ""style_dict: Dict[<type>, <type>] = ..."")  [var-annotated]
"
667026239,35436,BUG: pd.read_parquet with pyarrow fails when row number is 0 and contains Pandas extensions type,Holi0317,open,2020-07-28T12:16:53Z,2020-09-02T01:22:22Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
df = pd.DataFrame([], columns=[""s""]).astype({""s"": ""string""})
df.to_parquet(""out.parquet"")
df2 = pd.read_parquet(""out.parquet"") # <- Exception thrown here
```

```python-traceback
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
    df2 = pd.read_parquet(""out.parquet"")
  File ""/usr/local/lib/python3.8/site-packages/pandas/io/parquet.py"", line 312, in read_parquet
    return impl.read(path, columns=columns, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/pandas/io/parquet.py"", line 126, in read
    result = self.api.parquet.read_table(
  File ""pyarrow/array.pxi"", line 715, in pyarrow.lib._PandasConvertible.to_pandas
  File ""pyarrow/table.pxi"", line 1565, in pyarrow.lib.Table._to_pandas
  File ""/usr/local/lib/python3.8/site-packages/pyarrow/pandas_compat.py"", line 779, in table_to_blockmanager
    blocks = _table_to_blocks(options, table, categories, ext_columns_dtypes)
  File ""/usr/local/lib/python3.8/site-packages/pyarrow/pandas_compat.py"", line 1116, in _table_to_blocks
    return [_reconstruct_block(item, columns, extension_columns)
  File ""/usr/local/lib/python3.8/site-packages/pyarrow/pandas_compat.py"", line 1116, in <listcomp>
    return [_reconstruct_block(item, columns, extension_columns)
  File ""/usr/local/lib/python3.8/site-packages/pyarrow/pandas_compat.py"", line 738, in _reconstruct_block
    pd_ext_arr = pandas_dtype.__from_arrow__(arr)
  File ""/usr/local/lib/python3.8/site-packages/pandas/core/arrays/string_.py"", line 83, in __from_arrow__
    return StringArray._concat_same_type(results)
  File ""/usr/local/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py"", line 171, in _concat_same_type
    return cls(np.concatenate(to_concat))
  File ""<__array_function__ internals>"", line 5, in concatenate
ValueError: need at least one array to concatenate
```

#### Problem description

All of the code is produced with pyarrow. fastparquet is not affected by this bug as it does not support pandas extension type
(See: https://github.com/dask/fastparquet/issues/465).

This problem only occurs when the row number is 0 and contains pandas extension type (`Int64`, `string`, etc).
It does not happen when there is 1 row. Even if the row contains `pd.NA`. For example, following code works:

```python
import pandas as pd
df = pd.DataFrame([{""s"": pd.NA}], columns=[""s""]).astype({""s"": ""Int64""})
df.to_parquet(""out.parquet"")
pd.read_parquet(""out.parquet"")
```

The above code will restore the DataFrame correctly.

The problem does not occur when the column type is normal type. For example, following code works when the `dtype` is `int64`

```python
import pandas as pd
df = pd.DataFrame([], columns=[""s""]).astype({""s"": ""int""})
df.to_parquet(""out.parquet"")
pd.read_parquet(""out.parquet"")
```

When using the pyarrow library to read the parse the output parquet, `.read_table` would fail when converting to pandas DataFrame with similar errors from `pd.read_parquet`. This only occurs on stable. Correct result is produced on environment 1 and environment 2 (See bottom for environment descriptions)

```python
import pandas as pd
import pyarrow.parquet as pq
df = pd.DataFrame([], columns=[""s""]).astype({""s"": ""int""})
df.to_parquet(""out.parquet"")
table = pq.read_table(""out.parquet"")
table.to_pandas() # Exception here on stable. Correct result on env 1 and 2
```

```python-traceback
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
    table.to_pandas()
  File ""pyarrow/array.pxi"", line 715, in pyarrow.lib._PandasConvertible.to_pandas
  File ""pyarrow/table.pxi"", line 1565, in pyarrow.lib.Table._to_pandas
  File ""/usr/local/lib/python3.8/site-packages/pyarrow/pandas_compat.py"", line 779, in table_to_blockmanager
    blocks = _table_to_blocks(options, table, categories, ext_columns_dtypes)
  File ""/usr/local/lib/python3.8/site-packages/pyarrow/pandas_compat.py"", line 1116, in _table_to_blocks
    return [_reconstruct_block(item, columns, extension_columns)
  File ""/usr/local/lib/python3.8/site-packages/pyarrow/pandas_compat.py"", line 1116, in <listcomp>
    return [_reconstruct_block(item, columns, extension_columns)
  File ""/usr/local/lib/python3.8/site-packages/pyarrow/pandas_compat.py"", line 738, in _reconstruct_block
    pd_ext_arr = pandas_dtype.__from_arrow__(arr)
  File ""/usr/local/lib/python3.8/site-packages/pandas/core/arrays/integer.py"", line 113, in __from_arrow__
    return IntegerArray._concat_same_type(results)
  File ""/usr/local/lib/python3.8/site-packages/pandas/core/arrays/masked.py"", line 175, in _concat_same_type
    data = np.concatenate([x._data for x in to_concat])
  File ""<__array_function__ internals>"", line 5, in concatenate
ValueError: need at least one array to concatenate
```

However, reading the dataframe with `.read_pandas` would succeed with expected result in all environments.

```python
import pandas as pd
import pyarrow.parquet as pq
df = pd.DataFrame([], columns=[""s""]).astype({""s"": ""int""})
df.to_parquet(""out.parquet"")
table = pq.read_pandas(""out.parquet"")
df2 = table.to_pandas() # No exception here
df.equals(df2) # Retruns True
```

The produced DataFrame is as expected with correct column type. This could be used as a workround for now.

#### Expected Output

The parquet should be read successfuly.

#### Output of ``pd.show_versions()``

(The code is run under docker image `python:3.8-slim`)

##### Stable environment

All packages installed from pip directly

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.7.9-arch1-1
machine          : x86_64
processor        :
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.0
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>

##### Environment 1

Pandas and numpy compiled from source. pyarrow from their [nightly channel](https://arrow.apache.org/docs/python/install.html#installing-nightly-packages)

<details>

INSTALLED VERSIONS
------------------
commit           : 6302f7b98ad24adda2d5a98fef3956f04f28039d
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.7.9-arch1-1
Version          : #1 SMP PREEMPT Thu, 16 Jul 2020 19:34:49 +0000
machine          : x86_64
processor        :
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0rc0+8.g6302f7b
numpy            : 1.20.0.dev0+4690248
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.1.0.dev19
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>

##### Environment 2

Pandas and numpy installed from pip. pyarrow from their [nightly channel](https://arrow.apache.org/docs/python/install.html#installing-nightly-packages)

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.7.9-arch1-1
machine          : x86_64
processor        :
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.1.0.dev19
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>"
685897691,35899,REF: handle axis=None case inside DataFrame.any/all to simplify _reduce,jbrockmendel,closed,2020-08-25T23:27:39Z,2020-09-02T02:30:58Z,"Between this, #35881, and the PR coming after 35881, we'll be able to simplify _reduce quite a bit."
589524831,33095,Question/Suggestion : Window or Rolling does not have a `get_window` public method ,SylvainGuieu,closed,2020-03-28T07:16:43Z,2020-09-02T05:19:14Z,"#### Code Sample, a copy-pastable example if possible

```python
# somewhere
rolling = df.rolling(7, win_type=""bartlett"")

#somewhere else 
plot( rolling._get_window( win_type=rolling.win_type) ) 
```
#### Problem description

Sometime you receive a Rolling or Window from somewhere and want, e.g. plot its shape. 
Would be convenient to have a public method or property to get the array window weight instead of  private `._get_window` method. 
could not found one. "
680585059,35778,Added numba as an argument,erfannariman,closed,2020-08-17T23:30:02Z,2020-09-02T08:08:16Z,"- [x] closes #35550 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
689295641,36012,CI: Unpin MyPy,simonjayhawkins,closed,2020-08-31T15:29:23Z,2020-09-02T09:21:03Z,
690362532,36048,CI: pin setuptools on 1.1.x,simonjayhawkins,closed,2020-09-01T18:42:16Z,2020-09-02T09:23:22Z,"top answer on stack overflow is to pin https://stackoverflow.com/questions/63663362/django-python3-on-install-i-get-parent-module-setuptools-not-loaded

also see https://github.com/MacPython/pandas-wheels/pull/97#issuecomment-684938715

Note: 50.0.1 released a hour ago but still the same. https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=41325&view=logs&j=a67b4c4c-cd2e-5e3c-a361-de73ac9c05f9&t=9a6bfc0f-544f-57f9-291f-bf4b75b05642"
690638777,36058,Comma cleanup for #35925,JonathanShrek,closed,2020-09-02T01:29:17Z,2020-09-02T13:28:56Z,"- [x] pandas/tests/generic/test_finalize.py
- [x] pandas/tests/generic/test_to_xarray.py
- [x] pandas/tests/groupby/aggregate/test_numba.py
- [x] pandas/tests/groupby/test_apply.py
- [x] pandas/tests/groupby/test_categorical.py
- [x] pandas/tests/groupby/test_groupby.py
- [x] pandas/tests/groupby/test_groupby_dropna.py
- [x] pandas/tests/groupby/test_groupby_subclass.py
"
676717359,35667,DOC: document dropna kwarg of pd.factorize,FlorianWetschoreck,closed,2020-08-11T09:18:43Z,2020-09-02T15:00:50Z,"#### Location of the documentation
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.factorize.html

#### Documentation problem

The docs show the existence of a kwarg ""dropna"" which does not exist

#### Suggested fix for documentation

Delete the kwarg ""dropna""
"
603287425,33675,Inconsistent retrieval of DataFrame columns as Series.,geppi,closed,2020-04-20T14:35:50Z,2020-09-02T15:43:17Z,"1. Retrieve a DataFrame column by attribute as a Series.
2. Modify an element of this series.
3. As expected the modification has no effect on the original DataFrame.
4. But it changes the Series that is retrieved from a column by attribute or with the 'loc' method.
5. In contrast the Series retrieved from a column using the 'iloc' method is still the original.

This is at least inconsistent. However, I would also expect to see no change of the retrieved Series under point 4.

```python
import pandas as pd
pd.show_versions()
```

    
    INSTALLED VERSIONS
    ------------------
    commit           : None
    python           : 3.7.4.final.0
    python-bits      : 64
    OS               : Windows
    OS-release       : 10
    machine          : AMD64
    processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
    byteorder        : little
    LC_ALL           : None
    LANG             : None
    LOCALE           : None.None
    
    pandas           : 1.0.3
    numpy            : 1.18.1
    pytz             : 2019.3
    dateutil         : 2.8.1
    pip              : 20.0.2
    setuptools       : 40.8.0
    Cython           : None
    pytest           : None
    hypothesis       : None
    sphinx           : None
    blosc            : None
    feather          : None
    xlsxwriter       : 1.2.8
    lxml.etree       : 4.5.0
    html5lib         : None
    pymysql          : None
    psycopg2         : None
    jinja2           : 2.10.3
    IPython          : 7.11.1
    pandas_datareader: None
    bs4              : None
    bottleneck       : None
    fastparquet      : None
    gcsfs            : None
    lxml.etree       : 4.5.0
    matplotlib       : 3.1.3
    numexpr          : None
    odfpy            : None
    openpyxl         : 3.0.3
    pandas_gbq       : None
    pyarrow          : None
    pytables         : None
    pytest           : None
    pyxlsb           : None
    s3fs             : None
    scipy            : None
    sqlalchemy       : None
    tables           : None
    tabulate         : None
    xarray           : None
    xlrd             : None
    xlwt             : None
    xlsxwriter       : 1.2.8
    numba            : None
    


```python
df = pd.DataFrame([[1,2], [3,4]], index= ['a', 'b'], columns=['A', 'B'])
df
```




<div>

<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>A</th>
      <th>B</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>a</th>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>b</th>
      <td>3</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>




```python
s = df.A
s
```




    a    1
    b    3
    Name: A, dtype: int64




```python
s['c'] = 5
s
```




    a    1
    b    3
    c    5
    Name: A, dtype: int64




```python
df
```




<div>

<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>A</th>
      <th>B</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>a</th>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>b</th>
      <td>3</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.A
```




    a    1
    b    3
    c    5
    Name: A, dtype: int64




```python
df.loc[:,'A']
```




    a    1
    b    3
    c    5
    Name: A, dtype: int64




```python
df.iloc[:,0]
```




    a    1
    b    3
    Name: A, dtype: int64




```python
s['b'] = 2
```


```python
s
```




    a    1
    b    2
    c    5
    Name: A, dtype: int64




```python
df
```




<div>

<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>A</th>
      <th>B</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>a</th>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>b</th>
      <td>3</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.A
```




    a    1
    b    2
    c    5
    Name: A, dtype: int64




```python
df.loc[:,'A']
```




    a    1
    b    2
    c    5
    Name: A, dtype: int64




```python
df.iloc[:,0]
```




    a    1
    b    3
    Name: A, dtype: int64




```python

```

"
691010665,36067,TYP: update setup.cfg,simonjayhawkins,closed,2020-09-02T13:12:50Z,2020-09-02T15:47:31Z,fixes in #36017 merged between generating config in #36012 and merging #36012
691012530,36068,TYP: statically define attributes in plotting._matplotlib.core,simonjayhawkins,closed,2020-09-02T13:15:11Z,2020-09-02T15:49:20Z,"continuation of changes in #36016

pandas\plotting\_matplotlib\core.py:231: error: ""MPLPlot"" has no attribute ""style""  [attr-defined]
pandas\plotting\_matplotlib\core.py:232: error: ""MPLPlot"" has no attribute ""style""  [attr-defined]
pandas\plotting\_matplotlib\core.py:233: error: ""MPLPlot"" has no attribute ""style""  [attr-defined]
pandas\plotting\_matplotlib\core.py:235: error: ""MPLPlot"" has no attribute ""style""  [attr-defined]
pandas\plotting\_matplotlib\core.py:385: error: ""MPLPlot"" has no attribute ""label""; maybe ""ylabel"" or ""xlabel""?  [attr-defined]
pandas\plotting\_matplotlib\core.py:553: error: ""MPLPlot"" has no attribute ""mark_right""  [attr-defined]
pandas\plotting\_matplotlib\core.py:732: error: ""MPLPlot"" has no attribute ""style""  [attr-defined]
pandas\plotting\_matplotlib\core.py:733: error: ""MPLPlot"" has no attribute ""style""  [attr-defined]
pandas\plotting\_matplotlib\core.py:735: error: ""MPLPlot"" has no attribute ""style""  [attr-defined]
pandas\plotting\_matplotlib\core.py:738: error: ""MPLPlot"" has no attribute ""style""  [attr-defined]
pandas\plotting\_matplotlib\core.py:739: error: ""MPLPlot"" has no attribute ""style""  [attr-defined]
pandas\plotting\_matplotlib\core.py:741: error: ""MPLPlot"" has no attribute ""style""  [attr-defined]
pandas\plotting\_matplotlib\core.py:1008: error: ""ScatterPlot"" has no attribute ""label""  [attr-defined]
pandas\plotting\_matplotlib\core.py:1075: error: ""LinePlot"" has no attribute ""stacked""  [attr-defined]
pandas\plotting\_matplotlib\core.py:1180: error: ""LinePlot"" has no attribute ""stacked""  [attr-defined]
pandas\plotting\_matplotlib\core.py:1269: error: ""AreaPlot"" has no attribute ""stacked""  [attr-defined]
pandas\plotting\_matplotlib\core.py:1351: error: ""BarPlot"" has no attribute ""stacked""  [attr-defined]
pandas\plotting\_matplotlib\core.py:1427: error: ""BarPlot"" has no attribute ""stacked""  [attr-defined]

"
690510643,36057,CLN remove unnecessary trailing commas,tiagohonorato,closed,2020-09-01T23:19:03Z,2020-09-02T16:14:07Z,"#35925

- pandas/tests/arithmetic/test_interval.py
- pandas/tests/arithmetic/test_numeric.py
- pandas/tests/arrays/boolean/test_logical.py

I am not including ""pandas/io/sas/sas_xport.py"" and ""pandas/io/stata.py"" as there is a PR already open for those."
690641822,36059,CLN remove unnecessary trailing commas in groupby tests,xcz011,closed,2020-09-02T01:32:16Z,2020-09-02T16:20:00Z,"xref #35925
"
691099732,36071,Backport PR #35852 on branch 1.1.x (API: replace dropna=False option with na_sentinel=None in factorize),meeseeksmachine,closed,2020-09-02T15:05:47Z,2020-09-02T16:37:26Z,Backport PR #35852: API: replace dropna=False option with na_sentinel=None in factorize
690453768,36051,BUG: frame._item_cache not cleared when Series is altered,jbrockmendel,closed,2020-09-01T21:16:16Z,2020-09-02T16:55:54Z,"- [x] closes #33675
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
690306449,36045,"BUG: NDFrame.replace wrong exception type, wrong return when size==0",jbrockmendel,closed,2020-09-01T17:04:47Z,2020-09-02T16:57:21Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
687606909,35937,BUG: BlockSlider not clearing index._cache,jbrockmendel,closed,2020-08-27T23:33:20Z,2020-09-02T17:00:22Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
691147849,36072,Backport PR #36051 on branch 1.1.x (BUG: frame._item_cache not cleared when Series is altered),meeseeksmachine,closed,2020-09-02T16:02:19Z,2020-09-02T17:09:18Z,Backport PR #36051: BUG: frame._item_cache not cleared when Series is altered
690784238,36062,"QST: CSV: export, import and new lines",kuraga,closed,2020-09-02T07:18:58Z,2020-09-02T17:47:23Z,"- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

```python
df = pd.DataFrame([ ['a\rb'] ], columns=['A'])
df.to_csv('df.csv', index=False, sep=';', encoding='utf-8')
df_imported = pd.read_csv('df.csv', sep=';', encoding='utf-8')
(len(df), len(df_imported)) # => (1, 2)
```

On Linux, symbol `\r` is not escaped by `pd.to_csv`. But seems like `pd.read_csv` expects something else.

Is it bug or feature? How to avoid this?"
690394263,36049,CLN: rename private functions used across modules,jbrockmendel,closed,2020-09-01T19:35:37Z,2020-09-02T18:27:14Z,
688739736,35991,TYP: misc typing in core\indexes\base.py,simonjayhawkins,closed,2020-08-30T15:33:36Z,2020-09-02T18:49:29Z,
691192494,36073,CLN clearing unnecessary trailing commas,sarthakvk,closed,2020-09-02T16:51:04Z,2020-09-02T19:21:45Z,"This PR is related to issue #35925, black 19.10b0 and upgraded version both passes these changes."
683975844,35852,API: replace dropna=False option with na_sentinel=None in factorize,charlesdong1991,closed,2020-08-22T10:50:31Z,2020-09-02T19:41:06Z,"- [x] closes #35667
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
677814465,35690,DatetimeIndex.get_slice_bound(...) raises TypeErrors for unexpected YYYY-MM-DD/datetime.date/Timestamp combinations,RhysU,closed,2020-08-12T16:12:54Z,2020-09-02T21:36:38Z,"Below find a handful of reproducible observations in Pandas 1.0.5 re: `DatetimeIndex.get_slice_bound(...)` where I'm genuinely not sure what the correct behaviors should all be.  Some of these appear related to #34077 in that `slice_locs(...)` uses `get_slice_bound(...)` under the covers.

Observations inlined and expected behaviors discussed afterwards:
```
########################################
OBSERVATIONS 1 using a UTC DatetimeIndex
########################################

import datetime
import pandas as pd
import pandas.util.testing as put

# Generate a UTC-localized DatetimeIndex
df = put.makeTimeDataFrame()
df = df.tz_localize(""utc"")
index = df.index

# Show the generated DatetimeIndex, which should look like:
# DatetimeIndex(['2000-01-03 00:00:00+00:00', '2000-01-04 00:00:00+00:00',
#                ...
#                '2000-02-10 00:00:00+00:00', '2000-02-11 00:00:00+00:00'],
#               dtype='datetime64[ns, UTC]', freq='B')
print(index)

# (A) When the date is inside the DatetimeIndex, this call completes.
index.get_slice_bound(datetime.date(2000, 1, 7), kind=""ix"", side=""left"")

# (B) Notice date before start of index
# TypeError: searchsorted requires compatible dtype or scalar, not date
index.get_slice_bound(datetime.date(2000, 1, 1), kind=""ix"", side=""left"")

# (C) Notice date after end of index
# TypeError: searchsorted requires compatible dtype or scalar, not date
index.get_slice_bound(datetime.date(2020, 1, 1), kind=""ix"", side=""left"")


# (D) When the Timestamp is inside the DatetimeIndex, this call completes
index.get_slice_bound(pd.Timestamp(""2000-01-07""), kind=""ix"", side=""left"")

# (E) Notice Timestamp before start of index
# TypeError: Cannot compare tz-naive and tz-aware datetime-like objects
index.get_slice_bound(pd.Timestamp(""2000-01-01""), kind=""ix"", side=""left"")

# (F) Notice Timestamp after end of index
# TypeError: Cannot compare tz-naive and tz-aware datetime-like objects
index.get_slice_bound(pd.Timestamp(""2020-01-01""), kind=""ix"", side=""left"")
```
Discussion:
1. Above, I do not know if (A)-(C) should behave as (E)-(F) or not.
2. Above, I suspect (D) should raise as (E)-(F) do.
3. I can see arguments where datetime.date's lacking tzinfo could be inferred to be the DatetimeIndex.tzinfo.  Then (A)-(C) would not raise.
4. I can see arguments where Timestamps lacking tzinfo could be inferred to be the DatetimeIndex.tzinfo.  Then (D)-(F) would not raise.
5. I don't expect any data-dependence, meaning that the specific YYYY-MM-DD should not impact if something raises.
6. I have not tested datetime.datetime under these circumstances.
7. I believe some of these behaviors may have changed since the 0.2-series.

Again, observations inlined and expected behaviors discussed afterwards:
```
########################################################
OBSERVATIONS 2 using a DatetimeIndex with tzinfo is None
########################################################

import datetime
import pandas as pd
import pandas.util.testing as put

# Generate a non-localized DatetimeIndex
df = put.makeTimeDataFrame()
index = df.index
assert index.tzinfo is None

# Show the generated DatetimeIndex, which should look like:
# DatetimeIndex(['2000-01-03', '2000-01-04', '2000-01-05', '2000-01-06',
#                ...
#                '2000-02-10', '2000-02-11'],
#               dtype='datetime64[ns]', freq='B')
print(index)

# (G) When the date is inside the DatetimeIndex, this call completes.
index.get_slice_bound(datetime.date(2000, 1, 7), kind=""ix"", side=""left"")

# (H) Notice date before start of index
# TypeError: searchsorted requires compatible dtype or scalar, not date
index.get_slice_bound(datetime.date(2000, 1, 1), kind=""ix"", side=""left"")

# (I) Notice date after end of index
# TypeError: searchsorted requires compatible dtype or scalar, not date
index.get_slice_bound(datetime.date(2020, 1, 1), kind=""ix"", side=""left"")


# (J) When the Timestamp is inside the DatetimeIndex, this call completes
index.get_slice_bound(pd.Timestamp(""2000-01-07""), kind=""ix"", side=""left"")

# (K) Call completes for Timestamp before start of index
index.get_slice_bound(pd.Timestamp(""2000-01-01""), kind=""ix"", side=""left"")

# (L) Call completes for Timestamp after end of index
index.get_slice_bound(pd.Timestamp(""2020-01-01""), kind=""ix"", side=""left"")
```
Discussion:

1. Above, I expect (H)-(I) to behave as (G).
2. Above, (J)-(K) appear correct to me.
3. I don't expect any data-dependence, meaning that the specific YYYY-MM-DD should not impact if something raises.
4. I have not tested datetime.datetime under these circumstances.
5. I believe some of these behaviors may have changed since the 0.2-series.

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.10.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.67-ts1
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.utf8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.16.6
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.0.3
setuptools       : 40.8.0
Cython           : 0.29.20
pytest           : 5.3.1
hypothesis       : 3.57.0
sphinx           : 1.8.5
blosc            : 1.5.1
feather          : None
xlsxwriter       : 1.0.2
lxml.etree       : 4.3.4
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.2.1
fastparquet      : 0.3.3
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.0.3
numexpr          : 2.6.4
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.0
pytables         : None
pytest           : 5.3.1
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.1
sqlalchemy       : 1.2.1
tables           : 3.5.2
tabulate         : 0.8.3
xarray           : 0.10.0
xlrd             : 1.1.0
xlwt             : 1.3.0
xlsxwriter       : 1.0.2
numba            : 0.50.1
```"
614918823,34077,BUG: DatetimeIndex.slice_locs() doesn't hande date objects not in index,seth-p,closed,2020-05-08T18:51:54Z,2020-09-02T21:36:38Z,"I would expect [7] below to return the same result as [5] (just as [6] returns the same result as [4]).

```
In [1]: import pandas as pd, datetime as dt                                                                                                                                                                                    

In [2]: pd.__version__                                                                                                                                                                                                         
Out[2]: '1.0.3'

In [3]: a = pd.DatetimeIndex(['2010-01-01', '2010-01-03'])                                                                                                                                                                     

In [4]: a.slice_locs(dt.datetime(2010, 1, 1), dt.datetime(2010, 1, 3))                                                                                                                                                         
Out[4]: (0, 2)

In [5]: a.slice_locs(dt.datetime(2010, 1, 1), dt.datetime(2010, 1, 2))                                                                                                                                                         
Out[5]: (0, 1)

In [6]: a.slice_locs(dt.date(2010, 1, 1), dt.date(2010, 1, 3))                                                                                                                                                                 
Out[6]: (0, 2)

In [7]: a.slice_locs(dt.date(2010, 1, 1), dt.date(2010, 1, 2))                                                                                                                                                                 
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

TypeError: an integer is required

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
~/.conda/envs/build/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2645             try:
-> 2646                 return self._engine.get_loc(key)
   2647             except KeyError:

pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine._date_check_type()

KeyError: datetime.date(2010, 1, 2)

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

TypeError: an integer is required

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
~/.conda/envs/build/lib/python3.7/site-packages/pandas/core/indexes/datetimes.py in get_loc(self, key, method, tolerance)
    714         try:
--> 715             return Index.get_loc(self, key, method, tolerance)
    716         except (KeyError, ValueError, TypeError):

~/.conda/envs/build/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2647             except KeyError:
-> 2648                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2649         indexer = self.get_indexer([key], method=method, tolerance=tolerance)

pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine._date_check_type()

KeyError: datetime.date(2010, 1, 2)

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

KeyError: 1262390400000000000

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
~/.conda/envs/build/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2645             try:
-> 2646                 return self._engine.get_loc(key)
   2647             except KeyError:

pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

KeyError: Timestamp('2010-01-02 00:00:00')

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

KeyError: 1262390400000000000

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
~/.conda/envs/build/lib/python3.7/site-packages/pandas/core/indexes/datetimes.py in get_loc(self, key, method, tolerance)
    727                     stamp = stamp.tz_localize(self.tz)
--> 728                 return Index.get_loc(self, stamp, method, tolerance)
    729             except KeyError:

~/.conda/envs/build/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2647             except KeyError:
-> 2648                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2649         indexer = self.get_indexer([key], method=method, tolerance=tolerance)

pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

KeyError: Timestamp('2010-01-02 00:00:00')

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
~/.conda/envs/build/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_slice_bound(self, label, side, kind)
   4840         try:
-> 4841             slc = self.get_loc(label)
   4842         except KeyError as err:

~/.conda/envs/build/lib/python3.7/site-packages/pandas/core/indexes/datetimes.py in get_loc(self, key, method, tolerance)
    729             except KeyError:
--> 730                 raise KeyError(key)
    731             except ValueError as e:

KeyError: datetime.date(2010, 1, 2)

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-7-f3a1babc1d65> in <module>
----> 1 a.slice_locs(dt.date(2010, 1, 1), dt.date(2010, 1, 2))

~/.conda/envs/build/lib/python3.7/site-packages/pandas/core/indexes/base.py in slice_locs(self, start, end, step, kind)
   4929         end_slice = None
   4930         if end is not None:
-> 4931             end_slice = self.get_slice_bound(end, ""right"", kind)
   4932         if end_slice is None:
   4933             end_slice = len(self)

~/.conda/envs/build/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_slice_bound(self, label, side, kind)
   4842         except KeyError as err:
   4843             try:
-> 4844                 return self._searchsorted_monotonic(label, side)
   4845             except ValueError:
   4846                 # raise the original KeyError

~/.conda/envs/build/lib/python3.7/site-packages/pandas/core/indexes/base.py in _searchsorted_monotonic(self, label, side)
   4793     def _searchsorted_monotonic(self, label, side=""left""):
   4794         if self.is_monotonic_increasing:
-> 4795             return self.searchsorted(label, side=side)
   4796         elif self.is_monotonic_decreasing:
   4797             # np.searchsorted expects ascending sort order, have to reverse

~/.conda/envs/build/lib/python3.7/site-packages/pandas/core/indexes/datetimes.py in searchsorted(self, value, side, sorter)
    858         elif not isinstance(value, DatetimeArray):
    859             raise TypeError(
--> 860                 ""searchsorted requires compatible dtype or scalar, ""
    861                 f""not {type(value).__name__}""
    862             )

TypeError: searchsorted requires compatible dtype or scalar, not date
```"
675527643,35627,ENH: change to_sql default from Text to UnicodeText,gordthompson,open,2020-08-08T13:43:57Z,2020-09-02T23:05:32Z,"#### Is your feature request related to a problem?

Yes. Databases that differentiate between Unicode and SBCS columns (notably Microsoft SQL Server `nvarchar` vs. `varchar`) can fail simple round-trips when using `to_sql` to write columns with Unicode strings.

#### Describe the solution you'd like

Change the default column `dtype` from `Text` ...

https://github.com/pandas-dev/pandas/blob/a9cb64ad105f8f7051f9d77b66e1bfe7c09d386f/pandas/io/sql.py#L1080

... to `UnicodeText`.

#### API breaking implications

None that I can see.

#### Describe alternatives you've considered

An alternative would be to modify the behaviour of `Text` in SQLAlchemy's mssql dialect, but SQLA already has `UnicodeText` and it solves the problem.

#### Additional context

```python
import pandas as pd
import sqlalchemy as sa

engine = sa.create_engine(""mssql+pyodbc://@mssqlLocal"")

df = pd.DataFrame(
    [(""English"", ""CHEERS!""), (""Greek"", ""ΟΠΑ!"")], columns=[""language"", ""toast""]
)
df.to_sql(""pd_test"", engine, index=False, if_exists=""replace"")
print(pd.read_sql_query(""SELECT * FROM pd_test"", engine))
""""""console output:
  language    toast
0  English  CHEERS!
1    Greek     ???!
""""""
```
"
553760015,31219,ENH: maybe_dispatch_to_dunder_op handle min/max,jbrockmendel,closed,2020-01-22T19:56:19Z,2020-09-02T23:44:08Z,"I'm running up against a case where I need np.maximum.reduce(period_array) to work; itd be nice if that could be handled by maybe_dispatch to call PeriodArray.max

cc @jorisvandenbossche @TomAugspurger "
674737005,35595,BUG: The query method does not support index or column named `datetime`,esse-byte,open,2020-08-07T03:53:17Z,2020-09-02T23:47:04Z,"```python
In [1]: import pandas as pd
In [2]: index = pd.MultiIndex.from_product([['T0', 'T1'], ['A', 'B']], names=['datetime', 'count'])
In [3]: frame = pd.DataFrame({'isnull': range(len(index))}, index=index)
In [4]: frame.query('count == ""A""') # works
Out[4]: 
                isnull
datetime count        
T0       A           0
T1       A           2

In [5]: frame.query('isnull < 2') # works
Out[5]: 
                isnull
datetime count        
T0       A           0
         B           1

In [6]: frame.query('datetime == ""T0""') # failed caused by KeyError
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
D:\home\tools\Anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
   2645             try:
-> 2646                 return self._engine.get_loc(key)
   2647             except KeyError:

...

D:\home\tools\Anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
   2646                 return self._engine.get_loc(key)
   2647             except KeyError:
-> 2648                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2649         indexer = self.get_indexer([key], method=method, tolerance=tolerance)
   2650         if indexer.ndim > 1 or indexer.size > 1:

pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()
pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()
pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()
pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()
KeyError: False

In [7]: index2 = pd.MultiIndex.from_product([pd.date_range('20200101', '20200202'), ['A', 'B']], names=['datetime', 'count'])
In [8]: frame2 = pd.DataFrame({'isnull': range(len(index2))}, index=index2)
In [9]: frame2.query('datetime == ""2020-01-01""') # failed caused by TypeError
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
D:\home\tools\Anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
   2645             try:
-> 2646                 return self._engine.get_loc(key)
   2647             except KeyError:

...

D:\home\tools\Anaconda\lib\site-packages\pandas\core\indexes\datetimes.py in get_loc(self, key, method, tolerance)
    721 
    722             try:
--> 723                 stamp = Timestamp(key)
    724                 if stamp.tzinfo is not None and self.tz is not None:
    725                     stamp = stamp.tz_convert(self.tz)

pandas\_libs\tslibs\timestamps.pyx in pandas._libs.tslibs.timestamps.Timestamp.__new__()
pandas\_libs\tslibs\conversion.pyx in pandas._libs.tslibs.conversion.convert_to_tsobject()
TypeError: Cannot convert input [False] of type <class 'numpy.bool_'> to Timestamp

```

#### Problem description

The `query` method failed when the query expression contains `datetime`.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : Chinese (Simplified)_China.936

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : None
tables           : 3.6.1
tabulate         : 0.8.3
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.50.1

</details>
"
655339567,35247,PERF: DataFrame.query slow when it contains PeriodIndex/Array,semio,open,2020-07-12T06:14:10Z,2020-09-02T23:57:32Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.


#### Code Sample, a copy-pastable example

```python
In [1]: import numpy as np

In [2]: import pandas as pd

In [23]: country = list(range(200))

In [24]: year = list(range(1600, 2100))

In [25]: idx = pd.MultiIndex.from_product([country, year], names=['country', 'year'])

In [27]: values = np.random.rand(idx.shape[0], 1)

In [30]: df = pd.DataFrame(values, index=idx, columns=['values']).reset_index()

In [41]: df2 = df.copy()

In [42]: df2['year'] = df2['year'].astype(str).apply(lambda x: pd.Period(x, 'Y'))


# performance when year column is int dtype
In [47]: %timeit df.query(""year > 2020"")
3.48 ms ± 80.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [48]: %timeit df[df.year > 2020]
1.07 ms ± 16.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

# performance when year column is Peroid dtype
In [49]: %timeit df2.query(""year > '2020'"")
1.03 s ± 18.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [50]: %timeit df2[df2.year > '2020']
1.46 ms ± 21.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

```

#### Problem description

In above example, the `year` column of `df2` is a Series of Period objects.  Running query() on the year column is a lot slower than running indexing (1.03s vs 1.46ms). Changing `engine` and `parser` for query() doesn't help. 


#### Expected Output

I was expecting performance difference between querying and boolean indexing is similar to that when the year column's dtype is int (~3x) 

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : zh_CN.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 41.2.0
Cython           : None
pytest           : 5.4.2
hypothesis       : 5.16.0
sphinx           : 3.0.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.14.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.1
matplotlib       : None
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.2
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
644054452,34958,ENH: DataFrame.query() should prioritize column names over types defined in local scope. ,chrisjcameron,open,2020-06-23T18:28:26Z,2020-09-03T01:09:49Z,"#### Is your feature request related to a problem?

If a column name matches a defined type it does not seem to be possible to include the column in a .query() string. For instance, `df.query(""Timestamp < 10"")` throws a `TypeError: '<' not supported between instances of 'type' and 'int'` for a dataframe where `df[df.Timestamp < 10]` yields the expected result. 

#### Describe the solution you'd like

Ideally, a query string should prioritize the the column or row index values for the dataframe whose .query method is called. This the natural scope of a query on a dataframe and we already have special operators  (e.g. `@`) to indicate that objects that exist outside the dataframe ""namespace"". The current behavior to automatically search outside the dataframe is almost a bug. 

#### Describe alternatives you've considered

Alternative 1: Perhaps the backtick escaping for column names with spaces could be extended to prefer matching dataframe column names over classes in the local scope. It does not make as much sense namespace-wise but it is less breaking than the proposed solution. 

"
637209785,34714,REF: BlockManager.apply_allow_failures,jbrockmendel,closed,2020-06-11T17:43:39Z,2020-09-03T02:56:33Z,"cc @mroeschke w/r/t rolling `._apply`, @WillAyd w/r/t `_cython_agg_blocks`, and anyone else w/r/t `DataFrame._reduce`with `numeric_only=None`,  `apply.FrameApply` with `ignore_failures=True`.

All of these do something along the lines of:

```
results = []
exclude = []
for i, block in enumerate(mgr.blocks):
    try:
          res = func(block.values)
          results.append(res)
    except:
          exclude.append(i)

out = reconstruct(results, exclude)
```

Two things should be done with this pattern:
1) Deprecate it, since it is a disproportionate maintainence burden
2) implement something like `BlockManager.apply_with_ignore_failures` that would resemble BlockManager.apply but with the try/except logic\*

\* We _could_ add that logic into BlockManager.apply, but BM.apply assumes the output has the same shape as the original frame, which I don't think holds for the cases mentioned above, so that would be a little more invasive than just adding a try/except."
685918327,35900,"REF: use BlockManager.apply for cython_agg_blocks, apply_blockwise",jbrockmendel,closed,2020-08-26T00:30:11Z,2020-09-03T02:57:42Z,"- [x] closes #34714
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
677118946,35677,to_csv to Google Cloud Storage ignores compression,VelizarVESSELINOV,closed,2020-08-11T18:57:54Z,2020-09-03T03:06:18Z,"#### Code Sample, a copy-pastable example

```python
dtf.to_csv('gs://mybucket/test1.csv.gz')
dtf.to_csv('gs://mybucket/test2.csv.gz', compression='gzip')
```

#### Problem description

Compression is not working for a file located on gs://

#### Expected Output

Working as a local version.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Sun Jul  5 00:43:10 PDT 2020; root:xnu-6153.141.1~9/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.18.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 41.2.0
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.13.0
pandas_datareader: 0.9.0
bs4              : None
bottleneck       : None
fsspec           : 0.8.0
fastparquet      : None
gcsfs            : 0.6.2
matplotlib       : 3.2.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 0.7.10
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
573839788,32392,read_csv from Google Cloud Storage ignores encoding,EgorBEremeev,closed,2020-03-02T09:35:03Z,2020-09-03T03:06:19Z,"#### Code Sample, a copy-pastable example if possible

```python
    dataframe = pd.read_csv('gs://mybucket/my_file', encoding = 'cp1251')

```
#### Problem description

Reading csv files which have encoding other than utf-8, like cp1251, from the Google Cloud Storage fails with error:

> UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 0: invalid continuation byte

the stacktrace from the Google Cloud Function environment:

> Traceback (most recent call last):
>   File ""/env/local/lib/python3.7/site-packages/google/cloud/functions/worker.py"", line 383, in run_background_function
>     _function_handler.invoke_user_function(event_object)
>   File ""/env/local/lib/python3.7/site-packages/google/cloud/functions/worker.py"", line 217, in invoke_user_function
>     return call_user_function(request_or_event)
>   File ""/env/local/lib/python3.7/site-packages/google/cloud/functions/worker.py"", line 214, in call_user_function
>     event_context.Context(**request_or_event.context))
>   File ""/user_code/main.py"", line 60, in load_csv_to_bq
>     na_filter=False)
>   File ""/env/local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 676, in parser_f
>     return _read(filepath_or_buffer, kwds)
>   File ""/env/local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 448, in _read
>     parser = TextFileReader(fp_or_buf, **kwds)
>   File ""/env/local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 880, in __init__
>     self._make_engine(self.engine)
>   File ""/env/local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 1114, in _make_engine
>     self._engine = CParserWrapper(self.f, **self.options)
>   File ""/env/local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 1891, in __init__
>     self._reader = parsers.TextReader(src, **kwds)
>   File ""pandas/_libs/parsers.pyx"", line 529, in pandas._libs.parsers.TextReader.__cinit__
>   File ""pandas/_libs/parsers.pyx"", line 748, in pandas._libs.parsers.TextReader._get_header
> UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 0: invalid continuation byte

It looks that in `pandas` ignoring of `encoding` parameter happens, because in the `pandas.io.gcs.get_filepath_or_buffer`  the `mode = 'rb'` is passed  to call of `GCSFileSystem.open(filepath_or_buffer, mode)`

Tracing back to the moment of the first actual setting the `mode` parameter we have stop on this line:

`pandas.io.common.py`
```
def get_filepath_or_buffer(
    filepath_or_buffer, encoding=None, compression=None, mode=None
)
```

, because in the call of `get_filepath_or_buffer()` performed from here 
https://github.com/pandas-dev/pandas/blob/29d6b0232aab9576afa896ff5bab0b994760495a/pandas/io/parsers.py#L430-L432
we do not pass value of `mode` and default `mode=None` works.

But in the current gcsf master `GCSFileSystem.open()` has been removed and `fsspec.AbstractFileSystem.open()` [has works instead](https://github.com/intake/filesystem_spec/blob/4c66e096d32dafe264e2d6707992ee6935685944/fsspec/spec.py#L717):

where applying of passed `encoding` for the **text reading\writing** is now implemented:

```
        if ""b"" not in mode:
            mode = mode.replace(""t"", """") + ""b""

            text_kwargs = {
                k: kwargs.pop(k)
                for k in [""encoding"", ""errors"", ""newline""]
                if k in kwargs
            }
            return io.TextIOWrapper(
                self.open(path, mode, block_size, **kwargs), **text_kwargs
            )
```





#### Expected Output
The encoding value passed into pd.read_csv() is applyied while reading from GCS, csv files are read.

As I could suggest for read_csv() we need pass `mode=r` and for to_csv() (see #26124) we need pass `mode=w`  in the call of `get_filepath_or_buffer()`. But I'm not sure where in code it's better to implement this change.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS   
------------------   
commit           : None   
python           : 3.7.6.final.0   
python-bits      : 64   
OS               : Linux   
OS-release       : 4.4.0   
machine          : x86_64   
processor        : x86_64   
byteorder        : little   
LC_ALL           : None   
LANG             : None   
LOCALE           : en_US.UTF-8   
   
pandas           : 1.0.1   
numpy            : 1.18.1   
pytz             : 2019.3   
dateutil         : 2.8.1   
pip              : 20.0.2   
setuptools       : 45.2.0   
Cython           : None   
pytest           : None   
hypothesis       : None   
sphinx           : None   
blosc            : None   
feather          : None   
xlsxwriter       : None   
lxml.etree       : None   
html5lib         : None   
pymysql          : None   
psycopg2         : None   
jinja2           : 2.10   
IPython          : None   
pandas_datareader: None   
bs4              : None   
bottleneck       : None   
fastparquet      : None   
gcsfs            : 0.6.0   
lxml.etree       : None   
matplotlib       : None   
numexpr          : None   
odfpy            : None   
openpyxl         : None   
pandas_gbq       : 0.13.1   
pyarrow          : None   
pytables         : None   
pytest           : None   
pyxlsb           : None   
s3fs             : None   
scipy            : None   
sqlalchemy       : None   
tables           : None   
tabulate         : None   
xarray           : None   
xlrd             : None   
xlwt             : None   
xlsxwriter       : None   
numba            : None

</details>
"
434425210,26124,to_csv to Google Cloud Storage ignores encoding,vackosar,closed,2019-04-17T18:29:51Z,2020-09-03T03:06:19Z,"#### Code Sample, a copy-pastable example if possible

```python
        df.to_csv(path_or_buf='gs://mybucket/my_file', encoding='utf8')

```
#### Problem description

Instead of expected UTF-8 file is stored in latin-1 on Windows. I can also see that current implementation of method ```pandas.io.gcs.get_filepath_or_buffer``` ignores encoding argument.

I can submit PR if are open to it.

#### Expected Output

Uploaded file encoded in UTF8 even on Windows.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.1.final.0
python-bits: 64
OS: Linux
OS-release: 4.18.0-17-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: None
pip: 10.0.1
setuptools: 39.1.0
Cython: None
numpy: 1.16.2
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: 0.9.0
pandas_datareader: None
gcsfs: 0.2.1

</details>
"
690038181,36034,TYP: Postponed Evaluation of Annotations (PEP 563),simonjayhawkins,closed,2020-09-01T10:47:32Z,2020-09-03T08:51:04Z,"a few files as POC/for discussion

from https://www.python.org/dev/peps/pep-0563/

> PEP 3107 added support for arbitrary annotations on parts of a function definition. Just like default values, annotations are evaluated at function definition time. This creates a number of issues for the type hinting use case:

> - forward references: when a type hint contains names that have not been defined yet, that definition needs to be expressed as a string literal;
> - type hints are executed at module import time, which is not computationally free.

> Postponing the evaluation of annotations solves both problems."
431919816,26050,Problem with DataFrame.replace using None,josegcpa,closed,2019-04-11T09:25:05Z,2020-09-03T09:21:39Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np

ar = np.random.normal(size=[100,10])
df = pd.DataFrame(ar).astype(str)

df.replace('0',None)
```
#### Problem description

Every time I run something similar to this (replacing, in a string DataFrame/DataFrame column, a specific string with `None`), the columns get filled with the element directly above it in the column; this creates a lot of confusion since None is, pretty much, THE ""pythonic"" way of signalling the absence of a variable/whatever. It works fine if replaced with np.nan, but if anything, I would expect them to work very approximately the same.

#### Expected Output

The expected output would be all strings matching the expression in `replace` to either be replaced with either `None` or `NaN`, or at least a warning message alerting for this behaviour.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.8.final.0
python-bits: 64
OS: Darwin
OS-release: 18.5.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.24.1
pytest: None
pip: 19.0.3
setuptools: 40.6.2
Cython: None
numpy: 1.16.0
scipy: 1.2.0
pyarrow: None
xarray: None
IPython: 7.2.0
sphinx: None
patsy: None
dateutil: 2.7.5
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: 4.7.1
html5lib: 0.9999999
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
545209245,30668,Concatenating Single-element dense series with SparseArray Series Raises Error,henighan,closed,2020-01-03T23:49:31Z,2020-09-03T09:30:33Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np


a = pd.Series(pd.SparseArray([1, None]), dtype=np.float)
b = pd.Series([1], dtype=np.float)
pd.concat([a, b])
```

With interpreter output

```python
>>>import pandas as pd
>>>import numpy as np
>>>
>>>
>>> a = pd.Series(pd.SparseArray([1, None]), dtype=np.float)
>>> b = pd.Series([1], dtype=np.float)
>>> pd.concat([a, b], axis=0)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/henighan/Documents/henighan-pandas/pandas/core/reshape/concat.py"", line 246, in concat
    return op.get_result()
  File ""/Users/henighan/Documents/henighan-pandas/pandas/core/reshape/concat.py"", line 426, in get_result
    [x._data for x in self.objs], self.new_axes
  File ""/Users/henighan/Documents/henighan-pandas/pandas/core/internals/managers.py"", line 1629, in concat
    values = concat_compat(values)
  File ""/Users/henighan/Documents/henighan-pandas/pandas/core/dtypes/concat.py"", line 117, in concat_compat
    return _concat_sparse(to_concat, axis=axis, typs=typs)
  File ""/Users/henighan/Documents/henighan-pandas/pandas/core/dtypes/concat.py"", line 478, in _concat_sparse
    for x in to_concat
  File ""/Users/henighan/Documents/henighan-pandas/pandas/core/dtypes/concat.py"", line 478, in <listcomp>
    for x in to_concat
  File ""/Users/henighan/Documents/henighan-pandas/pandas/core/arrays/sparse/array.py"", line 361, in __init__
    data, kind=kind, fill_value=fill_value, dtype=dtype
  File ""/Users/henighan/Documents/henighan-pandas/pandas/core/arrays/sparse/array.py"", line 1504, in make_sparse
    if arr.ndim > 1:
AttributeError: 'float' object has no attribute 'ndim'
```
#### Problem description

When concatenating a series of a sparse type (eg `Sparse[float64, nan]` above) with a serires of a dense type (eg `float64` above) that has only a single element, the above exception is raised.

I believe this may be the result of the `squeeze` happening here:
https://github.com/henighan/pandas/blob/45d8d77f27cf0dbc8cefe932f8fb64f6982b9527/pandas/core/dtypes/concat.py#L477

If I remove this `squeeze`, it resolves the issue for me (yielding the output below), and all tests still pass on my mac when running `./test_fast.sh`. If this seems right to others, I'd be happy to open a pull request.

#### Expected Output

```python
>>> import pandas as pd
>>> import numpy as np
>>>
>>>
>>> a = pd.Series(pd.SparseArray([1, None]), dtype=np.float)
>>> b = pd.Series([1], dtype=np.float)
>>> pd.concat([a, b])
0    1.0
1    NaN
0    1.0
dtype: Sparse[float64, nan]
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
>>> pd.show_versions()
/Users/henighan/Documents/henighan-pandas/pandas/core/index.py:29: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.
  FutureWarning,

INSTALLED VERSIONS
------------------
commit           : 45d8d77f27cf0dbc8cefe932f8fb64f6982b9527
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.26.0.dev0+1586.g45d8d77f2
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 44.0.0.post20200102
Cython           : 0.29.14
pytest           : 5.3.2
hypothesis       : 5.1.0
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.10.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.3.2
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.12
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.46.0

</details>
"
691768106,36085,CI: MyPy fixup,simonjayhawkins,closed,2020-09-03T08:39:18Z,2020-09-03T10:22:21Z,"https://github.com/pandas-dev/pandas/runs/1064831108

```
mypy --version
mypy 0.782
Performing static analysis using mypy
pandas/io/common.py:168: error: unused 'type: ignore' comment
pandas/io/common.py:170: error: Incompatible default for argument ""encoding"" (default has type ""None"", argument has type ""str"")  [assignment]
pandas/io/common.py:172: error: Incompatible default for argument ""mode"" (default has type ""None"", argument has type ""str"")  [assignment]
Found 3 errors in 1 file (checked 1037 source files)
```"
690962658,36065,BUG: Index.equals returns incorrect result,YarShev,closed,2020-09-02T12:01:45Z,2020-09-03T10:47:02Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas
i1 = pandas.date_range(""2008-01-01"", periods=1000, freq=""12H"")
i2 = pandas.date_range(""2008-01-01"", periods=1000, freq=""12H"")
i2.freq = None
i1.equals(i2)
True
# but freqs are now equal
i1.freq == i2.freq
False
```

#### Problem description

Could anyone please explain? Is it normal behavior for `equals`?

#### Output of ``pd.show_versions()``

<details>

pandas           : 1.1.1
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 41.2.0
Cython           : None
pytest           : 5.4.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : 0.4.1
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.14.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : 0.7.3
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : 3.6.1
tabulate         : None
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
690503166,36056,conda install -c anaconda pandas=1.1.1 fails in Windows ,eafpres,closed,2020-09-01T22:59:12Z,2020-09-03T10:47:35Z,"#### Question about pandas

I am trying to update Pandas in a venv under Windows 10 using miniconda.  I have Pandas 1.0.3 installed.  
I think part of the issue is there are pip installed packages in the environment  
This is unavoidable; some things I use cannot be installed by conda  
Is the only solution to build a new env from scratch, get pandas in there, then try to install remaining dependencies?

Here is my current env:  

```
conda list
# packages in environment at C:\Users\bbate\Miniconda3\envs\keras-gpu-2:
#
# Name                    Version                   Build  Channel
_anaconda_depends         2019.03                  py37_0
_py-xgboost-mutex         2.0                       cpu_0
absl-py                   0.9.0                    pypi_0    pypi
adjusttext                0.7.3                    pypi_0    pypi
alabaster                 0.7.12                   py37_0
alembic                   1.4.0                      py_0    conda-forge
anaconda                  custom                   py37_1
anaconda-client           1.7.2                    py37_0
anaconda-project          0.8.4                      py_0
argh                      0.26.2                   py37_0
arrow                     0.15.6           py37hc8dfbb8_1    conda-forge
asn1crypto                0.24.0                   py37_3    intel
astor                     0.8.1                    pypi_0    pypi
astroid                   2.3.3                    py37_0
astropy                   4.0              py37he774522_0
atomicwrites              1.3.0                    py37_1
attrs                     19.3.0                     py_0
autopep8                  1.4.4                      py_0
babel                     2.8.0                      py_0
backcall                  0.1.0                    py37_0
backports                 1.0                        py_2
backports.os              0.1.1                    py37_0
backports.shutil_get_terminal_size 1.0.0                    py37_2
bcrypt                    3.1.7            py37he774522_0
beautifulsoup4            4.8.2                    py37_0
binaryornot               0.4.4                      py_1    conda-forge
bitarray                  1.2.1            py37he774522_0
bkcharts                  0.2                      py37_0
blas                      1.0                         mkl
bleach                    3.1.0                    py37_0
blosc                     1.16.3               h7bd577a_0
bokeh                     2.1.0            py37hc8dfbb8_0    conda-forge
boost-cpp                 1.72.0               h0caebb8_0    conda-forge/label/cf202003
boto                      2.49.0                   py37_0
boto3                     1.9.66                   py37_0    anaconda
botocore                  1.12.189                   py_0
bottleneck                1.3.2            py37h2a96729_0
brotlipy                  0.7.0           py37h4ab8f01_1000    conda-forge
bs4                       0.0.1                    pypi_0    pypi
bzip2                     1.0.8                he774522_0
ca-certificates           2020.7.22                     0    anaconda
cachetools                4.0.0                    pypi_0    pypi
cairo                     1.16.0            h63a05c6_1001    conda-forge
certifi                   2020.6.20                py37_0    anaconda
cffi                      1.12.3                   py37_2    intel
cfitsio                   3.470                hfa6e2cd_2    conda-forge/label/cf202003
chardet                   3.0.4                    py37_3    intel
click                     7.0                      py37_0
click-plugins             1.1.1                      py_0    conda-forge/label/cf202003
cliff                     2.15.0                   py37_0    conda-forge
cligj                     0.5.0                      py_0    conda-forge/label/cf202003
cloudpickle               1.3.0                      py_0
clyent                    1.2.2                    py37_1
cmd2                      0.9.15                   py37_0    conda-forge
colorama                  0.4.3                      py_0
colorcet                  2.0.1                      py_0    conda-forge
coloredlogs               10.0                     py37_0
colorlog                  4.1.0                    py37_1    conda-forge
comtypes                  1.1.7                    py37_0
confuse                   1.3.0              pyh9f0ad1d_0    conda-forge
console_shortcut          0.1.1                         4
contextlib2               0.6.0.post1                py_0
cookiecutter              1.6.0                 py37_1000    conda-forge
cryptography              2.7                      py37_0    intel
curl                      7.68.0               h4496350_0    conda-forge/label/cf202003
cycler                    0.10.0                   py37_7    intel
cython                    0.29.13          py37ha68da19_0    intel
cytoolz                   0.10.1           py37he774522_0
daal                      2020.0                intel_166    intel
daal4py                   2020.0           py37ha68da19_8    intel
dask                      2.11.0                     py_0
dask-core                 2.11.0                     py_0
datashader                0.11.0             pyh9f0ad1d_0    conda-forge
datashape                 0.5.4                      py_1    conda-forge
decorator                 4.4.1                      py_0
defusedxml                0.6.0                      py_0
descartes                 1.1.0                      py_4    conda-forge
diff-match-patch          20181111                   py_0
distributed               2.11.0                   py37_0
docutils                  0.16                     py37_0
entrypoints               0.3                      py37_0
et_xmlfile                1.0.1                    py37_0
expat                     2.2.9                he025d50_2    conda-forge/label/cf202003
fastcache                 1.1.0            py37he774522_0
ffmpeg                    4.2.3                ha925a31_0    conda-forge
filelock                  3.0.12                     py_0
fiona                     1.8.9.post2      py37h3234bc7_0    conda-forge/label/cf202003
flake8                    3.8.3                      py_0    anaconda
flask                     1.1.1                      py_0
flask-compress            1.4.0                    pypi_0    pypi
freetype                  2.10.1                        1    intel
freexl                    1.0.5             hd288d7e_1002    conda-forge/label/cf202003
fsspec                    0.6.2                      py_0
future                    0.18.2                   py37_0
gast                      0.2.2                    pypi_0    pypi
gdal                      2.4.2            py37he6b6c38_8    conda-forge/label/cf202003
geopandas                 0.7.0                      py_1    conda-forge/label/cf202003
geos                      3.7.2                he025d50_2    conda-forge/label/cf202003
geotiff                   1.5.1                h4b1d854_3    conda-forge/label/cf202003
get_terminal_size         1.0.0                h38e98db_0
gettext                   0.19.8.1          hb01d8f6_1002    conda-forge
gevent                    1.4.0            py37he774522_0
glib                      2.64.3               he4de6d7_0    conda-forge
glob2                     0.7                        py_0
gobject-introspection     1.64.1           py37hf0dd101_1    conda-forge
google-auth               1.11.2                   pypi_0    pypi
google-auth-oauthlib      0.4.1                    pypi_0    pypi
google-pasta              0.1.8                    pypi_0    pypi
graphviz                  2.38                 hfd603c8_2    anaconda
greenlet                  0.4.15           py37hfa6e2cd_0
grpcio                    1.27.2                   pypi_0    pypi
h5py                      2.9.0           nompi_py37h9dfa0df_1103    conda-forge/label/cf202003
hdf4                      4.2.13            hf8e6fe8_1003    conda-forge/label/cf202003
hdf5                      1.10.4          nompi_hcc15c50_1106    conda-forge/label/cf202003
heapdict                  1.0.1                      py_0
hiplot                    0.1.4                    pypi_0    pypi
holoviews                 1.13.2             pyh9f0ad1d_0    conda-forge
html5lib                  1.0.1                    py37_0
htmlmin                   0.1.12                     py_1    conda-forge
humanfriendly             8.2                      py37_0
hypothesis                5.5.4                      py_0
icc_rt                    2020.0                intel_166    intel
icu                       58.2                 ha66f8fd_1
idna                      2.6                      py37_3    intel
imagehash                 4.1.0              pyh9f0ad1d_0    conda-forge
imageio                   2.6.1                    py37_0
imageio-ffmpeg            0.4.2                      py_0    conda-forge
imagesize                 1.2.0                      py_0
imbalanced-learn          0.7.0                      py_1    conda-forge
impi_rt                   2019.6                intel_166    intel
importlib-metadata        1.6.1            py37hc8dfbb8_0    conda-forge
importlib_metadata        1.5.0                    py37_0
inflect                   4.1.0                    py37_0
intel-openmp              2020.0                intel_166    intel
intelpython               2020.0                        1    intel
intervaltree              3.0.2                      py_0
ipykernel                 5.1.4            py37h39e3cac_0
ipympl                    0.5.6              pyh9f0ad1d_1    conda-forge
ipython                   7.12.0           py37h5ca1d4c_0
ipython_genutils          0.2.0                    py37_0
ipywidgets                7.5.1                      py_0
isort                     4.3.21                   py37_0
itsdangerous              1.1.0                    py37_0
jaraco.itertools          5.0.0                      py_0
jdcal                     1.4.1                      py_0
jedi                      0.17.1                   py37_0    anaconda
jinja2                    2.11.1                     py_0
jinja2-time               0.2.0                      py_2    conda-forge
jmespath                  0.9.4                      py_0    anaconda
joblib                    0.16.0                     py_0    conda-forge
jpeg                      9c                hfa6e2cd_1001    conda-forge/label/cf202003
json5                     0.9.1                      py_0
jsonschema                3.2.0                    py37_0
jupyter                   1.0.0                    py37_7
jupyter_client            5.3.4                    py37_0
jupyter_console           6.1.0                      py_0
jupyter_contrib_core      0.3.3                      py_2    conda-forge
jupyter_contrib_nbextensions 0.5.1                    py37_0    conda-forge
jupyter_core              4.6.1                    py37_0
jupyter_highlight_selected_word 0.2.0                 py37_1000    conda-forge
jupyter_latex_envs        1.4.4                 py37_1000    conda-forge
jupyter_nbextensions_configurator 0.4.1                    py37_0    conda-forge
jupyterlab                1.2.6              pyhf63ae98_0
jupyterlab_server         1.0.6                      py_0
kealib                    1.4.10            heacb130_1003    conda-forge/label/cf202003
keras-applications        1.0.8                    pypi_0    pypi
keras-preprocessing       1.1.0                    pypi_0    pypi
keyring                   21.1.0                   py37_0
kiwisolver                1.0.1                    py37_2    intel
krb5                      1.16.4               hc04afaa_0
lazy-object-proxy         1.4.3            py37he774522_0
libarchive                3.3.3                h0643e63_5
libcurl                   7.68.0               h4496350_0    conda-forge/label/cf202003
libffi                    3.2.1             h6538335_1007    conda-forge
libgdal                   2.4.2                h4f71e3f_8    conda-forge/label/cf202003
libiconv                  1.15                 hfa6e2cd_0    intel
libkml                    1.3.0             h7e985d0_1011    conda-forge/label/cf202003
liblief                   0.9.0                ha925a31_3
libnetcdf                 4.6.2             h396784b_1001    conda-forge/label/cf202003
libpng                    1.6.37               hfe6a214_1    conda-forge
libpq                     11.5                 hb0bdaea_1    conda-forge/label/cf202003
libsodium                 1.0.16               h9d3ae62_0
libspatialindex           1.9.3                h33f27b4_0
libspatialite             4.3.0a            h01b1fc4_1030    conda-forge/label/cf202003
libssh2                   1.8.2                h642c060_2    conda-forge/label/cf202003
libtiff                   4.1.0                h56a325e_0
libxgboost                0.90                          1    anaconda
libxml2                   2.9.9                hfa6e2cd_0    intel
libxslt                   1.1.33               h579f668_0
llvmlite                  0.31.0           py37ha1f5b8b_1    intel
locket                    0.2.0                    py37_1
lxml                      4.5.0            py37h1350720_0
lz4-c                     1.8.1.2              h2fa13f4_0    intel
lzo                       2.10                 hfa6e2cd_0    intel
m2cgen                    0.8.0                    pypi_0    pypi
m2w64-expat               2.1.1                         2
m2w64-gcc-libgfortran     5.3.0                         6
m2w64-gcc-libs            5.3.0                         7
m2w64-gcc-libs-core       5.3.0                         7
m2w64-gettext             0.19.7                        2
m2w64-gmp                 6.1.0                         2
m2w64-libiconv            1.14                          6
m2w64-libwinpthread-git   5.0.0.4634.697f757               2
m2w64-xz                  5.2.2                         2
mako                      1.1.2                      py_0
markdown                  3.2.1                    pypi_0    pypi
markupsafe                1.1.1            py37he774522_0
matplotlib                2.2.3            py37hd159220_0
matplotlib-base           3.2.1            py37h911224e_0    conda-forge
mccabe                    0.6.1                    py37_1
menuinst                  1.4.16                   py37_0    intel
missingno                 0.4.2                      py_0    conda-forge
missingpy                 0.2.0                    pypi_0    pypi
mistune                   0.8.4            py37he774522_0
mkl                       2020.0                intel_166    intel
mkl-service               2.3.0                    py37_0    intel
mkl_fft                   1.0.15           py37ha68da19_3    intel
mkl_random                1.1.0            py37ha68da19_0    intel
mock                      4.0.1                      py_0
modin                     0.7.3                      py_0    conda-forge
more-itertools            8.2.0                      py_0
moviepy                   1.0.1                      py_0    conda-forge
mpmath                    1.1.0                    py37_0
msgpack-python            0.6.1            py37h74a9793_1
msys2-conda-epoch         20160418                      1
multipledispatch          0.6.0                    py37_0
munch                     2.5.0                      py_0    conda-forge/label/cf202003
nbconvert                 5.6.1                    py37_0
nbformat                  5.0.4                      py_0
networkx                  2.4                        py_0
nltk                      3.4.5                    py37_0
nose                      1.3.7                    py37_2
notebook                  6.0.3                    py37_0
numba                     0.48.0              np117py37_0    intel
numexpr                   2.7.0                    py37_1    intel
numpy                     1.17.4           py37ha68da19_4    intel
numpy-base                1.17.4                   py37_4    intel
numpydoc                  0.9.2                      py_0
oauthlib                  3.1.0                    pypi_0    pypi
olefile                   0.46                     py37_0
openjpeg                  2.3.1                h57dd2e7_3    conda-forge/label/cf202003
openpyxl                  3.0.3                      py_0
openssl                   1.1.1g               he774522_1    anaconda
opt-einsum                3.1.0                    pypi_0    pypi
optuna                    1.0.0                      py_0    conda-forge
packaging                 20.1                       py_0
pandas                    1.0.3            py37h3bbf574_1    conda-forge
pandas-profiling          2.8.0                      py_0    conda-forge
pandas-summary            0.0.41                     py_1    conda-forge
pandoc                    2.2.3.2                       0
pandocfilters             1.4.2                    py37_1
panel                     0.9.5                      py_1    conda-forge
param                     1.9.3                      py_0    conda-forge
paramiko                  2.7.1                      py_0
parso                     0.7.0                      py_0    anaconda
partd                     1.1.0                      py_0
path                      13.1.0                   py37_0
path.py                   12.4.0                        0
pathlib2                  2.3.5                    py37_0
pathtools                 0.1.2                      py_1
patsy                     0.5.1                    py37_0
pbr                       5.4.2                      py_0    conda-forge
pcre                      8.44                 h6538335_0    conda-forge
pep8                      1.7.1                    py37_0
pexpect                   4.8.0                    py37_0
phik                      0.10.0                     py_0    conda-forge
pickleshare               0.7.5                    py37_0
pillow                    7.0.0            py37hcc1f983_0
pip                       20.2.2                   pypi_0    pypi
pixman                    0.38.0            hfa6e2cd_1003    conda-forge
pkginfo                   1.5.0.1                  py37_0
plotly                    4.8.1                      py_0
plotly_express            0.4.1                      py_0    plotly
pluggy                    0.13.1                   py37_0
ply                       3.11                     py37_0
poppler                   0.67.0               h1707e21_8    conda-forge/label/cf202003
poppler-data              0.4.9                         1    conda-forge/label/cf202003
postgresql                11.5                 h06f7779_1    conda-forge/label/cf202003
powershell_shortcut       0.0.1                         3
poyo                      0.5.0                      py_0    conda-forge
prettytable               0.7.2                      py_3    conda-forge
proglog                   0.1.9                      py_0    conda-forge
proj4                     6.1.1                hc2d0af5_1    conda-forge/label/cf202003
prometheus_client         0.7.1                      py_0
prompt_toolkit            3.0.3                      py_0
protobuf                  3.11.3                   pypi_0    pypi
psutil                    5.6.7            py37he774522_0
py                        1.8.1                      py_0
py-lief                   0.9.0            py37ha925a31_3
py-xgboost                0.90                     py37_1    anaconda
pyasn1                    0.4.8                    pypi_0    pypi
pyasn1-modules            0.2.8                    pypi_0    pypi
pycairo                   1.19.1           py37h6430cfb_3    conda-forge
pycodestyle               2.6.0                      py_0    anaconda
pycosat                   0.6.3                    py37_3    intel
pycparser                 2.18                     py37_2    intel
pycrypto                  2.6.1            py37hfa6e2cd_9
pyct                      0.4.6                      py_0    conda-forge
pyct-core                 0.4.6                      py_0    conda-forge
pycurl                    7.43.0.5         py37h7a1dbc1_0
pydocstyle                4.0.1                      py_0
pydotplus                 2.0.2                      py_3    anaconda
pyflakes                  2.2.0                      py_0    anaconda
pygments                  2.5.2                      py_0
pygobject                 3.36.1           py37hf6b2db1_0    conda-forge
pylint                    2.4.4                    py37_0
pynacl                    1.3.0            py37h62dcd97_0
pynndescent               0.4.8                      py_1    anaconda
pyodbc                    4.0.30           py37ha925a31_0
pyopenssl                 17.5.0                   py37_2    intel
pyparsing                 2.2.0                    py37_2    intel
pyperclip                 1.7.0                      py_0    conda-forge
pyproj                    2.3.1            py37he1416cd_0    conda-forge/label/cf202003
pyqt                      5.9.2            py37h6538335_2
pyreadline                2.1                      py37_1
pyrsistent                0.15.7           py37he774522_0
pysocks                   1.6.7                    py37_1    intel
pytables                  3.5.2            py37h6a20dd8_0    conda-forge/label/cf202003
pytest                    5.3.5                    py37_0
pytest-arraydiff          0.3              py37h39e3cac_0
pytest-astropy            0.8.0                      py_0
pytest-astropy-header     0.1.2                      py_0
pytest-doctestplus        0.5.0                      py_0
pytest-openfiles          0.4.0                      py_0
pytest-remotedata         0.3.2                    py37_0
python                    3.7.6                h60c2a47_2
python-dateutil           2.8.0                    py37_0    intel
python-editor             1.0.4                      py_0
python-jsonrpc-server     0.3.4                      py_0
python-language-server    0.34.1                   py37_0    anaconda
python-libarchive-c       2.8                     py37_13    intel
python_abi                3.7                     1_cp37m    conda-forge
pytz                      2019.1                   py37_0    intel
pyviz_comms               0.7.5              pyh9f0ad1d_0    conda-forge
pywavelets                1.1.1            py37he774522_0
pywin32                   224             py37hfa6e2cd_1002    intel
pywin32-ctypes            0.2.0                 py37_1000
pywinpty                  0.5.7                    py37_0
pyyaml                    5.1.1                    py37_0    intel
pyzmq                     18.1.1           py37ha925a31_0
qdarkstyle                2.8                        py_0
qt                        5.9.7            vc14h73c81de_0  [vc14]
qtawesome                 0.6.1                      py_0
qtconsole                 4.6.0                    py37_1
qtpy                      1.9.0                      py_0
requests                  2.23.0                   pypi_0    pypi
requests-oauthlib         1.3.0                    pypi_0    pypi
retrying                  1.3.3                    py37_2
rope                      0.16.0                     py_0
rsa                       4.0                      pypi_0    pypi
rtree                     0.9.3            py37h21ff451_0
ruamel_yaml               0.15.64                  py37_1    intel
s3transfer                0.1.13                   py37_0    anaconda
scikit-image              0.16.2           py37h47e9c7a_0
scikit-learn              0.23.1           py37h25d0782_0
scikit-surprise           1.1.0           py37hbc2f12b_1002    conda-forge
scipy                     1.4.1                    pypi_0    pypi
seaborn                   0.10.0                     py_0
send2trash                1.5.0                    py37_0
setuptools                41.0.1                   py37_0    intel
shapely                   1.6.4           py37ha35856d_1006    conda-forge/label/cf202003
simplegeneric             0.8.1                    py37_2
singledispatch            3.4.0.3                  py37_0
sip                       4.19.8           py37h6538335_0
six                       1.15.0                     py_0    anaconda
snappy                    1.1.7                h777316e_3
snowballstemmer           2.0.0                      py_0
sortedcollections         1.1.2                    py37_0
sortedcontainers          2.1.0                    py37_0
soupsieve                 1.9.5                    py37_0
sphinx                    2.3.1                      py_0
sphinxcontrib             1.0                      py37_1
sphinxcontrib-applehelp   1.0.1                      py_0
sphinxcontrib-devhelp     1.0.1                      py_0
sphinxcontrib-htmlhelp    1.0.2                      py_0
sphinxcontrib-jsmath      1.0.1                      py_0
sphinxcontrib-qthelp      1.0.2                      py_0
sphinxcontrib-serializinghtml 1.1.3                      py_0
sphinxcontrib-websupport  1.2.0                      py_0
spyder                    4.1.4                    py37_0    anaconda
spyder-kernels            1.9.2                    py37_0    anaconda
spyder-terminal           0.3.1            py37hc8dfbb8_1    spyder-ide
sqlalchemy                1.3.13           py37he774522_0
sqlite                    3.31.1               he774522_0
statsmodels               0.11.0           py37he774522_0
stevedore                 1.30.1                     py_0    conda-forge
stumpy                    1.3.0                      py_0    conda-forge
sympy                     1.5.1                    py37_0
tangled-up-in-unicode     0.0.6              pyh9f0ad1d_0    conda-forge
tbb                       2020.0           vc14_intel_166  [vc14]  intel
tbb4py                    2020.0             py37_intel_0  [vc14]  intel
tblib                     1.6.0                      py_0
tensorboard               2.1.0                    pypi_0    pypi
tensorflow                2.1.0                    pypi_0    pypi
tensorflow-estimator      2.1.0                    pypi_0    pypi
tensorflow-gpu            2.1.0                    pypi_0    pypi
tensorflow-gpu-estimator  2.1.0                    pypi_0    pypi
termcolor                 1.1.0                    pypi_0    pypi
terminado                 0.8.3                    py37_0
testpath                  0.4.4                      py_0
threadpoolctl             2.1.0              pyh5ca1d4c_0
tk                        8.6.8                hfa6e2cd_0
toml                      0.10.1                     py_0
toolz                     0.10.0                     py_0
tornado                   6.0.3            py37he774522_3
tqdm                      4.47.0             pyh9f0ad1d_0    conda-forge
traitlets                 4.3.3                    py37_0
tslearn                   0.3.1            py37hbc2f12b_0    conda-forge
typing                    3.6.4                    py37_0
typing_extensions         3.7.4.2                    py_0    conda-forge
ujson                     1.35             py37hfa6e2cd_0
umap-learn                0.4.4            py37hc8dfbb8_0    conda-forge
unicodecsv                0.14.1                   py37_0
urllib3                   1.24.1                   py37_2    intel
vc                        14.1                 h0510ff6_4
visions                   0.4.4              pyh9f0ad1d_0    conda-forge
vs2015_runtime            14.16.27012          hf0eaf9b_1
watchdog                  0.10.2                   py37_0
wcwidth                   0.1.8                      py_0
webencodings              0.5.1                    py37_1
werkzeug                  1.0.0                      py_0
wheel                     0.31.0                   py37_3    intel
whichcraft                0.6.1                      py_0    conda-forge
widgetsnbextension        3.5.1                    py37_0
win_inet_pton             1.0.1                    py37_4    intel
win_unicode_console       0.5                      py37_0
wincertstore              0.2                      py37_3    intel
winpty                    0.4.3                         4
wrapt                     1.11.2           py37he774522_0
xarray                    0.15.1                     py_0    conda-forge
xerces-c                  3.2.2             h6538335_1004    conda-forge/label/cf202003
xlrd                      1.2.0                    py37_0
xlsxwriter                1.2.7                      py_0
xlwings                   0.17.1                   py37_0
xlwt                      1.3.0                    py37_0
xz                        5.2.4                h2fa13f4_7    intel
yaml                      0.1.7                         2    intel
yapf                      0.28.0                     py_0
zeromq                    4.3.1                h33f27b4_3
zict                      1.0.0                      py_0
zipp                      2.2.0                      py_0
zlib                      1.2.11           vc14hfa6e2cd_7  [vc14]  intel
zstd                      1.3.7                h508b16e_0

```
Here is what happens:  
```
conda install -c anaconda pandas=1.1.1
Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
```
The list of conflicts is hundreds of lines long; here are the last few hundred...
```
Package snappy conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> blosc -> snappy[version='>=1.1.7,<2.0a0|>=1.1.8,<2.0a0']
defaults/win-64::blosc==1.16.3=h7bd577a_0 -> snappy[version='>=1.1.7,<2.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> snappy
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> snappy
conda-forge/label/cf202003/win-64::pytables==3.5.2=py37h6a20dd8_0 -> blosc[version='>=1.16.3,<2.0a0'] -> snappy[version='>=1.1.7,<2.0a0|>=1.1.8,<2.0a0']

Package ipaddress conflicts for:
conda-forge/win-64::jupyter_highlight_selected_word==0.2.0=py37_1000 -> notebook[version='>=4.0'] -> ipaddress
conda-forge/noarch::jupyter_contrib_core==0.3.3=py_2 -> notebook[version='>=4.0'] -> ipaddress
intel/win-64::pyopenssl==17.5.0=py37_2 -> cryptography[version='>=2.1.4'] -> ipaddress
intel/win-64::urllib3==1.24.1=py37_2 -> cryptography[version='>=1.3.4'] -> ipaddress
defaults/noarch::jupyterlab_server==1.0.6=py_0 -> notebook[version='>=4.2.0'] -> ipaddress
defaults/noarch::paramiko==2.7.1=py_0 -> cryptography[version='>=2.5'] -> ipaddress
defaults/win-64::jupyter==1.0.0=py37_7 -> notebook -> ipaddress
defaults/noarch::botocore==1.12.189=py_0 -> urllib3[version='>=1.20,<1.26'] -> ipaddress
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> notebook[version='>=4.0'] -> ipaddress
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> notebook[version='>=4.0'] -> ipaddress
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> cryptography -> ipaddress
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> notebook[version='>=4.0'] -> ipaddress
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> ipaddress
defaults/win-64::widgetsnbextension==3.5.1=py37_0 -> notebook[version='>=4.4.1'] -> ipaddress
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> notebook -> ipaddress
defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0 -> notebook[version='>=4.3.1'] -> ipaddress
conda-forge/noarch::requests==2.24.0=pyh9f0ad1d_0 -> urllib3[version='>=1.21.1,<1.26,!=1.25.0,!=1.25.1'] -> ipaddress

Package funcsigs conflicts for:
defaults/noarch::pytest-astropy-header==0.1.2=py_0 -> pytest[version='>=3.0'] -> funcsigs[version='>=1.0']
conda-forge/noarch::phik==0.10.0=py_0 -> numba[version='>=0.38.1'] -> funcsigs
conda-forge/win-64::umap-learn==0.4.4=py37hc8dfbb8_0 -> numba[version='>=0.46,!=0.47'] -> funcsigs
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> funcsigs
defaults/noarch::pytest-openfiles==0.4.0=py_0 -> pytest[version='>=3.1'] -> funcsigs[version='>=1.0']
defaults/win-64::pytest-remotedata==0.3.2=py37_0 -> pytest[version='>=3.1'] -> funcsigs[version='>=1.0']
conda-forge/label/cf202003/win-64::pytables==3.5.2=py37h6a20dd8_0 -> mock -> funcsigs
anaconda/noarch::pynndescent==0.4.8=py_1 -> numba[version='>=0.46'] -> funcsigs
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> numba -> funcsigs[version='>=1.0']
conda-forge/noarch::stumpy==1.3.0=py_0 -> numba[version='>=0.42.0'] -> funcsigs
defaults/win-64::pytest-arraydiff==0.3=py37h39e3cac_0 -> pytest -> funcsigs[version='>=1.0']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> numba[version='>=0.37.0,<0.49'] -> funcsigs
defaults/noarch::pytest-astropy==0.8.0=py_0 -> pytest[version='>=3.1'] -> funcsigs[version='>=1.0']
conda-forge/win-64::tslearn==0.3.1=py37hbc2f12b_0 -> numba -> funcsigs
defaults/noarch::pytest-doctestplus==0.5.0=py_0 -> pytest[version='>=3.0'] -> funcsigs[version='>=1.0']

Package python-dateutil conflicts for:
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> bokeh -> python-dateutil[version='>=2.1|>=2.7.3|>=2.6.1|>=2.5.*']
conda-forge/noarch::visions==0.4.4=pyh9f0ad1d_0 -> pandas[version='>=0.25.3'] -> python-dateutil[version='>=2.6.1|>=2.7.3']
conda-forge/win-64::bokeh==2.1.0=py37hc8dfbb8_0 -> python-dateutil[version='>=2.1']
conda-forge/win-64::matplotlib-base==3.2.1=py37h911224e_0 -> python-dateutil
conda-forge/noarch::datashape==0.5.4=py_1 -> python-dateutil
conda-forge/noarch::xarray==0.15.1=py_0 -> pandas[version='>=0.25'] -> python-dateutil[version='>=2.6.1|>=2.7.3']
conda-forge/noarch::missingno==0.4.2=py_0 -> matplotlib -> python-dateutil[version='>=2.5.*|>=2.6.1|>=2.7.3']
defaults/noarch::botocore==1.12.189=py_0 -> python-dateutil[version='>=2.1,<3.0.0']
conda-forge/win-64::arrow==0.15.6=py37hc8dfbb8_1 -> python-dateutil
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> bokeh[version='>=1.1.0'] -> python-dateutil[version='>=2.1|>=2.7.3|>=2.6.1|>=2.5.*']
conda-forge/label/cf202003/noarch::geopandas==0.7.0=py_1 -> pandas[version='>=0.23'] -> python-dateutil[version='>=2.5.*|>=2.6.1|>=2.7.3']
conda-forge/noarch::phik==0.10.0=py_0 -> matplotlib-base[version='>=2.2.3'] -> python-dateutil[version='>=2.5.*|>=2.6.1|>=2.7.3']
conda-forge/noarch::jinja2-time==0.2.0=py_2 -> arrow -> python-dateutil
conda-forge/noarch::optuna==1.0.0=py_0 -> alembic -> python-dateutil
anaconda/win-64::spyder-kernels==1.9.2=py37_0 -> jupyter_client[version='>=5.3.4'] -> python-dateutil[version='>=2.1']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> anaconda-client -> python-dateutil[version='>=2.1|>=2.6.1|>=2.7.3|>=2.5.*']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> python-dateutil
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> matplotlib[version='>=2.0.0'] -> python-dateutil
conda-forge/noarch::ipympl==0.5.6=pyh9f0ad1d_1 -> matplotlib-base[version='>=2.2.0'] -> python-dateutil
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> jupyter_client -> python-dateutil[version='>=2.1']
defaults/win-64::statsmodels==0.11.0=py37he774522_0 -> pandas[version='>=0.21'] -> python-dateutil[version='>=2.5.*|>=2.6.1|>=2.7.3']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> python-dateutil
conda-forge/noarch::alembic==1.4.0=py_0 -> python-dateutil
defaults/win-64::qtconsole==4.6.0=py37_1 -> jupyter_client[version='>=4.1'] -> python-dateutil[version='>=2.1']
modin==0.7.3 -> pandas==1.0.3 -> python-dateutil[version='>=2.6.1']
defaults/win-64::anaconda-client==1.7.2=py37_0 -> python-dateutil
defaults/win-64::bkcharts==0.2=py37_0 -> pandas -> python-dateutil[version='>=2.5.*|>=2.6.1|>=2.7.3']
anaconda/win-64::s3transfer==0.1.13=py37_0 -> botocore[version='>=1.3.0,<2.0.0'] -> python-dateutil[version='>=2.1,<2.7.0|>=2.1,<2.8.1|>=2.1,<3.0.0']
plotly/noarch::plotly_express==0.4.1=py_0 -> pandas[version='>=0.20.0'] -> python-dateutil[version='>=2.5.*|>=2.6.1|>=2.7.3']
defaults/win-64::notebook==6.0.3=py37_0 -> jupyter_client[version='>=5.3.4'] -> python-dateutil[version='>=2.1']
defaults/noarch::anaconda-project==0.8.4=py_0 -> anaconda-client -> python-dateutil[version='>=2.6.1']
defaults/noarch::seaborn==0.10.0=py_0 -> matplotlib[version='>=2.1.2'] -> python-dateutil[version='>=2.5.*|>=2.6.1|>=2.7.3']
conda-forge/noarch::pandas-summary==0.0.41=py_1 -> pandas -> python-dateutil[version='>=2.5.*|>=2.6.1|>=2.7.3']
defaults/win-64::matplotlib==2.2.3=py37hd159220_0 -> python-dateutil
defaults/noarch::dask==2.11.0=py_0 -> bokeh[version='>=1.0.0'] -> python-dateutil[version='>=2.1|>=2.7.3|>=2.6.1|>=2.5.*']
defaults/win-64::jupyter_client==5.3.4=py37_0 -> python-dateutil[version='>=2.1']
boto3 -> botocore[version='>=1.17.48,<1.18.0'] -> python-dateutil[version='>=2.1,<2.7.0|>=2.1,<2.8.1|>=2.1,<3.0.0']
conda-forge/noarch::descartes==1.1.0=py_4 -> matplotlib-base -> python-dateutil
defaults/noarch::jupyter_console==6.1.0=py_0 -> jupyter_client -> python-dateutil[version='>=2.1']
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> matplotlib-base[version='>=3.2.0'] -> python-dateutil[version='>=2.6.1|>=2.7.3']
conda-forge/noarch::panel==0.9.5=py_1 -> bokeh[version='>=2.0'] -> python-dateutil[version='>=2.1']

Package bkcharts conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> bkcharts
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> bkcharts
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> bokeh -> bkcharts[version='>=0.2']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> bokeh -> bkcharts[version='>=0.2']

Package geos conflicts for:
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> geos[version='>=3.7.2,<3.7.3.0a0']
conda-forge/label/cf202003/win-64::shapely==1.6.4=py37ha35856d_1006 -> geos[version='>=3.7.2,<3.7.3.0a0']
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> geos[version='>=3.7.2,<3.7.3.0a0']
conda-forge/label/cf202003/win-64::libspatialite==4.3.0a=h01b1fc4_1030 -> geos[version='>=3.7.2,<3.7.3.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> geos[version='>=3.6.2,<3.6.3.0a0|>=3.7.1,<3.7.2.0a0|>=3.7.2,<3.7.3.0a0|>=3.8.0,<3.8.1.0a0']
conda-forge/label/cf202003/noarch::geopandas==0.7.0=py_1 -> shapely -> geos[version='>=3.6.2,<3.6.3.0a0|>=3.7.1,<3.7.2.0a0|>=3.8.0,<3.8.1.0a0|>=3.7.2,<3.7.3.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> libspatialite[version='>=4.3.0a,<4.4.0a0'] -> geos[version='>=3.6.2,<3.6.3.0a0|>=3.7.1,<3.7.2.0a0|>=3.8.0,<3.8.1.0a0']

Package pytest conflicts for:
defaults/win-64::pytest-remotedata==0.3.2=py37_0 -> pytest[version='>=3.1']
defaults/noarch::pytest-openfiles==0.4.0=py_0 -> pytest[version='>=3.1']
defaults/noarch::pytest-astropy==0.8.0=py_0 -> pytest-arraydiff[version='>=0.1'] -> pytest[version='>=2.8.0|>=2.8|>=3.0|>=4.0|>=4.6']
defaults/noarch::pytest-astropy-header==0.1.2=py_0 -> pytest[version='>=3.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pytest
defaults/win-64::astropy==4.0=py37he774522_0 -> pytest-astropy -> pytest[version='>=3.1|>=3.1.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> astropy -> pytest[version='<3.7|<4|>=3.1|>=3.1.0|>=4.0|>=3.0|>=2.8|>=2.8.0|>=4.6']
defaults/noarch::pytest-doctestplus==0.5.0=py_0 -> pytest[version='>=3.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pytest
defaults/win-64::pytest-arraydiff==0.3=py37h39e3cac_0 -> pytest
defaults/noarch::pytest-astropy==0.8.0=py_0 -> pytest[version='>=3.1']

Package pyqt conflicts for:
anaconda/win-64::spyder==4.1.4=py37_0 -> qtconsole[version='>=4.6.0'] -> pyqt
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pyqt
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> matplotlib[version='>=2.0.0'] -> pyqt[version='5.*|5.6.*|5.9.*|>=5.6,<6.0a0|>=5.9.2,<5.10.0a0']
defaults/win-64::qtconsole==4.6.0=py37_1 -> pyqt
defaults/noarch::seaborn==0.10.0=py_0 -> matplotlib[version='>=2.1.2'] -> pyqt[version='5.*|5.6.*|5.9.*|>=5.6,<6.0a0|>=5.9.2,<5.10.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> matplotlib -> pyqt[version='5.*|5.6.*|5.9.*|>=5.6,<6.0a0|>=5.9.2,<5.10.0a0|>=5.6,<5.13']
anaconda/win-64::spyder==4.1.4=py37_0 -> pyqt[version='>=5.6,<5.13']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> pyqt[version='>=5.6,<5.13']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pyqt
defaults/win-64::jupyter==1.0.0=py37_7 -> qtconsole -> pyqt[version='>=5.9.2,<5.10.0a0']
defaults/win-64::matplotlib==2.2.3=py37hd159220_0 -> pyqt=5.9
conda-forge/noarch::missingno==0.4.2=py_0 -> matplotlib -> pyqt[version='5.*|5.6.*|5.9.*|>=5.6,<6.0a0|>=5.9.2,<5.10.0a0']

Package docutils conflicts for:
defaults/noarch::numpydoc==0.9.2=py_0 -> sphinx -> docutils[version='>=0.11|>=0.12']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> docutils
defaults/noarch::botocore==1.12.189=py_0 -> docutils[version='>=0.10']
anaconda/win-64::s3transfer==0.1.13=py37_0 -> botocore[version='>=1.3.0,<2.0.0'] -> docutils[version='>=0.10|>=0.10,<0.15|>=0.10,<0.16']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> docutils
anaconda/win-64::spyder==4.1.4=py37_0 -> sphinx[version='>=0.6.6'] -> docutils[version='>=0.11|>=0.12']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> sphinx -> docutils[version='>=0.11|>=0.12']
boto3 -> botocore[version='>=1.17.48,<1.18.0'] -> docutils[version='>=0.10|>=0.10,<0.15|>=0.10,<0.16']

Package lz4-c conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> lz4-c
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> lz4-c
defaults/win-64::blosc==1.16.3=h7bd577a_0 -> lz4-c[version='>=1.8.1.2,<1.9.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> blosc -> lz4-c[version='>=1.8.0,<1.9.0a0|>=1.8.1.2,<1.9.0a0|>=1.9.2,<1.10.0a0']
intel/win-64::python-libarchive-c==2.8=py37_13 -> libarchive -> lz4-c[version='>=1.8.0,<1.9.0a0|>=1.8.1.2,<1.9.0a0|>=1.9.2,<1.10.0a0']
conda-forge/label/cf202003/win-64::pytables==3.5.2=py37h6a20dd8_0 -> blosc[version='>=1.16.3,<2.0a0'] -> lz4-c[version='>=1.8.1.2,<1.9.0a0|>=1.9.2,<1.10.0a0']
defaults/win-64::libarchive==3.3.3=h0643e63_5 -> lz4-c[version='>=1.8.1.2,<1.9.0a0']

Package sip conflicts for:
anaconda/win-64::spyder==4.1.4=py37_0 -> pyqt[version='>=5.6,<5.13'] -> sip[version='4.18.*|>=4.19.4|>=4.19.4,<=4.19.8|>=4.19.13,<=4.19.14']
defaults/win-64::pyqt==5.9.2=py37h6538335_2 -> sip[version='>=4.19.4,<=4.19.8']
defaults/win-64::qtconsole==4.6.0=py37_1 -> pyqt -> sip[version='4.18.*|>=4.19.4|>=4.19.4,<=4.19.8|>=4.19.13,<=4.19.14']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pyqt -> sip[version='4.18.*|>=4.19.4|>=4.19.4,<=4.19.8|>=4.19.13,<=4.19.14']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> sip
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> sip
defaults/win-64::matplotlib==2.2.3=py37hd159220_0 -> pyqt=5.9 -> sip[version='>=4.19.13,<=4.19.14|>=4.19.4|>=4.19.4,<=4.19.8']

Package rtree conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> spyder -> rtree[version='>=0.8.3']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> rtree
anaconda/win-64::spyder==4.1.4=py37_0 -> rtree[version='>=0.8.3']
conda-forge/label/cf202003/noarch::geopandas==0.7.0=py_1 -> rtree
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> rtree[version='>=0.8.3']

Package pyodbc conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pyodbc
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pyodbc

Package tbb4py conflicts for:
defaults/win-64::patsy==0.5.1=py37_0 -> numpy[version='>=1.4.0'] -> tbb4py
conda-forge/noarch::stumpy==1.3.0=py_0 -> numba[version='>=0.42.0'] -> tbb4py
conda-forge/noarch::visions==0.4.4=pyh9f0ad1d_0 -> numpy -> tbb4py
defaults/noarch::pytest-doctestplus==0.5.0=py_0 -> numpy[version='>=1.10'] -> tbb4py
defaults/win-64::matplotlib==2.2.3=py37hd159220_0 -> numpy -> tbb4py
intel/win-64::numpy-base==1.17.4=py37_4 -> tbb4py
conda-forge/win-64::tslearn==0.3.1=py37hbc2f12b_0 -> numba -> tbb4py
conda-forge/noarch::datashape==0.5.4=py_1 -> numpy[version='>=1.7'] -> tbb4py
conda-forge/noarch::phik==0.10.0=py_0 -> numba[version='>=0.38.1'] -> tbb4py
intel/win-64::numba==0.48.0=np117py37_0 -> tbb4py
defaults/win-64::bkcharts==0.2=py37_0 -> numpy[version='>=1.7.1'] -> tbb4py
conda-forge/win-64::umap-learn==0.4.4=py37hc8dfbb8_0 -> numba[version='>=0.46,!=0.47'] -> tbb4py
intel/win-64::numpy==1.17.4=py37ha68da19_4 -> numpy-base==1.17.4=py37_4 -> tbb4py
defaults/win-64::pytest-arraydiff==0.3=py37h39e3cac_0 -> numpy -> tbb4py
anaconda/win-64::py-xgboost==0.90=py37_1 -> numpy -> tbb4py
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> numpy[version='>=1.0'] -> tbb4py
conda-forge/noarch::moviepy==1.0.1=py_0 -> numpy -> tbb4py
conda-forge/win-64::bokeh==2.1.0=py37hc8dfbb8_0 -> numpy[version='>=1.11.3'] -> tbb4py
defaults/noarch::dask==2.11.0=py_0 -> numpy[version='>=1.13.0'] -> tbb4py
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> numba[version='>=0.37.0,<0.49'] -> tbb4py
defaults/noarch::seaborn==0.10.0=py_0 -> numpy[version='>=1.13.3'] -> tbb4py
conda-forge/noarch::imagehash==4.1.0=pyh9f0ad1d_0 -> numpy -> tbb4py
plotly/noarch::plotly_express==0.4.1=py_0 -> numpy[version='>=1.11'] -> tbb4py
conda-forge/noarch::pandas-summary==0.0.41=py_1 -> numpy -> tbb4py
defaults/win-64::imageio==2.6.1=py37_0 -> numpy -> tbb4py
anaconda/noarch::pynndescent==0.4.8=py_1 -> numba[version='>=0.46'] -> tbb4py
intel/win-64::mkl_random==1.1.0=py37ha68da19_0 -> numpy-base[version='>=1.1.0,<2.0a0'] -> tbb4py
conda-forge/noarch::imbalanced-learn==0.7.0=py_1 -> numpy[version='>=1.11'] -> tbb4py
intel/win-64::mkl_fft==1.0.15=py37ha68da19_3 -> numpy-base[version='>=1.0.15,<2.0a0'] -> tbb4py
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> numba -> tbb4py
conda-forge/noarch::missingno==0.4.2=py_0 -> numpy -> tbb4py
conda-forge/noarch::optuna==1.0.0=py_0 -> numpy -> tbb4py

Package importlib-metadata conflicts for:
defaults/win-64::pytest-arraydiff==0.3=py37h39e3cac_0 -> pytest -> importlib-metadata[version='>=0.12']
defaults/noarch::pytest-astropy==0.8.0=py_0 -> pytest[version='>=3.1'] -> importlib-metadata[version='>=0.12']
defaults/noarch::pytest-openfiles==0.4.0=py_0 -> pytest[version='>=3.1'] -> importlib-metadata[version='>=0.12']
defaults/win-64::inflect==4.1.0=py37_0 -> importlib_metadata -> importlib-metadata[version='>=1.1.3,<1.1.4.0a0|>=1.6.0,<1.6.1.0a0|>=1.6.1,<1.6.2.0a0|>=1.7.0,<1.7.1.0a0']
defaults/win-64::pytest==5.3.5=py37_0 -> importlib_metadata[version='>=0.12'] -> importlib-metadata[version='>=1.1.3,<1.1.4.0a0|>=1.6.0,<1.6.1.0a0|>=1.6.1,<1.6.2.0a0|>=1.7.0,<1.7.1.0a0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> importlib-metadata
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> importlib_metadata -> importlib-metadata[version='>=0.12|>=1.1.3,<1.1.4.0a0|>=1.6.0,<1.6.1.0a0|>=1.6.1,<1.6.2.0a0|>=1.7.0,<1.7.1.0a0']
anaconda/win-64::python-language-server==0.34.1=py37_0 -> flake8[version='>=3.8.0'] -> importlib-metadata
anaconda/noarch::flake8==3.8.3=py_0 -> importlib-metadata
defaults/win-64::jsonschema==3.2.0=py37_0 -> importlib_metadata -> importlib-metadata[version='>=1.1.3,<1.1.4.0a0|>=1.6.0,<1.6.1.0a0|>=1.6.1,<1.6.2.0a0|>=1.7.0,<1.7.1.0a0']
defaults/noarch::pytest-doctestplus==0.5.0=py_0 -> pytest[version='>=3.0'] -> importlib-metadata[version='>=0.12']
anaconda/win-64::spyder==4.1.4=py37_0 -> keyring -> importlib-metadata
defaults/noarch::pytest-astropy-header==0.1.2=py_0 -> pytest[version='>=3.0'] -> importlib-metadata[version='>=0.12']
conda-forge/noarch::markdown==3.2.2=py_0 -> importlib-metadata
defaults/win-64::pytest-remotedata==0.3.2=py37_0 -> pytest[version='>=3.1'] -> importlib-metadata[version='>=0.12']
conda-forge/noarch::panel==0.9.5=py_1 -> markdown -> importlib-metadata
defaults/win-64::keyring==21.1.0=py37_0 -> importlib_metadata -> importlib-metadata[version='>=1.1.3,<1.1.4.0a0|>=1.6.0,<1.6.1.0a0|>=1.6.1,<1.6.2.0a0|>=1.7.0,<1.7.1.0a0']
defaults/win-64::path==13.1.0=py37_0 -> importlib_metadata[version='>=0.5'] -> importlib-metadata[version='>=1.1.3,<1.1.4.0a0|>=1.6.0,<1.6.1.0a0|>=1.6.1,<1.6.2.0a0|>=1.7.0,<1.7.1.0a0']
defaults/win-64::pluggy==0.13.1=py37_0 -> importlib_metadata[version='>=0.12'] -> importlib-metadata[version='>=1.1.3,<1.1.4.0a0|>=1.6.0,<1.6.1.0a0|>=1.6.1,<1.6.2.0a0|>=1.7.0,<1.7.1.0a0']

Package chardet conflicts for:
conda-forge/noarch::pyct==0.4.6=py_0 -> requests -> chardet[version='>=3.0.2,<3.1.0|>=3.0.2,<4']
defaults/win-64::anaconda-client==1.7.2=py37_0 -> requests[version='>=2.9.1'] -> chardet[version='>=3.0.2,<3.1.0|>=3.0.2,<4']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> requests -> chardet[version='>=2.0.0|>=3.0.2,<3.1.0|>=3.0.2,<4']
anaconda/win-64::spyder==4.1.4=py37_0 -> chardet[version='>=2.0.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> chardet
defaults/noarch::anaconda-project==0.8.4=py_0 -> requests -> chardet[version='>=3.0.2,<3.1.0|>=3.0.2,<4']
conda-forge/noarch::binaryornot==0.4.4=py_1 -> chardet
conda-forge/noarch::requests==2.24.0=pyh9f0ad1d_0 -> chardet[version='>=3.0.2,<4']
conda-forge/win-64::cookiecutter==1.6.0=py37_1000 -> binaryornot[version='>=0.2.0'] -> chardet[version='>=3.0.2,<3.1.0|>=3.0.2,<4']
conda-forge/noarch::moviepy==1.0.1=py_0 -> requests[version='>=2.8.1,<3.0'] -> chardet[version='>=3.0.2,<3.1.0|>=3.0.2,<4']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> requests -> chardet[version='>=2.0|>=2.0.0|>=3.0.2,<3.1.0|>=3.0.2,<4']
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> requests[version='>=2.23.0'] -> chardet[version='>=3.0.2,<3.1.0|>=3.0.2,<4']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> chardet

Package wheel conflicts for:
defaults/win-64::python==3.7.6=h60c2a47_2 -> pip -> wheel
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> wheel
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> wheel
intel/win-64::pip==19.3.1=py37_2 -> wheel
conda-forge/noarch::pbr==5.4.2=py_0 -> pip -> wheel

Package cloudpickle conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> cloudpickle
modin==0.7.3 -> dask[version='>=2.1.0'] -> cloudpickle[version='>=0.2.1|>=0.2.2|>=1.5.0|>=1.3.0']
anaconda/win-64::spyder-kernels==1.9.2=py37_0 -> cloudpickle
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> dask[version='>=0.18.0'] -> cloudpickle[version='>=0.2.1|>=0.2.2']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> dask -> cloudpickle[version='>=0.2.1|>=0.2.2|>=1.5.0|>=1.3.0|>=0.5.0']
defaults/noarch::dask==2.11.0=py_0 -> cloudpickle[version='>=0.2.1']
anaconda/win-64::spyder==4.1.4=py37_0 -> cloudpickle[version='>=0.5.0']
anaconda/win-64::spyder==4.1.4=py37_0 -> spyder-kernels[version='>=1.9.2,<1.10.0'] -> cloudpickle
defaults/win-64::distributed==2.11.0=py37_0 -> cloudpickle[version='>=0.2.2']
defaults/noarch::dask==2.11.0=py_0 -> distributed[version='>=2.11.0'] -> cloudpickle[version='>=0.2.2|>=1.3.0|>=1.5.0']
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> cloudpickle[version='>=0.2.1']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> cloudpickle[version='>=0.5.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> cloudpickle

Package send2trash conflicts for:
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> notebook[version='>=4.0'] -> send2trash
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> send2trash
defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0 -> notebook[version='>=4.3.1'] -> send2trash
conda-forge/win-64::jupyter_highlight_selected_word==0.2.0=py37_1000 -> notebook[version='>=4.0'] -> send2trash
conda-forge/noarch::jupyter_contrib_core==0.3.3=py_2 -> notebook[version='>=4.0'] -> send2trash
defaults/win-64::notebook==6.0.3=py37_0 -> send2trash
defaults/win-64::widgetsnbextension==3.5.1=py37_0 -> notebook[version='>=4.4.1'] -> send2trash
defaults/noarch::jupyterlab_server==1.0.6=py_0 -> notebook[version='>=4.2.0'] -> send2trash
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> notebook -> send2trash
defaults/win-64::jupyter==1.0.0=py37_7 -> notebook -> send2trash
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> send2trash
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> notebook[version='>=4.0'] -> send2trash
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> notebook[version='>=4.0'] -> send2trash

Package pathlib2 conflicts for:
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> ipython[version='>=5.0'] -> pathlib2
defaults/noarch::pytest-openfiles==0.4.0=py_0 -> pytest[version='>=3.1'] -> pathlib2[version='>=2.2.0']
defaults/win-64::inflect==4.1.0=py37_0 -> importlib_metadata -> pathlib2
defaults/noarch::pytest-astropy-header==0.1.2=py_0 -> pytest[version='>=3.0'] -> pathlib2[version='>=2.2.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pathlib2
defaults/noarch::pytest-astropy==0.8.0=py_0 -> pytest[version='>=3.1'] -> pathlib2[version='>=2.2.0']
defaults/win-64::ipython==7.12.0=py37h5ca1d4c_0 -> pickleshare -> pathlib2
defaults/win-64::keyring==21.1.0=py37_0 -> importlib_metadata -> pathlib2
defaults/noarch::pytest-doctestplus==0.5.0=py_0 -> pytest[version='>=3.0'] -> pathlib2[version='>=2.2.0']
defaults/win-64::path==13.1.0=py37_0 -> importlib_metadata[version='>=0.5'] -> pathlib2
defaults/win-64::pytest==5.3.5=py37_0 -> importlib_metadata[version='>=0.12'] -> pathlib2
defaults/win-64::pytest-remotedata==0.3.2=py37_0 -> pytest[version='>=3.1'] -> pathlib2[version='>=2.2.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pytest -> pathlib2[version='>=2.2.0']
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> ipython[version='>=5.4.0'] -> pathlib2
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pathlib2
defaults/win-64::pluggy==0.13.1=py37_0 -> importlib_metadata[version='>=0.12'] -> pathlib2
defaults/win-64::pytest-arraydiff==0.3=py37h39e3cac_0 -> pytest -> pathlib2[version='>=2.2.0']
anaconda/win-64::spyder==4.1.4=py37_0 -> pickleshare[version='>=0.4'] -> pathlib2
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> ipython -> pathlib2
defaults/noarch::jupyter_console==6.1.0=py_0 -> ipython -> pathlib2
defaults/noarch::ipywidgets==7.5.1=py_0 -> ipython[version='>=4.0.0'] -> pathlib2
defaults/win-64::jsonschema==3.2.0=py37_0 -> importlib_metadata -> pathlib2
defaults/win-64::nbconvert==5.6.1=py37_0 -> testpath -> pathlib2

Package statsmodels conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> statsmodels
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> seaborn -> statsmodels[version='>=0.5.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> statsmodels
plotly/noarch::plotly_express==0.4.1=py_0 -> statsmodels[version='>=0.9.0']
conda-forge/noarch::missingno==0.4.2=py_0 -> seaborn -> statsmodels[version='>=0.5.0']

Package tqdm conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> tqdm
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> tqdm
conda-forge/noarch::panel==0.9.5=py_1 -> tqdm
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> panel[version='>=0.7.0'] -> tqdm
conda-forge/noarch::moviepy==1.0.1=py_0 -> proglog[version='<=1.0.0'] -> tqdm
conda-forge/noarch::moviepy==1.0.1=py_0 -> tqdm[version='>=4.11.2,<5.0']
conda-forge/noarch::proglog==0.1.9=py_0 -> tqdm
conda-forge/noarch::optuna==1.0.0=py_0 -> tqdm
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> tqdm[version='>=4.43.0']

Package qt conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> qt
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> qt
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pyqt -> qt[version='5.6.*|5.9.*|>=5.9.7,<5.10.0a0|>=5.9.6,<5.10.0a0|>=5.9.4,<5.10.0a0']
defaults/win-64::pyqt==5.9.2=py37h6538335_2 -> qt[version='5.9.*|>=5.9.6,<5.10.0a0']
defaults/win-64::matplotlib==2.2.3=py37hd159220_0 -> pyqt=5.9 -> qt[version='5.9.*|>=5.9.7,<5.10.0a0|>=5.9.6,<5.10.0a0|>=5.9.4,<5.10.0a0']
anaconda/win-64::spyder==4.1.4=py37_0 -> pyqt[version='>=5.6,<5.13'] -> qt[version='5.6.*|5.9.*|>=5.9.7,<5.10.0a0|>=5.9.6,<5.10.0a0|>=5.9.4,<5.10.0a0']
defaults/win-64::qtconsole==4.6.0=py37_1 -> pyqt -> qt[version='5.6.*|5.9.*|>=5.9.7,<5.10.0a0|>=5.9.6,<5.10.0a0|>=5.9.4,<5.10.0a0']

Package snowballstemmer conflicts for:
defaults/noarch::numpydoc==0.9.2=py_0 -> sphinx -> snowballstemmer[version='>=1.1']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> sphinx -> snowballstemmer[version='>=1.1']
anaconda/win-64::python-language-server==0.34.1=py37_0 -> pydocstyle[version='>=2.0.0'] -> snowballstemmer
defaults/noarch::pydocstyle==4.0.1=py_0 -> snowballstemmer
anaconda/win-64::spyder==4.1.4=py37_0 -> sphinx[version='>=0.6.6'] -> snowballstemmer[version='>=1.1']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> snowballstemmer
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> snowballstemmer

Package setuptools conflicts for:
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> matplotlib[version='>=2.0.0'] -> setuptools
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> ipython[version='>=5.0'] -> setuptools[version='>=18.5']
conda-forge/noarch::pbr==5.4.2=py_0 -> pip -> setuptools
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> setuptools
defaults/noarch::anaconda-project==0.8.4=py_0 -> anaconda-client -> setuptools
conda-forge/noarch::phik==0.10.0=py_0 -> joblib[version='>=0.14.1'] -> setuptools
conda-forge/noarch::stumpy==1.3.0=py_0 -> numba[version='>=0.42.0'] -> setuptools
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> ipython -> setuptools[version='>=18.5|>=40.0']
defaults/noarch::pytest-astropy-header==0.1.2=py_0 -> pytest[version='>=3.0'] -> setuptools[version='>=40.0']
conda-forge/noarch::xarray==0.15.1=py_0 -> setuptools
conda-forge/win-64::umap-learn==0.4.4=py37hc8dfbb8_0 -> setuptools
defaults/noarch::plotly==4.8.1=py_0 -> setuptools
defaults/win-64::bleach==3.1.0=py37_0 -> setuptools
intel/win-64::wheel==0.31.0=py37_3 -> setuptools
conda-forge/noarch::imbalanced-learn==0.7.0=py_1 -> joblib[version='>=0.11'] -> setuptools
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> imagehash -> setuptools
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> setuptools
defaults/win-64::ipython==7.12.0=py37h5ca1d4c_0 -> setuptools[version='>=18.5']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> setuptools
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> jupyter_contrib_core[version='>=0.3.3'] -> setuptools
defaults/win-64::python==3.7.6=h60c2a47_2 -> pip -> setuptools
defaults/win-64::ipython==7.12.0=py37h5ca1d4c_0 -> pygments -> setuptools
defaults/win-64::terminado==0.8.3=py37_0 -> pywinpty -> setuptools
defaults/win-64::pytest-arraydiff==0.3=py37h39e3cac_0 -> pytest -> setuptools[version='>=40.0']
defaults/win-64::pylint==2.4.4=py37_0 -> astroid[version='>=2.3.0,<2.4'] -> setuptools
defaults/noarch::pygments==2.5.2=py_0 -> setuptools
defaults/win-64::anaconda-client==1.7.2=py37_0 -> setuptools
conda-forge/win-64::bokeh==2.1.0=py37hc8dfbb8_0 -> jinja2[version='>=2.7'] -> setuptools
anaconda/win-64::spyder==4.1.4=py37_0 -> pygments[version='>=2.0'] -> setuptools
defaults/noarch::jinja2==2.11.1=py_0 -> setuptools
defaults/win-64::jsonschema==3.2.0=py37_0 -> setuptools
modin==0.7.3 -> distributed[version='>=2.3.2'] -> setuptools
defaults/win-64::scikit-learn==0.23.1=py37h25d0782_0 -> joblib[version='>=0.11'] -> setuptools
defaults/noarch::prompt_toolkit==3.0.3=py_0 -> pygments -> setuptools
conda-forge/noarch::descartes==1.1.0=py_4 -> matplotlib-base -> setuptools
conda-forge/win-64::jupyter_highlight_selected_word==0.2.0=py37_1000 -> setuptools
defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0 -> jinja2[version='>=2.10'] -> setuptools
defaults/win-64::matplotlib==2.2.3=py37hd159220_0 -> setuptools
defaults/noarch::ipywidgets==7.5.1=py_0 -> ipython[version='>=4.0.0'] -> setuptools[version='>=18.5']
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> jupyter_contrib_core[version='>=0.3.2'] -> setuptools
conda-forge/noarch::joblib==0.16.0=py_0 -> setuptools
defaults/win-64::pytest==5.3.5=py37_0 -> setuptools[version='>=40.0']
conda-forge/noarch::hiplot==0.1.17=py_0 -> ipython[version='>=7.0.1'] -> setuptools[version='>=18.5']
conda-forge/noarch::imageio-ffmpeg==0.4.2=py_0 -> setuptools
conda-forge/win-64::cmd2==0.9.15=py37_0 -> setuptools
defaults/noarch::dask==2.11.0=py_0 -> distributed[version='>=2.11.0'] -> setuptools
conda-forge/noarch::jinja2-time==0.2.0=py_2 -> jinja2 -> setuptools
defaults/win-64::sympy==1.5.1=py37_0 -> setuptools
defaults/win-64::distributed==2.11.0=py37_0 -> setuptools
defaults/noarch::seaborn==0.10.0=py_0 -> matplotlib[version='>=2.1.2'] -> setuptools
defaults/win-64::nbconvert==5.6.1=py37_0 -> bleach -> setuptools
defaults/noarch::jupyterlab_server==1.0.6=py_0 -> jinja2[version='>2.10*'] -> setuptools
defaults/noarch::numpydoc==0.9.2=py_0 -> sphinx -> setuptools
defaults/win-64::nose==1.3.7=py37_2 -> setuptools
defaults/win-64::pytest-remotedata==0.3.2=py37_0 -> pytest[version='>=3.1'] -> setuptools[version='>=40.0']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> numba[version='>=0.37.0,<0.49'] -> setuptools[version='>=41.2']
conda-forge/noarch::visions==0.4.4=pyh9f0ad1d_0 -> networkx[version='>=2.4'] -> setuptools
conda-forge/noarch::jupyter_contrib_core==0.3.3=py_2 -> setuptools
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> ipython -> setuptools[version='>=18.5']
intel/win-64::pip==19.3.1=py37_2 -> setuptools
conda-forge/win-64::matplotlib-base==3.2.1=py37h911224e_0 -> setuptools
conda-forge/noarch::imagehash==4.1.0=pyh9f0ad1d_0 -> setuptools
defaults/noarch::flask==1.1.1=py_0 -> jinja2[version='>=2.10'] -> setuptools
conda-forge/noarch::missingno==0.4.2=py_0 -> matplotlib -> setuptools
anaconda/win-64::python-language-server==0.34.1=py37_0 -> flake8[version='>=3.8.0'] -> setuptools[version='>=30.0.0']
conda-forge/win-64::tslearn==0.3.1=py37hbc2f12b_0 -> joblib -> setuptools
defaults/win-64::isort==4.3.21=py37_0 -> setuptools
conda-forge/noarch::optuna==1.0.0=py_0 -> joblib -> setuptools
conda-forge/label/cf202003/noarch::munch==2.5.0=py_0 -> setuptools[version='>=17.1']
defaults/win-64::notebook==6.0.3=py37_0 -> jinja2 -> setuptools
conda-forge/noarch::moviepy==1.0.1=py_0 -> imageio-ffmpeg[version='>=0.2.0'] -> setuptools
anaconda/win-64::python-language-server==0.34.1=py37_0 -> setuptools
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> ipython[version='>=5.4.0'] -> setuptools[version='>=18.5']
defaults/noarch::jupyter_console==6.1.0=py_0 -> ipython -> setuptools[version='>=18.5']
defaults/noarch::pytest-astropy==0.8.0=py_0 -> pytest[version='>=3.1'] -> setuptools[version='>=40.0']
defaults/noarch::networkx==2.4=py_0 -> setuptools
defaults/noarch::pytest-openfiles==0.4.0=py_0 -> pytest[version='>=3.1'] -> setuptools[version='>=40.0']
defaults/noarch::nbformat==5.0.4=py_0 -> jsonschema[version='>=2.4,!=2.5.0'] -> setuptools
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> setuptools
plotly/noarch::plotly_express==0.4.1=py_0 -> plotly[version='>=4.1.0'] -> setuptools
defaults/win-64::qtconsole==4.6.0=py37_1 -> pygments -> setuptools
defaults/noarch::pytest-doctestplus==0.5.0=py_0 -> pytest[version='>=3.0'] -> setuptools[version='>=40.0']
defaults/win-64::clyent==1.2.2=py37_1 -> setuptools
conda-forge/noarch::panel==0.9.5=py_1 -> markdown -> setuptools
conda-forge/label/cf202003/noarch::geopandas==0.7.0=py_1 -> fiona -> setuptools
conda-forge/win-64::scikit-surprise==1.1.0=py37hbc2f12b_1002 -> setuptools
conda-forge/win-64::cookiecutter==1.6.0=py37_1000 -> jinja2[version='>=2.7'] -> setuptools
conda-forge/noarch::ipympl==0.5.6=pyh9f0ad1d_1 -> matplotlib-base[version='>=2.2.0'] -> setuptools
anaconda/noarch::flake8==3.8.3=py_0 -> setuptools[version='>=30.0.0']
defaults/win-64::astroid==2.3.3=py37_0 -> setuptools
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> munch -> setuptools[version='>=17.1']
anaconda/noarch::pynndescent==0.4.8=py_1 -> setuptools
conda-forge/win-64::cliff==2.15.0=py37_0 -> cmd2!=0.8.3 -> setuptools

Package expat conflicts for:
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> expat[version='>=2.2.5,<2.3.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> expat[version='>=2.2.5,<2.3.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> libkml[version='>=1.3.0,<1.4.0a0'] -> expat[version='>=2.2.5,<3.0a0|>=2.2.9,<2.3.0a0']
conda-forge/label/cf202003/win-64::libkml==1.3.0=h7e985d0_1011 -> expat[version='>=2.2.9,<2.3.0a0']
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> expat[version='>=2.2.5,<2.3.0a0']

Package iniconfig conflicts for:
defaults/noarch::pytest-doctestplus==0.5.0=py_0 -> pytest[version='>=3.0'] -> iniconfig
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pytest -> iniconfig
defaults/noarch::pytest-astropy-header==0.1.2=py_0 -> pytest[version='>=3.0'] -> iniconfig
defaults/noarch::pytest-openfiles==0.4.0=py_0 -> pytest[version='>=3.1'] -> iniconfig
defaults/noarch::pytest-astropy==0.8.0=py_0 -> pytest[version='>=3.1'] -> iniconfig
defaults/win-64::pytest-remotedata==0.3.2=py37_0 -> pytest[version='>=3.1'] -> iniconfig
defaults/win-64::pytest-arraydiff==0.3=py37h39e3cac_0 -> pytest -> iniconfig

Package future conflicts for:
defaults/noarch::python-jsonrpc-server==0.3.4=py_0 -> future
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> backports.os -> future
conda-forge/win-64::cookiecutter==1.6.0=py37_1000 -> future[version='>=0.15.2']
anaconda/win-64::python-language-server==0.34.1=py37_0 -> python-jsonrpc-server[version='>=0.3.2'] -> future
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> future

Package toolz conflicts for:
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> cytoolz[version='>=0.7.3'] -> toolz[version='>=0.10.0|>=0.8.0']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> toolz[version='>=0.7.4']
defaults/noarch::dask==2.11.0=py_0 -> toolz[version='>=0.7.3']
defaults/win-64::distributed==2.11.0=py37_0 -> toolz[version='>=0.7.4']
defaults/win-64::distributed==2.11.0=py37_0 -> cytoolz[version='>=0.7.4'] -> toolz[version='>=0.10.0|>=0.8.0']
defaults/noarch::dask==2.11.0=py_0 -> cytoolz[version='>=0.7.3'] -> toolz[version='>=0.10.0|>=0.8.0|>=0.8.2|>=0.7.4']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> cytoolz -> toolz[version='>=0.10.0|>=0.8.0|>=0.8.2|>=0.7.3|>=0.7.4']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> dask[version='>=0.18.0'] -> toolz[version='>=0.7.3|>=0.8.2']
defaults/noarch::partd==1.1.0=py_0 -> toolz
defaults/win-64::cytoolz==0.10.1=py37he774522_0 -> toolz[version='>=0.10.0']
modin==0.7.3 -> dask[version='>=2.1.0'] -> toolz[version='>=0.7.3|>=0.8.2|>=0.7.4']
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> toolz[version='>=0.7.3']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> toolz
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> toolz

Package jupyterlab conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> jupyterlab
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> jupyterlab

Package cycler conflicts for:
conda-forge/noarch::phik==0.10.0=py_0 -> matplotlib-base[version='>=2.2.3'] -> cycler[version='>=0.10']
conda-forge/win-64::matplotlib-base==3.2.1=py37h911224e_0 -> cycler[version='>=0.10']
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> matplotlib-base[version='>=2.2'] -> cycler[version='>=0.10']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> cycler
conda-forge/noarch::descartes==1.1.0=py_4 -> matplotlib-base -> cycler[version='>=0.10']
conda-forge/noarch::missingno==0.4.2=py_0 -> matplotlib -> cycler[version='>=0.10']
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> matplotlib-base[version='>=3.2.0'] -> cycler[version='>=0.10']
defaults/win-64::matplotlib==2.2.3=py37hd159220_0 -> cycler[version='>=0.10']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> matplotlib -> cycler[version='>=0.10']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> cycler
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> matplotlib[version='>=2.0.0'] -> cycler[version='>=0.10']
conda-forge/noarch::ipympl==0.5.6=pyh9f0ad1d_1 -> matplotlib-base[version='>=2.2.0'] -> cycler[version='>=0.10']
defaults/noarch::seaborn==0.10.0=py_0 -> matplotlib[version='>=2.1.2'] -> cycler[version='>=0.10']

Package defusedxml conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> defusedxml
defaults/win-64::jupyter==1.0.0=py37_7 -> nbconvert -> defusedxml
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> nbconvert[version='>=4.2'] -> defusedxml
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> nbconvert -> defusedxml
anaconda/win-64::spyder==4.1.4=py37_0 -> nbconvert[version='>=4.0'] -> defusedxml
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> nbconvert -> defusedxml
defaults/win-64::nbconvert==5.6.1=py37_0 -> defusedxml
defaults/win-64::notebook==6.0.3=py37_0 -> nbconvert -> defusedxml
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> defusedxml

Package intel-openmp conflicts for:
intel/win-64::mkl_random==1.1.0=py37ha68da19_0 -> mkl -> intel-openmp[version='2019.*|2020.*']
intel/win-64::numexpr==2.7.0=py37_1 -> mkl[version='>=2018.0.0'] -> intel-openmp[version='2019.*|2020.*']
defaults/win-64::scikit-learn==0.23.1=py37h25d0782_0 -> mkl[version='>=2019.4,<2021.0a0'] -> intel-openmp[version='2019.*|2020.*']
intel/win-64::mkl_fft==1.0.15=py37ha68da19_3 -> mkl -> intel-openmp[version='2019.*|2020.*']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> intel-openmp
intel/win-64::mkl==2020.0=intel_166 -> intel-openmp=2020
intel/win-64::mkl-service==2.3.0=py37_0 -> mkl -> intel-openmp[version='2019.*|2020.*']
intel/win-64::numpy-base==1.17.4=py37_4 -> mkl -> intel-openmp[version='2019.*|2020.*']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> intel-openmp
defaults/win-64::scipy==1.5.0=py37h9439919_0 -> mkl[version='>=2019.4,<2021.0a0'] -> intel-openmp[version='2019.*|2020.*']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> mkl -> intel-openmp[version='2019.*|2020.*']

Package mkl-service conflicts for:
anaconda/win-64::py-xgboost==0.90=py37_1 -> numpy -> mkl-service[version='>=2,<3.0a0']
defaults/noarch::seaborn==0.10.0=py_0 -> numpy[version='>=1.13.3'] -> mkl-service[version='>=2,<3.0a0']
intel/win-64::mkl_random==1.1.0=py37ha68da19_0 -> numpy-base[version='>=1.1.0,<2.0a0'] -> mkl-service[version='>=2,<3.0a0|>=2.3.0|>=2.1.0']
conda-forge/noarch::xarray==0.15.1=py_0 -> numpy[version='>=1.17'] -> mkl-service[version='>=2,<3.0a0']
defaults/win-64::bottleneck==1.3.2=py37h2a96729_0 -> numpy[version='>=1.14.6,<2.0a0'] -> mkl-service[version='>=2,<3.0a0']
conda-forge/label/cf202003/win-64::h5py==2.9.0=nompi_py37h9dfa0df_1103 -> numpy[version='>=1.14.6,<2.0a0'] -> mkl-service[version='>=2,<3.0a0']
defaults/win-64::scikit-learn==0.23.1=py37h25d0782_0 -> mkl-service[version='>=2,<3.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> numpy[version='>=1.14.6,<2.0a0'] -> mkl-service[version='>=2,<3.0a0']
intel/win-64::daal4py==2020.0=py37ha68da19_8 -> numpy[version='>=1.15'] -> mkl-service[version='>=2,<3.0a0']
plotly/noarch::plotly_express==0.4.1=py_0 -> numpy[version='>=1.11'] -> mkl-service[version='>=2,<3.0a0']
defaults/win-64::bkcharts==0.2=py37_0 -> numpy[version='>=1.7.1'] -> mkl-service[version='>=2,<3.0a0']
defaults/win-64::pywavelets==1.1.1=py37he774522_0 -> numpy[version='>=1.14.6,<2.0a0'] -> mkl-service[version='>=2,<3.0a0']
intel/win-64::mkl_fft==1.0.15=py37ha68da19_3 -> numpy-base[version='>=1.0.15,<2.0a0'] -> mkl-service[version='>=2,<3.0a0|>=2.3.0|>=2.1.0']
conda-forge/win-64::scikit-surprise==1.1.0=py37hbc2f12b_1002 -> numpy[version='>=1.14.6,<2.0a0'] -> mkl-service[version='>=2,<3.0a0']
conda-forge/noarch::datashape==0.5.4=py_1 -> numpy[version='>=1.7'] -> mkl-service[version='>=2,<3.0a0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> mkl-service
defaults/win-64::patsy==0.5.1=py37_0 -> numpy[version='>=1.4.0'] -> mkl-service[version='>=2,<3.0a0']
conda-forge/noarch::moviepy==1.0.1=py_0 -> numpy -> mkl-service[version='>=2,<3.0a0']
defaults/win-64::astropy==4.0=py37he774522_0 -> numpy[version='>=1.16.5,<2.0a0'] -> mkl-service[version='>=2,<3.0a0']
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> numpy[version='>=1.16.0'] -> mkl-service[version='>=2,<3.0a0']
intel/win-64::numpy==1.17.4=py37ha68da19_4 -> mkl_fft[version='>=1.0.15,<2.0a0'] -> mkl-service[version='>=2,<3.0a0|>=2.1.0']
conda-forge/noarch::imagehash==4.1.0=pyh9f0ad1d_0 -> numpy -> mkl-service[version='>=2,<3.0a0']
intel/win-64::numba==0.48.0=np117py37_0 -> numpy[version='>=1.17,<1.18.0a0'] -> mkl-service[version='>=2,<3.0a0']
conda-forge/noarch::stumpy==1.3.0=py_0 -> numpy[version='>=1.13'] -> mkl-service[version='>=2,<3.0a0']
conda-forge/noarch::optuna==1.0.0=py_0 -> numpy -> mkl-service[version='>=2,<3.0a0']
defaults/noarch::pytest-doctestplus==0.5.0=py_0 -> numpy[version='>=1.10'] -> mkl-service[version='>=2,<3.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> mkl-service
conda-forge/win-64::tslearn==0.3.1=py37hbc2f12b_0 -> numpy[version='>=1.14.6,<2.0a0'] -> mkl-service[version='>=2,<3.0a0']
defaults/noarch::dask==2.11.0=py_0 -> numpy[version='>=1.13.0'] -> mkl-service[version='>=2,<3.0a0']
conda-forge/label/cf202003/win-64::pytables==3.5.2=py37h6a20dd8_0 -> numexpr -> mkl-service[version='>=2,<3.0a0']
anaconda/noarch::pynndescent==0.4.8=py_1 -> numpy[version='>=1.13'] -> mkl-service[version='>=2,<3.0a0']
conda-forge/noarch::phik==0.10.0=py_0 -> numpy[version='>=1.15.4'] -> mkl-service[version='>=2,<3.0a0']
intel/win-64::numpy-base==1.17.4=py37_4 -> mkl-service[version='>=2.1.0']
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> numpy[version='>=1.0'] -> mkl-service[version='>=2,<3.0a0']
conda-forge/win-64::umap-learn==0.4.4=py37hc8dfbb8_0 -> numpy[version='>=1.17'] -> mkl-service[version='>=2,<3.0a0']
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> numpy[version='>=1.14.6,<2.0a0'] -> mkl-service[version='>=2,<3.0a0']
defaults/win-64::statsmodels==0.11.0=py37he774522_0 -> numpy[version='>=1.14.6,<2.0a0'] -> mkl-service[version='>=2,<3.0a0']
conda-forge/noarch::pandas-summary==0.0.41=py_1 -> numpy -> mkl-service[version='>=2,<3.0a0']
conda-forge/noarch::imbalanced-learn==0.7.0=py_1 -> numpy[version='>=1.11'] -> mkl-service[version='>=2,<3.0a0']
intel/win-64::numexpr==2.7.0=py37_1 -> numpy[version='>=1.17,<1.18.0a0'] -> mkl-service[version='>=2,<3.0a0']
conda-forge/noarch::visions==0.4.4=pyh9f0ad1d_0 -> numpy -> mkl-service[version='>=2,<3.0a0']
defaults/win-64::matplotlib==2.2.3=py37hd159220_0 -> numpy -> mkl-service[version='>=2,<3.0a0']
defaults/win-64::pytest-arraydiff==0.3=py37h39e3cac_0 -> numpy -> mkl-service[version='>=2,<3.0a0']
conda-forge/noarch::missingno==0.4.2=py_0 -> numpy -> mkl-service[version='>=2,<3.0a0']
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> numpy[version='>=1.14.6,<2.0a0'] -> mkl-service[version='>=2,<3.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> mkl_fft -> mkl-service[version='>=2,<3.0a0|>=2.3.0|>=2.1.0']
defaults/win-64::imageio==2.6.1=py37_0 -> numpy -> mkl-service[version='>=2,<3.0a0']
conda-forge/win-64::matplotlib-base==3.2.1=py37h911224e_0 -> numpy[version='>=1.14.6,<2.0a0'] -> mkl-service[version='>=2,<3.0a0']
conda-forge/win-64::bokeh==2.1.0=py37hc8dfbb8_0 -> numpy[version='>=1.11.3'] -> mkl-service[version='>=2,<3.0a0']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> numpy[version='>=1.7'] -> mkl-service[version='>=2,<3.0a0']
conda-forge/label/cf202003/win-64::shapely==1.6.4=py37ha35856d_1006 -> numpy[version='>=1.14.6,<2.0a0'] -> mkl-service[version='>=2,<3.0a0']
defaults/win-64::scipy==1.5.0=py37h9439919_0 -> mkl-service[version='>=2,<3.0a0']

Package libnetcdf conflicts for:
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> libnetcdf[version='>=4.6.2,<4.6.3.0a0']
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> libnetcdf[version='>=4.6.2,<4.6.3.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> libnetcdf[version='>=4.6.2,<4.6.3.0a0']

Package libsodium conflicts for:
anaconda/win-64::spyder==4.1.4=py37_0 -> pyzmq[version='>=17'] -> libsodium[version='>=1.0.16,<1.0.17.0a0']
defaults/win-64::pyzmq==18.1.1=py37ha925a31_0 -> zeromq[version='>=4.3.1,<4.3.2.0a0'] -> libsodium[version='>=1.0.16,<1.0.17.0a0']
defaults/win-64::notebook==6.0.3=py37_0 -> pyzmq[version='>=17'] -> libsodium[version='>=1.0.16,<1.0.17.0a0']
defaults/win-64::zeromq==4.3.1=h33f27b4_3 -> libsodium[version='>=1.0.16,<1.0.17.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> libsodium
anaconda/win-64::spyder-kernels==1.9.2=py37_0 -> pyzmq[version='>=17'] -> libsodium[version='>=1.0.16,<1.0.17.0a0']
defaults/win-64::jupyter_client==5.3.4=py37_0 -> pyzmq[version='>=13'] -> libsodium[version='>=1.0.16,<1.0.17.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pyzmq -> libsodium[version='>=1.0.16,<1.0.17.0a0|>=1.0.18,<1.0.19.0a0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> libsodium

Package krb5 conflicts for:
conda-forge/label/cf202003/win-64::libnetcdf==4.6.2=h396784b_1001 -> curl[version='>=7.59.0,<8.0a0'] -> krb5[version='>=1.16.1,<1.17.0a0|>=1.17.1,<1.18.0a0|>=1.18.2,<1.19.0a0|>=1.16.4,<1.17.0a0']
defaults/win-64::pycurl==7.43.0.5=py37h7a1dbc1_0 -> libcurl[version='>=7.67.0,<8.0a0'] -> krb5[version='>=1.16.1,<1.17.0a0|>=1.17.1,<1.18.0a0|>=1.18.2,<1.19.0a0|>=1.16.4,<1.17.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> curl -> krb5[version='>=1.16.1,<1.17.0a0|>=1.17.1,<1.18.0a0|>=1.18.2,<1.19.0a0|>=1.16.4,<1.17.0a0']
conda-forge/label/cf202003/win-64::libpq==11.5=hb0bdaea_1 -> krb5[version='>=1.16.3,<1.17.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> libpq[version='>=11.5,<12.0a0'] -> krb5[version='>=1.14.2,<1.15.0a0|>=1.16,<1.17.0a0|>=1.16.1,<1.17.0a0|>=1.16.3,<1.17.0a0|>=1.17.1,<1.18.0a0']
conda-forge/label/cf202003/win-64::postgresql==11.5=h06f7779_1 -> krb5[version='>=1.16.3,<1.17.0a0']
conda-forge/label/cf202003/win-64::libcurl==7.68.0=h4496350_0 -> krb5[version='>=1.16.4,<1.17.0a0']
conda-forge/label/cf202003/win-64::poppler==0.67.0=h1707e21_8 -> curl[version='>=7.64.1,<8.0a0'] -> krb5[version='>=1.16.1,<1.17.0a0|>=1.17.1,<1.18.0a0|>=1.18.2,<1.19.0a0|>=1.16.4,<1.17.0a0']
conda-forge/label/cf202003/win-64::curl==7.68.0=h4496350_0 -> krb5[version='>=1.16.4,<1.17.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> krb5
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> krb5

Package libtiff conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pillow -> libtiff[version='>=4.0.10,<5.0a0|>=4.1.0,<5.0a0|>=4.0.9,<5.0a0|>=4.0.8,<5.0a0']
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> pillow[version='>=4.3.0'] -> libtiff[version='>=4.0.10,<5.0a0|>=4.1.0,<5.0a0|>=4.0.9,<5.0a0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> libtiff
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> pillow[version='>=3.1.1'] -> libtiff[version='>=4.0.10,<5.0a0|>=4.1.0,<5.0a0|>=4.0.9,<5.0a0|>=4.0.8,<5.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> libtiff[version='>=4.0.10,<5.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> libtiff
conda-forge/label/cf202003/win-64::poppler==0.67.0=h1707e21_8 -> openjpeg[version='>=2.3.1,<2.4.0a0'] -> libtiff[version='>=4.1.0,<5.0a0']
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> pillow -> libtiff[version='>=4.0.10,<5.0a0|>=4.1.0,<5.0a0|>=4.0.9,<5.0a0|>=4.0.8,<5.0a0']
conda-forge/noarch::imagehash==4.1.0=pyh9f0ad1d_0 -> pillow -> libtiff[version='>=4.0.10,<5.0a0|>=4.1.0,<5.0a0|>=4.0.9,<5.0a0|>=4.0.8,<5.0a0']
conda-forge/win-64::bokeh==2.1.0=py37hc8dfbb8_0 -> pillow[version='>=4.0'] -> libtiff[version='>=4.0.10,<5.0a0|>=4.1.0,<5.0a0|>=4.0.9,<5.0a0|>=4.0.8,<5.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> geotiff[version='>=1.5.1,<1.5.2.0a0'] -> libtiff[version='>=4.1.0,<5.0a0']
conda-forge/label/cf202003/win-64::geotiff==1.5.1=h4b1d854_3 -> libtiff[version='>=4.0.10,<5.0a0']
defaults/win-64::imageio==2.6.1=py37_0 -> pillow -> libtiff[version='>=4.0.10,<5.0a0|>=4.1.0,<5.0a0|>=4.0.9,<5.0a0|>=4.0.8,<5.0a0']
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> libtiff[version='>=4.0.10,<5.0a0']
conda-forge/label/cf202003/win-64::poppler==0.67.0=h1707e21_8 -> libtiff[version='>=4.0.10,<5.0a0']
defaults/win-64::pillow==7.0.0=py37hcc1f983_0 -> libtiff[version='>=4.1.0,<5.0a0']
conda-forge/label/cf202003/win-64::openjpeg==2.3.1=h57dd2e7_3 -> libtiff[version='>=4.1.0,<5.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> libtiff[version='>=4.0.10,<5.0a0']

Package paramiko conflicts for:
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> paramiko[version='>=2.4.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> paramiko
anaconda/win-64::spyder==4.1.4=py37_0 -> paramiko[version='>=2.4.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> spyder -> paramiko[version='>=2.4.0']

Package botocore conflicts for:
boto3 -> botocore[version='>=1.10.24,<1.11.0|>=1.10.32,<1.11.0|>=1.10.45,<1.11.0|>=1.10.62,<1.11.0|>=1.11.2,<1.12.0|>=1.12.111,<1.13.0|>=1.12.134,<1.13.0|>=1.12.162,<1.13.0|>=1.12.199,<1.13.0|>=1.12.234,<1.13.0|>=1.13.13,<1.14.0|>=1.13.19,<1.14.0|>=1.13.39,<1.14.0|>=1.14.0,<1.15.0|>=1.14.14,<1.15.0|>=1.15.0,<1.16.0|>=1.15.12,<1.16.0|>=1.15.39,<1.16.0|>=1.16.11,<1.17.0|>=1.17.12,<1.18.0|>=1.17.31,<1.18.0|>=1.17.37,<1.18.0|>=1.17.39,<1.18.0|>=1.17.43,<1.18.0|>=1.17.45,<1.18.0|>=1.17.48,<1.18.0|>=1.12.82,<1.13.0|>=1.12.66,<1.13.0|>=1.12.35,<1.13.0|>=1.12.21,<1.13.0|>=1.12.13,<1.13.0|>=1.12.7,<1.13.0|>=1.10.4,<1.11.0|>=1.9.18,<1.10.0|>=1.8.46,<1.9.0|>=1.8.0,<1.9.0|>=1.7.0,<1.8.0']
boto3 -> s3transfer[version='>=0.3.0,<0.4.0'] -> botocore[version='>=1.12.36,<2.0.0|>=1.3.0,<2.0.0']
anaconda/win-64::s3transfer==0.1.13=py37_0 -> botocore[version='>=1.3.0,<2.0.0']

Package backports conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> backports
defaults/win-64::backports.os==0.1.1=py37_0 -> backports
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> backports
defaults/win-64::backports.shutil_get_terminal_size==1.0.0=py37_2 -> backports
defaults/win-64::get_terminal_size==1.0.0=h38e98db_0 -> backports.shutil_get_terminal_size -> backports

Package widgetsnbextension conflicts for:
conda-forge/noarch::ipympl==0.5.6=pyh9f0ad1d_1 -> ipywidgets[version='>=7.5.0,<8.0'] -> widgetsnbextension[version='>=3.5.0,<3.6.0']
defaults/noarch::ipywidgets==7.5.1=py_0 -> widgetsnbextension[version='>=3.5.0,<3.6.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> widgetsnbextension
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> ipywidgets[version='>=7.5.1'] -> widgetsnbextension[version='>=3.5.0,<3.6.0']
defaults/win-64::jupyter==1.0.0=py37_7 -> ipywidgets -> widgetsnbextension[version='>=1.2.6|>=3.0.0|>=3.1.0,<4.0|>=3.2.0,<4.0.0|>=3.3.0,<3.4.0|>=3.4.0,<3.5.0|>=3.5.0,<3.6.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> ipywidgets -> widgetsnbextension[version='>=1.2.6|>=3.0.0|>=3.1.0,<4.0|>=3.2.0,<4.0.0|>=3.3.0,<3.4.0|>=3.4.0,<3.5.0|>=3.5.0,<3.6.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> widgetsnbextension

Package numpydoc conflicts for:
anaconda/win-64::spyder==4.1.4=py37_0 -> numpydoc[version='>=0.6.0']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> numpydoc[version='>=0.6.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> numpydoc
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> spyder -> numpydoc[version='>=0.6.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> numpydoc
defaults/win-64::ipython==7.12.0=py37h5ca1d4c_0 -> jedi[version='>=0.10'] -> numpydoc

Package multipledispatch conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> multipledispatch
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> datashape[version='>=0.5.1'] -> multipledispatch[version='>=0.4.7']
conda-forge/noarch::datashape==0.5.4=py_1 -> multipledispatch[version='>=0.4.7']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> multipledispatch

Package sortedcontainers conflicts for:
anaconda/win-64::spyder==4.1.4=py37_0 -> intervaltree -> sortedcontainers
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> sortedcontainers
defaults/win-64::sortedcollections==1.1.2=py37_0 -> sortedcontainers[version='>=2.0']
defaults/noarch::hypothesis==5.5.4=py_0 -> sortedcontainers[version='>=2.1.0,<3.0.0']
defaults/noarch::dask==2.11.0=py_0 -> distributed[version='>=2.11.0'] -> sortedcontainers[version='!=2.0.0,!=2.0.1']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> distributed -> sortedcontainers[version='!=2.0.0,!=2.0.1|>=2.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> sortedcontainers
modin==0.7.3 -> distributed[version='>=2.3.2'] -> sortedcontainers[version='!=2.0.0,!=2.0.1']
defaults/noarch::pytest-astropy==0.8.0=py_0 -> hypothesis -> sortedcontainers[version='>=2.1.0,<3.0.0']
defaults/win-64::distributed==2.11.0=py37_0 -> sortedcontainers[version='!=2.0.0,!=2.0.1']
defaults/noarch::intervaltree==3.0.2=py_0 -> sortedcontainers

Package typing_extensions conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> bokeh -> typing_extensions[version='>=3.7.4']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> typing_extensions
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> bokeh[version='>=1.1.0'] -> typing_extensions[version='>=3.7.4']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> bokeh -> typing_extensions[version='>=3.7.4']
defaults/noarch::dask==2.11.0=py_0 -> bokeh[version='>=1.0.0'] -> typing_extensions[version='>=3.7.4']
conda-forge/noarch::panel==0.9.5=py_1 -> bokeh[version='>=2.0'] -> typing_extensions[version='>=3.7.4']
conda-forge/win-64::bokeh==2.1.0=py37hc8dfbb8_0 -> typing_extensions[version='>=3.7.4']

Package winpty conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> winpty
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pywinpty -> winpty[version='>=0.4.3,<1.0a0']
defaults/win-64::terminado==0.8.3=py37_0 -> pywinpty -> winpty[version='>=0.4.3,<1.0a0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> winpty
defaults/win-64::pywinpty==0.5.7=py37_0 -> winpty

Package jupyter_core conflicts for:
defaults/win-64::widgetsnbextension==3.5.1=py37_0 -> notebook[version='>=4.4.1'] -> jupyter_core[version='>=4.4.0|>=4.6.0|>=4.6.1']
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> jupyter_client -> jupyter_core[version='>=4.6.0']
defaults/noarch::ipywidgets==7.5.1=py_0 -> nbformat[version='>=4.2.0'] -> jupyter_core
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> notebook[version='>=4.0'] -> jupyter_core[version='>=4.4.0|>=4.6.0|>=4.6.1']
defaults/win-64::qtconsole==4.6.0=py37_1 -> jupyter_client[version='>=4.1'] -> jupyter_core[version='>=4.6.0']
defaults/noarch::nbformat==5.0.4=py_0 -> jupyter_core
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> jupyter_client -> jupyter_core[version='>=4.4.0|>=4.6.0|>=4.6.1']
defaults/win-64::notebook==6.0.3=py37_0 -> jupyter_core[version='>=4.6.1']
defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0 -> notebook[version='>=4.3.1'] -> jupyter_core[version='>=4.4.0|>=4.6.0|>=4.6.1']
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> notebook[version='>=4.0'] -> jupyter_core[version='>=4.4.0|>=4.6.0|>=4.6.1']
conda-forge/noarch::jupyter_contrib_core==0.3.3=py_2 -> notebook[version='>=4.0'] -> jupyter_core[version='>=4.4.0|>=4.6.0|>=4.6.1']
anaconda/win-64::spyder==4.1.4=py37_0 -> nbconvert[version='>=4.0'] -> jupyter_core
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> notebook -> jupyter_core[version='>=4.4.0|>=4.6.0|>=4.6.1']
anaconda/win-64::spyder-kernels==1.9.2=py37_0 -> jupyter_client[version='>=5.3.4'] -> jupyter_core[version='>=4.6.0']
defaults/win-64::anaconda-client==1.7.2=py37_0 -> nbformat -> jupyter_core
conda-forge/noarch::jupyter_contrib_core==0.3.3=py_2 -> jupyter_core
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> jupyter_core
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> jupyter_contrib_core[version='>=0.3'] -> jupyter_core[version='>=4.4.0|>=4.6.0|>=4.6.1']
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> jupyter_core
defaults/noarch::jupyter_console==6.1.0=py_0 -> jupyter_client -> jupyter_core[version='>=4.6.0']
defaults/win-64::notebook==6.0.3=py37_0 -> jupyter_client[version='>=5.3.4'] -> jupyter_core[version='>=4.6.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> jupyter_core
defaults/win-64::jupyter==1.0.0=py37_7 -> nbconvert -> jupyter_core[version='>=4.4.0|>=4.6.0|>=4.6.1']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> jupyter_core
conda-forge/win-64::jupyter_highlight_selected_word==0.2.0=py37_1000 -> jupyter_contrib_core[version='>=0.3'] -> jupyter_core[version='>=4.4.0|>=4.6.0|>=4.6.1']
defaults/noarch::jupyterlab_server==1.0.6=py_0 -> notebook[version='>=4.2.0'] -> jupyter_core[version='>=4.4.0|>=4.6.0|>=4.6.1']
defaults/win-64::nbconvert==5.6.1=py37_0 -> jupyter_core
defaults/win-64::jupyter_client==5.3.4=py37_0 -> jupyter_core[version='>=4.6.0']
defaults/win-64::qtconsole==4.6.0=py37_1 -> jupyter_core

Package scipy conflicts for:
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> scipy[version='>=1.4.1']
conda-forge/noarch::imagehash==4.1.0=pyh9f0ad1d_0 -> scipy
conda-forge/win-64::umap-learn==0.4.4=py37hc8dfbb8_0 -> scikit-learn[version='>=0.20'] -> scipy
conda-forge/noarch::missingno==0.4.2=py_0 -> seaborn -> scipy[version='>=0.15.2|>=1.0.1']
defaults/win-64::statsmodels==0.11.0=py37he774522_0 -> scipy[version='>=1.0']
defaults/noarch::seaborn==0.10.0=py_0 -> scipy[version='>=1.0.1']
defaults/win-64::scikit-learn==0.23.1=py37h25d0782_0 -> scipy
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> scipy
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> scipy
plotly/noarch::plotly_express==0.4.1=py_0 -> patsy[version='>=0.5'] -> scipy[version='>=0.14|>=1.0']
plotly/noarch::plotly_express==0.4.1=py_0 -> scipy[version='>=0.18']
anaconda/noarch::pynndescent==0.4.8=py_1 -> scipy[version='>=1.0']
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> scipy[version='>=0.19']
defaults/win-64::patsy==0.5.1=py37_0 -> scipy
conda-forge/noarch::missingno==0.4.2=py_0 -> scipy
anaconda/win-64::py-xgboost==0.90=py37_1 -> scipy
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> imagehash -> scipy[version='>=1.1.0']
conda-forge/noarch::phik==0.10.0=py_0 -> scipy[version='>=1.1.0']
conda-forge/win-64::tslearn==0.3.1=py37hbc2f12b_0 -> scipy
conda-forge/win-64::umap-learn==0.4.4=py37hc8dfbb8_0 -> scipy[version='>=1.3.1']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> scipy
conda-forge/noarch::optuna==1.0.0=py_0 -> scipy
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> scikit-image -> scipy[version='>=0.14|>=0.15.2|>=0.17|>=0.19|>=1.0.1|>=1.0']
conda-forge/noarch::stumpy==1.3.0=py_0 -> scipy[version='>=1.2.1']
anaconda/noarch::pynndescent==0.4.8=py_1 -> scikit-learn[version='>=0.19'] -> scipy
defaults/win-64::statsmodels==0.11.0=py37he774522_0 -> patsy[version='>=0.5.1'] -> scipy
conda-forge/noarch::imbalanced-learn==0.7.0=py_1 -> scipy

Package msys2-conda-epoch conflicts for:
defaults/win-64::m2w64-expat==2.1.1=2 -> msys2-conda-epoch[version='>=20160418']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> msys2-conda-epoch
conda-forge/win-64::libffi==3.2.1=h6538335_1007 -> m2w64-gcc-libs -> msys2-conda-epoch[version='>=20160418']
defaults/win-64::m2w64-gcc-libs==5.3.0=7 -> msys2-conda-epoch[version='>=20160418']
defaults/win-64::m2w64-libiconv==1.14=6 -> msys2-conda-epoch[version='>=20160418']
defaults/win-64::pywinpty==0.5.7=py37_0 -> m2w64-gcc-libs -> msys2-conda-epoch[version='>=20160418']
defaults/win-64::m2w64-gettext==0.19.7=2 -> msys2-conda-epoch[version='>=20160418']
defaults/win-64::m2w64-xz==5.2.2=2 -> msys2-conda-epoch[version='>=20160418']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> msys2-conda-epoch
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> m2w64-xz -> msys2-conda-epoch[version='>=20160418']
defaults/win-64::m2w64-libwinpthread-git==5.0.0.4634.697f757=2 -> msys2-conda-epoch[version='>=20160418']
defaults/win-64::m2w64-gcc-libgfortran==5.3.0=6 -> msys2-conda-epoch[version='>=20160418']
defaults/win-64::m2w64-gcc-libs-core==5.3.0=7 -> msys2-conda-epoch[version='>=20160418']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> m2w64-gcc-libgfortran -> msys2-conda-epoch[version='>=20160418']
anaconda/win-64::libxgboost==0.90=1 -> m2w64-gcc-libs -> msys2-conda-epoch[version='>=20160418']
defaults/win-64::m2w64-gmp==6.1.0=2 -> msys2-conda-epoch[version='>=20160418']

Package ipykernel conflicts for:
anaconda/win-64::spyder==4.1.4=py37_0 -> qtconsole[version='>=4.6.0'] -> ipykernel[version='>=4.1|>=5.1.3']
anaconda/win-64::spyder-kernels==1.9.2=py37_0 -> ipykernel[version='>=5.1.3']
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> notebook[version='>=4.0'] -> ipykernel
defaults/noarch::jupyterlab_server==1.0.6=py_0 -> notebook[version='>=4.2.0'] -> ipykernel
defaults/win-64::qtconsole==4.6.0=py37_1 -> ipykernel[version='>=4.1']
defaults/noarch::ipywidgets==7.5.1=py_0 -> ipykernel[version='>=4.5.1']
defaults/win-64::jupyter==1.0.0=py37_7 -> ipywidgets -> ipykernel[version='>=4.1|>=4.2.2|>=4.5.1']
defaults/win-64::jupyter==1.0.0=py37_7 -> ipykernel
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> notebook[version='>=4.0'] -> ipykernel
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> ipykernel
conda-forge/noarch::jupyter_contrib_core==0.3.3=py_2 -> notebook[version='>=4.0'] -> ipykernel
defaults/noarch::jupyter_console==6.1.0=py_0 -> ipykernel
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> ipywidgets -> ipykernel[version='>4.9.0|>=4.1|>=4.2.2|>=4.5.1|>=5.1.3|>=4.8.2']
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> notebook -> ipykernel
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> notebook[version='>=4.0'] -> ipykernel
defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0 -> notebook[version='>=4.3.1'] -> ipykernel
defaults/win-64::notebook==6.0.3=py37_0 -> ipykernel
defaults/win-64::widgetsnbextension==3.5.1=py37_0 -> notebook[version='>=4.4.1'] -> ipykernel
conda-forge/win-64::jupyter_highlight_selected_word==0.2.0=py37_1000 -> notebook[version='>=4.0'] -> ipykernel
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> ipywidgets[version='>=7.5.1'] -> ipykernel[version='>=4.5.1']
conda-forge/noarch::ipympl==0.5.6=pyh9f0ad1d_1 -> ipykernel[version='>=4.7']
conda-forge/noarch::ipympl==0.5.6=pyh9f0ad1d_1 -> ipywidgets[version='>=7.5.0,<8.0'] -> ipykernel[version='>=4.5.1']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> ipykernel

Package click conflicts for:
conda-forge/label/cf202003/noarch::geopandas==0.7.0=py_1 -> fiona -> click[version='>=4.0,<8']
defaults/win-64::distributed==2.11.0=py37_0 -> click[version='>=6.6']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> click
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> distributed -> click[version='>=2.0|>=5.1|>=6.6']
conda-forge/noarch::hiplot==0.1.17=py_0 -> flask -> click[version='>=2.0|>=5.1']
conda-forge/label/cf202003/noarch::cligj==0.5.0=py_0 -> click[version='>=4.0']
modin==0.7.3 -> distributed[version='>=2.3.2'] -> click[version='>=6.6']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> click-plugins[version='>=1.0'] -> click[version='>=3.0|>=4.0']
conda-forge/win-64::cookiecutter==1.6.0=py37_1000 -> click[version='>=5.0']
conda-forge/label/cf202003/noarch::click-plugins==1.1.1=py_0 -> click[version='>=3.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> click
defaults/noarch::flask==1.1.1=py_0 -> click[version='>=5.1']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> click[version='>=4.0,<8']
defaults/noarch::dask==2.11.0=py_0 -> distributed[version='>=2.11.0'] -> click[version='>=6.6']
conda-forge/noarch::flask-compress==1.5.0=pyh9f0ad1d_0 -> flask -> click[version='>=2.0|>=5.1']

Package boost-cpp conflicts for:
conda-forge/label/cf202003/win-64::libkml==1.3.0=h7e985d0_1011 -> boost-cpp[version='>=1.72.0,<1.72.1.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> libkml[version='>=1.3.0,<1.4.0a0'] -> boost-cpp[version='>=1.72.0,<1.72.1.0a0']

Package psutil conflicts for:
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> psutil[version='>=5.2|>=5.3']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> psutil
modin==0.7.3 -> distributed[version='>=2.3.2'] -> psutil[version='>=5.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> distributed -> psutil[version='>=5.0|>=5.3|>=5.2']
defaults/noarch::pytest-openfiles==0.4.0=py_0 -> psutil
defaults/noarch::pytest-astropy==0.8.0=py_0 -> pytest-openfiles[version='>=0.3.1'] -> psutil
defaults/win-64::distributed==2.11.0=py37_0 -> psutil[version='>=5.0']
defaults/noarch::dask==2.11.0=py_0 -> distributed[version='>=2.11.0'] -> psutil[version='>=5.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> psutil
anaconda/win-64::spyder==4.1.4=py37_0 -> psutil[version='>=5.3']

Package werkzeug conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> werkzeug
defaults/noarch::flask==1.1.1=py_0 -> werkzeug[version='>=0.14']
conda-forge/noarch::hiplot==0.1.17=py_0 -> flask -> werkzeug[version='>=0.14|>=0.15|>=0.7|>=0.7,<1.0.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> werkzeug
conda-forge/noarch::flask-compress==1.5.0=pyh9f0ad1d_0 -> flask -> werkzeug[version='>=0.14|>=0.15|>=0.7|>=0.7,<1.0.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> flask -> werkzeug[version='>=0.14|>=0.15|>=0.7|>=0.7,<1.0.0']

Package colorama conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> sphinx -> colorama[version='>=0.3.5']
conda-forge/noarch::pbr==5.4.2=py_0 -> pip -> colorama
conda-forge/noarch::optuna==1.0.0=py_0 -> colorlog -> colorama
defaults/win-64::ipython==7.12.0=py37h5ca1d4c_0 -> colorama
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> colorama
defaults/noarch::pytest-astropy-header==0.1.2=py_0 -> pytest[version='>=3.0'] -> colorama
defaults/noarch::ipywidgets==7.5.1=py_0 -> ipython[version='>=4.0.0'] -> colorama
defaults/noarch::jupyter_console==6.1.0=py_0 -> ipython -> colorama
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> ipython[version='>=5.0'] -> colorama
defaults/noarch::pytest-openfiles==0.4.0=py_0 -> pytest[version='>=3.1'] -> colorama
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> ipython[version='>=5.4.0'] -> colorama
defaults/win-64::pytest-remotedata==0.3.2=py37_0 -> pytest[version='>=3.1'] -> colorama
defaults/win-64::coloredlogs==10.0=py37_0 -> colorama
defaults/noarch::pytest-astropy==0.8.0=py_0 -> pytest[version='>=3.1'] -> colorama
anaconda/win-64::python-language-server==0.34.1=py37_0 -> pylint -> colorama
defaults/noarch::numpydoc==0.9.2=py_0 -> sphinx -> colorama[version='>=0.3.5']
conda-forge/win-64::cliff==2.15.0=py37_0 -> cmd2!=0.8.3 -> colorama
defaults/win-64::pytest-arraydiff==0.3=py37h39e3cac_0 -> pytest -> colorama
conda-forge/noarch::hiplot==0.1.17=py_0 -> ipython[version='>=7.0.1'] -> colorama
defaults/win-64::pytest==5.3.5=py37_0 -> colorama
conda-forge/win-64::cmd2==0.9.15=py37_0 -> colorama
defaults/win-64::python==3.7.6=h60c2a47_2 -> pip -> colorama
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> colorama
conda-forge/win-64::colorlog==4.1.0=py37_1 -> colorama
anaconda/win-64::spyder==4.1.4=py37_0 -> pylint[version='>=1.0'] -> colorama[version='>=0.3.5']
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> ipython -> colorama
defaults/win-64::pylint==2.4.4=py37_0 -> colorama
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> coloredlogs -> colorama
defaults/noarch::pytest-doctestplus==0.5.0=py_0 -> pytest[version='>=3.0'] -> colorama

Package decorator conflicts for:
defaults/win-64::traitlets==4.3.3=py37_0 -> decorator
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> decorator
conda-forge/noarch::moviepy==1.0.1=py_0 -> decorator[version='>=4.0.2,<5.0']
defaults/win-64::nbconvert==5.6.1=py37_0 -> traitlets[version='>=4.2'] -> decorator
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> traitlets -> decorator
defaults/noarch::nbformat==5.0.4=py_0 -> traitlets[version='>=4.1'] -> decorator
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> ipython -> decorator
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> traitlets[version='>=4.1'] -> decorator
conda-forge/noarch::hiplot==0.1.17=py_0 -> ipython[version='>=7.0.1'] -> decorator
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> decorator
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> ipython[version='>=5.4.0'] -> decorator
defaults/noarch::networkx==2.4=py_0 -> decorator[version='>=4.3.0']
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> networkx[version='>=2.0'] -> decorator[version='>=3.4.0|>=4.1.0|>=4.3.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> networkx -> decorator[version='>=3.4.0|>=4.1.0|>=4.3.0']
defaults/noarch::ipywidgets==7.5.1=py_0 -> ipython[version='>=4.0.0'] -> decorator
conda-forge/noarch::jupyter_contrib_core==0.3.3=py_2 -> traitlets -> decorator
defaults/win-64::ipython==7.12.0=py37h5ca1d4c_0 -> decorator
defaults/noarch::jupyter_console==6.1.0=py_0 -> ipython -> decorator
defaults/win-64::qtconsole==4.6.0=py37_1 -> traitlets -> decorator
defaults/win-64::notebook==6.0.3=py37_0 -> traitlets[version='>=4.2.1'] -> decorator
defaults/win-64::jupyter_core==4.6.1=py37_0 -> traitlets -> decorator
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> ipython[version='>=5.0'] -> decorator
defaults/win-64::jupyter_client==5.3.4=py37_0 -> traitlets -> decorator
conda-forge/noarch::visions==0.4.4=pyh9f0ad1d_0 -> networkx[version='>=2.4'] -> decorator[version='>=4.3.0']

Package packaging conflicts for:
defaults/win-64::pytest==5.3.5=py37_0 -> packaging
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> packaging
conda-forge/noarch::pbr==5.4.2=py_0 -> pip -> packaging
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> bokeh -> packaging[version='>=16.8']
defaults/noarch::pytest-openfiles==0.4.0=py_0 -> pytest[version='>=3.1'] -> packaging
defaults/noarch::pytest-astropy-header==0.1.2=py_0 -> pytest[version='>=3.0'] -> packaging
defaults/noarch::pytest-doctestplus==0.5.0=py_0 -> pytest[version='>=3.0'] -> packaging
conda-forge/noarch::panel==0.9.5=py_1 -> bokeh[version='>=2.0'] -> packaging[version='>=16.8']
defaults/win-64::python==3.7.6=h60c2a47_2 -> pip -> packaging
defaults/noarch::numpydoc==0.9.2=py_0 -> sphinx -> packaging
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> packaging
defaults/noarch::dask==2.11.0=py_0 -> bokeh[version='>=1.0.0'] -> packaging[version='>=16.8']
defaults/win-64::nbconvert==5.6.1=py37_0 -> bleach -> packaging
defaults/noarch::pytest-astropy==0.8.0=py_0 -> pytest[version='>=3.1'] -> packaging
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> bokeh -> packaging[version='>=16.8']
anaconda/win-64::spyder==4.1.4=py37_0 -> sphinx[version='>=0.6.6'] -> packaging
defaults/win-64::pytest-remotedata==0.3.2=py37_0 -> pytest[version='>=3.1'] -> packaging
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> bokeh[version='>=1.1.0'] -> packaging[version='>=16.8']
defaults/win-64::pytest-arraydiff==0.3=py37h39e3cac_0 -> pytest -> packaging
conda-forge/win-64::bokeh==2.1.0=py37hc8dfbb8_0 -> packaging[version='>=16.8']

Package pyzmq conflicts for:
anaconda/win-64::spyder-kernels==1.9.2=py37_0 -> jupyter_client[version='>=5.3.4'] -> pyzmq[version='>=13']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pyzmq
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> jupyter_client -> pyzmq[version='>=13|>=13.0']
defaults/win-64::notebook==6.0.3=py37_0 -> pyzmq[version='>=17']
anaconda/win-64::spyder-kernels==1.9.2=py37_0 -> pyzmq[version='>=17']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> pyzmq[version='>=17']
anaconda/win-64::spyder==4.1.4=py37_0 -> pyzmq[version='>=17']
defaults/noarch::jupyter_console==6.1.0=py_0 -> jupyter_client -> pyzmq[version='>=13|>=13.0']
conda-forge/win-64::jupyter_highlight_selected_word==0.2.0=py37_1000 -> notebook[version='>=4.0'] -> pyzmq[version='>=17']
defaults/noarch::jupyterlab_server==1.0.6=py_0 -> notebook[version='>=4.2.0'] -> pyzmq[version='>=17']
conda-forge/noarch::jupyter_contrib_core==0.3.3=py_2 -> notebook[version='>=4.0'] -> pyzmq[version='>=17']
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> notebook[version='>=4.0'] -> pyzmq[version='>=17']
defaults/win-64::jupyter_client==5.3.4=py37_0 -> pyzmq[version='>=13']
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> notebook[version='>=4.0'] -> pyzmq[version='>=17']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pyzmq
defaults/win-64::jupyter==1.0.0=py37_7 -> notebook -> pyzmq[version='>=17']
defaults/win-64::widgetsnbextension==3.5.1=py37_0 -> notebook[version='>=4.4.1'] -> pyzmq[version='>=17']
defaults/win-64::qtconsole==4.6.0=py37_1 -> jupyter_client[version='>=4.1'] -> pyzmq[version='>=13|>=13.0']
defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0 -> notebook[version='>=4.3.1'] -> pyzmq[version='>=17']
defaults/win-64::notebook==6.0.3=py37_0 -> jupyter_client[version='>=5.3.4'] -> pyzmq[version='>=13']
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> notebook[version='>=4.0'] -> pyzmq[version='>=17']
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> notebook -> pyzmq[version='>=17']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> jupyter_client -> pyzmq[version='>=13|>=13.0|>=17']

Package jsonschema conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> jupyterlab_server -> jsonschema[version='>=2.4,!=2.5.0|>=3.0.1']
defaults/win-64::nbconvert==5.6.1=py37_0 -> nbformat[version='>=4.4'] -> jsonschema[version='>=2.4,!=2.5.0']
defaults/noarch::ipywidgets==7.5.1=py_0 -> nbformat[version='>=4.2.0'] -> jsonschema[version='>=2.4,!=2.5.0']
defaults/win-64::anaconda-client==1.7.2=py37_0 -> nbformat -> jsonschema[version='>=2.4,!=2.5.0']
defaults/win-64::notebook==6.0.3=py37_0 -> nbformat -> jsonschema[version='>=2.4,!=2.5.0']
defaults/noarch::nbformat==5.0.4=py_0 -> jsonschema[version='>=2.4,!=2.5.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> jsonschema
defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0 -> jupyterlab_server[version='>=1.0.0,<2.0.0'] -> jsonschema[version='>=3.0.1']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> jsonschema
defaults/noarch::jupyterlab_server==1.0.6=py_0 -> jsonschema[version='>=3.0.1']

Package wcwidth conflicts for:
conda-forge/win-64::cliff==2.15.0=py37_0 -> cmd2!=0.8.3 -> wcwidth
defaults/win-64::pytest-arraydiff==0.3=py37h39e3cac_0 -> pytest -> wcwidth
defaults/noarch::pytest-astropy-header==0.1.2=py_0 -> pytest[version='>=3.0'] -> wcwidth
conda-forge/win-64::cmd2==0.9.15=py37_0 -> wcwidth
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> wcwidth
defaults/noarch::jupyter_console==6.1.0=py_0 -> prompt_toolkit[version='>=2.0.0,<3.1.0,!=3.0.0,!=3.0.1'] -> wcwidth
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> wcwidth
defaults/noarch::pytest-doctestplus==0.5.0=py_0 -> pytest[version='>=3.0'] -> wcwidth
defaults/win-64::pytest-remotedata==0.3.2=py37_0 -> pytest[version='>=3.1'] -> wcwidth
defaults/win-64::pytest==5.3.5=py37_0 -> wcwidth
defaults/noarch::prompt_toolkit==3.0.3=py_0 -> wcwidth
defaults/noarch::pytest-astropy==0.8.0=py_0 -> pytest[version='>=3.1'] -> wcwidth
defaults/win-64::ipython==7.12.0=py37h5ca1d4c_0 -> prompt_toolkit[version='>=2.0.0,<4,!=3.0.0,!=3.0.1'] -> wcwidth
defaults/noarch::pytest-openfiles==0.4.0=py_0 -> pytest[version='>=3.1'] -> wcwidth

Package pyopenssl conflicts for:
defaults/noarch::botocore==1.12.189=py_0 -> urllib3[version='>=1.20,<1.26'] -> pyopenssl[version='>=0.14']
conda-forge/noarch::requests==2.24.0=pyh9f0ad1d_0 -> urllib3[version='>=1.21.1,<1.26,!=1.25.0,!=1.25.1'] -> pyopenssl[version='>=0.14']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> urllib3 -> pyopenssl[version='>=0.14']
intel/win-64::urllib3==1.24.1=py37_2 -> pyopenssl[version='>=0.14']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pyopenssl
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pyopenssl

Package libxslt conflicts for:
defaults/win-64::lxml==4.5.0=py37h1350720_0 -> libxslt[version='>=1.1.33,<2.0a0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> libxslt
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> lxml -> libxslt[version='>=1.1.32,<2.0a0|>=1.1.33,<2.0a0|>=1.1.34,<2.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> libxslt
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> lxml -> libxslt[version='>=1.1.32,<2.0a0|>=1.1.33,<2.0a0|>=1.1.34,<2.0a0']

Package qtawesome conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> qtawesome
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> qtawesome[version='>=0.5.7']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> qtawesome
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> spyder -> qtawesome[version='>=0.4.1|>=0.5.7']
anaconda/win-64::spyder==4.1.4=py37_0 -> qtawesome[version='>=0.5.7']

Package astropy conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> astropy
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> astropy
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> astropy[version='>=4.0']

Package intervaltree conflicts for:
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> intervaltree
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> intervaltree
anaconda/win-64::spyder==4.1.4=py37_0 -> intervaltree
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> spyder -> intervaltree

Package libxml2 conflicts for:
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> libspatialite[version='>=4.3.0a,<4.4.0a0'] -> libxml2[version='>=2.9.4,<2.10.0a0|>=2.9.8,<2.10.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> libxml2[version='>=2.9.9,<2.10.0a0']
defaults/win-64::lxml==4.5.0=py37h1350720_0 -> libxslt[version='>=1.1.33,<2.0a0'] -> libxml2[version='>=2.9.10,<2.10.0a0|>=2.9.8,<2.10.0a0']
conda-forge/label/cf202003/win-64::libspatialite==4.3.0a=h01b1fc4_1030 -> libxml2[version='>=2.9.9,<2.10.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> libxml2
intel/win-64::python-libarchive-c==2.8=py37_13 -> libarchive -> libxml2[version='>=2.9.10,<2.10.0a0|>=2.9.4,<2.10.0a0|>=2.9.7,<2.10.0a0|>=2.9.8,<2.10.0a0|>=2.9.9,<2.10.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> libxml2[version='>=2.9.9,<2.10.0a0']
defaults/win-64::libarchive==3.3.3=h0643e63_5 -> libxml2[version='>=2.9.8,<2.10.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> libarchive -> libxml2[version='>=2.9.10,<2.10.0a0|>=2.9.4,<2.10.0a0|>=2.9.7,<2.10.0a0|>=2.9.8,<2.10.0a0|>=2.9.9,<2.10.0a0']
conda-forge/label/cf202003/win-64::postgresql==11.5=h06f7779_1 -> libxml2[version='>=2.9.9,<2.10.0a0']
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> lxml -> libxml2[version='>=2.9.10,<2.10.0a0|>=2.9.9,<2.10.0a0|>=2.9.8,<2.10.0a0|>=2.9.7,<2.10.0a0|>=2.9.4,<2.10.0a0']
defaults/win-64::libxslt==1.1.33=h579f668_0 -> libxml2[version='>=2.9.8,<2.10.0a0']
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> libxml2[version='>=2.9.9,<2.10.0a0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> libxml2
defaults/win-64::lxml==4.5.0=py37h1350720_0 -> libxml2[version='>=2.9.9,<2.10.0a0']

Package more-itertools conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pytest -> more-itertools[version='>=4.0,<6.0|>=4.0|>=4.0.0']
defaults/noarch::pytest-astropy-header==0.1.2=py_0 -> pytest[version='>=3.0'] -> more-itertools[version='>=4.0,<6.0|>=4.0|>=4.0.0']
conda-forge/win-64::importlib-metadata==1.6.1=py37hc8dfbb8_0 -> zipp[version='>=0.5'] -> more-itertools
defaults/win-64::pytest==5.3.5=py37_0 -> more-itertools[version='>=4.0']
defaults/win-64::pytest-remotedata==0.3.2=py37_0 -> pytest[version='>=3.1'] -> more-itertools[version='>=4.0,<6.0|>=4.0|>=4.0.0']
defaults/noarch::pytest-openfiles==0.4.0=py_0 -> pytest[version='>=3.1'] -> more-itertools[version='>=4.0,<6.0|>=4.0|>=4.0.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> more-itertools
defaults/noarch::jaraco.itertools==5.0.0=py_0 -> more-itertools[version='>=4.0.0']
defaults/win-64::pytest-arraydiff==0.3=py37h39e3cac_0 -> pytest -> more-itertools[version='>=4.0,<6.0|>=4.0|>=4.0.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> more-itertools
defaults/noarch::pytest-astropy==0.8.0=py_0 -> pytest[version='>=3.1'] -> more-itertools[version='>=4.0,<6.0|>=4.0|>=4.0.0']
defaults/win-64::importlib_metadata==1.5.0=py37_0 -> zipp[version='>=0.5'] -> more-itertools
defaults/noarch::pytest-doctestplus==0.5.0=py_0 -> pytest[version='>=3.0'] -> more-itertools[version='>=4.0,<6.0|>=4.0|>=4.0.0']

Package urllib3 conflicts for:
boto3 -> botocore[version='>=1.17.48,<1.18.0'] -> urllib3[version='>=1.20,<1.24|>=1.20,<1.25|>=1.20,<1.26']
defaults/win-64::anaconda-client==1.7.2=py37_0 -> requests[version='>=2.9.1'] -> urllib3[version='>=1.21.1|>=1.21.1,<1.23|>=1.21.1,<1.24|>=1.21.1,<1.25|>=1.21.1,<1.26,!=1.25.0,!=1.25.1']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> requests -> urllib3[version='>=1.21.1|>=1.21.1,<1.23|>=1.21.1,<1.24|>=1.21.1,<1.25|>=1.21.1,<1.26,!=1.25.0,!=1.25.1']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> urllib3
conda-forge/noarch::moviepy==1.0.1=py_0 -> requests[version='>=2.8.1,<3.0'] -> urllib3[version='>=1.21.1|>=1.21.1,<1.23|>=1.21.1,<1.24|>=1.21.1,<1.25|>=1.21.1,<1.26,!=1.25.0,!=1.25.1']
anaconda/win-64::s3transfer==0.1.13=py37_0 -> botocore[version='>=1.3.0,<2.0.0'] -> urllib3[version='>=1.20,<1.24|>=1.20,<1.25|>=1.20,<1.26']
conda-forge/win-64::cookiecutter==1.6.0=py37_1000 -> requests[version='>=2.18.0'] -> urllib3[version='>=1.21.1|>=1.21.1,<1.23|>=1.21.1,<1.24|>=1.21.1,<1.25|>=1.21.1,<1.26,!=1.25.0,!=1.25.1']
conda-forge/noarch::requests==2.24.0=pyh9f0ad1d_0 -> urllib3[version='>=1.21.1,<1.26,!=1.25.0,!=1.25.1']
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> requests[version='>=2.23.0'] -> urllib3[version='>=1.21.1|>=1.21.1,<1.26,!=1.25.0,!=1.25.1']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> requests -> urllib3[version='>=1.21.1|>=1.21.1,<1.23|>=1.21.1,<1.24|>=1.21.1,<1.25|>=1.21.1,<1.26,!=1.25.0,!=1.25.1']
conda-forge/noarch::pyct==0.4.6=py_0 -> requests -> urllib3[version='>=1.21.1|>=1.21.1,<1.23|>=1.21.1,<1.24|>=1.21.1,<1.25|>=1.21.1,<1.26,!=1.25.0,!=1.25.1']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> urllib3
defaults/noarch::botocore==1.12.189=py_0 -> urllib3[version='>=1.20,<1.26']
defaults/noarch::anaconda-project==0.8.4=py_0 -> requests -> urllib3[version='>=1.21.1|>=1.21.1,<1.23|>=1.21.1,<1.24|>=1.21.1,<1.25|>=1.21.1,<1.26,!=1.25.0,!=1.25.1']

Package m2w64-gmp conflicts for:
defaults/win-64::m2w64-gettext==0.19.7=2 -> m2w64-gcc-libs -> m2w64-gmp
anaconda/win-64::libxgboost==0.90=1 -> m2w64-gcc-libs -> m2w64-gmp
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> m2w64-gmp
conda-forge/win-64::libffi==3.2.1=h6538335_1007 -> m2w64-gcc-libs -> m2w64-gmp
defaults/win-64::m2w64-gcc-libgfortran==5.3.0=6 -> m2w64-gcc-libs-core -> m2w64-gmp
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> m2w64-gmp
defaults/win-64::pywinpty==0.5.7=py37_0 -> m2w64-gcc-libs -> m2w64-gmp
defaults/win-64::m2w64-gcc-libs-core==5.3.0=7 -> m2w64-gmp
defaults/win-64::m2w64-xz==5.2.2=2 -> m2w64-gcc-libs -> m2w64-gmp
defaults/win-64::m2w64-gcc-libs==5.3.0=7 -> m2w64-gmp

Package anaconda-project conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> anaconda-project
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> anaconda-project

Package sphinxcontrib-htmlhelp conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> sphinx -> sphinxcontrib-htmlhelp
anaconda/win-64::spyder==4.1.4=py37_0 -> sphinx[version='>=0.6.6'] -> sphinxcontrib-htmlhelp
defaults/noarch::numpydoc==0.9.2=py_0 -> sphinx -> sphinxcontrib-htmlhelp
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> sphinxcontrib-htmlhelp

Package cytoolz conflicts for:
defaults/win-64::distributed==2.11.0=py37_0 -> cytoolz[version='>=0.7.4']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> dask -> cytoolz[version='>=0.7.3|>=0.8.2|>=0.7.4']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> dask[version='>=0.18.0'] -> cytoolz[version='>=0.7.3|>=0.8.2']
defaults/noarch::dask==2.11.0=py_0 -> cytoolz[version='>=0.7.3']
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> cytoolz[version='>=0.7.3']
defaults/noarch::dask==2.11.0=py_0 -> distributed[version='>=2.11.0'] -> cytoolz[version='>=0.7.4|>=0.8.2']
modin==0.7.3 -> dask[version='>=2.1.0'] -> cytoolz[version='>=0.7.3|>=0.8.2|>=0.7.4']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> cytoolz
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> cytoolz

Package atomicwrites conflicts for:
defaults/noarch::pytest-doctestplus==0.5.0=py_0 -> pytest[version='>=3.0'] -> atomicwrites[version='>=1.0']
defaults/win-64::pytest-arraydiff==0.3=py37h39e3cac_0 -> pytest -> atomicwrites[version='>=1.0']
anaconda/win-64::spyder==4.1.4=py37_0 -> atomicwrites[version='>=1.2.0']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> atomicwrites[version='>=1.2.0']
defaults/win-64::pytest==5.3.5=py37_0 -> atomicwrites[version='>=1.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> atomicwrites
defaults/noarch::pytest-openfiles==0.4.0=py_0 -> pytest[version='>=3.1'] -> atomicwrites[version='>=1.0']
defaults/noarch::pytest-astropy==0.8.0=py_0 -> pytest[version='>=3.1'] -> atomicwrites[version='>=1.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> atomicwrites
defaults/win-64::pytest-remotedata==0.3.2=py37_0 -> pytest[version='>=3.1'] -> atomicwrites[version='>=1.0']
defaults/noarch::pytest-astropy-header==0.1.2=py_0 -> pytest[version='>=3.0'] -> atomicwrites[version='>=1.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pytest -> atomicwrites[version='>=1.0|>=1.2.0']

Package requests conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> anaconda-client -> requests[version='>2.0.0|>=1.2.3|>=2.9.1|>=2.5.0|>=2.0.0']
anaconda/win-64::spyder==4.1.4=py37_0 -> sphinx[version='>=0.6.6'] -> requests[version='>2.0.0|>=2.0.0|>=2.5.0']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> requests
defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0 -> jupyterlab_server[version='>=1.0.0,<2.0.0'] -> requests
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> requests
conda-forge/win-64::cookiecutter==1.6.0=py37_1000 -> requests[version='>=2.18.0']
conda-forge/noarch::colorcet==2.0.1=py_0 -> pyct[version='>=0.4.4'] -> requests
defaults/win-64::anaconda-client==1.7.2=py37_0 -> requests[version='>=2.9.1']
conda-forge/noarch::panel==0.9.5=py_1 -> pyct[version='>=0.4.4'] -> requests
defaults/noarch::anaconda-project==0.8.4=py_0 -> anaconda-client -> requests[version='>=2.9.1']
conda-forge/noarch::pbr==5.4.2=py_0 -> pip -> requests
defaults/noarch::numpydoc==0.9.2=py_0 -> sphinx -> requests[version='>2.0.0|>=2.0.0|>=2.5.0']
conda-forge/noarch::moviepy==1.0.1=py_0 -> requests[version='>=2.8.1,<3.0']
conda-forge/noarch::pyct==0.4.6=py_0 -> requests
defaults/win-64::python==3.7.6=h60c2a47_2 -> pip -> requests
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> requests
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> bokeh -> requests[version='>=1.2.3']
defaults/noarch::anaconda-project==0.8.4=py_0 -> requests
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> requests[version='>=2.23.0']

Package zeromq conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> zeromq
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> zeromq
defaults/win-64::notebook==6.0.3=py37_0 -> pyzmq[version='>=17'] -> zeromq[version='>=4.2.5,<4.2.6.0a0|>=4.3.1,<4.4.0a0|>=4.3.2,<4.3.3.0a0|>=4.3.1,<4.3.2.0a0']
anaconda/win-64::spyder==4.1.4=py37_0 -> pyzmq[version='>=17'] -> zeromq[version='>=4.2.5,<4.2.6.0a0|>=4.3.1,<4.4.0a0|>=4.3.2,<4.3.3.0a0|>=4.3.1,<4.3.2.0a0']
defaults/win-64::pyzmq==18.1.1=py37ha925a31_0 -> zeromq[version='>=4.3.1,<4.3.2.0a0']
defaults/win-64::jupyter_client==5.3.4=py37_0 -> pyzmq[version='>=13'] -> zeromq[version='>=4.2.5,<4.2.6.0a0|>=4.3.1,<4.4.0a0|>=4.3.2,<4.3.3.0a0|>=4.3.1,<4.3.2.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pyzmq -> zeromq[version='>=4.2.5,<4.2.6.0a0|>=4.3.1,<4.4.0a0|>=4.3.2,<4.3.3.0a0|>=4.3.1,<4.3.2.0a0']
anaconda/win-64::spyder-kernels==1.9.2=py37_0 -> pyzmq[version='>=17'] -> zeromq[version='>=4.2.5,<4.2.6.0a0|>=4.3.1,<4.4.0a0|>=4.3.2,<4.3.3.0a0|>=4.3.1,<4.3.2.0a0']

Package pandas conflicts for:
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> pandas[version='>=0.25.3,!=1.0.1,!=1.0.0,!=1.0.2']
defaults/win-64::bkcharts==0.2=py37_0 -> pandas
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pandas
modin==0.7.3 -> dask[version='>=2.1.0'] -> pandas[version='>=0.21.0|>=0.23.0']
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> pandas[version='>=0.20.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pandas
plotly/noarch::plotly_express==0.4.1=py_0 -> statsmodels[version='>=0.9.0'] -> pandas[version='>=0.14|>=0.21']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> dask -> pandas[version='>=0.14|>=0.14.0|>=0.19.0|>=0.21.0|>=0.23.0|>=0.22.0|>=0.21']
conda-forge/noarch::phik==0.10.0=py_0 -> pandas[version='>=0.23.4']
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> missingno[version='>=0.4.2'] -> pandas[version='>=0.23.4|>=0.25.3']
conda-forge/noarch::pandas-summary==0.0.41=py_1 -> pandas
defaults/noarch::seaborn==0.10.0=py_0 -> pandas[version='>=0.22.0']
plotly/noarch::plotly_express==0.4.1=py_0 -> pandas[version='>=0.20.0']
conda-forge/noarch::visions==0.4.4=pyh9f0ad1d_0 -> pandas[version='>=0.25.3']
conda-forge/noarch::missingno==0.4.2=py_0 -> pandas
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> pandas[version='>=0.20.3']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> bokeh -> pandas[version='>=0.18.0|>=0.19.0|>=0.21.0|>=0.23.0|>=0.25|>=0.24|>=0.19.2']
defaults/win-64::statsmodels==0.11.0=py37he774522_0 -> pandas[version='>=0.21']
conda-forge/noarch::missingno==0.4.2=py_0 -> seaborn -> pandas[version='>=0.14.0|>=0.22.0']
modin==0.7.3 -> pandas==1.0.3
conda-forge/label/cf202003/noarch::geopandas==0.7.0=py_1 -> pandas[version='>=0.23']
conda-forge/noarch::xarray==0.15.1=py_0 -> pandas[version='>=0.25']
defaults/noarch::dask==2.11.0=py_0 -> pandas[version='>=0.23.0']

Package hypothesis conflicts for:
conda-forge/win-64::cmd2==0.9.15=py37_0 -> attrs -> hypothesis
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> hypothesis
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> attrs -> hypothesis
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> attrs[version='>=17'] -> hypothesis
defaults/noarch::pytest-astropy==0.8.0=py_0 -> hypothesis
defaults/win-64::astropy==4.0=py37he774522_0 -> pytest-astropy -> hypothesis

Package pyasn1 conflicts for:
anaconda/win-64::spyder==4.1.4=py37_0 -> paramiko[version='>=2.4.0'] -> pyasn1[version='>=0.1.7']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pyasn1

Package m2w64-gcc-libs-core conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> m2w64-gcc-libs-core
defaults/win-64::m2w64-gcc-libgfortran==5.3.0=6 -> m2w64-gcc-libs-core
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> m2w64-gcc-libs-core
defaults/win-64::m2w64-xz==5.2.2=2 -> m2w64-gcc-libs -> m2w64-gcc-libs-core
defaults/win-64::pywinpty==0.5.7=py37_0 -> m2w64-gcc-libs -> m2w64-gcc-libs-core
defaults/win-64::m2w64-gcc-libs==5.3.0=7 -> m2w64-gcc-libs-core
anaconda/win-64::libxgboost==0.90=1 -> m2w64-gcc-libs -> m2w64-gcc-libs-core
defaults/win-64::m2w64-gettext==0.19.7=2 -> m2w64-gcc-libs -> m2w64-gcc-libs-core
conda-forge/win-64::libffi==3.2.1=h6538335_1007 -> m2w64-gcc-libs -> m2w64-gcc-libs-core

Package pytest-openfiles conflicts for:
defaults/win-64::astropy==4.0=py37he774522_0 -> pytest-astropy -> pytest-openfiles[version='>=0.2|>=0.3.0|>=0.3|>=0.3.1']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pytest-openfiles
defaults/noarch::pytest-astropy==0.8.0=py_0 -> pytest-openfiles[version='>=0.3.1']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pytest-astropy -> pytest-openfiles[version='>=0.2|>=0.3.0|>=0.3|>=0.3.1']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pytest-openfiles

Package blosc conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pytables -> blosc[version='>=1.14.3,<2.0a0|>=1.14.4,<2.0a0|>=1.15.0,<2.0a0|>=1.16.3,<2.0a0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> blosc
conda-forge/label/cf202003/win-64::pytables==3.5.2=py37h6a20dd8_0 -> blosc[version='>=1.16.3,<2.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> blosc

Package brotlipy conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> urllib3 -> brotlipy[version='>=0.6.0']
defaults/noarch::botocore==1.12.189=py_0 -> urllib3[version='>=1.20,<1.26'] -> brotlipy[version='>=0.6.0']
conda-forge/noarch::requests==2.24.0=pyh9f0ad1d_0 -> urllib3[version='>=1.21.1,<1.26,!=1.25.0,!=1.25.1'] -> brotlipy[version='>=0.6.0']
conda-forge/noarch::flask-compress==1.5.0=pyh9f0ad1d_0 -> brotlipy
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> brotlipy
conda-forge/noarch::hiplot==0.1.17=py_0 -> flask-compress -> brotlipy

Package dask conflicts for:
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> dask[version='>=0.18.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> dask
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> scikit-image -> dask[version='>=0.5']
modin==0.7.3 -> dask[version='>=2.1.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> dask

Package proj conflicts for:
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> geotiff[version='>=1.5.1,<1.5.2.0a0'] -> proj[version='>=6.2.1,<6.2.2.0a0|>=7.0.1,<7.0.2.0a0']
conda-forge/label/cf202003/noarch::geopandas==0.7.0=py_1 -> pyproj[version='>=2.2.0'] -> proj[version='>=6.2.1,<6.2.2.0a0|>=7.0.1,<7.0.2.0a0']

Package geotiff conflicts for:
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> geotiff[version='>=1.5.1,<1.5.2.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> geotiff[version='>=1.5.1,<1.5.2.0a0']
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> geotiff[version='>=1.5.1,<1.5.2.0a0']

Package contextvars conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> distributed -> contextvars
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> contextvars
defaults/noarch::dask==2.11.0=py_0 -> distributed[version='>=2.11.0'] -> contextvars
modin==0.7.3 -> distributed[version='>=2.3.2'] -> contextvars

Package hdf5 conflicts for:
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> hdf5[version='>=1.10.4,<1.10.5.0a0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> hdf5
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> hdf5[version='>=1.10.4,<1.10.5.0a0']
conda-forge/label/cf202003/win-64::pytables==3.5.2=py37h6a20dd8_0 -> hdf5[version='>=1.10.4,<1.10.5.0a0']
conda-forge/label/cf202003/win-64::libnetcdf==4.6.2=h396784b_1001 -> hdf5[version='>=1.10.4,<1.10.5.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> hdf5
conda-forge/label/cf202003/win-64::kealib==1.4.10=heacb130_1003 -> hdf5[version='>=1.10.4,<1.10.5.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> hdf5[version='>=1.10.4,<1.10.5.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> h5py -> hdf5[version='1.10.1|1.10.2.*|1.10.3.*|>=1.10.1,<1.10.2.0a0|>=1.10.2,<1.10.3.0a0|>=1.10.4,<1.10.5.0a0|>=1.8.20,<1.9.0a0|>=1.8.18,<1.8.19.0a0|>=1.8.18,<1.9.0a0|1.8.17|1.8.16']
conda-forge/label/cf202003/win-64::h5py==2.9.0=nompi_py37h9dfa0df_1103 -> hdf5[version='>=1.10.4,<1.10.5.0a0']

Package tblib conflicts for:
defaults/win-64::distributed==2.11.0=py37_0 -> tblib[version='>=1.6.0']
modin==0.7.3 -> distributed[version='>=2.3.2'] -> tblib[version='>=1.6.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> tblib
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> distributed -> tblib[version='>=1.6.0']
defaults/noarch::dask==2.11.0=py_0 -> distributed[version='>=2.11.0'] -> tblib[version='>=1.6.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> tblib

Package cmd2 conflicts for:
conda-forge/win-64::cliff==2.15.0=py37_0 -> cmd2!=0.8.3
conda-forge/noarch::optuna==1.0.0=py_0 -> cliff -> cmd2!=0.8.3

Package python-editor conflicts for:
conda-forge/noarch::optuna==1.0.0=py_0 -> alembic -> python-editor[version='>=0.3']
conda-forge/noarch::alembic==1.4.0=py_0 -> python-editor[version='>=0.3']

Package scikit-image conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> scikit-image
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> scikit-image

Package gettext conflicts for:
conda-forge/win-64::pygobject==3.36.1=py37hf6b2db1_0 -> glib[version='>=2.58.3,<3.0a0'] -> gettext[version='>=0.19.8.1,<1.0a0']
conda-forge/label/cf202003/win-64::poppler==0.67.0=h1707e21_8 -> glib[version='>=2.58.3,<3.0a0'] -> gettext[version='>=0.19.8.1,<1.0a0']
conda-forge/win-64::gobject-introspection==1.64.1=py37hf0dd101_1 -> glib[version='>=2.58.3,<3.0a0'] -> gettext[version='>=0.19.8.1,<1.0a0']
conda-forge/win-64::glib==2.64.3=he4de6d7_0 -> gettext[version='>=0.19.8.1,<1.0a0']

Package xlwings conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> xlwings
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> xlwings

Package pexpect conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pexpect
anaconda/win-64::spyder==4.1.4=py37_0 -> pexpect[version='>=4.4.0']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> pexpect[version='>=4.4.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> spyder -> pexpect[version='>=4.4.0']

Package postgresql conflicts for:
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> postgresql
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> postgresql
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> postgresql

Package networkx conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> scikit-image -> networkx[version='>=1.8,<2.0|>=1.8|>=2.0']
conda-forge/noarch::visions==0.4.4=pyh9f0ad1d_0 -> networkx[version='>=2.4']
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> networkx[version='>=2.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> networkx
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> networkx
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> visions[version='>=0.4.4'] -> networkx[version='>=2.4']

Package pandoc conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pandoc
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> nbconvert -> pandoc[version='>=1.12.1|>=1.12.1,<2.0.0']
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> nbconvert[version='>=4.2'] -> pandoc[version='>=1.12.1|>=1.12.1,<2.0.0']
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> nbconvert -> pandoc[version='>=1.12.1|>=1.12.1,<2.0.0']
defaults/win-64::jupyter==1.0.0=py37_7 -> nbconvert -> pandoc[version='>=1.12.1|>=1.12.1,<2.0.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pandoc
defaults/win-64::nbconvert==5.6.1=py37_0 -> pandoc[version='>=1.12.1']
anaconda/win-64::spyder==4.1.4=py37_0 -> nbconvert[version='>=4.0'] -> pandoc[version='>=1.12.1|>=1.12.1,<2.0.0']
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> nbconvert -> pandoc[version='>=1.12.1|>=1.12.1,<2.0.0']
defaults/win-64::notebook==6.0.3=py37_0 -> nbconvert -> pandoc[version='>=1.12.1|>=1.12.1,<2.0.0']

Package libpq conflicts for:
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> postgresql -> libpq[version='10.3|10.3|10.4|10.4|10.5|10.5|11.1|11.1|11.2|11.2|12.2|11.5',build='hb0bdaea_1|h0990ea7_0|h5fe2233_0|h1ac0bd9_0|h1ac0bd9_0|h5fe2233_0|h3235a2c_0|h3235a2c_0|h3235a2c_0|h4410098_0|h4410098_0|hc4dcbb0_0']
conda-forge/label/cf202003/win-64::postgresql==11.5=h06f7779_1 -> libpq==11.5=hb0bdaea_1
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> libpq[version='>=11.5,<12.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> libpq[version='>=11.5,<12.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> libpq[version='>=11.5,<12.0a0']

Package joblib conflicts for:
conda-forge/win-64::tslearn==0.3.1=py37hbc2f12b_0 -> joblib
conda-forge/noarch::optuna==1.0.0=py_0 -> joblib
defaults/win-64::scikit-learn==0.23.1=py37h25d0782_0 -> joblib[version='>=0.11']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> nltk -> joblib[version='>=0.11']
conda-forge/win-64::scikit-surprise==1.1.0=py37hbc2f12b_1002 -> joblib
conda-forge/noarch::phik==0.10.0=py_0 -> joblib[version='>=0.14.1']
conda-forge/noarch::imbalanced-learn==0.7.0=py_1 -> joblib[version='>=0.11']
anaconda/win-64::py-xgboost==0.90=py37_1 -> scikit-learn -> joblib[version='>=0.11']
conda-forge/win-64::tslearn==0.3.1=py37hbc2f12b_0 -> scikit-learn -> joblib[version='>=0.11']
anaconda/noarch::pynndescent==0.4.8=py_1 -> scikit-learn[version='>=0.19'] -> joblib[version='>=0.11']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> joblib
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> phik[version='>=0.9.10'] -> joblib[version='>=0.14.1']
conda-forge/win-64::umap-learn==0.4.4=py37hc8dfbb8_0 -> scikit-learn[version='>=0.20'] -> joblib[version='>=0.11']

Package terminado conflicts for:
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> notebook[version='>=4.0'] -> terminado[version='>=0.8.1']
defaults/win-64::widgetsnbextension==3.5.1=py37_0 -> notebook[version='>=4.4.1'] -> terminado[version='>=0.8.1']
defaults/win-64::jupyter==1.0.0=py37_7 -> notebook -> terminado[version='>=0.8.1']
defaults/win-64::notebook==6.0.3=py37_0 -> terminado[version='>=0.8.1']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> terminado
defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0 -> notebook[version='>=4.3.1'] -> terminado[version='>=0.8.1']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> terminado
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> terminado
conda-forge/noarch::jupyter_contrib_core==0.3.3=py_2 -> notebook[version='>=4.0'] -> terminado[version='>=0.8.1']
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> notebook -> terminado[version='>=0.8.1']
defaults/noarch::jupyterlab_server==1.0.6=py_0 -> notebook[version='>=4.2.0'] -> terminado[version='>=0.8.1']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> notebook -> terminado[version='>=0.8.1']
conda-forge/win-64::jupyter_highlight_selected_word==0.2.0=py37_1000 -> notebook[version='>=4.0'] -> terminado[version='>=0.8.1']
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> notebook[version='>=4.0'] -> terminado[version='>=0.8.1']
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> notebook[version='>=4.0'] -> terminado[version='>=0.8.1']

Package prompt-toolkit conflicts for:
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> ipython[version='>=5.0'] -> prompt-toolkit[version='!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0']
defaults/noarch::jupyter_console==6.1.0=py_0 -> ipython -> prompt-toolkit[version='!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0|>=3.0.6,<3.0.7.0a0|>=3.0.5,<3.0.6.0a0|>=3.0.4,<3.0.5.0a0|>=3.0.7,<3.0.8.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> ipython -> prompt-toolkit[version='!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0|>=3.0.6,<3.0.7.0a0|>=3.0.5,<3.0.6.0a0|>=3.0.4,<3.0.5.0a0|>=3.0.7,<3.0.8.0a0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> prompt-toolkit
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> ipython -> prompt-toolkit[version='!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0']
defaults/win-64::ipython==7.12.0=py37h5ca1d4c_0 -> prompt_toolkit[version='>=2.0.0,<4,!=3.0.0,!=3.0.1'] -> prompt-toolkit[version='>=3.0.4,<3.0.5.0a0|>=3.0.5,<3.0.6.0a0|>=3.0.6,<3.0.7.0a0|>=3.0.7,<3.0.8.0a0']
conda-forge/noarch::hiplot==0.1.17=py_0 -> ipython[version='>=7.0.1'] -> prompt-toolkit[version='!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0']
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> ipython[version='>=5.4.0'] -> prompt-toolkit[version='!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0']
defaults/noarch::ipywidgets==7.5.1=py_0 -> ipython[version='>=4.0.0'] -> prompt-toolkit[version='!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0']

Package openmp conflicts for:
intel/win-64::mkl_random==1.1.0=py37ha68da19_0 -> icc_rt[version='>=16.0.3'] -> openmp=2018
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> icc_rt -> openmp=2018
intel/win-64::mkl_fft==1.0.15=py37ha68da19_3 -> icc_rt[version='>=16.0.3'] -> openmp=2018
intel/win-64::numba==0.48.0=np117py37_0 -> icc_rt -> openmp
intel/win-64::numexpr==2.7.0=py37_1 -> mkl[version='>=2018.0.0'] -> openmp=2018
intel/win-64::mkl-service==2.3.0=py37_0 -> mkl -> openmp=2018
intel/win-64::numpy-base==1.17.4=py37_4 -> icc_rt[version='>=16.0.3'] -> openmp=2018

Package dask-core conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> dask -> dask-core[version='0.15.2.*|0.15.3.*|0.15.4.*|0.16.0.*|0.16.1.*|0.17.0.*|0.17.1.*|0.17.2.*|0.17.3.*|0.17.4.*|0.17.5.*|0.18.0.*|0.18.1.*|0.18.2.*|0.19.0.*|0.19.1.*|0.19.2.*|0.19.3.*|0.19.4.*|0.20.0.*|0.20.1.*|0.20.2.*|1.0.0.*|1.1.1.*|1.1.2.*|1.1.3.*|1.1.4.*|1.1.5.*|1.2.0.*|1.2.2.*|2.0.0.*|2.1.0.*|2.10.0.*|2.10.1.*|2.11.0.*|2.12.0.*|2.13.0.*|2.14.0.*|2.15.0.*|2.16.0.*|2.17.0.*|2.17.2.*|2.18.1.*|2.19.0.*|2.20.0.*|2.22.0.*|2.23.0.*|2.24.0.*|2.9.2.*|2.9.1.*|2.9.0.*|2.8.1.*|2.8.0.*|2.7.0.*|2.6.0.*|2.5.2.*|2.5.0.*|2.4.0.*|2.3.0.*|2.2.0.*|2.25.0.*|>=2.9.0|>=2.7.0|>=2.5.2|>=2|>=0.18.0|>=0.17.0|>=0.16.0|>=0.15.2|>=0.15.0|>=0.9.0|>=0.15']
defaults/noarch::dask==2.11.0=py_0 -> dask-core=2.11.0
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> dask-core
defaults/win-64::distributed==2.11.0=py37_0 -> dask-core[version='>=2.9.0']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> dask[version='>=0.18.0'] -> dask-core[version='0.18.0.*|0.18.1.*|0.18.2.*|0.19.0.*|0.19.1.*|0.19.2.*|0.19.3.*|0.19.4.*|0.20.0.*|0.20.1.*|0.20.2.*|1.0.0.*|1.1.1.*|1.1.2.*|1.1.3.*|1.1.4.*|1.1.5.*|1.2.0.*|1.2.2.*|2.0.0.*|2.1.0.*|2.10.0.*|2.10.1.*|2.11.0.*|2.12.0.*|2.13.0.*|2.14.0.*|2.15.0.*|2.16.0.*|2.17.0.*|2.17.2.*|2.18.1.*|2.19.0.*|2.20.0.*|2.22.0.*|2.23.0.*|2.24.0.*|2.9.2.*|2.9.1.*|2.9.0.*|2.8.1.*|2.8.0.*|2.7.0.*|2.6.0.*|2.5.2.*|2.5.0.*|2.4.0.*|2.3.0.*|2.2.0.*|2.25.0.*']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> dask-core
modin==0.7.3 -> dask[version='>=2.1.0'] -> dask-core[version='2.1.0.*|2.10.0.*|2.10.1.*|2.11.0.*|2.12.0.*|2.13.0.*|2.14.0.*|2.15.0.*|2.16.0.*|2.17.0.*|2.17.2.*|2.18.1.*|2.19.0.*|2.20.0.*|2.22.0.*|2.23.0.*|2.24.0.*|2.9.2.*|2.9.1.*|2.9.0.*|2.8.1.*|2.8.0.*|2.7.0.*|2.6.0.*|2.5.2.*|2.5.0.*|2.4.0.*|2.3.0.*|2.2.0.*|2.25.0.*|>=2.9.0|>=2.7.0|>=2.5.2|>=2']
defaults/noarch::dask==2.11.0=py_0 -> distributed[version='>=2.11.0'] -> dask-core[version='>=2.9.0']
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> dask-core[version='>=0.15.0']

Package param conflicts for:
conda-forge/noarch::colorcet==2.0.1=py_0 -> param[version='>=1.7.0']
conda-forge/noarch::panel==0.9.5=py_1 -> pyct[version='>=0.4.4'] -> param[version='>=1.7.0']
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> param[version='>=1.9.3,<2.0']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> param[version='>=1.6.1']
conda-forge/noarch::pyct-core==0.4.6=py_0 -> param[version='>=1.7.0']
conda-forge/noarch::panel==0.9.5=py_1 -> param[version='>=1.9.3']
conda-forge/noarch::pyct==0.4.6=py_0 -> pyct-core=0.4.6 -> param[version='>=1.7.0']
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> panel[version='>=0.7.0'] -> param[version='>=1.9.2|>=1.9.3']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> colorcet[version='>=0.9.0'] -> param[version='>=1.7.0']
conda-forge/noarch::pyviz_comms==0.7.5=pyh9f0ad1d_0 -> param

Package pandocfilters conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pandocfilters
defaults/win-64::jupyter==1.0.0=py37_7 -> nbconvert -> pandocfilters[version='>=1.4.1']
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> nbconvert -> pandocfilters[version='>=1.4.1']
defaults/win-64::notebook==6.0.3=py37_0 -> nbconvert -> pandocfilters[version='>=1.4.1']
defaults/win-64::nbconvert==5.6.1=py37_0 -> pandocfilters[version='>=1.4.1']
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> nbconvert -> pandocfilters[version='>=1.4.1']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pandocfilters
anaconda/win-64::spyder==4.1.4=py37_0 -> nbconvert[version='>=4.0'] -> pandocfilters[version='>=1.4.1']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> nbconvert -> pandocfilters[version='>=1.4.1']
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> nbconvert[version='>=4.2'] -> pandocfilters[version='>=1.4.1']

Package pytest-astropy conflicts for:
defaults/win-64::astropy==4.0=py37he774522_0 -> pytest-astropy
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pytest-astropy
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pytest-astropy
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> astropy[version='>=4.0'] -> pytest-astropy

Package sphinxcontrib conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> sphinxcontrib
defaults/noarch::sphinxcontrib-websupport==1.2.0=py_0 -> sphinxcontrib
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> sphinxcontrib

Package backports.shutil_which conflicts for:
defaults/win-64::terminado==0.8.3=py37_0 -> pywinpty -> backports.shutil_which
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pywinpty -> backports.shutil_which
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> backports.shutil_which

Package backcall conflicts for:
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> ipython -> backcall
defaults/noarch::ipywidgets==7.5.1=py_0 -> ipython[version='>=4.0.0'] -> backcall
defaults/noarch::jupyter_console==6.1.0=py_0 -> ipython -> backcall
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> ipython[version='>=5.4.0'] -> backcall
defaults/win-64::ipython==7.12.0=py37h5ca1d4c_0 -> backcall
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> backcall
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> backcall
conda-forge/noarch::hiplot==0.1.17=py_0 -> ipython[version='>=7.0.1'] -> backcall
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> ipython[version='>=5.0'] -> backcall

Package msgpack-python conflicts for:
modin==0.7.3 -> distributed[version='>=2.3.2'] -> msgpack-python[version='>=0.6.0']
defaults/win-64::distributed==2.11.0=py37_0 -> msgpack-python[version='>=0.6.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> distributed -> msgpack-python[version='>=0.6.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> msgpack-python
defaults/noarch::dask==2.11.0=py_0 -> distributed[version='>=2.11.0'] -> msgpack-python[version='>=0.6.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> msgpack-python

Package pynacl conflicts for:
defaults/noarch::paramiko==2.7.1=py_0 -> pynacl[version='>=1.0.1']
anaconda/win-64::spyder==4.1.4=py37_0 -> paramiko[version='>=2.4.0'] -> pynacl[version='>=1.0.1']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pynacl

Package alabaster conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> alabaster
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> alabaster
anaconda/win-64::spyder==4.1.4=py37_0 -> sphinx[version='>=0.6.6'] -> alabaster[version='>=0.7,<0.8']
defaults/noarch::numpydoc==0.9.2=py_0 -> sphinx -> alabaster[version='>=0.7,<0.8']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> sphinx -> alabaster[version='>=0.7,<0.8']

Package libspatialindex conflicts for:
anaconda/win-64::spyder==4.1.4=py37_0 -> rtree[version='>=0.8.3'] -> libspatialindex[version='>=1.9.3,<1.9.4.0a0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> libspatialindex
conda-forge/label/cf202003/noarch::geopandas==0.7.0=py_1 -> rtree -> libspatialindex[version='>=1.9.3,<1.9.4.0a0']
defaults/win-64::rtree==0.9.3=py37h21ff451_0 -> libspatialindex[version='>=1.9.3,<1.9.4.0a0']

Package m2w64-gcc-libgfortran conflicts for:
defaults/win-64::m2w64-gcc-libs==5.3.0=7 -> m2w64-gcc-libgfortran
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> m2w64-gcc-libgfortran
defaults/win-64::m2w64-xz==5.2.2=2 -> m2w64-gcc-libs -> m2w64-gcc-libgfortran
conda-forge/win-64::libffi==3.2.1=h6538335_1007 -> m2w64-gcc-libs -> m2w64-gcc-libgfortran
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> m2w64-gcc-libgfortran
defaults/win-64::pywinpty==0.5.7=py37_0 -> m2w64-gcc-libs -> m2w64-gcc-libgfortran
anaconda/win-64::libxgboost==0.90=1 -> m2w64-gcc-libs -> m2w64-gcc-libgfortran
defaults/win-64::m2w64-gettext==0.19.7=2 -> m2w64-gcc-libs -> m2w64-gcc-libgfortran

Package imagesize conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> imagesize
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> imagesize
anaconda/win-64::spyder==4.1.4=py37_0 -> sphinx[version='>=0.6.6'] -> imagesize
defaults/noarch::numpydoc==0.9.2=py_0 -> sphinx -> imagesize

Package pysocks conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pysocks
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pysocks
intel/win-64::urllib3==1.24.1=py37_2 -> pysocks[version='>=1.5.6,<2.0,!=1.5.7']
conda-forge/noarch::requests==2.24.0=pyh9f0ad1d_0 -> urllib3[version='>=1.21.1,<1.26,!=1.25.0,!=1.25.1'] -> pysocks[version='>=1.5.6,<2.0,!=1.5.7']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> urllib3 -> pysocks[version='>=1.5.6,<2.0,!=1.5.7']
defaults/noarch::botocore==1.12.189=py_0 -> urllib3[version='>=1.20,<1.26'] -> pysocks[version='>=1.5.6,<2.0,!=1.5.7']

Package html5lib conflicts for:
defaults/win-64::python==3.7.6=h60c2a47_2 -> pip -> html5lib
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> html5lib
conda-forge/noarch::pbr==5.4.2=py_0 -> pip -> html5lib
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> html5lib
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> bleach -> html5lib[version='>=0.999,!=0.9999,!=0.99999,<0.99999999|>=0.99999999|>=0.99999999pre,!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8']
defaults/win-64::nbconvert==5.6.1=py37_0 -> bleach -> html5lib[version='>=0.999,!=0.9999,!=0.99999,<0.99999999|>=0.99999999|>=0.99999999pre,!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8']

Package backports.functools_lru_cache conflicts for:
defaults/noarch::seaborn==0.10.0=py_0 -> matplotlib[version='>=2.1.2'] -> backports.functools_lru_cache
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> astroid -> backports.functools_lru_cache
conda-forge/noarch::jinja2-time==0.2.0=py_2 -> arrow -> backports.functools_lru_cache
conda-forge/noarch::missingno==0.4.2=py_0 -> matplotlib -> backports.functools_lru_cache
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> matplotlib[version='>=2.0.0'] -> backports.functools_lru_cache
anaconda/win-64::spyder==4.1.4=py37_0 -> pylint[version='>=1.0'] -> backports.functools_lru_cache
defaults/win-64::pylint==2.4.4=py37_0 -> isort[version='>=4.2.5,<5'] -> backports.functools_lru_cache
anaconda/win-64::python-language-server==0.34.1=py37_0 -> pylint -> backports.functools_lru_cache
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> backports.functools_lru_cache
defaults/win-64::beautifulsoup4==4.8.2=py37_0 -> soupsieve[version='>=1.2'] -> backports.functools_lru_cache

Package pywin32 conflicts for:
defaults/noarch::nbformat==5.0.4=py_0 -> jupyter_core -> pywin32[version='>=1.0']
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> jupyter_core -> pywin32[version='>=1.0']
defaults/win-64::qtconsole==4.6.0=py37_1 -> jupyter_client[version='>=4.1'] -> pywin32[version='>=1.0']
intel/win-64::menuinst==1.4.16=py37_0 -> pywin32
defaults/win-64::notebook==6.0.3=py37_0 -> jupyter_client[version='>=5.3.4'] -> pywin32[version='>=1.0']
defaults/noarch::jupyter_console==6.1.0=py_0 -> jupyter_client -> pywin32[version='>=1.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pywin32
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> jupyter_client -> pywin32[version='>=1.0']
defaults/win-64::jupyter_client==5.3.4=py37_0 -> pywin32[version='>=1.0']
defaults/win-64::nbconvert==5.6.1=py37_0 -> jupyter_core -> pywin32[version='>=1.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pywin32
anaconda/win-64::spyder-kernels==1.9.2=py37_0 -> jupyter_client[version='>=5.3.4'] -> pywin32[version='>=1.0']
anaconda/win-64::spyder==4.1.4=py37_0 -> keyring -> pywin32
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> jupyter_core -> pywin32[version='>=1.0']
defaults/win-64::xlwings==0.17.1=py37_0 -> pywin32
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> jupyter_client -> pywin32[version='>=1.0']
defaults/win-64::jupyter_core==4.6.1=py37_0 -> pywin32[version='>=1.0']
conda-forge/noarch::jupyter_contrib_core==0.3.3=py_2 -> jupyter_core -> pywin32[version='>=1.0']

Package shapely conflicts for:
conda-forge/label/cf202003/noarch::geopandas==0.7.0=py_1 -> shapely
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> shapely

Package tangled-up-in-unicode conflicts for:
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> tangled-up-in-unicode[version='>=0.0.6']
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> visions[version='>=0.4.4'] -> tangled-up-in-unicode[version='>=0.0.4']
conda-forge/noarch::visions==0.4.4=pyh9f0ad1d_0 -> tangled-up-in-unicode[version='>=0.0.4']

Package patsy conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> patsy
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> statsmodels -> patsy[version='>=0.4.0|>=0.5.1']
plotly/noarch::plotly_express==0.4.1=py_0 -> statsmodels[version='>=0.9.0'] -> patsy[version='>=0.4.0|>=0.5.1']
defaults/win-64::statsmodels==0.11.0=py37he774522_0 -> patsy[version='>=0.5.1']
plotly/noarch::plotly_express==0.4.1=py_0 -> patsy[version='>=0.5']
conda-forge/noarch::missingno==0.4.2=py_0 -> seaborn -> patsy
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> patsy

Package backports_abc conflicts for:
defaults/win-64::jupyter_client==5.3.4=py37_0 -> tornado[version='>=4.1'] -> backports_abc[version='>=0.4']
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> tornado -> backports_abc[version='>=0.4']
defaults/noarch::anaconda-project==0.8.4=py_0 -> tornado[version='>=4.2'] -> backports_abc[version='>=0.4']
defaults/win-64::distributed==2.11.0=py37_0 -> tornado[version='>=5'] -> backports_abc[version='>=0.4']
defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0 -> tornado[version='!=6.0.0,!=6.0.1,!=6.0.2'] -> backports_abc[version='>=0.4']
defaults/win-64::notebook==6.0.3=py37_0 -> tornado[version='>=5.0'] -> backports_abc[version='>=0.4']
defaults/win-64::terminado==0.8.3=py37_0 -> tornado[version='>=4'] -> backports_abc[version='>=0.4']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> backports_abc
conda-forge/win-64::matplotlib-base==3.2.1=py37h911224e_0 -> tornado -> backports_abc[version='>=0.4']
conda-forge/win-64::bokeh==2.1.0=py37hc8dfbb8_0 -> tornado[version='>=5.1'] -> backports_abc[version='>=0.4']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> tornado -> backports_abc[version='>=0.4']
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> tornado[version='>=4.2'] -> backports_abc[version='>=0.4']
conda-forge/noarch::jupyter_contrib_core==0.3.3=py_2 -> tornado -> backports_abc[version='>=0.4']
defaults/win-64::matplotlib==2.2.3=py37hd159220_0 -> tornado -> backports_abc[version='>=0.4']
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> tornado -> backports_abc[version='>=0.4']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> tornado -> backports_abc[version='>=0.4']

Package cryptography-vectors conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> cryptography -> cryptography-vectors=2.3
intel/win-64::urllib3==1.24.1=py37_2 -> cryptography[version='>=1.3.4'] -> cryptography-vectors=2.3
intel/win-64::pyopenssl==17.5.0=py37_2 -> cryptography[version='>=2.1.4'] -> cryptography-vectors=2.3

Package ipython conflicts for:
defaults/noarch::jupyter_console==6.1.0=py_0 -> ipython
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> ipython
anaconda/win-64::spyder-kernels==1.9.2=py37_0 -> ipykernel[version='>=5.1.3'] -> ipython[version='>=5.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> ipython
defaults/win-64::jupyter==1.0.0=py37_7 -> ipykernel -> ipython[version='>=4.0.0|>=4.0|>=5.0']
defaults/noarch::jupyter_console==6.1.0=py_0 -> ipykernel -> ipython[version='>=4.0.0|>=4.0|>=5.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> ipykernel -> ipython[version='>=4.0.0|>=4.0|>=5.0']
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> ipython[version='>=5.0']
defaults/win-64::qtconsole==4.6.0=py37_1 -> ipykernel[version='>=4.1'] -> ipython[version='>=4.0.0|>=4.0|>=5.0']
defaults/noarch::ipywidgets==7.5.1=py_0 -> ipykernel[version='>=4.5.1'] -> ipython[version='>=4.0|>=5.0']
conda-forge/noarch::ipympl==0.5.6=pyh9f0ad1d_1 -> ipykernel[version='>=4.7'] -> ipython[version='>=4.0.0|>=4.0|>=5.0']
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> ipython[version='>=5.4.0']
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> ipywidgets[version='>=7.5.1'] -> ipython[version='>=4.0.0']
defaults/win-64::notebook==6.0.3=py37_0 -> ipykernel -> ipython[version='>=4.0.0|>=4.0|>=5.0']
conda-forge/noarch::hiplot==0.1.17=py_0 -> ipython[version='>=7.0.1']
defaults/noarch::ipywidgets==7.5.1=py_0 -> ipython[version='>=4.0.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> ipython
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> jupyter_latex_envs[version='>=1.3.8'] -> ipython

Package wrapt conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> astroid -> wrapt=1.11
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> wrapt
defaults/win-64::astroid==2.3.3=py37_0 -> wrapt
defaults/win-64::pylint==2.4.4=py37_0 -> astroid[version='>=2.3.0,<2.4'] -> wrapt
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> wrapt

Package pytest-astropy-header conflicts for:
defaults/win-64::astropy==4.0=py37he774522_0 -> pytest-astropy -> pytest-astropy-header[version='>=0.1']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pytest-astropy-header
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pytest-astropy -> pytest-astropy-header[version='>=0.1']
defaults/noarch::pytest-astropy==0.8.0=py_0 -> pytest-astropy-header[version='>=0.1']

Package yapf conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> yapf
anaconda/win-64::spyder==4.1.4=py37_0 -> python-language-server[version='>=0.34.0,<1.0.0'] -> yapf
anaconda/win-64::python-language-server==0.34.1=py37_0 -> yapf

Package spyder-kernels conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> spyder -> spyder-kernels[version='0.*|>=0.1,<1|>=0.4.2,<1|>=0.4.3,<1|>=0.5.0,<1|>=1.8.1,<2.0.0|>=1.9.0,<1.10.0|>=1.9.1,<1.10.0|>=1.9.2,<1.10.0|>=1.8.1,<1.9.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> spyder-kernels
anaconda/win-64::spyder==4.1.4=py37_0 -> spyder-kernels[version='>=1.9.2,<1.10.0']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> spyder-kernels[version='>=1.8.1,<1.9.0|>=1.8.1,<2.0.0|>=1.9.0,<1.10.0|>=1.9.1,<1.10.0|>=1.9.2,<1.10.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> spyder-kernels

Package mock conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pytables -> mock
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> mock
conda-forge/label/cf202003/win-64::pytables==3.5.2=py37h6a20dd8_0 -> mock

Package matplotlib-base conflicts for:
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> matplotlib-base[version='>=2.2']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> matplotlib-base
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> matplotlib -> matplotlib-base[version='3.1.2|3.1.2|3.1.2|3.1.3|3.1.3|3.1.3|>=3.2.1,<3.2.2.0a0|>=3.2.2,<3.2.3.0a0|>=3.3.1,<3.3.2.0a0',build='py38h64f37c6_1|py36h64f37c6_1|py36h64f37c6_0|py37h64f37c6_0|py38h64f37c6_0|py37h64f37c6_1']
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> phik[version='>=0.9.10'] -> matplotlib-base[version='>=2.2.3']
conda-forge/noarch::descartes==1.1.0=py_4 -> matplotlib-base
defaults/noarch::seaborn==0.10.0=py_0 -> matplotlib[version='>=2.1.2'] -> matplotlib-base[version='3.1.2|3.1.2|3.1.2|3.1.3|3.1.3|3.1.3|>=3.2.1,<3.2.2.0a0|>=3.2.2,<3.2.3.0a0|>=3.3.1,<3.3.2.0a0',build='py38h64f37c6_1|py36h64f37c6_1|py36h64f37c6_0|py37h64f37c6_0|py38h64f37c6_0|py37h64f37c6_1']
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> matplotlib-base[version='>=3.2.0']
conda-forge/noarch::ipympl==0.5.6=pyh9f0ad1d_1 -> matplotlib-base[version='>=2.2.0']
conda-forge/noarch::missingno==0.4.2=py_0 -> matplotlib -> matplotlib-base[version='3.1.2|3.1.2|3.1.2|3.1.3|3.1.3|3.1.3|>=3.2.1,<3.2.2.0a0|>=3.2.2,<3.2.3.0a0|>=3.3.1,<3.3.2.0a0',build='py38h64f37c6_1|py36h64f37c6_1|py36h64f37c6_0|py37h64f37c6_0|py38h64f37c6_0|py37h64f37c6_1']
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> matplotlib[version='>=2.0.0'] -> matplotlib-base[version='3.1.2|3.1.2|3.1.2|3.1.3|3.1.3|3.1.3|>=3.2.1,<3.2.2.0a0|>=3.2.2,<3.2.3.0a0|>=3.3.1,<3.3.2.0a0',build='py38h64f37c6_1|py36h64f37c6_1|py36h64f37c6_0|py37h64f37c6_0|py38h64f37c6_0|py37h64f37c6_1']
conda-forge/noarch::phik==0.10.0=py_0 -> matplotlib-base[version='>=2.2.3']

Package argh conflicts for:
defaults/win-64::watchdog==0.10.2=py37_0 -> argh[version='>=0.24.1']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> argh
anaconda/win-64::spyder==4.1.4=py37_0 -> watchdog -> argh[version='>=0.24.1']

Package heapdict conflicts for:
defaults/win-64::distributed==2.11.0=py37_0 -> zict[version='>=0.1.3'] -> heapdict
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> heapdict
defaults/noarch::zict==1.0.0=py_0 -> heapdict
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> heapdict

Package entrypoints conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> entrypoints
defaults/win-64::nbconvert==5.6.1=py37_0 -> entrypoints[version='>=0.2.2']
defaults/win-64::keyring==21.1.0=py37_0 -> entrypoints
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> nbconvert -> entrypoints[version='>=0.2.2']
anaconda/win-64::spyder==4.1.4=py37_0 -> keyring -> entrypoints[version='>=0.2.2']
defaults/win-64::qtconsole==4.6.0=py37_1 -> jupyter_client[version='>=4.1'] -> entrypoints
defaults/win-64::notebook==6.0.3=py37_0 -> jupyter_client[version='>=5.3.4'] -> entrypoints[version='>=0.2.2']
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> nbconvert[version='>=4.2'] -> entrypoints[version='>=0.2.2']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> nbconvert -> entrypoints[version='>=0.2.2']
anaconda/win-64::spyder-kernels==1.9.2=py37_0 -> jupyter_client[version='>=5.3.4'] -> entrypoints
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> entrypoints
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> jupyter_client -> entrypoints
defaults/noarch::jupyter_console==6.1.0=py_0 -> jupyter_client -> entrypoints
defaults/win-64::jupyter==1.0.0=py37_7 -> nbconvert -> entrypoints[version='>=0.2.2']
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> nbconvert -> entrypoints[version='>=0.2.2']

Package itsdangerous conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> itsdangerous
conda-forge/noarch::flask-compress==1.5.0=pyh9f0ad1d_0 -> flask -> itsdangerous[version='>=0.21|>=0.24']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> flask -> itsdangerous[version='>=0.21|>=0.24']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> itsdangerous
conda-forge/noarch::hiplot==0.1.17=py_0 -> flask -> itsdangerous[version='>=0.21|>=0.24']
defaults/noarch::flask==1.1.1=py_0 -> itsdangerous[version='>=0.24']

Package ujson conflicts for:
anaconda/win-64::python-language-server==0.34.1=py37_0 -> ujson[version='<=1.35']
anaconda/win-64::python-language-server==0.34.1=py37_0 -> python-jsonrpc-server[version='>=0.3.2'] -> ujson
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> ujson
defaults/noarch::python-jsonrpc-server==0.3.4=py_0 -> ujson
anaconda/win-64::spyder==4.1.4=py37_0 -> python-language-server[version='>=0.34.0,<1.0.0'] -> ujson[version='<=1.35']

Package unicodecsv conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> unicodecsv
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> unicodecsv

Package parso conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> parso
defaults/win-64::ipython==7.12.0=py37h5ca1d4c_0 -> jedi[version='>=0.10'] -> parso[version='0.1.0|>=0.1.0,<0.2|>=0.2.0|>=0.3.0|>=0.5.0|>=0.5.2|>=0.7.0|>=0.7.0,<0.8.0']
anaconda/win-64::spyder==4.1.4=py37_0 -> parso=0.7.0
anaconda/win-64::python-language-server==0.34.1=py37_0 -> jedi[version='>=0.17.0,<0.18.0a0'] -> parso[version='>=0.7.0|>=0.7.0,<0.8.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> parso
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> jedi -> parso[version='0.1.0|>=0.1.0,<0.2|>=0.2.0|>=0.3.0|>=0.5.0|>=0.5.2|>=0.7.0|>=0.7.0,<0.8.0|0.7.0.*|0.5.2.*']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> parso[version='0.5.2.*|0.7.0.*']
anaconda/win-64::jedi==0.17.1=py37_0 -> parso[version='>=0.7.0,<0.8.0']
anaconda/win-64::spyder==4.1.4=py37_0 -> jedi=0.17.1 -> parso[version='>=0.7.0,<0.8.0']

Package progress conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> progress
defaults/win-64::python==3.7.6=h60c2a47_2 -> pip -> progress
conda-forge/noarch::pbr==5.4.2=py_0 -> pip -> progress
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pip -> progress

Package qtpy conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> qtpy
defaults/noarch::qtawesome==0.6.1=py_0 -> qtpy
anaconda/win-64::spyder==4.1.4=py37_0 -> qtpy[version='>=1.5.0']
defaults/win-64::jupyter==1.0.0=py37_7 -> qtconsole -> qtpy
anaconda/win-64::spyder==4.1.4=py37_0 -> qtawesome[version='>=0.5.7'] -> qtpy
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> spyder -> qtpy[version='>=1.1|>=1.2.0|>=1.5.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> qtpy
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> qtpy[version='>=1.5.0']

Package rope conflicts for:
anaconda/win-64::spyder==4.1.4=py37_0 -> python-language-server[version='>=0.34.0,<1.0.0'] -> rope[version='>=0.10.5']
anaconda/win-64::python-language-server==0.34.1=py37_0 -> rope[version='>=0.10.5']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> rope
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> spyder -> rope[version='>=0.10.5|>=0.9.4']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> rope

Package typing conflicts for:
conda-forge/win-64::bokeh==2.1.0=py37hc8dfbb8_0 -> typing_extensions[version='>=3.7.4'] -> typing[version='>=3.7.4']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> typing
defaults/noarch::numpydoc==0.9.2=py_0 -> sphinx -> typing
anaconda/win-64::spyder==4.1.4=py37_0 -> sphinx[version='>=0.6.6'] -> typing
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> sphinx -> typing
conda-forge/noarch::optuna==1.0.0=py_0 -> typing

Package ply conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> ply
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> ply

Package asn1crypto conflicts for:
defaults/noarch::paramiko==2.7.1=py_0 -> cryptography[version='>=2.5'] -> asn1crypto[version='>=0.21.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> asn1crypto
intel/win-64::cryptography==2.7=py37_0 -> asn1crypto[version='>=0.21.0']
intel/win-64::urllib3==1.24.1=py37_2 -> cryptography[version='>=1.3.4'] -> asn1crypto[version='>=0.21.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> asn1crypto
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> cryptography -> asn1crypto[version='>=0.21.0']
intel/win-64::pyopenssl==17.5.0=py37_2 -> cryptography[version='>=2.1.4'] -> asn1crypto[version='>=0.21.0']

Package bleach conflicts for:
anaconda/win-64::spyder==4.1.4=py37_0 -> nbconvert[version='>=4.0'] -> bleach
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> nbconvert -> bleach
defaults/win-64::notebook==6.0.3=py37_0 -> nbconvert -> bleach
conda-forge/win-64::jupyter_nbextensions_configurator==0.4.1=py37_0 -> nbconvert -> bleach
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> bleach
defaults/win-64::nbconvert==5.6.1=py37_0 -> bleach
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> bleach
defaults/win-64::jupyter==1.0.0=py37_7 -> nbconvert -> bleach
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> nbconvert[version='>=4.2'] -> bleach

Package libssh2 conflicts for:
defaults/win-64::pycurl==7.43.0.5=py37h7a1dbc1_0 -> libcurl[version='>=7.67.0,<8.0a0'] -> libssh2[version='>=1.8.2,<1.9.0a0|>=1.8.2,<2.0a0|>=1.9.0,<2.0a0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> libssh2
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> curl -> libssh2[version='1.*|>=1.8.0,<2.0a0|>=1.8.2,<2.0a0|>=1.9.0,<2.0a0|>=1.8.2,<1.9.0a0']
conda-forge/label/cf202003/win-64::libnetcdf==4.6.2=h396784b_1001 -> libssh2
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> libnetcdf[version='>=4.6.2,<4.6.3.0a0'] -> libssh2
conda-forge/label/cf202003/win-64::libnetcdf==4.6.2=h396784b_1001 -> curl[version='>=7.59.0,<8.0a0'] -> libssh2[version='>=1.8.0,<2.0a0|>=1.8.2,<2.0a0|>=1.9.0,<2.0a0|>=1.8.2,<1.9.0a0']
conda-forge/label/cf202003/win-64::curl==7.68.0=h4496350_0 -> libssh2[version='>=1.8.2,<1.9.0a0']
conda-forge/label/cf202003/win-64::libcurl==7.68.0=h4496350_0 -> libssh2[version='>=1.8.2,<1.9.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> libssh2
conda-forge/label/cf202003/win-64::poppler==0.67.0=h1707e21_8 -> curl[version='>=7.64.1,<8.0a0'] -> libssh2[version='>=1.8.2,<1.9.0a0|>=1.8.2,<2.0a0|>=1.9.0,<2.0a0']

Package prompt_toolkit conflicts for:
conda-forge/noarch::hiplot==0.1.17=py_0 -> ipython[version='>=7.0.1'] -> prompt_toolkit[version='>=2.0.0,<3|>=2.0.0|>=2.0.0,<4,!=3.0.0,!=3.0.1']
defaults/noarch::ipywidgets==7.5.1=py_0 -> ipython[version='>=4.0.0'] -> prompt_toolkit[version='>=1.0.4,<2.0.0|>=2.0.0|>=2.0.0,<4,!=3.0.0,!=3.0.1|>=2.0.0,<3']
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> ipython[version='>=5.0'] -> prompt_toolkit[version='>=1.0.4,<2.0.0|>=2.0.0|>=2.0.0,<4,!=3.0.0,!=3.0.1|>=2.0.0,<3']
defaults/noarch::jupyter_console==6.1.0=py_0 -> ipython -> prompt_toolkit[version='>=1.0.4,<2.0.0|>=2.0.0|>=2.0.0,<4,!=3.0.0,!=3.0.1|>=2.0.0,<3']
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> ipython[version='>=5.4.0'] -> prompt_toolkit[version='>=1.0.4,<2.0.0|>=2.0.0|>=2.0.0,<4,!=3.0.0,!=3.0.1|>=2.0.0,<3']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> prompt_toolkit
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> ipython -> prompt_toolkit[version='>=1.0.4,<2.0.0|>=2.0.0|>=2.0.0,<4,!=3.0.0,!=3.0.1|>=2.0.0,<3']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> ipython -> prompt_toolkit[version='>=1.0.0,<2|>=1.0.4,<2.0.0|>=2.0.0|>=2.0.0,<4,!=3.0.0,!=3.0.1|>=2.0.0,<3|>=2.0.0,<3.1.0,!=3.0.0,!=3.0.1|>=2.0.0,<2.1.0']
defaults/noarch::jupyter_console==6.1.0=py_0 -> prompt_toolkit[version='>=2.0.0,<3.1.0,!=3.0.0,!=3.0.1']
defaults/win-64::jupyter==1.0.0=py37_7 -> jupyter_console -> prompt_toolkit[version='>=1.0.0,<2|>=2.0.0,<2.1.0|>=2.0.0,<3.1.0,!=3.0.0,!=3.0.1']
defaults/win-64::ipython==7.12.0=py37h5ca1d4c_0 -> prompt_toolkit[version='>=2.0.0,<4,!=3.0.0,!=3.0.1']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> prompt_toolkit

Package boto conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> boto
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> boto

Package pympler conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> attrs -> pympler
conda-forge/win-64::cmd2==0.9.15=py37_0 -> attrs -> pympler
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> attrs[version='>=17'] -> pympler
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> pympler

Package pixman conflicts for:
conda-forge/win-64::pycairo==1.19.1=py37h6430cfb_3 -> cairo[version='>=1.16.0,<1.17.0a0'] -> pixman[version='>=0.38.0,<0.39.0a0']
conda-forge/win-64::gobject-introspection==1.64.1=py37hf0dd101_1 -> cairo[version='>=1.16.0,<1.17.0a0'] -> pixman[version='>=0.38.0,<0.39.0a0']
conda-forge/win-64::cairo==1.16.0=h63a05c6_1001 -> pixman[version='>=0.38.0,<0.39.0a0']

Package hdf4 conflicts for:
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> hdf4[version='>=4.2.13,<4.3.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> hdf4[version='>=4.2.13,<4.3.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> hdf4[version='>=4.2.13,<4.3.0a0']
conda-forge/label/cf202003/win-64::libnetcdf==4.6.2=h396784b_1001 -> hdf4[version='>=4.2.13,<4.3.0a0']

Package configparser conflicts for:
defaults/win-64::jsonschema==3.2.0=py37_0 -> importlib_metadata -> configparser[version='>=3.5']
defaults/win-64::pluggy==0.13.1=py37_0 -> importlib_metadata[version='>=0.12'] -> configparser[version='>=3.5']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> entrypoints -> configparser[version='>=3.5']
anaconda/win-64::python-language-server==0.34.1=py37_0 -> pydocstyle[version='>=2.0.0'] -> configparser
defaults/win-64::path==13.1.0=py37_0 -> importlib_metadata[version='>=0.5'] -> configparser[version='>=3.5']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> configparser
anaconda/win-64::spyder==4.1.4=py37_0 -> pylint[version='>=1.0'] -> configparser
defaults/win-64::inflect==4.1.0=py37_0 -> importlib_metadata -> configparser[version='>=3.5']
defaults/win-64::pytest==5.3.5=py37_0 -> importlib_metadata[version='>=0.12'] -> configparser[version='>=3.5']
defaults/win-64::nbconvert==5.6.1=py37_0 -> entrypoints[version='>=0.2.2'] -> configparser
defaults/win-64::keyring==21.1.0=py37_0 -> entrypoints -> configparser[version='>=3.5']

Package cfitsio conflicts for:
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> cfitsio[version='>=3.470,<3.471.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> cfitsio[version='>=3.470,<3.471.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> cfitsio[version='>=3.470,<3.471.0a0']

Package get_terminal_size conflicts for:
defaults/noarch::jupyter_console==6.1.0=py_0 -> ipython -> get_terminal_size
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> ipython -> get_terminal_size
defaults/noarch::ipywidgets==7.5.1=py_0 -> ipython[version='>=4.0.0'] -> get_terminal_size
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> get_terminal_size
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> get_terminal_size
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> ipython[version='>=5.0'] -> get_terminal_size
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> ipython[version='>=5.4.0'] -> get_terminal_size

Package bottleneck conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> bottleneck
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> bottleneck

Package flask conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> flask
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> flask
conda-forge/noarch::hiplot==0.1.17=py_0 -> flask
conda-forge/noarch::flask-compress==1.5.0=pyh9f0ad1d_0 -> flask

Package proj4 conflicts for:
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> libspatialite[version='>=4.3.0a,<4.4.0a0'] -> proj4[version='>=4.9.3,<4.9.4.0a0|>=5.1.0,<5.1.1.0a0|>=5.2.0,<5.2.1.0a0']
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> proj4[version='>=6.1.1,<6.1.2.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> proj4[version='>=6.1.1,<6.1.2.0a0']
conda-forge/label/cf202003/win-64::geotiff==1.5.1=h4b1d854_3 -> proj4[version='>=6.1.1,<6.1.2.0a0']
conda-forge/label/cf202003/noarch::geopandas==0.7.0=py_1 -> pyproj[version='>=2.2.0'] -> proj4[version='>=6.1.1,<6.1.2.0a0']
conda-forge/label/cf202003/win-64::pyproj==2.3.1=py37he1416cd_0 -> proj4[version='>=6.1.1,<6.1.2.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> proj4[version='>=6.1.1,<6.1.2.0a0']
conda-forge/label/cf202003/win-64::libspatialite==4.3.0a=h01b1fc4_1030 -> proj4[version='>=6.1.1,<6.1.2.0a0']

Package pyreadline conflicts for:
conda-forge/label/cf202003/win-64::h5py==2.9.0=nompi_py37h9dfa0df_1103 -> pyreadline
defaults/win-64::humanfriendly==8.2=py37_0 -> pyreadline
conda-forge/win-64::cmd2==0.9.15=py37_0 -> pyreadline
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pyreadline
defaults/win-64::coloredlogs==10.0=py37_0 -> humanfriendly[version='>=4.7'] -> pyreadline
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pyreadline
conda-forge/win-64::cliff==2.15.0=py37_0 -> cmd2!=0.8.3 -> pyreadline

Package _low_priority conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> libarchive -> _low_priority
intel/win-64::python-libarchive-c==2.8=py37_13 -> libarchive -> _low_priority

Package soupsieve conflicts for:
conda-forge/noarch::hiplot==0.1.17=py_0 -> beautifulsoup4 -> soupsieve[version='>=1.2']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> soupsieve
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> soupsieve
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> beautifulsoup4 -> soupsieve[version='>=1.2']
defaults/win-64::beautifulsoup4==4.8.2=py37_0 -> soupsieve[version='>=1.2']

Package isort conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pylint -> isort[version='>=4.2.5|>=4.2.5,<5|>=4.2.5,<6']
anaconda/win-64::spyder==4.1.4=py37_0 -> pylint[version='>=1.0'] -> isort[version='>=4.2.5|>=4.2.5,<5|>=4.2.5,<6']
anaconda/win-64::python-language-server==0.34.1=py37_0 -> pylint -> isort[version='>=4.2.5|>=4.2.5,<5|>=4.2.5,<6']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> isort
defaults/win-64::pylint==2.4.4=py37_0 -> isort[version='>=4.2.5,<5']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> isort

Package keyring conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> keyring
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> keyring
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> keyring
anaconda/win-64::spyder==4.1.4=py37_0 -> keyring

Package mako conflicts for:
conda-forge/noarch::optuna==1.0.0=py_0 -> alembic -> mako
conda-forge/noarch::alembic==1.4.0=py_0 -> mako

Package m2w64-gettext conflicts for:
defaults/win-64::m2w64-xz==5.2.2=2 -> m2w64-gettext
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> m2w64-xz -> m2w64-gettext

Package libarchive conflicts for:
intel/win-64::python-libarchive-c==2.8=py37_13 -> libarchive
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> libarchive
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> libarchive

Package m2w64-xz conflicts for:
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> m2w64-xz
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> m2w64-xz
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> m2w64-xz

Package zict conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> distributed -> zict[version='>=0.1.2|>=0.1.3']
defaults/noarch::dask==2.11.0=py_0 -> distributed[version='>=2.11.0'] -> zict[version='>=0.1.3']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> zict
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> zict
modin==0.7.3 -> distributed[version='>=2.3.2'] -> zict[version='>=0.1.3']
defaults/win-64::distributed==2.11.0=py37_0 -> zict[version='>=0.1.3']

Package win_inet_pton conflicts for:
intel/win-64::urllib3==1.24.1=py37_2 -> pysocks[version='>=1.5.6,<2.0,!=1.5.7'] -> win_inet_pton
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> win_inet_pton
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> win_inet_pton
intel/win-64::pysocks==1.6.7=py37_1 -> win_inet_pton

Package humanfriendly conflicts for:
defaults/win-64::coloredlogs==10.0=py37_0 -> humanfriendly[version='>=4.7']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> coloredlogs -> humanfriendly[version='>=3.2|>=4.7']

Package nose conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> nose
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> nose

Package nltk conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> nltk
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> nltk

Package pyct conflicts for:
conda-forge/noarch::colorcet==2.0.1=py_0 -> pyct[version='>=0.4.4']
conda-forge/noarch::panel==0.9.5=py_1 -> pyct[version='>=0.4.4']
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> panel[version='>=0.7.0'] -> pyct[version='>=0.4.4']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> colorcet[version='>=0.9.0'] -> pyct[version='>=0.4.4']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> pyct[version='>=0.4.5']

Package freexl conflicts for:
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> freexl[version='>=1.0.5,<2.0a0']
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> freexl[version='>=1.0.5,<2.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> freexl[version='>=1.0.5,<2.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> libspatialite[version='>=4.3.0a,<4.4.0a0'] -> freexl[version='>=1.0.4,<2.0a0']
conda-forge/label/cf202003/win-64::libspatialite==4.3.0a=h01b1fc4_1030 -> freexl[version='>=1.0.5,<2.0a0']

Package beautifulsoup4 conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> beautifulsoup4
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> beautifulsoup4
conda-forge/noarch::hiplot==0.1.17=py_0 -> beautifulsoup4

Package pcre conflicts for:
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> pcre[version='>=8.43,<9.0a0']
conda-forge/label/cf202003/win-64::poppler==0.67.0=h1707e21_8 -> glib[version='>=2.58.3,<3.0a0'] -> pcre[version='>=8.44,<9.0a0']
conda-forge/win-64::gobject-introspection==1.64.1=py37hf0dd101_1 -> glib[version='>=2.58.3,<3.0a0'] -> pcre[version='>=8.44,<9.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> pcre[version='>=8.43,<9.0a0']
conda-forge/win-64::pygobject==3.36.1=py37hf6b2db1_0 -> glib[version='>=2.58.3,<3.0a0'] -> pcre[version='>=8.44,<9.0a0']
conda-forge/win-64::glib==2.64.3=he4de6d7_0 -> pcre[version='>=8.44,<9.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> pcre[version='>=8.43,<9.0a0']

Package click-plugins conflicts for:
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> click-plugins[version='>=1.0']
conda-forge/label/cf202003/noarch::geopandas==0.7.0=py_1 -> fiona -> click-plugins[version='>=1.0']

Package path conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> path.py -> path[version='<13.2']
defaults/noarch::path.py==12.4.0=0 -> path[version='<13.2']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> path

Package clyent conflicts for:
defaults/win-64::anaconda-client==1.7.2=py37_0 -> clyent[version='>=1.2.2']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> anaconda-client -> clyent[version='>=1.2.0|>=1.2.2']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> clyent
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> clyent
defaults/noarch::anaconda-project==0.8.4=py_0 -> anaconda-client -> clyent[version='>=1.2.0|>=1.2.2']

Package munch conflicts for:
conda-forge/label/cf202003/noarch::geopandas==0.7.0=py_1 -> fiona -> munch
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> munch

Package menuinst conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> menuinst
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> menuinst

Package fastcache conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> fastcache
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> fastcache
defaults/win-64::sympy==1.5.1=py37_0 -> fastcache

Package flake8 conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> flake8
anaconda/win-64::python-language-server==0.34.1=py37_0 -> flake8[version='>=3.8.0']
anaconda/win-64::spyder==4.1.4=py37_0 -> python-language-server[version='>=0.34.0,<1.0.0'] -> flake8[version='>=3.8.0']

Package pywavelets conflicts for:
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> imagehash -> pywavelets
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pywavelets
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> scikit-image -> pywavelets[version='>=0.4.0']
conda-forge/noarch::imagehash==4.1.0=pyh9f0ad1d_0 -> pywavelets
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pywavelets
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> pywavelets[version='>=0.4.0']

Package sphinxcontrib-qthelp conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> sphinxcontrib-qthelp
defaults/noarch::numpydoc==0.9.2=py_0 -> sphinx -> sphinxcontrib-qthelp
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> sphinx -> sphinxcontrib-qthelp
anaconda/win-64::spyder==4.1.4=py37_0 -> sphinx[version='>=0.6.6'] -> sphinxcontrib-qthelp

Package backports.shutil_get_terminal_size conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> backports.shutil_get_terminal_size
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> backports.shutil_get_terminal_size
defaults/noarch::ipywidgets==7.5.1=py_0 -> ipython[version='>=4.0.0'] -> backports.shutil_get_terminal_size
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> ipython[version='>=5.4.0'] -> backports.shutil_get_terminal_size
defaults/win-64::ipykernel==5.1.4=py37h39e3cac_0 -> ipython[version='>=5.0'] -> backports.shutil_get_terminal_size
defaults/win-64::get_terminal_size==1.0.0=h38e98db_0 -> backports.shutil_get_terminal_size
conda-forge/win-64::jupyter_latex_envs==1.4.4=py37_1000 -> ipython -> backports.shutil_get_terminal_size
defaults/noarch::jupyter_console==6.1.0=py_0 -> ipython -> backports.shutil_get_terminal_size

Package libcurl conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> libcurl
conda-forge/label/cf202003/win-64::libnetcdf==4.6.2=h396784b_1001 -> curl[version='>=7.59.0,<8.0a0'] -> libcurl[version='7.59.0|7.59.0|7.60.0|7.60.0|7.61.0|7.61.0|7.61.1|7.61.1|7.61.1|7.62.0|7.62.0|7.63.0|7.63.0|7.63.0|7.63.0|7.64.0|7.64.0|7.64.1|7.64.1|7.65.2|7.65.2|7.65.3|7.65.3|7.67.0|7.68.0|7.69.1|7.71.0|7.71.1|7.68.0',build='h0990ea7_0|hc4dcbb0_0|hc4dcbb0_0|h0990ea7_0|h7a46e7a_0|h7602738_0|h7602738_0|h7a46e7a_0|h2a8f88b_0|h2a8f88b_0|h7a46e7a_0|h2a8f88b_0|h7a46e7a_0|h7a46e7a_2|h7a46e7a_0|h2a8f88b_0|h7a46e7a_0|h2a8f88b_0|h7a46e7a_0|h2a8f88b_0|h2a8f88b_0|h2a8f88b_0|h2a8f88b_0|h2a8f88b_0|h2a8f88b_1|h2a8f88b_2|h7a46e7a_1000|h2a8f88b_1000|h4496350_0']
defaults/win-64::pycurl==7.43.0.5=py37h7a1dbc1_0 -> libcurl[version='>=7.67.0,<8.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> libcurl
conda-forge/label/cf202003/win-64::curl==7.68.0=h4496350_0 -> libcurl==7.68.0=h4496350_0
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> curl -> libcurl[version='7.57.0|7.58.0|7.58.0|7.59.0|7.59.0|7.60.0|7.60.0|7.61.0|7.61.0|7.61.1|7.61.1|7.61.1|7.62.0|7.62.0|7.63.0|7.63.0|7.63.0|7.63.0|7.64.0|7.64.0|7.64.1|7.64.1|7.65.2|7.65.2|7.65.3|7.65.3|7.67.0|7.68.0|7.69.1|7.71.0|7.71.1|7.68.0|>=7.67.0,<8.0a0|>=7.65.3,<8.0a0|>=7.64.1,<8.0a0|>=7.63.0,<8.0a0|>=7.61.0,<8.0a0|>=7.60.0,<8.0a0',build='h7a46e7a_0|h7a46e7a_0|h7602738_0|h0990ea7_0|hc4dcbb0_0|hc4dcbb0_0|h0990ea7_0|h7a46e7a_0|h7602738_0|h7602738_0|h7a46e7a_0|h2a8f88b_0|h2a8f88b_0|h7a46e7a_0|h2a8f88b_0|h7a46e7a_0|h7a46e7a_2|h7a46e7a_0|h2a8f88b_0|h7a46e7a_0|h2a8f88b_0|h7a46e7a_0|h2a8f88b_0|h2a8f88b_0|h2a8f88b_0|h2a8f88b_0|h2a8f88b_0|h2a8f88b_1|h2a8f88b_2|h7a46e7a_1000|h2a8f88b_1000|h4496350_0']
conda-forge/label/cf202003/win-64::poppler==0.67.0=h1707e21_8 -> curl[version='>=7.64.1,<8.0a0'] -> libcurl[version='7.64.1|7.64.1|7.65.2|7.65.2|7.65.3|7.65.3|7.67.0|7.68.0|7.69.1|7.71.0|7.71.1|7.68.0',build='h7a46e7a_0|h2a8f88b_0|h7a46e7a_0|h2a8f88b_0|h7a46e7a_0|h2a8f88b_0|h2a8f88b_0|h2a8f88b_0|h2a8f88b_0|h2a8f88b_0|h2a8f88b_1|h4496350_0']

Package jdcal conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> jdcal
defaults/noarch::openpyxl==3.0.3=py_0 -> jdcal
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> jdcal

Package threadpoolctl conflicts for:
conda-forge/win-64::tslearn==0.3.1=py37hbc2f12b_0 -> scikit-learn -> threadpoolctl
anaconda/noarch::pynndescent==0.4.8=py_1 -> scikit-learn[version='>=0.19'] -> threadpoolctl
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> threadpoolctl
conda-forge/noarch::imbalanced-learn==0.7.0=py_1 -> scikit-learn[version='>=0.23'] -> threadpoolctl
conda-forge/win-64::umap-learn==0.4.4=py37hc8dfbb8_0 -> scikit-learn[version='>=0.20'] -> threadpoolctl
defaults/win-64::scikit-learn==0.23.1=py37h25d0782_0 -> threadpoolctl
anaconda/win-64::py-xgboost==0.90=py37_1 -> scikit-learn -> threadpoolctl
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> scikit-learn -> threadpoolctl

Package openjpeg conflicts for:
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> openjpeg[version='>=2.3.1,<2.4.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> openjpeg[version='>=2.3.1,<2.4.0a0']
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> openjpeg[version='>=2.3.1,<2.4.0a0']
conda-forge/label/cf202003/win-64::poppler==0.67.0=h1707e21_8 -> openjpeg[version='>=2.3.1,<2.4.0a0']

Package pkginfo conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pkginfo
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pkginfo

Package functools32 conflicts for:
defaults/noarch::jupyterlab_server==1.0.6=py_0 -> jsonschema[version='>=3.0.1'] -> functools32
defaults/noarch::seaborn==0.10.0=py_0 -> matplotlib[version='>=2.1.2'] -> functools32
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> jsonschema -> functools32
defaults/noarch::nbformat==5.0.4=py_0 -> jsonschema[version='>=2.4,!=2.5.0'] -> functools32
conda-forge/noarch::missingno==0.4.2=py_0 -> matplotlib -> functools32
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> functools32
defaults/win-64::scikit-image==0.16.2=py37h47e9c7a_0 -> matplotlib[version='>=2.0.0'] -> functools32

Package path.py conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> path.py
anaconda/win-64::spyder==4.1.4=py37_0 -> pickleshare[version='>=0.4'] -> path.py
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> path.py
defaults/win-64::ipython==7.12.0=py37h5ca1d4c_0 -> pickleshare -> path.py

Package pycrypto conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pycrypto
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pycrypto

Package glib conflicts for:
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> poppler[version='>=0.67.0,<0.68.0a0'] -> glib[version='>=2.58.3,<3.0a0']
conda-forge/win-64::pygobject==3.36.1=py37hf6b2db1_0 -> glib[version='>=2.58.3,<3.0a0']
conda-forge/win-64::gobject-introspection==1.64.1=py37hf0dd101_1 -> glib[version='>=2.58.3,<3.0a0']
conda-forge/label/cf202003/win-64::poppler==0.67.0=h1707e21_8 -> glib[version='>=2.58.3,<3.0a0']

Package jupyter conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> jupyter
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> jupyter

Package sphinxcontrib-applehelp conflicts for:
anaconda/win-64::spyder==4.1.4=py37_0 -> sphinx[version='>=0.6.6'] -> sphinxcontrib-applehelp
defaults/noarch::numpydoc==0.9.2=py_0 -> sphinx -> sphinxcontrib-applehelp
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> sphinxcontrib-applehelp
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> sphinx -> sphinxcontrib-applehelp

Package curl conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> curl
conda-forge/label/cf202003/win-64::poppler==0.67.0=h1707e21_8 -> curl[version='>=7.64.1,<8.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> libnetcdf[version='>=4.6.2,<4.6.3.0a0'] -> curl[version='>=7.59.0,<8.0a0|>=7.64.1,<8.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pycurl -> curl[version='7.55.*|>=7.55.1,<8.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> curl
conda-forge/label/cf202003/win-64::libnetcdf==4.6.2=h396784b_1001 -> curl[version='>=7.59.0,<8.0a0']

Package pyflakes conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pyflakes
anaconda/win-64::python-language-server==0.34.1=py37_0 -> pyflakes[version='>=2.2.0,<2.3.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pyflakes
anaconda/win-64::spyder==4.1.4=py37_0 -> python-language-server[version='>=0.34.0,<1.0.0'] -> pyflakes[version='>=2.2.0,<2.3.0']
anaconda/noarch::flake8==3.8.3=py_0 -> pyflakes[version='>=2.2.0,<2.3.0']

Package markdown conflicts for:
conda-forge/noarch::panel==0.9.5=py_1 -> markdown
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> panel[version='>=0.7.0'] -> markdown

Package json5 conflicts for:
defaults/noarch::jupyterlab_server==1.0.6=py_0 -> json5
defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0 -> jupyterlab_server[version='>=1.0.0,<2.0.0'] -> json5
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> json5
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> jupyterlab_server -> json5

Package gevent conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> gevent
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> gevent

Package greenlet conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> greenlet
defaults/win-64::gevent==1.4.0=py37he774522_0 -> greenlet[version='>=0.4.14']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> greenlet
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> gevent -> greenlet[version='>=0.4.10|>=0.4.13|>=0.4.14']

Package libgdal conflicts for:
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> gdal -> libgdal[version='2.3.2|2.3.2|2.3.3|3.0.2|>=2.2.2,<2.3.0a0|2.4.2',build='h1155b67_0|h10f50ba_0|heaf30cb_0|hcd84ac5_0|h4f71e3f_8']
conda-forge/label/cf202003/noarch::geopandas==0.7.0=py_1 -> fiona -> libgdal[version='>=2.2.2,<2.3.0a0|>=2.3.3,<2.4.0a0|>=3.0.2,<3.1.0a0|>=2.4.2,<2.5.0a0']

Package backports.os conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> backports.os
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> backports.os

Package sortedcollections conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> sortedcollections
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> sortedcollections

Package partd conflicts for:
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> dask[version='>=0.18.0'] -> partd[version='>=0.3.10|>=0.3.8']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> partd
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> dask -> partd[version='>=0.3.10|>=0.3.8']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> partd
defaults/noarch::dask==2.11.0=py_0 -> partd[version='>=0.3.10']
modin==0.7.3 -> dask[version='>=2.1.0'] -> partd[version='>=0.3.10']

Package pydocstyle conflicts for:
anaconda/win-64::spyder==4.1.4=py37_0 -> python-language-server[version='>=0.34.0,<1.0.0'] -> pydocstyle[version='>=2.0.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pydocstyle
anaconda/win-64::python-language-server==0.34.1=py37_0 -> pydocstyle[version='>=2.0.0']

Package pycurl conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pycurl
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pycurl

Package qtconsole conflicts for:
defaults/win-64::jupyter==1.0.0=py37_7 -> qtconsole
anaconda/win-64::spyder==4.1.4=py37_0 -> qtconsole[version='>=4.6.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> spyder -> qtconsole[version='>=4.2|>=4.6.0']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> qtconsole[version='>=4.6.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> qtconsole
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> qtconsole

Package lxml conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> lxml
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> lxml
conda-forge/win-64::jupyter_contrib_nbextensions==0.5.1=py37_0 -> lxml

Package icu conflicts for:
conda-forge/win-64::pycairo==1.19.1=py37h6430cfb_3 -> cairo[version='>=1.16.0,<1.17.0a0'] -> icu[version='>=58.2,<59.0a0']
defaults/win-64::qt==5.9.7=vc14h73c81de_0 -> icu[version='>=58.2,<59.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> icu[version='58.2.*|>=58.2,<59.0a0']
conda-forge/win-64::cairo==1.16.0=h63a05c6_1001 -> icu[version='>=58.2,<59.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> icu[version='58.2.*|>=58.2,<59.0a0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> icu
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> qt -> icu[version='>=58.2,<59.0a0']
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> icu[version='58.2.*|>=58.2,<59.0a0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> icu
conda-forge/win-64::gobject-introspection==1.64.1=py37hf0dd101_1 -> cairo[version='>=1.16.0,<1.17.0a0'] -> icu[version='>=58.2,<59.0a0']
defaults/win-64::pyqt==5.9.2=py37h6538335_2 -> qt=5.9 -> icu[version='>=58.2,<59.0a0']

Package cairo conflicts for:
conda-forge/win-64::pycairo==1.19.1=py37h6430cfb_3 -> cairo[version='>=1.16.0,<1.17.0a0']
conda-forge/win-64::gobject-introspection==1.64.1=py37hf0dd101_1 -> cairo[version='>=1.16.0,<1.17.0a0']
conda-forge/win-64::pygobject==3.36.1=py37hf6b2db1_0 -> gobject-introspection -> cairo[version='1.14.*|>=1.14.10,<2.0a0|>=1.14.12,<2.0a0|>=1.16.0,<1.17.0a0']

Package pathtools conflicts for:
defaults/win-64::watchdog==0.10.2=py37_0 -> pathtools[version='>=0.1.1']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pathtools
anaconda/win-64::spyder==4.1.4=py37_0 -> watchdog -> pathtools[version='>=0.1.1']

Package openpyxl conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> openpyxl
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> openpyxl

Package datashape conflicts for:
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> datashape[version='>=0.5.1']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> datashape

Package python-language-server conflicts for:
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> python-language-server[version='>=0.31.2,<0.32.0|>=0.31.9,<0.32.0|>=0.34.0,<1.0.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> spyder -> python-language-server[version='>=0.31.2,<0.32.0|>=0.31.9,<0.32.0|>=0.34.0,<1.0.0']
anaconda/win-64::spyder==4.1.4=py37_0 -> python-language-server[version='>=0.34.0,<1.0.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> python-language-server

Package pycodestyle conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pycodestyle
anaconda/noarch::flake8==3.8.3=py_0 -> pycodestyle[version='>=2.6.0,<2.7.0']
anaconda/win-64::python-language-server==0.34.1=py37_0 -> autopep8 -> pycodestyle[version='>=2.3|>=2.6']
defaults/noarch::autopep8==1.4.4=py_0 -> pycodestyle[version='>=2.3']
anaconda/win-64::spyder==4.1.4=py37_0 -> python-language-server[version='>=0.34.0,<1.0.0'] -> pycodestyle[version='>=2.6.0,<2.7.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pycodestyle
anaconda/win-64::python-language-server==0.34.1=py37_0 -> pycodestyle[version='>=2.6.0,<2.7.0']

Package qdarkstyle conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> qdarkstyle
anaconda/win-64::spyder==4.1.4=py37_0 -> qdarkstyle[version='>=2.8']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> spyder -> qdarkstyle[version='>=2.7|>=2.8']
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0'] -> qdarkstyle[version='>=2.7|>=2.8']

Package pyct-core conflicts for:
conda-forge/noarch::panel==0.9.5=py_1 -> pyct[version='>=0.4.4'] -> pyct-core=0.4.6
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> pyct[version='>=0.4.5'] -> pyct-core=0.4.6
conda-forge/noarch::pyct==0.4.6=py_0 -> pyct-core=0.4.6
conda-forge/noarch::colorcet==2.0.1=py_0 -> pyct[version='>=0.4.4'] -> pyct-core=0.4.6

Package ipywidgets conflicts for:
conda-forge/noarch::ipympl==0.5.6=pyh9f0ad1d_1 -> ipywidgets[version='>=7.5.0,<8.0']
conda-forge/noarch::pandas-profiling==2.8.0=py_0 -> ipywidgets[version='>=7.5.1']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> ipywidgets
defaults/win-64::jupyter==1.0.0=py37_7 -> ipywidgets
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> ipywidgets

Package pyperclip conflicts for:
conda-forge/win-64::cmd2==0.9.15=py37_0 -> pyperclip[version='>=1.5.27']
conda-forge/win-64::cliff==2.15.0=py37_0 -> cmd2!=0.8.3 -> pyperclip[version='>=1.5.27']

Package fsspec conflicts for:
modin==0.7.3 -> dask[version='>=2.1.0'] -> fsspec[version='>=0.4.0|>=0.5.1|>=0.6.0']
conda-forge/noarch::datashader==0.11.0=pyh9f0ad1d_0 -> dask[version='>=0.18.0'] -> fsspec[version='>=0.4.0|>=0.5.1|>=0.6.0']
defaults/noarch::dask==2.11.0=py_0 -> fsspec[version='>=0.6.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> dask -> fsspec[version='>=0.4.0|>=0.5.1|>=0.6.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> fsspec

Package mpmath conflicts for:
defaults/win-64::sympy==1.5.1=py37_0 -> mpmath[version='>=0.19']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> mpmath
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> mpmath
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> sympy -> mpmath[version='>=0.19']

Package liblief conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> liblief
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> liblief
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> py-lief -> liblief[version='0.10.1|0.9.0|0.9.0|0.9.0',build='ha925a31_2|ha925a31_0|ha925a31_3|ha925a31_0']
defaults/win-64::py-lief==0.9.0=py37ha925a31_3 -> liblief==0.9.0=ha925a31_3

Package pyrsistent conflicts for:
defaults/noarch::nbformat==5.0.4=py_0 -> jsonschema[version='>=2.4,!=2.5.0'] -> pyrsistent[version='>=0.14.0']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pyrsistent
defaults/noarch::jupyterlab_server==1.0.6=py_0 -> jsonschema[version='>=3.0.1'] -> pyrsistent[version='>=0.14.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> jsonschema -> pyrsistent[version='>=0.14.0']
defaults/win-64::jsonschema==3.2.0=py37_0 -> pyrsistent[version='>=0.14.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pyrsistent

Package contextlib2 conflicts for:
defaults/win-64::path==13.1.0=py37_0 -> importlib_metadata[version='>=0.5'] -> contextlib2
defaults/win-64::pytest==5.3.5=py37_0 -> importlib_metadata[version='>=0.12'] -> contextlib2
defaults/win-64::inflect==4.1.0=py37_0 -> importlib_metadata -> contextlib2
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> contextlib2
defaults/win-64::pluggy==0.13.1=py37_0 -> importlib_metadata[version='>=0.12'] -> contextlib2
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> contextlib2
defaults/win-64::keyring==21.1.0=py37_0 -> importlib_metadata -> contextlib2
defaults/win-64::jsonschema==3.2.0=py37_0 -> importlib_metadata -> contextlib2

Package cligj conflicts for:
conda-forge/label/cf202003/noarch::geopandas==0.7.0=py_1 -> fiona -> cligj[version='>=0.5']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> cligj[version='>=0.5']

Package pytables conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pytables
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pytables

Package mccabe conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> mccabe
anaconda/win-64::python-language-server==0.34.1=py37_0 -> pylint -> mccabe[version='>=0.6,<0.7']
anaconda/win-64::spyder==4.1.4=py37_0 -> pylint[version='>=1.0'] -> mccabe[version='>=0.6,<0.7|>=0.6.0,<0.7.0']
anaconda/win-64::python-language-server==0.34.1=py37_0 -> mccabe[version='>=0.6.0,<0.7.0']
defaults/win-64::pylint==2.4.4=py37_0 -> mccabe[version='>=0.6,<0.7']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pylint -> mccabe[version='>=0.6,<0.7']
anaconda/noarch::flake8==3.8.3=py_0 -> mccabe[version='>=0.6.0,<0.7.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> mccabe

Package pep8 conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pep8
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pep8

Package jmespath conflicts for:
boto3 -> jmespath[version='>=0.7.1,<1.0.0']
anaconda/win-64::s3transfer==0.1.13=py37_0 -> botocore[version='>=1.3.0,<2.0.0'] -> jmespath[version='>=0.7.1,<1.0.0']
defaults/noarch::botocore==1.12.189=py_0 -> jmespath[version='>=0.7.1,<1.0.0']

Package gdal conflicts for:
conda-forge/label/cf202003/noarch::geopandas==0.7.0=py_1 -> fiona -> gdal=2.2
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> gdal

Package powershell_shortcut conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> powershell_shortcut
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> powershell_shortcut

Package libspatialite conflicts for:
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> libspatialite[version='>=4.3.0a,<4.4.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> libspatialite[version='>=4.3.0a,<4.4.0a0']
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> libspatialite[version='>=4.3.0a,<4.4.0a0']

Package pywin32-ctypes conflicts for:
anaconda/win-64::spyder==4.1.4=py37_0 -> keyring -> pywin32-ctypes[version='!=0.1.0,!=0.1.1']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> keyring -> pywin32-ctypes[version='!=0.1.0,!=0.1.1']
defaults/win-64::keyring==21.1.0=py37_0 -> pywin32-ctypes
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> pywin32-ctypes

Package webencodings conflicts for:
conda-forge/noarch::pbr==5.4.2=py_0 -> pip -> webencodings
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> webencodings
defaults/win-64::nbconvert==5.6.1=py37_0 -> bleach -> webencodings
defaults/win-64::python==3.7.6=h60c2a47_2 -> pip -> webencodings
defaults/win-64::html5lib==1.0.1=py37_0 -> webencodings
defaults/win-64::bleach==3.1.0=py37_0 -> webencodings
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> webencodings

Package sphinxcontrib-serializinghtml conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> sphinxcontrib-serializinghtml
defaults/noarch::numpydoc==0.9.2=py_0 -> sphinx -> sphinxcontrib-serializinghtml
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> sphinx -> sphinxcontrib-serializinghtml
anaconda/win-64::spyder==4.1.4=py37_0 -> sphinx[version='>=0.6.6'] -> sphinxcontrib-serializinghtml

Package xlwt conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> xlwt
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> xlwt

Package anaconda-client conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> anaconda-client
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> anaconda-client
defaults/noarch::anaconda-project==0.8.4=py_0 -> anaconda-client

Package comtypes conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> comtypes
defaults/win-64::xlwings==0.17.1=py37_0 -> comtypes
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> comtypes

Package m2w64-expat conflicts for:
defaults/win-64::m2w64-xz==5.2.2=2 -> m2w64-gettext -> m2w64-expat
defaults/win-64::m2w64-gettext==0.19.7=2 -> m2w64-expat

Package bitarray conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> bitarray
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> bitarray

Package jaraco.itertools conflicts for:
conda-forge/win-64::importlib-metadata==1.6.1=py37hc8dfbb8_0 -> zipp[version='>=0.5'] -> jaraco.itertools
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> zipp -> jaraco.itertools
defaults/win-64::importlib_metadata==1.5.0=py37_0 -> zipp[version='>=0.5'] -> jaraco.itertools

Package pyviz_comms conflicts for:
conda-forge/noarch::panel==0.9.5=py_1 -> pyviz_comms[version='>=0.7.4']
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> pyviz_comms[version='>=0.7.4']
conda-forge/noarch::holoviews==1.13.2=pyh9f0ad1d_0 -> panel[version='>=0.7.0'] -> pyviz_comms[version='>=0.7.2|>=0.7.3']

Package poppler conflicts for:
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> poppler[version='>=0.67.0,<0.68.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> poppler[version='>=0.67.0,<0.68.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> poppler[version='>=0.67.0,<0.68.0a0']

Package astroid conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> astroid
anaconda/win-64::python-language-server==0.34.1=py37_0 -> pylint -> astroid[version='>=1.5.1|>=1.6,<2.0|>=2.0.0|>=2.2.0|>=2.3.0,<2.4|>=2.4.0,<2.5|>=2.4.0,<=2.5']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> astroid
defaults/win-64::pylint==2.4.4=py37_0 -> astroid[version='>=2.3.0,<2.4']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> pylint -> astroid[version='>=1.5.1|>=1.6,<2.0|>=2.0.0|>=2.2.0|>=2.3.0,<2.4|>=2.4.0,<2.5|>=2.4.0,<=2.5']
anaconda/win-64::spyder==4.1.4=py37_0 -> pylint[version='>=1.0'] -> astroid[version='>=1.5.1|>=1.6,<2.0|>=2.0.0|>=2.2.0|>=2.3.0,<2.4|>=2.4.0,<2.5|>=2.4.0,<=2.5']

Package h5py conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> h5py
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> h5py

Package daal4py conflicts for:
conda-forge/win-64::tslearn==0.3.1=py37hbc2f12b_0 -> scikit-learn -> daal4py
conda-forge/win-64::umap-learn==0.4.4=py37hc8dfbb8_0 -> scikit-learn[version='>=0.20'] -> daal4py
anaconda/noarch::pynndescent==0.4.8=py_1 -> scikit-learn[version='>=0.19'] -> daal4py
conda-forge/noarch::imbalanced-learn==0.7.0=py_1 -> scikit-learn[version='>=0.23'] -> daal4py
anaconda/win-64::py-xgboost==0.90=py37_1 -> scikit-learn -> daal4py
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> scikit-learn -> daal4py

Package sympy conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> sympy
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> sympy

Package xlrd conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> xlrd
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> xlrd

Package typed-ast conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> typed-ast
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> astroid -> typed-ast[version='>=1.3.0|>=1.4.0,<1.5']
defaults/win-64::pylint==2.4.4=py37_0 -> astroid[version='>=2.3.0,<2.4'] -> typed-ast

Package locket conflicts for:
defaults/noarch::dask==2.11.0=py_0 -> partd[version='>=0.3.10'] -> locket
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> locket
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> locket
defaults/noarch::partd==1.1.0=py_0 -> locket

Package py-lief conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> py-lief
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> py-lief

Package xerces-c conflicts for:
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> xerces-c[version='>=3.2.2,<3.2.3.0a0']
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> xerces-c[version='>=3.2.2,<3.2.3.0a0']
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> xerces-c[version='>=3.2.2,<3.2.3.0a0']

Package filelock conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> filelock
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> filelock

Package bcrypt conflicts for:
anaconda/win-64::spyder==4.1.4=py37_0 -> paramiko[version='>=2.4.0'] -> bcrypt[version='>=3.1.3']
defaults/noarch::paramiko==2.7.1=py_0 -> bcrypt[version='>=3.1.3']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> bcrypt

Package console_shortcut conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> console_shortcut
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> console_shortcut

Package ruamel_yaml conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> anaconda-client -> ruamel_yaml[version='<0.15|>=0.11.14']
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> ruamel_yaml
defaults/noarch::anaconda-project==0.8.4=py_0 -> anaconda-client -> ruamel_yaml[version='<0.15|>=0.11.14']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> ruamel_yaml
defaults/noarch::anaconda-project==0.8.4=py_0 -> ruamel_yaml

Package jupyterlab_server conflicts for:
defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0 -> jupyterlab_server[version='>=1.0.0,<2.0.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> jupyterlab -> jupyterlab_server[version='>=0.2.0,<0.3.0|>=1.0.0,<2.0.0|>=1.1.0,<2.0.0']
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> jupyterlab_server
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> jupyterlab_server

Package ffmpeg conflicts for:
conda-forge/noarch::imageio-ffmpeg==0.4.2=py_0 -> ffmpeg
conda-forge/noarch::moviepy==1.0.1=py_0 -> ffmpeg

Package stevedore conflicts for:
conda-forge/win-64::cliff==2.15.0=py37_0 -> stevedore[version='>=1.20.0']
conda-forge/noarch::optuna==1.0.0=py_0 -> cliff -> stevedore[version='>=1.20.0']

Package libllvm9 conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> llvmlite -> libllvm9
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> libllvm9
anaconda/noarch::pynndescent==0.4.8=py_1 -> llvmlite[version='>=0.30'] -> libllvm9

Package m2w64-libiconv conflicts for:
defaults/win-64::m2w64-xz==5.2.2=2 -> m2w64-gettext -> m2w64-libiconv
defaults/win-64::m2w64-gettext==0.19.7=2 -> m2w64-libiconv

Package retrying conflicts for:
plotly/noarch::plotly_express==0.4.1=py_0 -> plotly[version='>=4.1.0'] -> retrying[version='>=1.3.3']
defaults/noarch::plotly==4.8.1=py_0 -> retrying[version='>=1.3.3']

Package cython conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> cython
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> cython

Package spyder conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> spyder
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> spyder
spyder-ide/win-64::spyder-terminal==0.3.1=py37hc8dfbb8_1 -> spyder[version='>=4.0.0,<5.0.0']

Package arrow conflicts for:
conda-forge/noarch::jinja2-time==0.2.0=py_2 -> arrow
conda-forge/win-64::cookiecutter==1.6.0=py37_1000 -> jinja2-time[version='>=0.1.0'] -> arrow

Package kealib conflicts for:
conda-forge/label/cf202003/win-64::fiona==1.8.9.post2=py37h3234bc7_0 -> libgdal[version='>=2.4.2,<2.5.0a0'] -> kealib[version='>=1.4.10,<1.5.0a0']
conda-forge/label/cf202003/win-64::gdal==2.4.2=py37he6b6c38_8 -> libgdal==2.4.2=h4f71e3f_8 -> kealib[version='>=1.4.10,<1.5.0a0']
conda-forge/label/cf202003/win-64::libgdal==2.4.2=h4f71e3f_8 -> kealib[version='>=1.4.10,<1.5.0a0']

Package glob2 conflicts for:
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> glob2
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> glob2

Package xlsxwriter conflicts for:
defaults/win-64::_anaconda_depends==2019.03=py37_0 -> xlsxwriter
defaults/win-64::anaconda==custom=py37_1 -> _anaconda_depends -> xlsxwriterThe following specifications were found to be incompatible with your CUDA driver:

  - feature:/win-64::__cuda==10.1=0
  - feature:|@/win-64::__cuda==10.1=0

Your installed CUDA driver is: 10.1
```
"
685156845,35886,QST:,1994Shankar,closed,2020-08-25T04:18:23Z,2020-09-03T10:48:39Z,"import pandas as pd
import xlrd
book = xlrd.open_workbook(""Excel_to_panda.xlsx"")

getting below error:-

/usr/local/bin/python3.8 /Users/bhabanishankarkar/PycharmProjects/untitled/hello.py
Traceback (most recent call last):
  File ""/Users/bhabanishankarkar/PycharmProjects/untitled/hello.py"", line 71, in <module>
    book = xlrd.open_workbook(""Excel_to_panda.xlsx"")
  File ""/Users/bhabanishankarkar/Library/Python/3.8/lib/python/site-packages/xlrd/__init__.py"", line 145, in open_workbook
    raise XLRDError('ZIP file contents not a known type of workbook')
xlrd.biffh.XLRDError: ZIP file contents not a known type of workbook

Process finished with exit code 1

Can anyone help with this?
"
685930824,35901,BUG: Transformer Second Layer Access,PannuMuthu,closed,2020-08-26T01:11:16Z,2020-09-03T10:49:12Z,"pretrained_weights='bert-base-uncased'
tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)

`features = []
input_ids = torch.tensor([self.tokenizer.encode(phrase, add_special_tokens=True)])
with torch.no_grad():
    # Models outputs are tuples
    outputs = self.model(input_ids)
    last_hidden_states = outputs[0]
    avg_pool_hidden_states = np.average(last_hidden_states[0], axis=0))
    return avg_pool_hidden_states`
I am working on Sentence Similarity problem, given sentence S, find similar sentences W. So I want to encode sentence S and find the closest top-K sentences from W.

I have read the documentations, but I want to confirm few things how do I do. the following:

How do I get the second to last layer?
Am I average pooling the last hidden states correctly?"
687392360,35932,BUG:Installation errors on aarch64,tathastu871,closed,2020-08-27T16:49:33Z,2020-09-03T10:49:57Z,"- [+] I have checked that this issue has not already been reported.

- [+] I have confirmed this bug exists on the latest version of pandas.

- [+] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
pip3 install pandas
or
pip3 install pandas --no-build-isolation --no-binary :all:
or
python setup.py build_ext --inplace --force
or 
LDFLAGS="" -lm -lcompiler_rt"" pip3 install pandas --no-build-isolation --no-binary :all: --no-use-pep517
```

#### Problem description
```https://termbin.com/wzsr```
[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output

#### Output of ``pd.show_versions()``
When installed from source instead of pip it installs pandas but no submodules
Output of pd.show_versions()
```AttributeError: module 'pandas' has no attribute 'show_versions'```

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
691460363,36079,BUG:,nicolas-gervais,closed,2020-09-02T22:04:54Z,2020-09-03T11:01:04Z,"SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame"
690031894,36033,Coverting bug by null values from csv into panda nan to a python list,Kaufhold,closed,2020-09-01T10:37:24Z,2020-09-03T11:33:24Z,"Hello,
I read a csv file with panda and convert it to a python list.
The problem is that panda convert the null values from the csv to nan values.
After that I convert the panda dataframe to a python nested list and got nan values inside the python list that have the float type. 

Code:
`for chunk in pd.read_csv(pfad_incoming + record, chunksize=1000000, delimiter=';', quotechar=""'"",                                  escapechar='""'):

batch = [chunk.columns.values.tolist()] + chunk.values.tolist()`

A column:
[5000000026661893028, 26661987940, '2020-07-09 10:40:10.435000', 1, nan, nan, nan, nan, nan, nan, nan, nan, 'v5', nan]
The type of the nan values:
<class 'float'>

Is that a bug? In the python list, however, should nan be None, right?
Regards,
Phil
"
679080684,35717,CLN: remove extant uses of built-in filter function,simonjayhawkins,closed,2020-08-14T11:01:42Z,2020-09-03T12:15:58Z,No internet for a good part of yesterday so some time to experiment with code cleanups.
691582827,36083,BUG: Plotting charts in different orders creates different plot outputs,cinerea0,closed,2020-09-03T02:37:24Z,2020-09-03T13:31:55Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample

```python
import numpy as np
import pandas as pd

df = pd.DataFrame(
    {
        ""date"": [
            ""2000-01-01"",
            ""2000-01-01"",
            ""2000-01-01"",
            ""2000-01-02"",
            ""2000-01-02"",
            ""2000-01-02"",
            ""2000-01-03"",
            ""2000-01-03"",
            ""2000-01-03"",
        ],
        ""group"": [""A"", ""B"", ""C"", ""A"", ""B"", ""C"", ""A"", ""B"", ""C""],
    }
)
df[""count""] = np.random.randint(0, 5, df.shape[0])
df[""date""] = pd.to_datetime(df[""date""])

separate_plot = df.groupby([""date"", ""group""])[""count""].sum().unstack().plot()
separate_plot.get_figure().savefig(""separate_plot.png"")
total_plot = df.groupby(""date"")[""count""].sum().plot()
total_plot.get_figure().savefig(""total_plot.png"")
```

#### Problem description

Plotting the ""separate"" plot before the ""total"" plot results in the following graphs:

*separate_plot.png*:
![separate_plot](https://user-images.githubusercontent.com/49368915/92064588-4ad70a80-ed8d-11ea-8b7d-93d833a52e69.png)

*total_plot.png*:
![total_plot](https://user-images.githubusercontent.com/49368915/92064626-5f1b0780-ed8d-11ea-8820-b1e1fe34a5b4.png)

As you can see, the lines from the ""separate"" plot also appear on the ""total"" plot, despite not grouping by ""group"" in the ""total"" plot. In my testing, this only occurs when the ""separate"" plot is made before the ""total"" plot.

#### Expected Output

The graphs should look something like the following regardless of which is plotted first:

*separate_plot.png*:
![separate_plot](https://user-images.githubusercontent.com/49368915/92064962-38110580-ed8e-11ea-8f15-54d753d93835.png)

*total_plot.png*:
![total_plot](https://user-images.githubusercontent.com/49368915/92064987-46f7b800-ed8e-11ea-965c-90f46a31cde8.png)

#### Output of ``pd.show_versions()``

<details>

/usr/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.8.5_1
Version          : #1 SMP Thu Aug 27 08:23:40 UTC 2020
machine          : x86_64
processor        :
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.19.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 50.0.0
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
677311281,35681,BUG/ENH: compression for google cloud storage in to_csv,twoertwein,closed,2020-08-12T01:38:04Z,2020-09-03T15:56:27Z,"- [x] closes #35677, closes #26124, and closes #32392
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

By inferring the compression before converting the path to a file object `df.to_csv(""gs://mybucket/test2.csv.gz"", compression=""infer"", mode=""wb"")` works. By wrapping fsspec file-objects in a `TextIOWrapper` `df.to_csv(""gs://mybucket/test2.csv"", mode=""wb"")` works as well. Path-like objects that are internally converted to file-like objects (in `get_filepath_or_buffer`) are now always opened in binary mode (unless text mode is explicitly requested) and the potentially changed mode is returned (no need to specify `mode=""wb""` for google cloud files). As long as the google file is opened in binary mode (which is now always the case), we also honor the requested `encoding`.

This PR also fixes Zip compression for file objects not having a name."
691997117,36091,PERF: use from __future__ import annotations more,simonjayhawkins,closed,2020-09-03T13:58:29Z,2020-09-03T16:07:34Z,"xref https://github.com/pandas-dev/pandas/pull/36034#issuecomment-684964511

@WillAyd using `python -X importtime -c ""import pandas""` I don't see any different in import time after running a few times and things are cached.

so I don't think we gain anything from a performance POV and therefore only need to use it for stylistic reasons in a few modules and not across the codebase.

I've opened this PR, in case someone wants to test performance with these changes.

so feel free to close
"
691894724,36087,DOC: Add trailing dot,albertvillanova,closed,2020-09-03T11:38:59Z,2020-09-03T16:14:18Z,"
"
691933021,36090,DOC: add mypy version to whatsnew\v1.2.0.rst,simonjayhawkins,closed,2020-09-03T12:36:39Z,2020-09-03T16:45:03Z,xref https://github.com/pandas-dev/pandas/pull/36012#issuecomment-685188102
691884990,36086,DOC: minor fixes to whatsnew\v1.1.2.rst,simonjayhawkins,closed,2020-09-03T11:25:04Z,2020-09-03T16:46:30Z,https://pandas.pydata.org/pandas-docs/dev/whatsnew/v1.1.2.html
688674351,35985,DOC: make difference between numpy behaviour clearer in Dataframe.std() and Series.std(),timhunderwood,closed,2020-08-30T08:01:06Z,2020-09-03T16:58:36Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.DataFrame.std.html
https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.Series.std.html

#### Documentation problem

The `ddof` kwarg has a different default to numpy. This means using the `std()` method on a `Series` or `numpy.array` with the same values give different results:

```python
>>>df=pandas.Series([1,2,3])
>>>df.std()
Out[4]: 1.0
>>>df.values.std()
Out[5]: 0.816496580927726
```

#### Suggested fix for documentation

I assume the difference in behaviour is intentional, but I would suggest making this difference clearer in the documentation.

We could add the text: ""Note that this normalization is different to numpy, which by default normalizes by N (equivalent to `ddof=0`)."" 
A similar string could be added in the kwarg description for ddof.

This would make it clear from reading the pandas docs that the behaviour is different to numpy, rather than having to compare the numpy docs side by side after noticing a discrepancy. It would also make users more likely to consider which ddof value they want for their use case.

If you agree, I am happy to make the PR to change this.

"
688704996,35986,DOC: Add Notes about difference to numpy behaviour for ddof in std() GH35985,timhunderwood,closed,2020-08-30T11:51:51Z,2020-09-03T16:58:49Z,"- [x] closes #35985
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
692115121,36095,Backport PR #36086 on branch 1.1.x (DOC: minor fixes to whatsnew\v1.1.2.rst),meeseeksmachine,closed,2020-09-03T16:30:25Z,2020-09-03T18:02:50Z,Backport PR #36086: DOC: minor fixes to whatsnew\v1.1.2.rst
654860372,35215,BUG: memory spike in DatetimeIndex after .loc based assignment,mdering,closed,2020-07-10T15:28:09Z,2020-09-03T18:20:12Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
n = 1500000
idx = pd.date_range(end = pd.Timestamp.now().floor(""1T""), freq='1T', periods=n)
srs1 = pd.Series(np.random.random(n), idx)
srs2 = pd.Series(np.random.random(n), idx)
print(sys.getsizeof(srs1.index)) # 12000032
print(sys.getsizeof(srs2.index)) # 12000032
filter_srs = srs1[srs1 > .9].index
srs2.loc[filter_srs] = .9
print(sys.getsizeof(srs2.index)) # 53943072
assert (srs1.index == srs2.index).all()
```

#### Problem description

hi pandas, after filtering a series on some index, theres a strange spike in memory usage of the index where it can grow in size by a factor of > 4. I know this is a strange use case but its a MWE of a version where the `srs2` index might be mutated. The `nbytes` attributes of the indexes stay the same, but the overall memory usage according to sys grows larger and stays large. Also, if i do something like `srs2.index = pd.DatetimeIndex(srs2.index)` the memory usage falls back to a normal level
#### Expected Output
I would expect all of them to be the same.
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.5.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1.post20200622
Cython           : 0.29.20
pytest           : 5.4.3
hypothesis       : 5.18.3
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.1
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.9
numba            : 0.50.1

</details>
"
690463211,36052,CLN remove unnecessary trailing commas in pandas/io,sarthakvk,closed,2020-09-01T21:28:54Z,2020-09-03T20:04:49Z,"@MarcoGorelli can you review this PR this is related to issue #35925, if this looks good to you I can open another PR
- [x] pandas/io/sas/sas_xport.py
- [x] pandas/io/stata.py
  
- [x] passes `black pandas`

"
690185602,36037,BUG: transform zscore - weird behavior for not rounded floats,mglowacki100,closed,2020-09-01T14:22:00Z,2020-09-04T08:33:09Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

```
import pandas as pd
import numpy as np
from scipy import stats #1.15
from functools import partial


scorer = partial(stats.zscore, nan_policy='omit')
n = -0.405465999999999999999999999999999999999999999999999999
a = [np.nan, np.nan, np.nan, np.nan, n, n, n] 
df = pd.DataFrame({'c1': ['a', 'a', 'a', 'a', 'a', 'a', 'a'], 'c2': a})

df.groupby(['c1'])['c2'].transform(scorer)
```
Gives:
```
0    NaN
1    NaN
2    NaN
3    NaN
4   -1.0
5   -1.0
6   -1.0
Name: c2, dtype: float64
```

#### Problem description
It should give result like for e.g. `n=0.5` when it works as expected. 
I've checked and `n = -0.405465999999999999999999999999999999999999999999999999` and `stats.zscore(np.array(a))` and it worked correctly so I suppose issue is within pandas.
A little background about `stats.zscore` from `scipy`:
- returns `np.nan`  for `nans` (`nan_policy`)
- if rest element of vector are the same it should return `np.nan` due to division by 0 (std deviation is zero)

Long `n` is just for easy reproduction, issue happened with real data. My temporary workaround is to round values.
#### Expected Output
```
/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:2419: RuntimeWarning: invalid value encountered in true_divide
  return (a - mns) / sstd
0   NaN
1   NaN
2   NaN
3   NaN
4   NaN
5   NaN
6   NaN
```
#### Output of ``pd.show_versions()``
1.1.1
<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
692959310,36112,DOC: sync doc/source/whatsnew/v1.1.2.rst on 1.1.x,simonjayhawkins,closed,2020-09-04T09:28:13Z,2020-09-04T11:44:29Z,~~don't merge this yet. I'll make sure a test is failing first~~
606948286,33802,Why not support `GroupBy.rank(numeric_only=False)`?,UlionTse,closed,2020-04-26T07:54:43Z,2020-09-04T13:05:19Z,
608488033,33849,BUG: weird behaviour of pivot_table when  aggfunc tries to join string sequence containing None,jkukul,open,2020-04-28T17:27:14Z,2020-09-04T13:07:11Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame({""A"": [""foo"", ""foo"", ""foo"", ""foo"", ""foo"",
                          ""bar"", ""bar"", ""bar"", ""bar""],
                   ""B"": [""one"", ""one"", ""one"", ""two"", ""two"",
                         ""one"", ""one"", ""two"", None],
                   ""C"": [""small"", ""large"", ""large"", ""small"",
                         ""small"", ""large"", ""small"", ""small"",
                         ""large""]})
df.pivot_table(index='A', columns='C', values='B', aggfunc=lambda x: ' '.join(x))
```

#### Problem description

`None` value in the `B` column should make the aggregation fail or, alternatively, the aggregation should ignore the `None` value. Instead, the output looks like this:

```
C    large  small
A
bar  A B C  A B C
foo  A B C  A B C
```

It looks like the names of all the pivot-ed columns were actually aggregated, which is a very surprising behaviour.

#### Expected Output

`TypeError: sequence item X: expected str instance, NoneType found`, just like `''.join([..., None])` would return

Alternatively, the `None` value could be ignored and then the output could look like this:
```
C      large        small
A
bar      one     one two
foo  one one  one two two
```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None

pandas: 0.20.1
pytest: None
pip: 20.1
setuptools: 46.1.3
Cython: 0.28.5
numpy: 1.15.0
scipy: 1.1.0
xarray: None
IPython: 7.6.1
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2017.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.1.1
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: 4.8.0
html5lib: None
sqlalchemy: 1.2.1
pymysql: None
psycopg2: 2.8.3 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
pandas_gbq: None
pandas_datareader: None

</details>
"
613605811,34035,BUG: wrong join with nan in multiindex,gertjac,closed,2020-05-06T20:52:37Z,2020-09-04T13:12:22Z,"- [ ] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
df1 = pd.DataFrame([['1.','3.','4.'],
                    ['1.',pd.NA,'5.']],
                   columns=['i1', 'i2', 'v1']).set_index(['i1', 'i2'])
df2 = pd.DataFrame([['1.',pd.NA,'6.']],
                   columns=['i1', 'i2', 'v2']).set_index(['i1', 'i2'])
df1.join(df2, how='outer').loc[('1.', '3.')]['v2']
```

#### Problem description

the result is '6.'
so, the value of v2 in df2 appears as the value of v2 for index ('1.', '3.') after joining
this is unexpected for me, because the index is different from that in df2 ('1.',pd.NA)
this problem is also there with np.nan instead of pd.NA
and also in version 0.25.3 (when values are strings, but not when values are floats!)

#### Expected Output

NaN

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
 
pandas           : 1.0.3
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : 0.29.12
pytest           : 5.0.1
hypothesis       : None
sphinx           : 2.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.3.4
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.6.1
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.1.0
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.0.1
pyxlsb           : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : 1.3.5
tables           : 3.5.2
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8
numba            : 0.44.1

</details>
"
613954329,34046,"ENH: piped alias for .merge(left_index=True,right_index=True,how='outer')",MiguelArriaga,closed,2020-05-07T10:41:10Z,2020-09-04T13:13:38Z,"#### Is your feature request related to a problem?

The way to merge two dataframes by index and default 'outer' is to use concat(axis=1), but this does not work if I want to follow a piped workflow. I would like to do the equivalent of df.merge(left_index=True,right_index=True,how='outer') but in a less verbose way.

#### Describe the solution you'd like

```python
def merge_byindex(self,df2,how='outer',**kwargs):
    """"""Merge input DataFrame by index.

         It is equivalent to doing pd.DataFrame.merge(left_index=True,right_index=True,how='outer',**kwargs).
    """"""
    return self.merge(df2,left_index=True,right_index=True,how='outer',**kwargs)
```
#### API breaking implications

no breaking implications

#### Describe alternatives you've considered

The natural alternative is to just keep doing .merge(left_index=True,right_index=True,how='outer')

#### Additional context

The reasons for having this as a separate function are the following:

1. The level of verbosity required to do this simple operation seems unreasonable.
2. Seems that maybe this particular operation could be more efficient than a general merge.
3. It would greatly simplify the documentation for this particular operation




"
646894366,35038,BUG: DataFrame.append with empty DataFrame and Series with tz-aware datetime value allocated object column,simonjayhawkins,closed,2020-06-28T10:59:27Z,2020-09-04T13:26:59Z,"broken off #35032

This PR fixes situation where appending a series consecutively to a empty dataframe produces different result from appending the series in one step.

on master
```
>>> import pandas as pd
>>>
>>> pd.__version__
'1.1.0.dev0+1974.g0159cba6e'
>>>
>>> import dateutil
>>>
>>> date = pd.Timestamp(""2018-10-24 07:30:00"", tz=dateutil.tz.tzutc())
>>> date
Timestamp('2018-10-24 07:30:00+0000', tz='tzutc()')
>>>
>>> s = pd.Series({""date"": date, ""a"": 1.0, ""b"": 2.0})
>>> s
date    2018-10-24 07:30:00+00:00
a                               1
b                               2
dtype: object
>>>
>>> df = pd.DataFrame(columns=[""c"", ""d""])
>>> df
Empty DataFrame
Columns: [c, d]
Index: []
>>>
>>> result_a = df.append(s, ignore_index=True)
>>> result_a
     c    d    a    b                       date
0  NaN  NaN  1.0  2.0  2018-10-24 07:30:00+00:00
>>>
>>> result_a.dtypes
c        object
d        object
a       float64
b       float64
date     object
dtype: object
>>>
>>> result_b = result_a.append(s, ignore_index=True)
>>> result_b
     c    d    a    b                       date
0  NaN  NaN  1.0  2.0  2018-10-24 07:30:00+00:00
1  NaN  NaN  1.0  2.0  2018-10-24 07:30:00+00:00
>>>
>>> result_b.dtypes
c        object
d        object
a       float64
b       float64
date     object
dtype: object
>>>
>>> result = df.append([s, s], ignore_index=True)
>>> result
     c    d                      date    a    b
0  NaN  NaN 2018-10-24 07:30:00+00:00  1.0  2.0
1  NaN  NaN 2018-10-24 07:30:00+00:00  1.0  2.0
>>>
>>> result.dtypes
c                        object
d                        object
date    datetime64[ns, tzutc()]
a                       float64
b                       float64
dtype: object
>>>
```"
622562646,34292,BUG: left join between df with a single index and df with a multiindex produces an inner join,CuylenE,open,2020-05-21T15:02:28Z,2020-09-04T13:31:11Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample

```python
A = pd.DataFrame([1,2], columns=[""i""]).set_index([""i""])
B = pd.DataFrame([(1,4),(3,0),(1,5)], columns=[""i"", ""ii""]).set_index([""i"", ""ii""])
A.join(B, how=""left"")
#Empty DataFrame
#Columns: []
#Index: [(1, 4), (1, 5)]
```

#### Problem description

Joining a df with 1 index and a df with a multiindex always generates an inner join, no matter the value of ""how"". In this case index-value 2 from the left df is missing, while it should be kept.

The same happens when just joining the indexes. This problem does not happen when joining 2 multi-indexes.

It seems something in the implementation in file pandas\core\indexes\base.py , class Index, method _join_level goes wrong. The generated new codes are either calculated wrong or don't take into account the join-method. But that's as far as I've gotten.

#### Expected Output

Empty DataFrame
Columns: []
Index: [(1, 4), (1, 5), (2, nan)]


#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
629162718,34528,ENH: Implement categorical correlation,idankash,closed,2020-06-02T12:27:40Z,2020-09-04T13:33:38Z,"#### Is your feature request related to a problem?

Currently, whenever I want to visualize features correlation I'm using df.corr(), but when I have a dataset with categorical features I have to encode them before using df.corr() if I want to include the categorical features.

#### Describe the solution you'd like

I came across this article 
[https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9](https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9) 
and I wondered if I can implement the solution as a new feature or add it into corr().


#### API breaking implications

Maybe add another boolean parameter to corr which indicate that the user wants to include categorical features?

Thanks :)
"
631571377,34598,BUG: Groupby: Int-Datatype (of group-by-columns) always casted to int64,Mueller2-Patrick,closed,2020-06-05T12:59:55Z,2020-09-04T13:42:52Z,"- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of pandas.
- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.
---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

df = pd.DataFrame({'col1':[1,2,3],'col2':[10,11,12]})
df['col1'] = df['col1'].astype(np.int8)

print(df.dtypes)

print(df.groupby('col1', as_index=False)['col2'].count().dtypes)
```

#### Problem description

**Before the groupby:** The dtype of col1 is np.int8:
Output of 
```python
print(df.dtypes):

col1     int8
col2    int64
dtype: object
```

**After the groupby** the dtype of col1 is np.int64
Output of 
```python
print(df.groupby('col1', as_index=False)['col2'].count().dtypes):

col1    int64
col2    int64
dtype: object
```

#### Expected Output
```python
print(df.groupby('col1', as_index=False)['col2'].count().dtypes) gives output:

col1    int8
col2    int64
dtype: object
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.0.4
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 10.0.1
setuptools       : 45.2.0
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : 2.3.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.0.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.12.0
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.12
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
634273630,34637,ENH: specify function to determine NaN when using fillna(),JakobHenning,closed,2020-06-08T07:13:07Z,2020-09-04T13:47:45Z,"#### Is your feature request related to a problem?

When using `pd.fillna(df)` it does not fill all of the values that `pd.isna(df)` returns as `True`. Furthermore you might have some `NaN` values you want to fill, and some you don't

#### Describe the solution you'd like
include a `nan_function` to be parsed to `fillna()`. The `nan_function` should take a value (or row, or Series) as input and return True/False. Based on that `fillna` determines which values to replace.

```python

#examples
df = df.fillna(nan_function = pd.isnull, values = np.nan) #What pd.isnull returns as True is set to np.nan
df = df.fillna(nan_function = lambda x: x<0, values = np.nan) #Makes all negative numbers np.nan
df = df.fillna(nan_function = np.nan, values = 0) #Only fills np.nan with zeroes
```"
635308853,34662,BUG: read_excel with dtype=np.int32 can't handle empty or nan cells,joha1,closed,2020-06-09T10:27:06Z,2020-09-04T13:49:05Z,"- [x] I have checked that this issue has not already been reported.
Though there's some similar issues, like https://github.com/pandas-dev/pandas/issues/20377

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

**EDIT:**
(won't let me comment)
Apparently, it's possible with `dtype='Int64'` (capital `I` and in ticks).
https://pandas.pydata.org/pandas-docs/version/0.24/whatsnew/v0.24.0.html#optional-integer-na-support
Unfortunately, that's not very well documented. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html makes it sound like you're supposed to give it a numpy type.
So it's more of a documentation issue, instead of a bug
**/EDIT**

```python
# save the lines below as test.xls:

ID  string
1   a
2   b
3   c
    d
4   e
nan f
6   g

# code:
test = pd.read_excel('test.xls')
test
    ID string
0  1.0      a
1  2.0      b
2  3.0      c
3  NaN      d
4  4.0      e
5  NaN      f
6  6.0      g

# note that the IDs gained a decimal! They're needed as an ID to retrive some files, to that 0 is breaking things. So let's import `ID` as integer:

test = pd.read_excel('test.xls', dtype={'ID':np.int32})        

ValueError                                Traceback (most recent call last)
...
ValueError: Cannot convert non-finite values (NA or inf) to integer
...
ValueError: Unable to convert column ID to type <class 'numpy.int32'>

# similar things happen with the nan cell.
```

#### Problem description

Importing an excel file with `nan` or ` ` in a column of integers fails, when specifying this column as `dtype=np.int32`

#### Expected Output

setting `nan` or ` ` as `np.nan` or similar or having a documented workaround in the documentation. `na_filter=False` seems promising, but doesn't do anything.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.18.0-147.8.1.el8_1.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 47.1.1.post20200604
Cython           : 0.29.17
pytest           : 5.4.2
hypothesis       : 5.5.4
sphinx           : 3.0.4
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.8
numba            : 0.49.1


</details>
"
638744549,34791,ENH: Inconsistency when string refers both to index level and column label,ChrisStuff,closed,2020-06-15T10:39:59Z,2020-09-04T14:04:01Z,"While [groupby](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups) throws a ValueError when the string parameter refers both to an index level and column name, [query](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html?highlight=query%20method#the-query-method) gives the column precedence in such a case:

```python
>>> df = pd.DataFrame({'a':[1,2,3,4,5], 'b':[3,3,3,3,3]})
>>> df.index.name = 'a'
>>> df.query('a < b')
   a  b
a
0  1  3
1  2  3
>>> df.groupby('a') 
Traceback (most recent call last):
ValueError: 'a' is both an index level and a column label, which is ambiguous.
```

I am sure there are many other places where a string can refer to both an index level and column name. #27652 and #8162 are somehow linked.

Even though those issues might *not* be solved, it would be desirable to handle the above ambiguity consistently in places where both index levels and column labels can be referenced by a string argument."
644415605,34965,BUG: Unexpected behaviour of rolling with apply on DataFrame,oXwvdrbbj8S4wo9k8lSN,closed,2020-06-24T08:16:19Z,2020-09-04T14:26:11Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
When executed on a DataFrame, _rolling_ seems to select only certain columns for processing. For demonstration, I created a DataFrame that has three columns (A, B, and C), of which the first contains TimeDeltas and the other contain Floats. When using rolling, e.g. with _sum_, only the Floats are passed on.
Even stranger, when used in combination with apply, only the first column containing Floats is passed to the function, whereas I would have expected the corresponding part of the DataFrame. 

#### Code Sample, a copy-pastable example
```python
import pandas as pd
columns = [""A"", ""B"", ""C""]
index = list(range(10))
data = [[10**10,2,3]]*len(index)
df = pd.DataFrame(columns = columns, index = index, data=data)
df[""A""] = df[""A""].apply(pd.to_timedelta)
```
The resulting _df_ will look like this:
```python
         A  B  C
0 00:00:10  2  3
1 00:00:10  2  3
2 00:00:10  2  3
3 00:00:10  2  3
4 00:00:10  2  3
5 00:00:10  2  3
6 00:00:10  2  3
7 00:00:10  2  3
8 00:00:10  2  3
9 00:00:10  2  3
```
Applying _rolling_ with _sum_ like this:
```python
df.rolling(window=2).sum()
```
will result in the following output, in which the first column is missing:
```python
     B    C
0  NaN  NaN
1  4.0  6.0
2  4.0  6.0
3  4.0  6.0
4  4.0  6.0
5  4.0  6.0
6  4.0  6.0
7  4.0  6.0
8  4.0  6.0
9  4.0  6.0
```
To demonstrate the problem with _apply_, I created a custom function that simply outputs the number of columns (since I expected a DataFrame to be passed to the function:
```python
def get_num_columns(sub_df):
    print(sub_df)
    return len(sub_df.columns)
df.rolling(window=2).apply(get_num_columns, raw=False)
```
This produces the exception ""AttributeError: 'Series' object has no attribute 'columns'"" and the following printout:
```python
0    2.0
1    2.0
dtype: float64
```
#### Problem description
I would expect in both cases that the windowed DataFrame with all columns is used within the function (either _sum_ or _get\_num\_columns_). 

#### Expected Output
In the case of _sum_, I would either expect an Exception that tells the user that only Floats are acceptable or - preferably - the following output:
```python
         A    B    C
0      NaT  NaN  NaN
1 00:00:20  4.0  6.0
2 00:00:20  4.0  6.0
3 00:00:20  4.0  6.0
4 00:00:20  4.0  6.0
5 00:00:20  4.0  6.0
6 00:00:20  4.0  6.0
7 00:00:20  4.0  6.0
8 00:00:20  4.0  6.0
9 00:00:20  4.0  6.0
```
In the case of _apply_, I would have expected a DataFrame as input to the function. Therefore, the output of the function (without the prints) should be:
```python
0    3
1    3
2    3
3    3
4    3
5    3
6    3
7    3
8    3
9    3
```

#### Output of ``pd.show_versions()``
<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.76-linuxkit
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1.post20200616
Cython           : 0.29.20
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : 0.9.3
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.1
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : 3.6.1
tabulate         : None
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
689402243,36014,BUG: groupby and agg on read-only array gives ValueError: buffer source array is read-only,jeet-parekh,closed,2020-08-31T18:23:31Z,2020-09-04T14:28:16Z,"- [x] I have checked that this issue has not already been reported.
Two variants of this bug have been reported -  #35436  and #34857 

    EDIT: I read into those two issues a bit more. They don't seem similar. But I'll keep it there.

- [x] I have confirmed this bug exists on the latest version of pandas.
Bug exists in pandas 1.1.1

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import pyarrow as pa

df = pd.DataFrame(
    {
        ""sepal_length"": [5.1, 4.9, 4.7, 4.6, 5.0],
        ""species"": [""setosa"", ""setosa"", ""setosa"", ""setosa"", ""setosa""],
    }
)

context = pa.default_serialization_context()
data = context.serialize(df).to_buffer().to_pybytes()
df_new = context.deserialize(data)

# this fails
df_new.groupby([""species""]).agg({""sepal_length"": ""sum""})

# this works
# df_new.copy().groupby([""species""]).agg({""sepal_length"": ""sum""})
```

#### Problem description

This is the traceback.

```python-traceback
Traceback (most recent call last):
  File ""demo.py"", line 16, in <module>
    df_new.groupby([""species""]).agg({""sepal_length"": ""sum""})
  File ""/home/jeet/miniconda3/envs/rnd/lib/python3.8/site-packages/pandas/core/groupby/generic.py"", line 949, in aggregate
    result, how = self._aggregate(func, *args, **kwargs)
  File ""/home/jeet/miniconda3/envs/rnd/lib/python3.8/site-packages/pandas/core/base.py"", line 416, in _aggregate
    result = _agg(arg, _agg_1dim)
  File ""/home/jeet/miniconda3/envs/rnd/lib/python3.8/site-packages/pandas/core/base.py"", line 383, in _agg
    result[fname] = func(fname, agg_how)
  File ""/home/jeet/miniconda3/envs/rnd/lib/python3.8/site-packages/pandas/core/base.py"", line 367, in _agg_1dim
    return colg.aggregate(how)
  File ""/home/jeet/miniconda3/envs/rnd/lib/python3.8/site-packages/pandas/core/groupby/generic.py"", line 240, in aggregate
    return getattr(self, func)(*args, **kwargs)
  File ""/home/jeet/miniconda3/envs/rnd/lib/python3.8/site-packages/pandas/core/groupby/groupby.py"", line 1539, in sum
    return self._agg_general(
  File ""/home/jeet/miniconda3/envs/rnd/lib/python3.8/site-packages/pandas/core/groupby/groupby.py"", line 999, in _agg_general
    return self._cython_agg_general(
  File ""/home/jeet/miniconda3/envs/rnd/lib/python3.8/site-packages/pandas/core/groupby/groupby.py"", line 1033, in _cython_agg_general
    result, agg_names = self.grouper.aggregate(
  File ""/home/jeet/miniconda3/envs/rnd/lib/python3.8/site-packages/pandas/core/groupby/ops.py"", line 584, in aggregate
    return self._cython_operation(
  File ""/home/jeet/miniconda3/envs/rnd/lib/python3.8/site-packages/pandas/core/groupby/ops.py"", line 537, in _cython_operation
    result = self._aggregate(result, counts, values, codes, func, min_count)
  File ""/home/jeet/miniconda3/envs/rnd/lib/python3.8/site-packages/pandas/core/groupby/ops.py"", line 599, in _aggregate
    agg_func(result, counts, values, comp_ids, min_count)
  File ""pandas/_libs/groupby.pyx"", line 475, in pandas._libs.groupby._group_add
  File ""stringsource"", line 658, in View.MemoryView.memoryview_cwrapper
  File ""stringsource"", line 349, in View.MemoryView.memoryview.__cinit__
ValueError: buffer source array is read-only
```

In the `.agg` line that fails, if you do a min, max, median, or count aggregation, then it's going to work.

But if you do a sum or mean, then it fails.

#### Expected Output

I expected the aggregation to succeed without any error.

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-7642-generic
Version          : #46~1597422484~20.04~e78f762-Ubuntu SMP Wed Aug 19 14:35:06 UTC 
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200814
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
```

</details>
"
645620141,34992,ENH: pd.DataFrame.merge() customizable indicator names,ianzur,closed,2020-06-25T14:41:19Z,2020-09-04T14:35:42Z,"#### Is your feature request related to a problem?
When performing multiple merge operations I would love to specify a more informative name than ['left_only', 'right_only', 'both'] for column '_merge'.

#### Describe the solution you'd like
pd.DataFrame.merge() indicator param could accept a categorical index with a custom naming scheme. 
something like:

```
pandas.CategoricalIndex(
    categories=['my_custom_left_name', 'my_custom_right_name', 'custom_both'],
    name='custom_column_name',
)
```

#### API breaking implications
This should not break anything.

#### Describe alternatives you've considered
Use Series.cat.rename_categories() after merge to change indicator names.

"
646014446,35005,BUG: `fillna()` on multiple columns with a series,eggers,closed,2020-06-26T05:18:31Z,2020-09-04T14:36:39Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame([{'Cat1': None, 'Cat2': 'mouse', 'Cat3': 'dog'},{'Cat1': 'cat', 'Cat2': 'mouse', 'Cat3': 'dog'},])

print('One column')
print(df['Cat1'].fillna(df['Cat3']))
print()

print('Two columns with one column')
print(df[['Cat1', 'Cat2']].fillna(df['Cat3']))
print()

print('Two columns with two columns')
print(df[['Cat1', 'Cat2']].fillna(df[['Cat3', 'Cat3']]))
```

Output:
```
One column
0    dog
1    cat
Name: Cat1, dtype: object

Two columns with one column
   Cat1   Cat2
0  None  mouse
1   cat  mouse

Two columns with two columns
  Cat1   Cat2
0  NaN  mouse
1  cat  mouse
```

#### Problem description

`fillna()` on multiple columns with a series doesn't work in the same way that it does on a single column. I also tried to use a dataframe thinking that perhaps it needed a matching number of columns, but that only filled the column with `NaN` instead of the correct value.

#### Expected Output

```
One column
0    dog
1    cat
Name: Cat1, dtype: object

Two columns with one column
   Cat1   Cat2
0  dog  mouse
1   cat  mouse

Two columns with two columns
  Cat1   Cat2
0  dog  mouse
1  cat  mouse
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.9.0-12-amd64
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.4
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.1.1.post20200529
Cython           : 0.29.19
pytest           : 5.4.2
hypothesis       : 5.16.0
sphinx           : 3.0.4
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : 0.6.2
lxml.etree       : 4.5.1
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pytest           : 5.4.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.9
numba            : 0.49.1


</details>
"
646362435,35016,BUG: pd.Series.plot() with specific datetime values shows wrong dates when specifying a matplotlib DateFormatter,gshpychka,closed,2020-06-26T16:04:52Z,2020-09-04T14:41:06Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

Hello. I've run into a very weird and specific issue. 

When plotting a Series with some specific datetime values produced by pd.to_datetime() in the index, using pd.Series.plot() and supplying a custom matplotlib DateFormatter to the x axis, the x axis is all wrong.

I know this sounds confusing.

Here's a minimal example:

```python
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.dates import DateFormatter
%matplotlib inline

x_formatter = DateFormatter(""%Y"")

dates = [dt.date(2020,1,4), dt.date(2020,1,5), dt.date(2020,1,6)]
pd.Series([5,8,7], index=pd.to_datetime(dates)).plot()
ax = plt.gca()
ax.xaxis.set_major_formatter(x_formatter)
```

#### Problem description

[Here's what the output looks like](
https://i.imgur.com/DjqqeWa.png)

The formatted outputs ""0051"" as the year for this particular example. This only happens for some specific dates (I wasn't able to figure out a rule), and only when supplying a matplotlib formatter.

This does not happen when using matplotlib directly.
This also does not happen when NOT using pd.to_datetime()

#### Expected Output

'2020' labels at major tick locations.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.5.final.0
python-bits      : 32
OS               : Linux
OS-release       : 3.10.0-862.2.3.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.16.1
pytz             : 2018.5
dateutil         : 2.6.1
pip              : 20.1.1
setuptools       : 39.2.0
Cython           : 0.29.17
pytest           : 3.5.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.1.1
lxml.etree       : 4.2.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.4 (dt dec pq3 ext)
jinja2           : 2.10
IPython          : 6.4.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.1
matplotlib       : 2.2.2
numexpr          : None
odfpy            : None
openpyxl         : 2.5.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 3.5.1
pyxlsb           : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : 1.1.15
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.1.0
xlwt             : None
xlsxwriter       : 1.1.1
numba            : 0.40.0

</details>
"
630220255,34557,BUG: DataFrame size overflows on Windows,FHTMitchell,open,2020-06-03T18:22:22Z,2020-09-04T15:03:07Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

a = np.ones(2**30 + 1, dtype=np.int8)
df = pd.DataFrame.from_dict({'a': a, 'b': a})
print(df.size)  # --> -2147483646
print(df.sum(axis=0))  # --> 
# a   NaN
# b   NaN
# dtype: float64
```

#### Problem description

On windows, dataframes with more than 2^31 elements (max 32 bit int) result in negative sizes and nan results from operations performed on them. 

This problem only seems to exist on windows (64-bit Windows-10-10.0.18362-SP0). This seems to be from the assumption that `np.prod(self.shape)` will not overflow, but numpy [makes no such guarantee](https://numpy.org/doc/stable/reference/generated/numpy.prod.html). 

#### Expected Output

positive size, non-nan sum

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 85 Stepping 7, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.16.4
pytz             : 2018.9
dateutil         : 2.8.0
pip              : 19.2.2
setuptools       : 41.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>"
655997559,35264,DOC: Incomplete method description for str.match,Uvatha,closed,2020-07-13T16:52:59Z,2020-09-04T15:05:13Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.match.html

#### Documentation problem

The current method description is:

""Determine if each string matches a regular expression.""

This description does not specify that str.match only matches the beginning of a string.  This leads to confusion when regex does not function as expected.

#### Suggested fix for documentation

Change method description to:

""Determine if the beginning of each string matches a regular expression.""

This makes the documentation more accurate, and also makes the method description align with the match function from the re module, from which it is taken.
"
676430855,35658,BUG: IntervalIndex cannot be round-tripped pickled,benkrikler,closed,2020-08-10T21:34:50Z,2020-09-04T15:17:10Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd  
import numpy as np
import pickle as pkl 

interval = pd.IntervalIndex.from_breaks(np.linspace(0, 100, 5+1), closed=""left"")

print(""original:"", interval)
print(""pickle"", pkl.loads(pkl.dumps(interval)))
```

```output
original: IntervalIndex([[0.0, 20.0), [20.0, 40.0), [40.0, 60.0), [60.0, 80.0), [80.0, 100.0)],
              closed='left',
              dtype='interval[float64]')
pickle IntervalIndex([(0.0, 20.0], (20.0, 40.0], (40.0, 60.0], (60.0, 80.0], (80.0, 100.0]],
              closed='right',
              dtype='interval[float64]')
```

#### Problem description
Having recently updated to v1.1.0 from v1.0.5, it seems IntervalIndex is no longer preserving the closure of the intervals as being left or right.  When unpickling the IntervalIndex, the closure is reset to the default value of ""right"". This seems similar to the serialisation issues of #32037 and #35420, but those seem to be for older versions of the master branch. 

#### Expected Output
```output
original: [IntervalIndex([[0.0, 20.0), [20.0, 40.0), [40.0, 60.0), [60.0, 80.0), [80.0, 100.0)],
              closed='left',
              dtype='interval[float64]')]
pickle [IntervalIndex([[0.0, 20.0), [20.0, 40.0), [40.0, 60.0), [60.0, 80.0), [80.0, 100.0)],
              closed='left',
              dtype='interval[float64]')]
```
#### Output of ``pd.show_versions()``

<details>

python -c ""import pandas as pd; pd.show_versions()""

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1062.7.1.el7.x86_64
Version          : #1 SMP Mon Dec 2 17:33:29 UTC 2019
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 45.2.0
Cython           : None
pytest           : 4.3.0
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.17.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.48.0

</details>
"
659665799,35332,DOC: [Errno 2] No such file or directory,alik604,closed,2020-07-17T21:50:47Z,2020-09-04T15:21:48Z,"#### Location of the documentation
 
inline in the code

#### Documentation problem

the following is due to filename length, not _""No such file or directory""_

> FileNotFoundError: [Errno 2] No such file or directory: 'Queried Data - search_index__foo_sourcetype__foo_token_typetoken_typetoken_typetoken_typetotoken_typetoken_typetoken_token_typetytoken_typepetoken_typepetoken_SDSDDDypepeX_from_02-41-PM_July_17.csv'

#### Suggested fix for documentation

It appears to me there should be a `if len(myString) > 177, throw FileNameTooLong` 


here is a example 
```

file_description = ""search index = foo sourcetype = foo token_typetoken_typetoken_typetoken_typetotoken_typetoken_typetoken_token_typetytoken_typepetoken_typepetoken_SDSDDDypepe""

file_description += "" from "" + datetime.datetime.now().strftime(""%I-%M-%p %B %d"")
file_description = str(file_description).strip().replace(' ', '_')
file_description = re.sub(r'(?u)[^-\w.]', '', file_description)
file_description

len(file_description)
X_train.to_csv(""Queried Data - "" + file_description + "".csv"")
```
> search_index__foo_sourcetype__foo_token_typetoken_typetoken_typetoken_typetotoken_typetoken_typetoken_token_typetytoken_typepetoken_typepetoken_SDSDDDypepe_from_02-41-PM_July_17'

> 177
*Works as intended*  
---
if you add a letter to the end of `file_description`, length will be `178`. 

we get the following 
```text 

---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
<ipython-input-176-8fb818e0c708> in <module>
      7 
      8 len(file_description)
----> 9 X_train.to_csv(""Queried Data - "" + file_description + "".csv"")

~\Anaconda3\lib\site-packages\pandas\core\generic.py in to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)
   3202             decimal=decimal,
   3203         )
-> 3204         formatter.save()
   3205 
   3206         if path_or_buf is None:

~\Anaconda3\lib\site-packages\pandas\io\formats\csvs.py in save(self)
    186                 self.mode,
    187                 encoding=self.encoding,
--> 188                 compression=dict(self.compression_args, method=self.compression),
    189             )
    190             close = True

~\Anaconda3\lib\site-packages\pandas\io\common.py in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text)
    426         if encoding:
    427             # Encoding
--> 428             f = open(path_or_buf, mode, encoding=encoding, newline="""")
    429         elif is_text:
    430             # No explicit encoding

FileNotFoundError: [Errno 2] No such file or directory: 'Queried Data - search_index__foo_sourcetype__foo_token_typetoken_typetoken_typetoken_typetotoken_typetoken_typetoken_token_typetytoken_typepetoken_typepetoken_SDSDDDypepeX_from_02-41-PM_July_17.csv'
```


## outtake

I suggest a better error message to be used the file name is too long 

getting a `No such file or directory` caused me to waste time, as it was wrong. The file name was too long  "
663393759,35372,BUG: Pandas Read_html can't get values from table td data-sort-values,riyadfebrian,closed,2020-07-22T00:45:13Z,2020-09-04T15:29:03Z,"- [ ✔] I have checked that this issue has not already been reported.

- [✔ ] I have confirmed this bug exists on the latest version of pandas.

---

#### URL Example

[https://en.wikipedia.org/wiki/Comparison_of_text_editors](https://en.wikipedia.org/wiki/Comparison_of_text_editors)
```python
df = pd.read_html(url, flavor='bs4')
```

#### Problem description

I'm trying to scrape first table from WIkipedia, the column 'Open Source' and 'CLI available' are displaying the data with ✔ images to refer 'Yes' and ❌ for 'No'.  if we inspect the HTML code.

```html
<td data-sort-value=""Yes"" style=""background: #D2FFD2; color: black; vertical-align: middle; text-align: center;"" class=""table-yes2""><img alt=""Yes"" src=""//upload.wikimedia.org/wikipedia/commons/thumb/0/03/Green_check.svg/13px-Green_check.svg.png"" decoding=""async"" title=""Yes"" width=""13"" height=""13"" srcset=""//upload.wikimedia.org/wikipedia/commons/thumb/0/03/Green_check.svg/20px-Green_check.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/0/03/Green_check.svg/26px-Green_check.svg.png 2x"" data-file-width=""600"" data-file-height=""600"">
</td>
```
The value is embedded in `data-sort-value` but `pandas.read_html` maybe read the column value as image and return NaN



#### Output of ``pd.show_versions()``

<details>

commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_Indonesia.1252

pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.2
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
665802816,35414,BUG: `freq` attribute sometimes lost when concatenating series that are tz-aware,rwijtvliet,closed,2020-07-26T15:27:09Z,2020-09-04T15:31:53Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
s1 = pd.Series([1, 2, 3], index=pd.date_range('2020-03-29 00:00:00', periods=3, freq='D', tz='Europe/Berlin'))
s2 = pd.Series([4, 5], pd.date_range(s1.index[-1]+s1.index.freq, periods=2, freq=s1.index.freq))
r = pd.concat([s1, s2])

s1.index, s2.index, r.index
 DatetimeIndex(['2020-03-29 00:00:00+01:00', '2020-03-30 00:00:00+02:00',
                '2020-03-31 00:00:00+02:00'],
               dtype='datetime64[ns, Europe/Berlin]', freq='D'),
 DatetimeIndex(['2020-04-01 00:00:00+02:00', '2020-04-02 00:00:00+02:00'], dtype='datetime64[ns, Europe/Berlin]', freq='D'),
 DatetimeIndex(['2020-03-29 00:00:00+01:00', '2020-03-30 00:00:00+02:00',
                '2020-03-31 00:00:00+02:00', '2020-04-01 00:00:00+02:00',
                '2020-04-02 00:00:00+02:00'],
               dtype='datetime64[ns, Europe/Berlin]', freq=None)
```

#### Problem description

s1 and s2 have indices are in the same time zone with the same fixed frequency. This also hold when they are concatenated, however, the freq attribute is lost in the result, see above. Expected would be, to have `freq='D'`.

The problem does not always occur.

* When the DST change is not included, it seems to work:
```
  #Same code, change 2020-03-29 to 2020-03-01
 DatetimeIndex(['2020-03-01 00:00:00+01:00', '2020-03-02 00:00:00+01:00',
                '2020-03-03 00:00:00+01:00'],
               dtype='datetime64[ns, Europe/Berlin]', freq='D'),
 DatetimeIndex(['2020-03-04 00:00:00+01:00', '2020-03-05 00:00:00+01:00'], dtype='datetime64[ns, Europe/Berlin]', freq='D'),
 DatetimeIndex(['2020-03-01 00:00:00+01:00', '2020-03-02 00:00:00+01:00',
                '2020-03-03 00:00:00+01:00', '2020-03-04 00:00:00+01:00',
                '2020-03-05 00:00:00+01:00'],
               dtype='datetime64[ns, Europe/Berlin]', freq='D')
```

* When DST is included incorrectly (see [here](https://github.com/pandas-dev/pandas/issues/35388)), and all timestamps are 24h apart, it also works:
```
  #Same code, change 2020-03-29 to 2020-03-27
 DatetimeIndex(['2020-03-27 00:00:00+01:00', '2020-03-28 00:00:00+01:00',
                '2020-03-29 00:00:00+01:00'],
               dtype='datetime64[ns, Europe/Berlin]', freq='D'),
 DatetimeIndex(['2020-03-30 01:00:00+02:00', '2020-03-31 01:00:00+02:00'], dtype='datetime64[ns, Europe/Berlin]', freq='D'),
 DatetimeIndex(['2020-03-27 00:00:00+01:00', '2020-03-28 00:00:00+01:00',
                '2020-03-29 00:00:00+01:00', '2020-03-30 01:00:00+02:00',
                '2020-03-31 01:00:00+02:00'],
               dtype='datetime64[ns, Europe/Berlin]', freq='D')
```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en
LOCALE           : de_DE.cp1252

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
693230861,36118,REGR: ensure closed attribute of IntervalIndex is preserved in pickle roundtrip,jorisvandenbossche,closed,2020-09-04T14:30:02Z,2020-09-04T15:59:48Z,"Closes #35658

cc @jbrockmendel 

(I suppose the long term way would rather to properly pickle the IntervalArray itself and restore from that, instead of restoring from the left/right arrays, but would rather leave such a change for 1.2)"
692737459,36108,CLN: clean `pandas/compat.__init__.py`,fangchenli,closed,2020-09-04T05:27:54Z,2020-09-04T16:30:19Z,"Remove `is_platform_32bit()` from `pandas/compat.__init__.py`, use `IS64` instead to improve consistency. "
675576117,35635,ENH: to_sql support DB-API other than sqlite3/sqlalchemy (eg peewee ORM),wilberh,closed,2020-08-08T19:28:22Z,2020-09-04T16:37:29Z,"#### Is your feature request related to a problem?

I can't use, to_sql, api to insert data to a database table that complies with DB-API (pep-249) v2. 


#### Describe the solution you'd like


I would like to use, to_sql, to insert data to a database table that complis with DB-API (pep-249) v2.  
Example, I use peewee ORM (not sqlalchemy / sqlite3 connections) to interact with a database that has a DB-API v2 driver.


#### Describe alternatives you've considered

I'm pulling each row of the dataframe to do inserts using threading.  But I'd like this api to support connections other than sqlalchemy and sqlite3.

"
668752044,35476,BUG: Bug with Alpine installation,AminovE99,closed,2020-07-30T13:53:11Z,2020-09-04T17:03:34Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here

```

#### Problem description

I work on frappe framework and I have `docker-erpnext` to deploy application.
When I tried to install pandas library on my linux (alpine) system, I received error:

https://travis-ci.org/github/Monogramm/docker-erpnext/jobs/713250724#L4417

By the way, this library can be installed on debian and debian-slim, but cannot be installed on alpine.

I already checked problem #7512 but it did not helped to me.

#### Expected Output
Pandas should be installed

#### Output of ``pd.show_versions()``
`pandas==0.24.2`
version of python: 3.7
<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
692196386,36099,TYP: misc fixes for numpy types 2,simonjayhawkins,closed,2020-09-03T18:37:42Z,2020-09-04T17:05:36Z,"```
pandas\core\dtypes\cast.py:681: error: Incompatible types in assignment (expression has type ""DatetimeTZDtype"", variable has type ""dtype"")  [assignment]
pandas\core\dtypes\cast.py:715: error: Incompatible types in assignment (expression has type ""IntervalDtype"", variable has type ""dtype"")  [assignment]
pandas\core\dtypes\common.py:1392: error: Item ""dtype"" of ""Union[dtype, ExtensionDtype]"" has no attribute ""_is_boolean""  [union-attr]
pandas\core\dtypes\common.py:111: error: Function ""numpy.array"" is not valid as a type  [valid-type]
pandas\core\reshape\merge.py:1873: error: Function ""numpy.array"" is not valid as a type  [valid-type]
```"
693333012,36119,Backport PR #36118 on branch 1.1.x (REGR: ensure closed attribute of IntervalIndex is preserved in pickle roundtrip),meeseeksmachine,closed,2020-09-04T15:59:58Z,2020-09-04T17:49:50Z,Backport PR #36118: REGR: ensure closed attribute of IntervalIndex is preserved in pickle roundtrip
673209401,35561,"BUG:Dataframe.to_string, when set index=False, there is a white space in the head of every line",ZephyrShannon,closed,2020-08-05T02:43:18Z,2020-09-04T17:52:01Z,"- [x] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
d = {'col1': [""1"", ""2"", ""3""], 'col2': [4, 5, 6]}
df = pd.DataFrame(d)
print(df.to_string(index=False, formatters={'col1':'{:10}'.format}))

```

#### Problem description
there is a white space in the head of every line:
       col1  col2
 1              4
 2              5
 3              6
#### Expected Output
no white space in the head
       col1  col2
1              4
2              5
3              6

#### Output of ``pd.show_versions()``
<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Darwin
OS-release: 19.5.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: None
LOCALE: zh_CN.UTF-8
pandas: 0.24.2
pytest: None
pip: 19.0.3
setuptools: 41.0.0
Cython: None
numpy: 1.16.4
scipy: 1.3.0
pyarrow: None
xarray: None
IPython: 7.5.0
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.1.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: 4.5.1
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.8.1
gcsfs: None
</details>
"
693351326,36120,TYP: io,jbrockmendel,closed,2020-09-04T16:17:42Z,2020-09-04T17:53:49Z,@simonjayhawkins are there any typing-related areas that you'd suggest I prioritize?
693229552,36117,Backport PR #36061 on branch 1.1.x (BUG: groupby and agg on read-only array gives ValueError: buffer source array is read-only),meeseeksmachine,closed,2020-09-04T14:28:47Z,2020-09-04T17:59:04Z,Backport PR #36061: BUG: groupby and agg on read-only array gives ValueError: buffer source array is read-only
679530089,35735,"BUG: DataFrame.merge() fails with program termination, consumed entire available memory(Memory Leak)",HimanshuS24,closed,2020-08-15T09:02:11Z,2020-09-04T18:01:47Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
 
df1 = pd.read_csv(path_of_csv1)
df2 = pd.read_csv(path_of_csv2)
resultant_df = df1.merge(df2, left_on='left_csv_column_name', right_on='right_csv_column_name', how='inner') <= Exception thrown here.

```

```
[1] 31330 killed python
```

#### Problem description

Memory Leak occurs while trying to merge two Dataframes read from two different CSV's of bigger size (>=10Mb in my case)

For smaller CSV files (let's say of <=10Mb) each merging process is working fine. 
My system is having 8Gb of RAM and before starting the Merging process, Enough Memory is available(>3Gb) and after starting the merging process, the system consumes the entire available memory and killed the merging process in between with the below exception:-

```
[1] 31330 killed python
```

#### Expected Output

`resultant_df` contains the two DataFrame `df1` and `df2` based on the join type we have given as a parameter `how='inner'` in pandas merge function

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.4.0
Version          : Darwin Kernel Version 19.4.0: Wed Mar  4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 49.2.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None]

</details>
"
698328890,36274,REF: implement Categorical._validate_listlike,jbrockmendel,closed,2020-09-10T18:21:12Z,2020-09-12T21:19:38Z,I found it really confusing why we used slightly different dtype-comparisons in a bunch of different places (e.g CategoricalIndex.get_indexer vs CategoricalIndex.get_indexer_non_unique).  AFAICT It's just a matter of optimizations accruing over time without a helper method to keep them in sync. 
525771207,29735,sort_index on a multiindexed DataFrame with sparse columns fills with NaNs,marginalhours,closed,2019-11-20T12:24:32Z,2020-09-12T21:23:21Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
from scipy.sparse import csr_matrix

f = pd.DataFrame.sparse.from_spmatrix(csr_matrix((4, 4)), index=pd.MultiIndex.from_product([[1, 2], [1,2]]))

f
f.sort_index(level=0)
```
#### Problem description

When I create a MultiIndexed DataFrame with sparse columns, then call `sort_index()`, in the result `0.0` (the sparse fill value) has been replaced with `NaN` -- but it seems like only for columns which are all the fill value. 

It seems like this might also be an issue with `RangeIndex` as well, since if I make a simpler `DataFrame` and call `sort_index`, without any arguments it's ok but adding `level=0` (or in fact `level=(anything)` as a kwarg will cause the frame to be filled with `NaN` without raising any kind of warning 

I would expect the following behaviour: 

- Values preserved for `MultiIndex` 
- For `RangeIndex`, the level keyword should either be totally ignored (no impact on output) or should not be valid to pass in.

I don't think this is a duplicate, I searched the tracker but the combination of sparse + multiindex seems to be rare

#### Expected Output
```python
>>> f.sort_index(level=0)
       0    1    2    3
1 1  0.0  0.0  0.0  0.0
  2  0.0  0.0  0.0  0.0
2 1  0.0  0.0  0.0  0.0
  2  0.0  0.0  0.0  0.0
```

#### Actual Output

```python
>>> f.sort_index(level=0)
      0   1   2   3
1 1 NaN NaN NaN NaN
  2 NaN NaN NaN NaN
2 1 NaN NaN NaN NaN
  2 NaN NaN NaN NaN
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.0.0-36-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.2
setuptools       : 41.0.1
Cython           : 0.29.14
pytest           : 3.10.1
hypothesis       : None
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
696281247,36236,TST: add test case for sort_index on multiindexed Frame with sparse cols,ylin00,closed,2020-09-09T00:33:24Z,2020-09-12T21:23:25Z,"- [ ] closes #29735 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] add test case for sort_index on multiindexed Frame with sparse cols
"
700355436,36312,PERF: JoinUnit.is_na,jbrockmendel,closed,2020-09-12T19:48:43Z,2020-09-12T21:29:35Z,This along with #36309 gets #34683 to performance parity with master.
694201740,36150,REF: use BlockManager.apply in csv code,jbrockmendel,closed,2020-09-05T22:56:02Z,2020-09-12T21:30:15Z,"This doesn't get rid of internals usage entirely, but makes that access 1 layer less deep"
688737267,35989,DOC: DataFrame.query() are contradictory on whether Python keywords can used as identifiers,jpeacock29,closed,2020-08-30T15:18:20Z,2020-09-12T21:31:59Z,"#### Location of the documentation
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html#pandas.DataFrame.query

#### Documentation problem
The `expr` parameter is described as follows:

> You can refer to column names that contain spaces or operators by surrounding them in backticks. This way you can also escape names that start with a digit, or those that are a Python keyword. Basically when it is not valid Python identifier. See notes down for more details.

However, the notes that follow indicate:

> Python keywords may not be used as identifiers.

So the first part suggests Python keywords can be used, while the second part indicates they cannot. Testing suggests they cannot currently be used.

#### Suggested fix for documentation
Change the `expr` description to read:

> You can refer to column names that are not valid Python variable names by surrounding them in backticks. Thus column names containing spaces or punctuation (besides underscores) or starting with digits must be surrounded by backticks. (For example, a column named ""Area (cm^2)"" would be referenced as `Area (cm^2)`.) Column names which are Python keywords (like ""list"", ""for"", ""import"", etc) cannot be used."
700216345,36304,PERF: creating string Series/Arrays from sequence with many strings,topper-123,closed,2020-09-12T12:17:42Z,2020-09-12T21:33:08Z,"Improves performance of `pandas._libs.lib.ensure_string_array` is cases with many string elements.

Examples:

```python
>>> x = np.array([str(u) for u in range(1_000_000)], dtype=object)
>>> %timeit pd.Series(x, dtype=str)
344 ms ± 59.7 ms per loop  # v1.1.0
157 ms ± 7.04 ms per loop  # v1.1.1 and master
22.6 ms ± 191 µs per loop  # this PR
>>> %timeit pd.Series(x, dtype=""string"")
357 ms ± 40.2 ms per loop  # v1.1.0
148 ms ± 713 µs per loop  # v1.1.1 and master
26.3 ms ± 291 µs per loop  # this PR
```

#35519 is the cause of the improvement from 1.1.0 to 1.1.1.

Together with #35519 this PR means that the overhead of working with strings in pandas has gotten considerably smaller for cases when we many times instantiate new `Series`/`PandasArrays` with dtype `str`/`StringDtype`, i.e. probably quite often."
690487544,36055,STY/WIP: check for private imports/lookups,jbrockmendel,closed,2020-09-01T22:20:34Z,2020-09-12T21:34:28Z,"Mostly this is an amalgam of #33479, #33394, and #33393."
682206742,35811,BUG/QST: Series.transform with a dictionary,rhshadrach,closed,2020-08-19T21:55:53Z,2020-09-12T21:36:52Z,"What is the expected output of passing a dictionary to `Series.transform`? For example:

    s = pd.Series([1, 2, 3])
    result1 = s.transform({'a': lambda x: x + 1})
    result2 = s.transform({'a': lambda x: x + 1, 'b': lambda x: x + 2})

The docs say that `dict of axis labels -> functions` is acceptable, but I can't find any example in the docs where the output is described/shown. Under the hood, `Series.transform` is just calling `Series.aggregate` which produces the following outputs for `result1` and `result2`.

````
# result1
a  0    2
   1    3
   2    4
dtype: int64

# result2
a  0    2
   1    3
   2    4
b  0    3
   1    4
   2    5
dtype: int64
````

`result1` is deemed acceptable (the length of the result equals the length of the input) and is returned, but `result2` raises; it is not a transformation.

I am wondering if a better return would be a DataFrame where the keys are the column names ('a' and 'b' in this example)."
395020989,24518,DEPR: deprecate pandas/io/date_converters.py,mroeschke,closed,2018-12-31T19:11:29Z,2020-09-12T21:37:58Z,"This file contains one function that is used elsewhere in the code base (that can be moved), and the rest is unused and not advertised in the api.

Raising this issue to gauge whether we should outright remove it or deprecate this file since its technically public in `io`
"
679629529,35741,DEPR: Deprecate pandas/io/date_converters.py,avinashpancham,closed,2020-08-15T19:26:47Z,2020-09-12T21:38:03Z,"- [x] closes #24518
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
679650479,35747,BUG: Cannot index `frozenset` elements from a pd.Series or pd.DataFrame,jolespin,closed,2020-08-15T22:18:57Z,2020-09-12T21:39:40Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
print(pd.__version__)
# 1.1.0

# Create DataFrame
data = {0: {frozenset({'Otu000010', 'Otu000505'}): 'white',
  frozenset({'Otu000067', 'Otu000073'}): 'white',
  frozenset({'Otu000151', 'molar'}): 'white',
  frozenset({'Otu000380', 'etec_stp'}): 'white',
  frozenset({'Otu000281', 'ghrp'}): 'white'},
 14: {frozenset({'Otu000010', 'Otu000505'}): 'white',
  frozenset({'Otu000067', 'Otu000073'}): 'white',
  frozenset({'Otu000151', 'molar'}): 'white',
  frozenset({'Otu000380', 'etec_stp'}): 'white',
  frozenset({'Otu000281', 'ghrp'}): 'white'},
 28: {frozenset({'Otu000010', 'Otu000505'}): 'red',
  frozenset({'Otu000067', 'Otu000073'}): 'white',
  frozenset({'Otu000151', 'molar'}): 'blue',
  frozenset({'Otu000380', 'etec_stp'}): 'white',
  frozenset({'Otu000281', 'ghrp'}): 'white'}}
df = pd.DataFrame(data)

# Get first item in index
id_query = df.index[0]

# Grab a column
vector = df[14]

# Index the vector using query
vector[id_query]


# ---------------------------------------------------------------------------
# KeyError                                  Traceback (most recent call last)
# <ipython-input-145-493fc06aeb40> in <module>
#      24 
#      25 # Index the vector using query
# ---> 26 vector[id_query]

# ~/anaconda3/envs/soothsayer5_env/lib/python3.8/site-packages/pandas/core/series.py in __getitem__(self, key)
#     906             return self._get_values(key)
#     907 
# --> 908         return self._get_with(key)
#     909 
#     910     def _get_with(self, key):

# ~/anaconda3/envs/soothsayer5_env/lib/python3.8/site-packages/pandas/core/series.py in _get_with(self, key)
#     946 
#     947         # handle the dup indexing case GH#4246
# --> 948         return self.loc[key]
#     949 
#     950     def _get_values_tuple(self, key):

# ~/anaconda3/envs/soothsayer5_env/lib/python3.8/site-packages/pandas/core/indexing.py in __getitem__(self, key)
#     877 
#     878             maybe_callable = com.apply_if_callable(key, self.obj)
# --> 879             return self._getitem_axis(maybe_callable, axis=axis)
#     880 
#     881     def _is_scalar_access(self, key: Tuple):

# ~/anaconda3/envs/soothsayer5_env/lib/python3.8/site-packages/pandas/core/indexing.py in _getitem_axis(self, key, axis)
#    1097                     raise ValueError(""Cannot index with multidimensional key"")
#    1098 
# -> 1099                 return self._getitem_iterable(key, axis=axis)
#    1100 
#    1101             # nested tuple slicing

# ~/anaconda3/envs/soothsayer5_env/lib/python3.8/site-packages/pandas/core/indexing.py in _getitem_iterable(self, key, axis)
#    1035 
#    1036         # A collection of keys
# -> 1037         keyarr, indexer = self._get_listlike_indexer(key, axis, raise_missing=False)
#    1038         return self.obj._reindex_with_indexers(
#    1039             {axis: [keyarr, indexer]}, copy=True, allow_dups=True

# ~/anaconda3/envs/soothsayer5_env/lib/python3.8/site-packages/pandas/core/indexing.py in _get_listlike_indexer(self, key, axis, raise_missing)
#    1252             keyarr, indexer, new_indexer = ax._reindex_non_unique(keyarr)
#    1253 
# -> 1254         self._validate_read_indexer(keyarr, indexer, axis, raise_missing=raise_missing)
#    1255         return keyarr, indexer
#    1256 

# ~/anaconda3/envs/soothsayer5_env/lib/python3.8/site-packages/pandas/core/indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing)
#    1296             if missing == len(indexer):
#    1297                 axis_name = self.obj._get_axis_name(axis)
# -> 1298                 raise KeyError(f""None of [{key}] are in the [{axis_name}]"")
#    1299 
#    1300             # We (temporarily) allow for some missing keys with .loc, except in

# KeyError: ""None of [Index(['Otu000010', 'Otu000505'], dtype='object')] are in the [index]""
```

#### Problem description

In previous versions, I was able to use `frozenset` objects as the elements of the index.  These are great objects to have for network analysis where I use as edges in my pd.Series and pd.DataFrame
#### Expected Output

I should be able to index using these objects. 

#### Output of ``pd.show_versions()``

<details>

pandas v1.1.0
</details>
"
672540575,35534,BUG: Cannot select values in series using tuples when indexing with tuples in pandas 1.1.0,lfiedler,closed,2020-08-04T06:30:02Z,2020-09-12T21:39:40Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

if __name__ == ""__main__"":
    series = pd.Series([1, 2, 3, 4], index=[(""a"", ), (""b"", ), (""c"", ), (""d"", )])
    print(series[(""a"", )])

```

#### Problem description
On pandas 1.0.5 selecting values of a series which is indexed by tuples was possible via `__getitem__` using the desired tuple (see above code). In pandas 1.1.0 this suddenly raises a ValueError:
```
ValueError: Can only tuple-index with a MultiIndex
```
As this used to work in 1.0.5 and earlier versions (0.25 as far as I can recall) I expect this to be an unwanted side effect rather than a wanted change in behavior (in which case it would certainly be breaking).
Apart from this, I expected Series objects to behave to some degree like python dictionaries. In particular using tuples of strings as keys and selecting values by such tuples I think should be possible.

#### Expected Output
`1`

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-42-generic
Version          : #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1.post20200622
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
None

```

</details>
"
697283904,36261,ENH: consistently cast strings for DTA/TDA/PA.__setitem__,jbrockmendel,closed,2020-09-10T00:59:20Z,2020-09-12T23:41:15Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

ATM we cast strings for listlike inputs but not for scalars.
"
594540093,33303,"BUG: wrong df.groupby().groups when grouping with [Grouper(freq=), ...] (GH33132)",falcaopetri,closed,2020-04-05T16:23:21Z,2020-09-13T00:14:06Z,"- [ ] closes #33132 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
700354843,36311,DOC: Fix DataFrame.query contradiction on use of Python keywords as identifiers,ezebunandu,closed,2020-09-12T19:46:28Z,2020-09-13T01:14:44Z,"
- [ ] closes #35989 
"
674718608,35594,BUG: to_sql if_exists not working properly when schema is set on PostgreSQL,westhyena,closed,2020-08-07T02:52:40Z,2020-09-13T08:01:50Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import sqlalchemy
import pandas as pd 

uri = ""some uri to postgresql
engine = sqlalchemy.create_engine(uri)
conn = engine.connect()

df = pd.DataFrame([[1,2,3], [2,1,3]], columns=[""A"", ""B"", ""C""])

# works
df.to_sql(""tt"", conn, schema=""pg_temp"", if_exists=""append"", index=False)

# causes error
df.to_sql(""tt"", conn, schema=""pg_temp"", if_exists=""append"", index=False)

# works
df.to_sql(""tt"", conn, if_exists=""append"", index=False)
```

#### Problem description

```
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DuplicateTable) relation ""tt"" already exists

[SQL: 
CREATE TABLE pg_temp.tt (
	""A"" BIGINT, 
	""B"" BIGINT, 
	""C"" BIGINT
)

]
(Background on this error at: http://sqlalche.me/e/f405)
```

When I set to_sql with schema and if_exists=""append"", it works at first call(when table doesn't exists).
But when I call to_sql again(when table exists), it doesn't append the data. It tries to create table again and causes error.

Not setting the schema parameter worked fine when table exists.
"
696963630,36249,BUG: na parameter for str.startswith and str.endswith not propagating for Series with categorical dtype,asishm,closed,2020-09-09T16:31:45Z,2020-09-13T12:25:48Z,"- [x] closes #36241
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I parametrized dtype in existing tests. Should I create a separate test instead?"
694159160,36147,REGR: Series access with Index of tuples/frozenset,rhshadrach,closed,2020-09-05T20:02:05Z,2020-09-13T12:29:54Z,"- [x] closes #35534 
- [x] closes #35747
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
700549915,36328,Backport PR #36323 on branch 1.1.x (CI: install numpy from pip #36296),meeseeksmachine,closed,2020-09-13T11:17:33Z,2020-09-13T12:32:36Z,Backport PR #36323: CI: install numpy from pip #36296
699456547,36291,BUG: pandas series creation fails with OverflowError when given large integers,mdering,closed,2020-09-11T15:33:37Z,2020-09-13T12:34:52Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
>>> import pandas as pd
>>> pd.Series(1000000000000000000000)
0    1000000000000000000000
dtype: object
>>> pd.Series(1000000000000000000000, index = pd.date_range(pd.Timestamp.now().floor(""1D""), pd.Timestamp.now(), freq='T'))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/matt/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 327, in __init__
    data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True)
  File ""/Users/matt/opt/anaconda3/lib/python3.7/site-packages/pandas/core/construction.py"", line 475, in sanitize_array
    subarr = construct_1d_arraylike_from_scalar(value, len(index), dtype)
  File ""/Users/matt/opt/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py"", line 1555, in construct_1d_arraylike_from_scalar
    subarr.fill(value)
OverflowError: int too big to convert
>>> pd.Series(1000000000000000000000.0, index = pd.date_range(pd.Timestamp.now().floor(""1D""), pd.Timestamp.now(), freq='T'))
2020-09-11 00:00:00    1.000000e+21
2020-09-11 00:01:00    1.000000e+21
2020-09-11 00:02:00    1.000000e+21
2020-09-11 00:03:00    1.000000e+21
2020-09-11 00:04:00    1.000000e+21
                           ...
2020-09-11 11:24:00    1.000000e+21
2020-09-11 11:25:00    1.000000e+21
2020-09-11 11:26:00    1.000000e+21
2020-09-11 11:27:00    1.000000e+21
2020-09-11 11:28:00    1.000000e+21
Freq: T, Length: 689, dtype: float64

```

#### Problem description
Hi pandas, when creating a new series with very large integers, series creation fails. This is not true if you pass in a float, or if you just pass in one value of a series and no index. the traceback points to pandas code so I'm submitting a bug here.

#### Expected Output
I would expect this to fail more gracefully or either output a series of object type, or float type. when initializing an array in numpy, it transparently converts it to object type
```python
>>> np.array([1000000000000000000000]*1000).dtype
dtype('O')
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200814
Cython           : 0.29.21
pytest           : 6.0.1
hypothesis       : None
sphinx           : 3.2.1
blosc            : 1.7.0
feather          : None
xlsxwriter       : 1.3.3
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.8.0
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.19
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.1

</details>
"
700560644,36331,Backport PR #36249 on branch 1.1.x (BUG: na parameter for str.startswith and str.endswith not propagating for Series with categorical dtype),meeseeksmachine,closed,2020-09-13T12:26:20Z,2020-09-13T13:07:50Z,Backport PR #36249: BUG: na parameter for str.startswith and str.endswith not propagating for Series with categorical dtype
695715585,36212,BUG: df agg() issue when dataframe has a column called 'name',shyam-sreenivasan,closed,2020-09-08T09:35:01Z,2020-09-13T13:15:07Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
data = {""name"": [""abc"", ""xyz""]}
df = pd.DataFrame(data)
print(df.agg({'name': 'count'}))
```

#### Problem description

In the above code snippet, there is a column called 'name' in the dataframe and when executing it an exception is being thrown. Following the stacktrace , it is observed that in line 475 of core/base.py , df.name is being passed to the `name` argument of 

result = Series(result, name=getattr(self, ""name"", None))

when the dataframe has a column called 'name'.

The same code snippet works fine for any other column name. For example, if we change the column name to `nameee`. It executes fine.

Stack trace
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/.virtualenvs/dimensions-connectors/lib/python3.7/site-packages/pandas/core/base.py in _aggregate(self, arg, *args, **kwargs)
    470             try:
--> 471                 result = DataFrame(result)
    472             except ValueError:

~/.virtualenvs/dimensions-connectors/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy)
    467         elif isinstance(data, dict):
--> 468             mgr = init_dict(data, index, columns, dtype=dtype)
    469         elif isinstance(data, ma.MaskedArray):

~/.virtualenvs/dimensions-connectors/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype)
    282         ]
--> 283     return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)
    284 

~/.virtualenvs/dimensions-connectors/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype, verify_integrity)
     77         if index is None:
---> 78             index = extract_index(arrays)
     79         else:

~/.virtualenvs/dimensions-connectors/lib/python3.7/site-packages/pandas/core/internals/construction.py in extract_index(data)
    386         if not indexes and not raw_lengths:
--> 387             raise ValueError(""If using all scalar values, you must pass an index"")
    388 

ValueError: If using all scalar values, you must pass an index

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
~/.virtualenvs/dimensions-connectors/lib/python3.7/site-packages/pandas/core/frame.py in aggregate(self, func, axis, *args, **kwargs)
   7358         try:
-> 7359             result, how = self._aggregate(func, axis=axis, *args, **kwargs)
   7360         except TypeError as err:

~/.virtualenvs/dimensions-connectors/lib/python3.7/site-packages/pandas/core/frame.py in _aggregate(self, arg, axis, *args, **kwargs)
   7383             return result, how
-> 7384         return super()._aggregate(arg, *args, **kwargs)
   7385 

~/.virtualenvs/dimensions-connectors/lib/python3.7/site-packages/pandas/core/base.py in _aggregate(self, arg, *args, **kwargs)
    474                 # we have a dict of scalars
--> 475                 result = Series(result, name=getattr(self, ""name"", None))
    476 

~/.virtualenvs/dimensions-connectors/lib/python3.7/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath)
    228 
--> 229             name = ibase.maybe_extract_name(name, data, type(self))
    230 

~/.virtualenvs/dimensions-connectors/lib/python3.7/site-packages/pandas/core/indexes/base.py in maybe_extract_name(name, obj, cls)
   5658     if not is_hashable(name):
-> 5659         raise TypeError(f""{cls.__name__}.name must be a hashable type"")
   5660 

TypeError: Series.name must be a hashable type

The above exception was the direct cause of the following exception:

TypeError                                 Traceback (most recent call last)
<ipython-input-6-efe06ed8dce0> in <module>
      2 data = {""name"": [""abc"", ""xyz""]}
      3 df = pd.DataFrame(data)
----> 4 print(df.agg({'name': 'count'}))

~/.virtualenvs/dimensions-connectors/lib/python3.7/site-packages/pandas/core/frame.py in aggregate(self, func, axis, *args, **kwargs)
   7363                 f""incompatible data and dtype: {err}""
   7364             )
-> 7365             raise exc from err
   7366         if result is None:
   7367             return self.apply(func, axis=axis, args=args, **kwargs)

TypeError: DataFrame constructor called with incompatible data and dtype: Series.name must be a hashable type
```

#### Expected Output
name   2
dtype: int64

#### Output of ``pd.show_versions()``
INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.7.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 17.4.0
Version          : Darwin Kernel Version 17.4.0: Sun Dec 17 09:19:54 PST 2017; root:xnu-4570.41.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.18.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.1
setuptools       : 46.1.3
Cython           : 0.29.16
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.7.4
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.11
tables           : None
tabulate         : 0.7.7
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

<details>

</details>
"
696038456,36224,BUG: GH36212 DataFrame agg() raises error when DataFrame column name is `name`,leonarduschen,closed,2020-09-08T17:07:18Z,2020-09-13T13:15:12Z,"- [x] closes #36212 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
700408688,36317,PERF: constructing string Series,topper-123,closed,2020-09-12T23:02:45Z,2020-09-13T13:25:51Z,"Avoid needless call to `lib.infer_dtype`, when string dtype.

Performance example:

```python
>>> x = np.array([str(u) for u in range(1_000_000)], dtype=object)
>>> %timeit pd.Series(x, dtype=str)
344 ms ± 59.7 ms per loop  # v1.1.0
157 ms ± 7.04 ms per loop  # after #35519
22.6 ms ± 191 µs per loop  # after #36304
11.2 ms ± 48.6 µs per loop  # after this PR
```

Similar speed-up is possible for `pd.Series(x, dtype=""string"")`, but requires some refactorng of `StringArray`, so I'll do that is a seperate PR.

xref #35519 & #36304."
700561207,36332,Backport PR #36147 on branch 1.1.x (REGR: Series access with Index of tuples/frozenset),meeseeksmachine,closed,2020-09-13T12:30:03Z,2020-09-13T13:27:22Z,Backport PR #36147: REGR: Series access with Index of tuples/frozenset
700028361,36303,REGR: Fix IntegerArray unary ops regression,dsaxton,closed,2020-09-12T04:06:22Z,2020-09-13T13:28:19Z,"- [x] closes #36063
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
700407628,36316,BUG: Don't overflow with large int scalar,dsaxton,closed,2020-09-12T22:58:41Z,2020-09-13T13:33:14Z,"- [x] closes #36291
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
694485212,36169,BUG GH31355 fixed by adding more relevant error message,TAJD,closed,2020-09-06T18:47:12Z,2020-09-13T13:36:56Z,"- [ ] closes #31355 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Added a test to trigger the reported error using code from the issue. An if statement was included to return a more helpful error message - borrowing the solution from the issue. "
700247248,36307,TYPING: `df.columns.levels` raises mypy error on MultiIndex for columns,hweecat,closed,2020-09-12T13:59:41Z,2020-09-13T13:59:11Z,"Observed in PR #36305 

When adding check on inferred type for each level in MultiIndex for columns, error message pops up in CI (Typing validation with mypy):

`pandas/io/parquet.py:59: error: ""Index"" has no attribute ""levels""; maybe ""nlevels""?  [attr-defined]`

I understand that single-level Indexes do not have the ""levels"" attribute while MultiIndexes have the ""levels"" attribute. It seems that mypy static type checking may have inferred ""df.columns"" as Index type by default, hence flagging the error when using ""df.columns.levels"" in the code change.

Would it make sense to add a ""levels"" attribute to ""Index"" for consistency with MultiIndex, or are there any possible workarounds for this mypy error?"
694354285,36159,ENH: add set_td_classes method for CSS class addition to data cells,attack68,closed,2020-09-06T11:59:10Z,2020-09-13T14:04:17Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Simple extension to Styler to allow cells to be assigned CSS classes from a DataFrame of strings.
Seems to be a reasonably common request.
The existing framework, i.e. the `cell_context` dictionary (which previously did nothing) is leveraged.

@TomAugspurger 
"
700375693,36313,REF: de-duplicate _wrap_joined_index in MultiIndex,jbrockmendel,closed,2020-09-12T21:02:03Z,2020-09-13T15:03:15Z,"Follow-up to #36282

We'll also be able to clean up MultiIndex special-casing in _reindex_non_unique.  I expect others but cant say for sure yet."
698713545,36280,PERF: CategoricalDtype.__eq__,jbrockmendel,closed,2020-09-11T01:52:01Z,2020-09-13T15:03:54Z,"Three special cases where we can get better performance

1) categories have different lengths --> never equal
2) categories have different dtypes --> never equal
3) categories are not object-dtype --> can use use get_indexer instead of hashing

```
import pandas as pd
import numpy as np

dti = pd.date_range(""2016-01-01"", periods=10000, tz=""US/Pacific"")
dti2 = type(dti)(dti._data.copy())

np.random.shuffle(dti._data._data)
assert not dti.equals(dti2)

cd1 = pd.CategoricalDtype(dti)
cd2 = pd.CategoricalDtype(dti2)
cd3 = pd.CategoricalDtype(dti2[:-1])
cd4 = pd.CategoricalDtype(pd.Index(range(len(dti))))

In [5]: %timeit cd1 == cd2 
548 µs ± 3.94 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  # <-- master
239 µs ± 1.04 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  # <-- PR

In [6]: %timeit cd1 == cd3
543 µs ± 4.05 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)   # <-- master
5.05 µs ± 257 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) # <-- PR

In [7]: %timeit cd1 == cd4
389 µs ± 13.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  # <-- master
2.17 µs ± 63.1 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)  # <-- PR

```"
700399600,36315,REF: separate out helpers from iLoc._setitem_with_indexer,jbrockmendel,closed,2020-09-12T22:29:30Z,2020-09-13T15:04:15Z,
700470489,36322,REF: de-duplicate get_indexer_non_unique,jbrockmendel,closed,2020-09-13T03:08:51Z,2020-09-13T15:05:25Z,
694090670,36139,Updated series documentation to close #35406,TAJD,closed,2020-09-05T13:52:12Z,2020-09-13T15:28:37Z,"- [x ] closes #35406
- [x ] tests added / passed
- [ x] passes `black pandas`
- [ x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Not sure how to do whatsnew entry for this - first contribution! Do let me know how to improve.

Output of scripts/validate_docstring.py: 
```
################################################################################
########################## Docstring (pandas.Series)  ##########################
################################################################################

One-dimensional ndarray with axis labels (including time series).

Labels need not be unique but must be a hashable type. The object
supports both integer- and label-based indexing and provides a host of
methods for performing operations involving the index. Statistical
methods from ndarray have been overridden to automatically exclude
missing data (currently represented as NaN).

Operations between Series (+, -, /, *, **) align values based on their
associated index values-- they need not be the same length. The result
index will be the sorted union of the two indexes.

Parameters
----------
data : array-like, Iterable, dict, or scalar value
    Contains data stored in Series.

    .. versionchanged:: 0.23.0
       If data is a dict, argument order is maintained for Python 3.6
       and later.

index : array-like or Index (1d)
    Values must be hashable and have the same length as `data`.
    Non-unique index values are allowed. Will default to
    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like
    and index is None, then the values in the index are used to
    reindex the Series after it is created using the keys in the data.
dtype : str, numpy.dtype, or ExtensionDtype, optional
    Data type for the output Series. If not specified, this will be
    inferred from `data`.
    See the :ref:`user guide <basics.dtypes>` for more usages.
name : str, optional
    The name to give to the Series.
copy : bool, default False
    Copy input data.

################################################################################
################################## Validation ##################################
################################################################################

3 Errors found:
        Parameters {'fastpath'} not documented
        See Also section not found
        No examples section found
```"
700570280,36334,Backport PR #36316 on branch 1.1.x (BUG: Don't overflow with large int scalar),meeseeksmachine,closed,2020-09-13T13:29:10Z,2020-09-13T15:52:02Z,Backport PR #36316: BUG: Don't overflow with large int scalar
700570178,36333,Backport PR #36303 on branch 1.1.x (REGR: Fix IntegerArray unary ops regression),meeseeksmachine,closed,2020-09-13T13:28:28Z,2020-09-13T15:52:28Z,Backport PR #36303: REGR: Fix IntegerArray unary ops regression
695023951,36189,BUG: Inconsistent behavior of index values for `DataFrame.apply` and `Series.apply`,YarShev,closed,2020-09-07T11:52:53Z,2020-09-13T15:53:38Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
pdf = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])
pdf.apply([""sum"", lambda df: df.sum(), lambda df: df.sum()]) # lambdas have postfix 0, 1 ...
             A   B
sum         12  27
<lambda_0>  12  27
<lambda_1>  12  27
s = pd.Series([4] * 3)
s.apply([""sum"", lambda df: df.sum(), lambda df: df.sum()]) # lambdas have not postfix 0, 1 ...
sum         12
<lambda>    12
<lambda>    12
dtype: int64
```

#### Problem description

Could anyone explain please? Is it normal behavior? Why DataFrame's index has postfix for lambdas, but Series's index hasn't. The same behavior is observed for `agg`. There is no such behavior in pandas==1.0.5.

#### Output of ``pd.show_versions()``

<details>

pandas : 1.1.1
numpy : 1.18.4
pytz : 2020.1
dateutil : 2.8.1
pip : 20.1.1
setuptools : 41.2.0
Cython : None
pytest : 5.4.2
hypothesis : None
sphinx : None
blosc : None
feather : 0.4.1
xlsxwriter : None
lxml.etree : 4.5.0
html5lib : None
pymysql : None
psycopg2 : None
jinja2 : 2.11.2
IPython : 7.14.0
pandas_datareader: None
bs4 : 4.9.1
bottleneck : None
fsspec : 0.7.3
fastparquet : None
gcsfs : None
matplotlib : 3.2.1
numexpr : 2.7.1
odfpy : None
openpyxl : 3.0.3
pandas_gbq : None
pyarrow : 0.16.0
pytables : None
pyxlsb : None
s3fs : 0.4.2
scipy : 1.4.1
sqlalchemy : 1.3.17
tables : 3.6.1
tabulate : None
xarray : 0.15.1
xlrd : 1.2.0
xlwt : None
numba : None

</details>
"
696387789,36240,PERF: Allow groupby transform with numba engine to be fully parallelizable ,mroeschke,closed,2020-09-09T04:03:17Z,2020-09-13T17:25:07Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

New performance comparison with 10k groups
```
In [1]: df_g = pd.DataFrame({'a': range(10**4), 'b': range(10**4), 'c': range(10**4)})

In [2]: df_g.groupby('a').transform(lambda x: x + 1)
Out[2]:
          b      c
0         1      1
1         2      2
2         3      3
3         4      4
4         5      5
...     ...    ...
9995   9996   9996
9996   9997   9997
9997   9998   9998
9998   9999   9999
9999  10000  10000

[10000 rows x 2 columns]

In [3]: %timeit df_g.groupby('a').transform(lambda x: x + 1)
9.48 s ± 885 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [4]: def f(values, index):
   ...:     return values + 1
   ...:

In [5]: df_g.groupby('a').transform(f, engine='numba', engine_kwargs={'parallel': True})
Out[5]:
            b        c
0         1.0      1.0
1         2.0      2.0
2         3.0      3.0
3         4.0      4.0
4         5.0      5.0
...       ...      ...
9995   9996.0   9996.0
9996   9997.0   9997.0
9997   9998.0   9998.0
9998   9999.0   9999.0
9999  10000.0  10000.0

[10000 rows x 2 columns]

In [6]: %timeit df_g.groupby('a').transform(f, engine='numba', engine_kwargs={'parallel': True})
4.41 ms ± 34.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
696192973,36232,TYP: @final for some Index methods,jbrockmendel,closed,2020-09-08T21:35:02Z,2020-09-13T18:42:26Z,"cc @simonjayhawkins 

Tangential question: when I run flake8 it passes, but the pre-commit hook often gives me a bunch of messages like `pandas/core/indexes/base.py:956:51: F821 undefined name 'ignore'`.  Is this something I can fix by pinning a version or something?"
694087204,36138,TYP: misc property return types,simonjayhawkins,closed,2020-09-05T13:33:15Z,2020-09-13T19:12:11Z,
692284169,36102,TYP: misc fixes for numpy types 4,simonjayhawkins,closed,2020-09-03T20:03:57Z,2020-09-13T19:15:53Z,"```
pandas\plotting\_matplotlib\tools.py:356: error: Incompatible return value type (got ""ndarray"", expected ""Sequence[Any]"")  [return-value]
pandas\plotting\_matplotlib\tools.py:358: error: Incompatible return value type (got ""Union[ndarray, Any]"", expected ""Sequence[Any]"")  [return-value]
pandas\plotting\_matplotlib\tools.py:359: error: Incompatible return value type (got ""ndarray"", expected ""Sequence[Any]"")  [return-value]
pandas\core\dtypes\cast.py:1079: error: No overload variant of ""to_datetime"" matches argument types ""ndarray"", ""str""  [call-overload]
pandas\core\arrays\categorical.py:1818: error: Signature of ""_ndarray"" incompatible with supertype ""NDArrayBackedExtensionArray""  [override]
pandas/core/arrays/datetimelike.py:465: error: Signature of ""_ndarray"" incompatible with supertype ""NDArrayBackedExtensionArray""  [override]
```"
692217751,36100,TYP: misc fixes for numpy types 3,simonjayhawkins,closed,2020-09-03T19:04:50Z,2020-09-13T19:18:46Z,"```
pandas\core\reshape\merge.py:1931: error: Incompatible types in assignment (expression has type ""ndarray"", variable has type ""ExtensionArray"")  [assignment]
pandas\core\reshape\merge.py:1932: error: Incompatible types in assignment (expression has type ""ndarray"", variable has type ""ExtensionArray"")  [assignment]
pandas\core\reshape\merge.py:1941: error: Incompatible types in assignment (expression has type ""ndarray"", variable has type ""ExtensionArray"")  [assignment]
pandas\core\reshape\merge.py:1950: error: Incompatible types in assignment (expression has type ""ndarray"", variable has type ""ExtensionArray"")  [assignment]
pandas\core\reshape\merge.py:1951: error: Incompatible types in assignment (expression has type ""ndarray"", variable has type ""ExtensionArray"")  [assignment]
pandas\io\pytables.py:5084: error: Incompatible types in assignment (expression has type ""ndarray"", variable has type ""ExtensionArray"")  [assignment]
```"
700619012,36340,Backport PR #36231: BUG: Fixe unintentionally added suffix in DataFrame.apply/agg and Series.apply/agg,simonjayhawkins,closed,2020-09-13T18:12:29Z,2020-09-13T19:21:29Z,#36231
700619752,36341,REF: move ShallowMixin to groupby.base,jbrockmendel,closed,2020-09-13T18:16:12Z,2020-09-13T20:25:32Z,Rename GroupByMixin  -> GotItemMixin
700626958,36342,CLN: remove CategoricalIndex._create_from_codes,jbrockmendel,closed,2020-09-13T19:00:02Z,2020-09-13T20:26:17Z,Update some constructor calls in Categorical
637293432,34717,PERF: pd.Series.map too slow for huge dictionary,charlesdong1991,closed,2020-06-11T20:14:17Z,2020-09-13T20:28:44Z,"Just found out a performance issue with `pd.Series.map`, seems it is very slow when the input is a huge dictionary. 

I noticed a similar issue reported before: #21278 and indeed for `Series` input, the first run might be slow and then for the later runs, they are very fast because hashable indexing is built. However, it doesn't seem to apply to `dict` input.

I slightly changed the example in #21278, and the runtime doesn't change if being run multiple times. And it is much faster using `apply` and `dict.get`. 

So I am curious if this performance issue is being aware , and i would expect performance when a dict is assigned between `pd.Series.map` and `pd.Series.apply(lambda x: blabla)` is quite similar.

```python
n = 1000000
domain = np.arange(0, n)
ranges = domain+10
maptable = pd.Series(ranges, index=domain).sort_index().to_dict()
query_vals = pd.Series([1,2,3])

%timeit query_vals.map(maptable)
```

while much faster if doing below:
```python
query_vals.apply(lambda x: maptable.get(x))
```
"
602905060,33666,"`read_spss` only accepts `str`, not `Path`",cantudiaz,closed,2020-04-20T03:07:49Z,2020-09-13T20:29:42Z,"`pandas` documentation states that the `read_spss` function can take either strings or `pathlih.Path` as input. It probably has something to do with `pyreadstat.read_sav`.

Example:
```python
>>> import pandas as pd
>>> from pathlib import Path
>>> filepath = Path(""Documents/test.sav"")
>>> pd.read_spss(filepath)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/user/anaconda3/envs/env/lib/python3.7/site-packages/pandas/io/spss.py"", line 43, in read_spss
    path, usecols=usecols, apply_value_formats=convert_categoricals
TypeError: Argument 'filename_path' has incorrect type (expected str, got PosixPath)
```

Libraries versions:
```python
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.0.0-32-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.0.3
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : 4.3.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.0.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 4.3.1
pyxlsb           : None
s3fs             : None
scipy            : 1.2.3
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```
 "
700595187,36338,DOC/CLN: remove versionadded/changed:: 0.23,simonjayhawkins,closed,2020-09-13T15:51:36Z,2020-09-13T20:31:11Z,released 3 Aug 2018 https://github.com/pandas-dev/pandas/releases/tag/v0.23.4
643709577,34948,Pd.series.map performance,Rohith295,closed,2020-06-23T10:06:25Z,2020-09-13T20:31:59Z,"There are other places also to refactor to improve the performance, but this current change has greater impact aswell.

- [ ] closes #34717
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry `improved the performance of pd.series.map`
"
694534870,36174,BUG: Ensure read_spss accepts pathlib Paths (GH33666),drmrd,closed,2020-09-06T21:28:24Z,2020-09-13T20:33:16Z,"- [x] closes #33666
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Wrapped the `path` passed into `pandas.io.spss.read_spss` in a `pandas.io.common.stringify_path` call prior to passing it to `pyreadstat.read_sav`. This fixes #33666. I've also included tests validating `pd.read_spss` accepts strings and `pathlib.Path`s."
700570865,36335,ERR: Cartesian product error,TAJD,closed,2020-09-13T13:32:56Z,2020-09-13T20:41:16Z,"- [x] closes #31355
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Added a test to trigger the reported error using code from the issue. An if statement was included to return a more helpful error message - borrowing the solution from the issue.

Replacement of [this](https://github.com/pandas-dev/pandas/pull/36169) pull request."
519163998,29460,xticks unnecessarily rotated,yui-haider,closed,2019-11-07T10:04:50Z,2020-09-13T20:46:09Z,"#### Code Sample
```python
import pandas as pd
import matplotlib.pyplot as plt
from io import StringIO

csv_string = """"""x,y1,y2
0, 1, 1
1, 1, 2
2, 1, 3
3, 4, 2
4, 2, 4
5, 3, 3
""""""

data = pd.read_csv(StringIO(csv_string))
dataframe = pd.DataFrame(data)

dataframe.plot(x=""x"",
               y=[""y1"", ""y2""],
               kind=""line"",
               subplots=True,
               sharex=True,
               marker="","",
               linewidth=0.5,
               title=""Title""
               )

plt.show()
```
#### Problem description
The xticks are unnecessarily rotated:
![grafik](https://user-images.githubusercontent.com/9128021/68378413-5c8b5980-014c-11ea-9d5e-6226ff4cc10f.png)
Also the title overlaps the subplot, but I will create a new issue for that.

#### Expected Output
The xticks shouldn't be rotated.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.2.final.0
python-bits      : 32
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.6.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
import sys; print('Python %s on %s' % (sys.version, sys.platform))
Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)] on win32
</details>
"
700642621,36347,Backport PR #36174 on branch 1.1.x (BUG: Ensure read_spss accepts pathlib Paths (GH33666)),meeseeksmachine,closed,2020-09-13T20:33:46Z,2020-09-13T21:42:34Z,Backport PR #36174: BUG: Ensure read_spss accepts pathlib Paths (GH33666)
418967013,25610,DataFrameGroupBy.ffill with Duplicate Column Labels ValueError,datatravelgit,closed,2019-03-08T21:37:20Z,2020-09-13T22:18:22Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np


df1 = pd.DataFrame({'field1': [1, 2, 3, 4],
                   'field2': [1, 2, 3, 4],
                   'field3': [1, 2, 3, 4]
                   })
df2 = pd.DataFrame({'field1': [1, 2, np.nan, 4],
                   })

same_col = pd.concat([df1, df2], axis=1)

print(same_col)
#    field1  field2  field3  field1
# 0       1       1       1     1.0
# 1       2       2       2     2.0
# 2       3       3       3     NaN
# 3       4       4       4     4.0


print(same_col.ffill())
#    field1  field2  field3  field1
# 0       1       1       1     1.0
# 1       2       2       2     2.0
# 2       3       3       3     2.0
# 3       4       4       4     4.0

for k, v in same_col.groupby(by=['field2']):
    print(v.ffill())
    #    field1  field2  field3  field1
    # 0       1       1       1     1.0
    #    field1  field2  field3  field1
    # 1       2       2       2     2.0
    #    field1  field2  field3  field1
    # 2       3       3       3     NaN
    #    field1  field2  field3  field1
    # 3       4       4       4     4.0

same_col.groupby(by=['field2']).ffill()
ValueError: Buffer has wrong number of dimensions (expected 1, got 2)

```

#### Problem description
A _DataFrameGroupBy.ffill_ with 2 or more column with the same name produce an error:
`ValueError: Buffer has wrong number of dimensions (expected 1, got 2)`
Pandas 0.22.0 did not have this bud. It seems that it was introduced recently. Or if this is an expected behaviour, it must be consistent with the behaviour of a _DataFrame.ffill_ with 2 or more column with the same name and have a meaningful error.

#### Expected Output

```
#    field1  field2  field3  field1
# 0       1       1       1     1.0
# 1       2       2       2     2.0
# 2       3       3       3     NaN
# 3       4       4       4     4.0
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
pd.show_versions()
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.1.final.0
python-bits: 64
OS: Darwin
OS-release: 18.2.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: None
LOCALE: en_GB.UTF-8
pandas: 0.24.1
pytest: None
pip: 10.0.1
setuptools: 39.1.0
Cython: None
numpy: 1.16.2
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
602496631,33633,CLN: replaced Appender with doc,smartvinnetou,closed,2020-04-18T16:20:35Z,2020-09-13T22:21:03Z,"- [x] ref #31942
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] replaced the Appender/Substitute decorators with doc
"
700530458,36326,[TST]: Groupy raised ValueError for ffill with duplicate column names,phofl,closed,2020-09-13T09:10:35Z,2020-09-13T22:23:02Z,"- [x] closes #25610
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`


Was fixed in the past. Added test to avoid regression
"
673510416,35566,BUG: corrupted SAS datasets retain file handles,rxxg,closed,2020-08-05T12:44:20Z,2020-09-13T22:28:12Z,"- [ ] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
from pathlib import Path

path = ...
p = Path(path, 'corrupt.sas7bdat')
try:
    pd.read_sas(p)
except:
    input(""Press enter to release file"")
```

#### Problem description

When I run the above script with a corrupt database like the one attached [corrupt.zip](https://github.com/pandas-dev/pandas/files/5028407/corrupt.zip) the Python process my Windows system ends up holding on to the open file.

I can tell that the file has not been correctly closed on error since, for example, other processes cannot open the same file.

#### Expected Output

While the main Python process is waiting for input, other processes should be able to read the given file.

I can't find any way of finding out if the current Python process has an open file handle on a given file, but it is expected behaviour that other processes should be able to access the file before

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.17763
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.1.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 10.0.1
setuptools       : 39.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : None

</details>
"
674301803,35587,Ensure file is closed promptly in case of error,rxxg,closed,2020-08-06T13:10:10Z,2020-09-13T22:28:13Z,Fixes #35566.  Replaces #35567.
670918367,35505,DOC: ExcelWriter constructor types,alexhtn,closed,2020-08-01T14:29:29Z,2020-09-13T23:07:21Z,"#### Location of the documentation

[https://pandas.pydata.org/docs/dev/reference/api/pandas.ExcelWriter.html?highlight=excelwriter](https://pandas.pydata.org/docs/dev/reference/api/pandas.ExcelWriter.html?highlight=excelwriter)
or
[https://github.com/pandas-dev/pandas/blob/d9fff2792bf16178d4e450fe7384244e50635733/pandas/io/excel/_base.py#L544](https://github.com/pandas-dev/pandas/blob/d9fff2792bf16178d4e450fe7384244e50635733/pandas/io/excel/_base.py#L544)


#### Documentation problem

Parameter `path` has type `str`, but based on this feature [https://github.com/pandas-dev/pandas/issues/7074](url) I can pass `io.BytesIO`.

#### Suggested fix for documentation

Change type description to `str | io.BytesIO`.
"
673519341,35568,DOC: add type BinaryIO to path param #35505,alexhtn,closed,2020-08-05T12:57:36Z,2020-09-13T23:07:41Z,"- [ ] closes #35505
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
520621351,29532,Update DataFrame.to_feather() docstring/api documentation and deprecate fname argument.,simonjayhawkins,closed,2019-11-10T16:10:51Z,2020-09-13T23:10:56Z,"#### Code Sample, a copy-pastable example if possible

This code works. 

```python
import pandas as pd
import pandas.util.testing as tm
from io import BytesIO

df = pd.DataFrame({""a"": [1, 2, 3], ""b"": list(""abc"")})

buf = BytesIO()
df.to_feather(fname=buf)
result = pd.read_feather(path=buf)

tm.assert_frame_equal(result, df)
```
#### Problem description

Documentation doesn't mention accepting a buffer.

```
    def to_feather(self, fname):
        """"""
        Write out the binary feather-format for DataFrames.

        Parameters
        ----------
        fname : str
            String file path.
        """"""
```
#### Expected Output

change argument name to better reflect types accepted and be consistent with other io methods.

would need to deprecate `fname` argument.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
665518764,35408,DOC: update DataFrame.to_feather docstring,arw2019,closed,2020-07-25T05:27:28Z,2020-09-13T23:11:01Z,"- [x] closes #29532
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

The issue was that `DataFrame.to_feather` had an `fname` argument which in fact could be either a filename or a buffer. Most of the work to close #29532 was done in #30338 where `fname` argument was deprecated in favor of `path`. Here I update the docstring to let users know that they can use a buffer. "
398991602,24768,Concatenating rows with Int64 datatype coerces to object,janvanrijn,closed,2019-01-14T17:22:02Z,2020-09-13T23:25:29Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

df_a = pd.DataFrame({'a': [-1, -1, -1]})
df_b = pd.DataFrame({'b': [1, 1, 1]})
df_a['a'] = df_a['a'].astype('Int64')
df_b['b'] = df_b['b'].astype('Int64')

total = pd.concat([df_a, df_b], ignore_index=True)

print(total.dtypes)
```

output:
```
a    object
b    object
dtype: object
```
#### Problem description
When running the exact same code with floats, the output is similar to the expected output. It also happens with the append function
```
import pandas as pd

df_a = pd.DataFrame({'a': [-1, -1, -1]})
df_b = pd.DataFrame({'b': [1, 1, 1]})
df_a['a'] = df_a['a'].astype('Int64')
df_b['b'] = df_b['b'].astype('Int64')

total = df_a.append([df_b], ignore_index=True)

print(total.dtypes)
```

#### Expected Output
```
a    Int64
b    Int64
dtype: object
```

#### Output of ``pd.show_versions()``
```
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.7.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-43-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.0.dev0+997.ga197837
pytest: None
pip: 10.0.1
setuptools: 39.1.0
Cython: 0.28.2
numpy: 1.15.4
scipy: 1.1.0
pyarrow: 0.9.0
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.7.2
pytz: 2018.4
blosc: None
bottleneck: 1.2.1
tables: None
numexpr: 2.6.8
feather: 0.4.0
matplotlib: 2.2.2
openpyxl: None
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
```"
698528739,36278,Concatenating rows with Int64 datatype coerces to object,phofl,closed,2020-09-10T21:46:06Z,2020-09-13T23:54:12Z,"- [x] closes #24768
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`


Issue was fixed in the past.
"
700591891,36337,"BUG: iloc.__setitem__ with DataFrame value, multiple blocks, non-unique columns",jbrockmendel,closed,2020-09-13T15:32:48Z,2020-09-14T00:31:00Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
700240181,36306,BUG: DataFrameGroupBy.apply with as_index False column name,rhshadrach,open,2020-09-12T13:37:14Z,2020-09-14T03:24:50Z,"Example:

```
df = DataFrame({""A"": [1, 2], ""B"": [3, 4]})
g = df.groupby(""A"", as_index=False)
result = g.apply(len)
print(result)
```

results in

```
   A  NaN
0  1    1
1  2    1
```

If we were to use `pandas.core.common.get_callable_name`, then we would be able to have the column name be ""len"" in this case, and presumably others."
247348096,17154,read_csv returns different float values for same number,chrisyeh96,closed,2017-08-02T10:49:57Z,2020-09-14T03:59:23Z,"#### Code Sample, a copy-pastable example if possible
`test.csv`
```
-15.361
-15.361000
```

```
>>> import pandas as pd
>>> x = pd.read_csv('test.csv', header=None)
>>> x.loc[0, 0] == x.loc[1, 0]
False
```
#### Problem description / Expected output

The expected output of the code above is 
```
>>> x.loc[0, 0] == x.loc[1, 0]
True
```
We should expect both `-15.361` and `-15.361000` to be converted to the same `np.float64` representation. However, they are converted to different float values, differing in exactly the last bit of their floating point representation. For some reason, `-15.361` gets converted incorrectly to `0xC02EB8D4FDF3B645` whereas `-15.361000` is correctly to `0xC02EB8D4FDF3B646`.

For completeness, here are some more comparisons
`x.loc[1, 0]` is equal (`==`) to `np.float64('-15.361')`, `np.float64('-15.361000')`, and `float('-15.361000')`.
`x.loc[0, 0]` is not equal to any of those.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 3.13.0-73-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.20.3
pytest: None
pip: 9.0.1
setuptools: 36.2.4
Cython: None
numpy: 1.13.1
scipy: 0.19.1
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.9999999
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
pandas_gbq: None
pandas_datareader: None"
690834648,36063,BUG: Unary minus raises for series with Int64Dtype,mila,closed,2020-09-02T08:41:07Z,2020-09-14T04:19:37Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas (commit 73c1d32)

---


#### Code Sample, a copy-pastable example

```python
>>> import pandas as pd
>>> pd.__version__
'1.1.1'
>>> s = pd.Series([1, 2, 3], dtype=""Int64"")
>>> -s
Traceback (most recent call last):
  File ""..."", line 1, in <module>
    -s
  File "".../lib/python3.8/site-packages/pandas/core/generic.py"", line 1297, in __neg__
    arr = operator.neg(values)
TypeError: bad operand type for unary -: 'IntegerArray'
```


#### Problem description

I cannot negate series with Int64Dtype. This was possible in previous versions (but it returned object dtype).

#### Expected Output

```
>>> import pandas as pd
>>> pd.__version__
'1.0.5'
>>> s = pd.Series([1, 2, 3], dtype=""Int64"")
>>> -s
0    -1
1    -2
2    -3
dtype: object
```

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
623632623,34334,BUG: xticks unnecessarily rotated,MarcoGorelli,closed,2020-05-23T10:48:10Z,2020-09-14T07:25:10Z,"- [x] closes #29460
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

On `master`, xticklabels are rotated if
```python
            not self._use_dynamic_x()
            and data.index.is_all_dates
            and not self.subplots
            or (self.subplots and self.sharex)
```

There's a comment saying
```python
        if condition:
            # irregular TS rotated 30 deg. by default
            # probably a better place to check / set this.
            if not self._rot_set:
                self.rot = 30
```
, and it seems that the _intention_ was to rotate ticklabels if there's an irregular timeseries.

With the condition as it is written, they will also be rotated if the following is true:
```python
(self.subplots and self.sharex)
```

I don't see why that should warrant rotating the ticklabels, so I'd like to suggest that there was a bracketing error.

Also, the condition `data.index.is_all_dates` is (as far as I can tell) only relevant if `use_index=True`, so I've changed that.

Here's what the OP's plot looks like with these changes:

![image](https://user-images.githubusercontent.com/33491632/82729010-04810280-9cec-11ea-825c-5244a2a1f828.png)


Here's what it looks like on master:

![image](https://user-images.githubusercontent.com/33491632/82729022-1bbff000-9cec-11ea-9954-4e75488f91b8.png)

Irregular time series still have their ticklabels rotated:
![image](https://user-images.githubusercontent.com/33491632/84986706-e796ee00-b136-11ea-9df1-d067a0de134b.png)

Regular time series' ticklabels don't rotate:
![image](https://user-images.githubusercontent.com/33491632/84986756-fd0c1800-b136-11ea-857d-738f17bb74c1.png)

(unless you pass `xcompat=True`, but that's unaffected by this change)
"
696188706,36231,BUG: Fix unintentionally added suffix in DataFrame.apply/agg and Series.apply/agg,charlesdong1991,closed,2020-09-08T21:26:39Z,2020-09-14T09:54:41Z,"- [x] closes #36189
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
693379165,36122,Regression in pandas/io/excel/_odfreader.py  (UnboundLocalError: local variable 'spaces' referenced before assignment),cpbl,closed,2020-09-04T16:45:44Z,2020-09-14T11:29:05Z,"An OpenDocument spreadsheet with a single cell containing a superscript causes a `UnboundLocalError: local variable 'spaces' referenced before assignment` crash in Pandas.

I'm attaching the .ods file.
 This used to work fine (one month ago on my machine). I tried upgrading to Pandas 1.1.1 and get the same bug. It looks like what's below:

```
In [2]: import pandas as pd

In [3]: pd.read_excel('/home/meuser/tmp/pandas-bug.ods', sheet_name='Sheet1', engine='odf')
---------------------------------------------------------------------------
UnboundLocalError                         Traceback (most recent call last)
<ipython-input-3-0fec79f4321d> in <module>
----> 1 pd.read_excel('/home/meuser/tmp/pandas-bug.ods', sheet_name='Sheet1', engine='odf')

~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs)
    294                 )
    295                 warnings.warn(msg, FutureWarning, stacklevel=stacklevel)
--> 296             return func(*args, **kwargs)
    297 
    298         return wrapper

~/.local/lib/python3.8/site-packages/pandas/io/excel/_base.py in read_excel(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)
    309         )
    310 
--> 311     return io.parse(
    312         sheet_name=sheet_name,
    313         header=header,

~/.local/lib/python3.8/site-packages/pandas/io/excel/_base.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)
    904             DataFrame from the passed in Excel file.
    905         """"""
--> 906         return self._reader.parse(
    907             sheet_name=sheet_name,
    908             header=header,

~/.local/lib/python3.8/site-packages/pandas/io/excel/_base.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)
    441                 sheet = self.get_sheet_by_index(asheetname)
    442 
--> 443             data = self.get_sheet_data(sheet, convert_float)
    444             usecols = _maybe_convert_usecols(usecols)
    445 

~/.local/lib/python3.8/site-packages/pandas/io/excel/_odfreader.py in get_sheet_data(self, sheet, convert_float)
     89             for j, sheet_cell in enumerate(sheet_cells):
     90                 if sheet_cell.qname == table_cell_name:
---> 91                     value = self._get_cell_value(sheet_cell, convert_float)
     92                 else:
     93                     value = self.empty_value

~/.local/lib/python3.8/site-packages/pandas/io/excel/_odfreader.py in _get_cell_value(self, cell, convert_float)
    173             return float(cell_value)
    174         elif cell_type == ""string"":
--> 175             return self._get_cell_string_value(cell)
    176         elif cell_type == ""currency"":
    177             cell_value = cell.attributes.get((OFFICENS, ""value""))

~/.local/lib/python3.8/site-packages/pandas/io/excel/_odfreader.py in _get_cell_string_value(self, cell)
    209                     if fragment.qname == text_s:
    210                         spaces = int(fragment.attributes.get((TEXTNS, ""c""), 1))
--> 211                     value.append("" "" * spaces)
    212         return """".join(value)

UnboundLocalError: local variable 'spaces' referenced before assignment
> /home/meuser/.local/lib/python3.8/site-packages/pandas/io/excel/_odfreader.py(211)_get_cell_string_value()
    208                 elif isinstance(fragment, Element):
    209                     if fragment.qname == text_s:
    210                         spaces = int(fragment.attributes.get((TEXTNS, ""c""), 1))
--> 211                     value.append("" "" * spaces)
    212         return """".join(value)

```


[pandas-bug.zip](https://github.com/pandas-dev/pandas/files/5176030/pandas-bug.zip)

Versions:

In [2]: pd.show_versions()

```
INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-45-generic
Version          : #49-Ubuntu SMP Wed Aug 26 13:38:52 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_CA.UTF-8
LOCALE           : en_CA.UTF-8

pandas           : 1.1.1
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.7.3
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : 0.29.21
pytest           : 4.6.9
hypothesis       : None
sphinx           : 1.8.5
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.1.0
xlwt             : 1.3.0
numba            : None

```
"
402869048,24915,"Cannot do agg if dataframe has column ""name""",ghost,closed,2019-01-24T19:51:07Z,2020-09-14T13:40:28Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
import pandas as pd

df = pd.DataFrame({""a"": range(10)})
df.agg({""a"": 'mean'}) # this line runs good

df[""name""] = range(10)
df.agg({""a"": 'mean'}) # now we have RuntimeError: maximum recursion depth exceeded
```
#### Problem description

If a dataframe has a column which is called ""name"", doing aggregation on the dataframe will cause RuntimeError.

##### But if the column is not called ""name"" but with some variant like ""name0"", ""name_"", it runs good.

```python
import pandas as pd

df = pd.DataFrame({""a"": range(10)})
df.agg({""a"": 'mean'}) # this line runs good

df[""name_""] = range(10)  # not ""name""
df.agg({""a"": 'mean'}) # this line runs good as well
```

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.15.final.0
python-bits: 64
OS: Darwin
OS-release: 18.0.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.23.4
pytest: None
pip: 18.1
setuptools: 40.6.3
Cython: None
numpy: 1.16.0
scipy: 1.2.0
pyarrow: 0.11.1
xarray: None
IPython: 5.8.0
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.9999999
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: 0.2.0
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
700599954,36339,REF: use check_setitem_lengths in DTA.__setitem__,jbrockmendel,closed,2020-09-13T16:16:57Z,2020-09-14T15:58:27Z,"Turning this into a two-line check will make it easy to add to `PandasArray.__setitem__` and `Categorical.__setitem__`, at which point we'll be able to move these methods up to `NDArrayBackedExtensionArray.__setitem__`"
694522333,36172,Clarify docs for df.to_sql `chunksize`,gbrova,closed,2020-09-06T20:45:00Z,2020-09-14T20:25:13Z,"- [x] closes #35891
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Note: this is a docs pr, so not adding tests or a whatsnew entry. "
701191049,36361,DOC: New storage_options for read_csv needs a docstring,Dr-Irv,closed,2020-09-14T15:09:45Z,2020-09-14T20:26:01Z,"#### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.read_csv.html
#### Documentation problem

The new `storage_options` keyword for `read_csv` and `read_table` is not documented in the docstring.  

#### Suggested fix for documentation

Add docstring

@martindurant 

NOTE:  I noticed this when running `validate_docstrings` locally for `read_csv` and it reported this.
"
588852694,33055,[WIP] Make Index.values read-only,ivirshup,closed,2020-03-27T03:01:13Z,2020-09-14T20:48:10Z,"- [ ] closes #33001
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This is a PR for looking around enforcing immutability of `pd.Index` objects. It will be a draft, but the main idea is to show what tests fail and what would need to change for this to be implemented. Because of this, I'm going to add some commits incrementally, so there is a history in the build logs."
701291406,36364,DOC: Added docstring for storage_options GH36361,ghost,closed,2020-09-14T17:31:20Z,2020-09-15T01:33:00Z,"DOC: Added docstring for storage_options GH36361

- [x] closes #36361
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
42895809,8283,CLN/TST: move consoliate sort_index to core/generic.py,jreback,closed,2014-09-16T15:31:42Z,2020-09-15T02:11:48Z,"xref #8282
xref #8239 (this issue won't touch the API per se)

expand extend: `core/generic.py/NDFrame.sort_index`: https://github.com/pydata/pandas/blob/master/pandas/core/generic.py#L1606

with the frame version.
https://github.com/pydata/pandas/blob/master/pandas/core/frame.py#L2689

needs generic docs (e.g. look how reindex is done)

as prob best to start with the frame version that handles ndim=2. Need tests for added capabilities to Series/Panel, e.g. mainly supporting sort_index on a multi-level index.
"
701321698,36366,REF: share code for __setitem__,jbrockmendel,closed,2020-09-14T18:20:09Z,2020-09-15T02:28:19Z,
701334812,36367,REF: _assert_can_do_op -> _validate_scalar,jbrockmendel,closed,2020-09-14T18:39:08Z,2020-09-15T02:29:00Z,We have a lot of methods that do something like this; trying to consolidate them with consistent naming conventions.
701258291,36362,"REF: _unbox_scalar, _unbox_listlike for Categorical",jbrockmendel,closed,2020-09-14T16:40:52Z,2020-09-15T02:29:36Z,
701270019,36363,DOC: move release note for #36175,simonjayhawkins,closed,2020-09-14T16:57:09Z,2020-09-15T07:19:17Z,xref https://github.com/pandas-dev/pandas/pull/36175#issuecomment-691994652
673032510,35553,[MINOR] Fix unnecessary pluralization,deepyaman,closed,2020-08-04T19:33:30Z,2020-09-15T10:55:35Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
685539535,35893,Fix interpolate limit area and limit direction with pad,simonjayhawkins,closed,2020-08-25T14:35:26Z,2020-09-15T11:34:43Z,"fork of #31048

@jreback  I don't seem to be able to push updates to #31048, so could close and carry on here"
701022411,36355,Backport PR #36175: BUG: read_excel for ods files raising UnboundLocalError in certain cases,simonjayhawkins,closed,2020-09-14T11:44:37Z,2020-09-15T13:18:14Z,#36175
691499864,36080,BUG: add py39 compat check for ast.slice #32766,fangchenli,closed,2020-09-02T23:19:47Z,2020-09-15T15:13:49Z,"- [x] closes #32766
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
694578132,36177,Move sort index to generic,fangchenli,closed,2020-09-06T23:57:53Z,2020-09-15T15:14:21Z,"- [x] closes #8283
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
700492472,36323,CI: install numpy from pip #36296,fangchenli,closed,2020-09-13T04:42:13Z,2020-09-15T15:15:02Z,"Part of #36296
"
698322671,36272,CI: xfail failing parquet test,jbrockmendel,closed,2020-09-10T18:15:21Z,2020-09-15T16:09:48Z,
702196880,36387,Updated files inside the pandas/tests/scalar folder,Abishek15592,closed,2020-09-15T19:26:03Z,2020-09-15T20:23:16Z,"Successfully removed the unnecessary trailing commas from the below files:

1. pandas/tests/scalar/test_na_scalar.py
2. pandas/tests/scalar/timestamp/test_arithmetic.py
3. pandas/tests/scalar/timestamp/test_constructors.py"
582635301,32761,Rolling mean on a time-based period with one data point does not return the exact mean,bcbrock,closed,2020-03-16T22:28:45Z,2020-09-15T22:20:48Z,"First, thank you all for creating and maintaining this package, and for the obvious care that has gone into the numerical analysis of the `rolling` methods.

#### Code Sample, a copy-pastable example if possible

```python
import numpy as np
import pandas as pd
print('Pandas Version', pd.__version__)
pd.options.display.float_format = '{:,.15f}'.format
np.random.seed(42)

index = pd.date_range(start='2020-01-01', end= '2020-01-02', freq='1s').append(pd.DatetimeIndex(['2020-01-03']))
data = np.random.rand(len(index))

df = pd.DataFrame({'data':data}, index=index)
df['mean'] = df.rolling('60s').mean()
df
```
Result

```
>>> print('Pandas Version', pd.__version__)
Pandas Version 1.0.2
...
>>> df
                                 data              mean
2020-01-01 00:00:00 0.374540118847362 0.374540118847362
2020-01-01 00:00:01 0.950714306409916 0.662627212628639
2020-01-01 00:00:02 0.731993941811405 0.685749455689561
2020-01-01 00:00:03 0.598658484197037 0.663976712816430
2020-01-01 00:00:04 0.156018640442437 0.562385098341631
...                               ...               ...
2020-01-01 23:59:57 0.888246435024149 0.461337636515083
2020-01-01 23:59:58 0.825973496654809 0.468473569401559
2020-01-01 23:59:59 0.487629482790733 0.473417509587717
2020-01-02 00:00:00 0.705341871943242 0.480022530163534
2020-01-03 00:00:00 0.105076984427931 0.105076984428225

[86402 rows x 2 columns]
```
#### Problem description

In the example above I show a rolling mean over a 1-minute window, but the final point jumps ahead by one day. Since this is the only point in the final window, I expect the mean here to be *exactly* the single value in the window. You can see however that there is some error in the low-order 4 dights displayed.

I believe this is due to an omission in the function `roll_mean_variable` which I am reading in the file `pandas/_libs/window/aggregations.pyx`. I believe that the function should zero the running sum whenever the number of observations goes to 0 after a ""remove"" operation. Otherwise, some noise from previous periods will be left in the running sum.

If I apply this patch to my local copy then I get the exact mean. 

```
diff --git a/pandas/_libs/window/aggregations.pyx b/pandas/_libs/window/aggregations.pyx
index 495b436..76ba1dd 100644
--- a/pandas/_libs/window/aggregations.pyx
+++ b/pandas/_libs/window/aggregations.pyx
@@ -344,20 +344,24 @@ def roll_mean_variable(ndarray[float64_t] values, ndarray[int64_t] start,
                     val = values[j]
                     add_mean(val, &nobs, &sum_x, &neg_ct)
 
             else:
 
                 # calculate deletes
                 for j in range(start[i - 1], s):
                     val = values[j]
                     remove_mean(val, &nobs, &sum_x, &neg_ct)
 
+                # Reset sum if NOBS goes to 0
+                if nobs == 0:
+                    sum_x = 0
+
                 # calculate adds
                 for j in range(end[i - 1], e):
                     val = values[j]
                     add_mean(val, &nobs, &sum_x, &neg_ct)
 
             output[i] = calc_mean(minp, nobs, neg_ct, sum_x)
 
             if not is_monotonic_bounds:
                 for j in range(s, e):
                     val = values[j]
```


I understand that there is a philosophical question: Given that we know that most rolling means are likely not bit-accurate anyway, is it worth doing a check for this special case? One argument would be that for time series like mine with gaps here and there, the gaps would serve to clear the accumulated errors every time a gap occurred, and in general the accuracy of the procedure would improve.

I notice that the rolling variance calculations *do* reset the accumulators to 0 when they completely depopulate a window. I am curious though, is there a numerical-analysis reason for doing the adds before the removes there? This order nullifies the resetting-effect of clearing the accumulators I believe, because my windows will never go to empty with this ordering. This is the code I'm referring to: 

https://github.com/pandas-dev/pandas/blob/5da500af76d0031021a8e906d8f34dc3e6ded234/pandas/_libs/window/aggregations.pyx#L520

Thank you,

Bishop Brock

#### Expected Output

I expect the rolling mean of any window specified with a time period with a single data point to be equal to that data point. 

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.2+0.g7485dbe6f.dirty
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.2.1
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.1
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.2.1
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.9
tables           : 3.5.2
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.1
numba            : 0.45.1

</details>
"
156154168,13254,PERF/COMPAT: use kahan summation in .rolling(..).sum(),jreback,closed,2016-05-22T14:15:41Z,2020-09-15T22:20:49Z,"from [mailing list](https://groups.google.com/forum/#!topic/pydata/Bl7QLr-Y5Z0)

to avoid imprecision errors as the rolling computations are evaluated marginally (sliding the window and adding new / subtracting old). We tend to accumulate floating point errors.

I looks like just an extra sum and subtract so I think the perf impact would be minimal.

This might be able to be applied to `.rolling(..).mean()` as well. Note that `.rolling(..).std/var` already use Welford's algo for better precision.

https://en.wikipedia.org/wiki/Kahan_summation_algorithm

implementation from `numpy`

```
cdef double kahan_sum(double *darr, npy_intp n):
    cdef double c, y, t, sum
    cdef npy_intp i
    sum = darr[0]
    c = 0.0
    for i from 1 <= i < n:
        y = darr[i] - c
        t = sum + y
        c = (t-sum) - y
        sum = t
    return sum

```
"
86732438,10319,rolling_mean returns small negative value on Series with non-negative values,ahu2012,closed,2015-06-09T20:55:29Z,2020-09-15T22:20:49Z,"We came upon this issue today while trying to take a square root of a non-negative series. 

A quick search came up with this old issue: https://github.com/pydata/pandas/issues/2527.The issue was reproduced on 0.15 by tweaking the code from the previous report, which is below:

```
from pandas import date_range, Series
s = Series(index=date_range('1999-02-03','1999-04-05'))
s['1999-02-03'] = 0.00012456
s['1999-02-04'] = 0.0003
s['1999-04-04'] = -0.0
s['1999-04-05'] = -0.0
In [21]: s.rolling(1).mean()['1999-04-05']
Out[21]: -5.421010862427522e-20
```

which returns -5.4210108624275222e-20.
"
560782346,31735,BUG: groupby _cython_agg_blocks implicitly assumes unique columns,jbrockmendel,closed,2020-02-06T05:07:00Z,2020-09-15T22:33:48Z,"xref #31616, the two test cases that adds both have unique columns.  Editing test_agg_split_object_part_datetime to make columns non-unique breaks it:

```
df = pd.DataFrame(
            {
                ""A"": pd.date_range(""2000"", periods=4),
                ""B"": [""a"", ""b"", ""c"", ""d""],
                ""C"": [1, 2, 3, 4],
                ""D"": [""b"", ""c"", ""d"", ""e""],
                ""E"": pd.date_range(""2000"", periods=4),
                ""F"": [1, 2, 3, 4],
            }
).astype(object)
df.columns = [""A"", ""B"", ""B"", ""D"", ""E"", ""F""]

>>> result = df.groupby([0, 0, 0, 0]).min()
pandas.core.indexes.base.InvalidIndexError: Reindexing only valid with uniquely valued Index objects
```

cc @WillAyd @TomAugspurger "
263621187,17810,"Empty cells make Padas use float, even if read_csv(dtype={'FOO': str}) is used",MartinThoma,closed,2017-10-07T06:38:37Z,2020-09-15T22:34:51Z,"#### Code Sample, a copy-pastable example if possible

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd

csv_path = 'test.csv'
df = pd.read_csv(csv_path, delimiter=';', quotechar='""',
                 decimal=',', encoding=""ISO-8859-1"", dtype={'FOO': str})
df.FOO = df.FOO.map(lambda n: n.zfill(6))
print(df)
```

test.csv:

```
FOO;BAR
01,23;4,56
1,23;45,6
;987
```

#### Problem description

When I use `dtype={'FOO': str}`, I expect pandas to treat the column as a string. This seems to work, but when an empty cell is present Pandas seems to switch to float.

#### Expected Output

```
      FOO     BAR
0  001,23    4.56
1  001,23   45.60
2  000000  987.00
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Linux
OS-release: 4.10.0-35-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.20.3
pytest: 3.2.2
pip: 9.0.1
setuptools: 20.7.0
Cython: None
numpy: 1.13.3
scipy: 0.19.0
xarray: None
IPython: 6.2.1
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0b10
sqlalchemy: 1.1.14
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
pandas_gbq: None
pandas_datareader: None

</details>
"
700970703,36353,BUG: DataFrame.stack does not work when MultiIndex had duplicated names,cipri-tom,closed,2020-09-14T10:22:57Z,2020-09-15T22:35:41Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

`data.csv`:
```csv
    , Q1, Q1, Q1, Q2, Q2, Q2
    , V1, V2, V3, V1, V2, V3
Ram , 11, 19, 10, 10, 12, 14
Syam, 11, 19, 10, 10, 12, 14

```

```python
# Your code here
df = pd.read_clipboard(',', header=[0,1], index_col=0)
df.stack(0)

ValueError: The name      occurs multiple times, use a level number
```

#### Problem description

Well, I *am* using a level number, not a name, but there still is an error because the level names are duplicated: each is `    ` (4 spaces). I would expect it to work, or to have a more descriptive error, maybe upon creation of the data-frame: ""MultiIndex should not have identical names"".

I believe this is related to #19029 

However, interestingly `df.stack(1)` works correctly, no problems with duplicate names.

#### Expected Output
```
H2         V1   V2   V3
     H1
Ram   Q1   11   19   10
      Q2   10   12   14
Syam  Q1   11   19   10
      Q2   10   12   14
```

Solved it with:
```
df.columns.names = ['H1', 'H2']
df.stack(0) # works
```




#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.4.0
Version          : Darwin Kernel Version 19.4.0: Wed Mar  4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_UK.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.2
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.6.0.post20200814
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.14.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
542426915,30480,Adding sql functionality to pandas similar to spark sql,zbrookle,closed,2019-12-26T04:17:35Z,2020-09-15T22:40:55Z,"I would like to add functionality to pandas so that data frames can be queried like database tables, similar to the way that they can be in spark-sql. 

I think it should work in a similar fashion.

A table can be registered using register_temp_table(dataframe, table_name).

Then using ```pandas.query(""select * from table_name"")``` you can query the data frame or any other ones registered using standard sql syntax.

I've already implemented the entire thing, but I was told to open an issue for it. 

Also I'm aware that there is a package called pandassql but this package actually just puts a data frame into a sql lite database, as opposed to querying a data frame directly, and transforming the sql into pandas methods that are then applied to the data frame.

Motivation:
The motivation for this enhancement is to make pandas more accessible to a crowd of users that may not be as technical and also to provide ease of transition for legacy code in systems like sas that have SQL already embedded in their programs. I'll supply a context free grammar in my documentation to show exactly what this system can handle, but it can basically handle any traditional SQL select statement, including subqueries, joins, where clauses, group by clauses, any aggregate function already supported by pandas, limit, and order by clauses. It also has support for rank and dense_rank window functions. It can't do things that sql wouldn't normally do like cross tab and you can't use a user defined function in it although I think that could be a good add-on. 

Datatypes:
The interface supports all pandas datatypes, so to cast something as an integer the syntax would currently be cast(some_number as int64) or cast(some_int as object). I've played around with the idea of varchar, char, bigint and smallint, but I think those would be misleading as those aren't datatypes that are supported by pandas currently.

Errors:
Currently the exceptions that it will throw that come this api are based solely around trying to select from an unregistered table, or from submitting an improperly written sql query, both of which you wouldn't want to silence so there's only one error mode. 

Api Choices:
The reason I made the register_temp_table section of the api top level was to avoid attaching a method to DataFrame although if others think it might be better as a method, I would change it in that manner (DataFrame.register_temp_table(table_name)). The reason pandas.query is a top level method is that it's relational in structure. You can select from multiple tables and join them and such and so it wouldn't make sense for it to be on a DataFrame level. The only similarity to the .query DataFrame method though is the name. DataFrame.query is just an alternate way of expressing things like DataFrame[some_condition] whereas my .query encompasses a large amount of the pandas api.

Built In:
I have two reasons that I think this would be better built in. The first is that the target audience for this is less technical pandas users. Part of making this api easier to use is lessening the burden of researching code and learning how python works, so I think that for them to go looking for an external package may be hard to begin with and they would also need to know to look for one. 
My second reason is that, from using what I've built, I've found pandas a lot easier to use just as a developer.
Suppose we have a DataFrame with one column called A, it goes from 
This code:
```
dataframe[name_1] = dataframe[a] - 1
dataframe[name_2] = dataframe[a] + 1
dataframe = dataframe[dataframe[name_1] == dataframe[name_2]]
dataframe.drop(columns=['a'], inplace=True)
```

To this code:
```pd.query(""select a - 1 as name_1, a + 1 as name_2 from some_table where name_1 = name_2"")```

Also although I did implement register_temp_table as an api level function, it would serve best as a method on a DataFrame so that's another thing to consider. 

I can't really provide any support for the lark part, other than that it seemed like the best tool for what I was making.

I apologize for the style and such, I'll be fixing all that before I'm done. I implemented this outside of pandas first, so that's why there are so many style and documentation discrepancies.  

"
701418730,36370,DOC: Add dataframe_sql to eco system page,zbrookle,closed,2020-09-14T20:53:47Z,2020-09-15T22:40:59Z,"Adds dataframe_sql, a package for querying pandas dataframes with SQL, to the ECO system page. Also closes #30480"
695358939,36200,Backport PR #36191 on branch 1.1.x (TST: update test_series_factorize_na_sentinel_none for 32bit),meeseeksmachine,closed,2020-09-07T19:41:11Z,2020-09-07T20:35:07Z,Backport PR #36191: TST: update test_series_factorize_na_sentinel_none for 32bit
547667472,30859,Multicolumn GroupBy appears to convert unit64s to floats,brianwgoldman,closed,2020-01-09T19:13:45Z,2020-09-07T20:47:40Z,"#### Code Sample, a copy-pastable example if possible

```python
pd.DataFrame({'first': [1], 'second': [1], 'value': [16148277970000000000]}).groupby(['first', 'second'])['value'].max()

```
#### Problem description

When that code snippet runs, the result is not `16148277970000000000` as you would expect, but `16148277969999998976`. Note that `int(float(16148277970000000000)) == 16148277969999998976`.

Additional notes:

1. The problem only appears to happen if there are multiple groupby keys. For example just doing `groupby(['first'])` returns the expected result. So does removing the `groupby` statement entirely.
2. The problem is not specific to `max`. I get the same problem for `min`, `first`, `last`, `median`, `mean`, but not `head`, `tail`, or `apply`.
3. The problem also happens if you do `.transform(max)`.
4. The smallest number I've found so far that has a problem is 2**63 + 1


#### Expected Output
16148277970000000000


#### Output of ``pd.show_versions()``
<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.137+
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.5
pytz             : 2018.9
dateutil         : 2.6.1
pip              : 19.3.1
setuptools       : 42.0.2
Cython           : 0.29.14
pytest           : 3.6.4
hypothesis       : None
sphinx           : 1.8.5
blosc            : None
feather          : 0.4.0
xlsxwriter       : None
lxml.etree       : 4.2.6
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.6.1 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 5.5.0
pandas_datareader: 0.7.4
bs4              : 4.6.3
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : 0.6.0
lxml.etree       : 4.2.6
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 2.5.9
pandas_gbq       : 0.11.0
pyarrow          : 0.14.1
pytables         : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.12
tables           : 3.4.4
xarray           : 0.14.1
xlrd             : 1.1.0
xlwt             : 1.3.0
xlsxwriter       : None
</details>
"
694395754,36164,TST verify groupby doesn't alter unit64s to floats #30859,TAJD,closed,2020-09-06T14:12:04Z,2020-09-07T20:47:44Z,"- [x] closes #30859 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Logic of the test is to check that the value hasn't changes and that the dtype remains uint64."
695354790,36199,Backport PR #36134: BUG: shows correct package name when import_optional_dependency is ca…,simonjayhawkins,closed,2020-09-07T19:30:17Z,2020-09-07T20:51:23Z,xref #36134
689685997,36018,QST: is the new behavior of GroupByRolling for MultiIndex in v1.1.1 intended?,itholic,closed,2020-09-01T00:06:15Z,2020-09-07T21:06:29Z,"#### Question about pandas

Let's say we have a `Series` with `MultiIndex` like the below.

```python
>>> pser = pd.Series(
...     [1, 2, 3, 2],
...     index=pd.MultiIndex.from_tuples([(""a"", ""x""), (""a"", ""y""), (""b"", ""z""), (""c"", ""z"")]),
...     name=""a"",
... )

>>> pser
a  x    1
   y    2
b  z    3
c  z    2
Name: a, dtype: int64
```

Then, when I use `GroupByRolling`, In the version of pandas <= 1.0.5 shows result as below.
(Let's focus on the level of `MultiIndex` in the examples below)

```python
>>> pser.groupby(pser).rolling(2).max()
a
1  a  x    NaN
2  a  y    NaN
   c  z    2.0
3  b  z    NaN
Name: a, dtype: float64

>>> pser.groupby(pser).rolling(2).max().index
MultiIndex([(1, 'a', 'x'),
            (2, 'a', 'y'),
            (2, 'c', 'z'),
            (3, 'b', 'z')],
           names=['a', None, None])

>>> pser.groupby(pser).rolling(2).max().index.nlevels  # It keeps the index level.
3
```

However, In the pandas 1.1.0, the result seems different from the previous version as below.
(The level of `MultiIndex` is decreased after performing)

```python
>>> pser.groupby(pser).rolling(2).max()
a
1  (a, x)    NaN
2  (a, y)    NaN
   (c, z)    2.0
3  (b, z)    NaN
Name: a, dtype: float64

>>> pser.groupby(pser).rolling(2).max().index
MultiIndex([(1, ('a', 'x')),
            (2, ('a', 'y')),
            (2, ('c', 'z')),
            (3, ('b', 'z'))],
           names=['a', None])

>>> pser.groupby(pser).rolling(2).max().index.nlevels  # index level changed from 3 to 2
2
```

Is it intended behavior in pandas 1.1.1. ??

Thanks :)"
239031639,16784,DataFrame.replace: TypeError: Cannot compare types 'ndarray(dtype=int64)' and 'unicode',eromoe,closed,2017-06-28T01:58:47Z,2020-09-07T21:11:29Z,"#### Code Sample, a copy-pastable example if possible

```
path1 = '/some.xls'
df1 = pd.read_excel(path1)
columns_values_map={
    'positive': {
        '正面':1,
        '中立': 1,
        '负面':0
    }
}

df1.replace(columns_values_map)
```

#### Problem description

got error: `TypeError: Cannot compare types 'ndarray(dtype=int64)' and 'unicode'`

Actually `df1['positive']` only has value in (0, 1) , but I think it should not throw exception here.
"
695363800,36202,TST: DataFrame.replace: TypeError: Cannot compare types 'ndarray(dtype=int64)' and 'unicode',phofl,closed,2020-09-07T19:54:48Z,2020-09-07T21:13:36Z,"- [x] closes #16784
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Thanks @jbrockmendel for fixing my related issue"
694204424,36152,Fix compressed multiindex for output of groupby.rolling,phofl,closed,2020-09-05T23:16:51Z,2020-09-07T21:14:34Z,"- [x] closes #36018 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The number of leves for MultiIndex in input was compressed to one, because the tuples representing the Index rows were not unpacked. Also name was used instead of names -> resulted in missing name for every MultiIndex."
694530585,36173,REF: collect methods by topic,jbrockmendel,closed,2020-09-06T21:13:10Z,2020-09-07T21:47:37Z,"
"
694516782,36171,DTA/TDA/PA use self._data instead of self.asi8 for self._ndarray,jbrockmendel,closed,2020-09-06T20:26:35Z,2020-09-07T21:51:02Z,"This in turn will let us share more methods with the parent class, at the cost of re-shuffling where we do some of the view-casting."
695393373,36203,Backport PR #36152 on branch 1.1.x (Fix compressed multiindex for output of groupby.rolling),meeseeksmachine,closed,2020-09-07T21:13:12Z,2020-09-07T22:10:03Z,Backport PR #36152: Fix compressed multiindex for output of groupby.rolling
437816837,26218,"read_csv() crashes if engine='c', header=None, and 2+ extra columns",dargueta,closed,2019-04-26T19:37:34Z,2020-09-07T23:32:50Z,"#### Code Sample, a copy-pastable example if possible

```python
import io
import pandas as pd

# Create a CSV with a single row:
stream = io.StringIO(u'foo,bar,baz,bam,blah')

# Succeeds, returns a DataFrame with four columns and ignores the fifth.
pd.read_csv(
    stream,
    header=None,
    names=['one', 'two', 'three', 'four'],
    index_col=False,
    engine='c'
)

# Change the engine to 'python' and you get the same result.
stream.seek(0)
pd.read_csv(
    stream,
    header=None,
    names=['one', 'two', 'three', 'four'],
    index_col=False,
    engine='python'
)

# Succeeds, returns a DataFrame with three columns and ignores the extra two.
stream.seek(0)
pd.read_csv(
    stream,
    header=None,
    names=['one', 'two', 'three'],
    index_col=False,
    engine='python'
)


# Change the engine to 'c' and it crashes:
stream.seek(0)
pd.read_csv(
    stream,
    header=None,
    names=['one', 'two', 'three'],
    index_col=False,
    engine='c'
)
```

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/tux/.pyenv/versions/2.7.15/envs/gds/lib/python2.7/site-packages/pandas/io/parsers.py"", line 702, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""/Users/tux/.pyenv/versions/2.7.15/envs/gds/lib/python2.7/site-packages/pandas/io/parsers.py"", line 435, in _read
    data = parser.read(nrows)
  File ""/Users/tux/.pyenv/versions/2.7.15/envs/gds/lib/python2.7/site-packages/pandas/io/parsers.py"", line 1139, in read
    ret = self._engine.read(nrows)
  File ""/Users/tux/.pyenv/versions/2.7.15/envs/gds/lib/python2.7/site-packages/pandas/io/parsers.py"", line 1995, in read
    data = self._reader.read(nrows)
  File ""pandas/_libs/parsers.pyx"", line 899, in pandas._libs.parsers.TextReader.read
  File ""pandas/_libs/parsers.pyx"", line 914, in pandas._libs.parsers.TextReader._read_low_memory
  File ""pandas/_libs/parsers.pyx"", line 991, in pandas._libs.parsers.TextReader._read_rows
  File ""pandas/_libs/parsers.pyx"", line 1067, in pandas._libs.parsers.TextReader._convert_column_data
  File ""pandas/_libs/parsers.pyx"", line 1387, in pandas._libs.parsers.TextReader._get_column_name
IndexError: list index out of range
```

#### Problem description

The normal behavior for `read_csv()` is to ignore extra columns if it's given `names`. However, if the CSV has _two_ or more extra columns and the `engine` is `c` then it crashes. The exact same CSV can be read correctly if `engine` is `python`.

#### Expected Output

Behavior of reading a CSV with the C and Python engines should be identical.

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.15.final.0
python-bits: 64
OS: Darwin
OS-release: 17.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.24.2
pytest: 4.3.1
pip: 19.0.3
setuptools: 41.0.1
Cython: 0.29.5
numpy: 1.14.5
scipy: 1.1.0
pyarrow: 0.10.0
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2016.6.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: 2.5.14
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: 1.0.15
pymysql: None
psycopg2: 2.7.3.2 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: 0.1.6
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: 0.2.1
```

I tested this on Python 3.7.3 as well but for some reason `pd.show_versions()` keeps blowing up with:

```
Assertion failed: (PassInf && ""Expected all immutable passes to be initialized""), function addImmutablePass, file /Users/buildbot/miniconda3/conda-bld/llvmdev_1545076115094/work/lib/IR/LegacyPassManager.cpp, line 812.
Abort trap: 6 (core dumped)
```

</details>"
694636693,36180,REF: implement Categorical._validate_setitem_value,jbrockmendel,closed,2020-09-07T02:28:46Z,2020-09-08T00:12:46Z,Moving towards sharing more methods
529381538,29884,NumPy 1.18 behavior for sorting NaT,TomAugspurger,closed,2019-11-27T14:28:59Z,2020-09-08T00:15:51Z,"Followup to https://github.com/pandas-dev/pandas/pull/29877. cc @jbrockmendel if you could fill out the details.

* Adopt NaT at the end for Index? (we already do for Series)
* Report timedelta NaT-sorting issue to NumPy?"
694565485,36176,COMPAT: match numpy behavior for searchsorted on dt64/td64,jbrockmendel,closed,2020-09-06T23:14:17Z,2020-09-08T00:17:02Z,"- [x] closes #29884
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Should whatsnew be considered a bug or breaking change?"
694925934,36184,"DOC: Insert ""pandas"" into json_normalize examples",joseberlines,closed,2020-09-07T09:49:50Z,2020-09-08T00:30:20Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html

#### Documentation problem

It seems that in the examples cells the json_normalize is not preceded by pandas. (i.e. pandas dot)
This does not allow to simply copy and paste the examples.

#### Suggested fix for documentation

Just insert "" pandas. "" preceding the json_normalize() method in the example cells where it is missing."
695238601,36194,pandas docs json_normalize example,nzare,closed,2020-09-07T16:00:38Z,2020-09-08T03:37:02Z,"- [x] closes #36184 
- [x] Inserted "" pandas. "" preceding the json_normalize() method in the example cells where it was missing.
Documentation Location : [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html)"
681131216,35790,QST: need to refactor pandas/io/formats/latex.py?,ivanovmg,closed,2020-08-18T15:25:44Z,2020-09-08T07:01:19Z,"#### Refactoring

Recently ``pandas/io/formats/latex.py`` undertook some updates and extension of it functionality.
Adding more into it led to some code smell.

Recently there was a refactoring done by @SylvainLan, PR https://github.com/pandas-dev/pandas/pull/35649.
Even after that I see some improvements that can be done.
1. Split the process of writing the table into the beginning of the environment, the body and end of the environment.
2. Implement polymorphism to smoothly handle transition from regular table to longtable (currently there are multiple ifs here and there).

I made some changes in the local branch (not in sync with master because of the recent refactoring).
See https://github.com/ivanovmg/pandas/tree/refactor/latex-formatting for the preliminary solution.

Would you be interested in following this direction?
If so, then I would make it in sync with master and continue improvements.
"
695334022,36197,BUG: DataFrameGroupBy.rolling on an empty DataFrame throws a ValueError,rbu,closed,2020-09-07T18:39:53Z,2020-09-08T09:51:21Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---
#### Code Sample, a copy-pastable example

```python
import pandas as pd
pd.DataFrame({""s1"": []}).groupby(""s1"").rolling(window=1).sum()
```

#### Problem description

Since a pandas update, the groupby above started failing in the edge case of an empty DataFrame. It used to simply return an empty DataFrame before, which seems like a more robust behaviour to me.

#### Expected Output
```
>>> pd.DataFrame({""s1"": []}).groupby(""s1"").rolling(window=1).sum()
Empty DataFrame
Columns: []
Index: []
```

#### Actual Output
```
>>> pd.DataFrame({""s1"": []}).groupby(""s1"").rolling(window=1).sum()'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""site-packages/pandas/core/window/rolling.py"", line 2072, in sum
    return super().sum(*args, **kwargs)
  File ""site-packages/pandas/core/window/rolling.py"", line 1424, in sum
    window_func, center=self.center, floor=0, name=""sum"", **kwargs
  File ""site-packages/pandas/core/window/rolling.py"", line 2194, in _apply
    **kwargs,
  File ""site-packages/pandas/core/window/rolling.py"", line 528, in _apply
    blocks, obj = self._create_blocks(self._selected_obj)
  File ""site-packages/pandas/core/window/rolling.py"", line 2230, in _create_blocks
    list(self._groupby.grouper.indices.values())
  File ""<__array_function__ internals>"", line 6, in concatenate
ValueError: need at least one array to concatenate
```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.7.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.7.17-200.fc32.x86_64
Version          : #1 SMP Fri Aug 21 15:23:46 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.7.3
pip              : 19.3.1
setuptools       : 42.0.2
Cython           : None
pytest           : 5.1.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.3.3
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.3.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.0
fastparquet      : None
gcsfs            : None
matplotlib       : 3.0.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 0.12.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
695410876,36205,DOC: doc fix,simonjayhawkins,closed,2020-09-07T22:06:54Z,2020-09-08T10:23:17Z,https://github.com/pandas-dev/pandas/issues/35831#issuecomment-688518465
695727725,36213,Backport PR #36208 on branch 1.1.x (BUG: GroupbyRolling with an empty frame),meeseeksmachine,closed,2020-09-08T09:53:16Z,2020-09-08T10:49:23Z,Backport PR #36208: BUG: GroupbyRolling with an empty frame
695749762,36215,Backport PR #36205 on branch 1.1.x (DOC: doc fix),meeseeksmachine,closed,2020-09-08T10:22:30Z,2020-09-08T11:05:52Z,Backport PR #36205: DOC: doc fix
695741854,36214,ENH: add sep argument to DataFrame/Series.explode,erfannariman,closed,2020-09-08T10:09:42Z,2020-09-08T12:32:39Z,"Right now both `DataFrame.explode` and `Series.explode` will unnest a list to rows. But most of the times the origin of the problem is that we have a concatenated string with a seperator. So the solution is to overwrite the column while splitting it to a list and then calling `DataFrame.explode`:
```python
df = pd.DataFrame({
    ""a"":[""string1;string2;string3"", ""string4;string5""]
})
                         a
0  string1;string2;string3
1          string4;string5


df.assign(a=df[""a""].str.split("";"")).explode(""a"")
         a
0  string1
0  string2
0  string3
1  string4
1  string5
```
What would be more convenient is to have a `sep`  argument which then checks if the column dtype is ""object""  or  ""string"" and does the splitting itself:

```
df.explode(""a"", sep="";"")

         a
0  string1
0  string2
0  string3
1  string4
1  string5
```
Note: if argument `sep` is given and the column values is list-like, it should raise."
695803237,36217,Backport PR #36182 on branch 1.1.x (DOC: release date for 1.1.2),meeseeksmachine,closed,2020-09-08T11:50:25Z,2020-09-08T12:34:57Z,Backport PR #36182: DOC: release date for 1.1.2
695820319,36219,QST:Column deletion,chipxx,closed,2020-09-08T12:17:33Z,2020-09-08T12:45:23Z,"Hi, how do I delete the first column that is useless? Thanks

![pandas](https://user-images.githubusercontent.com/43586762/92475574-e927fd80-f1dd-11ea-9b76-bbb8a0763839.png)
"
555837443,31368,Index name assignment resolution logic changes depending on whether DataFrame has any rows,tlaytongoogle,closed,2020-01-27T21:15:57Z,2020-09-08T13:45:09Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

def get_name_of_assigned_index(df_size, series_size):
  df = pd.DataFrame({}, index=pd.RangeIndex(df_size, name='df_index'))
  series = pd.Series(1.23, index=pd.RangeIndex(series_size, name='series_index'))

  df['series'] = series

  return df.index.name
```
#### Problem description

`get_name_of_assigned_index(n, m)` returns `'df_index'` *unless* `n = 0`, in which case it returns `'series_index'`.

I don't know which of these is necessarily the better behavior, but the resulting index name ought to be consistent regardless of how many - or few - rows are in the DataFrame. Subsequent operations may rely on expectations about that name, so this behavior, wherein the name changes as an unintuitive consequence of a rowless frame, could cause bugs.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.7.final.0
python-bits: 64
OS: Linux
OS-release: 4.3.5-smp-821.23.0.0
machine: x86_64
processor: 
byteorder: little
LC_ALL: en_US.UTF-8
LANG: None
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: None
pip: None
setuptools: unknown
Cython: None
numpy: 1.16.4
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 2.0.0
sphinx: None
patsy: 0.4.1
dateutil: 2.8.1
pytz: 2019.3
blosc: None
bottleneck: None
tables: 3.5.2
numexpr: 2.6.10dev0
feather: None
matplotlib: 3.0.3
openpyxl: None
xlrd: 1.2.0
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.3
s3fs: None
fastparquet: None
pandas_gbq: 0+unknown
pandas_datareader: None
gcsfs: None

</details>
"
695875261,36220,TST: skip test_groupby_rolling_index_changed on 32bit,simonjayhawkins,closed,2020-09-08T13:32:35Z,2020-09-08T14:22:22Z,this is against 1.1.x directly. see https://github.com/pandas-dev/pandas/issues/35831#issuecomment-688862692
695903045,36221,Backport PR #36141 on branch 1.1.x (BUG: copying series into empty dataframe does not preserve dataframe index name),meeseeksmachine,closed,2020-09-08T14:06:41Z,2020-09-08T14:55:08Z,Backport PR #36141: BUG: copying series into empty dataframe does not preserve dataframe index name
695906761,36222,CLN remove trailing commas,tiagohonorato,closed,2020-09-08T14:10:59Z,2020-09-08T15:28:25Z,"#35925

From this set of 10 files, only the first 3 needed changes to comply if the new version of black:
- [x] pandas/tests/io/pytables/test_timezones.py
- [x] pandas/tests/io/test_feather.py
- [x] pandas/tests/io/test_s3.py
- [ ] pandas/tests/io/test_sql.py
- [ ] pandas/tests/io/test_stata.py
- [ ] pandas/tests/plotting/test_frame.py
- [ ] pandas/tests/resample/test_datetime_index.py
- [ ] pandas/tests/reshape/merge/test_merge_index_as_string.py
- [ ] pandas/tests/reshape/test_crosstab.py
- [ ] pandas/tests/reshape/test_get_dummies.py"
555808558,31364,"to_numeric with errors = ""coerce"" is adding digits at the end",jjotterson,closed,2020-01-27T20:18:27Z,2020-09-08T15:30:36Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
#problem: pandas to_numeric might give some errors when using coerce 
# it is adding digits at the end.

import pandas as pd


#minimal example
data = [{'value': '.'}, {'value': '.'}, {'value': '.'}, {'value': '.'}, {'value': '243.164'}, {'value': '245.968'}, {'value': '249.585'}, {'value': '259.745'}, {'value': '265.742'}, {'value': '272.567'}]
df = pd.DataFrame(data,columns=['value'])

df.value = pd.to_numeric(df.value,errors='coerce')

#looks as if all is good:
df.value 

#but
df.value[4]


#this can be random:
data2 = [{'value': '.'}, {'value': '.'}, {'value': '.'}, {'value': '.'}, {'value': '243.164'}, {'value': '245.968'}, {'value': '249.585'}, {'value': '259.745'}, {'value': '265.742'}, {'value': '272.567'}, {'value': '279.196'}, {'value': '280.366'}, {'value': '275.034'}, {'value': '271.351'}, {'value': '272.889'}, {'value': '270.627'}, {'value': '280.828'}, {'value': '290.383'}, {'value': '308.153'}, {'value': '319.945'}, {'value': '336.0'}, {'value': '344.09'}, {'value': '351.385'}, {'value': '356.178'}, {'value': '359.82'}, {'value': '361.03'}, {'value': '367.701'}, {'value': '380.812'}, {'value': '387.98'}, {'value': '391.749'}, {'value': '391.171'}, {'value': '385.97'}, {'value': '385.345'}, {'value': '386.121'}, {'value': '390.996'}, {'value': '399.734'}, {'value': '413.073'}, {'value': '421.532'}, {'value':
'430.221'}, {'value': '437.092'}, {'value': '439.746'}, {'value': '446.01'}, {'value': '451.191'}, {'value': '460.463'}, {'value': '469.779'}, {'value': '472.025'}, {'value': '479.49'}, {'value': '474.864'}, {'value': '467.54'}, {'value': '471.978'}]


#now 4, 36, and 47 are wrong, with different endings.
df2 = pd.DataFrame(data2,columns=['value'])

df2.value = pd.to_numeric(df2.value,errors='coerce')


df2.value[[4,36,47]].to_list()
```
#### Problem description

Current behavior:  pandas to_numeric, when using errors='coerce' seem to randomly add 
decimals at the end of the number. 

#### Expected Output

no decimals should be added at the end.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 85 Stepping 4, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : 0.29.12
pytest           : 5.0.1
hypothesis       : None
sphinx           : 2.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.3.4
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.6.1
pandas_datareader: 0.7.4
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.1.2
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : 1.3.5
tables           : 3.5.2
xarray           : 0.12.3
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8
</details>
"
695305629,36196,CLN: remove unused return value in _create_blocks,jbrockmendel,closed,2020-09-07T17:38:01Z,2020-09-08T15:31:24Z,The blocks are made unnecessary by the recent changes to use BlockManager.apply
292757790,19463,ENH: Support high precision converters in to_numeric,xmduhan,closed,2018-01-30T12:02:19Z,2020-09-08T15:58:34Z,"### 

```python
import pandas as pd
from pandas.compat import StringIO
data = StringIO(""""""
a, b
1, 1
1.7976931348623157e+308, 1
"""""")
pd.read_csv(data, dtype={'a': float, 'b': float}, engine='python')
```
This work!

```python
import pandas as pd
from pandas.compat import StringIO
data = StringIO(""""""
a, b
1, 1
1.7976931348623157e+308, 1
"""""")
pd.read_csv(data, dtype={'a': float, 'b': float}, engine='c')
```
This fail!
Message: ValueError: cannot safely convert passed user dtype of float64 for object dtyped data in column 0


### Full error stack:

```
ValueErrorTraceback (most recent call last)
<ipython-input-4-f830b6441439> in <module>()
      5 1.7976931348623157e+308, 1
      6 """""")
----> 7 pd.read_csv(data, dtype={'a': float, 'b': float}, engine='c')

/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)
    707                     skip_blank_lines=skip_blank_lines)
    708 
--> 709         return _read(filepath_or_buffer, kwds)
    710 
    711     parser_f.__name__ = name

/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc in _read(filepath_or_buffer, kwds)
    453 
    454     try:
--> 455         data = parser.read(nrows)
    456     finally:
    457         parser.close()

/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc in read(self, nrows)
   1067                 raise ValueError('skipfooter not supported for iteration')
   1068 
-> 1069         ret = self._engine.read(nrows)
   1070 
   1071         if self.options.get('as_recarray'):

/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc in read(self, nrows)
   1837     def read(self, nrows=None):
   1838         try:
-> 1839             data = self._reader.read(nrows)
   1840         except StopIteration:
   1841             if self._first_chunk:

pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.read()

pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._read_low_memory()

pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._read_rows()

pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._convert_column_data()

pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._convert_tokens()

ValueError: cannot safely convert passed user dtype of float64 for object dtyped data in column 0

```
<details>
### pd.show_versions():
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.6.final.0
python-bits: 64
OS: Linux
OS-release: 3.13.0-139-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: zh_CN.UTF-8
LOCALE: None.None

pandas: 0.22.0
pytest: None
pip: 9.0.1
setuptools: 38.4.0
Cython: 0.27.3
numpy: 1.14.0
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 5.5.0
sphinx: 1.6.6
patsy: 0.5.0
dateutil: 2.6.1
pytz: 2017.3
blosc: None
bottleneck: None
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 2.1.2
openpyxl: None
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: None
lxml: None
bs4: 4.6.0
html5lib: 0.9999999
sqlalchemy: 1.2.2
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.6.0
</details>

"
492491957,28394,Optionally disallow duplicate labels,TomAugspurger,closed,2019-09-11T22:13:09Z,2020-09-08T16:44:37Z,"This adds a property to NDFrame to disallow duplicate labels. This fixes a vexing issue with using pandas for ETL pipelines, where accidentally introducing duplicate labels can lead to confusing downstream behavior (e.g. `NDFrame.__getitem__` not reducing dimensionality).

When set (via the construction with `allow_duplicate_labels=False` or afterward via `.allows_duplicate_labels=False`), the presence of duplicate labels causes a DuplicateLabelError exception to be raised:

```pytb
In [3]: df = pd.DataFrame({""A"": [1, 2]}, index=['a', 'a'], allow_duplicate_labels=False)
---------------------------------------------------------------------------
DuplicateLabelError                       Traceback (most recent call last)
<ipython-input-3-1c8833763dfc> in <module>
----> 1 df = pd.DataFrame({""A"": [1, 2]}, index=['a', 'a'], allow_duplicate_labels=False)

~/sandbox/pandas/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy, allow_duplicate_labels)
    493
    494         NDFrame.__init__(
--> 495             self, mgr, fastpath=True, allow_duplicate_labels=allow_duplicate_labels
    496         )
    497

~/sandbox/pandas/pandas/core/generic.py in __init__(self, data, axes, copy, dtype, allow_duplicate_labels, fastpath)
    202
    203         if not self.allows_duplicate_labels:
--> 204             self._maybe_check_duplicate_labels()
    205
    206     def _init_mgr(self, mgr, axes=None, dtype=None, copy=False):

~/sandbox/pandas/pandas/core/generic.py in _maybe_check_duplicate_labels(self, force)
    240         if force or not self.allows_duplicate_labels:
    241             for ax in self.axes:
--> 242                 ax._maybe_check_unique()
    243
    244     @property

~/sandbox/pandas/pandas/core/indexes/base.py in _maybe_check_unique(self)
    540             # TODO: position, value, not too large.
    541             msg = ""Index has duplicates.""
--> 542             raise DuplicateLabelError(msg)
    543
    544     # --------------------------------------------------------------------

DuplicateLabelError: Index has duplicates.
```

This property is preserved through operations (using `_metadata` and `__finalize__`).

```pytb
In [4]: df = pd.DataFrame(index=['a', 'A'], allow_duplicate_labels=False)

In [5]: df.rename(str.upper)
---------------------------------------------------------------------------
DuplicateLabelError                       Traceback (most recent call last)
<ipython-input-5-17c8fb0b7c7f> in <module>
----> 1 df.rename(str.upper)
...

~/sandbox/pandas/pandas/core/indexes/base.py in _maybe_check_unique(self)
    540             # TODO: position, value, not too large.
    541             msg = ""Index has duplicates.""
--> 542             raise DuplicateLabelError(msg)
    543
    544     # --------------------------------------------------------------------

DuplicateLabelError: Index has duplicates.

```

---

### API design questions

- [ ] Do we want to be positive or negative?

```python
pd.Series(..., allow_duplicate_labels=True/False)
pd.Series(..., disallow_duplicate_labels=False/True)
pd.Series(..., require_unique_labels=False/True)
```

- [ ] In my proposal, the argument is different from the property. Do we like that? The rational is the argument is making a statement about what *to* do, while the property is making a statement about what *is* allowed.
```python
In [7]: s = pd.Series(allow_duplicate_labels=False)

In [8]: s.allows_duplicate_labels
Out[8]: False
```

- [ ] I'd like a method-chainable way of saying ""duplicates aren't allowed."" Some options

```python
# two methods.
s.disallow_duplicate().allow_duplicate()
```

I dislike that in combination with `.allows_duplicates`, since we'd have a property and a method that only differ by an `s`. Perhaps something like

```python
s.update_metdata(allows_duplicates=False)
```

But do people know that ""allows_duplicates"" is considered metadata?

---

TODO:

- [ ] many, many more tests
- [ ] Propagate metadata in more places (groupby, rolling, etc.)
- [ ] Backwards compatibility in `__finalize__` (we may be changing when we call it, and what we pass)

*Apologies for using the PR for discussion, but we needed a way to compare this to https://github.com/pandas-dev/pandas/pull/28334.*"
696085219,36225,DOC: whatsnew for Pandas release 1.1.2,engineerchange,closed,2020-09-08T18:25:34Z,2020-09-08T19:29:10Z,"#### Location of the documentation

https://github.com/pandas-dev/pandas/releases/tag/v1.1.2

#### Documentation problem

The ""what's new"" link doesn't appear to function for update 1.1.2.
"
694579458,36178,STY: de-privatize names imported across modules,jbrockmendel,closed,2020-09-07T00:02:21Z,2020-09-08T23:04:44Z,"getting close to the end of this and enabling a code_check
"
696180771,36229,CLN: re-use invalid_comparison in Categorical comparisons,jbrockmendel,closed,2020-09-08T21:12:00Z,2020-09-08T23:10:03Z,
695463419,36206,"REF: implement Categorical._box_func, make _box_func a method",jbrockmendel,closed,2020-09-08T00:50:57Z,2020-09-08T23:10:51Z,"Defining `_box_func` on `Categorical` is to facilitate more code sharing with datetimelike.  Also there are a bunch of places that do something _similar_ but slightly different that I want to standardize within `Categorical`.

Changing `_box_func` from a property-lambda to a method on the datetimelike arrays gives a slight perf boost:

```
In [1]: import pandas as pd                                                     
In [2]: dti = pd.date_range(""2016-01-01"", periods=3)                 
           
In [3]: %timeit dti[1]                                                          
8.73 µs ± 50.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)  # <-- master
8.08 µs ± 119 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)   # <-- PR

```
"
695483220,36207,Fixed pandas.json_normalize doctests errors,ylin00,closed,2020-09-08T01:52:16Z,2020-09-08T23:15:16Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] Changed `pandas.` to `pd.` in the docstring of `pandas.json_normalize`. Fixed the expected results.
"
678441584,35706,BUG: pandas dataframe style render produces incorrect html with multiindex,semnooij,closed,2020-08-13T13:34:04Z,2020-09-08T23:56:47Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd
import datetime

df = pd.DataFrame(
    data={
        ('level0', 'level1a',): [1234565.23424],
        ('level0', 'level1b',): [1234565.23424],
        ('level0', 'level1c',): [1234565.23424],
        ('level0', 'level1d',): [1234565.23424],
        ('level0', 'level1e',): [1234565.23424],
    },
    columns=pd.MultiIndex.from_tuples(
        [
            ('level0', 'level1a',),
            ('level0', 'level1b',),
            ('level0', 'level1c',),
            ('level0', 'level1d',),
            ('level0', 'level1e',),
        ]
    ),
    index=['test']
)

df.style.format('{:,.2f}').render()
```

#### Problem description
The render() of the styler produces incorrect html in case the columns are a MultiIndex.

#### Expected Output
re.sub(r'colspan=(\d)', r'colspan=""\1""', df.style.format('{:,.2f}').render())

The current implementation returns something like
<th class=""col_heading level0 col0"" colspan=5> while it should be
<th class=""col_heading level0 col0"" colspan=""5"">

See also https://www.w3schools.com/tags/att_td_colspan.asp

Same goes for MultiIndex indices where it concerns rowspan instead of colspan.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.6.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.111-1.el7.centos.x86_64
Version          : #1 SMP Wed Apr 17 17:45:41 CEST 2019
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 9.0.1
setuptools       : 28.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.14.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
696257148,36234,REF: pass setitem to unbox_scalar to de-duplicate validation,jbrockmendel,closed,2020-09-08T23:34:30Z,2020-09-09T00:58:12Z,Also fixes a test that is failing on master because _np_version_under_118 is no longer defined.
572269598,32304,BUG: Fix __ne__ comparison for Categorical,dsaxton,closed,2020-02-27T18:25:01Z,2020-09-09T01:01:00Z,"- [x] closes #32276
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
696258376,36235,STY: De-privatize imported names,jbrockmendel,closed,2020-09-08T23:38:09Z,2020-09-09T01:58:31Z,Getting close to the end on these.
696004956,36223,CLN: w3 formatting,attack68,closed,2020-09-08T16:14:42Z,2020-09-09T05:11:04Z,"- [x] closes #35706
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Cleans a very minor formatting error to be consistent with w3schools specification.

`colspan=2   --->  colspan=""2""`

Note this generally has no effect in browsers parsing HTML."
694894313,36183,DOC: Start 1.1.3,simonjayhawkins,closed,2020-09-07T09:10:51Z,2020-09-09T08:37:11Z,pr for tomorrow
694888000,36182,DOC: release date for 1.1.2,simonjayhawkins,closed,2020-09-07T09:03:18Z,2020-09-09T08:46:02Z,pr for tomorrow
683084189,35831,RLS: 1.1.2,simonjayhawkins,closed,2020-08-20T20:43:50Z,2020-09-09T09:07:11Z,"[![Manual Release](https://github.com/simonjayhawkins/pandas-release/workflows/Manual%20Release/badge.svg?branch=simonjayhawkins-patch-2)](https://github.com/simonjayhawkins/pandas-release/actions?query=workflow%3A%22Manual+Release%22)

Tracking issue for the 1.1.2 release.

https://github.com/pandas-dev/pandas/milestone/76

Please do not remove/change milestones from these issues without a note explaining the reasoning (changing milestones doesn't trigger notification)

During issue triage, regressions from 1.0.5 or 1.1.0  to 1.1.0 or 1.1.1 should be milestoned 1.1.2 in the first instance.
"
696591866,36242,Backport PR #36183 on branch 1.1.x (DOC: Start 1.1.3),meeseeksmachine,closed,2020-09-09T08:32:45Z,2020-09-09T09:28:47Z,Backport PR #36183: DOC: Start 1.1.3
561703265,31780,"Groupby get_group is not consistent - sometimes it returns the grouping columns, sometimes it doesn't",giuliobeseghi,closed,2020-02-07T15:22:36Z,2020-09-09T12:37:30Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

df = pd.DataFrame(
    {""a"": range(50), ""b"": range(0, 100, 2), ""c"": list(range(10)) * 5}
)

grouper = df.groupby(""c"")
grouper.get_group(0)
```
#### Problem description

This is quite weird and unfortunately I can't replicate it. The previous piece of code randomly returns one of the two options:

1)
|    |   a |   b |   c |
|---:|----:|----:|----:|
|  0 |   0 |   0 |   0 |
| 10 |  10 |  20 |   0 |
| 20 |  20 |  40 |   0 |
| 30 |  30 |  60 |   0 |
| 40 |  40 |  80 |   0 |

2)
|    |   a |   b |
|---:|----:|----:|
|  0 |   0 |   0 |
| 10 |  10 |  20 |
| 20 |  20 |  40 |
| 30 |  30 |  60 |
| 40 |  40 |  80 |

If I run it now, I get option 1 (which I assume it's the expected one, since `Groupby.get_group` doesn't say anything about dropping the column used in the groupby https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.get_group.html)

However, a few minutes earlier I ran this
```python
[grouper.get_group(i).do_something_with_c_column() for i in grouper.groups.keys()]
```

And it raised a `KeyError`. Which is the right behavior? Can anyone else replicate this?

#### Output of ``pd.show_versions()``

<details>

numexpr.utils - INFO - NumExpr defaulting to 8 threads.

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : 0.29.14
pytest           : 5.3.5
hypothesis       : 5.4.1
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.3.2
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : 0.8.3
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.48.0

</details>
"
695222137,36193,CLN: remove unnecessary trailing commas on issues #35925,satrio-hw,closed,2020-09-07T15:40:32Z,2020-09-09T13:24:19Z,"#35925

- pandas/tests/arrays/categorical/test_replace.py
- pandas/tests/arrays/sparse/test_array.py
- pandas/tests/arrays/test_array.py
- pandas/tests/arrays/test_timedeltas.py
- pandas/tests/base/test_conversion.py
- pandas/tests/dtypes/test_missing.py
- pandas/tests/extension/base/methods.py
- pandas/tests/extension/test_sparse.py
- pandas/tests/frame/indexing/test_setitem.py

please give me a feedback if there is something that I have to change.. thank you before"
696888248,36246,DOC: Fixed a broken JSON Table Schema link,danchev,closed,2020-09-09T14:57:05Z,2020-09-09T15:12:12Z,"Fixed a link to JSON Table Schema specification hosted on https://specs.frictionlessdata.io/table-schema/

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
692326111,36103,QST: Why does Pandas change data type from object to float after reading 32768 rows from CSV file?,dwervin,closed,2020-09-03T20:36:50Z,2020-09-09T20:40:24Z,"- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [x] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas
Using Pandas 1.0.3.

I have a CSV with 43,00 rows. I noticed that one of the columns is object type even though it has floats.  It looks like this happened because a few records contain a string that reads 'Bad Input'.  However, after 32768 rows, the data becomes floats.  I found it strange that this happens after exactly 32K rows.

I've used Pandas for a few years now, but I've never seen an inconsistency like this.

Is this a bug?  How do I work around this?


**Note**: If you'd still like to submit a question, please read [this guide](
https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your question.

```python
# Your code here, if applicable

```
"
697156099,36257,"BUG: Series.get() does not return default value in case one of slice(n,m) is not found",phofl,open,2020-09-09T21:02:38Z,2020-09-09T21:03:13Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
ds = pd.Series(['a', 'b', 'c'])
print(ds.get([1,-1]))
print(ds.get(slice(-1,2)))

```

#### Problem description

The first get call returns the default value None as expected. The second returns an empty series. This seems inconsistent. I propose that we return here None too.

#### Expected Output

None and None

#### Output of ``pd.show_versions()``

<details>

master

</details>
"
696846923,36244,DOC: Update numeric.py,Nikhil1O1,closed,2020-09-09T14:07:12Z,2020-09-09T22:22:24Z,"changed set to list as it is more appropriate

- [x] closes #36239 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
696372190,36239,DOC: Improve Index docstrings,Nikhil1O1,closed,2020-09-09T03:33:22Z,2020-09-09T22:25:46Z,"Index is now described as List instead of Set

- [x] closes #36170
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
677066221,35675,Fix cython3,tacaswell,closed,2020-08-11T17:40:20Z,2020-09-09T22:57:57Z,"I'm not sure if this actually works, but it gets cython to compile with py310.

- [x] closes #34014
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
544416237,30610,DEPR: Remove pandas.datetime,ryankarlos,closed,2020-01-02T00:36:40Z,2020-09-09T23:04:43Z,"A follow up issue from #30296 , where it was discussed that pd.datetime should also be deprecated to only allow imports directly from datetime

```
>>> import pandas as pd
>>> pd.datetime
<class 'datetime.datetime'>
```"
694365180,36160,CLN: _wrap_applied_output,rhshadrach,closed,2020-09-06T12:34:00Z,2020-09-10T00:07:20Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

All if statements above the changed block return, so we can remove the else's and flatten out the structure a bit."
694153234,36146,CLN: Separate transform tests,rhshadrach,closed,2020-09-05T19:29:24Z,2020-09-10T00:07:40Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Setup for #35964. There were two tests that are accidental duplicates; I've just copied them over to the transform modules. Will cleanup in #35964."
665651687,35412,CLN: _wrap_applied_output,rhshadrach,closed,2020-07-25T20:03:20Z,2020-09-10T00:07:45Z,"The majority of this change is the result of three operations:

1. Move code that returns up top, to avoid the heavily nested structure.
2. Move variable creation as close as possible to where they are used. Previously, in certain cases computations were being done and then going unused.
3. Combine duplicated code.

There are two sections that I was able to entirely remove:

````
ping = self.grouper.groupings[0]
if len(keys) == ping.ngroups:
    key_index = ping.group_index
    key_index.name = key_names[0]

    key_lookup = Index(keys)
    indexer = key_lookup.get_indexer(key_index)

    # reorder the values
    values = [values[i] for i in indexer]

    # update due to the potential reorder
    first_not_none = next(com.not_none(*values), None)
````

and

````
# GH5788 instead of stacking; concat gets the
# dtypes correct
from pandas.core.reshape.concat import concat

result = concat(
    values,
    keys=key_index,
    names=key_index.names,
    axis=self.axis,
).unstack()
result.columns = index
````

For the single test that I touched, it was checking that the categorical dtype of an index was being dropped after a groupby. I don't believe that is the correct behavior - that the categorical dtype should remain. I checked groupby/categorical bugs and didn't find any issues this closes."
697075386,36253,New feature/enhancement: it would be cool if read_csv() function can read n random lines from a csv file.,wenjunsun,closed,2020-09-09T19:06:11Z,2020-09-10T00:07:46Z,"#### Problem:

I am using Pandas for reading in data to do machine learning tasks. And one problem I encounter is that the data I have is usually very very big. For example one data set I have is about 16 GB big, and my RAM is only 12 GB. So instead of reading all the lines from this data, I know I can read in the first million rows from my data, and do machine learning on that. But what I really want to do is select 100 random lines from the entire file for it to be a valid sample from the 16 GB data. Pandas currently doesn't allow that.

#### Solution I would like

[pd.read_csv() should take in an additional parameter, called random. Random should be a tuple of 3 ints. (start, end, number of random lines). For example: pd.read_csv('my_data.csv', random = (100, 1000, 100)) should read in 100 random lines into python datafraem from 100 lines to 1000 lines within the data.csv file.]

#### API breaking implications

[existing API about nrows should be modified so that when random = (100, 1000, 100) is present, nrows doesn't do anything, since we specify the number of rows we need to read in parameter random already.]

#### alternatives I considered

[what we could do also is to have another parameter specifying the starting line and end line, for example start line = 10, end line = 100 will allow the reader to read from 10 lines and up to 100 lines in the csv file, nrows specifies how many lines starting from 10 we read. If Random = True then we read 10 random lines from 10 to 100 lines instead of reading from 10 - 20 lines.]

#### Additional context

[I haven't looked too deep into pandas to see if my idea is already being developed, but if there is any development, I would appreciate the link to that discussion post. I would very much like to help and code this thing if no one has done this before.]
[FYI: this is my first time to contribute to open source, so please let me know how to help code this feature.]

"
591288778,33186,pandas >= 1.0.0 fails to read an old HDF5 file,lschr,open,2020-03-31T17:31:53Z,2020-09-10T00:19:07Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
pd.read_hdf(""beads1.h5"", ""features"")
```
#### Problem description

While working on pandas 0.25, this fails starting with pandas 1.0.0. The HDF file was created in November 2015 or even earlier and is inside the [zip file](https://github.com/pandas-dev/pandas/files/4410582/beads1.zip). Other files created at the same time seem to work.

The error message is:
```
TypeError                                 Traceback (most recent call last)
<ipython-input-2-507ed8d14986> in <module>
----> 1 pd.read_hdf(""Software/sdt-python/tests/data_chromatic/beads1.h5"", ""features"")

~/anaconda3/lib/python3.7/site-packages/pandas/io/pytables.py in read_hdf(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)
    426             iterator=iterator,
    427             chunksize=chunksize,
--> 428             auto_close=auto_close,
    429         )
    430     except (ValueError, TypeError, KeyError):

~/anaconda3/lib/python3.7/site-packages/pandas/io/pytables.py in select(self, key, where, start, stop, columns, iterator, chunksize, auto_close)
    812         )
    813 
--> 814         return it.get_result()
    815 
    816     def select_as_coordinates(

~/anaconda3/lib/python3.7/site-packages/pandas/io/pytables.py in get_result(self, coordinates)
   1827 
   1828         # directly return the result
-> 1829         results = self.func(self.start, self.stop, where)
   1830         self.close()
   1831         return results

~/anaconda3/lib/python3.7/site-packages/pandas/io/pytables.py in func(_start, _stop, _where)
    796         # function to call on iteration
    797         def func(_start, _stop, _where):
--> 798             return s.read(start=_start, stop=_stop, where=_where, columns=columns)
    799 
    800         # create the iterator

~/anaconda3/lib/python3.7/site-packages/pandas/io/pytables.py in read(self, where, columns, start, stop)
   3067         for i in range(self.nblocks):
   3068 
-> 3069             blk_items = self.read_index(f""block{i}_items"")
   3070             values = self.read_array(f""block{i}_values"", start=_start, stop=_stop)
   3071 

~/anaconda3/lib/python3.7/site-packages/pandas/io/pytables.py in read_index(self, key, start, stop)
   2757         elif variety == ""regular"":
   2758             node = getattr(self.group, key)
-> 2759             index = self.read_index_node(node, start=start, stop=stop)
   2760             return index
   2761         else:  # pragma: no cover

~/anaconda3/lib/python3.7/site-packages/pandas/io/pytables.py in read_index_node(self, node, start, stop)
   2878                     data, kind, encoding=self.encoding, errors=self.errors
   2879                 ),
-> 2880                 **kwargs,
   2881             )
   2882 

~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in __new__(cls, data, dtype, copy, name, tupleize_cols, **kwargs)
    407                 if new_dtype is not None:
    408                     return cls(
--> 409                         new_data, dtype=new_dtype, copy=False, name=name, **kwargs
    410                     )
    411 

~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in __new__(cls, data, dtype, copy, name, tupleize_cols, **kwargs)
    411 
    412             if kwargs:
--> 413                 raise TypeError(f""Unexpected keyword arguments {repr(set(kwargs))}"")
    414             if subarr.ndim > 1:
    415                 # GH#13601, GH#20285, GH#27125

TypeError: Unexpected keyword arguments {'freq'}
```

#### Expected Output

```
             x           y    z       signal          bg          mass  \
0   128.664558    2.966443  0.0  4189.596881  417.007853  27574.470714   
1   355.112881    3.865741  0.0  1061.439002  266.074445  12902.725487   
2   169.607263    6.025977  0.0  3103.641263  273.718417  22003.694114   
3   396.070422    6.681506  0.0   799.983142  227.225677   9527.816676   
4   127.806587    9.435608  0.0  4543.589962  332.126465  33712.910416   
..         ...         ...  ...          ...         ...           ...   
61  284.447272   56.068638  0.0   502.772436  221.098239   7438.331854   
62  381.438571   96.586069  0.0   287.953711  227.066782   1912.087223   
63  217.752114    3.596804  0.0   150.887499  206.701723   1118.474095   
64  258.435664   54.131181  0.0   125.772954  209.084140   2859.549002   
65  328.954684  100.785512  0.0   506.879163  224.691759   5695.560215   

        size  frame  
0   1.023476      0  
1   1.390924      0  
2   1.062239      0  
3   1.376786      0  
4   1.086697      0  
..       ...    ...  
61  1.534483      0  
62  1.028022      0  
63  1.086166      0  
64  1.902240      0  
65  1.337291      0  

[66 rows x 8 columns]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-88-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : de_AT.UTF-8
LOCALE           : de_AT.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.4.4
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
696959285,36248,BUG:Inconsistent when use Series.get,liudengfeng,open,2020-09-09T16:25:44Z,2020-09-10T02:57:02Z,"- [ ] I have checked that this issue has not already been reported.

- [ + ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd
animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'],index=[-1,0,1,2,3])
animals.get([1,-1,2]) # ok

animals = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama'])
animals.get([1,2]) # ok

BUT：
animals.get([1,-1,2]) return None!!!
```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output
If you mix existing labels with non-existent labels, at least need to return the corresponding results of the existing labels?
#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : Chinese (Simplified)_China.936

pandas           : 1.1.2
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200814
Cython           : 0.29.21
pytest           : 6.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.8.0
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.19
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
696909430,36247,[BUG] loc does not preserve datatype with a single element,glyg,closed,2020-09-09T15:23:15Z,2020-09-10T11:22:13Z,"Opening a new issue as per @Dr-Irv suggestion on #31763
Sorry issue formatting got lost 


```py
import pandas as pd
import numpy as np

not_preserved = pd.DataFrame(np.arange(24).reshape((8, 3)), columns=list(""ABC""))
not_preserved['F'] = np.random.random(8)

print(""Mixed datatype DataFrame"")
print(not_preserved.dtypes)

print(""Single value loc: "")
print(not_preserved.loc[0, ['A', 'B', 'C']].dtypes)

print(""Single value through slice: "")
print(not_preserved.loc[0:0, ['A', 'B', 'C']].dtypes)

print(""The columns are cast to float:"")
print(not_preserved.loc[0, ['A', 'B', 'C']])

# This prints:
# Mixed datatype DataFrame
# A      int64
# B      int64
# C      int64
# F    float64
# dtype: object
# Single value loc: 
# float64
# Single value through slice: 
# A    int64
# B    int64
# C    int64
# dtype: object

# The columns are cast to float:
# A    0.0  
# B    1.0
# C    2.0
```

As a test:

```py
# A single cell is fine

assert isinstance(not_preserved.loc[0, ""A""], np.int64) # passes

assert not_preserved.loc[0:0, [""A"", ""B""]].dtypes[""A""] is np.dtype('int64') # passes
assert not_preserved.loc[0, [""A"", ""B""]].dtypes is np.dtype('int64') # AssertionError
```

This is with pandas 1.0.3 but was already the case in  0.24.1, it seems that what changed was the behavior of a `replace` method downstream in my code.
_Originally posted by @glyg in https://github.com/pandas-dev/pandas/issues/31763#issuecomment-586213659_"
638883923,34800,CI: Checks-and-Web-and-Docs-on-1.0.x,simonjayhawkins,closed,2020-06-15T14:08:05Z,2020-09-10T14:55:59Z,
685552829,35895,CI: Mark s3 tests parallel safe,TomAugspurger,closed,2020-08-25T14:49:51Z,2020-09-10T16:30:56Z,"Closes https://github.com/pandas-dev/pandas/issues/35856

I think we need to update the pytest pattern though, so this should
fail."
698271144,36270,FIX: allow sorting by tuple of column names,ivanovmg,closed,2020-09-10T17:28:16Z,2020-09-10T18:29:49Z,"- [ ] closes #36268
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
658742349,35317,"DOC: Rst Formatting, make sure continuation prompt are used.",Carreau,closed,2020-07-17T01:46:44Z,2020-09-10T18:53:46Z,"Without the `...` for some tools it may look like
`with ExcelWriter('path_to_file.xlsx',` is an input and
`date...` is the output.

I initially thought that I could use the hanging indent to separate
code continuation from output but `pandas/core/indexing.py`
proved me wrong as the example just after the one I fixed have
the first line of the output be indented.

"
643391204,34941,ENH: Add NDFrame.format for easier conversion to string dtype,topper-123,closed,2020-06-22T22:09:45Z,2020-09-10T18:54:58Z,"- [x] closes #17211
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This adds a ``format`` method to DataFrame and Series. This is useful for data transformation.

This method allows/makes it easier to do more complex conversion from arbitrary dtypes to ``string`` series, including combining several columns in a DataFrame to make the string series. For example we can now do this conversion quite easily:

```python
>>> df = pd.DataFrame({
...     'state_name': ['California', 'Texas', 'Florida'],
...     'state_abbreviation': ['CA', 'TX', 'FL'],
...     'population': [39_512_223, 28_995_881, 21_477_737],
...     }, index=[1, 2, 3])
>>> df
   state_name state_abbreviation  population
1  California                 CA    39512223
2       Texas                 TX    28995881
3     Florida                 FL    21477737

>>>  df.format(""{state_name:<10} ({state_abbreviation}): {population:,}"")
1    California (CA): 39,512,223
2    Texas      (TX): 28,995,881
3    Florida    (FL): 21,477,737
dtype: string
```

I still need to update text.rst, but would like feedback on this first, as this is a bit different than discussed in #17211. In that issue we e.g. only discussed a ``format`` method for ``Series``, while this also adds it for ``DataFrame``. In #17211 I also aired the idea of allowing series methods in the format string. I think that is technically quite difficult, so is not part of this PR."
566256342,32053,[#16737] Index type for Series with empty data,SaturnFromTitan,closed,2020-02-17T12:08:01Z,2020-09-10T18:57:12Z,"- [x] closes #16737
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

---------------------------------
I picked up all the notes from #16737 where it was suggested to use `Index` over `RangeIndex` for empty data.
"
684563260,35871,BUG: Regression in pd.read_sql_query between 1.0.5 and 1.1.0,rbubley,open,2020-08-24T10:41:56Z,2020-09-10T20:56:06Z,"- [ x] I have checked that this issue has not already been reported.

- [ x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import sqlalchemy
from config import  credentials

engine = sqlalchemy.create_engine(
        ""mysql+mysqldb://%(user)s:%(password)s@%(host)s/%(default)s"" % credentials,
        echo=False,
    )

sql_query = r""""""
SELECT 
  time_string,
  STR_TO_DATE(time_string, '%%d/%%m/%%Y'),
  data
FROM
  test_table
""""""

data = pd.read_sql_query(sql_query, engine)

print(data)
```

#### Problem description

In Pandas 1.0.5,  '%' in a sql query needed to be escaped as '%%'.
This changes in 1.1.0, but I couldn't see any relevant comment in the release notes. (The behaviour is the same in 1.1.1).

My feeling is that the new behaviour is actually better, but this still seems to be a regression (or an undocumented change).

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.8.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 17.7.0
Version          : Darwin Kernel Version 17.7.0: Wed Feb 27 00:43:23 PST 2019; root:xnu-4570.71.35~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.0
numpy            : 1.18.5
pytz             : 2018.9
dateutil         : 2.8.0
pip              : 20.2.2
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.17
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
575366096,32432,BUG: pandas.DataFrame.any bool_only parameter not working since v0.24.0,funjo,closed,2020-03-04T12:35:02Z,2020-09-11T01:11:44Z,"#### Code Sample, a copy-pastable example if possible
This is the code that I run to test the DataFrame.any function in different pandas versions
```python
import pandas as pd
pd.__version__
df = pd.DataFrame({""A"": [True, False], ""B"": [1, 2]})
df
df.dtypes
df.any(axis=1)
df.any(axis=1, bool_only=True)
```
#### Problem description

In pandas version 0.23.4, the output is expected when `` bool_only=True`` is specified

```python
>>> import pandas as pd
>>> pd.__version__
'0.23.4'
>>> df = pd.DataFrame({""A"": [True, False], ""B"": [1, 2]})
>>> df
       A  B
0   True  1
1  False  2
>>> df.dtypes
A     bool
B    int64
>>> df.any(axis=1)
0    True
1    True
dtype: bool
>>> df.any(axis=1, bool_only=True)
0     True
1    False
dtype: bool
```
However in pandas version 0.24.0, the final output is wrong for ``bool_only=True``
```python
>>> import pandas as pd
>>> pd.__version__
'0.24.0'
>>> df = pd.DataFrame({""A"": [True, False], ""B"": [1, 2]})
>>> df
       A  B
0   True  1
1  False  2
>>> df.dtypes
A     bool
B    int64
>>> df.any(axis=1)
0    True
1    True
dtype: bool
>>> df.any(axis=1, bool_only=True)
0    True
1    True
dtype: bool
```

#### Expected Output
```python
>>> df.any(axis=1, bool_only=True)
0     True
1    False
dtype: bool
```
#### Output of ``pd.show_versions()``
I can only reproduce this bug for pandas version v0.24.0 and above so I'll include details for v0.24.0.
<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.4.final.0
python-bits: 64
OS: Darwin
OS-release: 18.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.24.0
pytest: 5.0.1
pip: 20.0.2
setuptools: 40.8.0
Cython: None
numpy: 1.17.0
scipy: None
pyarrow: 0.14.1
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2019.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>"
692636558,36106,BUG: DataFrame.any with axis=1 and bool_only=True,jbrockmendel,closed,2020-09-04T02:55:33Z,2020-09-11T01:20:21Z,"- [x] closes #32432
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
329610210,21331,PERF/ENH: groupby.apply for user-defined function ~100x slower than ...,h-vetinari,closed,2018-06-05T20:20:58Z,2020-09-11T03:31:44Z,"I have several groupby problems crop up regularly at work**, and almost always have to write custom functions for operating on individual groups - which are then very slow to execute.

Looking for solutions, I came across
http://esantorella.com/2016/06/16/groupby/
and the update
https://github.com/esantorella/hdfe/blob/master/groupby.py

I had to adapt my function a little bit to be compatible with numpy- instead of pandas-indexing, but in one case I tested today, this sped up my code by *more than a factor 100*, and this wasn't even for the full dataset. In fact (for ~5mio rows), the pandas-native version didn't finish in 2h, whereas the adaptation based on the code linked above ran in ~35sec.

It would be very nice if such optimisations would be taken care of directly by pandas, instead of having to work around like this.

** one example among many: process data from source, deduplicate based on a given signature, process further, keep deduplicated record. Updates to this record need to recalculate any time one of the constituent records has changed"
619711761,34226,ENH: Add VariableForwardWindowIndexer,yohplala,closed,2020-05-17T14:04:39Z,2020-09-11T06:07:23Z,"#### Is your feature request related to a problem?

I would like to use the rolling() method with looking-forward window using a negative `window`.
I don't remember which other pandas function is using this approach of negative window for reversing the position of the window with respect to the actual index, but it is for the user very convenient.
I was convinced it was existing for rolling, but it appears it is not.

#### Describe the solution you'd like

```python
# Definition of a negative offset (this works)
offset = pd.tseries.frequencies.to_offset('-1h')

# Use it in rolling (this does not work)
start = '2020-01-01 08:00'
end = '2020-01-01 12:00'
intervals = pd.period_range(start = start, end=end, freq = '30T')
values = [i for i in range(0, len(intervals))]
ser = pd.Series(values, index = intervals)
test = ser.rolling(window = offset, closed='left').min()
```"
690999818,36066,BUG: isin has unexpected behaviour when using a dataframe with a large amount of rows (1.000.001),Hanspagh,closed,2020-09-02T12:58:34Z,2020-09-11T08:26:42Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np
nans = np.repeat(np.nan, 1_000_001)
big = pd.DataFrame({""test"":nans})
assert all(big[""test""].isin([np.nan])) == True
 
nans = np.repeat(np.nan, 1_000_000)
small = pd.DataFrame({""test"":nans})
assert all(small[""test""].isin([np.nan])) == True
assert all(small.dtypes == big.dtypes)
```

#### Problem description
The above code fails on the first assert, but not on the second.


#### Expected Output
The excepted behavior would be that both asserts either fail or succeeds

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.3.0
Version          : Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : 0.29.17
pytest           : 5.4.3
hypothesis       : 4.57.1
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.48.0

</details>
"
688356845,35965,BUG: DataFrame creates an object-dtype array for extension type scalars in dictionaries,justinessert,closed,2020-08-28T21:10:02Z,2020-09-11T13:03:03Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame({
    'a': [pd.Period('2020-01')],
    'b': pd.Period('2020-01')
})

df.dtypes
```

#### Problem description

In the example above, column `a` will be correctly created as a `period[m]` column, but column `b` will be an `object` type. This is similar to an [issue 34832](https://github.com/pandas-dev/pandas/issues/34832), but for a different DF instantiation method.

#### Expected Output

These two columns should have identical types. They should both be type `period[m]`.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : c413df6d6a85eb411cba39b907fe7a74bdf4be1f
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Sun Dec  1 18:59:03 PST 2019; root:xnu-4903.278.19~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+173.gc413df6d6.dirty
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1.post20200616
Cython           : 0.29.20
pytest           : 5.4.3
hypothesis       : 5.16.2
sphinx           : 3.1.1
blosc            : 1.9.1
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : 0.4.0
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.0

</details>
"
696348041,36237,CLN: simplify Categorical comparisons,jbrockmendel,closed,2020-09-09T02:47:03Z,2020-09-11T14:34:57Z,"Not as much as I had hoped, but not nothing."
695498559,36209,REF: share more EA methods,jbrockmendel,closed,2020-09-08T02:37:54Z,2020-09-11T14:36:10Z,
696358898,36238,DOC: Update groupby.rst,Nikhil1O1,closed,2020-09-09T03:09:02Z,2020-09-11T16:47:21Z,"The previous description still had a conflict between the column name and index level

- [x] closes #16870 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
516845064,29383,Series repr html only,big-o,closed,2019-11-03T15:31:41Z,2020-09-11T17:22:03Z,"- [x] closes #5563 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
382455835,23803,Feature request: add na_action argument to applymap,gwerbin,closed,2018-11-20T01:04:34Z,2020-09-11T17:40:12Z,"#### Code Sample, a copy-pastable example if possible

Desired method:

```python
x = pd.DataFrame(...)
x = x.applymap(foo, na_action='ignore')
```

Current method:

```python
x = pd.DataFrame(...)
x = x.apply(lambda y: y.map(foo, na_action='ignore'))
```

#### Problem description

The `na_action='ignore'` argument is extremely useful when processing messy data, and it would be nice to have on the `DataFrame.applymap` method as well.

`DataFrame.applymap` already uses very similar code, so ideally this shouldn't be too dramatic of a change.

[`DataFrame.applymap`](https://github.com/pandas-dev/pandas/blob/v0.23.4/pandas/core/frame.py#L6067-L6072)
[`Series.map` (1)](https://github.com/pandas-dev/pandas/blob/v0.23.4/pandas/core/series.py#L2997-L3000), [`Series.map` (2)](https://github.com/pandas-dev/pandas/blob/v0.23.4/pandas/core/base.py#L994-L1004)

For that matter, is there any reason why `DataFrame.applymap` shouldn't just be a wrapper for `.apply(lambda x: Series.map)`? Seems like it would be beneficial for deduplicating logic in the codebase anyway."
678354717,35704,ENH add na_action to DataFrame.applymap,jnothman,closed,2020-08-13T11:11:53Z,2020-09-11T17:40:18Z,"For symmetry with Series.map

- [x] closes #23803
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
412045794,25375,Strange behavior with median,WillKoehrsen,closed,2019-02-19T17:42:43Z,2020-09-11T18:22:28Z,"Consider the following dataframe:
```
       b           c     d     e  f     g     h
0   6.25  2018-04-01  True   NaN  7  54.0  64.0
1  32.50  2018-04-01  True   NaN  7  54.0  64.0
2  16.75  2018-04-01  True   NaN  7  54.0  64.0
3  29.25  2018-04-01  True   NaN  7  54.0  64.0
4  21.75  2018-04-01  True   NaN  7  54.0  64.0
5  21.75  2018-04-01  True  True  7  54.0  64.0
6   7.75  2018-04-01  True  True  7  54.0  64.0
7  23.25  2018-04-01  True  True  7  54.0  64.0
8  12.25  2018-04-01  True  True  7  54.0  64.0
9  30.50  2018-04-01  True   NaN  7  54.0  64.0
```
(copy and paste and use `df = pd.read_clipboard()` to create the dataframe)

Finding the medians initially works with no problem:

```python
df.median()

b    21.75
d     1.00
e     1.00
f     7.00
g    54.00
h    64.00
dtype: float64
```
However, if a column is dropped and then the `median` is found, the median for column `e` disappears:

```python
new_df = df.drop(columns=['b'])
new_df.median()

d     1.0
f     7.0
g    54.0
h    64.0
dtype: float64
```

This behavior is a little unexpected and finding the median for column e by itself still works:

```python
new_df['e'].median()
1.0
```

Using `skipna=False` does not make a difference:
```python
new_df.median(skipna=False)

d     1.0
f     7.0
g    54.0
h    64.0
dtype: float64
```

(it does for the original dataframe):

```python
df.median(skipna=False)

b    21.75
d     1.00
e      NaN
f     7.00
g    54.00
h    64.00
dtype: float64
```

The datatype of column `e` is `object` in both `df` and `new_df` and the only difference between the two dataframes is `new_df` does not have column `b`. Adding the column back into `new_df` does not resolve the issue. This only occurs when the first column `b` is dropped. It does not occur if column `e` is a float or integer datatype.


This behavior is present in both `pandas==0.22.0` and `pandas==0.24.1`

I also created a [Stack Overflow Question](https://stackoverflow.com/questions/54755354/strange-behavior-with-pandas-median) based on this issue."
697165570,36258,DOC: Deprecation Warnings in user guide,phofl,closed,2020-09-09T21:18:43Z,2020-09-11T21:33:03Z,"#### Location of the documentation

https://pandas.pydata.org/docs/user_guide/indexing.html#indexing-deprecate-loc-reindex-listlike

#### Documentation problem

We have a lot of warnings in the documentation ``Starting with 0.x the following is deprecated.`` Most of these things stopped working with 1.0. Should we update the docs and say, that this stopped working with 1.0?
"
697245997,36259,de-privatize imported names,jbrockmendel,closed,2020-09-09T23:49:42Z,2020-09-11T21:51:23Z,After this we're down to ~5 i think
699482564,36292,"[DOC]: Update deprecation warnings, which were already removed",phofl,closed,2020-09-11T15:59:32Z,2020-09-11T22:29:09Z,"- [x] closes #36258 

Updated the deprecation warnings.
"
698477454,36276,CLN: typo cleanups,jbrockmendel,closed,2020-09-10T20:50:05Z,2020-09-12T01:00:19Z,
664942198,35399,"API: read_csv, to_csv line_terminator keyword inconsistency",arw2019,closed,2020-07-24T06:12:53Z,2020-09-12T17:52:26Z,"- [x] closes #9568
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry (pending 1.2)

I followed discussion in #9568. The aim of this PR is to allow both `line_terminator` and `lineterminator` keyword args  to `read_csv` and `to_csv` but only document `line_terminator`. As per @jorisvandenbossche's suggestion we preserve `lineterminator` to stay compatible with `csv` dialects.
"
502714282,28790,Added test test_datetimeField_after_setitem for issue #6942,anirudnits,closed,2019-10-04T15:51:53Z,2020-09-12T18:04:23Z,"Added test `test_datetimeField_after_setitem` in `tests/generic/test_frame.py`.

- [x] closes #6942 
- [x] test added
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
691520673,36081,BUG: Unary pos/neg ops on IntegerArrays failing with TypeError,asishm,closed,2020-09-03T00:11:11Z,2020-09-12T18:20:11Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
698205610,36269,CLN: pandas/io/parsers.py,ivanovmg,closed,2020-09-10T16:26:00Z,2020-09-12T20:54:28Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Slight clean-up of ``pandas/io/parsers.py``."
700335581,36309,PERF: get_dtype_kinds,jbrockmendel,closed,2020-09-12T18:49:45Z,2020-09-12T20:55:51Z,"Upshot: avoid doing lots of non-performant is_foo_dtype checks.

This fixes a little more than half of the performance hit in the benchmark discussed in #34683."
699998283,36301,REF: de-duplicate sort_values,jbrockmendel,closed,2020-09-12T03:13:10Z,2020-09-12T20:56:09Z,
698753726,36282,REF: de-duplicate _wrap_joined_index,jbrockmendel,closed,2020-09-11T02:47:46Z,2020-09-12T20:57:10Z,
654973913,35219,BUG: resampling error when date_range includes a single DST,Flix6x,closed,2020-07-10T18:53:08Z,2020-09-12T21:07:53Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python

>>> import pandas as pd

>>> # works as expected (note the daylight savings transitions in the head and tail)
>>> pd.Series(1., pd.date_range('2020-03-28','2020-10-27', freq='D', tz=""Europe/Amsterdam"")).resample('24H').pad()

2020-03-28 00:00:00+01:00    1.0
2020-03-29 00:00:00+01:00    1.0
2020-03-30 01:00:00+02:00    1.0
2020-03-31 01:00:00+02:00    1.0
2020-04-01 01:00:00+02:00    1.0
                            ... 
2020-10-23 01:00:00+02:00    1.0
2020-10-24 01:00:00+02:00    1.0
2020-10-25 01:00:00+02:00    1.0
2020-10-26 00:00:00+01:00    1.0
2020-10-27 00:00:00+01:00    1.0
Freq: 24H, Length: 214, dtype: float64

>>> # fails unexpectedly
>>> pd.Series(1., pd.date_range('2020-03-28','2020-03-31', freq='D', tz=""Europe/Amsterdam"")).resample('24H').pad()

Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""/home/felix/anaconda3/envs/bvp-venv/lib/python3.6/site-packages/pandas/core/resample.py"", line 453, in pad
    return self._upsample(""pad"", limit=limit)
  File ""/home/felix/anaconda3/envs/bvp-venv/lib/python3.6/site-packages/pandas/core/resample.py"", line 1092, in _upsample
    result.index = res_index
  File ""/home/felix/anaconda3/envs/bvp-venv/lib/python3.6/site-packages/pandas/core/generic.py"", line 5287, in __setattr__
    return object.__setattr__(self, name, value)
  File ""pandas/_libs/properties.pyx"", line 67, in pandas._libs.properties.AxisProperty.__set__
  File ""/home/felix/anaconda3/envs/bvp-venv/lib/python3.6/site-packages/pandas/core/series.py"", line 401, in _set_axis
    self._data.set_axis(axis, labels)
  File ""/home/felix/anaconda3/envs/bvp-venv/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 178, in set_axis
    f""Length mismatch: Expected axis has {old_len} elements, new ""
ValueError: Length mismatch: Expected axis has 4 elements, new values have 3 elements

```

#### Problem description

In my first example, resampling from an offset of 1 day to an offset of 24 hours works as expected, but only when the start and end of the DatetimeIndex share the same timezone. In my second example the date range start and ends in a different timezone due to a daylight savings transition on 29 March 2020, for which resampling to 24 hours fails.

Possibly related issue:

- #35248

#### Expected Output

```python

>>> pd.Series(1., pd.date_range('2020-03-28','2020-03-31', freq='D', tz=""Europe/Amsterdam"")).resample('24H').pad()

2020-03-28 00:00:00+01:00    1.0
2020-03-29 00:00:00+01:00    1.0
2020-03-30 01:00:00+02:00    1.0
Freq: 24H, Length: 3, dtype: float64

```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.9.0-11-amd64
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1.post20200622
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
697568300,36264,Resample fix dst transition,Flix6x,closed,2020-09-10T07:48:04Z,2020-09-12T21:08:11Z,"Fix bug when resampling with DST transition.

- [x] closes #35219
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
697265199,36260,CLN: _wrap_applied_output,rhshadrach,closed,2020-09-10T00:25:23Z,2020-09-12T21:15:53Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
696510952,36241,"BUG: .str.startswith(..., na=False) consistency between categorical and string series (again)",dlimpid,closed,2020-09-09T07:11:56Z,2020-09-12T21:17:49Z,"#### Code Sample, a copy-pastable example

```pycon
>>> import pandas as pd
>>> import numpy as np
>>> s = pd.Series([""a"", ""b"", np.nan], index=[""a"", ""b"", np.nan], dtype=""category"")
>>> df = pd.DataFrame().assign(
...     cat_contains=s.str.contains(""a"", na=False),
...     cat_startswith=s.str.startswith(""a"", na=False),
...     cat_endswith=s.str.endswith(""a"", na=False),
...     str_contains=s.astype(""string"").str.contains(""a"", na=False),
...     str_startswith=s.astype(""string"").str.startswith(""a"", na=False),
...     str_endswith=s.astype(""string"").str.endswith(""a"", na=False),
... )
>>> df.info()
<class 'pandas.core.frame.DataFrame'>
Index: 3 entries, a to nan
Data columns (total 6 columns):
 #   Column          Non-Null Count  Dtype
---  ------          --------------  -----
 0   cat_contains    3 non-null      bool
 1   cat_startswith  2 non-null      object
 2   cat_endswith    2 non-null      object
 3   str_contains    3 non-null      boolean
 4   str_startswith  3 non-null      boolean
 5   str_endswith    3 non-null      boolean
dtypes: bool(1), boolean(3), object(2)
memory usage: 93.0+ bytes
>>> df
     cat_contains cat_startswith cat_endswith  str_contains  str_startswith  str_endswith
a            True           True         True          True            True          True
b           False          False        False         False           False         False
NaN         False            NaN          NaN         False           False         False
```

#### Problem description

`.str.startswith(..., na=False)` and `.str.endswith` should make missing values `False` when the calling series is of type categorical just like it does for string series.

Similar to #22158, but `.str.contains` works here.

#### Expected Output

```pycon
>>> df
     cat_contains cat_startswith cat_endswith  str_contains  str_startswith  str_endswith
a            True           True         True          True            True          True
b           False          False        False         False           False         False
NaN         False          False        False         False           False         False
```

#### Output of ``pd.show_versions()``

Using conda env with `conda create -n pandas112 -c conda-forge pandas=1.1.2`

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : Korean_Korea.949

pandas           : 1.1.2
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.6.0.post20200814
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
697106092,36254,searchsorted numpy compat for Period dtype,jbrockmendel,closed,2020-09-09T19:44:52Z,2020-09-12T21:17:52Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
697021364,36250,CLN: simplify Categorical comparisons,jbrockmendel,closed,2020-09-09T17:45:09Z,2020-09-12T21:18:29Z,"We have way too many ways of comparing Categorical dtypes.
"
676992045,35670,BUG: surprising and possibly erroneous behavior of GroupBy.apply with an indexed series (index winds up duplicated),aecay,closed,2020-08-11T15:51:35Z,2020-09-04T18:04:10Z,"- [x] I have checked that this issue has not already been reported.
  - I can't find anything in the bug tracker that matches the symptoms I'm reporting here, although it's a bit difficult to search (I'm not totally sure how to describe it)
- [x] I have confirmed this bug exists on the latest version of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

data = [{""label"": l, ""x"" : x, ""y"": x + 1} for l in (""foo"", ""bar"") for x in range(5)]
df = pd.DataFrame(data)
df = df.set_index([""label"", ""x""])
series = df[""y""]
series2 = series.groupby([""label""]).apply(lambda s: s[2:])
print(series2.index)

# Output:

MultiIndex([('bar', 'bar', 2),
            ('bar', 'bar', 3),
            ('bar', 'bar', 4),
            ('foo', 'foo', 2),
            ('foo', 'foo', 3),
            ('foo', 'foo', 4)],
           names=['label', 'label', 'x'])
```

#### Problem description

The ""label"" field is duplicated in the index of the result

#### Expected Output

I expect the index after the apply to be the same as before, ie to only contain ""label"" once

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.0.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-42-generic
Version          : #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.1
Cython           : 0.29.15
pytest           : 5.4.0
hypothesis       : None
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : 0.8.6
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
689926760,36030,BUG: wrong plotting in x-axis when index set to date,saturn99,closed,2020-09-01T08:12:25Z,2020-09-04T19:42:36Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

in `mydata` i have't no any data between `2000-01-03` to `2020-01-01`

but pandas wrong plotting in x-axis when index set to date

```python
d1 = [ '2000-01-01', 5 ]
d2 = [ '2000-01-02', -5 ]
d3 = [ '2020-01-02', 5 ]
mydata = pd.DataFrame( [d1,d2,d3] , columns=[""DATE"",""DATA""] )

mydata[""DATE""] = pd.to_datetime( mydata[""DATE""] )
mydata = mydata.set_index(""DATE"")

mydata.plot()
```
![index](https://user-images.githubusercontent.com/5188799/91824221-a8fdd380-ec4f-11ea-8837-66d039ec792d.png)


#### Expected Output

```python
d1 = [ '2000-01-01', 5 ]
d2 = [ '2000-01-02', -5 ]
d3 = [ '2020-01-02', 5 ]
mydata = pd.DataFrame( [d1,d2,d3] , columns=[""DATE"",""DATA""] )

# mydata[""DATE""] = pd.to_datetime( mydata[""DATE""] )
mydata = mydata.set_index(""DATE"")

mydata.plot()
```
![index](https://user-images.githubusercontent.com/5188799/91824423-fda14e80-ec4f-11ea-9eda-d5988d18eb2a.png)



#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.9.0-7-amd64
Version          : #1 SMP Debian 4.9.110-1 (2018-07-05)
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.utf8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None


</details>
"
689997340,36032,BUG: .dt.isocalendar().year,vukhanh1202,closed,2020-09-01T09:47:42Z,2020-09-04T20:29:25Z,"#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame(
    {
        ""Date"": [
            pd.to_datetime(""2016-01-01""),
            pd.to_datetime(""2016-01-04""),
            pd.to_datetime(""2016-01-05""),
            pd.to_datetime(""2016-01-06""),
            pd.to_datetime(""2016-01-07""),
            pd.to_datetime(""2016-01-08""),
        ]
    }
)
df[""Date""].dt.isocalendar().year
```

#### Problem description
new function .dt.isocalendar().year returns wrong value for the day of  pd.to_datetime('2016-01-08'). It is supposed to return 2016 but 2017

please check on that

"
671760921,35521,REGR: invalid cache after take operation with non-consolidated dataframe,on55,closed,2020-08-03T03:05:29Z,2020-09-04T20:48:43Z,"a program run ok with pandas 1.0.5,  but after upgrade to 1.1.0.  it fail.
i found sometimes when I change one cell of dataframe value,  print(df) and it doesn't change.
but i use  ==, it says changed.
for example:  original  cell value is A, then i change cell value to B.   A=B
print df, it show still A
df.at(x, x) == B, it says TRUE.

-----

MarcoGorelli's edit: here's a reproducible example:

```
import pandas as pd


position = pd.DataFrame(columns=[""code"", ""startdate""])
position = position.append([{""code"": ""a"", ""startdate"": 0}])

# These two lines should not change anything.
# BUT, commenting either of them out makes this code run as intendeed
position[""code""] == ""A""
position[position[""startdate""] == 0]

position.at[0, ""code""] = ""A""

print(position.at[0, ""code""])
print(position)
```

output:
```
A
  code startdate
0    a         0
```

expected output:
```
A
  code startdate
0    A         0
```"
687300342,35924,REF: use BlockManager.apply for DataFrameGroupBy.count,jbrockmendel,closed,2020-08-27T14:43:01Z,2020-09-04T20:50:50Z,
692650329,36107,STY: de-privatize funcs imported cross-module,jbrockmendel,closed,2020-09-04T03:15:36Z,2020-09-04T20:53:46Z,
692564786,36104,STY: De-privatize functions in io.excel imported elsewhere,jbrockmendel,closed,2020-09-04T01:09:10Z,2020-09-04T20:56:48Z,xref #36055
693589594,36127,Backport PR #36050 on branch 1.1.x (BUG: incorrect year returned in isocalendar for certain dates),meeseeksmachine,closed,2020-09-04T20:29:38Z,2020-09-04T21:58:14Z,Backport PR #36050: BUG: incorrect year returned in isocalendar for certain dates
693597713,36128,CLN: remove xfails/skips for no-longer-supported numpys,jbrockmendel,closed,2020-09-04T20:37:37Z,2020-09-04T22:03:52Z,
545694356,30732,to_csv swallows exception when writing to S3,vfilimonov,closed,2020-01-06T12:07:37Z,2020-09-05T00:01:23Z,"I'm not sure if this issue belongs to `pandas` or `s3fs`.

When writing to non-existing bucket or bucket without proper permissions no exception is raised. E.g. the following code will be executed normally:
```python
df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
df.to_csv('s3://very.weird.and.certainly.nonexistent.bucket/data.csv')  # No exception
```

In contrast, when writing to a local file without proper permissions results in an exception as it should be:
```python
dff.to_csv('/data.csv')  # PermissionError: [Errno 13] Permission denied: '/data.csv'
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.1.post20191125
Cython           : None
pytest           : 5.3.0
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : None
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.0.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.12.1
pytables         : None
s3fs             : 0.4.0
scipy            : 1.3.1
sqlalchemy       : 1.3.8
tables           : 3.4.4
xarray           : None
xlrd             : 1.1.0
xlwt             : None
xlsxwriter       : None

</details>
"
692876373,36110,QST:how can i cut down time when using pandas.style.to_excel()?,ouououououyy,closed,2020-09-04T08:15:21Z,2020-09-05T01:34:41Z,"#### Question about pandas
  I tested the function  **to_excel()**  from the following code using xlsx file which size is **50**M. 
the result is **df_style_red_mark**  spend 350s ,but **df** spend 60s. And i want to know why it is?
how can i cut down time when using **df_style_red_mark**?
```python
df_style_red_mark = df.style.apply(...)
```

```python
df_style_red_mark.to_excel(...)

df.to_excel(...)
```
"
691566210,36082,Comma cleanup,JonathanShrek,closed,2020-09-03T02:02:22Z,2020-09-05T03:10:54Z,"- [x] pandas/tests/indexes/datetimes/test_datetime.py
- [x] pandas/tests/indexes/datetimes/test_timezones.py
- [x] pandas/tests/indexes/multi/test_constructors.py
- [x] pandas/tests/indexes/multi/test_isin.py
- [x] pandas/tests/indexes/test_base.py
- [x] pandas/tests/indexes/timedeltas/test_scalar_compat.py
- [x] pandas/tests/indexes/timedeltas/test_searchsorted.py
- [x] pandas/tests/indexing/common.py
- [x] pandas/tests/indexing/test_callable.py
- [x] pandas/tests/indexing/test_check_indexer.py
- [x] pandas/tests/indexing/test_coercion.py
- [x] pandas/tests/indexing/test_floats.py
"
685309873,35889,BUG: groupby dropna=False with nan value in groupby causes ValueError when apply() ,cwkwong,closed,2020-08-25T09:04:32Z,2020-09-05T03:15:04Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas, version `1.1.1`

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

df = pd.DataFrame({
    'groups': ['a', 'a', 'b', np.nan],
    'tonnages': [10, 10, 20, 30]
})
dfg = df.groupby('groups', dropna=False)
rv = dfg.apply(lambda grp: pd.DataFrame({'values': list(range(len(grp)))}))
```

#### Problem description

`ValueError` raised on `dfg.apply(lambda grp: ...)`; with the following stacktrace:
<details>

```python
../../env3.8/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:859: in apply
    result = self._python_apply_general(f, self._selected_obj)
../../env3.8/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:894: in _python_apply_general
    return self._wrap_applied_output(
../../env3.8/lib/python3.8/site-packages/pandas/core/groupby/generic.py:1230: in _wrap_applied_output
    return self._concat_objects(keys, values, not_indexed_same=not_indexed_same)
../../env3.8/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:1145: in _concat_objects
    result = concat(
../../env3.8/lib/python3.8/site-packages/pandas/core/reshape/concat.py:274: in concat
    op = _Concatenator(
../../env3.8/lib/python3.8/site-packages/pandas/core/reshape/concat.py:454: in __init__
    self.new_axes = self._get_new_axes()
../../env3.8/lib/python3.8/site-packages/pandas/core/reshape/concat.py:519: in _get_new_axes
    return [
../../env3.8/lib/python3.8/site-packages/pandas/core/reshape/concat.py:520: in <listcomp>
    self._get_concat_axis() if i == self.bm_axis else self._get_comb_axis(i)
../../env3.8/lib/python3.8/site-packages/pandas/core/reshape/concat.py:576: in _get_concat_axis
    concat_axis = _make_concat_multiindex(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

indexes = [RangeIndex(start=0, stop=2, step=1), RangeIndex(start=0, stop=1, step=1), RangeIndex(start=0, stop=1, step=1)]
keys = Index(['a', 'b', nan], dtype='object', name='groups')
levels = [Index(['a', 'b', nan], dtype='object', name='groups')]
names = ['groups']

    def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiIndex:
    
        if (levels is None and isinstance(keys[0], tuple)) or (
            levels is not None and len(levels) > 1
        ):
            zipped = list(zip(*keys))
            if names is None:
                names = [None] * len(zipped)
    
            if levels is None:
                _, levels = factorize_from_iterables(zipped)
            else:
                levels = [ensure_index(x) for x in levels]
        else:
            zipped = [keys]
            if names is None:
                names = [None]
    
            if levels is None:
                levels = [ensure_index(keys)]
            else:
                levels = [ensure_index(x) for x in levels]
    
        if not all_indexes_same(indexes):
            codes_list = []
    
            # things are potentially different sizes, so compute the exact codes
            # for each level and pass those to MultiIndex.from_arrays
    
            for hlevel, level in zip(zipped, levels):
                to_concat = []
                for key, index in zip(hlevel, indexes):
                    mask = level == key
                    if not mask.any():
>                       raise ValueError(f""Key {key} not in level {level}"")
E                       ValueError: Key nan not in level Index(['a', 'b', nan], dtype='object', name='groups')

../../env3.8/lib/python3.8/site-packages/pandas/core/reshape/concat.py:631: ValueError
```

</details>

#### Expected Output

No error should be raised. With the above, if i omit the `nan`:

```python
df = pd.DataFrame({
    'groups': ['a', 'a', 'b'],
    'tonnages': [10, 10, 20]
})
dfg = df.groupby('groups', dropna=False)
rv = dfg.apply(lambda grp: pd.DataFrame({'values': list(range(len(grp)))}))
```
Then it works successfully with `rv` being:

```python
          values
groups          
a      0       0
       1       1
b      0       0
```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.2.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Thu Jun 18 20:50:10 PDT 2020; root:xnu-4903.278.43~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_AU.UTF-8
LOCALE           : en_AU.UTF-8

pandas           : 1.1.1
numpy            : 1.18.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 46.4.0
Cython           : 0.29.17
pytest           : 5.1.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 0.9.6
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : 1.3.1
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 1.8.6
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.12
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
687944273,35951,BUG: Fix DataFrame.groupby().apply() for NaN groups with dropna=False ,cwkwong,closed,2020-08-28T09:33:10Z,2020-09-05T03:15:11Z,"- [X] closes #35889 
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry in `v1.1.2.rst`
"
526350770,29764,GroupBy Doesn't Always Maintain Column Index Name,WillAyd,closed,2019-11-21T03:59:38Z,2020-09-05T03:18:12Z,"Just as a follow up to #29124 discovered during #29753 - looks like the column name is dropped for any/all and transformation functions in groupby

```python
>>> df = pd.DataFrame([[1]], columns=pd.Index([""a""], name=""idx""))
>>> df.groupby([1]).sum()
idx  a
1    1
>>> df.groupby([1]).any()
      a
1  True
>>> df.groupby([1]).shift()
    a
0 NaN
```"
638706497,34789,BUG: Replace raises TypeError if to_replace is Dict with numeric DataFrame and key of Dict is String,phofl,closed,2020-06-15T09:47:46Z,2020-09-05T03:21:50Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame({""Y0"": [1, 2], ""Y1"": [3, 4]})
df = df.replace({""replace_string"": ""test""})

```

#### Problem description

This raises a TypeError with the following Traceback:

```python
Traceback (most recent call last):
  File ""/home/developer/.config/JetBrains/PyCharmCE2020.1/scratches/scratch_1.py"", line 7, in <module>
    df = df.replace({""replace_string"": ""test""})
  File ""/home/developer/PycharmProjects/pandas/pandas/core/frame.py"", line 4277, in replace
    return super().replace(
  File ""/home/developer/PycharmProjects/pandas/pandas/core/generic.py"", line 6598, in replace
    return self.replace(
  File ""/home/developer/PycharmProjects/pandas/pandas/core/frame.py"", line 4277, in replace
    return super().replace(
  File ""/home/developer/PycharmProjects/pandas/pandas/core/generic.py"", line 6641, in replace
    new_data = self._mgr.replace_list(
  File ""/home/developer/PycharmProjects/pandas/pandas/core/internals/managers.py"", line 616, in replace_list
    masks = [comp(s, regex) for s in src_list]
  File ""/home/developer/PycharmProjects/pandas/pandas/core/internals/managers.py"", line 616, in <listcomp>
    masks = [comp(s, regex) for s in src_list]
  File ""/home/developer/PycharmProjects/pandas/pandas/core/internals/managers.py"", line 614, in comp
    return _compare_or_regex_search(values, s, regex)
  File ""/home/developer/PycharmProjects/pandas/pandas/core/internals/managers.py"", line 1946, in _compare_or_regex_search
    _check_comparison_types(False, a, b)
  File ""/home/developer/PycharmProjects/pandas/pandas/core/internals/managers.py"", line 1925, in _check_comparison_types
    raise TypeError(
TypeError: Cannot compare types 'ndarray(dtype=int64)' and 'str'

Process finished with exit code 1

```
#### Expected Output

The original DataFrame.

If at least one column is from dtype object, the replace works, for example the following works:

```python
import pandas as pd

df = pd.DataFrame({""Y0"": [1, ""2""], ""Y1"": [3, 4]})
df = df.replace({""replace_string"": ""test""})
```

Unexpectedly the following Code works:

```python
import pandas as pd

df = pd.DataFrame({""Y0"": [1, 2], ""Y1"": [3, 4]})
df = df.replace(to_replace=""replace_string"", value=""test"")
```

This is probably related to #16784

Tested on master.
"
687727698,35945,CLN: UserWarning in `pandas/plotting/_matplotlib/core.py`,fangchenli,closed,2020-08-28T05:37:56Z,2020-09-05T03:33:50Z,"```
pandas/tests/plotting/test_frame.py: 10 warnings
pandas/tests/plotting/test_series.py: 5 warnings
  /home/fangchenli/Workspace/pandas-VirosaLi/pandas/plotting/_matplotlib/core.py:1235: UserWarning: FixedFormatter should only be used together with FixedLocator
    ax.set_xticklabels(xticklabels)
```
"
692090857,36093,BUG: df.replace with numeric values and str to_replace,jbrockmendel,closed,2020-09-03T15:57:21Z,2020-09-05T03:41:43Z,"- [x] closes #34789
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

We also avoid copies by not calling `self.as_array` and instead moving the mask-finding to the block level."
693156522,36114,REGR: fix consolidation/cache issue with take operation,jorisvandenbossche,closed,2020-09-04T13:14:06Z,2020-09-05T06:04:33Z,"Closes #35521


@jbrockmendel the ""short term"" fix for 1.1.2"
679603837,35739,BUG: Invalid cache invalidation leading to assertion errors later,veselyvojtech,closed,2020-08-15T16:24:25Z,2020-09-05T07:27:53Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
In [44]: df = pd.DataFrame({""a"": [3, 4]})

In [45]: df
Out[45]: 
   a
0  3
1  4

In [46]: mask = df[""a""] > 0

In [47]: mask
Out[47]: 
0    True
1    True
Name: a, dtype: bool

In [48]: df_a = df.loc[:, ""a""]

In [49]: df_a[""a""] = 1

In [50]: df_a
Out[50]: 
0    3
1    4
a    1
Name: a, dtype: int64

In [51]: df.__dict__
Out[51]: 
{'_is_copy': None,
 '_mgr': BlockManager
 Items: Index(['a'], dtype='object')
 Axis 1: RangeIndex(start=0, stop=2, step=1)
 IntBlock: slice(0, 1, 1), 1 x 2, dtype: int64,
 '_item_cache': {'a': 0    3
  1    4
  a    1
  Name: a, dtype: int64},
 '_attrs': {}}

In [52]: mask
Out[52]: 
0    True
1    True
Name: a, dtype: bool

In [53]: df
Out[53]: 
   a
0  3
1  4

In [54]: df.loc[mask, ""a""]
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-54-e9cf788ed9f4> in <module>
----> 1 df.loc[mask, ""a""]

~/miniconda3/envs/conduit-poetry/lib/python3.8/site-packages/pandas/core/indexing.py in __getitem__(self, key)
    871                     # AttributeError for IntervalTree get_value
    872                     pass
--> 873             return self._getitem_tuple(key)
    874         else:
    875             # we by definition only have the 0th axis

~/miniconda3/envs/conduit-poetry/lib/python3.8/site-packages/pandas/core/indexing.py in _getitem_tuple(self, tup)
   1053             return self._multi_take(tup)
   1054 
-> 1055         return self._getitem_tuple_same_dim(tup)
   1056 
   1057     def _get_label(self, label, axis: int):

~/miniconda3/envs/conduit-poetry/lib/python3.8/site-packages/pandas/core/indexing.py in _getitem_tuple_same_dim(self, tup)
    751             # We should never have retval.ndim < self.ndim, as that should
    752             #  be handled by the _getitem_lowerdim call above.
--> 753             assert retval.ndim == self.ndim
    754 
    755         return retval

AssertionError: 


```

#### Problem description

By manipulating one column that was created by copying a part of the DataFrame, the cache for this one column is updated in the DataFrame. However, the cached data are not compatible with the DataFrame index and causes problems later, e.g. the AssertionError from the example.

#### Expected Output
Manipulating separate object would not influence the original object in a way that leads to AssertionError.

#### Output of ``pd.show_versions()``
<details>

```
INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.7.14-200.fc32.x86_64
Version          : #1 SMP Fri Aug 7 23:16:37 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : None
pytest           : 6.0.1
hypothesis       : None
sphinx           : None
blosc            : 1.9.1
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.17.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.0
fastparquet      : None
gcsfs            : 0.6.2
matplotlib       : None
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : 0.11.0
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
```
</details>
"
691918821,36089,STY: add code check for use of builtin filter function,simonjayhawkins,closed,2020-09-03T12:15:40Z,2020-09-05T07:30:48Z,xref https://github.com/pandas-dev/pandas/pull/35717#issuecomment-674051885
692167731,36098,TYP: misc fixes for numpy types,simonjayhawkins,closed,2020-09-03T18:00:13Z,2020-09-05T07:31:32Z,"```
pandas/core/dtypes/cast.py:1491: error: Incompatible return value type (got ""Type[object]"", expected ""Union[dtype, ExtensionDtype]"")  [return-value]
pandas/core/dtypes/cast.py:1553: error: Incompatible types in assignment (expression has type ""Type[object]"", variable has type ""Union[dtype, ExtensionDtype]"")  [assignment]
pandas/core/construction.py:601: error: Incompatible default for argument ""dtype_if_empty"" (default has type ""Type[object]"", argument has type ""Union[ExtensionDtype, str, dtype, Type[str], Type[float], Type[int], Type[complex], Type[bool]]"")  [assignment]
pandas/core/generic.py:6217: error: Argument ""dtype_if_empty"" to ""create_series_with_explicit_dtype"" has incompatible type ""Type[object]""; expected ""Union[ExtensionDtype, str, dtype, Type[str], Type[float], Type[int], Type[complex], Type[bool]]""  [arg-type]
pandas/tests/arrays/sparse/test_dtype.py:97: error: Argument 1 to ""SparseDtype"" has incompatible type ""Type[object]""; expected ""Union[ExtensionDtype, str, dtype, Type[str], Type[float], Type[int], Type[complex], Type[bool]]""  [arg-type]
pandas/tests/arrays/sparse/test_dtype.py:172: error: Argument 1 to ""SparseDtype"" has incompatible type ""Type[object]""; expected ""Union[ExtensionDtype, str, dtype, Type[str], Type[float], Type[int], Type[complex], Type[bool]]""  [arg-type]
pandas/tests/arrays/sparse/test_combine_concat.py:44: error: Argument 1 to ""SparseDtype"" has incompatible type ""Type[object]""; expected ""Union[ExtensionDtype, str, dtype, Type[str], Type[float], Type[int], Type[complex], Type[bool]]""  [arg-type]
pandas/tests/arrays/sparse/test_array.py:561: error: Argument 1 to ""SparseDtype"" has incompatible type ""Type[object]""; expected ""Union[ExtensionDtype, str, dtype, Type[str], Type[float], Type[int], Type[complex], Type[bool]]""  [arg-type]
```"
694010292,36135,Backport PR #36114 on branch 1.1.x (REGR: fix consolidation/cache issue with take operation),meeseeksmachine,closed,2020-09-05T06:05:41Z,2020-09-05T07:56:14Z,Backport PR #36114: REGR: fix consolidation/cache issue with take operation
690774484,36061,BUG: groupby and agg on read-only array gives ValueError: buffer source array is read-only,jeet-parekh,closed,2020-09-02T07:00:58Z,2020-09-05T08:26:11Z,"- [x] closes #36014 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I found multiple aggregate functions that were failing for a read-only array. I applied the change suggested in #36014 to all of them.

A couple of questions.

1. I had to add the Py_ssize_t cdefs to suppress the error below for `len(values) != len(labels)`. Let me know if there is another preferred way of doing it.

```
error: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘size_t’ {aka ‘long unsigned int’} [-Werror=sign-compare]
```

2. I see some functions don't have the `len(values) != len(labels)` check. Should it be there in all the functions?"
691911295,36088,TYP: activate Check for missing error codes,simonjayhawkins,closed,2020-09-03T12:05:01Z,2020-09-05T10:10:50Z,xref #35311
693520749,36125,DOC: add missing data handling info to pd.Categorical docstring,arw2019,closed,2020-09-04T19:21:33Z,2020-09-05T10:50:58Z,"- [x] closes #35162 
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
652496337,35162,ENH: should pandas.core.arrays.Categorical have a dropna=False option?,arw2019,closed,2020-07-07T17:30:41Z,2020-09-05T10:50:58Z,"I ran into this while working on #35078. Here's a simple reproducer:
```
In [1]: from pandas.core.arrays import Categorical
In [2]: Categorical([1, 2, 3,4,4,4, None, None])                                                                                                                                                    
Out[2]: 
[1, 2, 3, 4, 4, 4, NaN, NaN]
Categories (4, int64): [1, 2, 3, 4]
```

Do people think it's worth having a `dropna` argument for Categorical, so that one could do:
```
In [2]: Categorical([1, 2, 3,4,4,4, None, None], dropna=True)                                                                                                                                                    
Out[2]: 
[1, 2, 3, 4, 4, 4, NaN, NaN]
Categories (4, int64): [1, 2, 3, 4, NaN]
```
If we set `dropna=False` by default presumably there shouldn't be backward compatibility issues.

I can see arguments either way! In case we do want to add this I'm happy work on it (and the solution in #35078 will be a lot cleaner)"
692276152,36101,CLN removing trailing commas,sarthakvk,closed,2020-09-03T19:56:42Z,2020-09-05T12:39:23Z,"This PR is related to #35925, both current and new versions of black passes these formatting,
also I'd like to mention  that there seem to be a reported [bug](https://github.com/psf/black/issues/1629) in newer version of black which throws error after removing trailing comma from  `pandas/tests/scalar/timestamp/test_constructors.py` that's why I didn't change this formatting.
```diff
      diff --git a/pandas/tests/scalar/timestamp/test_constructors.py b/pandas/tests/scalar/timestamp/test_constructors.py
index 316a299ba..c70eacdfa 100644
--- a/pandas/tests/scalar/timestamp/test_constructors.py
+++ b/pandas/tests/scalar/timestamp/test_constructors.py
@@ -267,7 +267,7 @@ class TestTimestampConstructors:
                 hour=1,
                 minute=2,
                 second=3,
-                microsecond=999999,
+                microsecond=999999
             )
         ) == repr(Timestamp(""2015-11-12 01:02:03.999999""))
 
~
```
The workaround to this is after removing trailing comma from above code run black with --fast which will add trailing comma back again and and will not format afterwards on re-running black without --fast"
693627157,36129,CLN: remove unused args/kwargs,jbrockmendel,closed,2020-09-04T21:02:11Z,2020-09-05T14:08:13Z,
693692871,36130,CLN: De-privatize,jbrockmendel,closed,2020-09-04T21:47:24Z,2020-09-05T14:10:44Z,
665274277,35406,BUG: Creating a Series from a Series results in all NaN values,bashtage,closed,2020-07-24T16:17:08Z,2020-09-05T14:44:27Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import numpy as np
import pandas as pd

pd.Series(pd.Series(np.random.randn(100)),index=pd.date_range(""1980-1-1"",periods=100,freq=""D""))
```

output

```
1980-01-01   NaN
1980-01-02   NaN
1980-01-03   NaN
...
```

#### Problem description

Series accepts array-like, and Series is array-like.

#### Expected Output
A Series with the same values and my new index.
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : None
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: v0.10.0dev0+6.g642bd7b
bs4              : 4.9.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.1
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : None
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : None
numba            : 0.50.1


</details>
"
322038519,21003,pandas plotting raises ValueError on style strings that should be valid according to spec.,Austrianguy,closed,2018-05-10T18:42:02Z,2020-09-05T14:49:10Z,"#### Code Sample

```python
import pandas as pd
from numpy.random import random
import matplotlib.pyplot as plt

data = random((7,4))
fmt = 'd'

plt.plot(data, fmt, color='green') # works as expected (no error)

df = pd.DataFrame(data)
df.plot(color='green', style=fmt)

# The previous line raises ValueError:
# Cannot pass 'style' string with a color symbol and 'color' keyword argument.
# Please use one or the other or pass 'style' without a color symbol
```

#### Full Stack Trace
```python
Traceback (most recent call last):

  File ""<ipython-input-178-b2322b55aff5>"", line 1, in <module>
    runfile('G:/irreplacable stuff/Scripts/playground/playground.py', wdir='G:/irreplacable stuff/Scripts/playground')

  File ""C:\ProgramData\Anaconda2\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Anaconda2\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 87, in execfile
    exec(compile(scripttext, filename, 'exec'), glob, loc)

  File ""G:/irreplacable stuff/Scripts/playground/playground.py"", line 30, in <module>
    df.plot(color='green', style='d')

  File ""C:\ProgramData\Anaconda2\lib\site-packages\pandas\plotting\_core.py"", line 2677, in __call__
    sort_columns=sort_columns, **kwds)

  File ""C:\ProgramData\Anaconda2\lib\site-packages\pandas\plotting\_core.py"", line 1902, in plot_frame
    **kwds)

  File ""C:\ProgramData\Anaconda2\lib\site-packages\pandas\plotting\_core.py"", line 1727, in _plot
    plot_obj = klass(data, subplots=subplots, ax=ax, kind=kind, **kwds)

  File ""C:\ProgramData\Anaconda2\lib\site-packages\pandas\plotting\_core.py"", line 931, in __init__
    MPLPlot.__init__(self, data, **kwargs)

  File ""C:\ProgramData\Anaconda2\lib\site-packages\pandas\plotting\_core.py"", line 182, in __init__
    self._validate_color_args()

  File ""C:\ProgramData\Anaconda2\lib\site-packages\pandas\plotting\_core.py"", line 215, in _validate_color_args
    ""Cannot pass 'style' string with a color ""

ValueError: Cannot pass 'style' string with a color symbol and 'color' keyword argument. Please use one or the other or pass 'style' without a color symbol
```
#### Problem description

`df.plot` should just pass the `style` kwarg as `fmt` arg to `matplotlib.axes.plot` but it does does some extra (sloppy) validation where it thinks that valid marker style symbols are color symbols and raises an error if the color is already defined elsewhere. The problem is clearly in `_core.py`, line 213.

This problem affects the following standard marker styles (key corresponds to `m` in the example code):
```python
{u'd': u'thin_diamond',
 u'h': u'hexagon1',
 u'o': u'circle',
 u'p': u'pentagon',
 u's': u'square',
 u'v': u'triangle_down',
 u'x': u'x'}
```

#### Expected Output
`df.plot(color='green', style=fmt)` should simply plot the plot the same way as `plt.plot(data, fmt, color='green')` without raising errors. All legal values for `fmt` arg in `pyplot.plot` should be legal for the `style` kwarg in `df.plot`.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.14.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 45 Stepping 7, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en
LOCALE: None.None

pandas: 0.22.0
pytest: 3.5.0
pip: 9.0.3
setuptools: 39.0.1
Cython: 0.28.2
numpy: 1.14.2
scipy: 1.0.1
pyarrow: None
xarray: None
IPython: 5.6.0
sphinx: 1.7.2
patsy: 0.5.0
dateutil: 2.7.2
pytz: 2018.4
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 2.2.2
openpyxl: 2.5.2
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.4
lxml: 4.2.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.6
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
521449807,29570,In-memory to_pickle leads to I/O error,reidhin,closed,2019-11-12T10:06:31Z,2020-09-05T14:50:04Z,"#### Code Sample
```
# import libraries
import pandas as pd
import io

# show version
print(pd.__version__)
# 0.25.2

# create example dataframe
df = pd.DataFrame({""A"": [1, 2, 3, 4], ""B"": [5, 6, 7, 8]})

# create io-stream to act as surrogate file
stream = io.BytesIO()

# since the compression cannot be inferred from the filename, it has to be set explicitly.
df.to_pickle(stream, compression=None)

# stream.getvalue() can be used as binary load in an api call to save the dataframe in the cloud
print(stream.getvalue())
'''
correct output pandas version 0.24.1:
b'\x80\x04\x95\xe2\x02\x00\x00\x00\x00\x00\x00\x8c\x11pandas.core.frame\x94\x8c\tDataFrame\x94\x93\x94)\x81\x94}\x94(\x8c\x05_data\x94\x8c\x1epandas.core.internals.managers\x94\x8c\x0cBlockManager\x94\x93\x94)\x81\x94(]\x94(\x8c\x18pandas.core.indexes.base\x94\x8c\n_new_Index\x94\x93\x94h\x0b\x8c\x05Index\x94\x93\x94}\x94(\x8c\x04data\x94\x8c\x15numpy.core.multiarray\x94\x8c\x0c_reconstruct\x94\x93\x94\x8c\x05numpy\x94\x8c\x07ndarray\x94\x93\x94K\x00\x85\x94C\x01b\x94\x87\x94R\x94(K\x01K\x02\x85\x94h\x15\x8c\x05dtype\x94\x93\x94\x8c\x02O8\x94K\x00K\x01\x87\x94R\x94(K\x03\x8c\x01|\x94NNNJ\xff\xff\xff\xffJ\xff\xff\xff\xffK?t\x94b\x89]\x94(\x8c\x01A\x94\x8c\x01B\x94et\x94b\x8c\x04name\x94Nu\x86\x94R\x94h\r\x8c\x19pandas.core.indexes.range\x94\x8c\nRangeIndex\x94\x93\x94}\x94(h(N\x8c\x05start\x94K\x00\x8c\x04stop\x94K\x04\x8c\x04step\x94K\x01u\x86\x94R\x94e]\x94h\x14h\x17K\x00\x85\x94h\x19\x87\x94R\x94(K\x01K\x02K\x04\x86\x94h\x1e\x8c\x02i8\x94K\x00K\x01\x87\x94R\x94(K\x03\x8c\x01<\x94NNNJ\xff\xff\xff\xffJ\xff\xff\xff\xffK\x00t\x94b\x89C@\x01\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00\x05\x00\x00\x00\x00\x00\x00\x00\x06\x00\x00\x00\x00\x00\x00\x00\x07\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x94t\x94ba]\x94h\rh\x0f}\x94(h\x11h\x14h\x17K\x00\x85\x94h\x19\x87\x94R\x94(K\x01K\x02\x85\x94h!\x89]\x94(h%h&et\x94bh(Nu\x86\x94R\x94a}\x94\x8c\x060.14.1\x94}\x94(\x8c\x04axes\x94h\n\x8c\x06blocks\x94]\x94}\x94(\x8c\x06values\x94h7\x8c\x08mgr_locs\x94\x8c\x08builtins\x94\x8c\x05slice\x94\x93\x94K\x00K\x02K\x01\x87\x94R\x94uaust\x94b\x8c\x04_typ\x94\x8c\tdataframe\x94\x8c\t_metadata\x94]\x94ub.'

erroneous output pandas version 0.25.2:
Traceback (most recent call last):
  File ""C:/Projects/bug_report/report_bug.py"", line 20, in <module>
    print(stream.getvalue())
ValueError: I/O operation on closed file.
'''
```

#### Problem description

Occasionally I would like to save Pandas dataframes in the cloud. This can be done through api-calls in which the dataframe is uploaded as binary content. The binary content can be created through providing a io.BytesIO stream to the pandas.to_pickle method. Subsequently the binary content can be obtained by the method getvalue from io.BytesIO. This works perfectly in pandas version 0.24.1. However, when updating to pandas version 0.25.2, this ceases to work. Apparently the io.BytesIO stream gets now closed in the pandas.to_pickle method and can no longer be accessed.

#### Expected Output
A binary string as produced in pandas version 0.24.1, see code example above

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 0.25.2
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.6.0.post20191030
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
None

</details>
"
508561782,29054,ENH: Allow read_pickle to accept file-like objects,mar-ses,closed,2019-10-17T15:38:30Z,2020-09-05T14:50:04Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
import portalocker

with portalocker.Lock(""/path/to/file.pickle"", ""rb+"", timeout=60) as file:
    file.seek(0)        # not sure why I had this from before but it shouldn't cause an issue right?
    df = pd.read_pickle(file)

    # do stuff to df

    file.seek(0)
    file.truncate()   # I think here I was trying to erase the previous file just in case because of the read-mode
    file.to_pickle(file)

```

Alternatively, a more concise, self-contained, reproducible example:

```python
import portalocker
import pandas as pd

df = pd.DataFrame({'a':[1,2,3], 'b':[1,2,3]})
path = 'test.pickle'
df.to_pickle(path)

with portalocker.Lock(path, ""rb+"", timeout=60) as file:
    file.seek(0)
    df = pd.read_pickle(file)
```

Replace `portalocker.Lock` with `open(path, 'rb+')` and you get the same thing.

#### Problem description

The above used to work. However, having updated to the latest pandas version recently (0.25.1), it now produces an error on the `pd.read_pickle`, and based on other issues, it will also produce an error on `df.to_pickle()` if it gets that far. Am I doing it wrong or should I do it differently, or it is just a bug?

The error traceback is:

```
    df = pd.read_pickle(file)
  File ""/HOME/anaconda3/lib/python3.6/site-packages/pandas/io/pickle.py"", line 145, in read_pickle
    f, fh = _get_handle(path, ""rb"", compression=compression, is_text=False)
  File ""HOME/anaconda3/lib/python3.6/site-packages/pandas/io/common.py"", line 392, in _get_handle
    raise ValueError(msg)
ValueError: Unrecognized compression type: infer
```



#### Expected Output

#### Output of ``pd.show_versions()``

<details>

This didn't work as it couldn't determine the version for pip for some reason (I have pip, I'm not sure what's going on here), but the pandas version I'm running is: 0.25.1

On my other machine, which also has the error:

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.18.16-041816-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.25.1
numpy            : 1.14.3
pytz             : 2018.4
dateutil         : 2.7.3
pip              : 19.1.1
setuptools       : 39.1.0
Cython           : 0.28.2
pytest           : 3.5.1
hypothesis       : None
sphinx           : 1.7.4
blosc            : None
feather          : None
xlsxwriter       : 1.0.4
lxml.etree       : 4.2.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 6.4.0
pandas_datareader: None
bs4              : 4.6.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.1
matplotlib       : 3.1.1
numexpr          : 2.6.5
odfpy            : None
openpyxl         : 2.5.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : 1.2.7
tables           : 3.4.3
xarray           : None
xlrd             : 1.1.0
xlwt             : 1.3.0
xlsxwriter       : 1.0.4

</details>
"
438282803,26237,to_pickle compression does not work with in-memory buffers,akhmerov,closed,2019-04-29T12:06:45Z,2020-09-05T14:50:04Z,"#### Code Sample, a copy-pastable example if possible

```python
from io import BytesIO
import pandas
pandas.DataFrame([[]]).to_pickle(BytesIO(), compression=None)  # works
pandas.DataFrame([[]]).to_pickle(BytesIO())
# ValueError: Unrecognized compression type: infer (regression in 0.24 from 0.23)
pandas.DataFrame([[]]).to_pickle(BytesIO(), compression='zip')
# AttributeError: 'NoneType' object has no attribute 'find' (in 0.24)
# BadZipFile: File is not a zip file (in 0.22 and before)
```
#### Problem description

#22555 is closely related, but I believe this is a different issue because the errors occur at a different place in the code.

I believe the above is an issue because

- Despite the argument name is ""path"" and the docstring reads `path : string   File path`, the code contains multiple `path_or_buf` names. I'd be happy to make a PR amending the docstring if anybody confirms that the docstring is not precise.
- The code above is actually useful (I want to let the user export a dataframe from a webapp)
- `compression='infer'` failing is a regression

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.1.final.0
python-bits: 64
OS: Linux
OS-release: 5.0.0-13-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 4.4.1
pip: 19.1
setuptools: 41.0.1
Cython: 0.29.7
numpy: 1.16.3
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 7.5.0
sphinx: None
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.3
openpyxl: None
xlrd: 1.2.0
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
694094940,36140,TYP: remove string literals for type annotations in pandas\core\frame.py,simonjayhawkins,closed,2020-09-05T14:17:16Z,2020-09-05T15:09:24Z,
694073309,36137,"TYP: Check for use of Union[Series, DataFrame] instead of FrameOrSeriesUnion alias",simonjayhawkins,closed,2020-09-05T12:11:57Z,2020-09-05T15:10:22Z,
545257196,30677,BUG: Index.isin inconsistency with missing value,proost,closed,2020-01-04T08:14:14Z,2020-09-05T15:26:48Z,"DataFrame case, 
```python
In [15]: df = pd.DataFrame({'num_legs': [None, 4], 'num_wings': [np.nan, 0]},ind
    ...: ex=['falcon', 'dog'])                                                  

In [16]: df.isin([np.nan, None])                                                
Out[16]: 
        num_legs  num_wings
falcon      True       True
dog        False      False

In [17]: df.isin([None, None])                                                  
Out[17]: 
        num_legs  num_wings
falcon      True       True
dog        False      False

In [18]: df.isin([np.nan, np.nan])                                              
Out[18]: 
        num_legs  num_wings
falcon      True       True
dog        False      False

```
But Index case,

```
In [20]: idx = Index([np.nan,'a','b', None])                                    

In [21]: idx.isin([None])                                                       
Out[21]: array([False, False, False,  True])

In [22]: idx.isin([np.nan])                                                     
Out[22]: array([ True, False, False, False])

```
#### Problem description

'.isin' handle NA values differently depending on object types. 'np.nan' and 'None' both are NA value. so, 'Index.isin' do not distinguish 'np.nan' and 'None' like 'DataFrame' case.


#### Expected Output

```
In [20]: idx = Index([np.nan,'a','b', None])                                    

In [21]: idx.isin([None])                                                       
Out[21]: array([True, False, False,  True])

In [22]: idx.isin([np.nan])                                                     
Out[22]: array([ True, False, False, True])

```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.0.0-37-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : ko_KR.UTF-8
LOCALE           : ko_KR.UTF-8

pandas           : 0.25.1
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.2.1
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.1
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.9
tables           : 3.5.2
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.1


</details>
"
491892748,28376,How to fix your data returning Nan instead of float values in Python 3 for categorical data,ghost,closed,2019-09-10T20:54:37Z,2020-09-05T15:40:32Z,"Just a quick one. I am dealing with categorical data. 

After grouping by, I tried to see what the data looks like and this is what I get. what can I do?

    import pandas as pd
    import matplotlib.pyplot as plt
    import numpy as np

    pd.set_option('display.max_rows', 500)
    pd.set_option('display.max_columns', 500)
    pd.set_option('display.width', 1000)

    happiness_dataset= pd.read_csv('happiness-cantril-ladder.csv')

    ### Lets explore the dataset ####
    happiness_dataset.info()

    happiness_dataset.describe()

    happiness_dataset.columns = ['Country','Country_code','Year','Happiness_level']  ### rename the columns

    happiness_dataset['Happiness_level'] = pd.to_numeric(happiness_dataset['Happiness_level'],errors='coerce')

    happiness_dataset.info()


    groupby_country_v= pd.DataFrame(happiness_dataset.groupby('Country')['Happiness_level'].mean())

    print(groupby_country_v)

This is what I get when I print the groupby_country_v

                                  Happiness_level
    Country
    Afghanistan                               NaN
    Albania                                   NaN
    Algeria                                   NaN
    Angola                                    NaN
    Argentina                                 NaN
    Armenia                                   NaN
    Australia                                 NaN
    Austria                                   NaN
    Azerbaijan                                NaN
    Bahrain                                   NaN
    Bangladesh                                NaN
    Belarus                                   NaN
    Belgium                                   NaN
    Belize                                    NaN
    Benin                                     NaN
    Bhutan                                    NaN
    Bolivia                                   NaN
    Bosnia and Herzegovina                    NaN
    Botswana                                  NaN
    Brazil                                    NaN
    Bulgaria                                  NaN
    Burkina Faso                              NaN
    Burundi                                   NaN
"
691458403,36078,BUG: Unary pos/neg ops on IntegerArrays failing with TypeError,asishm,closed,2020-09-02T22:02:12Z,2020-09-05T15:56:29Z,"- [ ] closes #36063
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
679592746,35736,BUG/ENH: to_pickle/read_pickle support compression for file ojects,twoertwein,closed,2020-08-15T15:24:31Z,2020-09-05T17:47:58Z,"- [x] closes #26237, closes #29054, and closes #29570
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This was basically already supported but `to/read_pickle` set `compression` to `None` for file objects.

Some functions called `get_filepath_or_buffer` (might convert a string to a file object) before calling `infer_compression` (doesn't work with file objects). I moved `infer_compression` and `get_compression_method` inside `get_filepath_or_buffer`."
694102873,36143,CLN: unused case in compare_or_regex_search,jbrockmendel,closed,2020-09-05T15:00:00Z,2020-09-05T18:49:02Z,"
"
606633801,33780,API: Should indexing on columns give views or copies?,jbrockmendel,closed,2020-04-24T23:21:34Z,2020-09-05T19:03:15Z,"Migrating the discussion from #33597:

We can improve performance (and memory footprint) by returning views in more cases.  The downsides are 1) the returned DataFrames are less-consolidated and 2) users may be relying on getting copies in some situations."
607657521,33823,API: copy(deep=False) -> view?,jbrockmendel,closed,2020-04-27T15:50:23Z,2020-09-05T19:03:23Z,"These should behave the same, shouldn't they?  If so, we should use the more standard usage (view).

Ditto for `_shallow_copy()` with no args."
634978213,34657,PERF: don't consolidate at __init__,jbrockmendel,closed,2020-06-08T22:34:53Z,2020-09-05T19:03:39Z,"xref mailing list [discussion](https://mail.python.org/pipermail/pandas-dev/2020-June/001246.html)

ATM most calls to `DataFrame(foo)` will end up going through either `core.internals.managers.create_manager_from_blocks` or `core.internals.managers.create_block_manager_from_arrays`, both of which call `mgr._consolidate_inplace()` before returning a `BlockManager` object.  I think we should consider disabling this consolidation.  (There is also consolidation-by-another-name within form_blocks that im still tracking down)

Behavior-wise, consider:

```
arr = np.random.randn(10)
ser = pd.Series(arr)

df = pd.DataFrame({""A"": arr, ""B"": ser})
```

ATM we get a new 2x10 array backing `df`.  The proposed change would mean that we keep 2 separate arrays, and `df[""B""]` shares data with `ser`.  I find the proposed behavior more intuitive.

Performance-wise, we expect faster construction but slower subsequent operations. [note to self: get some estimates/measurements here].  For long-lived DataFrames where the subsequent slowdowns add up, we can suggest users call `_consolidate()` (which i guess we would have to de-privatize)
"
540559538,30360,DISC/REF: split frame.py and generic.py into smaller files,jbrockmendel,closed,2019-12-19T20:53:51Z,2020-09-05T19:03:49Z,"These files are big enough that GH has trouble displaying a blame.  I think we should consider splitting pieces of these off into other files.  Brainstorming:

- `_make_logical_function`, `_make_cum_function`, `_make_stat_function_ddof`, `_make_stat_function`, `_make_min_count_stat_function` at the bottom of generic could have their own file, maybe `ops.unary`
- shared_docs could go in a dedicated file (this might also avoid some circular import issues IIRC)
- IO pass-through methods could go into mixin classes (xref #30350 motivating this idea)
  - to_json, to_hdf, to_sql, to_pickle, to_clipboard, to_xarray, to_latex, to_csv make up ~1200 lines of generic
  - to_gbq, to_stata, to_feather, to_parquet, to_html make up ~450 lines of frame.py

Thoughts?"
516642505,29361,TYPING: scalar type that matches lib.is_scalar,jbrockmendel,closed,2019-11-02T16:12:39Z,2020-09-05T19:03:57Z,"ATM pandas._typing has a `Scalar = Union[str, int, float, bool]`.  It would be helpful to have a Scalar type that reflected the behavior of `lib.is_scalar`

"
512082640,29208,REF: Dependency Structure Simplification,jbrockmendel,closed,2019-10-24T17:34:19Z,2020-09-05T19:04:03Z,"This is intended to describe a) possible refactors that would simplify the intra-pandas dependency structure, b) the benefits we would see from implementing these refactors, and c) the difficulties each would entail.

<b>Straightforward Circular Dependencies</b>
- [ ] imports from `pd.core.util.hashing` in `pd.util.__init__`
  Fix: Just remove that import, difficulty depends on if that needs a deprecation cycle.
- [ ] runtime import of `tseries.frequencies.to_offset` into `tslibs`
  Fix: Move most of tseries.offsets and about half of tseries.frequencies into tslibs
    - `tseries.offsets.generate_range` is the only part of `tseries.offsets` that is not needed; it can go to `arrays._ranges`, which is the only place where it is used
    - `tslibs.offsets` would become a very big module unless it were split into smaller pieces
    - `cache_readonly` is needed by some classes in `tseries.offsets` (which would be moved to `tslibs.offsets`), would need to put somewhere ""upstream"" of tslibs.
- [x] `_libs.internals` runtime import of `Int64Index`
   - For `BlockPlacement.isin`, which is only used in one place in `core.internals`, the `isin` method could be refactored out of the class.
   - This one is really only worthwhile if we expect to implement tests/benchmarks specific to `_libs.internals` 
- [x] runtime import of `pd.unique` in `IntervalTree.get_loc_interval`
   - I _think_ its the case that the argument passed to pd.unique is always int64 (@jschendel is this accurate?).  If so, implementing a `unique_int64` in either `_libs.hashtables` or `_libs.algos` takes about 3 lines, could be a nice fastpath for a very common case.

<b>core.dtypes</b>
- `core.dtypes.common` is used all over the place.  It would be really nice if its dependencies were simple.  But:
   - it imports `CategoricalDtype`, which has runtime imports from `pandas.core.util.hashing`, in particular functions that have runtime imports of `Categorical`, `Index`, `MultiIndex`, and `factorize`
   - `algorithms.factorize` has runtime imports from `pandas.core.sorting`, which has a dependency on `algorithms` and runtime import of `Index`.
   - I _think_ the ""heavy"" dependencies on `Categorical`/`Index`/`MultiIndex` are mostly just for the constructors and could be refactored into much lighter-weight implementations.  I'm imagining something like `core.factorization` that depended only on `_libs` and maybe a few of the `is_foo_dtype` functions (the latter would still be circular, but a much smaller circle)
"
589693331,33111,Bug 29764 groupby loses index name sometimes,phofl,closed,2020-03-29T01:19:03Z,2020-09-05T19:04:06Z,"- [x] closes #29764
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry´

Some group by functions lost the column index name. For the functions running into ```_get_cythonized_result``` in groupby.py, the index name was just ignored when defining the column names. So the following functions ```_wrap_aggregated_output``` and ```_wrap_transformed_output``` in the class DataFrameGroupBy had not acces to this information, because it was already lost there. I collected the information beforehand and defined the Index Name accordingly.

We could refactor both methods a bit at a later stage, because at it is right now (and was before) the first few lines are duplicates."
693376581,36121,Bug 29764 groupby loses index name sometimes,phofl,closed,2020-09-04T16:42:54Z,2020-09-05T19:04:08Z,"- [x] closes #29764
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry´

Issue was fixed for sum and any with c9144ca54dcc924995acae3d9dcb890a5802d7c0.

So the only thing left was to add the index name in `_wrap_transformed_output`. I kept the tests from #33111, to show, that alle cases are now handled correctly.

I hope it is okay, to add a new PR for this?"
466061384,27316,EA: Make validate_fill_value part of interface,jbrockmendel,closed,2019-07-10T02:14:53Z,2020-09-05T19:04:25Z,"We defined validate_fill_value for DTA/TDA/PA and I'm thinking it may be worth requiring more generally and using on `ExtensionBlock`.  In particular, `ExtensionBlock._can_hold_element` ATM unconditionally returns `True`.  _can_hold_element isn't well-documented, but my intuition as to what it _should_ mean more or less matches what _validate_fill_value means."
467050242,27344,INT: avoid accessing self.loc._foo outside of indexing.py,jbrockmendel,closed,2019-07-11T18:47:30Z,2020-09-05T19:04:31Z,"grepping for `.loc.` and `.iloc.` I see two uses of `self.loc._convert_to_indexer` and two uses of `self.loc._setitem_with_indexer` in core.frame, and two uses of `obj.loc._getitem_axis` in `io.pytables`"
592927171,33247,BUG: Fix droped result column in groupby with as_index False,phofl,closed,2020-04-02T21:27:15Z,2020-09-05T19:04:49Z,"- [x] closes #32240
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The aggregate function *lost* the results of the aggregate operation, if the ```as_index=False``` flag was set and one of the result columns is relabeled. The index was reset at first, adding columns to the DataFrame. The relabeling operation only took the columns before resetting the index as input. So the weird error occurred. I changed the order of the index reset and the relabeling. This fixes the issue."
376206806,23437,ENH: empty() method for EA/Series/Index,jbrockmendel,closed,2018-11-01T00:15:40Z,2020-09-05T19:05:15Z,"There are a few places where we need an empty array-like of the same type+dtype as self.  e.g. in #23431 I used `self._data[:0]`.  A dedicated `empty()` method might pretty that up.

https://github.com/pandas-dev/pandas/pull/23431#discussion_r229905762

"
379548481,23624,Why does DatetimeIndex have _timezone in addition to _tz?,jbrockmendel,closed,2018-11-11T18:19:10Z,2020-09-05T19:05:37Z,"Name seems redundant.  Can we document why it exists?

ref https://github.com/pandas-dev/pandas/pull/23587#issuecomment-437687293"
246538799,17112,MultiIndex - Comparison with Mixed Frequencies (and other FUBAR),jbrockmendel,closed,2017-07-29T18:55:02Z,2020-09-05T19:05:57Z,"Setup:

```
index = pd.Index(['PCE']*4, name='Variable')
data = [
	pd.Period('2018Q2'),
	pd.Period('2021', freq='5A-Dec'),
	pd.Period('2026', freq='10A-Dec'),
	pd.Period('2017Q2')
	]
ser = pd.Series(data, index=index, name='Period')
```

In the real-life version of this issue, 'Period' is a column in a DataFrame and I need to append it as a new level to the index.  The snippets here show the problem(s) in both py2 and py3, but for reasons unknown `df.set_index('Period', append=True)` goes through fine in py2.

The large majority of Period values are quarterly-frequency.

py2
```
>>> pd.__version__
'0.20.2'
>>> ser.sort_values()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/site-packages/pandas/core/series.py"", line 1710, in sort_values
    argsorted = _try_kind_sort(arr[good])
  File ""/usr/local/lib/python2.7/site-packages/pandas/core/series.py"", line 1696, in _try_kind_sort
    return arr.argsort(kind=kind)
  File ""pandas/_libs/period.pyx"", line 725, in pandas._libs.period._Period.__richcmp__ (pandas/_libs/period.c:11842)
pandas._libs.period.IncompatibleFrequency: Input has different freq=10A-DEC from Period(freq=Q-DEC)

>>> ser.to_frame()
         Period
Variable       
PCE      2018Q2
PCE        2021
PGDP       2026
PGDP     2017Q2
>>> ser.to_frame().set_index('Period', append=True)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/site-packages/pandas/core/frame.py"", line 2836, in set_index
    index = MultiIndex.from_arrays(arrays, names=names)
  File ""/usr/local/lib/python2.7/site-packages/pandas/core/indexes/multi.py"", line 1100, in from_arrays
    labels, levels = _factorize_from_iterables(arrays)
  File ""/usr/local/lib/python2.7/site-packages/pandas/core/categorical.py"", line 2193, in _factorize_from_iterables
    return map(list, lzip(*[_factorize_from_iterable(it) for it in iterables]))
  File ""/usr/local/lib/python2.7/site-packages/pandas/core/categorical.py"", line 2165, in _factorize_from_iterable
    cat = Categorical(values, ordered=True)
  File ""/usr/local/lib/python2.7/site-packages/pandas/core/categorical.py"", line 310, in __init__
    raise NotImplementedError(""> 1 ndim Categorical are not ""
NotImplementedError: > 1 ndim Categorical are not supported at this time
```

No idea why it thinks Categorical is relevant here.  That doesn't happen in py3.

For the purposes of `sort_values`, refusing to sort might make sense.  But when all I care about is `set_index`, I'm pretty indifferent to the ordering.

py3
```
>>> pd.__version__
'0.20.2'
>>> ser.sort_values()
pandas._libs.period.IncompatibleFrequency: Input has different freq=Q-DEC from Period(freq=5A-DEC)

During handling of the above exception, another exception occurred:
SystemError: <built-in function isinstance> returned a result with an error set
[...]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/site-packages/pandas/core/series.py"", line 1710, in sort_values
    argsorted = _try_kind_sort(arr[good])
  File ""/usr/local/lib/python3.5/site-packages/pandas/core/series.py"", line 1696, in _try_kind_sort
    return arr.argsort(kind=kind)
  File ""pandas/_libs/period.pyx"", line 723, in pandas._libs.period._Period.__richcmp__ (pandas/_libs/period.c:11713)
  File ""/usr/local/lib/python3.5/site-packages/pandas/tseries/offsets.py"", line 375, in __ne__
    return not self == other
  File ""/usr/local/lib/python3.5/site-packages/pandas/tseries/offsets.py"", line 364, in __eq__
    if isinstance(other, compat.string_types):
SystemError: <built-in function isinstance> returned a result with an error set

>>> ser.to_frame().set_index('Period', append=True)
pandas._libs.period.IncompatibleFrequency: Input has different freq=Q-DEC from Period(freq=5A-DEC)

During handling of the above exception, another exception occurred:
SystemError: <built-in function isinstance> returned a result with an error set
[...]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/site-packages/pandas/core/frame.py"", line 2836, in set_index
    index = MultiIndex.from_arrays(arrays, names=names)
  File ""/usr/local/lib/python3.5/site-packages/pandas/core/indexes/multi.py"", line 1100, in from_arrays
    labels, levels = _factorize_from_iterables(arrays)
  File ""/usr/local/lib/python3.5/site-packages/pandas/core/categorical.py"", line 2193, in _factorize_from_iterables
    return map(list, lzip(*[_factorize_from_iterable(it) for it in iterables]))
  File ""/usr/local/lib/python3.5/site-packages/pandas/core/categorical.py"", line 2193, in <listcomp>
    return map(list, lzip(*[_factorize_from_iterable(it) for it in iterables]))
  File ""/usr/local/lib/python3.5/site-packages/pandas/core/categorical.py"", line 2165, in _factorize_from_iterable
    cat = Categorical(values, ordered=True)
  File ""/usr/local/lib/python3.5/site-packages/pandas/core/categorical.py"", line 298, in __init__
    codes, categories = factorize(values, sort=True)
  File ""/usr/local/lib/python3.5/site-packages/pandas/core/algorithms.py"", line 567, in factorize
    assume_unique=True)
  File ""/usr/local/lib/python3.5/site-packages/pandas/core/algorithms.py"", line 486, in safe_sort
    sorter = values.argsort()
  File ""pandas/_libs/period.pyx"", line 723, in pandas._libs.period._Period.__richcmp__ (pandas/_libs/period.c:11713)
  File ""/usr/local/lib/python3.5/site-packages/pandas/tseries/offsets.py"", line 375, in __ne__
    return not self == other
  File ""/usr/local/lib/python3.5/site-packages/pandas/tseries/offsets.py"", line 364, in __eq__
    if isinstance(other, compat.string_types):
SystemError: <built-in function isinstance> returned a result with an error set
```

I have no idea what to make of this.






A problem that I have *not* been able to replicate with a copy/pasteable subset of the data:
```
>>> mi = pd.MultiIndex.from_arrays([period.index, period])
>>> mi
[... prints roughly what we'd expect...]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/site-packages/pandas/core/base.py"", line 800, in shape
    return self._values.shape
  File ""/usr/local/lib/python3.5/site-packages/pandas/core/base.py"", line 860, in _values
    return self.values
  File ""/usr/local/lib/python3.5/site-packages/pandas/core/indexes/multi.py"", line 667, in values
    self._tuples = lib.fast_zip(values)
  File ""pandas/_libs/lib.pyx"", line 549, in pandas._libs.lib.fast_zip (pandas/_libs/lib.c:10513)
ValueError: all arrays must be same length

>>> mi.names
FrozenList(['Variable', None])
>>> mi[0]
('CPROF', 'Period')
>>> mi[1]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/site-packages/pandas/core/indexes/multi.py"", line 1377, in __getitem__
    if lab[key] == -1:
IndexError: index 1 is out of bounds for axis 0 with size 1
```

AFAICT it took the _name_ 'Period' and made that the only value in the new level of the MultiIndex.  Really no idea what's going on here."
555022042,31298,Aggregate with categorical variables but without groupby,GregKarabinos,open,2020-01-25T01:10:47Z,2020-09-05T19:11:16Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd 

df = pd.DataFrame( {""col_1"" : [1, 2, 3, 4, 5],
                  ""col_2"" : [3, 5, 3, 4, 9],
                  ""col_3"" : [""cat"", ""dog"", ""dog"", ""cat"", ""cat""],
                  ""col_4"" : [""feline"", ""canine"", ""canine"", ""feline"", ""feline""]})

func_list = [lambda x : len(x),
             lambda x : x.isnull().sum()]

num_col = df.select_dtypes(include=""number"").columns
cat_col = df.select_dtypes(include=""object"").columns

#works correctly for numeric columns
print(df[num_col].agg(func_list))
print(df[cat_col].agg(func_list))

```
#### Problem description

There is an issue with aggregating and applying a list of functions.  These functions should be able to be applied to categorical or object columns but cannot currently. 

#### Expected Output

I would expect the output for the categorical and numeric columns to be the same with these simple functions.

#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.0.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.2.1
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.1
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.9
tables           : 3.5.2
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.1

"
623955659,34357,"BUG: merge drops index components sometimes, when join is empty",phofl,open,2020-05-24T22:06:32Z,2020-09-05T20:42:22Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

left = pd.DataFrame({'a': [1], 'i': [1]}).set_index(['a', 'i'])
right = pd.DataFrame({'i': [1]}).set_index(['i'])

df = pd.merge(
    left,
    right, 
    left_on=""i"",
    right_on=""i"",
    how=""left"")
print(df)
print(df.reset_index())

df = pd.merge(
    left,
    right,    
    right_index=True,
    left_on=""i"",
    how=""left"")
print(df)
print(df.reset_index())
```

#### Problem description

The first merge changes the index of the left `DataFrame` in a way, that the index component `a` is dropped.

```
Empty DataFrame
Columns: []
Index: [1]


   i
0  1
```

The second merge returns the original index respectively `DataFrame` after resetting the index. I think this is the expected output.

#### Expected Output

```
Empty DataFrame
Columns: []
Index: [(1, 1)]


   a  i
0  1  1
```

#### Output of ``pd.show_versions()``

Was tested on master."
685052078,35878,BUG: MultiIndex assignment via loc,matthewgilbert,closed,2020-08-24T23:38:04Z,2020-09-05T21:18:51Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

The behavior of assignment via `loc` seems to have changed as of `1.0.0`.

```python
import pandas

TS = pandas.Timestamp

s = pandas.DataFrame([
    [""A1"", TS(""2020-06-15""), -573.75],
    [""A2"", TS(""2020-06-15""), -198.75],
    [""A3"", TS(""2020-06-15""), -1.25],
], columns=[""sid"", ""date"", ""price""]).set_index([""sid"", ""date""])[""price""]
s.loc[[]] = 0
print(s)
```

```
sid  date      
A1   2020-06-15    0.0
A2   2020-06-15    0.0
A3   2020-06-15    0.0
Name: price, dtype: float64
```

I would expect this to behave the same as in in `0.25.3` which would assign nothing. I assume this is a bug because I can't find any discussion of the changed behavior in the [Changelog](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v1.0.0.html).

Note that selection or assignment without a `MultiIndex` behavior appears to be unchanged, i.e.

```python
print(s.loc[[]])
```
```
Series([], dtype: int64)
```
```python
s = pandas.DataFrame([
    [""A1"", -573.75],
    [""A2"", -198.75],
    [""A3"", -1.25],
], columns=[""sid"", ""price""]).set_index([""sid""])[""price""]
s.loc[[]] = 0
print(s)
```
```
sid
A1   -573.75
A2   -198.75
A3     -1.25
Name: price, dtype: float64
```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 068e6542b0b28d80d8ea5953b6a49a2608f6a541
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-42-generic
Version          : #46~18.04.1-Ubuntu SMP Fri Jul 10 07:21:24 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+141.g068e6542b
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200814
Cython           : 0.29.21
pytest           : 6.0.1
hypothesis       : 5.27.0
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.17.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None


</details>
"
675313247,35614,ENH: Make Series.explode work for sets,dsaxton,closed,2020-08-07T22:06:58Z,2020-09-05T22:36:43Z,"Currently Series.explode does nothing for sets but I think this would be just as useful as lists or tuples. It also allows for a way to easily enforce uniqueness at the row level before exploding.

I don't think this would be breaking, unless something is specifically relying on the operation not working.

```python
import pandas as pd                                                                                                                                                                                  

ser = pd.Series([{1, 2}, {1, 2, 3}]) 
ser
# 0       {1, 2}
# 1    {1, 2, 3}
# dtype: object                                                                                                                                                               
ser.explode()                                                                                                                                                                                        
# 0       {1, 2}
# 1    {1, 2, 3}
# dtype: object
```

Desired output:

```python
ser.explode()
# 0    1
# 0    2
# 1    1
# 1    2
# 1    3
# dtype: object
```"
675576677,35637,ENH: Make explode work for sets,dsaxton,closed,2020-08-08T19:33:06Z,2020-09-05T22:36:46Z,"- [x] closes #35614
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
690279211,36044,BUG: Set type check is too strict when creating Series from dict keys,krassowski,closed,2020-09-01T16:20:39Z,2020-09-05T23:13:45Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
from pandas import Series
Series({'a': 1, 'b': 2}.keys())
```

#### Problem description

Since Python 3.7 dictionaries are ordered, and therefore their keys are also ordered. In older pandas versions (prior to 1.1?) it was possible to create a `Series` from keys of a dictionary. A fix to #32582 intended to prevent an issue with sets seems to be breaking the creation of `Series` from keys of a dictionary.

Currently the following is raised:

```python
/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath)
    325                     data = data.copy()
    326             else:
--> 327                 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True)
    328 
    329                 data = SingleBlockManager.from_array(data, index)

/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure)
    450         subarr = _try_cast(arr, dtype, copy, raise_cast_failure)
    451     elif isinstance(data, abc.Set):
--> 452         raise TypeError(""Set type is unordered"")
    453     elif lib.is_scalar(data) and index is not None and dtype is not None:
    454         data = maybe_cast_to_datetime(data, dtype)

TypeError: Set type is unordered
```

#### Expected Output

Same as of `Series(list({'a': 1, 'b': 2}.keys()))`, i.e.:

```
0    a
1    b
dtype: object
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-42-generic
Version          : #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
687007439,35918,REF: window/test_dtypes.py with pytest idioms,mroeschke,closed,2020-08-27T07:19:08Z,2020-09-06T04:05:06Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
694110245,36145,DOC: add userwarning doc about mpl #35684,fangchenli,closed,2020-09-05T15:39:27Z,2020-09-06T05:16:07Z,"- [x] closes #35684
- [x] whatsnew entry
"
692738463,36109,CLN: use IS64 instead of is_platform_32bit #36108,fangchenli,closed,2020-09-04T05:30:05Z,2020-09-06T05:16:50Z,"- [x] closes #36108
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
690484716,36054,BUG: Don't raise when constructing Series from ordered set,dsaxton,closed,2020-09-01T22:14:11Z,2020-09-06T12:58:55Z,"- [x] closes #36044
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This may or may not be a regression, since before the change that caused the error to be raised the output was still wrong (same bug that motivated the initial patch):

```python
In [2]: keys = {'a': 1, 'b': 2}.keys()

In [3]: pd.Series(keys)
Out[3]: 
0    (a, b)
1    (a, b)
dtype: object
```"
686887868,35914,Make MultiIndex.get_loc raise for unhashable type,dsaxton,closed,2020-08-27T02:25:38Z,2020-09-06T13:01:22Z,"- [x] closes #35878
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
694373236,36163,Backport PR #35914 on branch 1.1.x (Make MultiIndex.get_loc raise for unhashable type),meeseeksmachine,closed,2020-09-06T13:01:29Z,2020-09-06T15:13:33Z,Backport PR #35914: Make MultiIndex.get_loc raise for unhashable type
694372718,36162,Backport PR #36054 on branch 1.1.x (BUG: Don't raise when constructing Series from ordered set),meeseeksmachine,closed,2020-09-06T12:59:34Z,2020-09-06T15:13:55Z,Backport PR #36054: BUG: Don't raise when constructing Series from ordered set
667909950,35460,BUG: AssertionError: Number of Block dimensions (1) must equal number of axes (2) when typing a column,frndrs,closed,2020-07-29T14:54:57Z,2020-09-06T16:58:33Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame(
            {""e1_timestamp"": [],
                ""e2_timestamp"": []
            }
        )

# The problematic code. Remove this and there are no exceptions. 

for timefield in [
        ""e1_timestamp"",
        ""e2_timestamp"",
    ]:
        df[timefield] = pd.to_datetime(df[timefield], utc=True)
#

df = df.append({
                    ""orderId"": 1,
                    ""e1_timestamp"": pd.to_datetime(pd.NaT, utc=True),
                    ""e2_timestamp"": pd.to_datetime(pd.NaT, utc=True),
                },ignore_index=True,)
df
```

#### Problem description

Why initializing an empty dataframe, defining its type as shown in the example and then appending a value, there is an ```AssertionError: Number of Block dimensions (1) must equal number of axes (2)``` exception.

If we remove the for loop, it does work: 
```
df = pd.DataFrame(
            {""e1_timestamp"": [],
                ""e2_timestamp"": []
            }
        )

df = df.append({
                    
                    ""e1_timestamp"": pd.to_datetime(pd.NaT, utc=True),
                    ""e2_timestamp"": pd.to_datetime(pd.NaT, utc=True),
                },ignore_index=True,)


df
```

We see that we could do ```df = df.astype({""e1_timestamp"": 'datetime64[ns]', ""e1_timestamp"":  'datetime64[ns]'})``` instead of that for loop, but there seems to be a bug in this new version. With pandas==1.0.5 the for loop does not raise an exception.

#### Expected Output

There should be no assertion error

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.9.184-linuxkit
Version          : #1 SMP Tue Jul 2 22:58:16 UTC 2019
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.16.0
pytz             : 2019.1
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.4.0
Cython           : None
pytest           : 4.1.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : 0.9.3
psycopg2         : 2.8.2 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.2.0
sqlalchemy       : 1.2.16
tables           : None
tabulate         : 0.8.3
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
668165942,35471,BUG: astype ignore errors not working for categorical/StringArray,traubms,closed,2020-07-29T21:18:27Z,2020-09-06T17:07:54Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Problem description

It seems to be ignoring `errors='ignore'` kwarg. The behavior in 0.25.3 is expected.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
pd.__version__
# '1.0.5'

pd.Series(pd.Categorical(['A', 'B'])).astype(float, errors='ignore')
# ---------------------------------------------------------------------------
# ValueError                                Traceback (most recent call last)
# <ipython-input-4-3d39cd9c1cdd> in <module>
# ----> 1 pd.Series(pd.Categorical(['A', 'B'])).astype(float, errors='ignore')
#
# ...
# ValueError: could not convert string to float: 'A'
```


#### Expected Output


```python
import pandas as pd
pd.__version__
# '0.25.3'

pd.Series(pd.Categorical(['A', 'B'])).astype(float, errors='ignore')
# 0    A
# 1    B
# dtype: category
# Categories (2, object): [A, B]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1062.4.1.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : None
pytest           : 6.0.0
hypothesis       : None
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : 1.3.2
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.5.2
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 6.0.0
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.18
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : 1.2.9
numba            : 0.50.1
</details>
"
694230377,36155,CLN: backport mpl warning fix to 1.1.2,fangchenli,closed,2020-09-06T02:44:40Z,2020-09-06T17:18:15Z,backport #35946 and #36145
673956841,35584,BUG: `Index.sort_values` fails with TypeError,galipremsagar,closed,2020-08-06T01:57:41Z,2020-09-06T17:47:41Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
>>> import pandas as pd
>>> pd.__version__
'1.1.0'
>>> sr = pd.Series(['a',None,'c',None,'e'])
>>> sr.sort_values()
0       a
2       c
4       e
1    None
3    None
dtype: object
>>> idx = pd.Index(['a',None,'c',None,'e'])
>>> idx
Index(['a', None, 'c', None, 'e'], dtype='object')
>>> idx.sort_values()
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""/Users/pgali/PycharmProjects/del/venv1/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 4448, in sort_values
    _as = idx.argsort()
  File ""/Users/pgali/PycharmProjects/del/venv1/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 4563, in argsort
    return result.argsort(*args, **kwargs)
TypeError: '<' not supported between instances of 'NoneType' and 'str'

```

#### Problem description
`Index.sort_values()` fails when there are `None` values in it. However `Series.sort_values` performs sorting as expected in similar scenario.


#### Expected Output
We should be able to sort the values of an index similar to that of a series when there are `None` values.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Sun Jul  5 00:43:10 PDT 2020; root:xnu-6153.141.1~9/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_US.UTF-8
pandas           : 1.1.0
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.1.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
403902048,24980,Bug on .to_string(index=False),simontorres,closed,2019-01-28T16:37:47Z,2020-09-06T17:49:27Z,"#### Code Sample, a copy-pastable example if possible
Related issue (very old though) #11833

```python
import pandas

print(""pandas.__version__ == {:s}"".format(pandas.__version__))

columns = ['name', 'age']
values = [
    ['name_one', '31'],
    ['name_two', '32']]
data_frame = pandas.DataFrame(values, columns=columns)

# filter

filtered = data_frame[(data_frame['name'] == 'name_one') & (data_frame['age'] == '31')]
# single value comes with a blank at the beginning when running 0.24.0

print(""==={:s}==="".format(filtered.name.to_string(index=False)))

```
The output of this running on pandas 0.24.0 is:
(I put this in a file called ``pandasbug.py``)

```shell 
(goodman_pipeline) [simon@ctioy9 sandbox]$ python3.7 pandasbug.py 
pandas.__version__ == 0.24.0
=== name_one===
```

#### Problem description

Exactly when ``0.24.0`` was released my automatic builds started to fail. The problem, a file not found.
This was erroneously reported by me on astropy/ccdproc#658 
I'm using a pandas data frame to filter a set of reference files and when extracting the file name and join it to a full path it turns out a non-existing path such as:
``/full/path/to/[unwanted-blank-here]file_name.fits`` (``/full/path/to/ file_name.fits``)

[This travis build](https://travis-ci.org/soar-telescope/goodman_pipeline/jobs/485402650) will show the real-world error caused, if you can access it.

#### Expected Output
In my system I have pandas 0.20.3 working with python 3.6 and the latests working build ran pandas ``0.23.4``

```shell
(goodman_pipeline) [simon@ctioy9 sandbox]$ python3.6 pandasbug.py 
pandas.__version__ == 0.20.3
===name_one===
```

#### Output of ``pd.show_versions()``

<details>
>>> import pandas 
>>> pandas.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.2.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.0-1.el7.elrepo.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.0
pytest: 4.1.1
pip: 18.1
setuptools: 40.6.3
Cython: 0.29.2
numpy: 1.15.2
scipy: 1.2.0
pyarrow: None
xarray: None
IPython: None
sphinx: 1.8.3
patsy: None
dateutil: 2.7.5
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.1
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
692104591,36094,BUG: extra leading space in to_string when index=False,onshek,closed,2020-09-03T16:14:40Z,2020-09-06T17:49:33Z,"- [x] closes #24980 
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This PR is mainly from #29670 contributed by @charlesdong1991, and I made a few changes on his work.
The code have passed all tests modified and added in `test_format.py` and `test_to_latex.py`.
Any comment is welcomed!"
694453637,36167,"Backport PR #35979 on branch 1.1.x (BUG: Respect errors=""ignore"" during extension astype)",meeseeksmachine,closed,2020-09-06T17:07:33Z,2020-09-06T18:06:24Z,"Backport PR #35979: BUG: Respect errors=""ignore"" during extension astype"
694453148,36166,Backport PR #36115 on branch 1.1.x (REGR: append tz-aware DataFrame with tz-naive values),meeseeksmachine,closed,2020-09-06T17:05:57Z,2020-09-06T18:07:56Z,Backport PR #36115: REGR: append tz-aware DataFrame with tz-naive values
109138129,11207,Copy on Write via weakrefs,nickeubank,closed,2015-09-30T17:11:45Z,2020-09-06T18:44:21Z,"Aims to eventually close #10954 , alternative to #10973 

In the interest of advancing the discussion on this, here's a skeleton of a different copy on write implementation. 

**Currently**

When setting on a view, converts to copy. 

```
 df = pd.DataFrame({'col1':[1,2], 'col2':[3,4]})
 intermediate = df.loc[1:1,]
 intermediate['col1'] = -99


intermediate

Out[8]: 
   col1  col2
1   -99     4

df

Out[9]: 
   col1  col2
0     1     3
1     2     4
```

Chained-indexing will ALWAYS fail. 

```
df = pd.DataFrame({'col1':[1,2], 'col2':[3,4]})
df.loc[1:1,]['col1'] = -99
df

Out[11]: 
   col1  col2
1     2     4
```

`SettingWithCopy` warning disabled, thought not fully removed (figure this is too early stage to really chase that down). 

Forward Protection for `loc` setting (proof of concept, added oct 2):

```
 df = pd.DataFrame({'col1':[1,2], 'col2':[3,4]})
 intermediate = df.loc[1:1,]

 df.loc[1,'col1'] = -99
 intermediate

  Out[11]: 
   col1  col2
0     1     3
1     2     4
```

**Goals / Places I'd love help**
- We've discussed keeps full-column slices as views, since we can guarantee these will always be views. I created an attribute called `_is_column_view` that can be set to `True` when a slice is a full column (e.g. `new = df['col1']`, but I'm not sure how to actually figure out if a slice is a full column, so right now it's initialized as `False` and just stays that way. 
- This has no protection against forward propagation -- situations where one sets values on a Frame and they propagate to views of that Frame. There IS a framework to address it, but it's incomplete. As it stands:
  - Frames now have a `_children` list attribute to keep references to their child `views`.
  - Before setting, there's a call to a function to try and convert the children from `views` to `copies`. 
    But as it stand, I don't know how to really manage those references (i.e. how to put references to children in that `_children` list). 

@jreback 
@shoyer 
@JanSchulz 
@TomAugspurger 
"
114479488,11500,Copy on write using weakrefs,nickeubank,closed,2015-11-01T16:41:21Z,2020-09-06T18:46:05Z,"Working model of copy-on-write. Aims to close #10954, alternative to #10973, extension of #11207.
## Copy-on-Write Behavior:

**Setting on child doesn't affect parent, but still uses views when can for efficiency**

```
parent = pd.DataFrame({'col1':[1,2], 'col2':[3,4]})
child = parent.loc[0:0,]
child._is_view

Out[1]: True

child.loc[0:0] = -88
child

Out[2]: 
   col1  col2
0   -88   -88

parent

Out[3]: 
   col1  col2
0     1     3
1     2     4
```

**Setting on parent doesn't affect child**

```
parent = pd.DataFrame({'col1':[1,2], 'col2':[3,4]})
child = parent.loc[0:0,]
child._is_view

Out[4]: True

parent.loc[0:0, 'col1'] = -88
child

Out[5]: 
   col1  col2
0     1     3

parent

Out[6]: 
   col1  col2
0   -88     3
1     2     4
```

**One exception is dictionary-like access, where views are preserved**
_(as suggested here: https://github.com/pydata/pandas/issues/10954#issuecomment-136521398 )_

```
parent = pd.DataFrame({'col1':[1,2], 'col2':[3,4]})
child = parent['col1']
child._is_view

Out[7]: True

parent.loc[0:0, 'col1'] = -88
child

Out[8]: 
0   -88
1     2
Name: col1, dtype: int64

parent

Out[9]: 
   col1  col2
0   -88     3
1     2     4
```

**Safe for views of views**

```
parent = pd.DataFrame({'col1':[1,2], 'col2':[3,4]})
child = parent.loc[0:0,]
child._is_view
Out[20]: True

child_of_child = child.loc[0:0,'col1':'col1']
child_of_child._is_view

Out[21]: True

child_of_child.loc[0:0, 'col1'] = -88
child_of_child

Out[22]: 
   col1
0   -88

child

Out[23]: 
   col1  col2
0     1     3

parent

Out[24]: 
   col1  col2
0     1     3
1     2     4
```
## Chained indexing behavior now consistent

**Will always fail unless first class is a dictionary-like call for a single series (since that will always be a view)**

**Will fail if first call not dict-like**

```
parent = pd.DataFrame({'col1':[1,2], 'col2':[3,4]})
parent.loc[0:0, 'col1']['col1'] = -88
parent

Out[10]: 
   col1  col2
0     1     3
1     2     4
```

**Will always succeed if first call dict-like**

```
parent = pd.DataFrame({'col1':[1,2], 'col2':[3,4]})
parent['col1'].loc[0:0] = -88
parent

Out[11]: 
   col1  col2
0   -88     3
1     2     4
```

To Do:
- Get feedback on implementation; 
- if sound in behavior and principles, remove all SettingWithCopy machinery
- Add docs

@jreback 
@TomAugspurger 
@shoyer 
@JanSchulz 
cc @ellisonbg
cc @CarstVaartjes 
"
104569632,10973,"COMPAT: remove SettingWithCopy warning, and use copy-on-write, #10954",jreback,closed,2015-09-02T20:56:46Z,2020-09-06T18:46:29Z,"closes #10954
- deprecates the option `mode.chained_assignment` as its now unused (it shows the deprecation if you are explicity using it, more friendly this way).
- deprecate `NDFrame.is_copy` property
- remove `SettingWithCopyWarning` entirely

TODO:
- [ ] some systematic testing of the chaining might help (e.g. iterate thru the `iloc/loc/[]`, chain, and set various values including dtype changes
- [ ] huge note explaining the rationale & change structure

semantics:

Intermediate assignment trigger copy-on-write and do not propogate.

```
In [1]: df = DataFrame({'col1':[1,2], 'col2':[3,4]})

In [2]: intermediate = df.loc[1:1,]

In [3]: intermediate['col1'] = -99

In [4]: intermediate
Out[4]: 
   col1  col2
1   -99     4

In [5]: df
Out[5]: 
   col1  col2
0     1     3
1     2     4
```

Chained assignments _always_ work!

```
In [6]: df = DataFrame({'col1':[1,2], 'col2':[3,4]})

In [7]: df.loc[1:1,]['col1'] = -99

In [8]: df
Out[8]: 
   col1  col2
0     1     3
1   -99     4
```

Even true with cross-sections that change dtype

```
In [1]: df = DataFrame({'col1':[1,2], 'col2':[3,4]})

In [2]: df.loc[1]['col2'] = 'foo'

In [3]: df
Out[3]: 
   col1 col2
0     1    3
1     2  foo
```

except for a really egregious case, which will raise a `SettingWithCopyError`  (maybe I could even fix this....)

```
In [10]: df.loc[1:1]['col2'].replace(10,5,inplace=True)
SettingWithCopyError: chained indexing detected, you can fix this ......
```

This is an invalid assignment. I suppose we _could_ make it work, but we currently havent' allowed the `.dt` to be used as a setitem accessor

```
In [9]: s = Series(pd.date_range('20130101',periods=3))

In [12]: s.dt.hour[0] = 5
SettingImmutableError: modifications to a property of a datetimelike object are not supported and are discarded. Change values on the original.
```

cc @nickeubank 
cc @JanSchulz 
cc @ellisonbg
cc @CarstVaartjes 
@shoyer 

special thanks to @JanSchulz for some reference checking code :)
"
126576804,12036,Copy on write using weakrefs (part 2),nickeubank,closed,2016-01-14T04:50:39Z,2020-09-06T18:48:21Z,"Continuation of #11500 
"
694233476,36156,STY: De-privatize imported names,jbrockmendel,closed,2020-09-06T03:06:33Z,2020-09-06T19:04:13Z,getting close to being able to enable a code_check for these
694210330,36154,REF: share more EA methods,jbrockmendel,closed,2020-09-06T00:05:22Z,2020-09-06T19:29:40Z,
638328709,34765,"BUG: read_excel issue - 3 (header is a list, index_col is not range(N))",kuraga,open,2020-06-14T10:54:51Z,2020-09-07T03:41:09Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

pd.read_excel('test.xlsx', header=[0, 1], index_col=1, engine='openpyxl')
# or
pd.read_excel('test.xlsx', header=[0, 1], index_col=[0, 2], engine='openpyxl')
# or
pd.read_excel('test.xlsx', header=[0, 1], index_col=[1, 2], engine='openpyxl')
```
(`engine` could be `xlrd` or `openpyxl`, doesn't matter.)

[test.xlsx](https://github.com/pandas-dev/pandas/files/4776274/test.xlsx) (but could be different)

#### Problem description

```
Traceback (most recent call last):
  File ""test.py"", line 3, in <module>
    pd.read_excel('test.xlsx', header=[0, 1], index_col=[0, 2], engine='openpyxl')
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 334, in read_excel
    **kwds,
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 888, in parse
    **kwds,
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 521, in parse
    ].columns.set_names(header_names)
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 1325, in set_names
    idx._set_names(names, level=level)
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 1239, in _set_names
    raise ValueError(f""Length of new names must be 1, got {len(values)}"")
ValueError: Length of new names must be 1, got 2
```

#### Expected Output

No exceptions.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.42-calculate
machine          : x86_64
processor        : Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz
byteorder        : little
LC_ALL           : None
LANG             : ru_RU.utf8
LOCALE           : ru_RU.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.4.0.post20200518
Cython           : 0.29.17
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.8
numba            : None
</details>
"
690442106,36050,BUG: incorrect year returned in isocalendar for certain dates,asishm,closed,2020-09-01T20:56:01Z,2020-09-07T05:14:36Z,"- [x] closes #36032
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
645891404,34999,BUG: item_cache not cleared on DataFrame.values,jbrockmendel,closed,2020-06-25T22:37:02Z,2020-09-07T06:58:36Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

"
623657882,34339,BUG: Fix concat of frames with extension types (no reindexed columns),jorisvandenbossche,closed,2020-05-23T13:11:22Z,2020-09-07T10:52:48Z,"This PR tries to fix the case of concatting dataframes with extension dtypes and non-extension dtypes, like this example (giving object dtype on master, instead of preserving Int64 dtype):

```
In [15]: df1 = pd.DataFrame({""a"": pd.array([1, 2, 3], dtype=""Int64"")}) 
    ...: df2 = pd.DataFrame({""a"": np.array([4, 5, 6])})  

In [16]: pd.concat([df1, df2]) 
Out[16]: 
   a
0  1
1  2
2  3
0  4
1  5
2  6

In [17]: pd.concat([df1, df2]).dtypes 
Out[17]: 
a    object
dtype: object
```

Up to now, the `_get_common_dtype` machinery of the ExtensionArrays was only used for concatting Series (`axis=0` in `concat_compat`). Now I also try to make use of this for DataFrame columns, if there is any extension dtype in the to-concat arrays. This is checked in `_concatenate_join_units`. And we need to handle the dimension-mismach. I decided to handle that inside the internals code (reducing the dimension for non-EAs, and if the concat result is not an EA, adding back a dimension), so `concat_compat` does not need to be aware of this.


Partly overlapping with https://github.com/pandas-dev/pandas/pull/33522 (cc @TomAugspurger), but this PR does *not yet* fix the original case the other PR is fixing: concat with reindexed (thus all missing) columns becoming object instead of preserving object dtype."
687824893,35948,"DOC: ""engine=odf"" option should ask you to install pyodf instead of odf in pandas.read_excel",TrigonaMinima,closed,2020-08-28T06:44:41Z,2020-09-07T11:16:12Z,"#### Location of the documentation

""pandas.read_excel""

#### Documentation problem

When `engine=""odf""` is used and `pyodf` is not installed it gives an error that you should install optional dependency odf as `pip or conda install odf`. This pip/conda command fails because there's no module called `odf`. This could also be a security vulnerability if someone puts a malicious ""odf"" module on pypi.

#### Suggested fix for documentation

The error should ask you to install odf using `pip or conda install pyodf`
"
472785714,27584,CLN: Centralised _check_percentile,hedonhermdev,closed,2019-07-25T10:23:11Z,2020-09-07T11:32:12Z,"- Fixes  #27559
- Moved the _check_percentile method on NDFrame to algorithms as
check_percentile.
- Changed the references to _check_percentile in pandas/core/series.py
and pandas/core/frame.py

- [x] closes #27559
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
685083193,35882,BUG: item_cache invalidation in get_numeric_data,jbrockmendel,closed,2020-08-25T01:13:32Z,2020-09-07T11:33:18Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
622689869,34299,BUG: TestMergeDtypes.test_merge_on_ints_floats_warning fails (Linux/openSUSE/Factory),mcepl,closed,2020-05-21T18:23:54Z,2020-09-07T14:21:16Z,"- [X] I have checked that this issue has not already been reported (cannot find any ``test_merge`` issue)

- [X] I have confirmed this bug exists on the latest version of pandas (testing on 1.0.3, which is the latest released).

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas. (This happened while building the package for openSUSE, so we care to use only officially released versions).

#### Problem description

While running the test suite, this one test fails consistently:
```
[  363s] + export PYTHONPATH=/home/abuild/rpmbuild/BUILDROOT/python-pandas-1.0.3-0.i386/usr/lib/python3.8/site-packages
[  363s] + PYTHONPATH=/home/abuild/rpmbuild/BUILDROOT/python-pandas-1.0.3-0.i386/usr/lib/python3.8/site-packages
[  363s] + xvfb-run py.test-3.8 -n auto -v /home/abuild/rpmbuild/BUILDROOT/python-pandas-1.0.3-0.i386/usr/lib/python3.8/site-packages/pandas/tests -k 'not test_oo_optimizable and not test_encode_non_c_locale and not test_maybe_promote_int_with_int'
[  368s] ============================= test session starts ==============================
[  368s] platform linux -- Python 3.8.2, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /usr/bin/python3
[  368s] cachedir: .pytest_cache
[  368s] hypothesis profile 'ci' -> deadline=timedelta(milliseconds=500), suppress_health_check=[HealthCheck.too_slow], database=DirectoryBasedExampleDatabase('/home/abuild/rpmbuild/BUILD/pandas-1.0.3/.hypothesis/examples')
[  368s] rootdir: /home/abuild/rpmbuild
[  368s] plugins: forked-1.1.3, hypothesis-5.11.0, xdist-1.32.0, mock-3.1.0
[ 1070s] =================================== FAILURES ===================================
[ 1070s] ______________ TestMergeDtypes.test_merge_on_ints_floats_warning _______________
[ 1070s] [gw0] linux -- Python 3.8.2 /usr/bin/python3
[ 1070s] 
[ 1070s] self = <pandas.tests.reshape.merge.test_merge.TestMergeDtypes object at 0x9419e988>
[ 1070s] 
[ 1070s]     def test_merge_on_ints_floats_warning(self):
[ 1070s]         # GH 16572
[ 1070s]         # merge will produce a warning when merging on int and
[ 1070s]         # float columns where the float values are not exactly
[ 1070s]         # equal to their int representation
[ 1070s]         A = DataFrame({""X"": [1, 2, 3]})
[ 1070s]         B = DataFrame({""Y"": [1.1, 2.5, 3.0]})
[ 1070s]         expected = DataFrame({""X"": [3], ""Y"": [3.0]})
[ 1070s]     
[ 1070s]         with tm.assert_produces_warning(UserWarning):
[ 1070s]             result = A.merge(B, left_on=""X"", right_on=""Y"")
[ 1070s]             tm.assert_frame_equal(result, expected)
[ 1070s]     
[ 1070s]         with tm.assert_produces_warning(UserWarning):
[ 1070s]             result = B.merge(A, left_on=""Y"", right_on=""X"")
[ 1070s]             tm.assert_frame_equal(result, expected[[""Y"", ""X""]])
[ 1070s]     
[ 1070s]         # test no warning if float has NaNs
[ 1070s]         B = DataFrame({""Y"": [np.nan, np.nan, 3.0]})
[ 1070s]     
[ 1070s]         with tm.assert_produces_warning(None):
[ 1070s]             result = B.merge(A, left_on=""Y"", right_on=""X"")
[ 1070s] >           tm.assert_frame_equal(result, expected[[""Y"", ""X""]])
[ 1070s] 
[ 1070s] ../../BUILDROOT/python-pandas-1.0.3-0.i386/usr/lib/python3.8/site-packages/pandas/tests/reshape/merge/test_merge.py:1484: 
[ 1070s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1070s] 
[ 1070s] self = <contextlib._GeneratorContextManager object at 0x9419e340>, type = None
[ 1070s] value = None, traceback = None
[ 1070s] 
[ 1070s]     def __exit__(self, type, value, traceback):
[ 1070s]         if type is None:
[ 1070s]             try:
[ 1070s] >               next(self.gen)
[ 1070s] E               AssertionError: Caused unexpected warning(s): [('RuntimeWarning', RuntimeWarning('invalid value encountered in equal'), '/home/abuild/rpmbuild/BUILDROOT/python-pandas-1.0.3-0.i386/usr/lib/python3.8/site-packages/pandas/core/reshape/merge.py', 1091)]
[ 1070s] 
[ 1070s] /usr/lib/python3.8/contextlib.py:120: AssertionError
```

#### Expected Output

All tests pass.

#### Output of ``pd.show_versions()``

<details>

[  403s] + python3 -c 'import pandas; print(pandas.show_versions())'
[  404s] /usr/lib/python3.8/site-packages/setuptools/version.py:1: UserWarning: Module pandah
[  404s]   import pkg_resources
[  404s] 
[  404s] INSTALLED VERSIONS
[  404s] ------------------
[  404s] commit           : None
[  404s] python           : 3.8.2.final.0
[  404s] python-bits      : 32
[  404s] OS               : Linux
[  404s] OS-release       : 5.6.12-1-default
[  404s] machine          : i686
[  404s] processor        : i686
[  404s] byteorder        : little
[  404s] LC_ALL           : en_US.UTF-8
[  404s] LANG             : en_US.UTF-8
[  404s] LOCALE           : en_US.UTF-8
[  404s] 
[  404s] pandas           : 1.0.3
[  404s] numpy            : 1.18.4
[  404s] pytz             : 2020.1
[  404s] dateutil         : 2.8.1
[  404s] pip              : None
[  404s] setuptools       : 44.0.0
[  404s] Cython           : 0.29.17
[  404s] pytest           : 5.3.5
[  404s] hypothesis       : 5.11.0
[  404s] sphinx           : None
[  404s] blosc            : None
[  404s] feather          : None
[  404s] xlsxwriter       : 1.2.7
[  404s] lxml.etree       : 4.5.0
[  404s] html5lib         : None
[  404s] pymysql          : None
[  404s] psycopg2         : None
[  404s] jinja2           : 2.11.2
[  404s] IPython          : None
[  404s] pandas_datareader: None
[  404s] bs4              : 4.9.0
[  404s] bottleneck       : None
[  404s] fastparquet      : None
[  404s] gcsfs            : None
[  404s] lxml.etree       : 4.5.0
[  404s] matplotlib       : None
[  404s] numexpr          : None
[  404s] odfpy            : None
[  404s] openpyxl         : 3.0.3
[  404s] pandas_gbq       : None
[  404s] pyarrow          : None
[  404s] pytables         : None
[  404s] pytest           : 5.3.5
[  404s] pyxlsb           : None
[  404s] s3fs             : None
[  404s] scipy            : None
[  404s] sqlalchemy       : 1.3.16
[  404s] tables           : None
[  404s] tabulate         : None
[  404s] xarray           : None
[  404s] xlrd             : 1.2.0
[  404s] xlwt             : 1.3.0
[  404s] xlsxwriter       : 1.2.7
[  404s] numba            : None
[  404s] None

</details>

[Complete build log with all details of the process](https://github.com/pandas-dev/pandas/files/4664063/_log.txt.zip)

"
682576575,35819,QST:Time index problem in version 1.1.0,maozi07,closed,2020-08-20T09:46:00Z,2020-09-07T14:49:41Z,"- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [x] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

**Note**: If you'd still like to submit a question, please read [this guide](
https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your question. 
I'm using pandas 1.1.0 on macos, python:3.7
As below it's weird, when I select time larger than 2020/03/01 it's rise error, but if it's 2019, all things go right
I'v tried make some random data, but can't reproduce
I don't konw it's something wrong with my data or a pandas bug with version 1.1.0(it's ok in version 1.0.1)
Very appreciate for someone can give help

```python
pd.__version__
'1.1.0'
df.index
DatetimeIndex(['2020-07-02 08:55:59', '2020-07-03 07:59:32',
               '2020-07-06 06:57:38', '2020-07-11 09:25:35',
               '2020-07-09 10:02:25', '2020-07-13 07:10:12',
               '2020-07-16 07:58:52', '2020-07-17 10:46:43',
               '2020-07-18 08:00:36', '2020-07-22 11:47:43',
               ...
               '2020-08-01 07:58:25', '2020-08-01 07:59:05',
               '2020-08-01 07:58:50', '2020-08-01 07:57:50',
               '2020-07-29 02:50:06', '2020-08-01 07:58:20',
               '2020-08-01 07:58:30', '2020-08-01 07:58:21',
               '2020-08-01 07:59:08', '2020-08-01 07:58:53'],
              dtype='datetime64[ns]', name='sub_time', length=4715513, freq=None)

df['20200701':]
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-54-0be901d6256c> in <module>
----> 1 df['20200701':]

/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key)
   2880             # either we have a slice or we have a string that can be converted
   2881             #  to a slice for partial-string date indexing
-> 2882             return self._slice(indexer, axis=0)
   2883 
   2884         # Do we have a (boolean) DataFrame?

/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _slice(self, slobj, axis)
   3546         Slicing with this method is *always* positional.
   3547         """"""
-> 3548         assert isinstance(slobj, slice), type(slobj)
   3549         axis = self._get_block_manager_axis(axis)
   3550         result = self._constructor(self._mgr.get_slice(slobj, axis=axis))

AssertionError: <class 'numpy.ndarray'>

df['20190701':]

                                     A         B  ...    C    D
a_time                                                   ...                    
2020-07-02 08:55:59              aa  cc  ...  22  1.2


```
"
694454508,36168,Comma cleanup,JonathanShrek,closed,2020-09-06T17:10:01Z,2020-09-07T19:04:48Z,"- [x] pandas/tests/indexing/test_iloc.py
- [x] pandas/tests/indexing/test_indexing.py
- [x] pandas/tests/indexing/test_loc.py
- [x] pandas/tests/internals/test_internals.py
- [x] pandas/tests/io/formats/test_css.py
- [x] pandas/tests/io/formats/test_info.py
- [x] pandas/tests/io/json/test_compression.py
- [x] pandas/tests/io/json/test_pandas.py
- [x] pandas/tests/io/parser/test_c_parser_only.py
- [x] pandas/tests/io/parser/test_parse_dates.py
- [x] pandas/tests/io/parser/test_usecols.py
"
695010296,36188,Backport PR #35882 on branch 1.1.x (BUG: item_cache invalidation in get_numeric_data),meeseeksmachine,closed,2020-09-07T11:34:34Z,2020-09-07T19:06:15Z,Backport PR #35882: BUG: item_cache invalidation in get_numeric_data
695041685,36192,TST: test_datetime64_factorize on 32bit,simonjayhawkins,closed,2020-09-07T12:16:13Z,2020-09-07T19:07:35Z,"- [ ] closes #35921
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
687241378,35921,test_datetime64_factorize failing on MacPython windows py_3.8_32/linux py_3.8_32,simonjayhawkins,closed,2020-08-27T13:25:44Z,2020-09-07T19:08:52Z,"test added in #35775

https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=40968&view=results"
693974714,36134,BUG: shows correct package name when import_optional_dependency is ca…,hs2361,closed,2020-09-05T04:19:41Z,2020-09-07T19:30:44Z,"…lled. fixes #35948

- [x] closes #35948
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
695037938,36191,TST: update test_series_factorize_na_sentinel_none for 32bit,simonjayhawkins,closed,2020-09-07T12:11:14Z,2020-09-07T19:41:30Z,"xref https://github.com/pandas-dev/pandas/issues/35831#issuecomment-688163069

(I've not tested this locally, after backport can retrigger https://github.com/MacPython/pandas-wheels/pull/97 to check)"
695005882,36187,DOC: move release note for #36155,simonjayhawkins,closed,2020-09-07T11:28:54Z,2020-09-07T19:44:44Z,"xref https://github.com/pandas-dev/pandas/pull/36155#issuecomment-687846461

does not need backport. release note already in 1.1.2 on backport branch"
694649310,36181,REF: use _validate_foo pattern in Categorical,jbrockmendel,closed,2020-09-07T02:57:23Z,2020-09-07T20:10:41Z,"Similar to how we did it in datetimelike, eventually these validator methods are going to end up sharing most of their code"
701087555,36356,DOC: Example for natural sort using key argument,erfannariman,closed,2020-09-14T13:12:14Z,2020-09-15T22:59:54Z,"- [x] closes #36286 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
702291936,36390,CLN: Clean test_arithmetic.py,dsaxton,closed,2020-09-15T21:38:34Z,2020-09-15T23:25:07Z,"General cleaning (i.e., replacing for loops) trying to make these easier to follow / debug"
702369265,36392,ENH: read_hdf() - support for generic buffers has not been implemented,lucasthahn,closed,2020-09-16T01:03:03Z,2020-09-16T01:39:07Z,"#### Is your feature request related to a problem?

As of the latest verison of master, `read_hdf()` does not support generic buffers as input, and raises a `NotImplementedError` if one is passed in. This stems from line 374 of `pandas/pandas/io/pytables.py`. 

#### Describe the solution you'd like

There's already a nice stub in which to insert this code; somebody just needs to implement it. If nobody is currently working on this, I'm happy to give a try myself! 

#### API breaking implications

Shouldn't break anything; just would be adding more functionality.

#### Describe alternatives you've considered

For my specific use case I've found a workaround which involves not writing to a generic buffer, but if `pandas` were to have this functionality, my own code would certainly be more elegant.

#### Additional context

As I said above, happy to try to do this myself - seems like a good first contribution and not really any potential to break anything else! But if someone's already working on this, please let me know and I will spare myself the effort. Thanks!"
701426137,36371,BUG: Fix MultiIndex column stacking with dupe names,dsaxton,closed,2020-09-14T21:06:39Z,2020-09-16T09:53:02Z,"- [x] closes #36353
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
702244487,36389,[TST]: Groupby raised error with duplicate column names,phofl,closed,2020-09-15T20:41:42Z,2020-09-16T11:58:56Z,"- [x] closes #31735 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Added a test to avoid regression"
700664213,36348,[BUG]: Implement Kahan summation for rolling().mean() to avoid numerical issues,phofl,closed,2020-09-13T22:50:51Z,2020-09-16T11:59:19Z,"- [x] closes #36031
- [x] closes #32761
- [x] closes #10319 
- [x] xref #11645 (fixes the issue expect the 0, will add an additional pr about the doc)
- [x] closes #13254
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I implemented the Kahan summation as suggested by @jreback. I used the variable names from https://en.wikipedia.org/wiki/Kahan_summation_algorithm. If there exists a name convention am not aware of, I am happy to rename the variables."
584811314,32851,Unexpected behavior when inverting column of dtype object with boolean values,Ranfir,closed,2020-03-20T02:55:19Z,2020-09-16T12:31:31Z,"#### Code Sample, a copy-pastable example if possible

```python
Python 3.8.2 (default, Mar 14 2020, 03:36:42)
[GCC 7.5.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas as pd
>>> pd.__version__
'1.0.3'
>>> x = pd.DataFrame()
>>> x[0] = None
>>> x.loc[0, 0] = False
>>> x.loc[1, 0] = True
>>> x
       0
0  False
1   True
>>> ~x
    0
0  -1
1  -2
>>>
```
#### Problem description
Unexpected behavior when inverting column of dtype object with boolean values.

#### Expected Output
```
>>> ~x
       0
0  True
1   False
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-18362-Microsoft
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
699773839,36296,BLD/CI: 3.9 support,jreback,closed,2020-09-11T21:37:31Z,2020-09-16T15:06:57Z,"- [x] CI for the 3.9 build is from source, we should update this to use numpy wheels (just became available IIUC) #36323
- [ ] pass tests & update whatsnew for 3.9 build so we can include in 1.1.3

cc @pandas-dev/pandas-core "
701692747,36379,DOC: move release note for #36175 (pt2),simonjayhawkins,closed,2020-09-15T07:40:23Z,2020-09-16T15:23:57Z,xref https://github.com/pandas-dev/pandas/pull/36363#pullrequestreview-488151435
698560986,36279,QST: Python 3.9 testing in CI,wumpus,closed,2020-09-10T22:23:16Z,2020-09-16T15:28:46Z,"I use Travis CI to test, and I've tested all of my projects against the upcoming python 3.9 release. The only problem I'm seeing is dependent packages is pandas.

Pandas + 3.9 takes a long time to build and then dies with a compiler error. It also does this when I install cython first, which is the usual workaround when a new version of Python is close to release. Is some not-yet-mainstream cython version needed? I suppose there will be a wheel by the time 3.9.0 comes out in October?

Example CI log: https://travis-ci.com/github/wumpus/paramsurvey/jobs/383337409

```
  pandas/_libs/writers.c:4942:5: error: â€˜_PyUnicode_get_wstr_lengthâ€™ is deprecated [-Werror=deprecated-declarations]
       __pyx_v_l = PyUnicode_GET_SIZE(__pyx_v_val);
       ^
  In file included from /opt/python/3.9-dev/include/python3.9/unicodeobject.h:1026:0,
                   from /opt/python/3.9-dev/include/python3.9/Python.h:97,
                   from pandas/_libs/writers.c:33:
  /opt/python/3.9-dev/include/python3.9/cpython/unicodeobject.h:446:26: note: declared here
   static inline Py_ssize_t _PyUnicode_get_wstr_length(PyObject *op) {
                            ^
  pandas/_libs/writers.c:4942:5: error: â€˜PyUnicode_AsUnicodeâ€™ is deprecated [-Werror=deprecated-declarations]
       __pyx_v_l = PyUnicode_GET_SIZE(__pyx_v_val);
       ^
  In file included from /opt/python/3.9-dev/include/python3.9/unicodeobject.h:1026:0,
                   from /opt/python/3.9-dev/include/python3.9/Python.h:97,
                   from pandas/_libs/writers.c:33:
  /opt/python/3.9-dev/include/python3.9/cpython/unicodeobject.h:580:45: note: declared here
   Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
                                               ^
  pandas/_libs/writers.c:4942:5: error: â€˜_PyUnicode_get_wstr_lengthâ€™ is deprecated [-Werror=deprecated-declarations]
       __pyx_v_l = PyUnicode_GET_SIZE(__pyx_v_val);
       ^
  In file included from /opt/python/3.9-dev/include/python3.9/unicodeobject.h:1026:0,
                   from /opt/python/3.9-dev/include/python3.9/Python.h:97,
                   from pandas/_libs/writers.c:33:
  /opt/python/3.9-dev/include/python3.9/cpython/unicodeobject.h:446:26: note: declared here
   static inline Py_ssize_t _PyUnicode_get_wstr_length(PyObject *op) {
                            ^
  cc1: all warnings being treated as errors
  error: command '/usr/bin/gcc' failed with exit code 1
```

The logfile contains complete version information about the travisci environment."
701690423,36378,DOC: move release note for #36175 (pt1),simonjayhawkins,closed,2020-09-15T07:36:25Z,2020-09-16T15:31:17Z,xref https://github.com/pandas-dev/pandas/pull/36363/files#r488227748
701661490,36376,CLN: Numba internal routines,mroeschke,closed,2020-09-15T06:48:02Z,2020-09-16T16:54:58Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

- Remove some leftover implementation after https://github.com/pandas-dev/pandas/pull/36240
- Delegate numba func caching to methods in `numba_.py`
- Combine some helper methods"
702909469,36399,CLN: remove trailing commas for black update,cjlynch278,closed,2020-09-16T16:32:48Z,2020-09-16T17:59:27Z,"Working on: CLN remove unnecessary trailing commas to get ready for new version of black #35925.
Checked the following files to make sure they are ready for black :
 pandas/tests/scalar/timestamp/test_arithmetic.py
 pandas/tests/scalar/timestamp/test_constructors.py
 pandas/tests/series/methods/test_argsort.py
 pandas/tests/series/methods/test_convert_dtypes.py
 pandas/tests/series/methods/test_drop_duplicates.py
 pandas/tests/series/methods/test_interpolate.py
 pandas/tests/series/methods/test_unstack.py
 pandas/tests/series/test_cumulative.py
 pandas/tests/test_algos.py

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
700587201,36336,CI: Add stale PR action,dsaxton,closed,2020-09-13T15:06:43Z,2020-09-16T18:32:16Z,"Adding a GitHub Action for ~closing stale PRs~ labeling PRs as stale which I think could help in managing the backlog

https://github.com/actions/stale"
396203168,24648,CI/TST: Check that unittest.mock is not being used in testing,simonjayhawkins,closed,2019-01-05T21:52:31Z,2020-09-17T00:59:13Z,xref https://github.com/pandas-dev/pandas/pull/24624#issuecomment-451658866
701933813,36383,BUG: df.sort_values w/ key function fails with multiple sort columns and Categorical sorting,kmatarese,closed,2020-09-15T13:32:56Z,2020-09-17T02:31:09Z,"- [ X] I have checked that this issue has not already been reported.

- [ X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame(
    {
        ""Name"": [""baz"", ""foo"", ""baz"", ""foo"", ""bar"", ""bar"", ""bar""],
        ""Criticality"": [""High"", ""Critical"", ""Low"", ""Medium"", ""High"", ""Low"", ""Medium""],
    }
)

SEVERITY = [""Critical"", ""High"", ""Medium"", ""Low""]

def sorter(column):
    if column.name != ""Criticality"":
        return column
    cat = pd.Categorical(column, categories=SEVERITY, ordered=True)
    return pd.Series(cat)

df.sort_values(by=[""Name"", ""Criticality""], key=sorter)
```

#### Problem description

This raises AttributeError: 'Series' object has no attribute 'categories'. If I sort by just Name, or just Criticality there are no issues. Likewise if I replace the sorter with the following it works sorting on both columns:

```python
def sorter(column):
    if column.name != ""Criticality"":
        return column
    mapper = {name: order for order, name in enumerate(SEVERITY)}
    return column.map(mapper)
```

#### Expected Output

I expected to be able to do a multi-column sort when one or more of them are Categorical, or at least being sorted categorically.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.7.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.0.0
Version          : Darwin Kernel Version 18.0.0: Wed Aug 22 20:13:40 PDT 2018; root:xnu-4903.201.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.2
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : 0.9.3
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.15
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
700640098,36346,ENH/BUG: consistently cast strs to datetimelike for searchsorted,jbrockmendel,closed,2020-09-13T20:18:21Z,2020-09-17T02:51:57Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

ATM we are pretty inconsistent in how we handle strings for (DTI|TDI|PI|Series).searchsorted.  The underlying EAs will cast scalar strings but not Sequene[str].  The Indexes will raise on both scalars and sequences.  Series follows the EA and casts ""2016-01-01"" but not `[""2016-01-01""]`.

This makes us consistently cast in all cases."
703120844,36408,CLN: err.orig in DataFrame.to_sql seems redundant,erfannariman,closed,2020-09-16T22:40:22Z,2020-09-17T09:31:23Z,"When going through the `DataFrame.sql` code in version `1.1.2` I found a part of the code confusing and seems redundant:

```python
except exc.SQLAlchemyError as err:
	# GH34431
	msg = ""(1054, \""Unknown column 'inf' in 'field list'\"")""
	err_text = str(err.orig)
	if re.search(msg, err_text):
		raise ValueError(""inf cannot be used with MySQL"") from err
	else:
		raise err
```

This is in pandas.io.sql line 1400-1407.


`err.orig` part seems wrong, and does not exist as a method in the sqlalchemy class `SQLAlchemyError`.

Should I remove it or am I missing something?"
703229648,36412,Bump flake8 version in pre-commit-config.yaml,dsaxton,closed,2020-09-17T03:11:47Z,2020-09-17T12:13:21Z,"The flake8 version used in pre-commit-config.yaml seems to emit spurious errors when there are mypy comments for ignoring specific kinds of type errors. Bumping to the latest version of flake8 seems to fix this.

```
flake8...................................................................Failed
- hook id: flake8
- exit code: 1

pandas/core/generic.py:701:36: F821 undefined name 'ignore'
pandas/core/generic.py:701:36: F821 undefined name 'arg'
pandas/core/generic.py:4220:39: F723 syntax error in type comment 'ignore[return-value, arg-type]'
pandas/core/generic.py:4283:39: F723 syntax error in type comment 'ignore[return-value, arg-type]'
```

For reference this is line 701 that it's complaining about:
```python
            new_values, *new_axes  # type: ignore[arg-type]
```

ref https://github.com/PyCQA/pyflakes/pull/455"
703558621,36425,"BUG: If a dataframe element is a list, using `.extend()` method affects the whole column",eric-downes,closed,2020-09-17T12:51:38Z,2020-09-17T13:56:42Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample

in pandas 1.1.1:

```python
import pandas as pd
df = pd.DataFrame({'a': [[]]*10}, index = range(10))
df.loc[0,'a'].extend([1])
assert (df.a.str.len()).sum() == 1 # fails
```

#### Problem description

The entire `a` column should not be filled when using `.loc` to reference a single element.  Instead, we get this
```python
>>> df
     a
0  [1]
1  [1]
2  [1]
3  [1]
4  [1]
5  [1]
6  [1]
7  [1]
8  [1]
9  [1]
```

I am not aware of another way of doing this, now that `.ix` has been removed.  It is inelegant to store iterables inside of df elements, but often necessary (e.g. temporary records of inconsistent length) or when `result_type='expand'` is not appropriate.

#### Expected Output

The script at the top should not trigger the assert; `df` should be:

```python
>>> df 
     a
0  [1]
1  []
2  []
3  []
4  []
5  []
6  []
7  []
8  []
9  []
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.19.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 47.1.0
Cython           : 0.29.20
pytest           : 6.0.1
hypothesis       : 5.33.1
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : None
xlwt             : 1.3.0
numba            : 0.51.2
</details>
"
703623672,36428,Update isort version in pre-commit config,clbarnes,closed,2020-09-17T14:06:12Z,2020-09-17T16:34:16Z,"Previously, a mismatch between the isort versions specified by
pre-commit and by the files used on CI made it impossible to make a
commit which passed both pre-commit and CI lints.

See #36426 . This is not a complete solution as the flake8 version differences are intentional and hopefully temporary (see https://github.com/pandas-dev/pandas/pull/36412#issuecomment-694072443 ), but it should resolve the current breakage."
703036599,36406,CLN: Clean series/test_arithmetic.py,dsaxton,closed,2020-09-16T20:01:22Z,2020-09-17T16:39:49Z,
702406902,36394,CLN: remove unnecessary _convert_index_indexer,jbrockmendel,closed,2020-09-16T02:55:20Z,2020-09-17T16:51:05Z,
702299571,36391,REF: share __getitem__ for Categorical/PandasArray/DTA/TDA/PA,jbrockmendel,closed,2020-09-15T21:54:39Z,2020-09-17T16:51:50Z,"
"
703236633,36414,REF: _validate_foo pattern for IntervalArray,jbrockmendel,closed,2020-09-17T03:32:13Z,2020-09-17T16:52:21Z,"Makes clear that these have very similar logic, can be shared after a few tweaks."
703247294,36415,REF: re-use validate_listlike for _convert_arr_indexer,jbrockmendel,closed,2020-09-17T04:04:58Z,2020-09-17T16:54:33Z,
703023139,36404,BUG: Categorical.sort_values inplace breaking views,jbrockmendel,closed,2020-09-16T19:37:25Z,2020-09-17T17:06:36Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
703247402,36416,REF: re-use _maybe_promote for _is_convertible_to_index_for_join,jbrockmendel,closed,2020-09-17T04:05:15Z,2020-09-17T17:07:09Z,
700511964,36325,PERF: StringArray construction,topper-123,closed,2020-09-13T06:52:50Z,2020-09-17T17:10:26Z,"Currently, when constructing through `Series(data, dtype=""string"")`, pandas first converts to strings/NA, then does a check that all scalars are actually strings or NA. The check is not needed in cases where we explicitly already have converted.

Performance example:

```python
>>> x = np.array([str(u) for u in range(1_000_000)], dtype=object)
>>> %timeit pd.Series(x, dtype=""string"")
357 ms ± 40.2 ms per loop  # v1.1.0
148 ms ± 713 µs per loop  # after #35519
26.3 ms ± 291 µs per loop  # after #36304
12.6 ms ± 115 µs per loop  # this PR
```

xref #35519, #36304 & #36317."
703304273,36417,REF: use BlockManager.to_native_types in formatting code,jorisvandenbossche,closed,2020-09-17T06:21:39Z,2020-09-17T18:40:47Z,Small follow-up on https://github.com/pandas-dev/pandas/pull/36150
688800963,35998,QST: What is the use of cell_context in the Styler?,grantstead,closed,2020-08-30T22:09:05Z,2020-09-17T19:04:01Z,"- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

**Note**: If you'd still like to submit a question, please read [this guide](
https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your question.

In pandas/io/formats/style.py (https://github.com/pandas-dev/pandas/blob/3ef3617c2e784b98db83ab437e7607459f68a1cc/pandas/io/formats/style.py#L265) the cell_context dict is created (empty).  It is referenced twice (on line 318 and 382), where values are accessed, if they are present.  The dict is however never updated.

What is the use of this variable?  It seems to me to be quite useless as there is no way to set it's content.  Is it purely there for future use?  I use v0.23 where it was already like this, so I expected it to have changed in master, but this doesn't seem to be the case.  Is there some way of setting it that I'm not aware of?

There are a few stack overflow questions (https://stackoverflow.com/questions/55236631/add-custom-css-class-to-column-or-cell-of-a-dataframe and https://stackoverflow.com/questions/62075616/add-style-class-to-pandas-dataframe-html) )by others,s not me) that ask how to set the class of a cell.  These seem related to how this dict's values could be set.  To ""implement"" support for classes, I simply overwrote the render method to call _translate and then to adjust the d[""body']'s content where I added to the cell's class value.  If cell_context could be set, then there would be no need to do this overriding after the fact (and actually benefit from the looking that is performed against cell_context)...   (I would like to respond to these two questions, possibly with my monkey-patching, but preferably with what happens in _translate()).

Thanks,
Grant

"
687366142,35928,Inconsistencies between python/cython groupby.agg behavior,jbrockmendel,closed,2020-08-27T16:08:57Z,2020-09-17T20:56:49Z,"This is pretty ugly, but tentatively is sufficient to make #34997 pass.

The upshot is that we have two problems:

1) in libreduction setting `setattr(cached_ityp, '_index_data',  islider.buf)` silently does the wrong thing for EA-backed indexes
2) when we go through the non-libreduction path, we do things slightly differently, which requires more patches to get tests passing.

cc @WillAyd
"
703605311,36427,DOC: Fix typo in docstring 'handler' --> 'handle',y2kbugger,closed,2020-09-17T13:46:49Z,2020-09-17T21:20:13Z,
701877627,36382,ADMIN: Update stale PR action,dsaxton,closed,2020-09-15T12:13:44Z,2020-09-17T22:03:07Z,I think also that it could be good to skip the PR message for when debugging is turned off and only label / manually review the labeled PRs to start (until we're comfortable with the tagging logic and message)
702803596,36397,BLD/CI: fix arm64 builds,jreback,closed,2020-09-16T14:14:45Z,2020-09-17T23:12:10Z,"https://travis-ci.org/github/pandas-dev/pandas/jobs/727691006

these are failing (and have been for a while).

I believe we *can* now use the numpy wheels so this should make this build fast enough to include in the main matrix (and avoid using the often ignored allow_failures).

- [ ] use wheels for numpy
- [ ] remove from the allow_failures matrix

this could be for 1.1.3 or 1.2"
617160052,34149,CI: Moving ARM64 build from allowed failures,ossdev07,closed,2020-05-13T05:44:58Z,2020-09-17T23:12:18Z,"@jreback ,

As suggested in #30641 , adding an issue on moving ARM64 build from allow_failures tag in Travis-CI. "
702910595,36400,REF: implement putmask for CI/DTI/TDI/PI,jbrockmendel,closed,2020-09-16T16:34:35Z,2020-09-17T23:20:00Z,Avoids casting to ndarray which in some cases means an object-dtype cast.
701450157,36372,REF: de-duplicate IntervalIndex compat code,jbrockmendel,closed,2020-09-14T21:41:47Z,2020-09-17T23:49:47Z,"cc @jschendel can you suggest a sentence for a docstring on why is_overlapping is the relevant check for these behaviors?

Maybe something related to `index.get_loc(some_int)` can be a non-integer?"
701570353,36374,BUG: FooIndex.insert casting datetimelike NAs incorrectly,jbrockmendel,closed,2020-09-15T03:10:16Z,2020-09-17T23:50:12Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

"
704173545,36445,import pandas doesn't work using VS Code through Anaconda.,ZacharyJWyman,closed,2020-09-18T07:56:58Z,2020-09-18T07:58:34Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd
```
output:
```
PS C:\Users\19712\OneDrive\Desktop\sentiment\Scripts> & C:/Users/19712/anaconda3/python.exe c:/Users/19712/OneDrive/Desktop/sentiment/Scripts/clean.py
Traceback (most recent call last):
  File ""c:/Users/19712/OneDrive/Desktop/sentiment/Scripts/clean.py"", line 1, in <module>
    import pandas as pd
  File ""C:\Users\19712\anaconda3\lib\site-packages\pandas\__init__.py"", line 32, in <module>
    from pandas._libs import hashtable as _hashtable, lib as _lib, tslib as _tslib
  File ""C:\Users\19712\anaconda3\lib\site-packages\pandas\_libs\__init__.py"", line 3, in <module>
    from .tslibs import (
  File ""C:\Users\19712\anaconda3\lib\site-packages\pandas\_libs\tslibs\__init__.py"", line 3, in <module>
    from .conversion import localize_pydatetime, normalize_date
  File ""pandas\_libs\tslibs\c_timestamp.pyx"", line 1, in init pandas._libs.tslibs.c_timestamp
  File ""pandas\_libs\tslibs\tzconversion.pyx"", line 1, in init pandas._libs.tslibs.tzconversion
  File ""pandas\_libs\tslibs\timedeltas.pyx"", line 1, in init pandas._libs.tslibs.timedeltas
  File ""pandas\_libs\tslibs\offsets.pyx"", line 1, in init pandas._libs.tslibs.offsets
  File ""pandas\_libs\tslibs\ccalendar.pyx"", line 12, in init pandas._libs.tslibs.ccalendar
  File ""C:\Users\19712\anaconda3\lib\site-packages\pandas\_config\__init__.py"", line 19, in <module>
    from pandas._config import dates  # noqa:F401
  File ""C:\Users\19712\anaconda3\lib\site-packages\pandas\_config\dates.py"", line 18, in <module>
    cf.register_option(
  File ""C:\Users\19712\anaconda3\lib\site-packages\pandas\_config\config.py"", line 732, in inner
    return func(pkey, *args, **kwds)
  File ""C:\Users\19712\anaconda3\lib\site-packages\pandas\_config\config.py"", line 456, in register_option
    if not re.match(""^"" + tokenize.Name + ""$"", k):  # type: ignore
AttributeError: module 'tokenize' has no attribute 'Name'
```"
704067967,36441,CLN: remove trailing commas,lacrosse91,closed,2020-09-18T04:10:34Z,2020-09-18T08:06:31Z,"#35925
Checked the following file to make sure they are ready for black :
- pandas/tests/groupby/transform/test_numba.py

CC @MarcoGorelli "
162308805,13513,ENH: add time-window capability to .rolling,jreback,closed,2016-06-26T02:15:05Z,2020-09-18T11:02:50Z,"xref #13327
closes #936 

This [notebook shows the usecase](http://nbviewer.jupyter.org/gist/jreback/186d09a99902a17a095d99ac6a5e4cd3)
- implement lint checking for cython (currently only for windows.pyx), xref #12995

This implements time-ware windows, IOW, to a `.rolling()` you can now pass a ragged / sparse timeseries and have it work with an offset (e.g. like `2s`). Previously you _could_ achieve these results by resampling first, then using an integer period (though you would have to jump thru hoops when crossing day boundaries and such). 

This now provides a nice easy / performant implementation (as indicated on the linked notebook, min/max impl is giving scaling issues).

```
In [1]: df = DataFrame({'B': range(5)})

In [2]: df.index = [Timestamp('20130101 09:00:00'),
   ...:             Timestamp('20130101 09:00:02'),
   ...:             Timestamp('20130101 09:00:03'),
   ...:             Timestamp('20130101 09:00:05'),
   ...:             Timestamp('20130101 09:00:06')]

In [3]: df
Out[3]: 
                     B
2013-01-01 09:00:00  0
2013-01-01 09:00:02  1
2013-01-01 09:00:03  2
2013-01-01 09:00:05  3
2013-01-01 09:00:06  4

In [4]: df.rolling(2, min_periods=1).sum()
Out[4]: 
                       B
2013-01-01 09:00:00  0.0
2013-01-01 09:00:02  1.0
2013-01-01 09:00:03  3.0
2013-01-01 09:00:05  5.0
2013-01-01 09:00:06  7.0

In [5]: df.rolling('2s', min_periods=1).sum()
Out[5]: 
                       B
2013-01-01 09:00:00  0.0
2013-01-01 09:00:02  1.0
2013-01-01 09:00:03  3.0
2013-01-01 09:00:05  3.0
2013-01-01 09:00:06  7.0
```
"
701153171,36358,Fix documentation for new float_precision on read_csv,Dr-Irv,closed,2020-09-14T14:25:26Z,2020-09-18T11:32:37Z,"- [x] closes https://github.com/pandas-dev/pandas/pull/36228#discussion_r487587534
- [ ] tests added / passed
     N/A
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
     N/A

A followup to PR #36228 to get the documentation string right for version changed. (was in wrong place, missing blank line)

"
694102668,36141,BUG: copying series into empty dataframe does not preserve dataframe index name,Dr-Irv,closed,2020-09-05T14:58:57Z,2020-09-18T11:34:07Z,"- [x] closes #31368
- [x] tests added / passed
   `indexing/test_partial.py:TestPartialSetting:test_index_name_empty`
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
694181735,36149,Make to_numeric default to correct precision,Dr-Irv,closed,2020-09-05T20:59:20Z,2020-09-18T11:34:47Z,"- [x] closes #31364
- [x] tests added / passed
  - tools/test_to_numeric.py:test_precision
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This relates to a very old issue #8002 where the default precision for CSV files could create wrong answers in the last bit.  @jreback was involved in the PR review for that, which created the default to not be the high precision parser.

For `to_numeric()`, I switched it to use `precise_xstrtod` instead of `xstrtod`  by default.

Unknown whether there are performance implications for `to_numeric()`, although this comment https://github.com/pandas-dev/pandas/issues/17154#issuecomment-319917647 from @jorisvandenbossche indicates that maybe we should consider switching to the higher precision parser by default in `read_csv()` anyway. 

Open question as to whether performance analysis of *this* PR is needed beyond what is shown below, which seems to indicate that using `precise_xstrtod` is *faster* than `xstrtod` 

The other possibility is to use a keyword argument in `to_numeric` that would allow the higher precision parser.  "
696125412,36228,"Change default of float_precision for read_csv and read_table to ""high""",Dr-Irv,closed,2020-09-08T19:35:57Z,2020-09-18T11:35:31Z,"- [x] closes #17154
- [x] tests added / passed
   - modified `tests/io/parser/test_c_parser.py` to make sure all 4 options are tested
   - added `tests/io/parser/test_c_parser.py:test_high_is_default`
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
  - for version 1.2

See discussion at bottom of #36149 for the performance tests.  Added `float_precision=""legacy""` so people can pick up the old parser.  Can't change default to ""high"" because of incompatibility with python parser
"
688400050,35969,QST: to_excel() deletes the existed sheets,MFalghanmi,closed,2020-08-28T23:11:06Z,2020-09-18T11:45:05Z,"- As per Pandas [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html), the to_excel() deletes the existed sheets in the excel document using the code below. I am not sure what is the issue.
```python
with pd.ExcelWriter('pay_past_5_years.xlsx', mode='a') as writer:  
    df.to_excel(writer, sheet_name='Sheet_name_3')
```
- The second question the mode is give error when using small 'a' char in mode. Here tip was found in [stackoverflow](https://stackoverflow.com/questions/54863238/pandas-excelwriter-valueerror-append-mode-is-not-supported-with-xlsxwriter/55683301)
```python
ValueError: Append mode is not supported with xlsxwriter!
````
**The Pandas version is 1.0.5**

Thank you
"
703910147,36435,"DOC: read_excel supports skiprows argument like read_csv, but tests and docs needed",Dr-Irv,closed,2020-09-17T21:05:54Z,2020-09-18T12:50:06Z,"#### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html
https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html

#### Documentation problem

In `read_csv`, we say that we support:
**skiprows           list-like, int or callable, optional**

_Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file._

_If callable, the callable function will be evaluated against the row indices, returning True
if the row should be skipped and False otherwise. An example of a valid callable argument 
would be lambda x: x in [0, 2]._


In `read_excel`, we say that we support:

**skiprows     list-like**

_Rows to skip at the beginning (0-indexed)._

It turns out that the `int` and `callable` arguments work fine with `read_excel()`, so we should indicate that in the documentation. 
 We also need to add tests for those 2 cases.

#### Suggested fix for documentation and additional tests

- copy the `read_csv` doc for `skiprows` over to `read_excel`
- add tests to `tests/io/excel/test_readers.py` for the `int` and `callable` options"
704250992,36446,CLN: 35925 rm trailing commas,lacrosse91,closed,2020-09-18T09:58:06Z,2020-09-18T13:00:20Z,"#35925
Checked the following files to make sure they are ready for black :

- /pandas/tests/indexes/base_class/test_indexing.py
- /pandas/tests/indexes/test_common.py
- /pandas/tests/indexes/test_numeric.py
"
704385812,36449,CLN upgrade to Python3.7+ syntax,MarcoGorelli,closed,2020-09-18T13:37:11Z,2020-09-18T13:38:26Z,"Given that the next version will be Python3.7+, there's some nice updates we can make"
703987081,36437,DOC: read_excel skiprows documentation matches read_csv (#36435),ahgamut,closed,2020-09-18T00:08:52Z,2020-09-18T14:02:19Z,"- [x] closes #36435 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Updated `read_excel` documentation to match `read_csv` for the `skiprows` parameter.
Changed `test_read_excel_skiprows_list` to `test_read_excel_skiprows`: now also tests `read_excel` with an `int` and a `callable`. "
704030441,36439,REF: share insert between DTI/TDI/PI,jbrockmendel,closed,2020-09-18T02:25:08Z,2020-09-18T14:24:39Z,
704000405,36438,REF: collect IntervalArray methods by topic,jbrockmendel,closed,2020-09-18T00:50:43Z,2020-09-18T14:27:30Z,cosmetic edits I find myself tempted to do on a bunch of branches
702864792,36398,Backport PR #36378: DOC: move release note for #36175 (pt1),simonjayhawkins,closed,2020-09-16T15:30:46Z,2020-09-18T15:31:05Z,#36378
666459305,35428,WIP BUG: Inconsistent date parsing of to_datetime,arw2019,closed,2020-07-27T17:25:40Z,2020-09-18T15:34:56Z,"- [x] closes #12585
- [x] tests added / passed
- [ ] documentation updated 
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry (pending 1.2)

This PR is an attempt to address concerns about datetime parsing. I'll update the docs if people approve this change.

#### Original problem
From #12585:
``` python
In [3]: pd.to_datetime([""31/12/2014"", ""10/03/2011""]) 
   ...:                                                                                                               
Out[3]: DatetimeIndex(['2014-12-31', '2011-03-10'], dtype='datetime64[ns]', freq=None)
```
The issue is that the first item is parsed as `DD\MM\YYYY` whereas for the second the format is `MM\DD\YYYY`, and there is no message to alert the user of inconsistency in the output.

The problem lies with the `dayfirst` argument that selects between `DD\MM\YYYY` and `MM\DD\YYYY`. In the parser:
https://github.com/pandas-dev/pandas/blob/04e9e0afd476b1b8bed930e47bf60ee20fa78f38/pandas/_libs/tslibs/parsing.pyx#L94-L163
we try  `DD\MM\YYYY` or `MM\DD\YYYY` first depending on `dayfirst` but if the result is invalid we try the other. 

#### Proposed change
I'd like to raise warnings whenever we parse in contradiction to the directive of `dayfirst`. The OP, where `dayfirst` defaults to `False`, now raises a Warning:
``` python
In [4]: pd.to_datetime([""31/12/2014"", ""10/03/2011""], dayfirst=False) 
   ...:                                                                                                               
/workspaces/pandas-arw2019/pandas/core/arrays/datetimes.py:2044: UserWarning: Parsing 31/12/2014 DD/MM format.
  result, tz_parsed = tslib.array_to_datetime(
Out[4]: DatetimeIndex(['2014-12-31', '2011-10-03'], dtype='datetime64[ns]', freq=None)
```
A downside of this change is that sometimes a consistent output produces unnecessary warnings (due to mismatch with the default value of `dayfirst`):
``` python
In [11]: pd.to_datetime([""31/12/2014"", ""30/04/2011"", ""31/05/2016""] )                                                 
/workspaces/pandas-arw2019/pandas/core/arrays/datetimes.py:2044: UserWarning: Parsing 31/12/2014 DD/MM format.
  result, tz_parsed = tslib.array_to_datetime(
/workspaces/pandas-arw2019/pandas/core/arrays/datetimes.py:2044: UserWarning: Parsing 30/04/2011 DD/MM format.
  result, tz_parsed = tslib.array_to_datetime(
/workspaces/pandas-arw2019/pandas/core/arrays/datetimes.py:2044: UserWarning: Parsing 31/05/2016 DD/MM format.
  result, tz_parsed = tslib.array_to_datetime(
Out[11]: DatetimeIndex(['2014-12-31', '2011-04-30', '2016-05-31'], dtype='datetime64[ns]', freq=None)
```
but these can be silenced with `infer_datetime_format`:
``` python
In [12]: pd.to_datetime([""31/12/2014"", ""30/04/2011"", ""31/05/2016""], infer_datetime_format=True )                      
Out[12]: DatetimeIndex(['2014-12-31', '2011-04-30', '2016-05-31'], dtype='datetime64[ns]', freq=None)
```
For invalid input (if it cannot be parsed according to a single format) we will *always* raise a warning, even with `infer_datetime_format=True`:
``` python
In [13]: pd.to_datetime([""31/12/2014"", ""03/31/2011""], infer_datetime_format=True)                                     
/workspaces/pandas-arw2019/pandas/core/arrays/datetimes.py:2044: UserWarning: Parsing 31/12/2014 DD/MM format.
  result, tz_parsed = tslib.array_to_datetime(
Out[13]: DatetimeIndex(['2014-12-31', '2011-03-31'], dtype='datetime64[ns]', freq=None)
```

#### Remaining issues
Personally I can see a case for raising an error with 
``` python
In [13]: pd.to_datetime([""31/12/2014"", ""03/31/2011""], infer_datetime_format=True)                                     
```
since there's no valid way to process that. I also think that 
``` python
In [11]: pd.to_datetime([""31/12/2014"", ""30/04/2011"", ""31/05/2016""])                                                 
```
shouldn't be raising warnings, but I feel much less strongly about that. That said, changing this behaviour will require more heavy-handed alterations to the codebase (and maybe performance implications - not sure).

I think that the warnings I added here will allow users to figure out the origin of these problems when they turn up and that should do away with most of the headaches."
704089245,36442,Update 03_subset_data.rst,anishmo99,closed,2020-09-18T05:11:24Z,2020-09-18T16:50:59Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
598897368,33524,ENH: Canonic SI frequency,JoElfner,closed,2020-04-13T13:43:44Z,2020-09-18T16:52:33Z,"Since pandas is widely used for data science, including working with time series, a canonic representation of the frequency, as defined by SI standards, physics etc. would be a nice improvement.

To summarize differences between the general definition of frequency and the pandas definition of frequency:
- General definition: Frequency is in the unit of `1/time_period`, for SI unis more specifically `1/second=1Hz`.
- pandas definition: Frequency is the time period, so it is exactly the opposite of the physical/general definition. 

Since many calculations require the actual frequency, for example summing up velocities to get the length, this would facilitate many calculations.

#### Proposed solution:
Additionally `pd.DatetimeIndex` etc. should get a new attribute, f.i. named `freq_canonic`, that represents the actual frequency of the index in Hertz.

To avoid breaking the API, I'd implement the frequency as a data attribute, which can be accessed by the user, but has no effect on time series calculations.

I could try making a pull request for this feature, if you are interested."
572651316,32325,ENH: Syntactic sugar for DatetimeIndex datetime slicing,Alpima-Quant,closed,2020-02-28T10:03:06Z,2020-09-18T17:09:57Z,"```python
import pandas as pd
from datetime import datetime
index = pd.date_range(start=datetime(2019, 1, 1), end=datetime(2020, 1, 1), freq='D')
ts = pd.Series(index=index, dtype=float)
start_date = datetime(2019, 6, 1)
index_from_ts_slice = ts[start_date:].index
# this errors:
try:
    index_from_index_slice = index[start_date:]
except TypeError as e:
    print(e)
# I can use slice_indexer to get what I want but it is uglier
index_from_slice_indexer = index[index.slice_indexer(start_date)]
assert (index_from_ts_slice == index_from_slice_indexer).all()
```
When a Series or a DataFrame has a sorted DatetimeIndex, one can slice these structures using datetimes and square brackets notation. However this kind of slicing is not supported directly on a DatetimeIndex.
Basically I really like the datetime slicing notation and I'd like index[start_date:] to be able to return the same slice of the index as ts[start_date:].index or index[index.slice_indexer(start_date)]


<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.9.93-boot2docker
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.0
numpy            : 1.14.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 39.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 0.9999999
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.4.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 2.0.0
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 0.17.1
sqlalchemy       : None
tables           : 3.4.2
tabulate         : None
xarray           : None
xlrd             : 1.1.0
xlwt             : None
xlsxwriter       : None
numba            : 0.35.0

</details>
"
704547271,36456,DOC: replaced link to the documentation about setting a value on a co…,wprazuch,closed,2020-09-18T17:41:11Z,2020-09-18T19:01:31Z,"…py of a slice from a DataFrame

- [x] closes #36448
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
704539244,36455,CLN: Clean some formatting,dsaxton,closed,2020-09-18T17:26:21Z,2020-09-18T20:21:59Z,ref https://github.com/pandas-dev/pandas/issues/36450
703953931,36436,CI: Failed Travis jobs due to GBQ,dsaxton,closed,2020-09-17T22:37:52Z,2020-09-18T21:30:42Z,"Lots of failed jobs coming from Travis lately with tracebacks like this:

```python
    @pytest.fixture()
    def gbq_dataset(self):
        # Setup Dataset
        _skip_if_no_project_id()
        _skip_if_no_private_key_path()
    
        dataset_id = ""pydata_pandas_bq_testing_"" + generate_rand_str()
    
>       self.client = _get_client()

pandas/tests/io/test_gbq.py:160: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pandas/tests/io/test_gbq.py:68: in _get_client
    return bigquery.Client(project=project_id, credentials=credentials)
../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/google/cloud/bigquery/client.py:179: in __init__
    project=project, credentials=credentials, _http=_http
../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/google/cloud/client.py:250: in __init__
    Client.__init__(self, credentials=credentials, client_options=client_options, _http=_http)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <google.cloud.bigquery.client.Client object at 0x7f5c46e157d0>
credentials = <google.oauth2.service_account.Credentials object at 0x7f5c46e15bd0>
_http = None
client_options = ClientOptions: {'api_endpoint': None, 'client_cert_source': None}

    def __init__(self, credentials=None, _http=None, client_options=None):
        if isinstance(client_options, dict):
            client_options = google.api_core.client_options.from_dict(client_options)
        if client_options is None:
            client_options = google.api_core.client_options.ClientOptions()
    
>       if credentials and client_options.credentials_file:
E       AttributeError: 'ClientOptions' object has no attribute 'credentials_file'
```

Line that's raising:

https://github.com/pandas-dev/pandas/blob/c8b44dda08ab3a2feb2c4572be960469c3f065f7/pandas/tests/io/test_gbq.py#L160

Sample Travis log:

https://api.travis-ci.org/v3/job/728073325/log.txt

cc @tswast if any ideas"
701687192,36377,BUG: Weird behavior for comparison operations `eq` and `ne`,YarShev,closed,2020-09-15T07:31:17Z,2020-09-18T21:55:06Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np
random_state = np.random.RandomState(seed=42)
ncols = 64
nrows = 156
test_data = {
    ""col{}"".format(int(i)): random_state.randint(
        0, 100, size=(nrows)
    )
    for i in range(ncols)
}
df = pd.DataFrame(test_data)
df.eq(""a"") # it works good
nrows = 157 # change row count
test_data = {
    ""col{}"".format(int(i)): random_state.randint(
        0, 100, size=(nrows)
    )
    for i in range(ncols)
}
df = pd.DataFrame(test_data)
df.eq(""a"") # it doesn't work
# ValueError: unknown type str32
nrows = 156 # return row count to valid value
ncols = 65 # change col count
test_data = {
    ""col{}"".format(int(i)): random_state.randint(
        0, 100, size=(nrows)
    )
    for i in range(ncols)
}
df = pd.DataFrame(test_data)
df.eq(""a"") # it doesn't work as well
# ValueError: unknown type str32
```

#### Problem description

Looks like the problem is related to size of operands for comparison operations. Could anyone explain please? Is it normal behavior?

#### Output of ``pd.show_versions()``

<details>

pandas : 1.1.2
numpy : 1.18.4
pytz : 2020.1
dateutil : 2.8.1
pip : 20.1.1
setuptools : 41.2.0
Cython : None
pytest : 5.4.2
hypothesis : None
sphinx : None
blosc : None
feather : 0.4.1
xlsxwriter : None
lxml.etree : 4.5.0
html5lib : None
pymysql : None
psycopg2 : None
jinja2 : 2.11.2
IPython : 7.14.0
pandas_datareader: None
bs4 : 4.9.1
bottleneck : None
fsspec : 0.7.3
fastparquet : None
gcsfs : None
matplotlib : 3.2.1
numexpr : 2.7.1
odfpy : None
openpyxl : 3.0.3
pandas_gbq : None
pyarrow : 0.16.0
pytables : None
pyxlsb : None
s3fs : 0.4.2
scipy : 1.4.1
sqlalchemy : 1.3.17
tables : 3.6.1
tabulate : None
xarray : 0.15.1
xlrd : 1.2.0
xlwt : None
numba : None

</details>
"
678214909,35700,BUG: pandas 1.1.0 introduces string comparison bug for larger datasets,Samreay,closed,2020-08-13T07:22:54Z,2020-09-18T21:55:06Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

# This works fine
df1 = pd.DataFrame({""a"": [1,2,3], ""b"": [""x"",""y"",""z""]})
print(df1 == ""x"")

# Loading in some bigger data from Kaggle https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data
# data and code file included in zip to make it easy
df2 = pd.read_csv(""AB_NYC_2019.csv"")

# On pandas v1.1.0 throws a ValueError: unknown type str32
print(df2 == ""x"")

# On pandas v1.1.0 throws a ValueError: unknown type str128
print(df2 == ""John"")
```
[reproduce.zip](https://github.com/pandas-dev/pandas/files/5067660/reproduce.zip)

#### Problem description

Previously comparing the dataframe to a string value would give a boolean mask as expected. It is now erroring. Confirmed that on v1.0.5 this works as expected, only on 1.1.0 does this error.

#### Expected Output

A boolean mask of true or false values with the same shape as the dataframe.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.3.1.post20200810
Cython           : 0.29.21
pytest           : 6.0.1
hypothesis       : 5.24.0
sphinx           : 3.2.0
blosc            : None
feather          : 0.4.0
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.0
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
117802835,11645,Bug when computing rolling_mean with extreme value,julienvienne,closed,2015-11-19T12:19:03Z,2020-09-18T21:59:24Z,"Hello,
Please consider the following code :

```
import pandas as pd
import numpy as ny

dates = pd.date_range(""2015-01-01"", periods=10, freq=""D"")
ts = pd.TimeSeries(data=range(10), index=dates, dtype=ny.float64)
ts_mean = pd.rolling_mean(ts, 5)
print(ts) 
2015-01-01    0
2015-01-02    1
2015-01-03    2
2015-01-04    3
2015-01-05    4
2015-01-06    5
2015-01-07    6
2015-01-08    7
2015-01-09    8
2015-01-10    9
Freq: D, dtype: float64

print(ts_mean)
2015-01-01   NaN
2015-01-02   NaN
2015-01-03   NaN
2015-01-04   NaN
2015-01-05     2
2015-01-06     3
2015-01-07     4
2015-01-08     5
2015-01-09     6
2015-01-10     7
Freq: D, dtype: float64
```

For the last date (2015-01-10), you should obtain 7, which corresponds to [5, 6, 7, 8, 9] mean value.
Now, please replace the 2015-01-03 value by -9+33 extreme value.

```
dates = pd.date_range(""2015-01-01"", periods=10, freq=""D"")
ts = pd.TimeSeries(data=range(10), index=dates, dtype=ny.float64)
ts[2] = -9e+33
print(ts)
2015-01-01    0.000000e+00
2015-01-02    1.000000e+00
2015-01-03   -9.000000e+33
2015-01-04    3.000000e+00
2015-01-05    4.000000e+00
2015-01-06    5.000000e+00
2015-01-07    6.000000e+00
2015-01-08    7.000000e+00
2015-01-09    8.000000e+00
2015-01-10    9.000000e+00
Freq: D, dtype: float64
```

And compute rolling_mean again :

```
ts_mean = pd.rolling_mean(ts, 5)
print(ts_mean)
2015-01-01             NaN
2015-01-02             NaN
2015-01-03             NaN
2015-01-04             NaN
2015-01-05   -1.800000e+33
2015-01-06   -1.800000e+33
2015-01-07   -1.800000e+33
2015-01-08    0.000000e+00
2015-01-09    1.000000e+00
2015-01-10    2.000000e+00
Freq: D, dtype: float64
```

As you can see,  from the 2015-01-08,  computation returns an incorrect result i.e [1, 2, 3] instead of [5, 6, 7]. The extreme value has introduced some perturbations in following date computation.

Best regards,
"
487789368,28244,data frame rolling std return wrong result with large elements,KaneFu,closed,2019-08-31T16:53:21Z,2020-09-18T21:59:52Z,"#### Code Sample, a copy-pastable example if possible

```python
arr = [9.54e+08, 6.225e-01, np.nan, 0, 1.14, 0]
arr = pd.DataFrame(arr)
print(arr.rolling(5,3).std())
###output
              0
0           NaN
1           NaN
2           NaN
3  5.507922e+08
4  4.770000e+08
5  0.000000e+00
#### Problem description

Expected output.iloc[5,0] = 0.551, as we can see the standeviation of last five elements shouldn't be zero. <br>
But if i change the first element from 9.54e+08 to 9.45e+07, the last standeviation is 0.816 which is still wrong. If i change the first element to 9.45e+5, everything is okay.  <br>
The first element should not affect the result since the window=5, but there seems to be a bug with large elements, and a magnitude of e+08 definitely will not overflow.  <br>
There must be some bugs.


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.1.final.0
python-bits: 64
OS: Darwin
OS-release: 18.6.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: en_US.UTF-8
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 3.0.7
pip: 19.2.1
setuptools: 36.4.0
Cython: 0.25.2
numpy: 1.13.1
scipy: 0.19.1
pyarrow: None
xarray: 0.12.3
IPython: 5.3.0
sphinx: 1.5.6
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.2
feather: None
matplotlib: 2.0.2
openpyxl: 2.4.7
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: 0.9.6
lxml.etree: 3.7.3
bs4: 4.6.0
</details>"
700688046,36349,ADMIN: Auto-label PRs for review,dsaxton,closed,2020-09-14T01:01:07Z,2020-09-18T22:11:03Z,"Not certain this will work, but nonetheless I think something like this would be helpful (analogous to ""Needs Triage"" for issues)"
703827903,36433,[DOC]: Add warning about rolling sums with large values,phofl,closed,2020-09-17T18:53:37Z,2020-09-18T22:11:36Z,"- [x] closes #11645 
- [x] closes #28244 (probably closes that one too)
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Added a warning within the user guide.
"
704390160,36450,CLN upgrade to Python3.7+ syntax,MarcoGorelli,closed,2020-09-18T13:43:02Z,2020-09-18T22:20:50Z,"Given that the next release will we Python3.7+, we might as well upgrade some of the syntax, e.g.

```diff
-    for make_index_func in make_index_funcs:
-        yield make_index_func
+    yield from make_index_funcs
```
or
```diff
-            return ""%s[%s, %s]"" % (cls, tp_repr, metadata_reprs)
+            return f""{cls}[{tp_repr}, {metadata_reprs}]""
```

We can do this using [pyupgrade](https://github.com/asottile/pyupgrade).

Here are the files that can be upgraded:

- [ ] pandas/_config/display.py
- [ ] pandas/_testing.py
- [ ] pandas/_vendored/typing_extensions.py
- [ ] pandas/_version.py
- [x] pandas/core/arrays/datetimes.py
- [x] pandas/core/arrays/sparse/array.py
- [x] pandas/core/common.py
- [x] pandas/core/computation/expr.py
- [x] pandas/core/computation/pytables.py
- [x] pandas/core/generic.py
- [x] pandas/core/groupby/base.py
- [x] pandas/core/groupby/generic.py
- [x] pandas/core/indexes/base.py
- [x] pandas/core/indexes/datetimelike.py
- [x] pandas/core/reshape/merge.py
- [ ] pandas/io/clipboard/__init__.py
- [ ] pandas/io/formats/excel.py
- [ ] pandas/io/html.py
- [ ] pandas/io/json/_json.py
- [ ] pandas/io/pytables.py
- [ ] pandas/io/stata.py
- [ ] pandas/plotting/_matplotlib/core.py
- [ ] pandas/tests/arrays/categorical/test_constructors.py
- [ ] pandas/tests/arrays/integer/test_construction.py
- [ ] pandas/tests/extension/decimal/array.py
- [ ] pandas/tests/extension/test_categorical.py
- [ ] pandas/tests/frame/test_constructors.py
- [ ] pandas/tests/groupby/test_groupby.py
- [ ] pandas/tests/groupby/test_grouping.py
- [ ] pandas/tests/indexes/test_base.py
- [ ] pandas/tests/indexing/test_indexing.py
- [ ] pandas/tests/io/formats/test_format.py
- [ ] pandas/tests/io/formats/test_to_csv.py
- [ ] pandas/tests/io/formats/test_to_html.py
- [ ] pandas/tests/io/formats/test_to_latex.py
- [ ] pandas/tests/io/json/test_ujson.py
- [ ] pandas/tests/io/parser/test_c_parser_only.py
- [ ] pandas/tests/io/parser/test_common.py
- [ ] pandas/tests/io/parser/test_encoding.py
- [ ] pandas/tests/io/parser/test_read_fwf.py
- [ ] pandas/tests/io/pytables/common.py
- [ ] pandas/tests/io/test_common.py
- [ ] pandas/tests/io/test_html.py
- [ ] pandas/tests/io/test_sql.py
- [ ] pandas/tests/reshape/test_get_dummies.py
- [ ] pandas/tests/scalar/test_na_scalar.py
- [ ] pandas/tests/series/test_analytics.py
- [ ] pandas/tests/series/test_constructors.py
- [ ] pandas/tests/series/test_dtypes.py
- [ ] pandas/tests/series/test_io.py
- [ ] scripts/validate_rst_title_capitalization.py
- [ ] scripts/validate_unwanted_patterns.py
- [ ] setup.py
- [ ] versioneer.py

If you're intereseted in helping out, please choose a batch of 10-15 to run pyupgrade on.

Install pyupgrade with:
```
pip install pyupgrade==2.7.2
```
Then, run
```
pyupgrade <files you want to upgrade> --py37-plus
```
e.g.
```
pyupgrade pandas/tests/io/test_html.py pandas/tests/io/json/test_ujson.py --py37-plus
```

See  #36453 for an example PR.

Once this is done, we can add it to pre-commit / CI."
704691721,36460,CI: Revert PR template,dsaxton,closed,2020-09-18T22:28:42Z,2020-09-18T22:50:25Z,"---

labels: ""Needs Review""

---

I don't know if this change had the intended effect, so opening this (partially as a test if it actually works)

Edit: Yeah, it didn't. I guess this doesn't work like the issue templates."
702014869,36384,REF: _is_compatible_with_other -> _can_union_without_object_cast,jbrockmendel,closed,2020-09-15T15:07:29Z,2020-09-18T23:18:17Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Changes the behavior in one test where ATM we behave differently for Series/ndarray than we do for Index arg"
704715277,36462,removed unnecessary trailing commas,sm1899,closed,2020-09-18T23:57:33Z,2020-09-19T00:05:30Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
704708476,36461,"REF: MultiIndex._validate_insert_value, IntervaArray._validate_setitem_value",jbrockmendel,closed,2020-09-18T23:27:35Z,2020-09-19T01:03:08Z,
504489086,28867,DEPR: deprecate Index.to_native_types() ?,jorisvandenbossche,closed,2019-10-09T08:17:19Z,2020-09-19T02:13:31Z,"We currently have a public `Index.to_native_types()` method (https://dev.pandas.io/docs/reference/api/pandas.Index.to_native_types.html) that somehow formats the values of the index.

Example:
```
In [3]: pd.date_range(""2012"", periods=3).to_native_types() 
Out[3]: array(['2012-01-01', '2012-01-02', '2012-01-03'], dtype=object)
```

I don't think this needs to be in the public API?"
347694350,22205,Inconsistent handling of nan-float64 in Series.isin(),realead,closed,2018-08-05T12:32:45Z,2020-09-19T02:14:25Z,"#### Code Sample, a copy-pastable example if possible

```python
N=10**6
s=pd.Series(np.full(N, np.nan, dtype=np.float64), copy=False)
s.isin([np.nan]).all()
```

results in `True`, however

```python
s=pd.Series(np.full(N+1, np.nan, dtype=np.float64), copy=False)
s.isin([np.nan]).all()
```
 results in `False`, even more  `s.isin([np.nan]).any()` results in `False`

#### Problem description

Obviously, the result should be the same in both cases.

#### Expected Output

 IMO `True` is more consistent with the behavior of Pandas in other functions (such as `pd.unique()`).

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.2.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-53-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.22.0
pytest: 3.2.1
pip: 10.0.1
setuptools: 36.5.0.post20170921
Cython: 0.28.3
numpy: 1.13.1
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.1.0
sphinx: 1.6.3
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.2
feather: None
matplotlib: 2.0.2
openpyxl: 2.4.8
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 0.9.8
lxml: 3.8.0
bs4: 4.6.0
html5lib: 0.9999999
sqlalchemy: 1.1.13
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: 0.1.3
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
698509860,36277,ENH: add peek function,Quetzalcohuatl,closed,2020-09-10T21:25:00Z,2020-09-19T03:34:41Z,"- [x]  closes #18691
- [x] tests added / passed
- [ ] passes black pandas
- [ ] passes git diff upstream/master -u -- ""*.py"" | flake8 --diff
- [x] whatsnew entry

added peek, which is essentially a concatenation of head and tail functions. The reason I add, is because often for sorted data, you are interested in looking at both the head and tail simultaneously to see the min/max range."
665127767,35401,use a try block in _expand_user instead of a typing condition,neutrinoceros,closed,2020-07-24T12:13:11Z,2020-09-19T07:22:39Z,"Noticed that `_expand_user`'s docstrings stated that expansion was performed ""if possible"", while the actual implementation was only applying expansion to str objects.
With a try block it should work with str, os.Pathlike and bytes objects (and everything else supported by `os.path.expanduser` in the future).
I except this change may be undesirable since it will force the output type to `str`. In this case I can replace this with a docstring update.
I'll complete the checklist with tests and whatsnew entry if the change is deemed desirable.

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
704720706,36463,Remove unnecessary trailing commas,sm1899,closed,2020-09-19T00:22:12Z,2020-09-19T07:36:13Z,
703316573,36418,DEPR: Index.to_native_types,jorisvandenbossche,closed,2020-09-17T06:46:25Z,2020-09-19T07:38:07Z,Closes #28867
704063181,36440,REGR: Series[numeric] comparison with str raising on numexpr path,jbrockmendel,closed,2020-09-18T03:57:06Z,2020-09-19T08:12:40Z,"- [x] closes #36377, closes #35700
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
671672973,35519,"REF: StringArray._from_sequence, use less memory",topper-123,closed,2020-08-02T19:15:46Z,2020-09-19T08:19:56Z,"- [x] closes #35499
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

@ldacey, can you try if this fixes your problem?

CC @simonjayhawkins  "
703131292,36410,CLN: sql static method,erfannariman,closed,2020-09-16T23:08:35Z,2020-09-19T10:15:10Z,"- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
702623731,36396,Backport PR #36371 on branch 1.1.x (BUG: Fix MultiIndex column stacking with dupe names),meeseeksmachine,closed,2020-09-16T09:53:39Z,2020-09-19T10:18:19Z,Backport PR #36371: BUG: Fix MultiIndex column stacking with dupe names
704833047,36473,Backport PR #36440 on branch 1.1.x (REGR: Series[numeric] comparison with str raising on numexpr path),meeseeksmachine,closed,2020-09-19T08:13:15Z,2020-09-19T10:20:14Z,Backport PR #36440: REGR: Series[numeric] comparison with str raising on numexpr path
704852268,36474,Backport PR #36266:: BUG: fix isin with nans and large arrays,simonjayhawkins,closed,2020-09-19T09:57:09Z,2020-09-19T11:14:46Z,#36266
703131191,36409,BUG: Concat typing,rhshadrach,closed,2020-09-16T23:08:20Z,2020-09-19T11:37:54Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @simonjayhawkins 

Attempting to add typing to results in aggregation.transform. Here, the obj can be a Series but the list passed into concat contain frames. mypy correctly identifies this as an issue, I think concat needs to be typed as FrameOrSeriesUnion for this purpose.

When I try just typing the function definition and not the overload, mypy complains:

    error: Overloaded function implementation does not accept all possible arguments of signature 2  [misc]

so I've changed the type of the overload to FrameOrSeriesUnion as well. I don't believe this is any weaker than the current state because it is an overload."
642295219,34878,BUG: Fix replace for different dtype equal value,dsaxton,closed,2020-06-20T03:03:15Z,2020-09-19T14:04:25Z,"Bug noticed when looking at a different issue https://github.com/pandas-dev/pandas/issues/34871#issuecomment-646664842

- [x] closes https://github.com/pandas-dev/pandas/issues/35376
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
646746916,35032,API: implement __array_function__ for ExtensionArray,simonjayhawkins,closed,2020-06-27T19:38:40Z,2020-09-19T14:14:17Z,xref #26380
457209864,26913,Tracking issue for EA Series Operations Support,ghost,closed,2019-06-18T00:53:59Z,2020-09-19T14:35:38Z,*Stale*
704888293,36477,Backport PR #36385 on branch 1.1.x (BUG: Always cast to Categorical in lexsort_indexer),meeseeksmachine,closed,2020-09-19T14:04:10Z,2020-09-19T15:07:17Z,Backport PR #36385: BUG: Always cast to Categorical in lexsort_indexer
681468537,35797,BUG: Fix is_categorical_dtype for Sparse[category],dsaxton,closed,2020-08-19T00:45:44Z,2020-09-19T16:39:42Z,"- [x] closes #35793
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
700637539,36345,PERF: styler uuid control and security,attack68,closed,2020-09-13T20:04:13Z,2020-09-19T16:42:45Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

`Styler` uuid is randomly generated from 16bytes, or 128bit entropy, which is poorly formatted for data transmission over the web.
`Styler` uuid1 is super-ceded by uuid4 which is more network secure.

This PR addresses the two above items by switching uuid method, and then coding the default entropy to 5 characters (20bit) which should be more than sufficient to avoid HTML table collision on a webpage, but make substantial data transfer savings for large tables.

uuid length remains configurable. "
704653275,36459,Align cython and python reduction code paths,jbrockmendel,closed,2020-09-18T20:57:43Z,2020-09-19T16:46:13Z,"Get rid of a few of the small behavior differences between these

(entirely disabling the cython path leads to 4 currently-xfailed tests passing, so there is still some subtle difference after this)"
704585846,36457,CLN: Update files (as per #36450) to Python 3.7+ syntax,ahgamut,closed,2020-09-18T18:52:33Z,2020-09-19T16:48:19Z,"Ran `pyupgrade` on all files unchecked in #36450. formatter pre-commit 
hook had no complaints; removed unnecessary imports from the following
to satisfy `flake8` hook:

```
pandas/tests/io/test_sql.py
pandas/test/series/test_io.py
```

- [x] closes #36450
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
704299416,36447,CLN: remove trailing commas for black update,Noahlq,closed,2020-09-18T11:17:13Z,2020-09-19T18:40:45Z,"File edited: 
pandas/tests/io/test_sql.py"
704978456,36481,Commas edit,Noahlq,closed,2020-09-19T18:42:22Z,2020-09-19T19:01:36Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
702063403,36385,BUG: Always cast to Categorical in lexsort_indexer,dsaxton,closed,2020-09-15T16:06:37Z,2020-09-19T19:39:03Z,"- [x] closes #36383
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
646541028,35023,"ASV: asvs for DataFrame.values, to_numpy",jbrockmendel,closed,2020-06-26T22:23:25Z,2020-09-19T19:55:09Z,xref #34999
701160894,36359,BUG: get_indexer methods return int64 instead of intp arrays,alexhlim,closed,2020-09-14T14:34:18Z,2020-09-19T19:56:06Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---
#### Code Sample, a copy-pastable example

```python
>>> import pandas as pd
>>> ax1 = pd.Index([1, 2, 3])
>>> ax2 = pd.Index([1, 1, 2])
>>> ans1 = ax1.get_indexer([1])
>>> ans2 = ax2.get_indexer_non_unique([1])
>>> print(ans1, ans1.dtype)
[0] int64
>>> print(ans2[0], ans2[0].dtype, ans2[1], ans2[1].dtype)
[0 1] int64 [] int64
```

#### Problem description
Found in #35498.  When looking at the implementation of the `get_indexer` or `get_indexer_non_unique` in pandas/_libs/index.pyx, I noticed that the returned array dtype will always be int64. Since these methods return indices arrays, I believe that intp is a more appropriate type because it will choose a size depending on ssize_t, which is guaranteed to be large enough to represent all possible indices in the array.

#### Expected Output

```python
[0] intp
[0 1] intp [] intp
```

#### Output of ``pd.show_versions()``

```python
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : 7b14cf6b0b9dbcddce7b9bb22a81c73bdebc1be8
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.76-linuxkit
Version          : #1 SMP Tue May 26 11:42:35 UTC 2020
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0rc0+406.g7b14cf6b0
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 45.2.0.post20200210
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : 5.20.2
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.4.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : 0.4.1
gcsfs            : 0.6.2
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.1
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.0
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.1
```
"
704952335,36480,CLN: Use https for @tm.network,alimcmaster1,closed,2020-09-19T17:41:23Z,2020-09-19T19:59:15Z,"- [x] ref #36467

 AssertionError: Caused unexpected warning(s): [('ResourceWarning', ResourceWarning(""unclosed <ssl.SSLSocket fd=18, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.43', 54722), raddr=('**172.217.204.95**', 443)>""), ), 

**This is a google address** -> which leads me to believe this resource warning is coming from our `network` function in `_testing.py` since it calls `urllib.request.urlopen(""https://www.google.com"")` 

Note: this doesn't solve the problem but we should use https. (Avoids a redirect)"
704883605,36476,Turn on stale PR GitHub action,dsaxton,closed,2020-09-19T13:33:28Z,2020-09-19T20:04:59Z,"I think it makes sense to turn this on now (it's only labeling stale PRs for now, and after doing a lot of this manually it seems to have calmed down). Later can add some sort of friendly reminder to update the PR. Also rather than closing these stalled PRs I'm wondering if they could serve as starting points for new contributors who want to pick up where someone else left off.

Some recent workflows here (you can search for the string ""Marking"" in the logs to find PRs that would have been labeled): https://github.com/pandas-dev/pandas/actions?query=workflow%3A%22Stale+PRs%22

@simonjayhawkins You'd mentioned doing this more frequently in debug mode, would you prefer longer intervals when running live?"

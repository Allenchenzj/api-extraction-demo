id,number,title,user,state,created_at,updated_at,body
350904976,22373,"version 0.23: RangeIndex argument order changed, docs stale",mikapfl,closed,2018-08-15T17:57:31Z,2020-09-28T15:39:32Z,"```python
>>> import pandas as pd
>>> pd.__version__
'0.23.4'
>>> pd.RangeIndex(0, 3, 1, 'name')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/mpfluege/.local/miniconda3/envs/pitchwalk/lib/python3.7/site-packages/pandas/core/indexes/range.py"", line 74, in __new__
    cls._validate_dtype(dtype)
  File ""/home/mpfluege/.local/miniconda3/envs/pitchwalk/lib/python3.7/site-packages/pandas/core/indexes/range.py"", line 161, in _validate_dtype
    if not (dtype is None or is_int64_dtype(dtype)):
  File ""/home/mpfluege/.local/miniconda3/envs/pitchwalk/lib/python3.7/site-packages/pandas/core/dtypes/common.py"", line 991, in is_int64_dtype
    tipo = _get_dtype_type(arr_or_dtype)
  File ""/home/mpfluege/.local/miniconda3/envs/pitchwalk/lib/python3.7/site-packages/pandas/core/dtypes/common.py"", line 1872, in _get_dtype_type
    return _get_dtype_type(np.dtype(arr_or_dtype))
TypeError: data type ""name"" not understood
```
#### Problem description

[Documentation of RangeIndex](https://pandas-docs.github.io/pandas-docs-travis/generated/pandas.RangeIndex.html) states that it has the arguments `start`, `stop`, `step`, `name` and `copy`. However, the actual signature is `RangeIndex(start=None, stop=None, step=None, dtype=None, copy=False, name=None, fastpath=False)`, which has `dtype` as fourth argument. This breaks scripts that relied on the fourth argument being the name, which used to be the case:
```python
>>> import pandas as pd
>>> pd.__version__
'0.22.0'
>>> pd.RangeIndex(0, 3, 1, 'name')
RangeIndex(start=0, stop=3, step=1, name='name')
```

#### Expected Output

Either revert to the old argument order or at least update the documentation.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.0.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.140-62-default
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: de_DE.UTF-8
LOCALE: de_DE.UTF-8

pandas: 0.23.4
pytest: None
pip: 10.0.1
setuptools: 40.0.0
Cython: None
numpy: 1.15.0
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: None
numexpr: 2.6.7
feather: None
matplotlib: 2.2.3
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
708302078,36606,CLN: assorted,jbrockmendel,closed,2020-09-24T16:14:11Z,2020-09-28T15:39:38Z,"- [x] closes #26982
- [x] closes #22373
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
709771496,36684,CLN remove unnecessary noqas from pandas/core,MarcoGorelli,closed,2020-09-27T16:33:35Z,2020-09-28T16:57:30Z,"- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
709114644,36639,CFG use black profile in setup.cfg for isort,MarcoGorelli,closed,2020-09-25T17:02:58Z,2020-09-28T16:57:33Z,Since isort v5 some of the existing configs are unnecessary
331478886,21437,`.diff(axis=1)` gives `NaNs` with different types. ,eoincondron,closed,2018-06-12T08:01:18Z,2020-09-28T17:42:00Z,"
```python
df = pd.DataFrame({'a': np.arange(3, dtype='float32'), 'b': np.arange(3, dtype='float64')})
df.diff(axis=1)

	a	b
0	NaN	NaN
1	NaN	NaN
2	NaN	NaN

df = pd.DataFrame({'a': np.arange(3, dtype='int32'), 'b': np.arange(3, dtype='int64')})
df.diff(axis=1)
	a	b
0	NaN	NaN
1	NaN	NaN
2	NaN	NaN
```
#### Problem description

When diffing across the column axis with different numeric types in the columns we get 
Not sure if this is intentional behaviour but given that `df.a - df.b` works as expected, I would think that `.diff` does the same. I think it should at least emit a warning when the types differ. 

#### Expected Output
```python
	a	b
0	NaN	0
1	NaN	0
2	NaN	0

```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.2.final.0
python-bits: 64
OS: Linux
OS-release: 3.10.0-327.el7.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.20.3
pytest: 3.2.1
pip: 9.0.1
setuptools: 36.4.0
Cython: None
numpy: 1.13.1
scipy: 0.19.1
xarray: None
IPython: 6.1.0
sphinx: None
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: 3.4.2
numexpr: 2.6.2
feather: None
matplotlib: 2.0.2
openpyxl: 2.4.8
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.9999999
sqlalchemy: 1.1.13
pymysql: 0.7.9.None
psycopg2: None
jinja2: 2.9.6
s3fs: None
pandas_gbq: None
pandas_datareader: None
</details>
"
705779186,36528,REF: test_to_latex,ivanovmg,closed,2020-09-21T17:19:02Z,2020-09-28T18:53:19Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Refactor/clean-up ``test_to_latex.py``.

- Split big test functions with multiple assertions into multiple functions
- Make readable expected strings by first indenting for the good visual appearance and then dedenting by the leading whitespace for the assertion."
709384740,36650,DOC: Fix remaining typos in docstrings 'handler' --> 'handle',y2kbugger,closed,2020-09-26T01:04:21Z,2020-09-28T22:40:46Z,Follow-on to https://github.com/pandas-dev/pandas/pull/36427#issuecomment-694249512
710474172,36711,Fix small typo in timedeltas error message,xh2,closed,2020-09-28T18:10:00Z,2020-09-28T23:07:59Z,"Small typo fix

- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
637323823,34719,CI: Failing TestDtype.test_check_dtype on master,TomAugspurger,closed,2020-06-11T21:15:44Z,2020-09-29T11:20:51Z,"https://travis-ci.org/github/pandas-dev/pandas/jobs/697381484#L3750

```
____________________ TestDtype.test_check_dtype[Int64Dtype] ____________________

[gw1] linux -- Python 3.9.0 /home/travis/virtualenv/python3.9-dev/bin/python

self = <pandas.tests.extension.test_integer.TestDtype object at 0x7f1440e52280>

data = <IntegerArray>

[   1,    2,    3,    4,    5,    6,    7,    8, <NA>,   10,   11,   12,   13,

   14,   15,   16,   17,...5,   86,   87,   88,   89,   90,   91,

   92,   93,   94,   95,   96,   97, <NA>,   99,  100]

Length: 100, dtype: Int64

    def test_check_dtype(self, data):

        dtype = data.dtype

    

        # check equivalency for using .dtypes

        df = pd.DataFrame(

            {""A"": pd.Series(data, dtype=dtype), ""B"": data, ""C"": ""foo"", ""D"": 1}

        )

    

        # np.dtype('int64') == 'Int64' == 'int64'

        # so can't distinguish

        if dtype.name == ""Int64"":

            expected = pd.Series([True, True, False, True], index=list(""ABCD""))

        else:

            expected = pd.Series([True, True, False, False], index=list(""ABCD""))

    

        # FIXME: This should probably be *fixed* not ignored.

        # See libops.scalar_compare

        with warnings.catch_warnings():

            warnings.simplefilter(""ignore"", DeprecationWarning)

            result = df.dtypes == str(dtype)

    

>       self.assert_series_equal(result, expected)

pandas/tests/extension/base/dtype.py:84: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pandas/tests/extension/base/base.py:13: in assert_series_equal

    return tm.assert_series_equal(left, right, *args, **kwargs)

pandas/_libs/testing.pyx:68: in pandas._libs.testing.assert_almost_equal

    cpdef assert_almost_equal(a, b,

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   raise_assert_detail(obj, msg, lobj, robj, index_values=index_values)

E   AssertionError: Series are different

E   

E   Series values are different (25.0 %)

E   [index]: [A, B, C, D]

E   [left]:  [True, True, False, False]

E   [right]: [True, True, False, True]

pandas/_libs/testing.pyx:183: AssertionError
```

There's a note there about not ignoring the warning. I wonder if that's been changed."
709792841,36686,TST: Add pytest-instafail to environment.yml,dsaxton,closed,2020-09-27T18:42:20Z,2020-09-29T13:34:44Z,"Pretty useful pytest plugin for getting immediate feedback on test failures instead of waiting for an entire test suite to run:

```
(pandas-dev) ➜  pandas git:(add-pytest-instafail) ✗ pytest pandas/tests/groupby --instafail
============================================================================================ test session starts ============================================================================================
platform darwin -- Python 3.8.5, pytest-6.0.2, py-1.9.0, pluggy-0.13.1
rootdir: /Users/danielsaxton/pandas, configfile: setup.cfg
plugins: hypothesis-5.28.0, forked-1.2.0, asyncio-0.12.0, xdist-2.1.0, cov-2.10.1, instafail-0.4.1
collected 4533 items                                                                                                                                                                                        

pandas/tests/groupby/test_allowlist.py .............................................................................................................................................................. [  3%]
...........................................................                                                                                                                                           [  4%]
pandas/tests/groupby/test_apply.py ............x.x.x...................................................x..........................F

________________________________________________________________________________________________ test_fails _________________________________________________________________________________________________

    def test_fails():
>       assert False
E       assert False

pandas/tests/groupby/test_apply.py:1101: AssertionError

pandas/tests/groupby/test_apply_mutate.py ...                                                                                                                                                         [  6%]
pandas/tests/groupby/test_bin_groupby.py ..........                                                                                                                                                   [  7%]
pandas/tests/groupby/test_categorical.py .....................................................................................xxx..............................sss................................x.. [ 10%]
...
```"
711137217,36723,DOC: Fix typo in docstring,MicaelJarniac,closed,2020-09-29T13:54:43Z,2020-09-29T16:39:28Z,
711236887,36725,TYP: update setup.cfg,simonjayhawkins,closed,2020-09-29T15:39:55Z,2020-09-29T17:17:43Z,
710787251,36721,CLN: Remove unnecessary rolling subclass,mroeschke,closed,2020-09-29T05:56:37Z,2020-09-29T20:19:48Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
707582890,36582,[BUG]: Fix regression when adding timeldeta_range to timestamp,phofl,closed,2020-09-23T18:25:52Z,2020-09-29T21:07:39Z,"- [x] closes #35897
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
706694030,36560,[BUG]: Fix regression in read_table with delim_whitespace=True,phofl,closed,2020-09-22T21:07:21Z,2020-09-29T21:07:59Z,"- [x] closes #35958
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

We could write a private function which is called from read_csv and read_table with the appropriate default_set alternatively.

Edit: Black_pandas formats files not touched by myself. Was there an update or change of guidelines?"
706668520,36557,[BUG]: Fix bug with pre epoch normalization,phofl,closed,2020-09-22T20:23:57Z,2020-09-29T21:08:14Z,"- [x] closes #36294
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The fix propsed by @jbrockmendel worked.

Hope the test is in the right place"
542381612,30469,CLN: OrderedDict -> Dict,alimcmaster1,closed,2019-12-25T20:13:50Z,2020-09-29T22:48:23Z,"- Follow up from #29212

Continue work done in https://github.com/pandas-dev/pandas/pull/29923
 
As of CPython 3.6 the implementation of dict maintains insertion order.

This became a language feature in python 3.7.

Note in python 3.8 support for reversed(dict) was added - but we don't seem to use that in the use cases i've eliminated. ( OrderedDict support this)

Add code check as per comment here: https://github.com/pandas-dev/pandas/pull/29923#issuecomment-559889216"
709828866,36693,CLN: OrderedDict->dict,jbrockmendel,closed,2020-09-27T22:28:56Z,2020-09-29T22:48:29Z,"- [x] closes #30469 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Grepping for OrderedDict in *.py files I see 84 more usages, but AFAICT these are all explicitly checking for the class, so I'm considering this as closing #30469."
657696528,35300,ENH: add percentage threshold to dropna,erfannariman,closed,2020-07-15T22:19:41Z,2020-09-29T23:45:56Z,"- [x] closes #35299 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
607448798,33819,BUG: Missing tick labels on twinned axes,ebardie,closed,2020-04-27T10:56:58Z,2020-09-30T00:11:53Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
#!/usr/bin/python3

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Create data
df = pd.DataFrame({'a': np.random.randn(1000),
                   'b': np.random.randn(1000)})

# Create figure
fig = plt.figure()
plots = fig.subplots(2, 3)

# Create *externally* shared axes
plots[0][0] = plt.subplot(231, sharex=plots[1][0])
# note: no plots[0][1] that's the twin only case
plots[0][2] = plt.subplot(233, sharex=plots[1][2])

# Create *internally* shared axes
# note: no plots[0][0] that's the external only case
twin_ax1 = plots[0][1].twinx()
twin_ax2 = plots[0][2].twinx()

# Plot data to primary axes
df['a'].plot(ax=plots[0][0], title=""External share only"").set_xlabel(""this label should never be visible"")
df['a'].plot(ax=plots[1][0])

df['a'].plot(ax=plots[0][1], title=""Internal share (twin) only"").set_xlabel(""this label should always be visible"")
df['a'].plot(ax=plots[1][1])

df['a'].plot(ax=plots[0][2], title=""Both"").set_xlabel(""this label should never be visible"")
df['a'].plot(ax=plots[1][2])

# Plot data to twinned axes
df['b'].plot(ax=twin_ax1, color='green')
df['b'].plot(ax=twin_ax2, color='yellow')

# Do it
plt.show()

```

#### Problem description

Multi-row and/or multi-column subplots can utilize shared axes.

An external share happens at axis creation when a sharex or sharey
parameter is specified.

An internal share, or twinning, occurs when an overlayed axis is created
by the Axes.twinx() or Axes.twiny() calls.

The two types of sharing can be distinguished after the fact in the
following manner. If two axes sharing an axis also have the same
position, they are not in an external axis share, they are twinned.

For externally shared axes Pandas automatically removes tick labels for
all but the last row and/or first column in
./pandas/plotting/_matplotlib/tools.py's function _handle_shared_axes().

_handle_shared_axes() should be interested in externally shared axes,
whether or not they are also twinned. It should, but doesn't, ignore
axes which are only twinned. Which means that twinned-only axes wrongly
also lose their tick labels.

The first image (produced by running the test code above) shows this, with the label on the axes in the middle spot of the top row (the one with the green plot) notably missing the ""this label should always be visible"" label:

![pandas externally shared axes - problem](https://user-images.githubusercontent.com/724661/80363195-c246db80-887b-11ea-8c47-4e560f34bb51.png)

#### Expected Output

With the proposed fix in PR #33767 the correct bevahiour is seen:

![pandas externally shared axes - fixed](https://user-images.githubusercontent.com/724661/80363257-de4a7d00-887b-11ea-87c5-c519a10bd41b.png)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : caa5121ba510ad727b5b2b0284f61351612f8142
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-40-lowlatency
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.3
numpy            : 1.18.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 41.1.0
Cython           : 0.29.10
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.1.2
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 5.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : 1.1.2
numba            : None


</details>
"
709883044,36699,CLN: Remove unused fixtures,dsaxton,closed,2020-09-28T02:30:15Z,2020-09-30T00:20:19Z,
165559722,13653,Series.to_string ignores empty lines at the start/end when index=False,alberthdev,closed,2016-07-14T13:16:54Z,2020-09-30T02:04:41Z,"When running `to_string` on a Series with empty strings at the beginning and end of the Series data, the empty strings are removed from the resulting output. These empty strings should be included to represent the empty data.
#### Code Sample:

``` python
import pandas as pd
pd.Series(["""", ""Hello"", ""World"", """", """", ""Mooooo"", """", """"]).to_string(index=False) 
```
#### Expected Output:

``` python
u'\nHello\nWorld\n     \n     \nMooooo\n\n'
```
#### Actual Output:

``` python
u'Hello\nWorld\n     \n     \nMooooo'
```
#### `pd.show_versions()`:

```
>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.8.final.0
python-bits: 64
OS: Linux
OS-release: 3.0.101-0.47.67-default
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.18.1
nose: 1.3.6
pip: 1.5.6
setuptools: 3.6
Cython: 0.24
numpy: 1.10.4
scipy: 0.17.0
statsmodels: 0.6.1
xarray: 0.7.2
IPython: 4.2.0
sphinx: 1.2.2
patsy: 0.3.0
dateutil: 2.3
pytz: 2015.4
blosc: None
bottleneck: 1.0.0
tables: 3.2.2
numexpr: 2.4
matplotlib: 1.5.1
openpyxl: 2.0.4
xlrd: 0.9.0
xlwt: 0.7.4
xlsxwriter: 0.7.3
lxml: 3.4.4
bs4: 4.3.2
html5lib: 1.0b8
httplib2: None
apiclient: None
sqlalchemy: 0.9.7
pymysql: 0.6.2.None
psycopg2: None
jinja2: 2.8
boto: 2.34.0
pandas_datareader: None
```
"
193881415,14811,Calling shift(n != 0) on an empty DatetimeIndex raises an IndexError,chazmo03,closed,2016-12-06T20:54:53Z,2020-09-30T02:12:04Z,"Calling `shift(0)` on an empty DatetimeIndex seems to return an empty index, but calling `shift(n)` with a non-zero `n` raises an IndexError. I would expect this to return an empty index.

```python
# this works
pd.date_range(start='20161021', end='20161021', freq='BM').shift(0)

# this raises
pd.date_range(start='20161021', end='20161021', freq='BM').shift(1)
# IndexError: index 0 is out of bounds for axis 0 with size 0

pd.__version__
# u'0.18.1'
```
"
476604631,27744,DEPR: remove Index.is_all_dates?,jbrockmendel,closed,2019-08-04T23:15:04Z,2020-09-30T02:17:40Z,xref #27712 
379150565,23598,PERF: Deprecate casting of index of dates to DatetimeIndex,TomAugspurger,closed,2018-11-09T12:52:58Z,2020-09-30T02:17:40Z,"```python
In [5]: index = pd.Index([pd.Timestamp('2001'), pd.Timestamp('2002')], dtype=object)

In [6]: pd.Series(1, index=index).index
Out[6]: DatetimeIndex(['2001-01-01', '2002-01-01'], dtype='datetime64[ns]', freq=None)
```

This was the root cause of https://github.com/pandas-dev/pandas/pull/23591. Why are we doing that?

Note that this doesn't affect the case of `index=[pd.Timestamp(...), pd.Timestamp(...)]`, as that would have previously been converted to a DatetimeIndex. It seems be only when you have an Index of datetimes.

I'm going through our test cases that hit this now."
401020335,24839,ValueError: cannot set WRITEABLE flag to True of this array,macd2,closed,2019-01-19T18:10:38Z,2020-09-30T02:23:32Z,"will need to revert the xfail decorator in:  https://github.com/pandas-dev/pandas/pull/25517 when this is fixed

#### Code Sample, a copy-pastable example if possible
Im getting all of a sudden this Error, any idea?

```python
# Your code here
 input_df = pd.read_hdf(path_or_buf='x.hdf5',key='/x',mode='r')
```
#### Problem description
Traceback :
````
Traceback (most recent call last):
  File ""..."", line 115, in <module>
    input_df = pd.read_hdf(path_or_buf='x.hdf5',key='/x',mode='r')
  File ""/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py"", line 394, in read_hdf
    return store.select(key, auto_close=auto_close, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py"", line 741, in select
    return it.get_result()
  File ""/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py"", line 1483, in get_result
    results = self.func(self.start, self.stop, where)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py"", line 734, in func
    columns=columns)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py"", line 2937, in read
    start=_start, stop=_stop)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py"", line 2489, in read_array
    ret = node[0][start:stop]
  File ""/usr/local/lib/python3.6/dist-packages/tables/vlarray.py"", line 681, in __getitem__
    return self.read(start, stop, step)[0]
  File ""/usr/local/lib/python3.6/dist-packages/tables/vlarray.py"", line 821, in read
    listarr = self._read_array(start, stop, step)
  File ""tables/hdf5extension.pyx"", line 2155, in tables.hdf5extension.VLArray._read_array
ValueError: cannot set WRITEABLE flag to True of this array
```
"
709826495,36691,BUG: DatetimeIndex.shift(1) with empty index,jbrockmendel,closed,2020-09-27T22:13:20Z,2020-09-30T02:42:51Z,"- [x] closes #14811
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
710390947,36705,CLN: remove unnecessary CategoricalIndex._convert_arr_indexer,jbrockmendel,closed,2020-09-28T15:58:40Z,2020-09-30T02:46:40Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
709430302,36657,CLN: More pytest idioms in pandas/tests/window,mroeschke,closed,2020-09-26T05:14:53Z,2020-09-30T03:51:00Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Removed some unused variables, added some fixtures for DataFrames used in `test_pairwise.py`, and utilize more `pytest.mark.parameterize`"
710527351,36715,TST: implement test to_string_empty_col for Series (GH13653),NomadicDaggy,closed,2020-09-28T19:40:21Z,2020-09-30T07:10:04Z,"- [x] closes #13653 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
606432127,33767,BUG: Fix missing tick labels on twinned axes,ebardie,closed,2020-04-24T16:20:26Z,2020-09-30T07:30:48Z,"## Background:

Multi-row and/or multi-column subplots can utilize shared axes.

An external share happens at axis creation when a sharex or sharey
parameter is specified.

An internal share, or twinning, occurs when an overlayed axis is created
by the Axes.twinx() or Axes.twiny() calls.

The two types of sharing can be distinguished after the fact in the
following manner. If two axes sharing an axis also have the same
position, they are not in an external axis share, they are twinned.

For externally shared axes Pandas automatically removes tick labels for
all but the last row and/or first column in
./pandas/plotting/_matplotlib/tools.py's function _handle_shared_axes().

## The problem:

_handle_shared_axes() should be interested in externally shared axes,
whether or not they are also twinned. It should, but doesn't, ignore
axes which are only twinned. Which means that twinned-only axes wrongly
lose their tick labels.

## The cure:

This commit introduces _has_externally_shared_axis() which identifies
externally shared axes and uses it to expand upon the existing use of
len(Axes.get_shared_{x,y}_axes().get_siblings(a{x,y})) in
_handle_shared_axes() which miss these cases.

## The demonstration test case:

Note especially the axis labels (and associated tick labels).

```python
#!/usr/bin/python3

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Create data
df = pd.DataFrame({'a': np.random.randn(1000),
                   'b': np.random.randn(1000)})

# Create figure
fig = plt.figure()
plots = fig.subplots(2, 3)

# Create *externally* shared axes
plots[0][0] = plt.subplot(231, sharex=plots[1][0])
# note: no plots[0][1] that's the twin only case
plots[0][2] = plt.subplot(233, sharex=plots[1][2])

# Create *internally* shared axes
# note: no plots[0][0] that's the external only case
twin_ax1 = plots[0][1].twinx()
twin_ax2 = plots[0][2].twinx()

# Plot data to primary axes
df['a'].plot(ax=plots[0][0], title=""External share only"").set_xlabel(""this label should never be visible"")
df['a'].plot(ax=plots[1][0])

df['a'].plot(ax=plots[0][1], title=""Internal share (twin) only"").set_xlabel(""this label should always be visible"")
df['a'].plot(ax=plots[1][1])

df['a'].plot(ax=plots[0][2], title=""Both"").set_xlabel(""this label should never be visible"")
df['a'].plot(ax=plots[1][2])

# Plot data to twinned axes
df['b'].plot(ax=twin_ax1, color='green')
df['b'].plot(ax=twin_ax2, color='yellow')

# Do it
plt.show()
```
See images produced by this code for problem and fixed cases at the bug report: #33819.

- [x] closes #33819
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
711223079,36724,TYP: Ignore remaining mypy errors for pandas\tests\*,simonjayhawkins,closed,2020-09-29T15:23:46Z,2020-09-30T11:06:32Z,"- [ ] closes #28926
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry


These are the only 3 errors outstanding in pandas\tests\* that are stopping us closing #28926

```
pandas\conftest.py:301: error: List item 0 has incompatible type ""Type[Index]""; expected ""Type[PandasObject]""  [list-item]
pandas\tests\window\conftest.py:79: error: List item 0 has incompatible type ""ParameterSet""; expected ""Sequence[Collection[object]]""  [list-item]
pandas\tests\window\conftest.py:330: error: List item 15 has incompatible type ""ParameterSet""; expected ""Sequence[Collection[object]]""  [list-item]
```

these appear to be from a mypy inference issue. It maybe best to `# type: ignore[list-item]` just these lines and remove the `ignore_errors=True` from setup.cfg that applies to the whole file for all error codes.

if the mypy error is resolved in a future release, the `warn_unused_ignores = True` in setup.cfg means that we will be alerted and the ignores can be removed."
711545828,36732,BUG: Dataframe filter no longer working in code,KPStarr,closed,2020-09-30T00:18:05Z,2020-09-30T11:11:51Z,"- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

I was wondering if there were any syntax changes that is preventing the code below from running properly. I am new to Python and I am learning to understand how it works. This code was working 24 hours ago and today it is not. I am using pandas version 0.25.1 on Anaconda
```python
dfs = pd.read_html('https://finance.yahoo.com/trending-tickers') 
for df in dfs:
    pass

df[""% Change""] = df[""% Change""].str.replace('%',"""")

filt_data = (df['Last Price'].astype(float) >= 10.00) & (df['Last Price'].astype(float) <= 100.00) & (df['% Change'].astype(float) > 1)

symbols_most_active = [i for i in df[filt_data].iloc[:,0]] 
print(symbols_most_active)

```
"
695783412,36216,Refactor StringMethods for extension arrays,TomAugspurger,closed,2020-09-08T11:17:48Z,2020-09-30T12:22:25Z,"This is an issue to track a refactor of StringMethods that'll be necessary for ArrowStringArray to use pyarrow compute algorithms.

We'll need to update StringMethods

1. Extract the array from the Series / Index
2. Dispatch the string methods
3. Wrap the result

So in my proposal, we'll have something like

```python
class StringMethods:
    def __init__(self, data, ...):
        self._array = data.array
        ...

    def upper(self):
        return self._wrap_result(self._array._str.upper())
```

cc @xhochy. I have a branch started and will hopefully finish it off this week."
711447134,36728,DOC: Update roadmap for completions,TomAugspurger,closed,2020-09-29T20:58:53Z,2020-09-30T12:37:44Z,"Moves ""Improved documentation"" to completed section.

I don't think any others can be marked as done yet, though ""Numba-accelerated operations"" may be getting close (is that right @mroeschke?)."
269649766,18035,"Reading binary file handle (mode ""rb"") doesn't work with read_fwf",prcastro,closed,2017-10-30T15:46:12Z,2020-09-30T13:10:04Z,"#### Code Sample, a copy-pastable example if possible

```python
with open(""file.txt"", ""rb"") as fh:
    data = pd.read_fwf(fh)
```

File (""file.txt""):

```
aas aas aas
bba bab b a
```

#### Problem description

Raises:

```
TypeError                                 Traceback (most recent call last)
<ipython-input-12-524e61d2241f> in <module>()
      1 with open(""file.txt"", ""rb"") as fh:
----> 2     data = pd.read_fwf(fh)

/disk1/home/_/.Anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py in read_fwf(filepath_or_buffer, colspecs, widths, **kwds)
    685     kwds['colspecs'] = colspecs
    686     kwds['engine'] = 'python-fwf'
--> 687     return _read(filepath_or_buffer, kwds)
    688 
    689 

/disk1/home/_/.Anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds)
    403 
    404     # Create the parser.
--> 405     parser = TextFileReader(filepath_or_buffer, **kwds)
    406 
    407     if chunksize or iterator:

/disk1/home/_/.Anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds)
    762             self.options['has_index_names'] = kwds['has_index_names']
    763 
--> 764         self._make_engine(self.engine)
    765 
    766     def close(self):

/disk1/home/_/.Anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine)
    993                                  ' ""c"", ""python"", or' ' ""python-fwf"")'.format(
    994                                      engine=engine))
--> 995             self._engine = klass(self.f, **self.options)
    996 
    997     def _failover_to_python(self):

/disk1/home/_/.Anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, **kwds)
   3313         # Support iterators, convert to a list.
   3314         self.colspecs = kwds.pop('colspecs')
-> 3315         PythonParser.__init__(self, f, **kwds)
   3316 
   3317     def _make_reader(self, f):

/disk1/home/_/.Anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, **kwds)
   1988         # Set self.data to something that can read lines.
   1989         if hasattr(f, 'readline'):
-> 1990             self._make_reader(f)
   1991         else:
   1992             self.data = f

/disk1/home/_/.Anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py in _make_reader(self, f)
   3317     def _make_reader(self, f):
   3318         self.data = FixedWidthReader(f, self.colspecs, self.delimiter,
-> 3319                                      self.comment, self.skiprows)

/disk1/home/_/.Anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, colspecs, delimiter, comment, skiprows)
   3217         self.comment = comment
   3218         if colspecs == 'infer':
-> 3219             self.colspecs = self.detect_colspecs(skiprows=skiprows)
   3220         else:
   3221             self.colspecs = colspecs

/disk1/home/_/.Anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py in detect_colspecs(self, n, skiprows)
   3282             rows = [row.partition(self.comment)[0] for row in rows]
   3283         for row in rows:
-> 3284             for m in pattern.finditer(row):
   3285                 mask[m.start():m.end()] = 1
   3286         shifted = np.roll(mask, 1)

TypeError: cannot use a string pattern on a bytes-like object
```

#### Expected Output

The data is loaded correctly

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.2.final.0
python-bits: 64
OS: Linux
OS-release: 3.10.0-693.1.1.el7.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.20.3
pytest: 3.2.1
pip: 9.0.1
setuptools: 27.2.0
Cython: 0.26
numpy: 1.12.1
scipy: 0.19.1
xarray: None
IPython: 6.1.0
sphinx: 1.6.3
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.2
feather: None
matplotlib: 2.1.0
openpyxl: 2.4.8
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 0.9.8
lxml: 3.8.0
bs4: 4.6.0
html5lib: 0.9999999
sqlalchemy: 1.1.13
pymysql: None
psycopg2: 2.7.3 (dt dec pq3 ext lo64)
jinja2: 2.9.6
s3fs: None
pandas_gbq: None
pandas_datareader: None
</details>"
711652395,36735,TST: read binary file objects with read_fwf,twoertwein,closed,2020-09-30T05:50:52Z,2020-09-30T13:10:08Z,"- [x] closes #18035
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry: not needed

`read_fwf` has already supported reading binary file objects. Added the test case from the issue.
"
709162551,36641,CLN: cleanup DataFrameInfo,ivanovmg,closed,2020-09-25T17:51:47Z,2020-09-30T14:25:06Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Slightly clean-up DataFrameInfo.
Make construction of header and separator cleaner."
372232964,23254,read_fwf: encoding is ignored when memory_map is True,ghost,closed,2018-10-20T17:15:12Z,2020-09-30T17:22:42Z,"```python

df = pd.read_fwf(
                fname, 
                names=columns, 
                header=None, 
                widths=col_len, 
                iterator=True,
                chunksize=chunk_size,
                encoding=""iso8859_1"",
                memory_map=True,
            )
```

#### Problem description

I need to read a file that contains German Umlaute (äüö).
pd.read_fwf() works great as long as memory_map is not True.
When enabling memory_map (as in the example above), a Unicode exception is thrown at the first position of a German Umlaut. I can toggle the exception by enabling/disabling memory_map.

#### Expected Output

option 1) Preferably encoding is supported with memory_map. 
option 2) If for technical reasons this combination is not possible, an exception pointing out that this combination is not allowed,
option 3) If it is not easy to detect whether the combination of memory_map and encoding is suitable (mihgt be it does work on Linux or for specific encodings, system defaults, ..), then a warning might be an option
option 4) if nothing works, then please update the documentation of read_csv, read_fwf, etc.. pointing at possible problems under specific circumstances. 

#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.6.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: AMD64 Family 21 Model 101 Stepping 1, AuthenticAMD
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.23.4
pytest: 3.8.2
pip: 18.1
setuptools: 40.4.3
Cython: None
numpy: 1.15.2
scipy: None
pyarrow: 0.11.0
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: 3.4.4
numexpr: 2.6.8
feather: None
matplotlib: None
openpyxl: 2.5.8
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: 1.2.12
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
"
711663571,36737,TST: honor encoding in read_fwf for memory-mapped files,twoertwein,closed,2020-09-30T06:14:33Z,2020-09-30T17:25:34Z,"- [x] closes #23254
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry: not needed

`encoding` was already working for memory-mapped files in `read_fwf`.
"
708586250,36622,Comment on stale PRs,dsaxton,closed,2020-09-25T02:11:21Z,2020-09-30T17:44:41Z,Updating the stale PR action to comment in the PR. Should help to automate the process of pinging contributors who go quiet. Also updating to run once daily instead of every six hours.
709745351,36682,TST: insert 'match' to bare pytest raises in pandas/tests/tseries/off…,krajatcl,closed,2020-09-27T14:00:19Z,2020-09-30T17:49:26Z,"…sets/test_ticks.py

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

ref: https://github.com/pandas-dev/pandas/issues/30999"
705366960,36520,TST: check inequality by comparing categorical with NaN ( #28384 ),junjunjunk,closed,2020-09-21T07:40:40Z,2020-09-30T19:35:21Z,"- [x] closes #28384
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
690346629,36047,BUG: Series.__mod__ behaves different with >1e4 rows,river-00,closed,2020-09-01T18:14:14Z,2020-09-30T20:27:08Z,"- [√] I have checked that this issue has not already been reported.

- [√] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd
ts = pd.Series([-2,-2,-2])
ts = pd.concat(5000 * [ts])
(ts[:10001]%4).iloc[-1]
(ts[:10000]%4).iloc[-1]
```
Gives
-2
2
#### Problem description

According to basic python module operations and previous versions of pandas, the module operation should always give a non-negative value, but when a Series larger than 10000 rows, the negative value module array operation gives a negative value.

#### Expected Output
2
2
#### Output of ``pd.show_versions()``
1.1.1

<details>



</details>
"
706630054,36552,REGR: Series.__mod__ behaves different with numexpr,simonjayhawkins,closed,2020-09-22T19:17:36Z,2020-09-30T20:29:25Z,"- [ ] closes #36047
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
711557162,36733,CI: disable ARM build,jbrockmendel,closed,2020-09-30T00:55:39Z,2020-09-30T20:59:15Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref #36719"
710398130,36706,CI: npdev new exception message,jbrockmendel,closed,2020-09-28T16:08:56Z,2020-09-30T21:13:54Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
711893304,36740,DOC: Summing by level on a category index produce extra columns.,gepcel,closed,2020-09-30T12:13:13Z,2020-10-01T01:18:41Z,"Generate a dataframe:
```
import pandas as pd
import numpy as np
column_cat = pd.CategoricalDtype(['One', 'Two', 'Three', 'Four'], ordered=True)
groups = list('AABBB')
numbers = pd.Categorical(['One', 'Two','One', 'Two', 'Three'], 
                         categories=column_cat.categories, ordered=True)
d = pd.DataFrame(data=np.arange(15).reshape(3, 5), 
                 columns=pd.MultiIndex.from_arrays([groups, numbers]))
```

<table border=""1"" class=""dataframe"">
  <thead>
    <tr>
      <th></th>
      <th colspan=""2"" halign=""left"">A</th>
      <th colspan=""3"" halign=""left"">B</th>
    </tr>
    <tr>
      <th></th>
      <th>One</th>
      <th>Two</th>
      <th>One</th>
      <th>Two</th>
      <th>Three</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>6</td>
      <td>7</td>
      <td>8</td>
      <td>9</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10</td>
      <td>11</td>
      <td>12</td>
      <td>13</td>
      <td>14</td>
    </tr>
  </tbody>
</table>




Now  do `d.sum(axis=1, level=1)`, get the following result:

<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>One</th>
      <th>Two</th>
      <th>Three</th>
      <th>Four</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>4</td>
      <td>4</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>12</td>
      <td>14</td>
      <td>9</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>24</td>
      <td>14</td>
      <td>0</td>
    </tr>
  </tbody>
</table>


Which produces a non existing column `Four`.

pandas 1.1.1, pandas 1.1.2"
709312702,36645,"REF: privatize in core.missing, remove unused kwarg",jbrockmendel,closed,2020-09-25T22:09:50Z,2020-10-01T01:30:10Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Sits on top of #36624"
709403100,36653,CLN: de-duplicate IntervalArray validators,jbrockmendel,closed,2020-09-26T02:24:36Z,2020-10-01T01:55:54Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
709788972,36685,DEPR: Deprecate params levels & codes in MultiIndex.copy,topper-123,closed,2020-09-27T18:18:08Z,2020-10-01T05:09:05Z,"Deprecates params `levels` & `codes` in `MultiIndex.copy`.
"
712386686,36756,BUG: call to pandas/pandas/core/construction.py  lead to import error,franckjulliard,closed,2020-10-01T00:48:01Z,2020-10-01T07:02:46Z,"pandas
Version: 1.1.2

https://github.com/pandas-dev/pandas/blob/master/pandas/core/construction.py

line 17 you can see
from pandas._typing import AnyArrayLike, ArrayLike, Dtype, DtypeObj

There is no more DtypeObj in pandas._typing and this lead to import error."
712185061,36747,[DOC]: Add explanation about DataFrame methods use all Categories,phofl,closed,2020-09-30T18:41:43Z,2020-10-01T07:26:09Z,"- [x] closes #36740

"
711662081,36736,QST:How to make dataframe output HTML format by default in Cpython shell,syejing,closed,2020-09-30T06:11:33Z,2020-10-01T07:39:58Z,"---

#### Question about pandas
I wrote jupyter myself. The kernel of the program uses Cpython instead of IPython. The problem is that when I output dataframe data, pandas does not output HTML strings under IPython notebook. How can I set the behavior of pandas to output HTML strings


```python
# Your code here, if applicable
[11]import pandas as pd
[12]data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],
        'year': [2000, 2001, 2002, 2001, 2002, 2003],
        'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}
[13]df= pd.DataFrame(data)
[14]df
Out:    state  year  pop
0    Ohio  2000  1.5
1    Ohio  2001  1.7
2    Ohio  2002  3.6
3  Nevada  2001  2.4
4  Nevada  2002  2.9
5  Nevada  2003  3.2
```
"
709905813,36700,CLN: Format doc code blocks,dsaxton,closed,2020-09-28T03:43:03Z,2020-10-01T08:08:54Z,Trying to make code blocks in documentation black compliant. Also a small update to the flake8-rst config making max-line-length equal to 88.
711996234,36742,TYP: some more static definitions of methods for DatetimeIndex,simonjayhawkins,closed,2020-09-30T14:26:27Z,2020-10-01T09:06:17Z,"xref #32100, https://github.com/pandas-dev/pandas/issues/31160#issuecomment-701244665"
712509266,36762,TST: insert 'match' to bare pytest raises in pandas/tests/indexing/te…,krajatcl,closed,2020-10-01T05:47:52Z,2020-10-01T10:54:48Z,"…st_chaining_and_caching.py

- [ ] ref https://github.com/pandas-dev/pandas/issues/30999
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
711559024,36734,DOC: Format more code blocks,dsaxton,closed,2020-09-30T01:01:17Z,2020-10-01T12:13:43Z,"Actually, contrary to https://github.com/pandas-dev/pandas/pull/36700#issuecomment-700957831 there is quite a lot more clean up to do (so maybe could make for some good first issues @simonjayhawkins), it's just that other docs contain blocks that blacken-docs can't parse (e.g., raw output or ipython code containing magic commands). A workaround is to comment them out, parse the doc, and then uncomment (the commented blocks would then have to be edited manually).

Is it possible to render blocks like this as pure code in rst which are allowed to raise?

```python
   .. code-block:: python

       >>> pd.DataFrame(np.random.randn(10, 2)).to_hdf('test_fixed.h5', 'df')
       >>> pd.read_hdf('test_fixed.h5', 'df', where='index>5')
       TypeError: cannot pass a where specification when reading a fixed format.
                  this store must be selected in its entirety
```"
712812766,36772,Update README.md,mayank1897,closed,2020-10-01T12:49:56Z,2020-10-01T14:40:05Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
709865170,36696,TYP: mostly datetimelike,jbrockmendel,closed,2020-09-28T01:25:33Z,2020-10-01T14:50:56Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
712285294,36751,Backport PR #36706 on branch 1.1.x (CI: npdev new exception message),meeseeksmachine,closed,2020-09-30T21:14:04Z,2020-10-01T16:48:39Z,Backport PR #36706: CI: npdev new exception message
712258174,36750,Backport PR #36552 on branch 1.1.x (REGR: Series.__mod__ behaves different with numexpr),meeseeksmachine,closed,2020-09-30T20:29:02Z,2020-10-01T16:49:48Z,Backport PR #36552: REGR: Series.__mod__ behaves different with numexpr
713035842,36781,"Revert ""Update README.md""",MarcoGorelli,closed,2020-10-01T17:22:24Z,2020-10-01T17:23:08Z,Reverts pandas-dev/pandas#36772
487633330,28235,pandas._lib.testing.assert_almost_equal seem to not use approximate equality for Series with complex doubles,oleksandr-pavlyk,closed,2019-08-30T19:14:39Z,2020-10-01T17:52:14Z,"#### Code Sample, a copy-pastable example if possible

```python
import numpy as np
import pandas as pd
from pandas.util.testing import assert_almost_equal

e1 = np.array([0.5831076580182805, -0.9083518696779991], 'd') 
e2 = np.array([0.5831076580182805, -0.9083518696779992], 'd') 

e1z = np.asanyarray(e1, np.complex128)
e2z = np.asanyarray(e2, np.complex128)

assert_almost_equal(pd.Series(e1), pd.Series(e2)) # gives True as expected

assert_almost_equal(pd.Series(e1z), pd.Series(e2z)) # unexpectedly fails

# both of equivalent tests on NumPy side pass
np.testing.assert_almost_equal(e1, e2)
np.testing.assert_almost_equal(e1z, e2z)
```
#### Problem description

When running tests on MacOSX with Pandas 0.25.1, the test `pandas.tests.computation.test_eval.TestMathNumExprPandas::test_result_complex128` fails because of this issue.

#### Expected Output

It is expected that `pandas._lib.testing.assert_almost_equal` would use `np.testing.assert_almost_equal` for all `np.floating` and `np.complexfloating` dtypes.

#### Output of ``pd.show_versions()``

<details>

```
In [4]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 15.4.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.1
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1.post20190716
Cython           : None
pytest           : 3.8.1
hypothesis       : 3.68.0
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 6.3.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
```

</details>
"
707555626,36580,BUG: use cmath to test complex number equality in pandas._testing,arw2019,closed,2020-09-23T17:39:39Z,2020-10-01T18:05:12Z,"- [x] closes #28235
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Adds `cmath`-based equality testing for complex numeric types (`complex`, `np.complex64` and `np.complex128`) "
572834925,32334,DataFrame.sort_index() with ascending passed as a list on a single level index gives wrong result,aniaan,closed,2020-02-28T15:44:13Z,2020-10-01T18:32:25Z,"I found some problems while using data_frame sort_index,  first, create dataframe

```
data = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['a', 'b', 'c'])
data = data.set_index('a')
data.sort_index(level=['a'], ascending=[False])
```
Then I want to sort index ""a"" in descending order,
```
data.sort_index(level=['a'], ascending=[False])
```
The above code does not sort index a in descending order,   I found the reason is that this dataframe is a single index structure, so when ascending = list is passed, it does not take effect

https://github.com/pandas-dev/pandas/blob/bf613c14f41b624b432d0d6b9a29007e7990d460/pandas/core/frame.py#L4777-L4783

Because when it is used, the index may be single index or multi-index. In order to use the effect uniformly,I think there should be a judgment here. If index is a single index and ascending type == list, ascending should be equal to ascending [0]，This may be more user friendly

If yes, can I mention pr"
712779866,36770,CI: troubleshoot travis ci on 1.1.x,simonjayhawkins,closed,2020-10-01T12:07:02Z,2020-10-01T19:19:23Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
713183039,36787,BUG: Rolling returned nan with FixedForwardWindowIndexer for count when window contained only missing values,phofl,closed,2020-10-01T21:04:25Z,2020-10-01T21:56:59Z,"- [x] closes #35579
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Issue still has milestone 1.1.3. If it's to late for this release, I'll move the whats new note to 1.1.4 for when the associated PR is merged.

cc @mroeschke "
712916145,36776,Very useful toolkit for pythonUpdate ,rajarajput2,closed,2020-10-01T14:48:55Z,2020-10-02T09:08:28Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
712857841,36774,scatterplot issue,raunakchhabra,closed,2020-10-01T13:43:30Z,2020-10-02T09:08:37Z,added basic idea to add labels to scatter plot
712229907,36749,Added notes for Jupyter & Colab users,sshiv5768,closed,2020-09-30T19:44:30Z,2020-10-02T09:08:48Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
687728518,35946,CLN: resolve UserWarning in `pandas/plotting/_matplotlib/core.py` #35945,fangchenli,closed,2020-08-28T05:40:08Z,2020-10-02T11:56:55Z,"- [x] closes #35945
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
713620951,36804,github --> GitHub,magnusmodol,closed,2020-10-02T13:27:01Z,2020-10-02T13:30:29Z,"Changed from ""github"" to ""GitHub"" in README.md to make it consistent throughout the document.
"
713024174,36780,DOC: Fix code style in documentation,erfannariman,closed,2020-10-01T17:05:57Z,2020-10-02T14:34:07Z,"ref #36777 
"
568551451,32136,groupby pd.Grouper with freq gives inconsistent groupings,mkowen1,closed,2020-02-20T20:29:57Z,2020-10-02T15:07:46Z,"#### Code Sample, a copy-pastable example if possible

```python
dates = pd.date_range(start='2020-02-01 09:30:00.01', periods=5, freq='14ms')
dates2 = pd.date_range(start='2020-02-01 11:00:00.01', periods=5, freq='8ms')
df = pd.DataFrame({'A': [1] * 10 + [2] * 10,
                   'B': np.concatenate((dates, dates2, dates, dates2)),
                   'C': np.arange(20) + 1})

df[""B""] = pd.to_datetime(df[""B""])
df['num_group'] = df.groupby([""A"", pd.Grouper(key=""B"", freq=""60ms"")]).ngroup()
df
```
For the desired group of len=5, where A==1 and B about 9:30:
num_group==0 has length 4, num_group==1 has length 1

```python
df['num_group'] = df.groupby([""A"", pd.Grouper(key=""B"", freq=""61ms"")]).ngroup()
```
num_group==0 has length 1, num_group==1 has length 4

```python
df['num_group'] = df.groupby([""A"", pd.Grouper(key=""B"", freq=""70ms"")]).ngroup()
```
num_group==0 has length 3, num_group==1 has length 2

```python
df['num_group'] = df.groupby([""A"", pd.Grouper(key=""B"", freq=""72ms"")]).ngroup()
```
num_group==0 has length 5 (expected output, this is all A==1 and B around 9:30)

```python
df['num_group'] = df.groupby([""A"", pd.Grouper(key=""B"", freq=""73ms"")]).ngroup()
```
same split of the group as freq='60ms'

For freq greater than ""100ms"" (have not rigorously tested), appears to be the same (expected) as the 72ms freq case.

#### Problem description
Performing a groupby on a pd.Grouper with varying freqs returns differing ngroups() for some reason. As I change freq from 60 to 73ms, it splits what should be one group into subgroups of varying sizes.

Grouping on the pd.Grouper and then calling ngroup would be an nice way to perform secondary groupby-aggregations using the num_group as a key instead of a time window. If we have a sparse timeseries, then resample or rolling would 1) create additional unwanted rows or 2) condense the ""B"" time series column such that there isn't a unique key to merge back to the original dataframe.

Since the first five rows are from 09:30:00.010 to 09:30:00.066, I would not expect to have to use a freq >= 100ms to group these 56ms rows together, and the freqs cause unexpected groupings without explanation.

#### Expected Output
Expected output for all of the above should return num_group a series of [0]*5 + [1]*5 + [2]*5 + [3]*5, instead of varying divisions with max(num_group) == e.g. 7

### Show Versions

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.15.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200209
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 2.2.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>"
712573058,36764,DOC: PR09 errors,Iqrar99,closed,2020-10-01T07:26:22Z,2020-10-02T17:34:27Z,"#### Location of the documentation

- [x]  [/core/frame.py](https://github.com/pandas-dev/pandas/blob/master/pandas/core/frame.py)
- [x] [/core/generic.py](https://github.com/pandas-dev/pandas/blob/master/pandas/core/generic.py)
- [x] [/core/flags.py](https://github.com/pandas-dev/pandas/blob/master/pandas/core/flags.py)
- [x] [/core/series.py](https://github.com/pandas-dev/pandas/blob/master/pandas/core/series.py)
- [x] [/core/groupby/generic.py](https://github.com/pandas-dev/pandas/blob/master/pandas/core/groupby/generic.py)
- [x] [/core/computation/eval.py](https://github.com/pandas-dev/pandas/blob/master/pandas/core/computation/eval.py)
- [x] [/io/pytables.py](https://github.com/pandas-dev/pandas/blob/master/pandas/io/pytables.py)
- [x] [/io/pickle.py](https://github.com/pandas-dev/pandas/blob/master/pandas/io/pickle.py)
- [x] [/io/json/_json.py](https://github.com/pandas-dev/pandas/blob/master/pandas/io/json/_json.py)

#### Documentation problem

PR09 pandas docstring errors. `Parameter description should finish with "".""`

#### Suggested fix for documentation

Add `.` at the end of the parameter descriptions.
"
713774982,36809,TST: insert 'match' to bare pytest raises in pandas/tests/indexing/te…,krajatcl,closed,2020-10-02T17:11:04Z,2020-10-02T18:09:06Z,"…st_indexing.py

- [ ] ref https://github.com/pandas-dev/pandas/issues/30999
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
581620683,32724,Inconsistent behavior of rolling with missing value,CharlesAuguste,closed,2020-03-15T10:48:39Z,2020-10-02T18:31:09Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import pandas as pd
>>> pd.__version__
'1.0.2'
>>> df = pd.DataFrame({""A"": [1, None], ""B"":[4, 5], ""C"": [7, 8]})
>>> df.rolling(min_periods=1, window=2, axis=1).sum()
     A    B     C
0  1.0  4.0  11.0
1  NaN  5.0  13.0
>>> df.T.rolling(min_periods=1, window=2).sum().T
     A    B     C
0  1.0  5.0  11.0
1  NaN  5.0  13.0
```

#### Problem description

In the above example, I would expect that rolling from one axis or transposing and rolling from the other axis give the same result. This is true when there is no missing value in the dataframe. However, here there is a missing value the value at position .loc[0, ""B""] ends up being different. 

In the first case the value is not summed with the value at .loc[0, ""A""], but in the second case it is. I think this may be an issue because I would expect to get the same output in both cases.

I observed this both in pandas 1.0.1 and 1.0.2.

#### Expected Output

```python
>>> df.rolling(min_periods=1, window=2, axis=1).sum()
     A    B     C
0  1.0  5.0  11.0
1  NaN  5.0  13.0
>>> df.T.rolling(min_periods=1, window=2).sum().T
     A    B     C
0  1.0  5.0  11.0
1  NaN  5.0  13.0
```

or
 
```python
>>> df.rolling(min_periods=1, window=2, axis=1).sum()
     A    B     C
0  1.0  4.0  11.0
1  NaN  5.0  13.0
>>> df.T.rolling(min_periods=1, window=2).sum().T
     A    B     C
0  1.0  4.0  11.0
1  NaN  5.0  13.0
``` 

#### Output of ``pd.show_versions()``

<details>

```
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-76-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.2
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0
Cython           : 0.29.15
pytest           : 3.2.1
hypothesis       : None
sphinx           : 1.5.6
blosc            : None
feather          : None
xlsxwriter       : 0.9.8
lxml.etree       : 3.8.0
html5lib         : 0.9999999
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 7.0.1
pandas_datareader: None
bs4              : 4.6.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 3.8.0
matplotlib       : 3.0.0
numexpr          : 2.6.2
odfpy            : None
openpyxl         : 2.4.8
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 3.2.1
pyxlsb           : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : 1.1.13
tables           : 3.4.2
tabulate         : None
xarray           : None
xlrd             : 1.1.0
xlwt             : 1.3.0
xlsxwriter       : 0.9.8
numba            : 0.42.1
```

</details>
"
713204108,36789,TST: Add test for 32724,phofl,closed,2020-10-01T21:43:02Z,2020-10-02T18:35:37Z,"- [x] closes #32724 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`


Was fixed in the past."
713836402,36812,timeseries.rst,JaspuneetSingh1,closed,2020-10-02T18:56:30Z,2020-10-02T19:39:41Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
713793226,36811,DOC: use blacken to fix code style in documentation #36777,karasinski,closed,2020-10-02T17:42:06Z,2020-10-02T20:15:07Z,"Addresses part of #36777
Ran blacken-tools on cookbook.rst and checked for warnings from flake8-rst"
713304612,36801,CLN: Use more pytest idioms in test_momemts_ewm.py,mroeschke,closed,2020-10-02T02:34:31Z,2020-10-02T20:27:26Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Use `pytest.mark.parameterize` and unnest helper test functions"
555032653,31302,ENH: Change default behavior of rolling.count to be consistent with others,fujiaxiang,closed,2020-01-25T02:41:42Z,2020-10-02T20:35:11Z,"Following on discussion happened in #30923, we may want to change the default behavior of `rolling.count` with regards to its parameter ``min_periods``, so it is consistent with all other similar APIs such as `rolling.mean` and `rolling.sum`.

#### Code Sample
With the updates from #30923
```python
>>> import numpy as np
>>> import pandas as pd
>>> s = pd.Series([1, 1, 1, np.nan, 1, 1, 1])
>>> s
0    1.0
1    1.0
2    1.0
3    NaN
4    1.0
5    1.0
6    1.0
dtype: float64

# rolling.mean and rolling.sum defaults min_periods to the same value as window size (3 in this case)
# notice that it requires not only the window size to be at least 3, but also the number of valid entries (not NaN) to be at least 3
>>> s.rolling(3).mean()  
0    NaN
1    NaN
2    1.0
3    NaN
4    NaN
5    NaN
6    1.0
dtype: float64

>>> s.rolling(3).sum()
0    NaN
1    NaN
2    3.0
3    NaN
4    NaN
5    NaN
6    3.0
dtype: float64

# the default value of min_periods for rolling.count is 0
# we may want to change this behavior so it's consistent with other APIs
>>> s.rolling(3).count()
0    1.0
1    2.0
2    3.0
3    2.0
4    2.0
5    2.0
6    3.0
dtype: float64

# notice that rolling.count requires window size to be at least equal to min_periods to give a result
# it doesn't care about how many valid entries (not NaN) to determine if it should output NaN
# we should retain this behavior because this function is meant to count the number of valid entries
>>> s.rolling(3, min_periods=3).count()
0    NaN
1    NaN
2    3.0
3    2.0
4    2.0
5    2.0
6    3.0
dtype: float64
```

#### Expected Output
```python
>>> s.rolling(3).count()
0    NaN
1    NaN
2    3.0
3    2.0
4    2.0
5    2.0
6    3.0
dtype: float64
```

#### Problem description
With the updates from #30923, the ``min_periods`` argument of `rolling.count` is now respected (it used to be completely ignored). However, the default value remains 0 for backward compatibility purpose. In future updates we probably want to change this default behavior so it's consistent with other similar APIs.

@mroeschke previously mentioned we needed to start with a ``DeprecationWarning`` to inform users of future changes, then probably in the following release make the actual change.

@jreback @WillAyd 
Let me know what you guys think!
"
673906998,35579,BUG: rolling count on FixedForwardWindowIndexer returns NaN instead of 0,goodwanghan,closed,2020-08-05T23:17:35Z,2020-10-02T20:35:12Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
from pandas.core.window.indexers import FixedForwardWindowIndexer

df = pd.DataFrame([[0, None], [0, None],[0, None],[0, 7]], columns=[""a"", ""b""])
res = df[""b""].rolling(window=FixedForwardWindowIndexer(window_size=2),min_periods=0).count()
print(res)
```

#### Problem description

Current output:
0    NaN
1    NaN
2    1.0
3    1.0

#### Expected Output

0    0.0
1    0.0
2    1.0
3    1.0

#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.6.13-100.fc30.x86_64
Version          : #1 SMP Fri May 15 00:36:06 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 9.0.3
setuptools       : 45.2.0
Cython           : None
pytest           : 4.6.4
hypothesis       : None
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.15.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : 0.7.4
fastparquet      : 0.4.1
gcsfs            : 0.6.2
matplotlib       : 3.3.0
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : 0.10.0
pyarrow          : 1.0.0
pytables         : None
pyxlsb           : None
s3fs             : 0.2.2
scipy            : 1.3.1
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.50.1
</details>
"
713174286,36784,CLN: test_moments_rolling.py for quantile/kurt/skew,mroeschke,closed,2020-10-01T20:49:33Z,2020-10-02T21:01:39Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
709367060,36649,DEPR: min_periods=None behavior for Rolling.count,mroeschke,closed,2020-09-26T00:16:08Z,2020-10-02T21:01:47Z,"- [x] closes #31302
- [x] closes #35579 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Additionally refactors `count` to take the `_apply` path to make this op consistent with the others."
334668380,21581,BUG: __rmatmul__ error handling,minggli,closed,2018-06-21T21:41:56Z,2020-10-02T21:12:36Z,"#### Code Sample, a copy-pastable example if possible

when ```dataframe.__rmatmul__``` is triggered, the DataFrame.dot is called. However, the error message doesn't seem to align with numpy.

```python
import numpy as np
from pandas import *

a = np.random.rand(10, 4)
b = np.random.rand(5, 3)

df = DataFrame(b)
# a.__matmul__ is called
a @ df

# df.__rmatmul__ is called
a.tolist() @ df
```
#### Problem description
```
>>> # a.__matmul__ is called
... a @ df
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
ValueError: shapes (10,4) and (5,3) not aligned: 4 (dim 1) != 5 (dim 0)
>>>
>>> # df.__rmatmul__ is called
... a.tolist() @ df
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/Users/mingli/GitHub/open/pandas/pandas/core/frame.py"", line 901, in __rmatmul__
    return self.T.dot(np.transpose(other)).T
  File ""/Users/mingli/GitHub/open/pandas/pandas/core/frame.py"", line 879, in dot
    r=rvals.shape))
ValueError: Dot product shape mismatch, (3, 5) vs (4, 10)
```
#### Expected Output
```
ValueError: Dot product shape mismatch, (10, 4) vs (5, 3)
```
#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: 7a74af7ec6cf38b7c92d3add7fc650b569426327
python: 3.5.5.final.0
python-bits: 64
OS: Darwin
OS-release: 17.4.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.24.0.dev0+147.g7a74af7ec
pytest: 3.6.1
pip: 10.0.1
setuptools: 28.8.0
Cython: 0.28.3
numpy: 1.14.5
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: 1.7.5
patsy: None
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
713189169,36788,API: why do DataFrame bool ops have default_axis=columns while arith/cmp have default_axis=None?,jbrockmendel,closed,2020-10-01T21:14:44Z,2020-10-02T21:41:08Z,"Changing the default_axis to None doesn't break any tests, and that would allow for a bit of code simplification.  So is there a reason for the current implementation?"
713788583,36810,CLN: test_moments_consistency_*.py,mroeschke,closed,2020-10-02T17:34:42Z,2020-10-02T21:42:16Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`


In-lining some common functions to make it easier to follow test failure tracebacks"
712428377,36761,ENH: Implement IntegerArray reductions,dsaxton,closed,2020-10-01T02:46:29Z,2020-10-02T21:46:32Z,"- [x] ref #33790
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I'm not sure if this is the ""best"" way to implement these reductions (see linked issue), but in any case I figure this is better than nothing since we can get these reductions pretty much for free. As far as I can tell FloatingArray can use the same machinery.

(I'm not able to pass on kwargs here because the masked reductions don't take them all, e.g. `axis`, and various tests break as a result.)"
712100133,36744,BUG: DTI/TDI.equals with i8,jbrockmendel,closed,2020-09-30T16:34:37Z,2020-10-02T21:47:36Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
713229345,36793,API: make DataFrame.__boolop__ default_axis match DataFrame.__arithop__ default_axis,jbrockmendel,closed,2020-10-01T22:39:18Z,2020-10-02T21:48:05Z,"- [x] closes #36788
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Fix a test warning generated by an ipython test, unrelated"
713220811,36792,ERR: error handling in DataFrame.__rmatmul__,jbrockmendel,closed,2020-10-01T22:19:02Z,2020-10-02T21:54:06Z,"- [x] closes #21581
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
713178786,36786,CLN: Remove param _set_identity from MultiIndex,topper-123,closed,2020-10-01T20:57:18Z,2020-10-02T21:56:57Z,"I don't see how `_set_identity`can be needed and other index classes don'r have this parameter, so best to just remove it.

Other index classes always end by calling `_reset_identity`, so I just do that here also. I'm not sure that's really needed, but that's for another day."
713853914,36815,DOC: uses black to fix formatting #36777,BrendanWilby,closed,2020-10-02T19:29:14Z,2020-10-02T22:05:30Z,"Fixes /docs/source/user_guide/merging.rst formatting. Passes flake8-rst test.

- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

ref https://github.com/pandas-dev/pandas/issues/36777"
558041019,31487,"Maybe wrong default axis with operators (add, sub, mul, div) between datetime-indexed df and series 1.0.0",giuliobeseghi,closed,2020-01-31T10:20:31Z,2020-10-02T22:39:17Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

index = pd.date_range(start='2020', periods=5)
df = pd.DataFrame([[1, 2, 3]] * 5, columns=['a', 'b', 'c'], index=index)
series = pd.Series([10, 20, 30, 40, 50], index=index)

print(df + series)
```

|  | 2020-01-01  00:00:00 | 	2020-01 02 00:00:00 |	2020-01-03 00:00:00 |	2020-01-04 00:00:00	| 2020-01-05 00:00:00	| a |	b |	c |
|---|---|---|---|---|---|---|---|---|
| 2020-01-01 |	NaN |	NaN |	NaN |	NaN |	NaN |	NaN |	NaN |	NaN |
| 2020-01-02 |	NaN |	NaN |	NaN |	NaN |	NaN |	NaN |	NaN |	NaN |
| 2020-01-03 |	NaN |	NaN |	NaN |	NaN |	NaN |	NaN |	NaN |	NaN |
| 2020-01-04 |	NaN |	NaN |	NaN |	NaN |	NaN |	NaN |	NaN |	NaN |
| 2020-01-05 |	NaN |	NaN |	NaN |	NaN |	NaN |	NaN |	NaN |	NaN |



#### Problem description
According to the docs (https://pandas.pydata.org/pandas-docs/stable/getting_started/dsintro.html#data-alignment-and-arithmetic):

> When doing an operation between DataFrame and Series, the default behavior is to align the Series index on the DataFrame columns, thus broadcasting row-wise

> In the special case of working with time series data, if the DataFrame index contains dates, the broadcasting will be column-wise

It seems to me that in both cases now the broadcasting is row-wise.

Is this an expected change for pandas 1.0.0 (I hope not - I never saw any FutureWarnings about it)? If so, the docs (and the examples) must be updated.

The same happens for the operators `-`, `/`, `*`, `%` 

#### Expected Output
Not sure if this is the expected output anymore, but it used to be equivalent to:

```python
df.add(series, axis=0)
```

| | a | b | c |
|---|-----|---|---|
| 2020-01-01 |	11 |	12 |	13 |
| 2020-01-02 |	21 |	22 |	23 |
| 2020-01-03 |	31 |	32 |	33 |
| 2020-01-04 |	41 |	42 |	43 |
| 2020-01-05 |	51 |	52 |	53 |

Although I can't replicate it, I'm pretty sure this was the behaviour until pandas 0.25.3

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : 0.29.14
pytest           : 5.3.4
hypothesis       : 4.54.2
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.4
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.3.2
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.47.0

</details>
"
713884135,36817,DOC: update code style for remaining intro tutorial docs for #36777,karasinski,closed,2020-10-02T20:28:20Z,2020-10-02T22:39:47Z,"Addresses part of #36777
Ran blacken-tools and checked for warnings from flake8-rst for the remaining 5 intro tutorials. Note that tutorial 3 met the formatting requirements as is, so there is no change there."
307260428,20439,Categorical.__setitem__ doesn't handle setting tuples,TomAugspurger,closed,2018-03-21T14:09:13Z,2020-10-02T22:43:24Z,"We allow tuples in the categories, since they're hashable. But `Categorical.__setitem__` has an incorrect check before setting.

```python
In [3]: c = pd.Categorical([(0, 1), (1, 2)])

In [5]: c[0] = (1, 2)
```

Raises

```pytb
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-4d3f2e8a3cfd> in <module>()
----> 1 c[0] = (1, 2)

/Users/taugspurger/miniconda3/envs/pandas-0.19.2/lib/python3.5/site-packages/pandas/core/categorical.py in __setitem__(self, key, value)
   1652         # something to np.nan
   1653         if len(to_add) and not isnull(to_add).all():
-> 1654             raise ValueError(""Cannot setitem on a Categorical with a new ""
   1655                              ""category, set the categories first"")
   1656

ValueError: Cannot setitem on a Categorical with a new category, set the categories first
```

If it's easy, this is worth supporting. If it's complicated, then maybe not."
435618702,26182,groupby apply failed on dataframe with DatetimeIndex,goldenbull,closed,2019-04-22T06:40:17Z,2020-10-02T22:59:01Z,"#### Code Sample, a copy-pastable example if possible

```python
# -*- coding: utf-8 -*-

import pandas as pd

def _do_calc_(_df):
    df2 = _df.copy()
    df2[""vol_ma20""] = df2[""volume""].rolling(20, min_periods=1).mean()
    return df2

dbars = pd.read_csv(""2019.dbar_ftridx.csv.gz"",
                    index_col=False,
                    encoding=""utf-8-sig"",
                    parse_dates=[""trade_day""])
dbars = dbars.set_index(""trade_day"", drop=False)  # everything works fine if this line is commented
df = dbars.groupby(""exchange"").apply(_do_calc_)
print(len(df))

```
#### Problem description
here is the input data file:
[2019.dbar_ftridx.csv.gz](https://github.com/pandas-dev/pandas/files/3102450/2019.dbar_ftridx.csv.gz)

this piece of code runs well with pandas 0.23, when upgraded to 0.24.2, it reports error:
```
Traceback (most recent call last):
  File ""D:/test/groupby_bug.py"", line 16, in <module>
    df = dbars.groupby(""exchange"").apply(_do_calc_)
  File ""C:\Anaconda3\lib\site-packages\pandas\core\groupby\groupby.py"", line 701, in apply
    return self._python_apply_general(f)
  File ""C:\Anaconda3\lib\site-packages\pandas\core\groupby\groupby.py"", line 712, in _python_apply_general
    not_indexed_same=mutated or self.mutated)
  File ""C:\Anaconda3\lib\site-packages\pandas\core\groupby\generic.py"", line 318, in _wrap_applied_output
    not_indexed_same=not_indexed_same)
  File ""C:\Anaconda3\lib\site-packages\pandas\core\groupby\groupby.py"", line 918, in _concat_objects
    sort=False)
  File ""C:\Anaconda3\lib\site-packages\pandas\core\reshape\concat.py"", line 228, in concat
    copy=copy, sort=sort)
  File ""C:\Anaconda3\lib\site-packages\pandas\core\reshape\concat.py"", line 292, in __init__
    obj._consolidate(inplace=True)
  File ""C:\Anaconda3\lib\site-packages\pandas\core\generic.py"", line 5156, in _consolidate
    self._consolidate_inplace()
  File ""C:\Anaconda3\lib\site-packages\pandas\core\generic.py"", line 5138, in _consolidate_inplace
    self._protect_consolidate(f)
  File ""C:\Anaconda3\lib\site-packages\pandas\core\generic.py"", line 5127, in _protect_consolidate
    result = f()
  File ""C:\Anaconda3\lib\site-packages\pandas\core\generic.py"", line 5136, in f
    self._data = self._data.consolidate()
  File ""C:\Anaconda3\lib\site-packages\pandas\core\internals\managers.py"", line 922, in consolidate
    bm = self.__class__(self.blocks, self.axes)
  File ""C:\Anaconda3\lib\site-packages\pandas\core\internals\managers.py"", line 114, in __init__
    self._verify_integrity()
  File ""C:\Anaconda3\lib\site-packages\pandas\core\internals\managers.py"", line 311, in _verify_integrity
    construction_error(tot_items, block.shape[1:], self.axes)
  File ""C:\Anaconda3\lib\site-packages\pandas\core\internals\managers.py"", line 1691, in construction_error
    passed, implied))
ValueError: Shape of passed values is (432, 27), indices imply (1080, 27)

```

If I do not call set_index() on the dataframe, it works fine. Seems there is something wrong with the DatetimeIndex?

I don't know if this error can be re-produced on your machine, I can re-produce the same error on all my machines.

#### Expected Output
4104

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
C:\Anaconda3\python.exe D:/test/groupby_bug.py

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 79 Stepping 1, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.2
pytest: 4.4.0
pip: 19.0.3
setuptools: 41.0.0
Cython: 0.29.7
numpy: 1.16.2
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 7.4.0
sphinx: 2.0.1
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: 1.2.1
tables: 3.5.1
numexpr: 2.6.9
feather: None
matplotlib: 3.0.3
openpyxl: 2.6.2
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.6
lxml.etree: 4.3.3
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.3.3
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
None

</details>
"
709567928,36671,[MRG] TST: Added test for groupby apply datetimeindex fix,amy12xx,closed,2020-09-26T16:31:18Z,2020-10-02T22:59:11Z,"- [x] closes #26182
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

worked on by @ezebunandu and @amy12xx
"
704797521,36468,CI: xfail html test #36467,fangchenli,closed,2020-09-19T04:16:05Z,2020-09-23T19:37:34Z,"Part of #36467

"
704115975,36443,CI: fix gbq test #36436,fangchenli,closed,2020-09-18T06:06:52Z,2020-09-23T19:39:08Z,"- [x] closes #36436

"
702372470,36393,BLD/CI: fix py39 ci #36296,fangchenli,closed,2020-09-16T01:12:16Z,2020-09-23T19:39:34Z,"closes  #36296 

Need  #36298 first."
697138136,36255,[DOC]: Improve doc of index.drop,phofl,closed,2020-09-09T20:32:37Z,2020-09-23T21:03:35Z,"- [x] closes #36227

Improved the docs a bit here. Closes the issue"
380005718,23658,CLN: Reestructure tools and scripts,datapythonista,closed,2018-11-13T00:23:13Z,2020-09-23T21:26:47Z,"I think it would be good to reestructure a bit our scripts, make sure all them have a header explaining what they do and how to use them, get rid of the ones not in use anymore (if any), and add the inventory of them to the contributing documentation, with what they do.

I'd move all them to `ci/` (or if the name is not good rename it; sklearn uses `build_tools`, numba `buildscripts`). And inside, I'd have different directories (names can probably be improved):
- `ci/deps/`: conda requirements files
- `ci/setup/`: scripts to create the environment, like downloading conda, building pandas...
- `ci/testing/`: scripts to run the tests
- `ci/checks/`: scripts to validate code, like `code_checks.sh` or `validate_docstrings.py`
- `ci/release/`: scripts used during the release
- `ci/benchmarks/`: I'd move the asv files inside `ci`
- `ci/config`: A directory for the yaml files with the CI configuration
- `ci/tools/`: scripts like `merge-pr.py`, `find_commits_touching_func.py`...

It would probably be good to unify scripts, like `script_single.sh` and `script_multi.sh` that share 70% of the code, and have a single script with an argument `run_tests.sh single` / `run_tests.sh multi`.

And it could also be useful to have a single script for things like the set up, so we have downloading and running conda in the same script, and it can be called all together, or just a part (like `code_checks.sh`), so we can do `setup_env.sh download`, `setup_env.sh create`, or simply `setup_env.sh` to do both.

@pandas-dev/pandas-core thoughts?"
441250560,26307,Missing `GL06` and `GL07` errors for invalid docstrings,Scowley4,closed,2019-05-07T14:01:37Z,2020-09-23T21:29:58Z,"As discovered in the process of working on #26301, there are invalid docstrings that should throw either `GL06 - Found unknown section` or `GL07 - Sections are in the wrong order` errors.

Without too much investigation, it's not clear why `GL06` is not being thrown. Currently unsure if this is even a problem because, as @datapythonista mentioned in comments on #26301, we may not even run these checks on the private methods.

The missing `GL07` are because section headers were only considered section headers if the following line had an equal number of `-----`'s (underlines).

This code is found here (where `content[0]` is the section header):
https://github.com/pandas-dev/pandas/blob/2bbc0c2c198374546408cb15fff447c1e306f99f/scripts/validate_docstrings.py#L383-L386

- [x] Reorder sections that are failing #26301 and merge
- [ ] ~Fix GL07 to correctly throw errors with invalid underlines~
- [ ] ~Investigate GL06~
- [ ] Add error for valid section title words with invalid number of underlines

If it's okay with others, I'd like to take a crack at this issue.

"
707470429,36577,QST: pyinstaller pandas,DiamondTop,closed,2020-09-23T15:32:49Z,2020-09-24T00:30:37Z,"- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [x] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

**Note**: If you'd still like to submit a question, please read [this guide](
https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your question.

```python
# Your code here, if applicable
File """", line 219, in call_with_frames_removed
File ""c:\users\leroy\anaconda3\lib\site-packages_pyinstaller_hooks_contrib\hooks\stdhooks\hook-IPython.py"", line 34, in
datas += collect_data_files('IPython.extensions', include_py_files=True)
File ""c:\users\leroy\anaconda3\lib\site-packages\PyInstaller\utils\hooks_init.py"", line 712, in collect_data_files
pkg_base, pkg_dir = get_package_paths(package)
File ""c:\users\leroy\anaconda3\lib\site-packages\PyInstaller\utils\hooks_init_.py"", line 506, in get_package_paths
file_attr = get_module_file_attribute(package)
File ""c:\users\leroy\anaconda3\lib\site-packages\PyInstaller\utils\hooks_init_.py"", line 308, in get_module_file_attribute
raise ImportError('Unable to load module attribute') from e
ImportError: Unable to load module attribute
```
error on the below when compling py file on pyinstaller. command is pyinstaller --noconsole --onefile filename.py

```
python code as per below.
import pandas
import tkinter as tk
from tkinter import filedialog
import os

def analysis():
path = filedialog.askopenfilename()
data = pandas.read_excel(path)
groups = data.groupby('Group')
table = groups.agg({'Revenue':sum, 'Group': len})
savename = filedialog.asksaveasfilename()
savename = savename.split('.')[0] + '.csv'
table.to_csv(savename)

if name == ""main"":
window = tk.Tk()
window.title('Analysis')
window.geometry(""200x150"")
frame = tk.Frame(window)
frame.grid(row=0, column=0)
button = tk.Button(frame, text='Generate Table', command=analysis)
button.grid(row=0, column=1)
window.mainloop()
```

full running codes here: https://pastebin.com/0z34GfNW

"
634430100,34640,CLN: remove `private_key` and `verbose` from gbq,charlesdong1991,closed,2020-06-08T09:23:21Z,2020-09-24T01:34:43Z,"Since `verbose` and `private_key` have been deprecated since https://github.com/pandas-dev/pandas/pull/30200, we should also remove both from function since they are not used at all? see them in `read_gbq` and `to_gbq`, e.g.

https://github.com/pandas-dev/pandas/blob/24857a2844be9ee6f604d93ea18301f587751f0a/pandas/io/gbq.py#L33

https://github.com/pandas-dev/pandas/blob/24857a2844be9ee6f604d93ea18301f587751f0a/pandas/io/gbq.py#L34"
634883776,34654,#34640: CLN: remove 'private_key' and 'verbose' from gbq,parkdj1,closed,2020-06-08T19:39:16Z,2020-09-24T01:34:48Z,"- [x ] closes #34640 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Hello, I am new to contributing to open source, so thank you in advance for any corrections and suggestions. Please let me know if I need to do anything else for this ticket!

I removed the code containing both 'private_key' and 'verbose' from the gbq.py file."
669639719,35492,DOC: Add note to docstring DataFrame.compare about identical labels,erfannariman,closed,2020-07-31T09:57:01Z,2020-09-24T08:07:01Z,"- [x] closes #35491
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
707170771,36569,CLN: clean up pandas core arrays,erfannariman,closed,2020-09-23T08:46:06Z,2020-09-24T08:07:12Z,Some clean up in `pandas/core/arrays/array.py`. 
705931965,36534,CLN: clean up blocks.py,erfannariman,closed,2020-09-21T21:34:38Z,2020-09-24T08:07:25Z,Clean up while going through code. Will probably check more files.
707906680,36593,BUG: Wrong order and ambiguous start point  for some period values,GYHHAHA,closed,2020-09-24T06:50:01Z,2020-09-24T08:13:44Z,"```python
>>>pd.period_range('1350-01-01', '1350-01-02', freq='6h').sort_values()
PeriodIndex(['1350-01-01 23:00', '1350-01-01 05:00', '1350-01-01 11:00',
             '1350-01-01 17:00', '1350-01-02 23:00'],
            dtype='period[6H]', freq='6H')
```
Though sorted, '1350-01-01 23:00' still locates the first.
```python
>>>pd.period_range('2150-01-01', '2150-01-02', freq='6h')
PeriodIndex(['2150-01-01 00:00', '2150-01-01 06:00', '2150-01-01 12:00',
             '2150-01-01 18:00', '2150-01-02 00:00'],
            dtype='period[6H]', freq='6H')
```
And why this starts at 00:00, but the former one starts at 23:00?

Thanks!"
550131145,31039,CLN: Possibly obsolete scripts,datapythonista,closed,2020-01-15T11:32:30Z,2020-09-24T08:43:49Z,"xref #31038

I'm unsure if the following scripts are still used:
```
scripts/find_commits_touching_func.py
ci/check_cache.sh
ci/check_git_tags.sh
ci/travis_process_gbq_encryption.sh
```

The next I assume are for the release, but may be there is something obsolete here:
```
scripts/build_dist_for_release.sh
scripts/build_dist.sh
scripts/download_wheels.py
```"
550537708,31061,WEB: Broken older versions link in the website,datapythonista,closed,2020-01-16T02:33:33Z,2020-09-24T09:49:26Z,"In the top navigation of the website (see pandas.io), in the Docs section, the `Older versions` link is broken.

In #30891 I proposed to just have a single link to the docs for now, so there is no broken link or inconsistencies. But there is no agreement on getting that merged, and some discussion is needed.

Opening this issue, so ideas on what to do can be discussed."
550127653,31038,CLN: Merge scripts/ and ci/ directories,datapythonista,closed,2020-01-15T11:24:32Z,2020-09-24T09:52:40Z,"While conceptually it can make sense to separate the scripts that are mostly called from the CI, than the ones that are not, I think in practice the division between `scripts/` and `ci/` directories is rather arbitrary.

An example is `scripts/validate_docstrings.py`. We use it from the CI, but we use it locally too to validate a single function. We have also scripts in `ci/` that are currently not called from the CI (not sure if they are still useful, will create a separate issue to discuss): `ci/check_git_tags.sh`, `ci/check_cache.sh` and `ci/travis_process_gbq_encryption.sh`

Personally, I think a different structure would make things clearer. An idea would be the next:
```
dev/release/  <- scripts used for the release
dev/validation/  <- scripts used for validation (docstrings, code checks...)
dev/deps/  <- `ci/deps`
dev/config/  <- `ci/azure`
dev/misc/  <- everything else (may be a specific directory for testing stuff?)
```

@jbrockmendel I think we discussed about this some time ago, and you didn't like the proposal then. Does this make sense as proposed here? Any idea if you don't like this proposal, but you agree the current structure is confusing?"
492753930,28409,DEPR: Move rarely used I/O connectors to third party modules,datapythonista,closed,2019-09-12T11:45:43Z,2020-09-24T09:56:13Z,"Related to #26804 (and indirectly related to #15862, #15008 if we standardize I/O connectors as part of this)

The recent survey shows that several of our  I/O connectors (Stata, SPSS, SAS and GBQ) are rarely used (from the chart I'd say around 3% of respondents said they used them):

![pandas_survey_io](https://user-images.githubusercontent.com/10058240/64779479-12279b00-d556-11e9-9154-9c8b1c03cb12.png)

While afaik all them are just wrappers around other libraries, and don't contain much code to maintain, I see few main immediate advantages:
- Allow an ecosystem of third-party modules
- Reduce the complexity and time of the CI
- Not commit to the connector API

**I/O ecosystem**

While the number of widely used formats is quite limited, the number of possible I/O connectors is huge. Several new connectors have been requested in the past, like markdown (#11052), avro (#11752), docx (#22518), and there are surely plenty of specialized and internal formats that didn't make sense to propose adding to pandas, but that people would appreciate having and using the same standard API (`pandas.read_*` / `DataFrame.to_*`) we have. And there may also be alternative versions of the connectors we provide (see for example this lightning talk by @dutc dicussing about an unsafe `pandas.read_csv`: https://youtu.be/QkQ5HHEu1b4?t=1554).

**CI**

For the CI, those modules include dependencies that need to be downloaded for every build. We also have files in `pandas/tests/io/data` that eventually we may need to update. For GBQ in particular, we have to store the password for the account we use to test, and those tests fail from time to time for connectivity problems. For the clipboard (not proposing to move to a third-party yet, but is a good candidate too), we require to emulate an X system in our tests (need to check if we finally fixed the tests, but they weren't running for a while because of problems on the CI set up).

And of course our test suite is taking some extra time to test all these connectors.

**API changes**

Being part of our public API, I think we need to commit to the API of those connectors, in the same way as we do with the rest of our API. But, for whatever reason it may make sense to discontinue some of these connectors (e.g. Google releasing a replacement for GBQ...), or make important changes to their parameters, and probably it makes sense to have different release cycles for them.

This also applies to connectors not yet in pandas, but that we may include if we keep the policy of adding to our code base the connectors for all the formats / services we consider relevant.

I know timing is not great, but because of this API commitment, I'd personally remove Stata, SPSS, SAS and GBQ before pandas 1.0."
707693572,36587,BUG: Cannot use pd.NA in a series with float dtype,thehomebrewnerd,closed,2020-09-23T21:33:17Z,2020-09-24T12:38:00Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
pd.Series([1.1, pd.NA], dtype='float64')
```

#### Problem description

Similar to the issue documented in #36586, if the goal of `pd.NA` is to provide a missing indicator that can be used across datatypes, it should also work with a `float` datatype. Currently, if a user attempts to use a `pd.NA` value in a series with dtype of float, an error is raised: `TypeError: float() argument must be a string or a number, not 'NAType'`.

This also appears related to the issue mentioned in #36585

#### Expected Output

```python
import pandas as pd
pd.Series([1.1, pd.NA], dtype='float64')

0    1.1
1    <NA>
dtype: float64
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Mon Feb 10 21:08:45 PST 2020; root:xnu-4903.278.28~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 40.8.0
Cython           : None
pytest           : 6.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
517120648,29394,"DataFrame.select_dtypes include/exclude ""int"" not recognized.",simonjayhawkins,closed,2019-11-04T12:29:11Z,2020-09-24T13:40:09Z,"#### Code Sample, a copy-pastable example if possible

code sample from DataFrame.select_dtypes docstring

```python
>>> import pandas as pd
>>>
>>> pd.__version__
'0.26.0.dev0+767.gb3490cbd9'
>>>
>>> df = pd.DataFrame({""a"": [1, 2] * 3, ""b"": [True, False] * 3, ""c"": [1.0, 2.0] * 3})
>>> df
   a      b    c
0  1   True  1.0
1  2  False  2.0
2  1   True  1.0
3  2  False  2.0
4  1   True  1.0
5  2  False  2.0
>>>
>>> df.select_dtypes(exclude=[""int""])
   a      b    c
0  1   True  1.0
1  2  False  2.0
2  1   True  1.0
3  2  False  2.0
4  1   True  1.0
5  2  False  2.0
>>>
```
#### Problem description

seen locally on master and 0.25.1

The docstring implies `""int""` is a valid dtype to exclude. Only `""int64""` produces the expected output shown in the docstring.

The same applies to the `include` argument.

#### Expected Output

```python
>>> df.select_dtypes(exclude=[""int64""])
       b    c
0   True  1.0
1  False  2.0
2   True  1.0
3  False  2.0
4   True  1.0
5  False  2.0
>>>

```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : b3490cbd98b96aa5064e87b11c391596e32f7993
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 58 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : None.None

pandas           : 0.26.0.dev0+767.gb3490cbd9
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.6.0.post20191030
Cython           : 0.29.13
pytest           : 5.2.2
hypothesis       : 4.36.2
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.2
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : 1.2.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
s3fs             : 0.3.4
scipy            : 1.3.1
sqlalchemy       : 1.3.10
tables           : 3.5.1
xarray           : 0.13.0
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.2
>>>

</details>
"
708135955,36601,CI: move arm64 build to allow_failures for now,jreback,closed,2020-09-24T12:38:42Z,2020-09-24T14:58:54Z,"https://travis-ci.org/github/pandas-dev/pandas/jobs/729859284

well at least we detected this early. ideally can investigate and fix, but for now need a green CI."
557856883,31476,DEPR: DataFrame.__getitem__[str] sometimes slices on index,jbrockmendel,closed,2020-01-31T01:27:32Z,2020-09-24T19:57:39Z,"xref #31334, #9595 

- This is a tiny corner case, at least as measured by tests cases (2 tests reach this)
- With this removed, the `__getitem__` API becomes much simpler to describe: ""DataFrame.__getitem__` is always column-based"""
697824438,36266,BUG: fix isin with nans and large arrays,Hanspagh,closed,2020-09-10T11:40:06Z,2020-09-24T20:20:07Z,"Does a np.isnan if nan is given to isin and we have a large enough array to trigger the `np.in1d` path
- [x] closes #22205
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
708409398,36609,CLN: de-duplicate _local_timestamps,jbrockmendel,closed,2020-09-24T19:09:00Z,2020-09-24T22:04:00Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
687648904,35940,BUG:Pandas 1.0.3 → 1.1.1 behavior change on DataFrame.apply() whith raw option and func returning string,m-hunsicker,closed,2020-08-28T01:42:52Z,2020-09-24T23:42:19Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

```python
df_1 = pd.DataFrame({'A': [""aa"",""bbb""]})
df_2 = pd.DataFrame({'A': [""bbb"",""aa""]})

def get_value(array):
        return array[0]

r_1 = df_1.apply(get_value, axis=1, raw=True)
r_2 = df_2.apply(get_value, axis=1, raw=True)

print(r_1)
print(r_2)
```

#### Output
0    aa
1    bb
dtype: object
0    bbb
1     aa
dtype: object

#### Problem description
The results are truncated when the smallest strings is first.  However, when the result (eg. array[0]) is printed before the return of the func, it's displays the correct value.  
(This issue occurred when using apply with the raw option for a function using several columns)

#### Expected Output
0    aa
1    bbb
dtype: object
0    bbb
1     aa
dtype: object


#### Output of ``pd.show_versions()``

<details>

Pandas 1.1.1
Numpy 1.19.1

</details>
"
603059347,33671,BUG: Multiplication of two serieses changes the the timezone from the given serieses,eyjay-ok,closed,2020-04-20T08:50:56Z,2020-09-25T00:31:58Z,"(since this is my first bug report I am happy for any feedback :) )

```python
import pandas as pd

cet_idx = pd.date_range(start=pd.to_datetime('today').normalize(), periods=10, freq='15min', tz='CET')
utc_idx = cet_idx.tz_convert('utc')

cet_series = pd.Series(data=2, index=cet_idx)
utc_series = pd.Series(data=2, index=utc_idx)

print(cet_series.index.tz)
>> <DstTzInfo 'CET' CET+1:00:00 STD>

out_series = utc_series * cet_series   # The output is irrelevant 

print(cet_series.index.tz)
>> <UTC>
```

#### Problem description

The timezone of the series 'cet_series' changes by the multiplication. Since it is only an input series the timezone should stay the same.

#### Expected Output
```python
... # the same initialization as above

print(cet_series.index.tz)
>> <DstTzInfo 'CET' CET+1:00:00 STD>

utc_series * cet_series   # The output is irrelevant 

print(cet_series.index.tz)
>> <DstTzInfo 'CET' CET+1:00:00 STD>
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-46-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.13
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
705156566,36503,BUG: alignment changing index on input series,jbrockmendel,closed,2020-09-20T18:08:38Z,2020-09-25T00:33:21Z,"- [x] closes #33671
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
707796378,36591,CI: Add rst backtick checker,dsaxton,closed,2020-09-24T02:08:34Z,2020-09-25T00:48:30Z,"Adding a pre-commit hook for detecting single backticks around code in RST files. Running for instance on the v1.2.0 whatsnew shows errors:
```
doc/source/whatsnew/v1.2.0.rst:120:- `Styler` now allows direct CSS class name addition to individual data cells (:issue:`36159`)
doc/source/whatsnew/v1.2.0.rst:225:- Performance improvements when creating DataFrame or Series with dtype `str` or :class:`StringDtype` from array with many string elements (:issue:`36304`, :issue:`36317`, :issue:`36325`, :issue:`36432`)
doc/source/whatsnew/v1.2.0.rst:230:- Performance improvement in :meth:`pd.to_datetime` with non-`ns` time unit for `float` `dtype` columns (:issue:`20445`)
doc/source/whatsnew/v1.2.0.rst:265:- Bug in :func:`date_range` was raising AmbiguousTimeError for valid input with `ambiguous=False` (:issue:`35297`)
doc/source/whatsnew/v1.2.0.rst:306:- Bug in :meth:`SeriesGroupBy.transform` now correctly handles missing values for `dropna=False` (:issue:`35014`)
doc/source/whatsnew/v1.2.0.rst:312:- Bug in :meth:`DataFrame.xs` when used with :class:`IndexSlice` raises ``TypeError`` with message `Expected label or tuple of labels` (:issue:`35301`)
doc/source/whatsnew/v1.2.0.rst:320:- In :meth:`read_csv` `float_precision='round_trip'` now handles `decimal` and `thousands` parameters (:issue:`35365`)
doc/source/whatsnew/v1.2.0.rst:322:- :meth:`to_csv` passes compression arguments for `'gzip'` always to `gzip.GzipFile` (:issue:`28103`)
doc/source/whatsnew/v1.2.0.rst:324:- :meth:`to_csv` and :meth:`read_csv` did not honor `compression` and `encoding` for path-like objects that are internally converted to file-like objects (:issue:`35677`, :issue:`26124`, and :issue:`32392`)
doc/source/whatsnew/v1.2.0.rst:327:- Bug in :meth:`read_csv` with `engine='python'` truncating data if multiple items present in first row and first element started with BOM (:issue:`36343`)
doc/source/whatsnew/v1.2.0.rst:328:- Removed ``private_key`` and ``verbose`` from :func:`read_gbq` as they are no longer supported in `pandas-gbq` (:issue:`34654` :issue:`30200`)
```

~This is going to fail for now so need to figure out how to make it pass.~"
708283346,36603,BUG: pd.cut fails when ordered is set to False and labels is set to a series,Mark-BC,closed,2020-09-24T15:47:43Z,2020-09-25T01:52:59Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python

import pandas as pd

test = pd.DataFrame([['a', 5], ['b', 2], ['c', 6], ['d', 3], ['e', 8]],
                    columns=['row_name', 'row_value'])
cuts = pd.DataFrame([[1, 'odd'], [2, 'even'], [3, 'odd'], [4, 'even'], [5, 'odd'],
                     [6, 'even'], [7, 'odd'], [8, 'even'], [9, 'odd'], [10, '']],
                    columns=['cut_value', 'cut_label'])
print(pd.cut(test.row_value, cuts.cut_value, labels=cuts.cut_label[:-1], ordered=False))

```

#### Problem description

When a user wants to call pd.cut with a set of labels that includes duplicate values, they must set ordered to False and set labels to the series of strings to be used. 

But running the sample code above, with ordered set to False and labels set to a series, gives this output:
```
Traceback (most recent call last):
  File ""/Users/mark/PycharmProjects/temp/bug1/cut_test.py"", line 8, in <module>
    print(pd.cut(test.row_value, cuts.cut_value, labels=cuts.cut_label[:-1], ordered=False))
  File ""/Users/mark/PycharmProjects/temp/bug1/venv/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 284, in cut
    ordered=ordered,
  File ""/Users/mark/PycharmProjects/temp/bug1/venv/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 384, in _bins_to_cuts
    if not ordered and not labels:
  File ""/Users/mark/PycharmProjects/temp/bug1/venv/lib/python3.7/site-packages/pandas/core/generic.py"", line 1327, in __nonzero__
    f""The truth value of a {type(self).__name__} is ambiguous. ""
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().

Process finished with exit code 1
```

#### Expected Output

The expected output is a series of strings taken from the labels list, as documented.

You can see from the Traceback that the error occurs at line 384 of tile.py. which is
```
    if not ordered and not labels:
```
If that line is changed to 
```
    if not ordered and labels is None:
```
then running the sample code again gives the correct output:
```
0    even
1     odd
2     odd
3    even
4     odd
Name: row_value, dtype: category
Categories (2, object): ['even', 'odd']

Process finished with exit code 0
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.5.0
Version          : Darwin Kernel Version 19.5.0: Tue May 26 20:41:44 PDT 2020; root:xnu-6153.121.2~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_CA.UTF-8
pandas           : 1.1.2
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
545305906,30681,CLN: de-privatize _parsers and _engines in pandas/core/computation and rename to something more descriptive,hwalinga,closed,2020-01-04T16:51:26Z,2020-09-25T07:36:29Z,"As a followup from #28215 

_parsers and _engines as used in the code for the `.query` and `.eval` methods should be de-privatized and named to something more informative. 

```
_engines: Dict[str, Type[AbstractEngine]] = {
    ""numexpr"": NumExprEngine,
    ""python"": PythonEngine,
}
```

```
_parsers = {""python"": PythonExprVisitor, ""pandas"": PandasExprVisitor}
```

@jreback "
708572472,36616,"Revert ""[BUG]: Fix regression when adding timeldeta_range to timestamp""",jbrockmendel,closed,2020-09-25T01:29:21Z,2020-09-25T08:52:06Z,Reverts pandas-dev/pandas#36582
708410647,36610,REGR: DataFrame.apply() with raw option and func returning string,simonjayhawkins,closed,2020-09-24T19:11:12Z,2020-09-25T09:06:31Z,"- [ ] closes #35940
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
708092708,36599,numpy version in py36_locale_slow_old_np on 1.1.x,simonjayhawkins,closed,2020-09-24T11:32:56Z,2020-09-25T09:14:15Z,"PR against 1.1.x

not sure if important but numpy version in https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=43144&view=logs&jobId=a69e7846-138e-5465-0656-921e8964615b&j=a69e7846-138e-5465-0656-921e8964615b&t=56da51de-fd5a-5466-5244-b5f65d252624 is 1.19.2

xref #33729"
602395231,33622,BUG: pd.Series.replace does not preserve the original dtype,kotamatsuoka,closed,2020-04-18T06:57:53Z,2020-09-25T09:33:30Z,"- [ ] closes #33484
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
708482229,36613,BUG: Fix unordered cut with Series labels,dsaxton,closed,2020-09-24T21:21:09Z,2020-09-25T09:43:22Z,"- [x] closes #36603
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
708774327,36631,Backport PR #36610 on branch 1.1.x (REGR: DataFrame.apply() with raw option and func returning string),meeseeksmachine,closed,2020-09-25T09:04:06Z,2020-09-25T10:10:33Z,Backport PR #36610: REGR: DataFrame.apply() with raw option and func returning string
324445428,21118,to_csv failing with encoding='utf-16',lgonzalezsa,closed,2018-05-18T15:00:24Z,2020-09-25T11:12:45Z,"#### Code Sample:

```python
df.to_csv('test.gz', sep='~',  header=False, index=False,compression='gzip',line_terminator='\r\n',encoding='utf-16', na_rep='')

```
/opt/anaconda/lib/python3.6/encodings/ascii.py in decode(self, input, final)
     24 class IncrementalDecoder(codecs.IncrementalDecoder):
     25     def decode(self, input, final=False):
---> 26         return codecs.ascii_decode(input, self.errors)[0]
     27 
     28 class StreamWriter(Codec,codecs.StreamWriter):

UnicodeDecodeError: 'ascii' codec can't decode byte 0xff in position 0: ordinal not in range(128)


#### Problem description

In first place, big thank you for supporting pandas, my life is easier and fun with pandas in the toolkit.
In previous version 0.22 we were able to do _to_csv_ with encoding='utf-16' to handle Japanese, Chinese among other content properly. Need the utf-16 encoding for next steps like upload data to MSSQL server in bulk mode.

I would like to know if I can use a workaround to continue have the support of uft-16.

Any other suggestions are welcome.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.5.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.114-42-default
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: POSIX
LOCALE: None.None

pandas: 0.23.0
pytest: 3.5.1
pip: 10.0.1
setuptools: 39.1.0
Cython: 0.28.2
numpy: 1.14.2
scipy: 1.1.0
pyarrow: 0.9.0
xarray: None
IPython: 6.4.0
sphinx: 1.7.4
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: 1.2.1
tables: 3.4.3
numexpr: 2.6.5
feather: None
matplotlib: 2.2.2
openpyxl: 2.5.3
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.4
lxml: 4.2.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.7
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
708800348,36633,Backport PR #36613 on branch 1.1.x (BUG: Fix unordered cut with Series labels),meeseeksmachine,closed,2020-09-25T09:43:31Z,2020-09-25T11:35:56Z,Backport PR #36613: BUG: Fix unordered cut with Series labels
708060898,36597,wheel build failing on MacPython.pandas-wheels,simonjayhawkins,closed,2020-09-24T10:42:57Z,2020-09-25T11:54:53Z,"opening a new issue for visibility.

https://github.com/pandas-dev/pandas/pull/36393#issuecomment-694299430

> wheel build failing https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=42575&view=logs&j=a846d25a-e32c-5640-1b53-e815fab94407

"
635537586,34668,BUG: GH29461 Strftime,matteosantama,closed,2020-06-09T15:29:49Z,2020-09-25T15:06:59Z,"- [x] closes #29461 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Closed the other PR while I sorted some things out. "
704823171,36471,"CI: add pre-commit action, include pyupgrade",MarcoGorelli,closed,2020-09-19T07:15:22Z,2020-09-25T18:27:43Z,"xref this comment https://github.com/pandas-dev/pandas/issues/36426#issuecomment-694238629 by @TomAugspurger 

> I would recommend running the linting commands on CI through pre-commit. That way the versions in .pre-commit-config.yaml are used everywhere.

So, this PR does that.

It also adds pyupgrade as a pre-commit hook (xref #36450 ) and removes some unnecessary configurations from the `.pre-commit-config.yaml` file (all of these hooks already specify `language: python`, see e.g. https://github.com/psf/black/blob/master/.pre-commit-hooks.yaml )

TODO
----
update https://pandas.pydata.org/docs/development/contributing.html#code-standards (thanks Ali!)"
703855779,36434,REF: pandas/io/formats/format.py,ivanovmg,closed,2020-09-17T19:34:09Z,2020-09-25T19:10:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Partially addresses https://github.com/pandas-dev/pandas/issues/36407

Before splitting ``DataFrameFormatter`` into more dedicated classes, I decided to refactor the class itself, to make the outstanding refactoring more manageable.

As suggested by @jreback, I made this refactor small, trying to split big functions into smaller ones and find some better naming.
Yet, rather small changes are done so far, just to keep diffs readable.
Once approved, I will move on to further refactor.
"
592777084,33239,"Unable to Build pandas with Py38, macOS (clang?) and Cython>0.29.13 (tp_print warning)",WillAyd,closed,2020-04-02T16:52:24Z,2020-09-25T22:36:13Z,"Dedicated issue for @jbrockmendel callout in https://github.com/pandas-dev/pandas/issues/30609#issuecomment-605373212

I opened an issue on the Cython side as well https://github.com/cython/cython/issues/3474

There's a rabbit hole of conversation with this going back to CPython . This may just be a temporary issue for Py38, so one ""workaround"" could be to just add `-Wno-deprecated-declaration` when these particular combination of things is detected, though of course an actual fix would be ideal"
454808666,26792,Annotate DataFrame (API exposed items from pandas.core.api),vaibhavhrt,closed,2019-06-11T17:13:54Z,2020-09-25T22:43:34Z,"Part of #26766 

<details>
<summary>

- [x] Part 1 #26867
</summary>

  - `_constructor`
  - `_constructor_expanddim`
  - `__init__`
  - `axes`
  - `shape`
  - `_is_homogeneous_type`

</details>

<details>
<summary>

- [x] Part 2 (Rendering Methods) #28453
</summary>

  - `_repr_fits_vertical_`
  - `_repr_fits_horizontal_`
  - `_info_repr`
  - `__repr__`
  - `_repr_html_`
  - `to_string`
</details>
<details>
<summary>

- [ ] Part 3 #28575 
</summary>

  - `style`
  - `items`
  - `iteritems`
  - `iterrows`
  - `itertuples`
  - `__len__`
  - `dot`
  - `__matmul__`
  - `__rmatmul__`
</details>
<details>
<summary>

- [ ] Part 4 (IO methods)
</summary>

  - `from_dict`
  - `to_numpy`
  - `to_dict`
  - `to_gbq`
  - `from_records`
  - `to_records`
  - `from_items`
  - `_from_arrays`
  - `to_stata`
  - `to_feather`
  - `to_parquet`
  - `to_html`
</details>
"
709159184,36640,fix test test warnings,jbrockmendel,closed,2020-09-25T17:47:53Z,2020-09-26T00:41:25Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
708583489,36620,BUG: Grouper should have a dropna attribute,arw2019,closed,2020-09-25T02:03:25Z,2020-09-26T01:17:59Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

xref #35612

#### Code Sample, a copy-pastable example

```python
In [1]: import pandas as pd 
   ...: df = pd.DataFrame({""A"": [0, 0, 1, None], ""B"": [10, 2, 10, None]}) 
   ...: gb = df.groupby('A', dropna=False)                                     

In [2]: gb.dropna                                                              
Out[2]: False

In [3]: gb.grouper.dropna                                                      
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-3-4d8081f9c78e> in <module>
----> 1 gb.grouper.dropna

AttributeError: 'BaseGrouper' object has no attribute 'dropna'
```

#### Problem description

Now that we support a `dropna` argument to `DataFrameGroupBy` and `SeriesGroupBy` we need to be able to use it in `Grouper` (#35751)

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : f34a56b4ddcf51a8a03a9d20f07ab039f64ff2e6
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-48-generic
Version          : #52-Ubuntu SMP Thu Sep 10 10:58:49 UTC 2020
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+484.gf34a56b4d
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 45.2.0.post20200210
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : 5.19.0
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.4.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : 0.4.0
gcsfs            : 0.6.2
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.1

</details>
"
705814996,36529,add generate_pip_deps_from_conda.py to pre-commit,MarcoGorelli,closed,2020-09-21T18:17:39Z,2020-09-26T01:19:12Z,This could be a local hook. I'll have a go at this later this week if no one takes it
709318776,36646,BUG: ndarray[td64] // TimedeltaArray,jbrockmendel,closed,2020-09-25T22:21:35Z,2020-09-26T01:19:16Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
709246634,36644,CLN: Fix some spelling,dsaxton,closed,2020-09-25T20:02:47Z,2020-09-26T01:19:47Z,
708582073,36619,CLN: share setitem/getitem validators,jbrockmendel,closed,2020-09-25T01:59:46Z,2020-09-26T01:21:03Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
699616251,36294,BUG:unexpected behavior of pandas 1.1.1 dt.normalize() on pre-epoch dates,koopmatt,closed,2020-09-11T18:26:34Z,2020-09-26T01:22:28Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
print(pd.__version__)
1.1.1

df = pd.DataFrame({'year': [1969, 2016],'month': [1, 1],'day': [1, 1],'hour': [9, 9]})
df=pd.to_datetime(df)

df

Out[10]: 
0   1969-01-01 09:00:00
1   2016-01-01 09:00:00
dtype: datetime64[ns]

df.dt.normalize()

Out[11]: 
0   1969-01-02
1   2016-01-01
dtype: datetime64[ns]
```

#### Problem description

I would expect pre-epoch behavior to be the same as post-epoch

#### Expected Output
```
Out[10]: 
0   1969-01-01 09:00:00
1   2016-01-01 09:00:00
dtype: datetime64[ns]

df.dt.normalize()

Out[11]: 
0   1969-01-01
1   2016-01-01
dtype: datetime64[ns]
```

#### Output of ``pd.show_versions()``

<details>
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.6.10.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.17763
machine          : AMD64
processor        : Intel64 Family 6 Model 85 Stepping 4, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en
LOCALE           : None.None

pandas           : 1.1.1
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200814
Cython           : 0.29.21
pytest           : 6.0.1
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.3
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.8.0
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.19
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.1


</details>
"
663620223,35376,BUG: inconsistent replace,qlieumontadv,closed,2020-07-22T09:37:32Z,2020-09-26T01:25:20Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Problem description

```python
>>> pd.DataFrame([[1,1.0],[2,2.0]]).replace(1.0, 5)
   0    1
0  1  5.0
1  2  2.0

>>> pd.DataFrame([[1,1.0],[2,2.0]]).replace(1, 5)
   0    1
0  5  5.0
1  2  2.0
```

#### Problem description

Maybe I don't understand somethink or this is just non-sens

#### Expected Output


```python
>>> pd.DataFrame([[1,1.0],[2,2.0]]).replace(1.0, 5)
   0    1
0  1  5.0
1  2  2.0

>>> pd.DataFrame([[1,1.0],[2,2.0]]).replace(1, 5)
   0    1
0  5  1.0
1  2  2.0
```
Or
```python
>>> pd.DataFrame([[1,1.0],[2,2.0]]).replace(1.0, 5)
   0    1
0  5  5.0
1  2  2.0

>>> pd.DataFrame([[1,1.0],[2,2.0]]).replace(1, 5)
   0    1
0  5  5.0
1  2  2.0
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-62-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : 0.6.2
lxml.etree       : None
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.5.1
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.50.1

</details>
"
685629622,35897,Regression in pandas 1.1.0: no longer allows adding a timestamp to timedelta range to create a datetime range.,analog-cbarber,closed,2020-08-25T16:35:54Z,2020-09-26T01:31:39Z,"Under pandas 1.0.5:

```python
>>> import pandas as pd
>>> pd.Timestamp.now() + pd.timedelta_range('0s','1s', periods=31)
DatetimeIndex([   '2020-08-25 12:27:57.690332',
               '2020-08-25 12:27:57.723665333',
               '2020-08-25 12:27:57.756998666',
                  '2020-08-25 12:27:57.790332',
               '2020-08-25 12:27:57.823665333',
               '2020-08-25 12:27:57.856998666',
                  '2020-08-25 12:27:57.890332',
               '2020-08-25 12:27:57.923665333',
               '2020-08-25 12:27:57.956998666',
                  '2020-08-25 12:27:57.990332',
               '2020-08-25 12:27:58.023665333',
               '2020-08-25 12:27:58.056998666',
                  '2020-08-25 12:27:58.090332',
               '2020-08-25 12:27:58.123665333',
               '2020-08-25 12:27:58.156998666',
                  '2020-08-25 12:27:58.190332',
               '2020-08-25 12:27:58.223665333',
               '2020-08-25 12:27:58.256998666',
                  '2020-08-25 12:27:58.290332',
               '2020-08-25 12:27:58.323665333',
               '2020-08-25 12:27:58.356998666',
                  '2020-08-25 12:27:58.390332',
               '2020-08-25 12:27:58.423665333',
               '2020-08-25 12:27:58.456998666',
                  '2020-08-25 12:27:58.490332',
               '2020-08-25 12:27:58.523665333',
               '2020-08-25 12:27:58.556998666',
                  '2020-08-25 12:27:58.590332',
               '2020-08-25 12:27:58.623665333',
               '2020-08-25 12:27:58.656998666',
                  '2020-08-25 12:27:58.690332'],
              dtype='datetime64[ns]', freq=None)
```

Under pandas 1.1.0:

```python
>>> import pandas as pd
>>> pd.Timestamp.now() + pd.timedelta_range('0s','1s', periods=31)
Traceback (most recent call last):
  File ""/Users/cbarber/miniconda3/envs/episodic-dev3/lib/python3.6/site-packages/pandas/core/arrays/datetimelike.py"", line 1157, in _validate_frequency
    raise ValueError
ValueError
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""/Users/cbarber/miniconda3/envs/episodic-dev3/lib/python3.6/site-packages/pandas/core/indexes/extension.py"", line 147, in method
    result = meth(_maybe_unwrap_index(other))
  File ""/Users/cbarber/miniconda3/envs/episodic-dev3/lib/python3.6/site-packages/pandas/core/arrays/datetimelike.py"", line 1446, in __radd__
    return self.__add__(other)
  File ""/Users/cbarber/miniconda3/envs/episodic-dev3/lib/python3.6/site-packages/pandas/core/ops/common.py"", line 65, in new_method
    return method(self, other)
  File ""/Users/cbarber/miniconda3/envs/episodic-dev3/lib/python3.6/site-packages/pandas/core/arrays/datetimelike.py"", line 1406, in __add__
    result = self._add_datetimelike_scalar(other)
  File ""/Users/cbarber/miniconda3/envs/episodic-dev3/lib/python3.6/site-packages/pandas/core/arrays/timedeltas.py"", line 441, in _add_datetimelike_scalar
    return DatetimeArray(result, dtype=dtype, freq=self.freq)
  File ""/Users/cbarber/miniconda3/envs/episodic-dev3/lib/python3.6/site-packages/pandas/core/arrays/datetimes.py"", line 284, in __init__
    type(self)._validate_frequency(self, freq)
  File ""/Users/cbarber/miniconda3/envs/episodic-dev3/lib/python3.6/site-packages/pandas/core/arrays/datetimelike.py"", line 1171, in _validate_frequency
    ) from e
ValueError: Inferred frequency None from passed values does not conform to passed frequency 33333333N
```

This error really makes no sense given the above code.

Obviously an unintended side-effect of some other fix.
"
708953775,36636,PERF: large perf regression in DataFrame repr,jorisvandenbossche,closed,2020-09-25T13:48:10Z,2020-09-26T07:11:26Z,"On master:

```
In [1]: df = pd.DataFrame(np.random.randn(1_000_000, 10))

In [3]: %timeit repr(df) 
2.5 s ± 340 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

while on released version this takes around 20ms

cc @ivanovmg I suppose this is from https://github.com/pandas-dev/pandas/pull/36434"
709391308,36652,Backport PR #36557 on branch 1.1.x ([BUG]: Fix bug with pre epoch normalization),meeseeksmachine,closed,2020-09-26T01:23:23Z,2020-09-26T08:24:06Z,Backport PR #36557: [BUG]: Fix bug with pre epoch normalization
704136802,36444,BUG: inconsistent replace,QuentinN42,closed,2020-09-18T06:50:27Z,2020-09-26T08:26:17Z,"- [x] tests added
- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] release note in `doc/source/whatsnew/v1.1.3.rst`

Must fix some replace in IntBlock if called with float integer (such as 1.0).


closes #35376"
708016925,36595,"Partial Revert ""ENH: infer freq in timedelta_range (#32377)""",simonjayhawkins,closed,2020-09-24T09:37:11Z,2020-09-26T08:36:51Z,"closes #35897

maybe alternative to #36582 if no changes since #32377 now fail (not tested locally)

cc @phofl @jbrockmendel "
708609684,36624,CLN: simplify interpolate_2d and callers,jbrockmendel,closed,2020-09-25T03:20:47Z,2020-09-26T09:55:07Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
583267032,32785,DISC: use of typing.cast,simonjayhawkins,closed,2020-03-17T20:06:04Z,2020-09-26T10:14:27Z,placeholder to address https://github.com/pandas-dev/pandas/pull/32730#discussion_r393485655
709472413,36658,Backport PR #36444 on branch 1.1.x (BUG: inconsistent replace),meeseeksmachine,closed,2020-09-26T08:26:56Z,2020-09-26T10:21:57Z,Backport PR #36444: BUG: inconsistent replace
709477350,36659,"Backport PR #36595 on branch 1.1.x (Partial Revert ""ENH: infer freq in timedelta_range (#32377)"")",meeseeksmachine,closed,2020-09-26T08:36:59Z,2020-09-26T10:24:52Z,"Backport PR #36595: Partial Revert ""ENH: infer freq in timedelta_range (#32377)"""
688241392,35958,BUG: read_table raises ValueError when delim_whitespace is set to True,cgmorton,closed,2020-08-28T17:34:40Z,2020-09-26T10:31:10Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import io
import pandas as pd

# Build a simple space delimited file buffer to read
f = io.StringIO(""a  b  c\n1 -2 -3\n4  5   6"")

# This raises an error
df = pd.read_table(f, delim_whitespace=True)

# Setting the ""sep"" parameter as suggested in the docs works
# df = pd.read_table(f, sep='\s+')
# print(df)

# Not setting the delim_whitespace parameter or setting it to False works, 
#   but without correct formatting.
# df = pd.read_table(f)
# print(df)
```

#### Problem description

If you set the delim_whitespace to True when calling read_table() on a space delimited file, I get the following exception:

```
ValueError: Specified a delimiter with both sep and delim_whitespace=True; you can only specify one.
```

#### Expected Output

I would expect the same output as you get if the sep='\s+' parameter is set (as suggested in the docs).

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.6.10.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Thu Jun 18 20:50:10 PDT 2020; root:xnu-4903.278.43~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.6.0.post20200814
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
709513188,36661,Backport PR #36560 on branch 1.1.x ([BUG]: Fix regression in read_table with delim_whitespace=True),meeseeksmachine,closed,2020-09-26T10:33:25Z,2020-09-26T11:32:18Z,Backport PR #36560: [BUG]: Fix regression in read_table with delim_whitespace=True
705158483,36504,CLN: Unify Series case in _wrap_applied_output,rhshadrach,closed,2020-09-20T18:19:53Z,2020-09-26T12:00:06Z,One block of _wrap_applied_output is only executed when `first_not_none` is a Series; moved it to the else clause.
706688937,36559,TYP/CLN: exclusions in BaseGroupBy,rhshadrach,closed,2020-09-22T20:58:50Z,2020-09-26T12:00:16Z,"Part of the code had exclusions as a list of hashables, change this to a set."
708480774,36612,CLN: Avoid importing Series in core.aggregation,rhshadrach,closed,2020-09-24T21:18:22Z,2020-09-26T12:00:22Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Breaks import cycles allowing aggregation to be imported at the top."
709520848,36663,CLN: lint fixup on 1.1.x,simonjayhawkins,closed,2020-09-26T11:29:00Z,2020-09-26T12:12:35Z,xref https://github.com/pandas-dev/pandas/pull/36658#issuecomment-699475451
645019199,34979,DOC: Add notes about M and Y to to_timedelata documentation.  (#34968),selasley,closed,2020-06-24T22:31:56Z,2020-09-26T13:20:32Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Not sure it closes these issues, but using ""M"" in the arg argument of to_timedelta caused confusion in #34968 and #27285.  #33094 is somewhat related. I came across another issue I can't find now where there was confusion about ""Y"" returning a days with times delta"
704942054,36479,BUG: KeyError after groupby().size() when index is multiindex and involves nan,hdou,closed,2020-09-19T17:30:49Z,2020-09-26T14:03:01Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd

data = [[pd.NA, 10], [pd.NA,10], [2, pd.NA]]
df = pd.DataFrame(data, columns=['A1', 'A2'])
print(df)

counts = df.groupby(['A1', 'A2'],dropna=False).size()
print(counts)

# If you uncomment the lines below, it works
# pk_file = 'tmp.df'
# counts.to_pickle(pk_file)
# counts = pd.read_pickle(pk_file)

idx = counts.index[0]
print(counts.loc[idx])  # KeyError: (2.0, nan)
```

#### Problem description
Unable to access the Series returned from groupby using apparently valid key
Line ""print(counts.loc[idx]"" raises KeyError.
If I uncomment the lines that store/load the data with pickle, then it works. No KeyError is raised.
[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output
The Series returned from groupby is accessible with loc with valid keys
#### Output of ``pd.show_versions()``
<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Thu Jun 18 20:50:10 PDT 2020; root:xnu-4903.278.43~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.2
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.1.0
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
</details>
"
709538526,36667,REGR: fillna not filling NaNs after pivot without explicitly listing pivot values,simonjayhawkins,closed,2020-09-26T13:33:09Z,2020-09-26T14:52:01Z,"- [ ] closes #36495

alternative to #34407 that only restores `self._consolidate_inplace()` to fix reported regression in #36495

This WILL fail see https://github.com/pandas-dev/pandas/pull/34407#issuecomment-699488350 and https://github.com/pandas-dev/pandas/pull/34407#issuecomment-699495512

if all green apart from those two failures can then maybe revert #35839 and #35854"
615212159,34093,DISC: Deprecate use_inf_as_na,dsaxton,closed,2020-05-09T16:28:58Z,2020-09-26T15:36:45Z,"I think a case could be made for deprecating pd.options.mode.use_inf_as_na. Few thoughts:

* Doesn't seem necessary or particularly useful: it's easy to get the same behavior by checking for NA _or_ infinity.
* Source of bugs: e.g., https://github.com/pandas-dev/pandas/issues/33655, https://github.com/pandas-dev/pandas/issues/33594. If an array has a cached mask attribute there's no guarantee that it's correct:
  ```python
  import numpy as np
  import pandas as pd

  arr = pd.Categorical([1.0, 2.0, np.inf])
  pd.options.mode.use_inf_as_na = True
  arr.isna().any()  # False
  ```
* General maintenance cost: several null-checking functions either have to be duplicated or equipped with boolean arguments to switch their behavior (e.g., _isna and _isna_ndarraylike from /core/dtypes/missing.py, or isnaobj and isnaobj_old from /_libs/missing.pyx). These could all be cleaned up if NA had _one_ meaning.

Thoughts?"
663450869,35373,ENH: Add validation checks for non-unique merge keys,LewisDavies,closed,2020-07-22T03:44:53Z,2020-09-26T15:44:19Z,"- [x] closes #27430
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

After implementing the suggestion in #27430 I realized it should also apply to `1:m` and `m:1` merges. When any type of `many` merge is specified, an error will be raised if the `many` side is actually unique."
661472132,35348,fix generate_range function in datetimes.py file,zky001,closed,2020-07-20T07:18:49Z,2020-09-26T15:45:47Z,"change some issues be metioned at #35342

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
709532577,36665,BUG: Join changes dtypes,AlbertoEAF,closed,2020-09-26T12:53:50Z,2020-09-26T16:33:43Z,"Hello,

#### Problem description

I have two pandas dataframes that I want to join using pandas (1.1.12).

However, when I join them, ""alerts"", in the table B gets its type changed from int64 to float64 (""alerts_cards""):

```python
(Pdb) A.dtypes
threshold_step      int64
precision         float64
alerts              int64
alerted_money     float64
dtype: object

(Pdb) B.dtypes
threshold_step      int64
precision         float64
alerts              int64
dtype: object

(Pdb) A.join(B, on=""threshold_step"", rsuffix=""_cards"", sort=True).dtypes
threshold_step            int64
precision               float64
alerts                    int64
alerted_money           float64
threshold_step_cards    float64
precision_cards         float64
alerts_cards            float64
dtype: object
```
I usually then remove the join key (""threshold_step_cards""), but now I'm noticing it became a float as well.

The join key column has the same entries in both tables (all integers in range 0 to 100) and there are no NaN's in my dataframes.

#### Expected Output

The join should preserve the datatypes of the original columns. I spent several hours looking for a bug in my code until with the debugger I found it came from this pandas join.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-118-generic
Version          : #119-Ubuntu SMP Tue Sep 8 12:30:01 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : pt_PT.UTF-8

pandas           : 1.1.2
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.2.0.post20200511
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.16
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>

Thank you"
389047642,24173,DOC: Fix all flake8 issues and warning in the documentation pages,datapythonista,closed,2018-12-09T18:03:12Z,2020-09-26T16:36:12Z,"This issue is to track the pending work in making the `.rst` documentation pages be free of PEP-8 and other flake8 issues, and to not generate warnings.

| Page                         | flake8 | warnings |
|------------------------------|--------|----------|
| `10min.rst`                  | FAIL: #24174 | FAIL |
| `advanced.rst`               | OK: #24182 | FAIL |
| `api.rst`                    | OK | FAIL |
| `basics.rst`                 | FAIL | FAIL |
| `categorical.rst`            | OK: #24182 | FAIL |
| `comparison_with_r.rst`      |  OK    | FAIL |
| `comparison_with_sas.rst`    | OK: #24182 | FAIL |
| `comparison_with_sql.rst`    | OK: #24182 | FAIL |
| `comparison_with_stata.rst`  | OK: #24182 | FAIL |
| `computation.rst`            | OK     | FAIL |
| `contributing.rst`           | OK: #24182 | FAIL |
| `contributing_docstring.rst` | FAIL | FAIL |
| `cookbook.rst`               | OK: #23794 | FAIL |
| `developer.rst`              | OK     | FAIL |
| `dsintro.rst`                | FAIL: #24175 | FAIL |
| `ecosystem.rst`              | OK     | FAIL |
| `enhancingperf.rst`          | FAIL: #24176 | FAIL |
| `extending.rst`              | OK: #24182 | FAIL |
| `gotchas.rst`                | OK     | FAIL |
| `groupby.rst`                | FAIL: #24178 | FAIL |
| `index.rst`                  | OK     | FAIL |
| `indexing.rst`               | PR: #24089 | FAIL |
| `install.rst`                | OK     | FAIL |
| `internals.rst`              | OK     | FAIL |
| `io.rst`                     | OK: #23791 | FAIL |
| `merging.rst`                | FAIL: #24179 | FAIL |
| `missing_data.rst`           | PR: #24089 | FAIL |
| `options.rst`                | PR: #24089 | FAIL |
| `overview.rst`               | OK     | FAIL |
| `r_interface.rst`            | OK     | FAIL |
| `release.rst`                | PR: #24089 | FAIL |
| `releases.rst`               | OK     | FAIL |
| `reshaping.rst`              | FAIL: #24180 | FAIL |
| `sparse.rst`                 | OK     | FAIL |
| `text.rst`                   | OK     | FAIL |
| `timedeltas.rst`             | OK     | FAIL |
| `timeseries.rst`             | OK     | FAIL |
| `tutorials.rst`              | OK     | FAIL |
| `visualization.rst`          | FAIL: #24181 | FAIL |

After this issue is closed, we shouldn't have any file (other than the whatsnew pages) in the `exclude` section of `flake8-rst` in `setup.cfg`, and we should be able to enable the `-W` flag for `sphinx-build` in `doc/make.py`.

CC: @FHaase "
709524614,36664,DOC: minor fix for 1.1.3 release notes,simonjayhawkins,closed,2020-09-26T11:58:40Z,2020-09-26T16:43:55Z,
709564429,36669,Backport PR #36664 on branch 1.1.x (DOC: minor fix for 1.1.3 release notes),meeseeksmachine,closed,2020-09-26T16:09:12Z,2020-09-26T16:56:14Z,Backport PR #36664: DOC: minor fix for 1.1.3 release notes
709567187,36670,DOC: Fix release note typo,dsaxton,closed,2020-09-26T16:26:22Z,2020-09-26T17:21:29Z,
709572671,36672,Backport PR #36670: DOC: Fix release note typo,dsaxton,closed,2020-09-26T17:00:11Z,2020-09-26T18:18:40Z,
708723563,36627,DOC: Replaced single backticks with double backticks in several rst files …,kaptajnen,closed,2020-09-25T07:45:46Z,2020-09-26T19:41:04Z,"…(#36617)

I did not fix all of the files, but it's a start :).
"
708782659,36632,Replace single with double backticks in RST file #36617,Shubhamsm,closed,2020-09-25T09:16:09Z,2020-09-26T22:50:28Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

closes https://github.com/pandas-dev/pandas/issues/36617"
708772597,36630,CLN:replace single with double backticks,satrio-hw,closed,2020-09-25T09:01:33Z,2020-09-26T22:53:15Z,"#36617


    doc/source/development/contributing.rst
    doc/source/development/contributing_docstring.rst
    doc/source/development/extending.rst
    doc/source/ecosystem.rst
    doc/source/getting_started/install.rst
    doc/source/getting_started/comparison/comparison_with_sql.rst
    doc/source/getting_started/intro_tutorials/06_calculate_statistics.rst
    doc/source/getting_started/intro_tutorials/08_combine_dataframes.rst
    doc/source/user_guide/categorical.rst
"
709578798,36673,Update pre-commit-config.yaml,dsaxton,closed,2020-09-26T17:36:22Z,2020-09-26T22:57:02Z,"Got this error from pre-commit when attempting to manually backport a PR. I guessed it may be because the pre-commit-config.yaml files between this branch and master were out of sync? That seemed to fix things for me, and regardless I figure it shouldn't be wrong to align these files (this PR is a straight checkout from upstream/master).

```
(pandas-dev) ➜  pandas git:(1.1.x) ✗ git commit -am 'Backport PR #36670: DOC: Fix release note typo'
[INFO] Initializing environment for https://github.com/pre-commit/mirrors-mypy.
[INFO] Installing environment for https://github.com/pre-commit/mirrors-mypy.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
An unexpected error has occurred: CalledProcessError: command: ('/Users/danielsaxton/opt/miniconda3/envs/pandas-dev/bin/python', '-mvirtualenv', '/Users/danielsaxton/.cache/pre-commit/repop3iybga6/py_env-python3.8', '-p', '/Users/danielsaxton/opt/miniconda3/envs/pandas-dev/bin/python')
return code: 1
expected return code: 0
stdout:
    ModuleNotFoundError: No module named 'virtualenv.seed.via_app_data'
```"
517940238,29417,DataFrame shift along columns not respecting position when dtypes are mixed,pirsquared,closed,2019-11-05T18:07:25Z,2020-09-26T22:58:30Z,"#### Setup

```python
import pandas as pd
df = pd.DataFrame(dict(
    A=[1, 2], B=[3., 4.], C=['X', 'Y'],
    D=[5., 6.], E=[7, 8], F=['W', 'Z']
))

df
df.shift(axis=1)
```

Outputs

```python
   A    B  C    D  E  F
0  1  3.0  X  5.0  7  W
1  2  4.0  Y  6.0  8  Z

    A   B    C    D    E  F
0 NaN NaN  NaN  3.0  1.0  X
1 NaN NaN  NaN  4.0  2.0  Y
```
#### Problem description
[See Associated Stackoverflow question](https://stackoverflow.com/q/58715801/2336654)

The shift places a column's values into the next column that shares the same dtype.  The expectation is that the column's values are placed into the adjacent column.

#### Expected Output

```python
dtypes = df.dtypes.shift(fill_value=object)
df_shifted = df.astype(object).shift(1, axis=1).astype(dtypes)

df_shifted

     A  B    C  D    E  F
0  NaN  1  3.0  X  5.0  7
1  NaN  2  4.0  Y  6.0  8
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1062.4.1.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.0
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : 0.29.12
pytest           : 5.0.1
hypothesis       : None
sphinx           : 2.1.2
blosc            : None
feather          : 0.4.0
xlsxwriter       : None
lxml.etree       : 4.3.4
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.6.1
pandas_datareader: 0.7.4
bs4              : 4.7.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.1.0
numexpr          : 2.6.9
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.11.1
pytables         : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : 1.3.5
tables           : 3.5.2
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
457608165,26929,BUG: DataFrame.shift with axis=1 shifts object columns to the next column with object dtype,nchrisr,closed,2019-06-18T17:32:55Z,2020-09-26T22:58:41Z,"#### Code Sample

```python2.7

import pandas as pd
import StringIO

# Store the csv string in a variable 
gps_string = """"""
""2010-01-12 18:00:00"",""$GPGGA"",""180439"",""7249.2150"",""N"",""11754.4238"",""W"",""2"",""10"",""0.9"",""-8.1"",""M"",""-12.4"",""M"","""",""*57"","""","""",""""
""2010-01-12 17:30:00"",""$GPGGA"",""173439"",""7249.2160"",""N"",""11754.4233"",""W"",""2"",""11"",""0.8"",""-4.5"",""M"",""-12.4"",""M"","""",""*5B"","""","""",""""
""2010-01-12 17:00:00"",""$GPGGA"",""170439"",""7249.2152"",""N"",""11754.4235"",""W"",""2"",""11"",""0.8"",""-3.1"",""M"",""-12.4"",""M"","""",""*5C"","""","""",""""
""2010-01-12 16:30:00"",""N"",""11754.4210"",""W"",""2"",""09"",""1.1"",""-13.1"",""M"",""-12.4"",""M"","""",""*6C"","""","""","""","""","""",""""
""2010-01-12 16:00:00"",""N"",""11754.4229"",""W"",""2"",""10"",""0.9"",""-2.9"",""M"",""-12.4"",""M"","""",""*53"","""","""","""","""","""",""""
""2010-01-12 15:30:00"",""N"",""11754.4269"",""W"",""2"",""09"",""0.8"",""-4.3"",""M"",""-12.4"",""M"","""",""*54"","""","""","""","""","""",""""
""2010-01-12 15:00:00"",""N"",""11754.4267"",""W"",""2"",""10"",""0.8"",""-1.6"",""M"",""-12.4"",""M"","""",""*56"","""","""","""","""","""",""""
""2010-01-12 14:30:00"",""$GPGGA"",""143439"",""7249.2152"",""N"",""11754.4253"",""W"",""2"",""11"",""0.7"",""-4.3"",""M"",""-12.4"",""M"","""",""*56"","""","""",""""
""2010-01-12 14:00:00"",""N"",""11754.4245"",""W"",""2"",""10"",""0.9"",""-7.0"",""M"",""-12.4"",""M"","""",""*50"","""","""","""","""","""",""""
""2010-01-12 13:30:00"",""$GPGGA"",""133439"",""7249.2134"",""N"",""11754.4243"",""W"",""2"",""11"",""0.7"",""-10.7"",""M"",""-12.4"",""M"","""",""*61"","""","""",""""
""2010-01-12 13:00:00"",""N"",""11754.4245"",""W"",""2"",""10"",""0.8"",""-5.5"",""M"",""-12.4"",""M"","""",""*56"","""","""","""","""","""",""""
""2010-01-12 12:30:00"",""N"",""11754.4226"",""W"",""2"",""10"",""0.9"",""-7.1"",""M"",""-12.4"",""M"","""",""*59"","""","""","""","""","""",""""
""2010-01-12 12:00:00"",""N"",""11754.4238"",""W"",""2"",""10"",""0.8"",""-6.5"",""M"",""-12.4"",""M"","""",""*51"","""","""","""","""","""",""""
""2010-01-12 11:30:00"",""N"",""11754.4227"",""W"",""2"",""10"",""0.8"",""0.1"",""M"",""-12.4"",""M"","""",""*73"","""","""","""","""","""",""""
""2010-01-12 11:00:00"",""-7.4"",""M"",""-12.4"",""M"","""",""*5F"","""","""","""","""","""","""","""","""","""","""","""",""""
""2010-01-12 10:30:00"",""N"",""11754.4271"",""W"",""2"",""08"",""1.1"",""-8.4"",""M"",""-12.4"",""M"","""",""*5A"","""","""","""","""","""","""" """"""

# Read the csv string into a dataframe.
gps_df = pd.read_csv(StringIO.StringIO(gps_string), header=None, index_col=0)
rows_to_shift = gps_df[gps_df[15].isnull()].index
gps_df.loc[rows_to_shift] = gps_df.loc[rows_to_shift].shift(periods=1, axis=1)
# create this file to see the outcome
gps_df.to_csv(""f.csv"") 
```
#### Problem description

I am trying to use the ```Dataframe. shift()```  function to move certain rows of data into their correct columns, and the ```Dataframe.shift()``` function is doing some weird things, it is creating an empty column of ```null(s)``` and moves one of the columns to the end of the ```dataframe```.

**Screenshots**

Original data:

<img width=""1268"" alt=""Screen Shot 2019-06-18 at 4 09 22 PM"" src=""https://user-images.githubusercontent.com/24297785/59721139-3c332080-91e6-11e9-9468-fc876234ecaa.png"">

Output after execution of the code

<img width=""1250"" alt=""Screen Shot 2019-06-18 at 4 31 15 PM"" src=""https://user-images.githubusercontent.com/24297785/59721303-9d5af400-91e6-11e9-9cdb-6988a32e9720.png"">


As seen above, the data that was originally in column 10  has been move to column 15  for some reason. Also a column with ```null``` values has been created in column 6.

I expect that the data should be moved to the right by one step, that is the data in each column should move to the column to the left of it, and the current behaviour is confusing, based on the documentation of what this function should do.

My expected output:

<img width=""1227"" alt=""Screen Shot 2019-06-18 at 4 16 40 PM"" src=""https://user-images.githubusercontent.com/24297785/59721366-ced3bf80-91e6-11e9-9330-7889d4afffc2.png"">

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.14.final.0
python-bits: 32
OS: Windows

</details>
"
255300672,17441,"Unexpected behavior in DataFrame.shift(..., axis=1) with missing data",mgoldwasser,closed,2017-09-05T14:38:59Z,2020-09-26T22:58:55Z,"#### Code Sample
```python
import numpy as np
import pandas as pd

def check_shift(t):
    print('Original:', t, sep='\n')
    a = t.shift(-5, axis=1)
    print('Incorrect result:', a, sep='\n')
    b = t.transpose().shift(-5).transpose()
    print('Correct result:', b, sep='\n')
    return(a.equals(b))

# create dummy dataframe
df = pd.DataFrame(np.random.randint(0, 1, size=(2, 10)))
# add some missing values 
df.iloc[0, 7:10] = np.NaN
# below evaluates to false, but should be true
print('Test passed:', check_shift(df))
```
#### Problem description

The shift operation with `axis=1` produces unexpected results when the underlying dataframe contains missing values. 

The issue is that unexpected additional missing values get inserted with the shift operation.


#### Expected Output
`print(check_shift(df))` should return true
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.1.final.0
python-bits: 64
OS: Darwin
OS-release: 16.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.20.3
pytest: None
pip: 9.0.1
setuptools: 32.2.0
Cython: None
numpy: 1.13.1
scipy: 0.19.0
xarray: None
IPython: 6.0.0
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: 0.4.0
matplotlib: 2.0.2
openpyxl: None
xlrd: 1.0.0
xlwt: None
xlsxwriter: None
lxml: None
bs4: 4.6.0
html5lib: 0.999999999
sqlalchemy: 1.1.10
pymysql: None
psycopg2: 2.7.1 (dt dec pq3 ext lo64)
jinja2: 2.9.6
s3fs: None
pandas_gbq: None
pandas_datareader: None
</details>
"
298799497,19804,DTI/TDI comparison ops with incompatible types,jbrockmendel,closed,2018-02-21T00:25:19Z,2020-09-26T23:18:30Z,"Timestamp and Timedelta comparisons against non-Timestamp/Timedelta-like others return False for `__eq__`, True for `__ne__` and raise `TypeError` for inequalities.  DatetimeIndex follows this convention in _some_ cases, but there are paths that will end up raising instead of returning True/False.  TimedeltaIndex doesn't follow this convention at all.

Is the current hodge-podge intentional?  If not I can put together a PR to ensure that `index.__cmp__(other) == np.array([index[n].__cmp__(other) for n in range(len(index))])`"
709603429,36676,CLN: moments/test_moments_rolling.py for apply,mroeschke,closed,2020-09-26T20:12:35Z,2020-09-26T23:39:40Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Overall easier to grok these tests."
709632855,36679,CLN: test_moments_rolling.py for sum,mroeschke,closed,2020-09-26T23:45:16Z,2020-09-27T00:27:03Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
694633442,36179,DEPR: string indexing along index for datetimes,jbrockmendel,closed,2020-09-07T02:21:18Z,2020-09-27T01:04:33Z,"- [x] closes #31476
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Let's get ourselves one less special case to worry about"
607087248,33809,BUG: Don't raise in DataFrame.corr with pd.NA,dsaxton,closed,2020-04-26T19:00:04Z,2020-09-27T03:27:52Z,"- [x] closes #33803
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
688606405,35979,"BUG: Respect errors=""ignore"" during extension astype",dsaxton,closed,2020-08-29T21:24:13Z,2020-09-27T03:27:54Z,"- [x] closes https://github.com/pandas-dev/pandas/issues/35471
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
364267519,22849,"Fix blank line skipping in read_fwf, issue 22693",xiejxie,closed,2018-09-27T01:09:01Z,2020-09-27T04:45:29Z,"- [x] closes #22693 
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Fixes the bug where blank lines are not skipped using read_fwf even when skip_blank_lines is set to true. Override default behaviour in FixedWidthFieldParser."
496048508,28545,CI: Move last builds from Travis to Azure,datapythonista,closed,2019-09-19T22:05:49Z,2020-09-27T14:10:20Z,"I think this makes sense, and it will simplify things having a single CI system.

If I'm not missing anything, there are few challenges:
- The number of jobs (I think we should get 10 extra concurrent jobs in Azure, since the CI is ok but will be too slow)
- Setting up databases in Azure (I think we currently set up Postgres and MySQL, and may be in the future we want SQL server too #9097)
- I think we have some encrypted stuff in azure for GBQ (probably can be easily moved to Azure secrets)

If there is any objection, let me know, otherwise I'll move forward with this.

CC: @bhavaniravi "
553120346,31184,_BaseOffset's parameter `n` should be prevented from being 0,MarcoGorelli,closed,2020-01-21T20:47:21Z,2020-09-27T21:49:14Z,"The docstring says
```
        Require that `n` be a nonzero integer.
```
but the ""nonzero"" condition isn't enforced.

Enforcing it would prevent the undesired behaviour noticed in #29901 , whereby
```
cbh = pd.offsets.CustomBusinessHour(start='07:00', end='01:00')
time = pd.Timestamp('2019-03-06 13:00:00')
time + 0*cbh
```
was returning
```
2019-03-07 07:00:00
```"
378827754,23571,"Timedelta/TimedeltaIndex does not accept ""unit"" higher than ns",jbrockmendel,closed,2018-11-08T16:59:44Z,2020-09-28T01:30:58Z,"```
>>> td = np.timedelta64(1000, 'ps')
>>> pd.Timedelta(td).value
1
>>> pd.Timedelta(1000, unit='ps')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pandas/_libs/tslibs/timedeltas.pyx"", line 1020, in pandas._libs.tslibs.timedeltas.Timedelta.__new__
  File ""pandas/_libs/tslibs/timedeltas.pyx"", line 136, in pandas._libs.tslibs.timedeltas.convert_to_timedelta64
  File ""pandas/_libs/tslibs/timedeltas.pyx"", line 225, in pandas._libs.tslibs.timedeltas.cast_from_unit
ValueError: cannot cast unit ps
```"
709632437,36678,CLN: test_moments_rolling.py for mean/std/var/count/median/min/max,mroeschke,closed,2020-09-26T23:41:04Z,2020-09-28T01:57:44Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
709798363,36687,Fix typo in timeseries.rst,cdeil,closed,2020-09-27T19:14:56Z,2020-09-28T02:44:06Z,
709545093,36668,REGR: fillna not filling NaNs after pivot without explicitly listing pivot values on 1.1.x,simonjayhawkins,closed,2020-09-26T14:15:29Z,2020-09-28T10:21:15Z,"- [ ] xref #36495

yet another alternative to fix directly on 1.1.x pending further discussion in #34407 on how to resolve the issues on master.

cc @jorisvandenbossche @jbrockmendel 

This won't close issue until fix on master. could copy test to master with xfail (and would need to sync release notes if we merge this direct to 1.1.x)"
710201704,36701,BUG: .replace() of an empty string for None returns value from previous row,ertuganova,closed,2020-09-28T11:57:24Z,2020-09-28T13:27:39Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
#### Code Sample, a copy-pastable example

```python
d = {'col1' : ['test']*9, 'col2': [0,'',3,4,'', '', 6, 7, '']}
test = pd.DataFrame(data=d)
In [4]:
test['col2']
Out[4]:
0    0
1     
2    3
3    4
4     
5     
6    6
7    7
8     
Name: col2, dtype: object
In [5]:
test['col2'].replace('', None)
Out[5]:
0    0
1    0
2    3
3    4
4    4
5    4
6    6
7    7
8    7
Name: col2, dtype: object
In [6]:
test['col2'].replace({'': None})
Out[6]:
0    0.0
1    NaN
2    3.0
3    4.0
4    NaN
5    NaN
6    6.0
7    7.0
8    NaN
Name: col2, dtype: float64
```

#### Problem description

.replace() of an empty string for None value returns the value of previous row (instead of None). However .replace() of the same empty strings using dict structure returns None as it was expected.
#### Expected Output

It is expected that .replace('', None) will return the same result as .replace({'': None})

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.7.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.5.0
Version          : Darwin Kernel Version 19.5.0: Tue May 26 20:41:44 PDT 2020; root:xnu-6153.121.2~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.2
numpy            : 1.17.2
pytz             : 2018.7
dateutil         : 2.7.5
pip              : 19.3.1
setuptools       : 41.6.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 7.1.1
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.0.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : 0.8.3
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
459113587,26982,DOC: Fix docstring quotes in pandas.tseries,datapythonista,closed,2019-06-21T09:51:24Z,2020-09-28T15:39:32Z,"In pandas we keep the quotes in the docstrings standardized in the next way:

```python
def foo():
    """"""
    This is correct.
    """"""
```

But for historical reasons we still have many of this form:
```python
def foo():
    """"""This is incorrect.""""""
```

We have a script that is able to detect the wrong cases, that gives the next errors in `pandas.tseries`:
```
$ ./scripts/validate_docstrings.py --errors=GL01,GL02 --prefix=pandas.tseries
pandas.tseries.offsets.DateOffset.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.DateOffset.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.BusinessDay.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.BusinessDay.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.BusinessHour.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.BusinessHour.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.CustomBusinessDay.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.CustomBusinessDay.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.CustomBusinessHour.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.CustomBusinessHour.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.MonthOffset.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.MonthOffset.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.MonthEnd.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.MonthEnd.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.MonthBegin.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.MonthBegin.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.BusinessMonthEnd.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.BusinessMonthEnd.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.BusinessMonthBegin.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.BusinessMonthBegin.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.CustomBusinessMonthEnd.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.CustomBusinessMonthEnd.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.CustomBusinessMonthBegin.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.CustomBusinessMonthBegin.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.SemiMonthOffset.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.SemiMonthOffset.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.SemiMonthEnd.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.SemiMonthEnd.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.SemiMonthBegin.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.SemiMonthBegin.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.Week.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.Week.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.WeekOfMonth.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.WeekOfMonth.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.LastWeekOfMonth.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.LastWeekOfMonth.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.QuarterOffset.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.QuarterOffset.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.BQuarterEnd.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.BQuarterEnd.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.BQuarterBegin.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.BQuarterBegin.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.QuarterEnd.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.QuarterEnd.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.QuarterBegin.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.QuarterBegin.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.YearOffset.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.YearOffset.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.BYearEnd.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.BYearEnd.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.BYearBegin.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.BYearBegin.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.YearEnd.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.YearEnd.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.YearBegin.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.YearBegin.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.FY5253.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.FY5253.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.FY5253Quarter.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.FY5253Quarter.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.Easter.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.Easter.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.Tick.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.Tick.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.Day.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.Day.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.Hour.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.Hour.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.Minute.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.Minute.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.Second.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.Second.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.Milli.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.Milli.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.Micro.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.Micro.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.Nano.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.Nano.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.BDay.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.BDay.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.BMonthEnd.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.BMonthEnd.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.BMonthBegin.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.BMonthBegin.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.CBMonthEnd.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.CBMonthEnd.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.CBMonthBegin.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.CBMonthBegin.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
pandas.tseries.offsets.CDay.normalize: Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
pandas.tseries.offsets.CDay.normalize: Closing quotes should be placed in the line after the last text in the docstring (do not close the quotes in the same line as the text, or leave a blank line between the last text and the quotes)
```
We should fix all them, in preparation to validate in the continuous integration that all docstrings in pandas follow our standard."
715875998,36920,TYP: check_untyped_defs core.computation.pytables,simonjayhawkins,closed,2020-10-06T17:23:38Z,2020-10-06T19:23:53Z,"pandas\core\computation\pytables.py:46: error: Argument 1 to ""__new__"" of ""object"" has incompatible type ""object""; expected ""Type[object]""  [arg-type]
pandas\core\computation\pytables.py:189: error: Incompatible types in assignment (expression has type ""Callable[[Any, int, Union[Mapping[str, str], Iterable[str], None], bool, bool, Optional[int]], str]"", variable has type ""partial[bytes]"")  [assignment]
pandas\core\computation\pytables.py:262: error: Incompatible types in assignment (expression has type ""Tuple[Any, ...]"", variable has type ""Optional[Tuple[Any, Any, Index]]"")  [assignment]
pandas\core\computation\pytables.py:351: error: Incompatible types in assignment (expression has type ""str"", variable has type ""None"")  [assignment]
pandas\core\computation\pytables.py:357: error: Incompatible types in assignment (expression has type ""str"", variable has type ""None"")  [assignment]
pandas\core\computation\pytables.py:364: error: Incompatible types in assignment (expression has type ""str"", variable has type ""None"")  [assignment]"
715773995,36917,TYP: check_untyped_defs core.computation.expressions,simonjayhawkins,closed,2020-10-06T15:12:29Z,2020-10-06T19:25:07Z,"pandas\core\computation\expressions.py:251: error: ""None"" not callable  [misc]

"
715040088,36894,ENH: allow non-consolidation in constructors,jbrockmendel,closed,2020-10-05T17:49:10Z,2020-10-06T21:25:41Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @TomAugspurger I think something like this is what is needed for #34872.

If this were a ""real"" PR, I'd probably not put `consolidate` as a `DataFrame.__init__` kwarg but instead set it to `True` and only change it to `False` based on the `copy` kwarg.

The usage in core.ops is not necessary, but is an example of where we can get a perf bump."
713944534,36819,ENH: Infer Enums as categorical,dzimmanck,open,2020-10-02T23:03:34Z,2020-10-06T21:30:29Z,"#### Is your feature request related to a problem?
This relates to #36124, but I think has other benefits. 

#### Describe the solution you'd like
I would like pandas to infer an array of Enums as categorical data.  Today, IntEnums are turned into Ints, which is not ideal behavior.

#### API breaking implications
This is a change to the way part of the current API behaves, but I believe it provides a more intuitive processing of Enum data.

#### Describe alternatives you've considered


#### Additional context

```python
# Your code here, if applicable

```
"
714419898,36870,TST: xfailed arithmetic tests,jbrockmendel,closed,2020-10-04T22:56:03Z,2020-10-06T22:07:35Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
708616300,36625,CLN: de-duplicate numeric type check in _libs/testing,arw2019,closed,2020-09-25T03:41:56Z,2020-10-06T22:08:07Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

In `testing.pyx` we keep a list of numeric dtypes and use that for type checking. But we have functions that do this in `tslibs/util.pxd` so it's better to use those instead"
564406668,31944,Errors in pandas._libs.lib.maybe_convert_objects leading to use of uninitialized memory,ahmadia,closed,2020-02-13T02:56:18Z,2020-10-06T22:43:17Z,"#### Code Sample 1

```python
import numpy as np
import pandas as pd
from pandas._libs import lib as lib
print(lib.maybe_convert_objects(np.array([np.nan, True], dtype='object'), safe=1))
```

#### Code Sample 2

```python
import numpy as np
import pandas as pd
from pandas._libs import lib as lib
print(lib.maybe_convert_objects(np.array([np.nan, np.datetime64()], dtype='object'), safe=1, convert_datetime=1))
```

#### Code Sample 3

```python
import math
import pandas as pd

z1 = pd.Series([True, False, pd.NA]).value_counts(dropna=False)
print(z1)
z2 = pd.Series({True: 1, False: 1, math.nan:1})
print(z1)
```

#### Problem description

This problem was originally detected by @benfuja when he noticed inconsistent output in the `value_counts` function when `dropna=False` for otherwise Boolean arrays containing a `pd.NA` indicator. Code Sample 3 is Ben's original example showing the odd behavior (note that z1 is printed twice, and depending on your malloc library, will likely change values between calls). When we tried to isolate what was happening, we ended up down a couple of rabbit holes leading to the `maybe_convert_objects` function.

What I suspect is happening is that when `maybe_convert_objects` is called by the Series index formatter, it is expecting that if a mixture of Booleans and `np.nan` are present, that the presence of `np.nan` will be preserved. Instead, I think what happens is that `maybe_convert_objects` erroneously returns garbage values when either Booleans or datetimes/timedeltas are present along with `np.nan` by returning a corresponding array with uninitialized empty memory in the locations where the `np.nan` values were present. Code Samples 1 and 2 are examples of this.

As noted in https://github.com/pandas-dev/pandas/issues/27417, there are probably a few opportunities to improve this function and its dependencies. I'm not particularly familiar with this code base, but it seems to me that the most correct minimal fix may be in the Seen class:

#### Proposed Minimal Fix

```python
    @property
    def numeric_(self):
-        return self.complex_ or self.float_ or self.int_
+        return self.complex_ or self.float_ or self.int_ or self.null_ or self.nan_ 
```

#### Rationale

The `maybe_convert_objects` method relies on the `Seen` class to tell it if it is safe to consider an array as containing only values of a specific type. When `maybe_convert_objects` encounters a `NaN` or a `None`, it stores the encountered value in the `floats` and `complexes` arrays and sets the `seen.nan_` / `seen.null_` flags. Later checks for whether an array is of a specific type will miss that these values were encountered if these flags are not checked for. This seemed like the most logical place to put these checks in `Seen`, since both `self.null_` and `self.nan_` are stored in the `floats/complexes` arrays earlier. Another option would be to fix the logic in `maybe_convert_objects` to more carefully check the `self.nan_/seen.null_` flags in the case statements.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 45.0.0.post20200113
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>

"
713130607,36783,Test that nan value counts are included when dropna=False. GH#31944,icanhazcodeplz,closed,2020-10-01T19:38:27Z,2020-10-06T22:43:21Z,"- [x] closes #31944
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Added test per @Dr-Irv request.
"
700557964,36330,CLN: Followup to 35964,rhshadrach,closed,2020-09-13T12:08:18Z,2020-10-06T22:47:09Z,"- [ ] Move whatsnew note to reshaping
- [ ] Maybe use is_list_like/is_dict_like
- [ ] Type results"
715900416,36922,DOC: code example isn't formatted in `pandas.DataFrame.to_sql`,hkennyv,closed,2020-10-06T18:01:16Z,2020-10-06T23:04:34Z,"#### Location of the documentation

The code example in [pandas.DataFrame.to_sql](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html?highlight=to_sql#pandas.DataFrame.to_sql) isn't formatted properly.

(it's the example that inserts User 4 and User 5 into the db)

**Note**: You can check the latest versions of the docs on `master` [here](https://pandas.pydata.org/docs/dev/).

#### Documentation problem

![image](https://user-images.githubusercontent.com/29909203/95242497-1d361480-07c4-11eb-9dd5-fa1eea90413f.png)

oh no, the code isn't formatted correctly!

#### Suggested fix for documentation

The reason is there's no blank line before the code snippet, see:

https://github.com/pandas-dev/pandas/blob/b58ee5779583b31de513b3d5c12f5c69c035e920/pandas/core/generic.py#L2692-L2695

Add a blank line above the code snippet :), i have a PR in progress"
714426676,36871,REF: IntervalIndex.equals defer to IntervalArray.equals,jbrockmendel,closed,2020-10-04T23:34:49Z,2020-10-06T23:10:30Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Small behavior change in that `interval_index.equals(series[interval])` will now return False, matching other Index subclasses."
707180150,36570,CLN clean ups in code,erfannariman,closed,2020-09-23T08:59:14Z,2020-10-07T01:06:22Z,"Started with whitespaces before colons, ended up doing some other clean ups as well."
688766368,35997,BUG: Reading from parquet throws UnknownTimeZoneError using timezone-aware date in index,alippai,closed,2020-08-30T18:19:25Z,2020-10-07T01:44:44Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

```python
import pandas as pd
from datetime import datetime, timezone

df = pd.DataFrame([[datetime.now(timezone.utc)]], columns=['date']).set_index('date')
df.to_parquet('out.parquet')
pd.read_parquet('out.parquet')
```

#### Problem description
The bug above happens with pandas 1.1.1 and pyarrow 1.0.1.
The timezone-aware date in the index should survive the parquet round trip.
If `date` is not index, or when I add parameter `ignore_metadata=True` to the `pyarrow.Table.to_pandas()` it works (but `date` won't be an index automatically)

#### Expected Output
A correct DataFrame

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.104-microsoft-standard
Version          : #1 SMP Wed Feb 19 06:37:35 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
</details>
"
652309115,35160,CI: add sql tests back to py36,jreback,closed,2020-07-07T13:09:27Z,2020-10-07T02:24:20Z,"xref https://github.com/pandas-dev/pandas/issues/34776

removed in the PR: https://github.com/pandas-dev/pandas/pull/35151, we should add them back.

cc @VirosaLi "
705152241,36501,Modify doc/source/whatsnew7,cleconte987,closed,2020-09-20T17:41:27Z,2020-10-07T02:42:07Z,"- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] modify files in doc/source/whatsnew 'v0.25.0.rst' 'v0.25.1.rst' 'v0.25.2.rst' 'v0.25.3.rst' 'v1.0.0.rst' 'v1.0.1.rst' 'v1.0.2.rst' 'v1.0.3.rst' 'v1.0.4.rst' 'v1.05.rst' 'v1.1.0.rst'"
706081760,36541,BUG: ValueError: cannot convert float NaN to integer when resetting MultiIndex with NaT values,ssche,closed,2020-09-22T05:09:50Z,2020-10-07T02:43:42Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
In [18]: ix = pd.MultiIndex.from_tuples([(pd.NaT, 1), (pd.NaT, 2)], names=['a', 'b'])

In [19]: ix
Out[19]: 
MultiIndex([('NaT', 1),
            ('NaT', 2)],
           names=['a', 'b'])

In [20]: d = pd.DataFrame({'x': [11, 12]}, index=ix)

In [21]: d
Out[21]: 
        x
a   b    
NaT 1  11
    2  12

In [22]: d.reset_index()
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-22-4653618060e8> in <module>
----> 1 d.reset_index()

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/frame.py in reset_index(self, level, drop, inplace, col_level, col_fill)
   4851                     name = tuple(name_lst)
   4852                 # to ndarray and maybe infer different dtype
-> 4853                 level_values = _maybe_casted_values(lev, lab)
   4854                 new_obj.insert(0, name, level_values)
   4855 

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/frame.py in _maybe_casted_values(index, labels)
   4784                     dtype = index.dtype
   4785                     fill_value = na_value_for_dtype(dtype)
-> 4786                     values = construct_1d_arraylike_from_scalar(
   4787                         fill_value, len(mask), dtype
   4788                     )

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/dtypes/cast.py in construct_1d_arraylike_from_scalar(value, length, dtype)
   1556 
   1557         subarr = np.empty(length, dtype=dtype)
-> 1558         subarr.fill(value)
   1559 
   1560     return subarr

ValueError: cannot convert float NaN to integer
```

#### Problem description

With the introduction and use of `groupby(..., dropna=False)` multiindex with NaT values are more likely to occur which exhibits a few issues that previously went undetected. This issue was discovered when finding a workaround for another `dropna=False` related issue (https://github.com/pandas-dev/pandas/issues/36060#issuecomment-695869490)

Further investigation shows that this may be an issue with numpy not accepting pd.NaT. The following code reproduces the issue in `construct_1d_arraylike_from_scalar`:

```
In [33]: a = np.empty(2, dtype='datetime64[ns]')

In [34]: a.fill(pd.NaT)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-34-9fa9ff66da99> in <module>
----> 1 a.fill(pd.NaT)

ValueError: cannot convert float NaN to integer
```

which led me to propose this fix:

```diff
--- a/pandas/core/dtypes/cast.py
+++ b/pandas/core/dtypes/cast.py
@@ -1559,6 +1559,12 @@ def construct_1d_arraylike_from_scalar(
             dtype = np.dtype(""object"")
             if not isna(value):
                 value = ensure_str(value)
+        elif isinstance(dtype, np.dtype) and dtype.kind == ""M"" and value is NaT:
+            # can't fill sub array directly with pandas' NaT:
+            #
+            # > a.fill(pd.NaT)
+            # ValueError: cannot convert float NaN to integer
+            value = np.datetime64(""NaT"")
 
         subarr = np.empty(length, dtype=dtype)
         subarr.fill(value)
```

the `value is NaT` check could possibly be extended to `isna(value)`...


#### Expected Output

No ValueError being raised

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 15539fa6217039c2e5cb70b1d942caec9c5a2de3
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_AU.UTF-8

pandas           : 1.2.0.dev0+453.g15539fa62.dirty
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 46.4.0
Cython           : 0.29.21
pytest           : 6.0.2
hypothesis       : 5.35.3
sphinx           : 3.2.1
blosc            : 1.9.2
feather          : None
xlsxwriter       : 1.3.4
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.8.2
fastparquet      : 0.4.1
gcsfs            : 0.7.1
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : 0.5.1
scipy            : 1.5.2
sqlalchemy       : 1.3.19
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.51.2


</details>
"
706856969,36563,Closes #36541 (BUG: ValueError: cannot convert float NaN to integer when resetting MultiIndex with NaT values),ssche,closed,2020-09-23T01:33:27Z,2020-10-07T02:43:48Z,"- [x] closes #36541
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
712842772,36773,DOC: Fix code block line length,dsaxton,closed,2020-10-01T13:26:00Z,2020-10-07T02:46:43Z,https://github.com/pandas-dev/pandas/pull/36734#discussion_r498058234
715901350,36923,DOC: add newline to fix code block format in `pandas.DataFrame.to_sql`,hkennyv,closed,2020-10-06T18:02:41Z,2020-10-07T02:48:19Z,"- [x] closes #36922 
- [x] tests added / passed n/a (didn't add any new tests/change code)
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

not sure if i need to add a whatsnew entry to 1.1.4 or 1.2.0...

would love to get a hacktoberfest tag if your org supports this. #36837 "
357208911,22606,TST: test that operations with Series(ExtensionArray) and list-likes work,jorisvandenbossche,closed,2018-09-05T12:37:43Z,2020-10-07T03:15:05Z,"xref https://github.com/pandas-dev/pandas/pull/22026/files#r212253565

Operations with Series holding an ExtensionArray and with a list as other operand work, eg:

```
In [29]: pd.Series([1, 2, 3], dtype='Int64') + [1, 2, 3]
Out[29]: 
0    2
1    4
2    6
dtype: Int64
```

but we should add an explicit test for it (now only test with other Series, and not for array / list)"
688713146,35987,TST: Verify operators with IntegerArray and list-likes (22606),avinashpancham,closed,2020-08-30T12:48:40Z,2020-10-07T03:15:09Z,"- [x] closes #22606
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
126479496,12034,ENH: Return RangeIndex for .difference and .symmetric_difference,jreback,closed,2016-01-13T18:13:36Z,2020-10-07T03:24:42Z,"xref https://github.com/pydata/pandas/pull/11892

possible to return `RangeIndex` more often for:
- [x] `union` #12109 
- [ ] `difference` & `sym_diff`
- [x] revisit `__floordiv__` #12070 
"
606175676,33760,BUG: inconsistent median support for datetimelike dtypes,jorisvandenbossche,closed,2020-04-24T09:30:01Z,2020-10-07T03:28:11Z,"https://github.com/pandas-dev/pandas/pull/29941 introduced support for taking the median of a datetime-like column. 
In released version, (pandas 1.0.3) we get:

```
In [20]: df = pd.DataFrame({'a': pd.date_range(""2012"", periods=3, freq='D')})  

In [21]: df.median()    
Out[21]: Series([], dtype: float64)

In [22]: df.median(numeric_only=False)  
...
TypeError: reduction operation 'median' not allowed for this dtype

In [23]: df['a'].median()  
...
TypeError: DatetimeIndex cannot perform the operation median
```

on master we now allow this for DataFrame (warning it will be in the future by default, and already allow it with numeric_only=False):

```
In [2]: df.median()  
/home/joris/miniconda3/envs/dev/bin/ipython:1: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64, datetime64tz, and PeriodDtype columns in a future version.
  #!/home/joris/miniconda3/envs/dev/bin/python
Out[2]: Series([], dtype: float64)

In [3]: df.median(numeric_only=False)  
Out[3]: 
a    2012-01-02 00:00:00
dtype: object
```

but for a single column it still fails:

```
In [4]: df['a'].median()  
...
TypeError: cannot perform median with type datetime64[ns]
```

So we should make this support consistent (add ""median"" support on DatetimeArray, so it also works for Series)

cc @jbrockmendel "
710468818,36710,BUG: df.diff axis=1 mixed dtypes,jbrockmendel,closed,2020-09-28T18:00:46Z,2020-10-07T03:31:26Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
706891871,36564,"ENH: return RangeIndex from difference, symmetric_difference",jbrockmendel,closed,2020-09-23T02:03:47Z,2020-10-07T03:32:08Z,"- [x] closes #12034
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
681817324,35802,BUG: Was trying to read an ods file and ran into UnboundLocalError in odfreader.py,taksuyu,closed,2020-08-19T12:54:55Z,2020-10-07T07:09:27Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
for file in os.listdir('data'): pandas.read_excel(pathlib.Path('data', file), engine='odf')

```
Sorry I don't have a minimal data example at this time.

#### Problem description

Was trying to test pandas reading a collection of ods files and ran into this error.
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/michael/.local/share/virtualenvs/merge-csv-NFbvYFrS/lib/python3.8/site-packages/pandas/util/_decorators.py"", line 296, in wrapper
    return func(*args, **kwargs)
  File ""/home/michael/.local/share/virtualenvs/merge-csv-NFbvYFrS/lib/python3.8/site-packages/pandas/io/excel/_base.py"", line 311, in read_excel
    return io.parse(
  File ""/home/michael/.local/share/virtualenvs/merge-csv-NFbvYFrS/lib/python3.8/site-packages/pandas/io/excel/_base.py"", line 906, in parse
    return self._reader.parse(
  File ""/home/michael/.local/share/virtualenvs/merge-csv-NFbvYFrS/lib/python3.8/site-packages/pandas/io/excel/_base.py"", line 443, in parse
    data = self.get_sheet_data(sheet, convert_float)
  File ""/home/michael/.local/share/virtualenvs/merge-csv-NFbvYFrS/lib/python3.8/site-packages/pandas/io/excel/_odfreader.py"", line 91, in get_sheet_data
    value = self._get_cell_value(sheet_cell, convert_float)
  File ""/home/michael/.local/share/virtualenvs/merge-csv-NFbvYFrS/lib/python3.8/site-packages/pandas/io/excel/_odfreader.py"", line 175, in _get_cell_value
    return self._get_cell_string_value(cell)
  File ""/home/michael/.local/share/virtualenvs/merge-csv-NFbvYFrS/lib/python3.8/site-packages/pandas/io/excel/_odfreader.py"", line 211, in _get_cell_string_value
    value.append("" "" * spaces)
UnboundLocalError: local variable 'spaces' referenced before assignment
```

I took a look at the [code in question](https://github.com/pandas-dev/pandas/blob/ddf4f2430dd0a6fd51d7409ad3f524aeb5cbace2/pandas/io/excel/_odfreader.py#L211) and it seems like the line may be on the wrong indent level?

#### Expected Output

The usual dataframes :+1: 

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-42-generic
Version          : #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 44.0.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
714020838,36826,CI add end-of-file-fixer,MarcoGorelli,closed,2020-10-03T06:43:25Z,2020-10-07T07:26:31Z,"xref https://github.com/pandas-dev/pandas/pull/36600#issuecomment-702976850

> ok if you'd fix the EOF issue can merge (and yes good idea to add to the pre-commit hook / separate PR pls)

Note: in addition to adding the end-of-file-fixer pre-commit hook, I applied the following

```diff
diff --git a/scripts/generate_pip_deps_from_conda.py b/scripts/generate_pip_deps_from_conda.py
index c417f58f6..c6d00eb58 100755
--- a/scripts/generate_pip_deps_from_conda.py
+++ b/scripts/generate_pip_deps_from_conda.py
@@ -94,7 +94,7 @@ def main(conda_fname, pip_fname, compare=False):
         f""# This file is auto-generated from {fname}, do not modify.\n""
         ""# See that file for comments about the need/usage of each dependency.\n\n""
     )
-    pip_content = header + ""\n"".join(pip_deps)
+    pip_content = header + ""\n"".join(pip_deps) + ""\n""
 
     if compare:
         with open(pip_fname) as pip_fd:
```

----

I don't know how many of these we want to apply this to, for now I'm just excluding licenses (which I don't think should be touched?), html, txt, svg,and csv files"
714265050,36855,BUG: Column name is missing after concat if the key contain only one column of string data,Super169,closed,2020-10-04T08:36:33Z,2020-10-07T09:00:17Z,"I'm going to develop a simple application to compare two dataframe with given keys (multiple index columns).
I need to generate 3 dataframe as below,
- final_1 : contain all rows in df1, plus empty row with keys in df2 only
- final_2 : contain all rows in df2, plus empty row with keys in df1 only
- compare_result: contain information of merge ('both','left_only','right_only'), together with column comparison result

Please find the source beblow, there has some problem with particular keys combination.
I try to make a simple dataset and include in the source.

In the attached dataset, 
- It works find with the keys ['org_code','sub_key'], but with error if only ['org_code']
- It also work after I convert the 'org_code' to create a int column of 'dummy_code'.
- It also failed using ['user_name'] as keys, but no error for ['amount'].

It seems that there will have problem if the keys contain only one column of string data (e.g. 'org_code').

It was found that the index column name is missing for df_all in those fail cases.
Which cause the error like:    **KeyError: ""['org_code'] not in index""**

df_all is generated using concat command, and it need to get a merged dataframe with columns of df1 and df2 under different group, and join by the key_columns.
```python
	df_all = pd.concat([df1.set_index(key_columns), df2.set_index(key_columns)], axis='columns', keys=['df1', 'df2'])
```

The workaround is to set the index name after df_all is generated if the key contain only one column.
i.e.
```Python
if len(key_columns) == 1:
    df_all.index.name = key_columns[0]
```
But it doesn't work if similar problem occurred in multiple columns situation.  So it'd be better if concat can keep the original column name.

May I know if this is the problem in concat command, or anything wrong in my case?  Thanks.



#### Code Sample, a copy-pastable example

```python
import tkinter as tk
from TableChecker import TableChecker
import pandas as pd

df1 = pd.DataFrame({
    'org_code': ['A222', 'A252', 'B100019'],
    'dummy_code': [222, 252, 100019],
    'sub_key': [1,2,3],
    'user_name': ['mary', 'john', 'david'],
    'amount': [222, 313,  100019],
})


df2= pd.DataFrame({
    'org_code': ['S6258', 'B100019'],
    'dummy_code': [6258, 100019],
    'sub_key': [4,3],
    'user_name': ['peter', 'david'],
    'amount': [2232, 126],
})

org_columns = list(df1.keys())
result_columns = list(df1.keys())
result_columns.append('_merge')

key_columns = ['org_code']


df_merge = pd.merge(df1[key_columns], df2[key_columns], how='outer', indicator=True)
df_merge.set_index(key_columns, inplace=True)
df_all = pd.concat([df1.set_index(key_columns), df2.set_index(key_columns)], axis='columns', keys=['df1', 'df2'])
df_diff = df_all['df1'] != df_all['df2']
result = pd.concat([df_merge, df_diff], axis=1)

compare_result = result.reset_index()[result_columns]
final_1 = df_all['df1'].reset_index()[org_columns]
final_2 = df_all['df2'].reset_index()[org_columns]

```

#### Problem description

By modifying the line key_columns =  ['org_code'] to change the key columns, it will have different result.
1) Error for single string data column:  e.g., ['org_code'] or  ['user_name']
2) No error for single numeric data column: e.g., ['dummy_code'], ['sub_key'] or ['amount']
3) No error for combined keys: e.g.,  ['org_code','sub_key'] ,['org_code','user_name'] or ['org_code','dummy_code','sub_key']

#### Expected Output

The column name of index column in df_all should not be empty, it should contain the original column name.


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 62 Stepping 4, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.950

pandas           : 1.1.2
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.6.0
Cython           : None
pytest           : 5.4.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
689024766,36004,BUG: Can't restore index from parquet with offset-specified timezone #35997,alippai,closed,2020-08-31T08:30:23Z,2020-10-07T09:23:21Z,"- [x] closes #35997
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
714348112,36859,REF/CLN: pandas/io/pytables.py,ivanovmg,closed,2020-10-04T16:17:03Z,2020-10-07T09:44:10Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Clean-up and minor refactor of ``pandas/io/pytables.py``.
- Extract method ``_identify_group``
- Clean-up some docstrings (spacing)
- Use suppress instead of try/except/pass
- Format error message using dedent
- Remove unnecessary empty lines as they compromise readability of some blocks"
710484883,36713,CLN: pandas/core/indexing.py,ivanovmg,closed,2020-09-28T18:27:50Z,2020-10-07T09:46:13Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Slight clean-up of ``pandas/core/indexing.py``.

- Use ``suppress`` instead of ``try/except/pass`` (starting from Python 3.7)
- Slight rearrangement of statements
- Extract method ``_validate_key_length``"
683935884,35847,BUG: inconsistent naming when combining indices of various types,iamlemec,closed,2020-08-22T05:52:00Z,2020-10-07T11:33:29Z,"This is building off of some issues seen in #13475 and subsequent PR. Essentially, the resulting index name when combining existing named indices with `union_indexes` is not consistent across index types. This primarily affects `concat` and the `DataFrame` constructor, which call `union_indexes`.

When combining named indices, there are three main name resolution rules I can think of:
- `ignore`: assign no name to output
- `unanimous`: assign name if all names agree
- `consensus`: assign name only if one unique non-null name

With some testing, below is my best understanding of what resolution rule the various index types use. Note that the behavior may differ depending on whether the indices are numerically equal or not, as with RangeIndex. For the not numerically equal case:
- `Index`: consensus
- `RangeIndex`: ignore
- `Int64Index`: unanimous
- `Float64Index`: unanimous
- `DateTimeIndex`: consensus
- `TimeDeltaIndex`: consensus
- `PeriodIndex`: unanimous
- `CategoricalIndex`: unanimous
- `MultiIndex`: unanimous (over all levels)

I'm not really taking a stand on the correct name resolution rule, but I think they should at least be consistent across index type! And of course MultiIndex is a bit more complicated. Seems possible things could be implemented in common for non-multi-indices in the higher level `union` function? I'm not really sure, but I'm happy to put some work into it.

The test is slightly different for each index type, but for the RangeIndex case, here's an MWE:
``` python
idx1 = pd.RangeIndex(0, 5, name='idx')
idx2 = pd.RangeIndex(2, 7, name='idx')
pd.core.indexes.api.union_indexes([idx1, idx2])
```
and the result will have no name (checked on master)."
610946664,33930,"BUG: Inconsistent name of binary operations between Series and ndarray, list, tuple",gshimansky,closed,2020-05-01T20:15:39Z,2020-10-07T15:00:01Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```
import numpy as np
import pandas as pd

s1 = pd.Series([1, 2, 3, 4, 5], name = ""s1"")
nparray = np.arange(1, len(s1) + 1)
s2 = s1 / nparray
print(""s2.name = "", s2.name)
s3 = s1.div(nparray)
print(""s3.name = "", s3.name)

numbers_list = [1.0, 2.0, 3.0, 4.0, 5.0]
s4 = s1 / numbers_list
print(""s4.name = "", s4.name)
s5 = s1.div(numbers_list)
print(""s5.name = "", s5.name)

numbers_tuple = (1.0, 2.0, 3.0, 4.0, 5.0)
s6 = s1 / numbers_tuple
print(""s6.name = "", s6.name)
s7 = s1.div(numbers_tuple)
print(""s7.name = "", s7.name)
```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

Depending on how binary operation is done, e.g. arithmetic operation or function call resulting `name` is different.
- In case of function call it is preserved because `Series` object with no `name` is constructed from ndarray, list or tuple https://github.com/pandas-dev/pandas/blob/master/pandas/core/ops/__init__.py#L476. Call for `get_op_result_name` with arguments of original `Series` object and constructed `Series` object with no name leads to name being set to `None`.
- In case of arithmetic operation no `Series` object is created in `_arith_method_SERIES` and call to `get_op_result_name` to arguments of `Series` and non-ABCSeries, non-ABCIndexClass leads to `name` of `Series` operand being preserved.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.1
setuptools       : 41.2.0
Cython           : 0.29.17
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : 0.4.0
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.0
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : 0.13.1
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : None
tabulate         : None
xarray           : 0.15.0
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

INSTALLED VERSIONS
------------------
commit           : 81093ba8214476e979ea55765d7ddb609f729ac9
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-26-generic
Version          : #28-Ubuntu SMP Wed Dec 18 05:37:46 UTC 2019
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0.dev0+1453.g81093ba82
numpy            : 1.17.3
pytz             : 2019.2
dateutil         : 2.7.3
pip              : 18.1
setuptools       : 41.1.0
Cython           : 0.29.17
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : 0.4.0
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.11
tables           : 3.6.1
tabulate         : None
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : None
numba            : 0.46.0

</details>
"
712416677,36760,BUG: inconsistent name-retention in Series ops,jbrockmendel,closed,2020-10-01T02:14:15Z,2020-10-07T15:00:09Z,"- [x] closes #33930
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

As a follow-up I think we can remove a bunch of name-based tests from tests.arithmetic."
709854085,36694,ENH: DatetimeArray/PeriodArray median,jbrockmendel,closed,2020-09-28T00:42:32Z,2020-10-07T15:00:51Z,"- [x] closes #33760
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

nanops.nanmedian needs to be re-worked, and there's a lot more we can do to share reduction tests.  saving for separate branches."
712592625,36766,BUG: fix inconsistent col spacing in info,ivanovmg,closed,2020-10-01T07:52:29Z,2020-10-07T15:08:18Z,"- [x] closes #36765
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
712589342,36765,BUG: inconsistent spacing in df.info(),ivanovmg,closed,2020-10-01T07:48:07Z,2020-10-07T15:08:18Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

See that the spacing between all columns is 2 spaces. Even if there is a long string in Non-Null Count column.
```python
df = pd.DataFrame({'long long column': np.random.rand(1000000)})
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1000000 entries, 0 to 999999
Data columns (total 1 columns):
 #   Column            Non-Null Count    Dtype
---  ------            --------------    -----
 0   long long column  1000000 non-null  float64
dtypes: float64(1)
memory usage: 7.6 MB
```

Note that there is only one space between # and Column columns when the number of columns is over 1000 (last line of tail).

```python
import pandas as pd
import numpy as np
df = pd.DataFrame(np.random.rand(3, 10001))
with open('out.txt', 'w') as buf:
    df.info(verbose=True, buf=buf)
```

```bash
$ tail out.txt
 9993  9993    float64
 9994  9994    float64
 9995  9995    float64
 9996  9996    float64
 9997  9997    float64
 9998  9998    float64
 9999  9999    float64
 10000 10000   float64
dtypes: float64(10001)
memory usage: 234.5 KB
```
#### Problem description

I find inconsistent behavior in spacing between columns in the output ``df.info()`` when dealing with dataframes having over 1000 columns.

#### Expected Output

Expect to have two spaces distance between all columns regardless of the col widths.


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f6ccbbc1ee852754a268c46454803bc9a3f9aebf
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.76-linuxkit
Version          : #1 SMP Tue May 26 11:42:35 UTC 2020
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+559.gf6ccbbc1e.dirty
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 45.2.0.post20200210
Cython           : 0.29.21
pytest           : 6.0.1
hypothesis       : 5.23.11
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.1
lxml.etree       : 4.4.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.8.0
fastparquet      : 0.4.1
gcsfs            : 0.6.2
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.2
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.0
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.1
</details>
"
716095549,36930,TYP/REF: define comparison methods non-dynamically,jbrockmendel,closed,2020-10-06T23:52:37Z,2020-10-07T15:16:20Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref #31160, cc @simonjayhawkins @WillAyd

This implements OpsMixin which is more mypy-friendly than the status quo, and uses it in Series, Index, and DatetimeLikeArrayMixin.  There are a handful more places where it can be used in an upcoming pass.

Will do the same for arithmetic/boolean ops if there is consensus that this is worth pursuing."
715890864,36921,TYP: check_untyped_defs core.groupby.ops,simonjayhawkins,closed,2020-10-06T17:47:20Z,2020-10-07T15:45:08Z,pandas\core\groupby\ops.py:889: error: Need type annotation for 'sdata'  [var-annotated]
539223694,30308,DOC/TST: Indexing with NA raises,TomAugspurger,closed,2019-12-17T18:00:10Z,2020-10-07T15:52:55Z,"xref https://github.com/pandas-dev/pandas/issues/29556, https://github.com/pandas-dev/pandas/issues/28778

We're already doing the right thing on master. This just documents that behavior, and adds a handful of tests.

I'm not sure if there are existing tests I should be parameterizing. I only found a couple in `tests/indexing/`, which I parametrized over bool and boolean dtype. Will add more if I've missed any. In the meantime, I've written new tests."
705865503,36531,Add generate pip dependency's from conda to pre-commit,erfannariman,closed,2020-09-21T19:41:11Z,2020-10-07T16:12:02Z,"closes: #36529

First try, not sure yet how to activate local virtual env, error message right now is:

```
black................................................(no files to check)Skipped
flake8...............................................(no files to check)Skipped
flake8-pyx...........................................(no files to check)Skipped
flake8-pxd...........................................(no files to check)Skipped
isort................................................(no files to check)Skipped
Generate pip dependency from conda.......................................Failed
- hook id: pip_to_conda
- exit code: 1

Traceback (most recent call last):
  File ""scripts/generate_pip_deps_from_conda.py"", line 20, in <module>
    import yaml
ModuleNotFoundError: No module named 'yaml'
```"
703230020,36413,fix inconsistent index naming with union/intersect #35847,iamlemec,closed,2020-09-17T03:12:58Z,2020-10-07T18:46:01Z,"- [X] closes #35847
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This takes care of some inconsistency in how names are handled by `Index` functions `union` and `intersection`, as discussed in #35847. I believe this covers all index types, either through the base class or in the subclass when necessary.

What I've implemented here actually uses the unanimous convention, wherein all input names must match to get assigned to the output. Originally, I was thinking consensus would be better (assign if there is only one non-None input name), but looking through the existing tests, it seems that unanimous was usually expected. I also had some worries about whether index names would become too ""contagious"" with consensus. Anyway, it's easy to change between the two if people have strong opinions on this."
706642104,36553,Call finalize in Series.__array_ufunc__,TomAugspurger,closed,2020-09-22T19:38:02Z,2020-10-07T22:11:13Z,xref #28283
716749154,36954,TYP/REF: use _cmp_method in EAs,jbrockmendel,closed,2020-10-07T18:14:48Z,2020-10-07T22:37:11Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
709362250,36648,TST: GH23452 test reorder_categories() on categorical index,fokoid,closed,2020-09-26T00:04:23Z,2020-10-07T23:05:45Z,"- [x] closes #23452
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Couldn't see entries for tests in whatsnew files so I assume it's not needed if only a test case is added This is my first Pandas contribution, looking forward to your feedback."
716890880,36960,ENH: typo in citing page,erfannariman,closed,2020-10-07T22:25:21Z,2020-10-07T23:34:11Z,Small typo in `citing.md`
599025451,33531,BUG: Do not use string Index like Datetimelike Index,nrebena,closed,2020-04-13T17:37:31Z,2020-10-08T00:29:21Z,"Follow up of conversation in https://github.com/pandas-dev/pandas/pull/32739#issuecomment-602271577.

The goal is to prevent index of string that look like datetimelike to be used as datetimelike.

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
707606907,36583,BUG: Read_Table and Read_Csv does not raise when delim_whitespace=True and sep=default is given,phofl,closed,2020-09-23T18:59:31Z,2020-10-08T00:46:50Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.



#### Code Sample, a copy-pastable example

```python
import io
f = io.StringIO(""a  b  c\n1 -2 -3\n4  5   6"")
df = pd.read_table(f, sep=""\t"", delim_whitespace=True)
df = pd.read_csv(f, sep="","", delim_whitespace=True)

```

#### Problem description

Both statement should raise a ``ValueError``

#### Expected Output

``ValueError``

Must also edit docs. See #36560 for discussion.

cc @simonjayhawkins 

#### Output of ``pd.show_versions()``

<details>

master

</details>
"
710446378,36709,"Fix delim_whitespace behavior in read_table, read_csv",OlehKSS,closed,2020-09-28T17:22:46Z,2020-10-08T00:46:57Z,"- [x] closes #36583
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
700637001,36344,DOC: Projected time for running entire performance test suite is outdated,avinashpancham,closed,2020-09-13T20:01:04Z,2020-10-08T00:59:25Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#running-the-performance-test-suite

**Note**: You can check the latest versions of the docs on `master` [here](https://pandas.pydata.org/docs/dev/).

#### Documentation problem

Ran entire performance test suite for #36161 and it took close to 8 hours. Docs state: ""Running the full test suite can take up to one hour and use up to 3GB of RAM"". From @jreback I understood that this is because the test suite has increase a lot in size, since the last doc update.

#### Suggested fix for documentation

I would add a disclaimer regarding your hardware and state that running the tests on a laptop can take up to X hours. For X we can take 8 or another number if others have experienced an even longer duration for running the full performance test suite. So we end up with something like this:

""Projected duration for running the full test suite may vary depending on your machine, but on a modern laptop it will take up to 8 hours and use up to 3GB of RAM""

"
714114365,36836,DOC: Update docs on estimated time of running full asv suite (36344),avinashpancham,closed,2020-10-03T15:32:45Z,2020-10-08T00:59:47Z,"- [x] closes #36344 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
716915135,36963,BUG: Still getting blank when using `pandas.Series.to_string(index=False)`,simontorres,closed,2020-10-07T23:27:51Z,2020-10-08T01:14:27Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas

print(""pandas.__version__ == {:s}"".format(pandas.__version__))

columns = ['name', 'age']
values = [
    ['name_one', '31'],
    ['name_two', '32']]
data_frame = pandas.DataFrame(values, columns=columns)

# filter

filtered = data_frame[(data_frame['name'] == 'name_one') & (data_frame['age'] == '31')]
# single value comes with a blank at the beginning when running 0.24.0

print(""==={:s}==="".format(filtered.name.to_string(index=False)))

```

#### Problem description

This was already reported by me on #24980 and supposedly fixed by PR #36094 but I keep getting the same result with pandas `1.1.3`.

Using the sample code above the output is `=== name_one===`

I might be missing something but I can't see what.

#### Expected Output

Using the sampe code provided I expect it to return `===name_one===`

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.6.12.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.9.0-1.el7.elrepo.x86_64
Version          : #1 SMP Sun Dec 11 15:43:54 EST 2016
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200814
Cython           : 0.29.21
pytest           : 6.0.2
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None


</details>
"
92685259,10492,DOC: Distinguish between different types of boolean indexing,jcjf,closed,2015-07-02T17:00:45Z,2020-10-08T01:23:14Z,"It's not clear from the docs that indexing with a boolean `ndarray` isn't the same as indexing with a boolean `Series`. It took me a while to realise that:

``` python
import pandas as pd
df = pd.DataFrame([[1, 2], [3, 4], [5, 6]], list('abc'), ['one', 'two'])
sr = (df['one'] > 2)
df.loc[sr, 'two']      # This is ok
df.iloc[sr, 1]         # But this is not
df.iloc[sr.values, 1]  # The right way to use iloc
```

I've since read through #3631 and it makes sense to me. However, even though the docs emphasise that `.iloc` is **purely integer-based**, it didn't click at the time that indexability of `Series` was the problem I was facing.
"
714469456,36873,REF: dont consolidate in is_mixed_type,jbrockmendel,closed,2020-10-05T02:44:21Z,2020-10-08T01:25:21Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Avoid copies, side-effects."
717015034,36968,REF: de-duplicate skipna logic,ivanovmg,closed,2020-10-08T04:41:24Z,2020-10-08T05:19:13Z,"De-duplicate logic in handling skipna
inside methods any() and all().

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
182874273,14418,read_csv and others derived from _read close user-provided filehandles,ebolyen,closed,2016-10-13T19:09:25Z,2020-10-08T12:35:07Z,"I believe the ""regression"" was introduced on [this line](https://github.com/pandas-dev/pandas/commit/4a805216d99b37955c97625d304980eff10cab56#diff-777d7549579ddc0c6e67596ad87e0d27R1447). That being said, tracking which filehandles a library owns vs what a user provided is _hard_, and I can't fault you guys if this is considered correct behavior from now on. Just wanted to bring it to your attention. Thanks!
#### A small, complete example of the issue

``` pytb
In [1]: import pandas as pd

In [2]: import io

In [3]: fh = io.StringIO('a,b\n1,2\n')

In [4]: fh.closed
Out[4]: False

In [5]: pd.read_csv(fh)
Out[5]: 
   a  b
0  1  2

In [6]: fh.closed
Out[6]: True

```
#### Expected Output

``` pytb
In [6]: fh.closed
Out[6]: False
```
#### Output of `pd.show_versions()`

<details>
## INSTALLED VERSIONS

commit: None
python: 3.4.4.final.0
python-bits: 64
OS: Linux
OS-release: 3.13.0-96-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.19.0
nose: 1.3.7
pip: 8.1.2
setuptools: 20.1.1
Cython: None
numpy: 1.11.2
scipy: 0.18.1
statsmodels: None
xarray: None
IPython: 5.1.0
sphinx: 1.5a2
patsy: None
dateutil: 2.5.3
pytz: 2016.4
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: 1.5.3
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.8
boto: None
pandas_datareader: None

</details>
"
714700891,36882,upgrade flake8 to 3.8.4 #36879,Tazminia,closed,2020-10-05T10:03:21Z,2020-10-08T12:47:02Z,"closes #36879

Here are the logs after the upgrade to 3.8.4:

```console
(base) root@33d4c5d14a5b:/home/pandas-tazminia# pre-commit run flake8 --all
[INFO] Initializing environment for https://github.com/python/black.
[INFO] Initializing environment for https://gitlab.com/pycqa/flake8.
[INFO] Initializing environment for https://gitlab.com/pycqa/flake8:flake8-comprehensions>=3.1.0.
[INFO] Initializing environment for https://github.com/PyCQA/isort.
[INFO] Initializing environment for https://github.com/asottile/pyupgrade.
[INFO] Initializing environment for https://github.com/pre-commit/pygrep-hooks.
[INFO] Initializing environment for https://github.com/asottile/yesqa.
[INFO] Installing environment for https://gitlab.com/pycqa/flake8.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://gitlab.com/pycqa/flake8.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
flake8...................................................................Passed
flake8-pyx...............................................................Passed
flake8-pxd...............................................................Passed
```"
541511899,30416,BUG: Series.any() and .all() don't return bool values if dtype=object,ShaharNaveh,closed,2019-12-22T23:02:28Z,2020-10-08T13:32:15Z,"- [x] closes #12863
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
717231702,36976,BUG:300 ,GB-kushagra,closed,2020-10-08T10:45:44Z,2020-10-08T13:34:09Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here

```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
714680087,36880,BUG: any() and all() behavior on string series is different from python,ThewBear,closed,2020-10-05T09:32:57Z,2020-10-08T14:15:13Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
pd.Series([""kkkk"", ""llll""]).any() # ""kkkk""
any([""kkkk"", ""llll""]) # True

pd.Series([""kkkk"", ""llll""]).all() # ""llll""
all([""kkkk"", ""llll""]) # True
```

#### Problem description

Python returns boolean not the value. So we should follow that.
https://stackoverflow.com/questions/10180344/why-does-python-any-return-a-bool-instead-of-the-value

#### Expected Output

The expected behavior is to match `any` and `all` built-in functions.

https://docs.python.org/3/library/functions.html#any
```python
def any(iterable):
    for element in iterable:
        if element:
            return True
    return False
```
https://docs.python.org/3/library/functions.html#all
```python
def all(iterable):
    for element in iterable:
        if not element:
            return False
    return True
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.7.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.112+
Version          : #⁠1 SMP Fri Sep 4 12:06:06 PDT 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.2
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200712
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.18
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
</details>
"
716709690,36947,QST: Use object property as key?,DorianCzichotzki,closed,2020-10-07T17:13:32Z,2020-10-08T14:16:47Z,"Is there a way to use a property of an object as the key when providing the object to a DataFrame?
I think the code makes clearer what I am looking for.

```python
class Key:
  def __init__(self, name: str):
    self.name = name
  
  # Something like that
  def __pandas_key__(self):
    return self.name

df = DataFrame({""col"": [1,2,3,4]})
key = Key(""col"")

df[key] # is [1,2,3,4]
```
"
716113658,36931,TYP: define RangeIndex methods non-dynamically,jbrockmendel,closed,2020-10-07T00:46:21Z,2020-10-08T15:13:50Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
716846774,36959,"TYP: core.missing, ops.docstrings, internals.ops, internals.managers, io.html",jbrockmendel,closed,2020-10-07T20:57:55Z,2020-10-08T15:21:06Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
716926240,36964,REF/TYP: use OpsMixin for logical methods,jbrockmendel,closed,2020-10-08T00:00:58Z,2020-10-08T15:21:59Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
716093059,36929,"TYP: define Index.any, Index.all non-dynamically",jbrockmendel,closed,2020-10-06T23:45:24Z,2020-10-08T15:26:28Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref #31160, cc @simonjayhawkins 

This unearths 2.5 issues:
1) Index.any/all says that we pass *args/**kwargs to np.any/all, but we don't
1b) the RangeIndex methods don't have args/kwargs at all
2) the subclasses that disable any/all don't totally make sense.  In particular it seems like Float64Index should support it and CategoricalIndex should support it iff `self.categories` supports it"
717243198,36978,CI Upgrade isort in pre-commit,Tazminia,closed,2020-10-08T11:03:24Z,2020-10-08T17:43:10Z,"- xref #36879 

```console
(base) root@ada0229371a4:/home/pandas-tazminia# pre-commit autoupdate && pre-commit run --all
Updating https://github.com/python/black ... [INFO] Initializing environment for https://github.com/python/black.
already up to date.
Updating https://gitlab.com/pycqa/flake8 ... [INFO] Initializing environment for https://gitlab.com/pycqa/flake8.
already up to date.
Updating https://github.com/PyCQA/isort ... [INFO] Initializing environment for https://github.com/PyCQA/isort.
updating 5.2.2 -> 5.6.0.
Updating https://github.com/asottile/pyupgrade ... [INFO] Initializing environment for https://github.com/asottile/pyupgrade.
already up to date.
Updating https://github.com/pre-commit/pygrep-hooks ... [INFO] Initializing environment for https://github.com/pre-commit/pygrep-hooks.
already up to date.
Updating https://github.com/asottile/yesqa ... [INFO] Initializing environment for https://github.com/asottile/yesqa.
already up to date.
Updating https://github.com/pre-commit/pre-commit-hooks ... [INFO] Initializing environment for https://github.com/pre-commit/pre-commit-hooks.
already up to date.
[INFO] Initializing environment for https://gitlab.com/pycqa/flake8:flake8-comprehensions>=3.1.0.
[INFO] Installing environment for https://github.com/python/black.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://gitlab.com/pycqa/flake8.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://gitlab.com/pycqa/flake8.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/PyCQA/isort.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/asottile/pyupgrade.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/asottile/yesqa.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/pre-commit/pre-commit-hooks.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
black....................................................................Passed
flake8...................................................................Passed
flake8-pyx...............................................................Passed
flake8-pxd...............................................................Passed
isort....................................................................Passed
pyupgrade................................................................Passed
rst ``code`` is two backticks............................................Passed
Generate pip dependency from conda.......................................Passed
Strip unnecessary `# noqa`s..............................................Passed
Fix End of Files.........................................................Passed
```
"
717241738,36977,rename function with agg,Jacques2101,closed,2020-10-08T11:01:20Z,2020-10-08T18:10:47Z,"I just update to 1.1.3 and it seems that there is a problem (bug?) to rename function after an aggregate call. All the functions call are named <lambda> so I can't use rename as previously.  
How can I do to rename my function ? 
I cannot do anymore 
```
 .rename(columns={'<lambda_0>': 'function 0',
                  '<lambda_1>': 'function 1',....
```
(this is of course an example of functions names)

```python
import pandas as pd
pdf = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])
pdf.agg([""sum"", lambda df: df.sum(), lambda df: df.sum()])
```
#### Output
			A	B
	sum		12	27
	<lambda>	12	27
	<lambda>	12	27

#### Expected Output
			A	B
	sum		12	27
	<lambda_0>	12	27
	<lambda_1>	12	27


#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Mon Aug 31 22:12:52 PDT 2020; root:xnu-6153.141.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.0.post20201006
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: 0.9.0
bs4              : 4.9.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : None
tables           : None
tabulate         : 0.8.7
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : None
numba            : None
<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

"
705300167,36518,Fix grammatical errors,ghost,closed,2020-09-21T05:40:55Z,2020-10-08T19:49:28Z,This PR fixes grammatical errors.
716491585,36940,REGR: DateOffset attributes update fails with no errors.,lgelmi,closed,2020-10-07T12:38:06Z,2020-10-08T21:10:10Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.


#### Code Sample

```python
import pandas

base = pandas.Timestamp(""2020-01-01"")
offset = pandas.DateOffset(years=1)
print(base + offset)
>>> 2021-01-01 00:00:00
print(offset)
>>> <DateOffset: years=1>
offset.years = 5
print(base + offset)
>>> 2021-01-01 00:00:00
print(offset)
>>> <DateOffset: years=5>
```

#### Problem description

I need to dynamically change some of the DateOffset attributes and just setting them seems like the most intuitive thing to do. 
And it seems to work: no error is raised and the attribute value is updated.
When actually using it, though, the original value is used.

#### Expected Output

This is obviously due to the way the DateOffset is implemented, as a wrapper for cpython timedelta and dateutils relativedelta.
The ideal outcome for this would be having the object update itself, but I can understand why the class is ""static"".
The way it is, it may be better to inhibit attribute assignation instead of failing silently.

Event better would be having some method to generate a new DateOffset with just a couple of fields changed. 
Depending on the change I may even be able to make the PR myself, once the desired behavior is clarified. :D

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-47-generic
Version          : #51~18.04.1-Ubuntu SMP Sat Sep 5 14:35:50 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.1.2
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : None
pytest           : 6.1.1
hypothesis       : None
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
714351261,36860,CI: Continuous benchmarking,dsaxton,closed,2020-10-04T16:33:11Z,2020-10-08T21:13:31Z,"I think it would be helpful if pandas had a set of performance benchmarks to run automatically during every CI run (it looks like there is something for the asvs, but it seems they almost never actually run?). It would reduce some of the friction involved in manually running the asv suite and pasting results as comments, and also help prevent certain things from slipping by just not thinking to run benchmarks.

There exists the pytest plugin pytest-benchmark which seems even more lightweight than asv, and appears to be what RAPIDS uses for a benchmark tool of their own https://github.com/rapidsai/benchmark. Could it be an option to have a GitHub Action that runs pytest-benchmark on every PR (I don't know a lot about the plugin, but I am assuming it could be configured to either take a delta between master and the given branch, as well as possibly between that branch and some baseline commit on master such as a major release)? It may also be possible to cache the result from master somewhere to prevent it from being rerun every time.

What constitutes ""failure"" is another question, and maybe it's best to configure things to only warn on failure instead of making the whole run red (would help address flaky benchmarks as well). Also we would presumably need fine-grained control over the architecture used by GitHub to make sure it doesn't drift, and I'm not sure if that's possible."
714219716,36848,REF: IndexOpsMixin wrapping,jbrockmendel,closed,2020-10-04T02:54:10Z,2020-10-08T21:19:55Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
716994410,36967,TYP: consistent return types blocks,jbrockmendel,closed,2020-10-08T03:39:26Z,2020-10-08T21:21:14Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
716735838,36952,"CLN: dont special-case should_store, CategoricalBlock",jbrockmendel,closed,2020-10-07T17:55:18Z,2020-10-08T21:22:21Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Moving towards avoiding special-casing pandas-internal EA subclasses."
716960916,36966,CLN: remove unnecessary BoolBlock.replace,jbrockmendel,closed,2020-10-08T01:54:45Z,2020-10-08T21:23:19Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The fastpath it implements is equivalent to the can_hold_element fastpath in Block.replace"
716912794,36962,BUG/API: tighter checks on DTI/TDI.equals,jbrockmendel,closed,2020-10-07T23:21:20Z,2020-10-08T21:27:59Z,"- [x] closes #33531
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Also fixes `TimedeltaIndex(Categorical(tdi))` (which DatetimeIndex already gets right)"
717582655,36986,BUG: ExtensionArray `test_unstack` assumes missing values are nan,jrm5100,closed,2020-10-08T18:53:08Z,2020-10-08T22:01:44Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

Running pytest with this fixture:

```python
import random

import pytest
from pandas_genomics import GenotypeDtype, GenotypeArray, Variant

random.seed(1855)

@pytest.fixture
def data():
    """"""Length-100 array for this type.
    * data[0] and data[1] should both be non missing
    * data[0] and data[1] should not be equal
    """"""
    alleles = ['A', 'T', 'G']
    variant = Variant(variant_id='rs12345', chromosome='chr1', coordinate=123456, alleles=alleles)
    genotypes = [variant.make_genotype('A', 'T'), variant.make_genotype('T', 'T')]
    for i in range(98):
        genotypes.append(variant.make_genotype(random.choice(alleles), random.choice(alleles)))
    return GenotypeArray(values=genotypes)

class TestReshaping(base.BaseReshapingTests):
    pass
```

#### Problem description

I'm working on an ExtensionArray as part of a package called [pandas-genomics](https://github.com/HallLab/pandas-genomics).
I've almost got all of the ExtensionArray tests passing.  It isn't possible to pass `test_unstack` without altering the test itself.  For example, the `series-index1` test case fails this assertion:

```python
expected = ser.astype(object).unstack(level=level)
result = result.astype(object)

self.assert_frame_equal(result, expected)
```

In this case, `result` is

```
     A          B
a  A/T  <Missing>
b  T/T        T/T
```

and `expected` is

```
     A    B
a  A/T  NaN
b  T/T  T/T
```

`<Missing>` is the na_value for my ExtensionDtype.

I've confirmed that this is resolved by using fill_value to supply the na_value
```python
expected = ser.astype(object).unstack(level=level, fill_value=data.dtype.na_value)
```

#### Expected Output

Test passes

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 42 Stepping 7, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
717016815,36969,DOC: make rename docs consistent,erictleung,closed,2020-10-08T04:46:42Z,2020-10-08T22:08:33Z,"Other doc text describing code end in a colon.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
716708345,36946,REGR: Make DateOffset immutable,dsaxton,closed,2020-10-07T17:11:21Z,2020-10-08T22:22:49Z,"- [x] closes #36940
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I'm assuming this is a regression and not behavior we want?"
717682772,36993,CI: xfail intermittently-failing tests,jbrockmendel,closed,2020-10-08T21:38:27Z,2020-10-08T23:44:22Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Usually I avoid `strict=False` like the plague, but these are a PITA recently."
714858485,36889,BUG: RollingGroupby no longer respects sort being disabled,cpmbailey,closed,2020-10-05T13:48:19Z,2020-10-09T00:46:15Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
pd.DataFrame({'foo': [2,1], 'bar': [2, 1]}).groupby('foo', sort=False).rolling(1).min()
       foo  bar
foo
1   1  1.0  1.0
2   0  2.0  2.0

```

#### Problem description

RollingGroupby no longer respects sort being disabled

#### Expected Output

Behaviour in 1.0.5
```python
pd.DataFrame({'foo': [2,1], 'bar': [2, 1]}).groupby('foo', sort=False).rolling(1).min()
       foo  bar
foo
2   0  2.0  2.0
1   1  1.0  1.0
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.6.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.2
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 9.0.1
setuptools       : 50.3.0
Cython           : 0.29.21
pytest           : 6.0.2
hypothesis       : None
sphinx           : 1.7.9
blosc            : 1.7.0
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : 0.4.1
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : 0.3.3
scipy            : 1.5.2
sqlalchemy       : 1.2.12
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.46.0

</details>
"
698935810,36284,"DOC: Fixed type hints for GroupBy.{any, all}",ytakashina,closed,2020-09-11T07:08:35Z,2020-10-09T01:50:00Z,"The current documentation says the return value of GroupBy.all and GroupBy.any is `bool`.

```
    def any(self, skipna: bool = True):
        """"""
        Return True if any value in the group is truthful, else False.

        Parameters
        ----------
        skipna : bool, default True
            Flag to ignore nan values during truth testing.

        Returns
        -------
        bool
        """"""
```

However, the actual returned type is DataFrame or Series (with the same shape as `GroupBy.sum()`).

```
In [25]: df
Out[25]:
     0    1    2    3
0  1.0  0.0  0.0  0.0
1  0.0  1.0  0.0  0.0
2  0.0  0.0  1.0  0.0
3  0.0  0.0  0.0  1.0

In [26]: df.groupby(0).any()
Out[26]:
         1      2      3
0
0.0   True   True   True
1.0  False  False  False

In [27]: type(df.groupby(0).any())
Out[27]: pandas.core.frame.DataFrame
```

This PR fixes the wrong type hints for `GroupBy.{any, all}`.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
709388961,36651,QST: Error importing pandas,huyaox,closed,2020-09-26T01:16:31Z,2020-10-09T09:02:52Z,"hello!
when I import any module,this  is a misstake show""No module named'pandas._libs.join'

why?and how to resoled this?

thanks!"
717199189,36975,BUG: mask() and where() do not work pandas.core.arrays.boolean.BooleanDtype,DriesSchaumont,closed,2020-10-08T09:58:38Z,2020-10-09T09:39:54Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example
This does not work:
```python
>>> import pandas as pd
>>> index = pd.MultiIndex.from_arrays([['a', 'a', 'b'], ['c', 'd', 'e']], names=['level1', 'level2'])
>>> test = pd.DataFrame([[1, 2],[3, 4],[5, 6]], index=index, dtype=pd.UInt32Dtype())
>>> column_sums = test.sum(level='level1')
>>> to_remove = column_sums < 100
>>> test.mask(to_remove, inplace=True, level='level1', axis=0, other=pd.NA)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""./site-packages/pandas/core/generic.py"", line 9034, in mask
    return self.where(
  File ""./site-packages/pandas/core/generic.py"", line 9004, in where
    return self._where(
  File ""./site-packages/pandas/core/generic.py"", line 8854, in _where
    new_data = self._mgr.putmask(
  File ""./site-packages/pandas/core/internals/managers.py"", line 547, in putmask
    return self.apply(
  File ""./site-packages/pandas/core/internals/managers.py"", line 406, in apply
    applied = getattr(b, f)(**kwargs)
  File ""./site-packages/pandas/core/internals/blocks.py"", line 1606, in putmask
    mask = _extract_bool_array(mask)
  File ""./site-packages/pandas/core/internals/blocks.py"", line 2869, in _extract_bool_array
    assert mask.dtype == bool, mask.dtype
AssertionError: object
```
This does work:
```python
>>> import pandas as pd
>>> index = pd.MultiIndex.from_arrays([['a', 'a', 'b'], ['c', 'd', 'e']], names=['level1', 'level2'])
>>> test = pd.DataFrame([[1, 2],[3, 4],[5, 6]], index=index, dtype=pd.UInt32Dtype())
>>> column_sums = test.sum(level='level1')
>>> to_remove = (column_sums < 100).astype(bool)
>>> test.mask(to_remove, inplace=True, level='level1', axis=0, other=pd.NA)
>>> test.info()
<class 'pandas.core.frame.DataFrame'>
MultiIndex: 3 entries, ('a', 'c') to ('b', 'e')
Data columns (total 2 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   0       0 non-null      UInt32
 1   1       0 non-null      UInt32
dtypes: UInt32(2)
memory usage: 346.0+ bytes
```
#### Problem description
Using mask() or where() with the BooleanDtype as condition raises an AssertionError.

#### Expected Output
mask() and where() should  work with both the bool and the pandas.core.arrays.boolean.BooleanDtype dtype.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-42-generic
Version          : #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 44.0.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
</details>
"
716679123,36945,use-python-language-in-pip-to-conda,MarcoGorelli,closed,2020-10-07T16:28:49Z,2020-10-09T11:07:01Z,"xref #36531

This is to avoid problems where a dev's own system's Python points to Python2"
717667992,36992,Backport PR #36946 on branch 1.1.x (REGR: Make DateOffset immutable),meeseeksmachine,closed,2020-10-08T21:11:31Z,2020-10-09T11:20:34Z,Backport PR #36946: REGR: Make DateOffset immutable
717929524,37000,BUG: Pandas Dataframe is converting float variable to integer,AslanDevbrat,closed,2020-10-09T07:40:53Z,2020-10-09T11:39:24Z,"- [ Done ] I have checked that this issue has not already been reported.

- [ Done ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
     
    df_mle.at[words_of_bigram[0], words_of_bigram[1]] = (df_bigram.at[i,'BigramCount']/frequencies.at['Frequency',words_of_bigram[0]])


```

#### Problem description

When I assigned the answer of the division which is a decimal value ( like 0.5) then value stored in the df_mle variable is an integer (like 0).

#### Expected Output
print(df_mle.at[words_of_bigram[0], words_of_bigram[1]])
>>> 0.5

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
718071835,37002,Backport PR #36911:  BUG: RollingGroupby not respecting sort=False,simonjayhawkins,closed,2020-10-09T11:29:48Z,2020-10-09T12:19:52Z,Backport PR #36911
718085734,37003,Backport PR #36753 on branch 1.1.x: BUG: Segfault with string Index when using Rolling after Groupby,simonjayhawkins,closed,2020-10-09T11:54:20Z,2020-10-09T13:12:16Z,Backport PR #36753 on branch 1.1.x
716732044,36948,REGR: Dataframe.agg no longer accepts positional arguments as of v1.1.0,pganssle,closed,2020-10-07T17:49:13Z,2020-10-09T13:32:56Z,"Currently, passing any positional arguments to the `*args` parameter of `DataFrame.agg` fails with a `TypeError`, but the documented behavior is that positional and keyword arguments are passed on to the function that you are aggregating with. A minimal reproducer:

```python
import pandas as pd

def f(x, a):
    return x.sum() + a

df = pd.DataFrame([1, 2])

print(df.agg(f, 0, 3))
```

This is a regression introduced in v1.1.0, which was introduced in 433c9007781080658553fbef1a4d0c2813b404c0 (GH-34377), and it was mainly because the previous code only incidentally worked. Prior to the ""offending"" commit, [all `TypeErrors` were caught](https://github.com/pandas-dev/pandas/commit/433c9007781080658553fbef1a4d0c2813b404c0#diff-1e79abbbdd150d4771b91ea60a4e1cc7R7336-R7341) and `.agg` would fall back to `apply`. This was incidentally (as far as I can tell) catching the `TypeError` raised from the fact that the `self._aggregate` call passed `axis` as a keyword argument, so:

```python
self._aggregate(func, axis=axis, *args, **kwargs)

...

def aggregate(self, func, axis, *args, **kwargs):
   ...
```

This means that `aggregate` takes `func` and the first positional argument from `*args` as `func` and `axis`, then takes `axis=1` to be `axis`, raising `TypeError` because `axis` is specified twice.

I believe the solution here is to pass `axis` to `self._aggregate` by position."
716734654,36950,REGR: Allow positional arguments in DataFrame.agg,pganssle,closed,2020-10-07T17:53:27Z,2020-10-09T13:33:42Z,"Although `DataFrame.agg` is documented as accepting `func, axis, *args, **kwargs` with `*args` and `**kwargs` passed to `func`, passing positional arguments raises a TypeError.

The reason for this is that the internal call to `self._aggregate` uses a keyword argument (axis) before passing *args and `**kwargs`, and as such the first positional argument is always interpreted as a second specification of `axis`, which raises TypeError.

Prior to commit 433c9007781080658553fbef1a4d0c2813b404c0, TypeErrors were being suppressed, falling back to `self.apply`, which in v1.1.0 turned into an error.

This fixes issue GH-36948.

- [x] closes #36948
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
717125672,36974,BUG: InvalidIndexError when doing fillna with index that is repeated in df but not in fillna argument,pablobd,open,2020-10-08T08:13:58Z,2020-10-09T13:55:24Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

I get an InvalidIndexError on pandas 1.0.3 when doing fillna to a DataFrame with repeated indexes and using a DataFrame with the same index but without repetitions. I've also tested this use case in pandas 1.1.3 (latest as today)

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame({""a"": 100*(list(range(10))+[None,None,None,None])}).sample(1000, random_state=12).set_index(pd.Series(list(range(100)) * 10, name = ""my_index""))
df_fillna = pd.DataFrame({""a"": list(range(100))}, index=list(range(100)))
df.fillna(df_fillna)
```

I expect to get a DF with each NaN with index i (there could be more than one) replaced with the unique value in df_fillna with index i but I get an InvalidIndexError.

#### Problem description

The output is an InvalidIndexError.

#### Expected Output

I would expect to replace each NaN with index i (there could be more than one) with the unique value in df_fillna with index i. So, the same as I get when doing this:

```python
df = pd.DataFrame({""a"": 100*(list(range(10))+[None,None,None,None])}).sample(1000, random_state=12).set_index(pd.Series(list(range(100)) * 10, name = ""my_index""))
df_fillna = pd.DataFrame({""a"": list(range(100))*10}, index=list(range(100))*10)
df.fillna(df_fillna)
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.0.3
numpy            : 1.18.0
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 39.1.0
Cython           : 0.29.21
pytest           : 6.0.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : 0.9.3
psycopg2         : 2.8.2 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 6.0.2
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.2.16
tables           : None
tabulate         : 0.8.3
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.51.2

</details>
"
718151574,37005,Backport PR #36950 on branch 1.1.x (REGR: Allow positional arguments in DataFrame.agg),meeseeksmachine,closed,2020-10-09T13:35:24Z,2020-10-09T14:35:07Z,Backport PR #36950: REGR: Allow positional arguments in DataFrame.agg
718091143,37004,CI: isort fixup on 1.1.x,simonjayhawkins,closed,2020-10-09T12:03:56Z,2020-10-09T14:36:31Z,isort-5.6.1
717586761,36987,Fix test_unstack,jrm5100,closed,2020-10-08T18:58:09Z,2020-10-09T15:46:31Z,"The test was failing if the ExtensionDtype had an na_value that wasn't equivalent to nan

- [x] closes #36986 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
473437178,27613,assert_series_equal doesn't distinguish between different NA values,jbrockmendel,closed,2019-07-26T16:21:22Z,2020-10-09T15:47:12Z,"The following, based on tests.indexing.test_datetime.TestDatetimeIndex.test_indexing_with_datetime_tz passes, I think incorrectly

```
now = pd.Timestamp.now()
left = pd.Series([now, pd.NaT, pd.NaT], dtype=object)
right = pd.Series([now, np.nan, np.nan], dtype=object)

tm.assert_series_equal(left, right)
```
"
468792216,27421,CI: Compatibility with pytest-azurepipelines,TomAugspurger,closed,2019-07-16T18:08:04Z,2020-10-09T18:26:47Z,"https://github.com/pandas-dev/pandas/pull/27416#issuecomment-511922995

Failing on the prerelease at https://pypi.org/project/pytest-azurepipelines/1.0.0a1/"
684531732,35869,BUG: Groupby rolling count with datetime not working correctly,nrcjea001,closed,2020-08-24T09:54:44Z,2020-10-09T20:02:09Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame({""column1"": range(6), 
                   ""column2"": range(6), 
                   'group': 3*['A','B'], 
                   'date':pd.date_range(end=""20190101"", periods=6)})

## Without Closed Parameter
df.groupby('group').rolling('3d',on='date')['column1'].count()
## Output
group   
A      0    1.0
       2    2.0
       4    3.0
B      1    1.0
       3    2.0
       5    3.0
Name: column1, dtype: float64

## With Closed Parameter
df.groupby('group').rolling('3d',on='date',closed='left')['column1'].count()
## Output
ValueError: closed only implemented for datetimelike and offset based windows
```

#### Problem description

In Pandas 1.1.1, although .sum() and .mean() are working correctly, the rolling groupby .count() is not working correctly with datetime. Furthermore, when closed parameter is added, ValueError is produced.

#### Expected Output
```
## Without Closed Parameter
group   
A      0    1.0
       2    2.0
       4    2.0
B      1    1.0
       3    2.0
       5    2.0
Name: column1, dtype: float64
## With Closed Parameter
group   
A      0    NaN
       2    1.0
       4    1.0
B      1    NaN
       3    1.0
       5    1.0
Name: column1, dtype: float64
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 11, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.1.1
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 49.2.1.post20200802
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : 0.8.0
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.48.0

</details>
"
716193651,36934,TST: RollingGroupby.count with closed specified,mroeschke,closed,2020-10-07T04:51:34Z,2020-10-09T20:05:17Z,"- [x] closes #35869
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Closed by #31302
"
293539376,19492,CLN: Standardize values coercion during Block initialization ,TomAugspurger,closed,2018-02-01T13:57:32Z,2020-10-09T21:13:01Z,"Splitting off from https://github.com/pandas-dev/pandas/pull/19268 (which is starting to give me unicorns fairly often)

After that, all blocks eventually end up calling `Block.__init__` (aside from `ScalarBlock`, which we'll ignore).

Most of the subclasses init methods are

1. maybe coerce values to the expected dtype
2. `call super().__init__`

It'd be nice to put 1 into favor of a `_maybe_coerce_values` method defined by each block, and remove all the subclasses init methods.

The one sticking point will be `DatetimeTZBlock.__init__`, which accepts a `dtype` parameter that no other block does.

https://github.com/pandas-dev/pandas/blob/35812eaaecebeeee0ddf07dee4b583c4eea07785/pandas/core/internals.py#L2582-L2590

We could *maybe* push that onto the callers, and then clean things up."
718283605,37009,CLN: standardize values coercion in Blocks,jbrockmendel,closed,2020-10-09T16:39:36Z,2020-10-09T21:25:54Z,"- [x] closes #19492
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
703800131,36432,PERF: construct DataFrame with string array and dtype=str,topper-123,closed,2020-09-17T18:06:58Z,2020-09-19T20:06:51Z,"Avoid inefficient call to `arr.astype()` when dtype is `str`, and use ensure_string_array instead.

Performance example:

```python
>>> x = np.array([str(u) for u in range(1_000_000)], dtype=object).reshape(500_000, 2)
>>> %timeit pd.DataFrame(x, dtype=str)
391 ms ± 17.7 ms per loop  # master
11.9 ms ± 131 µs per loop  # after this PR
```

xref #35519, #36304 & #36317.
"
700633573,36343,BUG: ZERO WIDTH NO-BREAK SPACE in column name causes a reading failure,omarbelaggoun,closed,2020-09-13T19:39:21Z,2020-09-19T20:14:06Z,"
- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

[test excel.xlsx](https://github.com/pandas-dev/pandas/files/5214984/test.excel.xlsx)

```python
# Your code here
import pandas as pd
df=pd.read_excel('test excel.xlsx')
df

```

#### Problem description

I have a customer that is sending me a file that has ZERO WIDTH NO-BREAK SPACE
only the first header reads unless the character is deleted 
I created a repro by copying the header from the original file to the new one in excel

#### Expected Output

``` 
Building Park Name	Building Name	New / Renewal	Address	City	State	Class 
```

#### Output of ``pd.show_versions()``



<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 41.2.0
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : None
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.12
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.8
</details>
"
307484543,20445,"pd.to_datetime, unit='s'  much slower for float64 than for int64",bchu,closed,2018-03-22T02:32:44Z,2020-09-19T20:24:19Z,"Calling `pd.to_datetime` with the `unit='s'` kwarg appears to be 1000x slower for float64 than for int64. There does not appear to be a difference in performance between the two types if the timestamps are first converted to nanoseconds and no unit is specified.

```python
timestamp_seconds_int = pd.Series(np.random.randint(1521685107 - 604800, 1521685107, 1000000, dtype='int64'))
timestamp_seconds_float = timestamp_seconds_int.astype('float64')
```

```python
%%timeit -r 3
pd.to_datetime(timestamp_seconds_int, unit='s')
Output: 12.4 ms ± 1.66 ms per loop (mean ± std. dev. of 3 runs, 100 loops each)
```

```python
%%timeit -r 3
pd.to_datetime(timestamp_seconds_float, unit='s')
Output: 6.88 s ± 138 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.4.final.0
python-bits: 64
OS: Darwin
OS-release: 17.4.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.0.dev0+658.g17c1fadb0
pytest: 3.0.6
pip: 9.0.3
setuptools: 38.5.2
Cython: 0.28.1
numpy: 1.14.1
scipy: 1.0.0
pyarrow: 0.8.0
xarray: 0.10.0
IPython: 6.2.1
sphinx: None
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2016.10
blosc: None
bottleneck: None
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 2.1.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.9999999
sqlalchemy: 1.1.10
pymysql: None
psycopg2: 2.7.1 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
703741796,36431,BUG: get_indexer returned dtype,alexhlim,closed,2020-09-17T16:35:37Z,2020-09-19T20:24:21Z,"- [x] closes #36359
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Changed the return dtype of IndexEngine’s `get_indexer()` and `get_indexer_non_unique()`. I decided to not include tests because dtypes are already implicitly tested in the following:

- pandas/tests/indexes/test_base.py (TestIndex.test_get_indexer)
- pandas/tests/base/test_misc.py (test_get_indexer_non_unique_dtype_mismatch)

Also, I ran the asv benchmarks:

```
asv continuous -f 1.1 upstream/master HEAD -b ^indexing_engines
BENCHMARKS NOT SIGNIFICANTLY CHANGED.
```
"
646613609,35027,"PERF: pd.to_datetime, unit='s' much slower for float64 than for int64",arw2019,closed,2020-06-27T05:33:39Z,2020-09-19T20:27:06Z,"- [x] closes #20445
- [x] tests passed
- [x] new benchmarks added
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

As per discussion in #20445 this PR addresses performance of `to_datetime` on a uniform type array of floats. The aim is to get a speed-up by implementing `astype`-ing, and avoid looping, for floats."
656157107,35267,BUG: Series.equals fails when comparing numpy arrays to scalars,avinashpancham,closed,2020-07-13T21:29:19Z,2020-09-19T20:29:43Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

arr = np.array([1, 2])
s1 = pd.Series([arr, arr])
s2 = s1.copy()

s1[1] = 9
s1.equals(s2) # This throws a ValueError

ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()

```

#### Problem description

Series.equals operation fails when comparing two Series that at the same position respectively have a single value and a list. Pandas then throws a ValueError since it cannot compare a single value to a list.

Related issues #20676 and #35237 
 
#### Expected Output
False, since the two series are not equal


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 9827ce00f95927ddf6a0565450bc175879454d4e
python           : 3.8.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.5.0
Version          : Darwin Kernel Version 19.5.0: Tue May 26 20:41:44 PDT 2020; root:xnu-6153.121.2~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.1.0.dev0+2088.g9827ce00f
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200712
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : 5.19.1
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : 0.4.0
gcsfs            : 0.6.2
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.0
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.1

</details>
"
694369857,36161,BUG: Enable Series.equals to compare numpy arrays to scalars,avinashpancham,closed,2020-09-06T12:50:08Z,2020-09-19T20:29:49Z,"- [x] closes #35267
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
558996209,31609,_check_for_bom at parsers.py remove all the fields behind BOM,limitspro,closed,2020-02-03T10:42:54Z,2020-09-19T22:02:56Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd # version 0.23.0, the same as 1.0.0
df=pd.read_csv(""1.csv"",sep=r"","") # 1.csv treat first row as header, which start with BOM due to some unknow reason.
print(df.columns)
```
#### Problem description
_check_for_bom will not just remove BOM, but also remove all the fields except for the first one.
due to code:
```
first_row = first_row[0] // line 2571 at parsers.py @0.23.0
first_row_bom = first_row[0] // line 2758 at parsers.py @latest code at github

```
Solution:
Change line 2775 at parsers.py @latest code at github to 
```
return [first_row_bom[1:]]+first_row[1:]
```
"
674827928,35596,How to stop a rolling window at nan values and continue after it? ,ghost,closed,2020-08-07T07:40:36Z,2020-09-19T22:04:00Z,"Posted the same question on stackoverflow. A user there said I should open a issue here on the github page, since it is a bug. 

 I have the following dataframe:

```
     df = pd.DataFrame([[0, 1, 2, 4, np.nan, np.nan, np.nan],
                   [0, 1, 2 ,np.nan, np.nan, np.nan,np.nan],
                   [0, 2, 2 ,np.nan, 2, np.nan,1]])
```
 
With output:

```
       0  1  2    3    4   5   6
    0  0  1  2  4.0  NaN NaN NaN
    1  0  1  2  NaN  NaN NaN NaN
    2  0  2  2  NaN  2.0 NaN 1.0
```

with dtypes:
`
        df.dtypes`

```
    0      int64
    1      int64
    2      int64
    3    float64
    4    float64
    5    float64
    6    float64
    dtype: object
```

Then the underneath rolling summation is applied:

    df.rolling(window = 7, min_periods =1, axis = 'columns').sum()

And the output is as follows:

```
         0    1    2    3    4    5    6
    0  0.0  1.0  3.0  4.0  4.0  4.0  4.0
    1  0.0  1.0  3.0  NaN  NaN  NaN  NaN
    2  0.0  2.0  4.0  NaN  2.0  2.0  3.0
```

I notice that the rolling window stops and starts again whenever the `dtype` of the next column is different. 

I however have a dataframe whereby all columns are of the same `object` type. 
`
`    df = df.astype('object')``

which has output:

```
         0    1    2    3    4    5    6
    0  0.0  1.0  3.0  7.0  7.0  7.0  7.0
    1  0.0  1.0  3.0  3.0  3.0  3.0  3.0
    2  0.0  2.0  4.0  4.0  6.0  6.0  7.0
```

My desired output however, stops and starts again after a `nan` value appears. This would look like:
```

         0    1    2    3    4    5    6
    0  0.0  1.0  3.0  7.0  NaN  NaN  NaN
    1  0.0  1.0  3.0  NaN  NaN  NaN  NaN
    2  0.0  2.0  4.0  NaN  2.0  NaN  3.0
```

I figured there must be a way that NaN values are not considered but also not filled in with values obtained from the rolling window. 

Anything would help!
"
313102805,20649,pandas.DataFrame.rolling produces incorrect value if integer and float objects are mixed,mustafaburny,closed,2018-04-10T21:53:25Z,2020-09-19T22:04:00Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

# Generate square dataframe of ones
df = pd.DataFrame(1, index=[1,2,3,4,5], columns=['a','b','c','d','e'])

# Make last column a series of type ""float"" instead of type ""int""
df['e'] = 1.0

# Compute rolling sum across rows
print(df.rolling(window=2, min_periods=1, axis=1).sum())

# Output. Note that the last column is incorrect.
# -----------------------------------------------
#      a    b    c    d    e
# 1  1.0  2.0  2.0  2.0  1.0
# 2  1.0  2.0  2.0  2.0  1.0
# 3  1.0  2.0  2.0  2.0  1.0
# 4  1.0  2.0  2.0  2.0  1.0
# 5  1.0  2.0  2.0  2.0  1.0
```
#### Problem description

In the above example, the last column of `df` was coerced into a `float` Series. Calculating a rolling sum of `df` produces an incorrect result for that last column. Incidentally, the correct result is produced by coercing the entire dataframe to a `float` object before computing the rolling sum.

#### Expected Output

```python
# Compute same rolling sum across rows, except convert all values to type ""float"" first
print(df.astype('float').rolling(window=2, min_periods=1, axis=1).sum())

# Output:
# -------
#      a    b    c    d    e
# 1  1.0  2.0  2.0  2.0  2.0
# 2  1.0  2.0  2.0  2.0  2.0
# 3  1.0  2.0  2.0  2.0  2.0
# 4  1.0  2.0  2.0  2.0  2.0
# 5  1.0  2.0  2.0  2.0  2.0
```

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit: None
python: 3.6.4.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.22.0
pytest: 3.3.2
pip: 9.0.1
setuptools: 38.4.0
Cython: 0.27.3
numpy: 1.14.0
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: 1.6.6
patsy: 0.5.0
dateutil: 2.6.1
pytz: 2017.3
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 2.1.2
openpyxl: 2.4.10
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.2
lxml: 4.1.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.1
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
624006014,34360,"ENH: Correct use of `longtable`, `label`, and `caption` with LaTeX",jdossgollin,closed,2020-05-25T02:31:13Z,2020-09-19T22:05:12Z,"#### Is your feature request related to a problem?

Currently `DataFrame.to_latex()` has a `longtable` argument, which writes the DataFrame to latex using the syntax of the longtable package. When used with the `label` and `caption` arguments, it writes a latex table with all of these features.

However, the caption and label are both placed between a `\begin{longtable}` line and `\endhead`. As a consequence, the table appears twice in the `listoftables` and leads to warnings about the reference being multiply defined

#### Describe the solution you'd like

the `.to_latex()` method when `longtable=True` and `caption=X` and `label=Y` should print out as follows

```latex
\begin{longtable}{...}% alignment characters
\caption{Caption here}\label{label-here}
\endfirsthead
\caption{Caption here}
\endhead
```

This will prevent a separate label from being defined multiple times in latex (once for each page the caption is printed on). See page 5 of http://www.texdoc.net/texmf-dist/doc/latex/tools/longtable.pdf.

#### API breaking implications

None

#### Describe alternatives you've considered

An alternative is to set the caption and label to False, and to add them myself. This works but defeats the purpose of calling the `.to_latex()` API"
699804238,36297,BUG: fix duplicate entries in LaTeX List of Tables when using longtable environments,jeschwar,closed,2020-09-11T22:15:58Z,2020-09-19T22:05:15Z,"- [x] closes #34360
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
704610924,36458,[BUG]: Rolling.sum() calculated wrong values when axis is one and dtypes are mixed,phofl,closed,2020-09-18T19:39:19Z,2020-09-19T22:17:54Z,"- [x] closes #20649
- [x] closes #35596
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

In case of ``axis=1`` and mixed dtypes, the ``_apply_blockwise`` did not calculate the sum for a complete row. Through converting the obj and consolidating the blocks we can avoid this."
704996889,36483,REF: share IntervalArray._validate_foo,jbrockmendel,closed,2020-09-19T20:56:14Z,2020-09-19T22:56:37Z,
699705555,36295,CLN: Remove trailing commas,cjlynch278,closed,2020-09-11T20:11:48Z,2020-09-20T01:17:40Z,"Working on: CLN remove unnecessary trailing commas to get ready for new version of black #35925.
Updated the following files: 
pandas/tests/series/methods/test_interpolate.py
pandas/tests/series/methods/test_unstack.py
pandas/tests/series/test_cumulative.py
pandas/tests/test_algos.py


- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
705025706,36487,Don't unlabel stale PR on update,dsaxton,closed,2020-09-20T01:25:08Z,2020-09-20T02:49:13Z,"Apparently the bot unlabels on any activity (not just activity from the owner), e.g., https://github.com/pandas-dev/pandas/pull/34584#issuecomment-693107104, so if you ask the person for an update and they don't respond, the PR is no longer stale. Probably better to do this manually for now."
705008938,36485,TYP: core.missing; PERF for needs_i8_conversion,jbrockmendel,closed,2020-09-19T22:36:42Z,2020-09-20T02:59:28Z,"Split off from a branch that shares `fillna` code between some of our EAs.

Preliminary to a PR that allows for an actually-inplace fillna implementation."
701299812,36365,BUG: Python Parser skipping over items if BOM present in first element of header,asishm,closed,2020-09-14T17:44:48Z,2020-09-20T03:35:30Z,"- [x] closes #36343
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
704454740,36452,ASV: added benchamark tests for DataFrame.to_numpy() and .values,hardikpnsp,closed,2020-09-18T15:08:47Z,2020-09-20T03:48:42Z,"- [x] closes #35023
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Adds benchmarks with single dtype and mixed frames of varying sizes.
xref #34999"
705039167,36489,Commas chaged,Noahlq,closed,2020-09-20T03:49:21Z,2020-09-20T03:52:17Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
704830831,36472,BUG: Regex parameter from replace function doesn't work on string dtype,GYHHAHA,closed,2020-09-19T07:57:51Z,2020-09-20T04:47:44Z,">>>import pandas as pd
>>>print(pd.__version__)
1.1.1
>>>s1 = pd.Series(['a','b'], dtype='string')
>>>s2 = pd.Series(['a','b'], dtype='object')
>>>s1.replace(r'[a]',pd.NA,regex=True)
0    a
1    b
dtype: string
>>>s2.replace(r'[a]',pd.NA,regex=True)
0    <NA>
1       b
dtype: object

Hi, it seems that regex doesn't function well for the replace method on string dtype, but normal on object dtype.

Besides, I also find setting pd.NA as repl of str.replace method will raise error, both on string and object.
Yet I think it may be better not to raise this error because the pd.NA indeed could be a missing string value.

>>>pd.Series(['a','b'],dtype='string').str.replace('a',pd.NA)
TypeError: repl must be a string or callable
>>>pd.Series(['a','b'],dtype='object').str.replace('a',pd.NA)
TypeError: repl must be a string or callable

Thanks!"
705045382,36492,BUG: pd.NA in str.replace raises error,GYHHAHA,closed,2020-09-20T04:55:11Z,2020-09-20T06:20:52Z,">>>pd.__version__
1.1.1

I find setting pd.NA as repl of str.replace method will raise error, both on string and object.
Yet I think it may be better not to raise this error because the pd.NA indeed could be a missing string value.

>>>pd.Series(['a','b'],dtype='string').str.replace('a',pd.NA)
TypeError: repl must be a string or callable
>>>pd.Series(['a','b'],dtype='object').str.replace('a',pd.NA)
TypeError: repl must be a string or callable

And also, pd.NA can't be the pat of str.replace. Maybe reasonable to allow this.

>>>pd.Series([pd.NA,'a'],dtype='string').str.replace(pd.NA,'1')
TypeError: object of type 'NAType' has no len()"
705096485,36497,PERF: DataFrame initialisation with dict with single key/value pair,topper-123,closed,2020-09-20T11:52:01Z,2020-09-20T12:02:12Z,"Faster initialisation of DataFrames, when the data is a single-item dict:

```python
>>> x = np.arange(1_000_000)
>>> %timeit pd.DataFrame(dict(a=x))
2.16 ms ± 66.7 µs per loop  # master
282 µs ± 4.63 µs per loop  # this PR
>>> x = np.array([str(u) for u in range(1_000_000)], dtype=object)
>>> %timeit pd.DataFrame(dict(a=x))
29.5 ms ± 131 µs per loop  # master
12.6 ms ± 55.8 µs per loop  # this PR
```

I don't think it's possible to optimize the case with more than 1 key/value pair, as concatenation will always take some time to perform."
553760059,31220,Index.sort_values puts missing values at the start with ascending=False,TomAugspurger,closed,2020-01-22T19:56:25Z,2020-09-20T20:47:21Z,"#### Code Sample, a copy-pastable example if possible

Index.sort_values places missing values at the start of the result with `ascending=False`

```python
In [4]: pd.Index([1, np.nan, 0]).sort_values(ascending=False)
Out[4]: Float64Index([nan, 1.0, 0.0], dtype='float64')

In [5]: pd.Series([1, np.nan, 0]).sort_values(ascending=False)
Out[5]:
0    1.0
2    0.0
1    NaN
dtype: float64
```
#### Problem description

This differs from Series.sort_values, which always leaves NA values at the end.

#### Expected Output

```python
Out[4]: Float64Index([1.0, 0.0, nan], dtype='float64')
```"
75153674,10100,feature request: Support for 'named' lambda functions in DataFrame.agg([]),jkokorian,closed,2015-05-11T09:32:01Z,2020-09-20T22:07:44Z,"I often have the situation where I would like to apply multiple aggregation functions to all the columns of a grouped dataframe, like:

``` python
grouped = df.groupby('somekey')
dfAggregated = grouped.agg([np.mean, np.std])
```

That works well, but sometimes (all the time, actually) I would also like to be able to use lambda functions this way, like:

``` python
grouped = df.groupby('somekey')
dfAggregated = grouped.agg([np.mean, np.std, lambda v: v.mean()/v.max()])
```

This works fine, but the resulting column name will now be 'lambda', which is ugly. This can be resolved by using the much more verbose syntax where you specify a dictionary for every column separately, but I would propose to allow the following syntax:

``` python
grouped = df.groupby('somekey')
dfAggregated = grouped.agg([np.mean,np.std,{'normalized_mean': lambda v: v.mean()/v.max()}])
```

The dictionary key should then be used as the resulting column name.

Interestingly, using this syntax in the version 0.16 does not produce an error, but produces a column named 'Nan', that is filled with tupple values: ('n','o','r','m','a','l','i','z','e','d','_','m','e','a','n'), which I don't think is of use to anyone:)
"
705154323,36502,BUG: to_numeric casts floats incorrectly,dcsaba89,closed,2020-09-20T17:54:30Z,2020-09-20T23:46:53Z,"- [ X ] I have checked that this issue has not already been reported.

- [ X ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame({'A': ['0.09', '0.63', '0.121909', '0.117863']})
df['B'] = pd.to_numeric(df['A'])
df['C'] = df['A'].astype(float)
```

#### Problem description
When floats as strings are passed to a column of a DataFrame, pd.to_numeric casts some of the given strings incorrectly.

In the above case df['C'] and df['A'] should have exactly the same values, but they differ in some decimals:

df['B'] = (0, 0.09) (1, 0.63) (2, 0.12190899999999999) (3, 0.11786300000000001)
df['C'] = (0, 0.09) (1, 0.63) (2, 0.121909) (3, 0.117863)

df['C'] is correct but df['B'] is incorrect.

The above issue does not occur when the same input is passed as list 
w = pd.to_numeric(['0.09', '0.63', '0.121909', '0.117863']) 

[0.09     0.63     0.121909 0.117863]

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.8.5.final.0
python-bits      : 32
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.1.2

</details>
"
593600088,33272,HUH?: DataFrame[categorical].replace,jbrockmendel,closed,2020-04-03T19:42:10Z,2020-09-21T02:01:49Z,"When we have a DataFrame with 2 Categorical columns with matching dtypes and do a `.replace` on that DataFrame, the results do not match what we would get if we operate column-wise.  Instead the results we get look like what we'd get if we stacked the columns, replaced, then unstacked.

i.e. AFAICT we are doing gymnastics to behave as if CategoricalBlock supported 2D Categorical.

From tests.frame.methods.test_replace:

```
replace_dict = {'a': 1, 'b': 2}
value = 3
df = pd.DataFrame([[1, 1], [2, 2]], columns=[""a"", ""b""], dtype=""category"")

final_data = [[3, 1], [2, 3]]
expected = pd.DataFrame(final_data, columns=[""a"", ""b""], dtype=""category"")
expected[""a""] = expected[""a""].cat.set_categories([1, 2, 3])
expected[""b""] = expected[""b""].cat.set_categories([1, 2, 3])

result = df.replace(replace_dict, value)
tm.assert_frame_equal(result, expected)

# this part is no longer from the test
col_wise = {col: df[col].replace(replace_dict[col], value) for col in df.columns}
result2 = pd.DataFrame(col_wise)

result2[""a""].dtype
CategoricalDtype(categories=[3, 2], ordered=False)
```
"
705253577,36515,Update outdated instructions in scripts/generate_pip_deps_from_conda.py,sm1899,closed,2020-09-21T03:14:37Z,2020-09-21T04:28:19Z,
704445505,36451,BUG: conversion of float32 to string shows too much precision,jorisvandenbossche,closed,2020-09-18T14:55:41Z,2020-09-21T06:47:19Z,"On master (but also on 1.1):

```
In [4]: pd.Series([0.1], dtype=""float64"").astype(""string"")
Out[4]: 
0    0.1
dtype: string

In [5]: pd.Series([0.1], dtype=""float32"").astype(""string"")
Out[5]: 
0    0.10000000149011612
dtype: string
```

When converting to the object-dtype string, it works as expected:

```
In [6]: pd.Series([0.1], dtype=""float64"").astype(""str"")
Out[6]: 
0    0.1
dtype: object

In [7]: pd.Series([0.1], dtype=""float32"").astype(""str"")
Out[7]: 
0    0.1
dtype: object
```

cc @topper-123 "
462214792,27108,API: Add NDFrame property to disallow duplicates,TomAugspurger,closed,2019-06-28T21:31:45Z,2020-09-21T11:34:08Z,"edit: see https://github.com/pandas-dev/pandas/issues/27108#issuecomment-527633246

---

I'd like to be able to have an index, and ensure that no operation introduces duplicates.

```python
idx = pd.Index(..., allow_duplicates=False)
s = pd.Series(..., index=idx)
```

From here, any pandas operation that introduces duplicates (e.g. `s.loc[['a', 'a']]`) would raise, rather than return an Index with two values."
579046891,32614,Negated str.isnumeric() fails on column with None when called together with notnull() in the same boolean expression,Khris777,closed,2020-03-11T06:52:27Z,2020-09-21T12:03:53Z,"#### Code Sample

Create a dataframe:
```
import pandas as pd
df = pd.DataFrame({'A': ['0', '1', '2', 'X', None]})
```

Obviously this fails:
```
print(df[df.A.str.isnumeric()])
```

So I can do this and it works:
```
print(df[df.A.notnull() & df.A.str.isnumeric()])
```

However when negating the `str.isnumeric()` with unary `~` it fails:
```
print(df[df.A.notnull() & ~df.A.str.isnumeric()])
```
Error message:
>TypeError: bad operand type for unary ~: 'NoneType'

Full error:

<details>

> print(df[df.A.notnull() & ~df.A.str.isnumeric()])
> Traceback (most recent call last):
> 
>   File ""<ipython-input-32-9d35109941d7>"", line 1, in <module>
>     df[df.A.notnull() & ~df.A.str.isnumeric()]
> 
>   File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\generic.py"", line 1473, in __invert__
>     new_data = self._data.apply(operator.invert)
> 
>   File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\internals\managers.py"", line 440, in apply
>     applied = b.apply(f, **kwargs)
> 
>   File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\internals\blocks.py"", line 390, in apply
>     result = func(self.values, **kwargs)
> 
> TypeError: bad operand type for unary ~: 'NoneType'

</details>

When doing it in two step it works:
```
dft = df[df.A.notnull()].copy()
print(dft[~dft.A.str.isnumeric()])
```

#### Problem description

It's inconsistent for the combined expression to fail because of the unary `~`.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 79 Stepping 1, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0.post20200308
Cython           : 0.29.15
pytest           : None
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : 0.3.3
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.0
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : None
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.8
numba            : 0.48.0

</details>
"
704724868,36464,BUG: Fix astype from float32 to string,dsaxton,closed,2020-09-19T00:43:45Z,2020-09-21T12:18:46Z,"- [x] closes #36451
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

It seems both bugs were caused by the same behavior from numpy:

```python
[ins] In [1]: import numpy as np

[ins] In [2]: arr = np.array([0.1], dtype=np.float32)

[ins] In [3]: arr
Out[3]: array([0.1], dtype=float32)

[ins] In [4]: np.asarray(arr, dtype=""object"")
Out[4]: array([0.10000000149011612], dtype=object)
```"
705383710,36521,BUG: pandas.DataFrame.where doesn't work when cond is of Pandas Dtype,JeremyVriens,closed,2020-09-21T08:09:50Z,2020-09-21T13:51:23Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame()
df[""input_col""] = [""a"", ""b"", ""c""]
df[""is_valid_row""] = pd.Series([1, 1, 0], dtype=""Int64"")
df[""output_col""] = df[[""input_col""]].where(df[""is_valid_row""] == 1)  # AssertionError is thrown here
```

#### Problem description

The code snippet is simplified to show that when using pandas.DataFrame.where in combination with a Pandas Dtype (Int64) an AssertionError is raised.
Note that when df[""input_col""] (= pandas.Series.where) or dtype=""int64"" (instead of ""Int64"") is used, the code behaves as expected.

This behaviour is broken since Pandas >= 1.1 (still an issue in 1.1.2).

#### Expected Output

output_col
a
b
NaN

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.8.2.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 85 Stepping 4, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252
pandas           : 1.1.2
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 46.2.0
Cython           : None
pytest           : 6.0.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.3.3
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.19
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
647711740,35057,ENH: Distinguish widths argument in read_fwf,erfannariman,closed,2020-06-29T22:26:21Z,2020-09-21T16:29:59Z,"- [x] closes #34953
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
705239726,36513,BUG:  Series.replace does not work as intended when replacing values with None,zzztpppp,closed,2020-09-21T02:15:15Z,2020-09-21T16:33:26Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
>>> pd.Series([1, 2,2 ,3, 3])
>>> s.replace(2, None)
0    1
1    1
2    1
3    3
4    3
dtype: int64



```

#### Problem description

This should replace all the 2's with None. 

#### Expected Output
```
0    1.0
1    NaN
2    NaN
3    3.0
4    3.0
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-957.el7.x86_64
Version          : #1 SMP Thu Nov 8 23:39:32 UTC 2018
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.1.0.post20200710
Cython           : 0.29.20
pytest           : 5.4.3
hypothesis       : 5.18.3
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.1
html5lib         : 1.1
pymysql          : 0.10.1
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.1

</details>
"
339127498,21784,Clearly document what a list-like is,toobaz,open,2018-07-07T08:09:36Z,2020-09-21T17:26:12Z,"I think we need a doc section which states what is accepted as list-like: each method which accepts list-likes would then link to that doc section.

This will basically detail the types for which ``pandas.core.dtypes.common.is_list_like`` returns ``True``, with the only exception of ``tuple``, which we should _not_ declare as list-like, but which some methods still accept for backward compatibility.
"
545233917,30673,REF: EA value_counts -> _value_counts,jbrockmendel,closed,2020-01-04T03:15:35Z,2020-09-21T18:21:36Z,"Instead of returning a Series, return a tuple with the index and values to be passed to Series.

Where possible I've changed the methods to use `_values_for_factorized` in the hopes of converging on a base class implementation.  This is proving elusive, suggestions welcome.  cc @TomAugspurger @jorisvandenbossche 

xref #22843, #23074."
703431738,36422,CLN: infer_dtype has no skipna argument,erfannariman,closed,2020-09-17T09:42:55Z,2020-09-21T18:26:55Z,"infer_dtype function in `lib.py` has no `skipna` argument, although it is documented in the docstring:

```python
def infer_dtype(foo=None, bar=None): # real signature unknown; restored from __doc__
    """"""
    Efficiently infer the type of a passed val, or list-like
        array of values. Return a string describing the type.
    
        Parameters
        ----------
        value : scalar, list, ndarray, or pandas type
        skipna : bool, default True
            Ignore NaN values when inferring the type.
    
        Returns
        -------
        str
            Describing the common type of the input data.
        Results can include:
```

The method is called a couple times with the `skipna=True` argument. For example in:

- `pandas/core/algorithms.py` line 220
- `pandas/core/dtypes/cast.py` line 131

Is this expected behaviour? Should I rename `foo -> value` and `bar -> skipna`?

"
705169046,36506,DOC: Update generate_pip_deps_from_conda.py #36494,dishak331,closed,2020-09-20T19:22:38Z,2020-09-21T19:12:29Z,"- [x] closes #36494
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
705634077,36524,TST: remove xfails with strict=False,simonjayhawkins,closed,2020-09-21T14:09:41Z,2020-09-21T19:29:07Z,xref https://github.com/pandas-dev/pandas/pull/35772#issuecomment-675558375
705067680,36496,Update outdated instructions in scripts/generate_pip_deps_from_conda.py #36494,sharonwoo,closed,2020-09-20T08:17:23Z,2020-09-21T21:11:51Z,"- [x] closes #36494 
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
705060078,36494,Update outdated instructions in scripts/generate_pip_deps_from_conda.py,MarcoGorelli,closed,2020-09-20T07:14:28Z,2020-09-21T21:12:09Z,"It says
```
Usage:

    Generate `requirements-dev.txt`
    $ ./conda_to_pip

    Compare and fail (exit status != 0) if `requirements-dev.txt` has not been
    generated with this script:
    $ ./conda_to_pip --compare
```
but it should be
```
Usage:

    Generate `requirements-dev.txt`
    $ python scripts/generate_pip_deps_from_conda.py

    Compare and fail (exit status != 0) if `requirements-dev.txt` has not been
    generated with this script:
    $ python scripts/generate_pip_deps_from_conda.py --compare
```

No need to ask if you can work on this, feel free to just submit a PR"
466155645,27319,DOC: Error in pd.cut documentation example regarding IntervalIndex usage.,msznajder,open,2019-07-10T07:51:30Z,2020-09-21T21:26:24Z,"#### Code Sample, a copy-pastable example if possible

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html

```
bins = pd.IntervalIndex.from_tuples([(0, 1), (2, 3), (4, 5)])
pd.cut([0, 0.5, 1.5, 2.5, 4.5], bins)
[NaN, (0, 1], NaN, (2, 3], (4, 5]]
Categories (3, interval[int64]): [(0, 1] < (2, 3] < (4, 5]]
```

#### Problem description

Proposed example in pd.cut IntervalIndex section does not take into consideration actual pd.cut behaviour which in above example results produces ranges with lots of missing values and nans in results. In docs example above for example intermediate values like 2 and 4 WILL NOT be included in any bins, so the actual values of 2 and 4 in the data will produce nans after cutting the attribute using pd.cut.

I assume here that user in 99% of the time when using cut to bucketize value space wants all values in the spectrum to be included. This usage example can lead to data loss.

Actual example should be along the lines:
```
bins = pd.IntervalIndex.from_tuples([(0, 1), (1, 3), (3, 5)])
```
resulting in the following bins:
```
Categories (3, interval[int64]): [(0, 1] < (1, 3] < (3, 5]]
```
EDIT:
There is a correct/proper example in IntervalIndex docs:

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.IntervalIndex.from_tuples.html#pandas.IntervalIndex.from_tuples"
705892555,36533,CLN: remove not existing argument,erfannariman,closed,2020-09-21T20:26:27Z,2020-09-21T22:10:22Z,"Small clean up of argument in docstrings which is not present in function.
"
484308907,28108,DataFrame.mean() ignores datetime series,BlaneG,closed,2019-08-23T03:16:57Z,2020-09-21T22:10:31Z,"#### Code Sample, a copy-pastable example if possible
```python
In [1]: import pandas as pd
In [2]: from datetime import datetime

In [3]: s = pd.Series([datetime(2014, 7, 9), 
   ...:            datetime(2014, 7, 10), 
   ...:            datetime(2014, 7, 11)])

In [4]: df = pd.DataFrame({'numeric':[1,2,3],
   ...:               'datetime':s})

In [5]: df.mean()
Out[5]: 
numeric    2.0
dtype: float64

In [6]: s.mean()
Out[6]: Timestamp('2014-07-10 00:00:00')
```
#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

As of pandas 0.25 it is possible to apply mean() to a datetime series.  However, DataFrame.mean() ignores datetimes series columns rather than returning the mean of the datetime series as one might expect.

#### Expected Output
When axis=0, output could be a dataframe with dtype as the first row, and the value as the second row.

"
555851303,31370,REF: IntervalIndex dispatch more methods to IntervalArray,jbrockmendel,closed,2020-01-27T21:43:20Z,2020-09-22T00:57:19Z,"I think we can de-duplicate some more code by a) noticing that IntervalArray._check_closed_matches is performing the same task as DatetimeLikeArrayMixin._check_compatible_with and b) dispatching to IntervalArray methods more.

`_concat_same_dtype` could become (this could even go as the default in ExtensionIndex) 

```
def _concat_same_dtype(self, to_concat, name):
        arr = type(self._data)._concat_same_type(to_concat)
        return type(self)._simple_new(arr, name=name)
```

`insert` could look like (also could go into ExtensionIndex):

```
def insert(self, loc, item):
        new_arr = type(self._data)._from_sequence([item])
        to_concat = [self[:loc]._data, new_arr, self.loc[loc:]._data]
        res_values = type(self._data)._concat_same_type(to_concat)
        return type(self)._simple_new(res_values, name=self.name)
```

Tried this and it chokes on item=NaT.

The integer-loc case for `delete` could look like:

```
def delete(self, loc):
    if is_integer(loc):
            to_concat = [self[:loc]._data, self[loc+1:]._data]
            arr = type(self._data)._concat_same_type(to_concat)
            return type(self)._simple_new(arr, name=self.name)
```

Probably others.  Thoughts, cc @jschendel "
554155669,31245,Verbose parameter in pandas.Series.apply,MastafaF,closed,2020-01-23T13:26:45Z,2020-09-22T01:30:44Z,"Hi,

I think it could be really useful to add a **Verbose** parameter in the **pandas.Series.apply** function. It can take a really long time to get an output so having some kind of verbosity with something like tqdm could be really useful. 

Cheers,"
487859359,28246,.transform inconsistent / error-prone behavior for list ,mglowacki100,open,2019-09-01T08:51:57Z,2020-09-22T01:33:43Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd


df = pd.DataFrame(data={'label': ['a', 'b', 'b', 'c'], 'wave': [1, 2, 3, 4], 'y': [0,0,0,0]})
###problem 1
df['n_tuple'] = df.groupby(['label'])[['wave']].transform(tuple)
df['n_set'] = df.groupby(['label'])[['wave']].transform(set)
df['n_frozenset'] = df.groupby(['label'])[['wave']].transform(frozenset)
df['n_dict'] = df.groupby(['label'])[['wave']].transform(dict)

df['n_list_problem'] = df.groupby(['label'])[['wave']].transform(list)
df['n_list_expected'] = df.groupby(['label'])[['wave']].transform(tuple)['wave'].apply(list)

###problem 2
df['n_sum'] = df.groupby(['label'])['wave'].transform(sum)
df['n_tuple_problem'] = df.groupby(['label'])['wave'].transform(tuple)
df['n_tuple'] = df.groupby(['label'])[['wave']].transform(tuple)

```
#### Problem description
I was hinted that it could be a bug: https://stackoverflow.com/questions/57743798/pandas-transform-inconsistent-behavior-for-list
There are two things, but let focus on **1** first: 
Result of 
`df['n_list_problem'] = df.groupby(['label'])[['wave']].transform(list)
`is not consistent with similar operations like `df['n_tuple'], df['n_set']` etc.
On stackoverflow I was pointed to issues regarding series lenght for rationale, but consensus was that, this behavior is confusing and error-prone.

The **2** problem:
`df['n_sum'] = df.groupby(['label'])['wave'].transform(sum)` works as expected, but
`df['n_tuple_problem'] = df.groupby(['label'])['wave'].transform(tuple)` gives unexpected result.
To get proper one, you need to coerce series into dataframe by `[[]]` instead of `[]` for `wave`.
`df['n_tuple'] = df.groupby(['label'])[['wave']].transform(tuple)
`
#### Expected Output
`df['n_list_expected'] = df.groupby(['label'])[['wave']].transform(tuple)['wave'].apply(list)
`

```
  label  wave  y n_tuple  n_list_problem n_list_expected  n_tuple_problem
0     a     1  0    (1,)               1             [1]                1
1     b     2  0  (2, 3)               2          [2, 3]                2
2     b     3  0  (2, 3)               3          [2, 3]                3
3     c     4  0    (4,)               4             [4]                4

```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-141-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.0
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : 0.29.13
pytest           : None
hypothesis       : None
sphinx           : 2.1.2
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.7.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
375961424,23427,Issue with fillna on Extension Array Unit Tests,achapkowski,open,2018-10-31T13:26:52Z,2020-09-22T01:33:43Z,"#### Code Sample, a copy-pastable example if possible

```python
import operator
import json
import numpy as np

from pandas.core.arrays import ExtensionArray
from pandas.core.dtypes.dtypes import ExtensionDtype
from arcgis.geometry import Geometry
import pandas as pd
from .parser import _to_geo_array


#####################################################################
class NumPyBackedExtensionArrayMixin(ExtensionArray):
    """"""
    Geo-Specific Extension Array Mixin
    """"""
    @property
    def dtype(self):
        """"""The dtype for this extension array, GeoType""""""
        return self._dtype

    @classmethod
    def _from_sequence(cls, scalars):
        return cls(scalars)

    @classmethod
    def _constructor_from_sequence(cls, scalars):
        return cls(scalars)

    @classmethod
    def _from_factorized(cls, values, original):
        return cls(values)

    @property
    def shape(self):
        return (len(self.data),)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, *args):
        result = operator.getitem(self.data, *args)
        if isinstance(result, (dict, Geometry)):
            return result
        elif isinstance(result, type(None)) or \
             isinstance(result, type(np.nan)):
            return None
        elif not isinstance(result, GeoArray):
            return GeoArray(result)
        return result


    def setitem(self, indexer, value):
        """"""Set the 'value' inplace.
        """"""
        self[indexer] = value
        return self

    @property
    def nbytes(self):
        return self._itemsize * len(self)

    def _formatting_values(self):
        return np.array(self._format_values(), dtype='object')

    def copy(self, deep=False):
        return type(self)(self.data.copy())

    @classmethod
    def _concat_same_type(cls, to_concat):
        return cls(np.concatenate([array.data for array in to_concat]))

    def tolist(self):
        return self.data.tolist()

    def argsort(self, axis=-1, kind='quicksort', order=None):
        return self.data.argsort()

    def unique(self):
        _, indices = np.unique(self.data, return_index=True)
        data = self.data.take(np.sort(indices))
        return self._from_ndarray(data)
#####################################################################
class GeoType(ExtensionDtype):
    name = 'geometry'
    type = Geometry
    kind = 'O'
    _record_type = np.dtype('O')
    na_value = None

    @classmethod
    def construct_from_string(cls, string):
        if string == cls.name:
            return cls()
        else:
            raise TypeError(""Cannot construct a '{}' from ""
                            ""'{}'"".format(cls, string))
#####################################################################
class GeoArray(NumPyBackedExtensionArrayMixin):
    """"""Array for Geometry data.
    """"""
    _dtype = GeoType()
    _itemsize = 8
    ndim = 1
    can_hold_na = True

    def __init__(self, values, copy=True):
        self.data = np.array(values, dtype='O', copy=copy)

    @classmethod
    def _from_ndarray(cls, data, copy=False):
        return cls(data, copy=copy)

    @property
    def na_value(self):
        return self.dtype.na_value

    def __repr__(self):
        formatted = self._format_values()
        return ""GeoArray({!r})"".format(formatted)

    def __str__(self):
        return self.__repr__()

    def _format_values(self):
        if self.data.ndim == 0:
            return """"
        return [_format(x) if x else None for x in self.data]

    @classmethod
    def from_geometry(cls, data, copy=False):
        """"""""""""
        if copy:
            data = data.copy()
        new = GeoArray([])
        new.data = np.array(data)
        return new

    def __setitem__(self, key, value):
        if value is None or  \
           (isinstance(value, str) and value == """"):
            self.data[key] = value
        else:
            value = Geometry(value)
            self.data[key] = value

    def __iter__(self):
        return iter(self.data.tolist())

    def __eq__(self, other):
        return self.data == other

    def equals(self, other):
        if not isinstance(other, type(self)):
            raise TypeError
        return (self.data == other.data).all()

    def _values_for_factorize(self):
        # Should hit pandas' UInt64Hashtable
        return self, 0

    def isna(self):
        return (self.data == self._dtype.na_value)

    @property
    def _parser(self):
        return lambda x: x

    def take(self, indexer, allow_fill=True, fill_value=None):
        mask = indexer == -1
        result = self.data.take(indexer)
        result[mask] = self.dtype.na_value
        return type(self)(result, copy=False)

    def _formatting_values(self):
        return np.array(self._format_values(), dtype='object')

    @classmethod
    def _concat_same_type(cls, to_concat):
        return cls(np.concatenate([array.data for array in to_concat]))

    def take_nd(self, indexer, allow_fill=True, fill_value=None):
        return self.take(indexer, allow_fill=allow_fill, fill_value=fill_value)

    def copy(self, deep=False):
        return type(self)(self.data.copy())

class Geometry(dict):
     """"""Universal Geometry Class that will use either Shapely or ArcPy Geometry objects.""""""
    _ao = None
    _type = None
    _HASARCPY = None
    _HASSHAPELY = None
    #----------------------------------------------------------------------
    def __init__(self, iterable=None):
        if iterable is None:
            iterable = {}
        self.update(iterable)
    #----------------------------------------------------------------------
    def is_valid(self):
        return True#_is_valid(self)
    #----------------------------------------------------------------------
    def _check_geometry_engine(self):
        if self._HASARCPY is None:
            try:
                import arcpy
                self._HASARCPY = True
            except:
                self._HASARCPY = False
        if self._HASSHAPELY is None:
            try:
                import shapely
                self._HASSHAPELY = True
            except:
                self._HASSHAPELY = False
        return self._HASARCPY, self._HASSHAPELY
    #----------------------------------------------------------------------
    def __setattr__(self, key, value):
        """"""sets the attribute""""""
        if key in {'_ao','_type', '_HASARCPY', '_HASSHAPELY',
                   '_ipython_canary_method_should_not_exist_'}:
            super(BaseGeometry, self).__setattr__(key,value)
        else:
            self[key] = value
            self._ao = None
    #----------------------------------------------------------------------
    def __setattribute__ (self, key, value):
        if key in {'_ao','_type', '_HASARCPY', '_HASSHAPELY',
                   '_ipython_canary_method_should_not_exist_'}:
            super(BaseGeometry, self).__setattr__(key,value)
        else:
            self[key] = value
            self._ao = None
    #----------------------------------------------------------------------
    def __setitem__(self, key, value):
        dict.__setitem__(self, key, value)
        self._ao = None
    #----------------------------------------------------------------------
    def __getattribute__ (self, name):
        return super(BaseGeometry, self).__getattribute__(name)
    #----------------------------------------------------------------------
    def __getattr__(self, name):
        try:
            if name in {'_ao',""_type"", '_HASARCPY', '_HASSHAPELY',
                        '_ipython_canary_method_should_not_exist_'}:
                return super(BaseGeometry, self).__getattr__(k)
            return dict.__getitem__(self, name)
        except:
            raise AttributeError(""'%s' object has no attribute '%s'"" % (type(self).__name__, name))
    #----------------------------------------------------------------------
    def __getitem__(self, k):
        return dict.__getitem__(self, k)

@pytest.fixture
def dtype():
    """"""A fixture providing the ExtensionDtype to validate.""""""
    return GeoType()

@pytest.fixture
def data():
    """"""Length-100 array for this type.""""""
    from arcgis.geometry._types import Geometry, Point
    data = [Point({'x':1,'y':2,'spatialReference': {'wkid':4326}})]
    data *= 100
    return GeoArray(data)


@pytest.fixture
def data_missing():
    """"""Length-2 array with [NA, Valid]""""""
    from arcgis.geometry._types import BaseGeometry, Point
    data = [None, BaseGeometry({'x':1,'y':2,'spatialReference': {'wkid':4326}})]
    return GeoArray(data)

@pytest.fixture
def na_cmp():
    """"""Binary operator for comparing NA values.

    Should return a function of two arguments that returns
    True if both arguments are (scalar) NA for your type.

    By default, uses ``operator.is_``
    """"""
    return operator.is_


@pytest.fixture
def na_value():
    """"""The scalar missing value for this type. Default 'None'""""""
    return None

class TestBaseMissing(base.BaseMissingTests): pass
TestBaseMissing().test_fillna_series(data_missing())  # I can't get this to pass.
```
#### Problem description

When creating a custom array and dtype the `test_fillna_series` always fails regardless of code change.  

I am at my wits end trying to see where I am going wrong.  The `isna()` works like a charms and returns the proper value.  When fillna is called it does not work at all I am getting:

```builtins.ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().```

So the `fillna` is passing in a series to ```__setitem__``` which is then it's trying to create a `Geometry` object from that.  But the ```__setitem__``` value is a `nan` value, which makes no sense at all.  I thought the value would be my replacement point.  Is something wrong with the fillna logic?

Also, when I modify the `__setitem__` to handle the series, it then sets the row, in this case index 0 to a GeoArray.



#### Expected Output

```
0    {""x"": 1, ""y"": 2, ""spatialReference"": {""wkid"": ...
1    {""x"": 1, ""y"": 2, ""spatialReference"": {""wkid"": ...
dtype: geometry
```




#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit: None
python: 3.6.6.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: English_United States.1252

pandas: 0.23.4
pytest: 3.7.1
pip: 18.0
setuptools: 40.0.0
Cython: None
numpy: 1.14.3
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.5.0
sphinx: None
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: 3.4.2
numexpr: 2.6.6
feather: None
matplotlib: 2.2.2
openpyxl: 2.5.5
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: None
lxml: 4.2.3
bs4: 4.6.3
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
375307024,23417,TST: Reorganize Frame/Series ops tests,sinhrks,closed,2018-10-30T03:40:24Z,2020-09-22T01:36:26Z,"Currently, similar tests for DataFrame and Series are written in either `test_arithmetic.py` or `test_operators.py`.

How about merging them, or split them to based on operators (like `test_op_arithmetic`, `test_op_comparison` and `test_op_unary`)."
706019922,36537,"Revert ""ENH: Optimize nrows in read_excel""",jbrockmendel,closed,2020-09-22T01:41:28Z,2020-09-22T03:17:29Z,Reverts pandas-dev/pandas#35974
383444212,23857,"apply doesn't work when returning 2D vector (shape=(N,1)) instead of 1D (shape=(N,))",NoamGit,open,2018-11-22T09:19:59Z,2020-09-22T03:54:28Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
import numpy as np
import pandas as pd
f = lambda x: np.reshape(x,(-1,1))
df = pd.DataFrame(np.random.randn(10,3), columns=['a','b','c'])
df.apply(f)
```

#### Problem description

Code is self explainable

#### Expected Output

#### Output of ``df.apply(f))``

<details>

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds)
     51     try:
---> 52         return getattr(obj, method)(*args, **kwds)
     53

~\Anaconda3\lib\site-packages\pandas\core\generic.py in __getattr__(self, name)
   4371                 return self[name]
-> 4372             return object.__getattribute__(self, name)
   4373

AttributeError: 'Series' object has no attribute 'reshape'

During handling of the above exception, another exception occurred:

Exception                                 Traceback (most recent call last)
<ipython-input-16-4d3ad4c6f441> in <module>()
----> 1 df.apply(f)

~\Anaconda3\lib\site-packages\pandas\core\frame.py in apply(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)
   6002                          args=args,
   6003                          kwds=kwds)
-> 6004         return op.get_result()
   6005
   6006     def applymap(self, func):

~\Anaconda3\lib\site-packages\pandas\core\apply.py in get_result(self)
    316                                       *self.args, **self.kwds)
    317
--> 318         return super(FrameRowApply, self).get_result()
    319
    320     def apply_broadcast(self):

~\Anaconda3\lib\site-packages\pandas\core\apply.py in get_result(self)
    140             return self.apply_raw()
    141
--> 142         return self.apply_standard()
    143
    144     def apply_empty_result(self):

~\Anaconda3\lib\site-packages\pandas\core\apply.py in apply_standard(self)
    246
    247         # compute the result using the series generator
--> 248         self.apply_series_generator()
    249
    250         # wrap results

~\Anaconda3\lib\site-packages\pandas\core\apply.py in apply_series_generator(self)
    275             try:
    276                 for i, v in enumerate(series_gen):
--> 277                     results[i] = self.f(v)
    278                     keys.append(v.name)
    279             except Exception as e:

<ipython-input-4-75939c74dee2> in <lambda>(x)
----> 1 f = lambda x: np.reshape(x,(-1,1))

~\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py in reshape(a, newshape, order)
    255            [5, 6]])
    256     """"""
--> 257     return _wrapfunc(a, 'reshape', newshape, order=order)
    258
    259

~\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds)
     60     # a downstream library like 'pandas'.
     61     except (AttributeError, TypeError):
---> 62         return _wrapit(obj, method, *args, **kwds)
     63
     64

~\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds)
     44         if not isinstance(result, mu.ndarray):
     45             result = asarray(result)
---> 46         result = wrap(result)
     47     return result
     48

~\Anaconda3\lib\site-packages\pandas\core\series.py in __array_wrap__(self, result, context)
    645         """"""
    646         return self._constructor(result, index=self.index,
--> 647                                  copy=False).__finalize__(self)
    648
    649     def __array_prepare__(self, result, context=None):

~\Anaconda3\lib\site-packages\pandas\core\series.py in __init__(self, data, index, dtype, name, copy, fastpath)
    272             else:
    273                 data = _sanitize_array(data, index, dtype, copy,
--> 274                                        raise_cast_failure=True)
    275
    276                 data = SingleBlockManager(data, index, fastpath=True)

~\Anaconda3\lib\site-packages\pandas\core\series.py in _sanitize_array(data, index, dtype, copy, raise_cast_failure)
   4159     elif subarr.ndim > 1:
   4160         if isinstance(data, np.ndarray):
-> 4161             raise Exception('Data must be 1-dimensional')
   4162         else:
   4163             subarr = com._asarray_tuplesafe(data, dtype=dtype)

Exception: ('Data must be 1-dimensional', 'occurred at index a')
```

</details>
"
399469122,24783,Single-level tuple index to multi-level index on pd.concat,harisbal,open,2019-01-15T18:10:14Z,2020-09-22T03:58:34Z,"```python
import pandas as pd
import numpy as np

s1 = pd.Series(np.random.randn(2), index=[('a', 'b'), ('x', 'y', 'z')])
s1.index.name = 'Idx'
s1.name = 's1'

s2 = pd.Series(np.random.randn(2), index=[('a', 'b'), ('j', 'k', 'l')])
s2.index.name = 'Idx'
s2.name = 's2'

# Result
pd.concat([s1, s2], axis=1, sort=False)
```
#### Problem description
Concatenating two series with a tuple as index, results in a multi-indexed dataframe

#### Expected Output
```
s1.to_frame().join(s2)
```
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.6.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.0.dev0+1028.gdb2066b7d
pytest: 3.8.1
pip: 18.1
setuptools: 40.2.0
Cython: 0.28.5
numpy: 1.15.1
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 5.0.0
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: 3.4.4
numexpr: 2.6.8
feather: None
matplotlib: 2.2.3
openpyxl: 2.5.6
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0.1
sqlalchemy: 1.2.11
pymysql: 0.9.2
psycopg2: 2.7.5 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
424140800,25833,RecursionError with self referencing column in str(df) on DataFrame,Doom312,open,2019-03-22T10:28:59Z,2020-09-22T04:05:45Z,"```python
# Your code here
b = a = pd.Series(
    [2, 1, 3],
    index=[2, 1, 3],
)
# display(b)
a['Hi'] = b
print(str(a))
```

This raises
```python
RecursionError: maximum recursion depth exceeded in __instancecheck__
```

It is annoying when this happens in interactive mode.

#### Expected Output

```
2                                     2
1                                     1
3                                     3
Hi    2    2
1    1
3    3
dtype: int64
dtype: object
```

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.2.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 158 Stepping 9, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.1
pytest: None
pip: 18.1
setuptools: 40.6.3
Cython: 0.29.4
numpy: 1.16.1
scipy: 1.2.0
pyarrow: None
xarray: None
IPython: 7.2.0
sphinx: None
patsy: 0.5.1
dateutil: 2.7.5
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: 1.2.18
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
```

</details>
"
421878636,25749,Setting to multi-index np array break the data,dddping,closed,2019-03-17T03:46:06Z,2020-09-22T04:09:14Z,"#### To re-produce

```python
# create single/multi index DataFrame
df_s = pd.DataFrame(np.arange(12).reshape(3, 4),columns=list('ABCD'),index=['bar', 'baz', 'foo'])
df_m = pd.DataFrame(np.arange(24).reshape(6, 4),columns=list('ABCD'),index=[
np.array(['bar', 'baz', 'foo','bar', 'baz', 'foo']),np.array(['one','one','one','two','two','two'])])

idx_s=df_s.index.to_numpy()
# change label to other value
idx_s[0]='qux'
# normal
assert df_s.loc['qux','A']==0
assert df_s.index.to_numpy()[0]=='qux'


idx_m=df_m.index.to_numpy()
# change label to other value
idx_m[0]=('qux','one')
# it get change from to_numpy return value
assert df_m.index.to_numpy()[0]==('qux','one')
# however indexing for ('qux','one') raise key error
assert df_m.loc('qux','one'),'A']==0 <-key error

```
#### Problem description

It is already mention from https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.to_numpy.html
**For NumPy dtypes, this will be a reference to the actual data stored in this Series or Index (assuming copy=False). Modifying the result in place will modify the data stored in the Series or Index (not that we recommend doing that).**

However it is really confusing that the label is in index.to_numpy() but it can't access using loc.

**Note**: I am new to Pandas. I start reading Getting started and User Guide from last week. It is a very good resource for newcomer. For the above issue, I don't have any suggestion, just feeling strange...

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.1.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.1
pytest: 4.1.0
pip: 10.0.1
setuptools: 39.0.1
Cython: None
numpy: 1.16.2
scipy: None
pyarrow: None
xarray: None
IPython: 7.3.0
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.3
openpyxl: None
xlrd: 1.2.0
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: 4.7.1
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
434235829,26119,Modify index of Series derived from Dataframe affects index of it's column.,dave-cz,open,2019-04-17T11:38:58Z,2020-09-22T04:18:10Z,"```python
import pandas as pd
from datetime import datetime
import pytz

dt_from = datetime(2019, 3, 30, 0, tzinfo=pytz.utc)
dt_to = datetime(2019, 3, 30, 4, tzinfo=pytz.utc)
idx = pd.date_range(dt_from, dt_to, freq='H')

df = pd.DataFrame(data={
    'a': {i: v for i, v in zip(idx, range(idx.size))}, 
    'b': {i: v+10 for i, v in zip(idx, range(idx.size))}}
)

>>> df
                           a   b
2019-03-30 00:00:00+00:00  0  10
2019-03-30 01:00:00+00:00  1  11
2019-03-30 02:00:00+00:00  2  12
2019-03-30 03:00:00+00:00  3  13
2019-03-30 04:00:00+00:00  4  14

s = df['a']  # df.loc[:, 'a'] instead of df['a'] works the same
s.index = s.index.tz_convert('Europe/Prague')

>>> s

2019-03-30 01:00:00+01:00     0
2019-03-30 02:00:00+01:00     1
2019-03-30 03:00:00+01:00     2
2019-03-30 04:00:00+01:00     3
2019-03-30 05:00:00+01:00     4
Name: a, dtype: int64

# good

>>> df

                           a   b
2019-03-30 00:00:00+00:00  0  10
2019-03-30 01:00:00+00:00  1  11
2019-03-30 02:00:00+00:00  2  12
2019-03-30 03:00:00+00:00  3  13
2019-03-30 04:00:00+00:00  4  14

# good

>>> df['a']

2019-03-30 01:00:00+01:00    0
2019-03-30 02:00:00+01:00    1
2019-03-30 03:00:00+01:00    2
2019-03-30 04:00:00+01:00    3
2019-03-30 05:00:00+01:00    4
Name: a, dtype: int64

# BAD INDEX

>>> df['b']

2019-03-30 00:00:00+00:00    10
2019-03-30 01:00:00+00:00    11
2019-03-30 02:00:00+00:00    12
2019-03-30 03:00:00+00:00    13
2019-03-30 04:00:00+00:00    14
Name: b, dtype: int64

# good
```

#### Workaround

```python
s = df['a'].copy()
```

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.15.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None
pandas: 0.24.2
pytest: None
pip: 19.0.3
setuptools: 40.8.0
Cython: 0.29.5
numpy: 1.14.6
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 5.8.0
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.8
feather: None
matplotlib: 2.2.3
openpyxl: 2.6.0
xlrd: 1.2.0
xlwt: None
xlsxwriter: 1.1.5
lxml.etree: 4.3.1
bs4: 4.7.1
html5lib: None
sqlalchemy: 1.2.17
pymysql: None
psycopg2: 2.7.7 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.7.0
gcsfs: None
</details>"
690676826,36060,BUG: MultiIndex with nan values obtained by `groupby` behaves different to MultiIndex.from_tuples(),ssche,open,2020-09-02T03:04:49Z,2020-09-22T05:20:22Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.
  I checked in 1.1.1

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
In [11]: import pandas as pd
    ...: import numpy as np
    ...: 
    ...: df = pd.DataFrame({
    ...:     'animal': ['Falcon', 'Falcon', 'Parrot', 'Parrot'],
    ...:     'type': [np.nan, np.nan, np.nan, np.nan],
    ...:     'speed': [380., 370., 24., 26.]
    ...: })
    ...: speed = df.groupby(['animal', 'type'], dropna=False)['speed'].first()

In [12]: speed
Out[12]: 
animal  type
Falcon  NaN     380.0
Parrot  NaN      24.0
Name: speed, dtype: float64

In [13]: speed.index.levels
Out[13]: FrozenList([['Falcon', 'Parrot'], [nan]])

In [14]: speed.index.codes
Out[14]: FrozenList([[0, 1], [0, 0]])

In [15]: # Reconstruct same index to allow for multiplication.
    ...: ix_wing = pd.MultiIndex.from_tuples(
    ...:     [('Falcon', np.nan), ('Parrot', np.nan)], names=['animal', 'type']
    ...: )
    ...: wing = pd.Series([42, 44], index=ix_wing)

In [16]: wing
Out[16]: 
animal  type
Falcon  NaN     42
Parrot  NaN     44
dtype: int64

In [17]: wing.index.levels
Out[17]: FrozenList([['Falcon', 'Parrot'], []])

In [18]: wing.index.codes
Out[18]: FrozenList([[0, 1], [-1, -1]])

In [19]: 

In [19]: 

In [19]: speed * wing
Out[19]: 
animal  type
Falcon  NaN    NaN
        NaN    NaN
Parrot  NaN    NaN
        NaN    NaN
dtype: float64


```

#### Problem description

I'm trying to perform combine two series (say multiplication for now). One of them is obtained by a groupby aggregation (say `first`) and the other series is constructed manually. Both series have a MultiIndex which _should_ be the same and a multiplication should work fine. However, it seems that `groupby(..., dropna=False)` creates a different MI which causes the operation to return an unexpected result.

#### Expected Output

I would expect the result of `speed * wing` to be

```
Falcon  NaN     15960.0
Parrot  NaN     1056.0
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.7.17-200.fc32.x86_64
Version          : #1 SMP Fri Aug 21 15:23:46 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_AU.UTF-8
LOCALE           : en_AU.UTF-8

pandas           : 1.1.1
numpy            : 1.18.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 47.3.1
Cython           : 0.29.17
pytest           : 5.1.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 0.9.6
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : None
bottleneck       : 1.3.1
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 1.8.6
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.12
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None


</details>
"
607587251,33821,BUG: repair 'style' kwd handling in DataFrame.plot (#21003),joooeey,closed,2020-04-27T14:16:13Z,2020-09-22T05:26:04Z,"- [x] closes #21003
- [x] tests added / passed
- [x] passes `black pandas`
    => `command not found`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
    => `command not found`
- [x] whatsnew entry
"
706049968,36539,DOC: Links to Scipy window functions are out of date,jpeacock29,closed,2020-09-22T03:24:40Z,2020-09-22T07:22:11Z,"#### Location of the documentation
- https://github.com/pandas-dev/pandas/blob/master/doc/source/user_guide/computation.rst
- https://github.com/pandas-dev/pandas/blob/00a510bfc0416d890a385697a8a0e5127602f9ec/pandas/core/window/rolling.py#L943

#### Documentation problem
Current link goes to https://docs.scipy.org/doc/scipy/reference/signal.html#window-functions which simply points users to https://docs.scipy.org/doc/scipy/reference/signal.windows.html#module-scipy.signal.windows.

#### Suggested fix for documentation
Pandas should link directly to the latter documentation.
"
705770970,36527,BUG: Different behaviour for index after assign dataframe new column by series,arybin93,closed,2020-09-21T17:06:02Z,2020-09-22T09:12:40Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

With pandas version 1.1.1

```python
import pandas as pd

df = pd.DataFrame()
series = pd.Series(1.23, index=pd.RangeIndex(4, name=""series_index""))
df['series'] = series
print(df)
```

#### Problem description

With pandas version 1.1.1, output:
```
                         series
series_index        
0                       1.23
1                       1.23
2                       1.23
3                       1.23
```
We perceive index from series

With pandas version 1.1.2, output:
```
   series
0    1.23
1    1.23
2    1.23
3    1.23
```
We lost the index from series

#### Expected Output
I am not sure about it. Could you explain is bug or feature of the latest version?
We used to perceive index from series and it worked before.

#### Output of ``pd.show_versions()``

<details>

[INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.104-microsoft-standard
Version          : #1 SMP Wed Feb 19 06:37:35 UTC 2020
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_US.UTF-8
pandas           : 1.1.2
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.0
Cython           : None
pytest           : 6.0.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.19
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None
]

</details>
"
705885969,36532,BUG: Fix issue in preserving index name on empty DataFrame,Dr-Irv,closed,2020-09-21T20:15:35Z,2020-09-22T11:59:24Z,"- [x] closes #36527
- [x] tests added / passed
   - `tests/indexing/test_partial.py:tesst_index_name_empty`
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
   - v1.1.3
"
706318907,36545,Backport PR #36532 on branch 1.1.x (BUG: Fix issue in preserving index name on empty DataFrame),meeseeksmachine,closed,2020-09-22T11:57:42Z,2020-09-22T12:49:36Z,Backport PR #36532: BUG: Fix issue in preserving index name on empty DataFrame
687354657,35925,CLN remove unnecessary trailing commas to get ready for new version of black,MarcoGorelli,closed,2020-08-27T15:52:10Z,2020-09-22T13:39:07Z,"The new version of `black` is consistent in how it handles the magic trailing commas. So if we upgrade `black` and apply it, lots of files will be changed. However, the diff needn't be so large if we remove unnecessary trailing commas before upgrading.

E.g. in pandas/core/aggregation.py there is

```python
def reconstruct_func(
    func: Optional[AggFuncType], **kwargs,
) -> Tuple[
    bool, Optional[AggFuncType], Optional[List[str]], Optional[List[int]],
]:
```

which has an unnecessary trailing comma.

The new version of `black` would transform this as

```python
def reconstruct_func(
    func: Optional[AggFuncType],
    **kwargs,
) -> Tuple[bool, Optional[AggFuncType], Optional[List[str]], Optional[List[int]],]:
```

However, if we instead remove the trailing comma and write it as

```python
def reconstruct_func(
    func: Optional[AggFuncType], **kwargs
) -> Tuple[bool, Optional[AggFuncType], Optional[List[str]], Optional[List[int]]]:
```

then both the current and the new versions of black will be OK with it.

So, PRs to remove some unnecessary trailing commas would be welcome - perhaps keep each PR limited to 5-10 files changed.

Files that (may) need changing are:

- [ ] /asv_bench/benchmarks/arithmetic.py
- [ ] /pandas/core/array_algos/replace.py
- [x] /doc/make.py
- [ ] /doc/source/conf.py
- [ ] /pandas/core/aggregation.py
- [ ] /pandas/core/algorithms.py
- [ ] /pandas/_vendored/typing_extensions.py
- [ ] /pandas/core/util/numba_.py
- [ ] /pandas/core/sorting.py
- [ ] /pandas/io/formats/latex.py
- [ ] /pandas/core/series.py
- [ ] /pandas/io/formats/format.py
- [ ] /pandas/core/frame.py
- [ ] /pandas/tests/arrays/sparse/test_array.py
- [ ] /pandas/tests/frame/test_analytics.py
- [x] /pandas/tests/indexes/base_class/test_indexing.py
- [x] /pandas/tests/indexes/test_common.py
- [x] /pandas/tests/indexes/test_numeric.py
- [ ] /pandas/tests/io/test_gcs.py
- [ ] /pandas/tests/io/test_parquet.py
- [ ] /pandas/tests/scalar/timestamp/test_constructors.py
- [ ] /pandas/tests/series/test_operators.py
- [ ] /scripts/tests/test_validate_docstrings.py

EDIT
----
updated list of files which need changing"
705341370,36519,Backport PR #36464: BUG: Fix astype from float32 to string,jorisvandenbossche,closed,2020-09-21T06:55:12Z,2020-09-22T13:39:53Z,Backport of https://github.com/pandas-dev/pandas/pull/36464
650748444,35116,TST: base test for ExtensionArray.astype to its own type + copy keyword,tomaszps,closed,2020-07-03T19:58:22Z,2020-09-22T13:41:46Z,"- [] closes #28488
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
n.b. I'm not certain. I get no output on the command line when I run this command.
- [x] whatsnew entry
"
494842114,28488,Add base test for ExtensionArray.astype and copy,TomAugspurger,closed,2019-09-17T20:42:48Z,2020-09-22T13:41:53Z,"We should be able to test astype to our own type

```diff
diff --git a/pandas/tests/extension/base/casting.py b/pandas/tests/extension/base/casting.py
index 7146443bf8..b33ab93031 100644
--- a/pandas/tests/extension/base/casting.py
+++ b/pandas/tests/extension/base/casting.py
@@ -1,3 +1,5 @@
+import pytest
+
 import pandas as pd
 from pandas.core.internals import ObjectBlock
 
@@ -21,3 +23,9 @@ class BaseCastingTests(BaseExtensionTests):
         result = pd.Series(data[:5]).astype(str)
         expected = pd.Series(data[:5].astype(str))
         self.assert_series_equal(result, expected)
+
+    @pytest.mark.parametrize('copy', [True, False])
+    def test_astype_own_type(self, data, copy):
+        result = data.astype(data.dtype, copy=copy)
+        assert (result is data) is (not copy)
+        self.assert_extension_array_equal(result, data)
```

A few tests fail this. Would welcome investigation.

`$ pytest pandas/tests/extension/ -k test_astype_own_type -v --tb=line`

```
========================================================================= FAILURES ==========================================================================
/Users/taugspurger/sandbox/pandas/pandas/tests/extension/base/casting.py:30: AssertionError
/Users/taugspurger/sandbox/pandas/pandas/tests/extension/base/casting.py:30: AssertionError
/Users/taugspurger/sandbox/pandas/pandas/tests/extension/base/casting.py:30: AssertionError
/Users/taugspurger/sandbox/pandas/pandas/tests/extension/base/casting.py:30: AssertionError
/Users/taugspurger/sandbox/pandas/pandas/tests/extension/base/casting.py:30: AssertionError
/Users/taugspurger/sandbox/pandas/pandas/tests/extension/base/casting.py:30: AssertionError
/Users/taugspurger/sandbox/pandas/pandas/tests/extension/base/casting.py:30: AssertionError
/Users/taugspurger/sandbox/pandas/pandas/tests/extension/base/casting.py:30: AssertionError
/Users/taugspurger/sandbox/pandas/pandas/core/arrays/base.py:390: TypeError: data type not understood
/Users/taugspurger/sandbox/pandas/pandas/core/arrays/base.py:390: TypeError: data type not understood
/Users/taugspurger/sandbox/pandas/pandas/core/arrays/base.py:390: TypeError: data type not understood
/Users/taugspurger/sandbox/pandas/pandas/core/arrays/base.py:390: TypeError: data type not understood
/Users/taugspurger/sandbox/pandas/pandas/tests/extension/base/casting.py:30: AssertionError
/Users/taugspurger/sandbox/pandas/pandas/tests/extension/base/casting.py:30: AssertionError
/Users/taugspurger/sandbox/pandas/pandas/tests/extension/base/casting.py:30: AssertionError
/Users/taugspurger/sandbox/pandas/pandas/tests/extension/base/casting.py:30: AssertionError
```"
702233982,36388,CLN: Made final changes to /pandas/tests/scalar,Abishek15592,closed,2020-09-15T20:24:54Z,2020-09-22T13:45:06Z,
706319915,36546,TST: add missing assert,jorisvandenbossche,closed,2020-09-22T11:59:14Z,2020-09-22T14:07:46Z,Small follow-up on https://github.com/pandas-dev/pandas/pull/36532
706020280,36538,validate fill_value in IntervalArray.take unconditionally,jbrockmendel,closed,2020-09-22T01:42:55Z,2020-09-22T14:54:53Z,xref #36466
706356399,36547,Backport PR #36546 on branch 1.1.x (TST: add missing assert),meeseeksmachine,closed,2020-09-22T12:51:42Z,2020-09-22T15:30:53Z,Backport PR #36546: TST: add missing assert
706479808,36549,CI: fix failing  pre-commit,simonjayhawkins,closed,2020-09-22T15:22:23Z,2020-09-22T16:22:51Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
646385961,35018,BUG-Fix: AssertionError when slicing MultiIndex and setting value of …,luckydenis,closed,2020-06-26T16:47:45Z,2020-09-22T16:39:03Z,"- [x] closes #34870 
- [x] tests added / passed (1 / 1)
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
688504648,35974,ENH: Optimize nrows in read_excel,MarcoGorelli,closed,2020-08-29T10:15:23Z,2020-09-22T17:00:12Z,"- [ ] closes #32727
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

based on #33281

----

output of asv benchmarks:

```
(pandas-dev) marco@marco-Predator-PH315-52:~/pandas-dev/asv_bench$ asv continuous -f 1.1 upstream/master optimise-nrows-excel -b excel.ReadExcel
· Creating environments..................................................................................................................................
· Discovering benchmarks
·· Uninstalling from conda-py3.8-Cython0.29.16-jinja2-matplotlib-numba-numexpr-numpy-odfpy-openpyxl-pytables-pytest-scipy-sqlalchemy-xlrd-xlsxwriter-xlwt
·· Building d0a8a687 <optimise-nrows-excel> for conda-py3.8-Cython0.29.16-jinja2-matplotlib-numba-numexpr-numpy-odfpy-openpyxl-pytables-pytest-scipy-sqlalchemy-xlrd-xlsxwriter-xlwt....................................
·· Installing d0a8a687 <optimise-nrows-excel> into conda-py3.8-Cython0.29.16-jinja2-matplotlib-numba-numexpr-numpy-odfpy-openpyxl-pytables-pytest-scipy-sqlalchemy-xlrd-xlsxwriter-xlwt..
· Running 4 total benchmarks (2 commits * 1 environments * 2 benchmarks)
[  0.00%] · For pandas commit c413df6d <master> (round 1/2):
[  0.00%] ·· Building for conda-py3.8-Cython0.29.16-jinja2-matplotlib-numba-numexpr-numpy-odfpy-openpyxl-pytables-pytest-scipy-sqlalchemy-xlrd-xlsxwriter-xlwt....................................
[  0.00%] ·· Benchmarking conda-py3.8-Cython0.29.16-jinja2-matplotlib-numba-numexpr-numpy-odfpy-openpyxl-pytables-pytest-scipy-sqlalchemy-xlrd-xlsxwriter-xlwt
[ 12.50%] ··· Setting up io.excel:62                                                                                                                                                                               ok
[ 12.50%] ··· Running (io.excel.ReadExcel.time_read_excel--)..
[ 25.00%] · For pandas commit d0a8a687 <optimise-nrows-excel> (round 1/2):
[ 25.00%] ·· Building for conda-py3.8-Cython0.29.16-jinja2-matplotlib-numba-numexpr-numpy-odfpy-openpyxl-pytables-pytest-scipy-sqlalchemy-xlrd-xlsxwriter-xlwt..
[ 25.00%] ·· Benchmarking conda-py3.8-Cython0.29.16-jinja2-matplotlib-numba-numexpr-numpy-odfpy-openpyxl-pytables-pytest-scipy-sqlalchemy-xlrd-xlsxwriter-xlwt
[ 37.50%] ··· Setting up io.excel:62                                                                                                                                                                               ok
[ 37.50%] ··· Running (io.excel.ReadExcel.time_read_excel--)..
[ 50.00%] · For pandas commit d0a8a687 <optimise-nrows-excel> (round 2/2):
[ 50.00%] ·· Benchmarking conda-py3.8-Cython0.29.16-jinja2-matplotlib-numba-numexpr-numpy-odfpy-openpyxl-pytables-pytest-scipy-sqlalchemy-xlrd-xlsxwriter-xlwt
[ 62.50%] ··· Setting up io.excel:62                                                                                                                                                                               ok
[ 62.50%] ··· io.excel.ReadExcel.time_read_excel                                                                                                                                                                   ok
[ 62.50%] ··· ========== ============
                engine               
              ---------- ------------
                 xlrd      953±6ms   
               openpyxl   1.66±0.03s 
                 odf      6.02±0.02s 
              ========== ============

[ 75.00%] ··· io.excel.ReadExcel.time_read_excel_nrows                                                                                                                                                             ok
[ 75.00%] ··· ========== ============
                engine               
              ---------- ------------
                 xlrd      878±20ms  
               openpyxl   1.67±0.02s 
                 odf      4.58±0.04s 
              ========== ============

[ 75.00%] · For pandas commit c413df6d <master> (round 2/2):
[ 75.00%] ·· Building for conda-py3.8-Cython0.29.16-jinja2-matplotlib-numba-numexpr-numpy-odfpy-openpyxl-pytables-pytest-scipy-sqlalchemy-xlrd-xlsxwriter-xlwt..
[ 75.00%] ·· Benchmarking conda-py3.8-Cython0.29.16-jinja2-matplotlib-numba-numexpr-numpy-odfpy-openpyxl-pytables-pytest-scipy-sqlalchemy-xlrd-xlsxwriter-xlwt
[ 87.50%] ··· Setting up io.excel:62                                                                                                                                                                               ok
[ 87.50%] ··· io.excel.ReadExcel.time_read_excel                                                                                                                                                                   ok
[ 87.50%] ··· ========== ============
                engine               
              ---------- ------------
                 xlrd      941±5ms   
               openpyxl   1.69±0.02s 
                 odf      6.15±0.04s 
              ========== ============

[100.00%] ··· io.excel.ReadExcel.time_read_excel_nrows                                                                                                                                                             ok
[100.00%] ··· ========== ============
                engine               
              ---------- ------------
                 xlrd      971±20ms  
               openpyxl   1.69±0.01s 
                 odf      6.07±0.03s 
              ========== ============

       before           after         ratio
     [c413df6d]       [d0a8a687]
     <master>         <optimise-nrows-excel>
-        971±20ms         878±20ms     0.90  io.excel.ReadExcel.time_read_excel_nrows('xlrd')

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE INCREASED.
```"
705753331,36526,BUG: modulo of pd.Int64Index returns negative values under some conditions,felixpatzelt,closed,2020-09-21T16:38:02Z,2020-09-22T19:37:12Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
(pd.Int64Index(list(range(0,24))*500) - 5) % 24

# returns Int64Index([-5, -4, -3, -2, -1,  0,  1,  2,  3,  4,
#            ...
#             9, 10, 11, 12, 13, 14, 15, 16, 17, 18],
#           dtype='int64', length=12000)

# note: (pd.Int64Index(list(range(0,24))*400) - 5) % 24 does not return negative values
```

#### Problem description

In Pandas 1.1.2, taking the modulo of an Int64Index can return negative values depending on the values in the index and on its length. This problem was not present in pandas 1.0.5. It is also inconsistent with the generally expected behaviour of modulo in python and numpy.

#### Expected Output

```
Int64Index([19, 20, 21, 22, 23,  0,  1,  2,  3,  4,
            ...
             9, 10, 11, 12, 13, 14, 15, 16, 17, 18],
           dtype='int64', length=9600)
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.7.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.1.2
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : 6.0.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : 0.10.1
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.2
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.2
sqlalchemy       : 1.3.19
tables           : 3.6.1
tabulate         : None
xarray           : 0.16.1
xlrd             : None
xlwt             : None
numba            : 0.51.2

</details>
"
706653911,36555,DOC: Fix missing spaces in whatsnew 1.1.3,nrebena,closed,2020-09-22T19:58:54Z,2020-09-22T21:54:21Z,Fix missing spaces preventing correct display of the issues.
703691858,36430,REGR: period_range giving incorrect values for large datetimes,dsaxton,closed,2020-09-17T15:29:36Z,2020-09-22T22:08:21Z,"From the pandas-dev mailing list:

> I encountered a bug in pandas 1.1.0 after updating from 1.0.5 that I wanted to pass along.  If a period range is created that extends past the maximum timestamp date of 2262-04-11, then the period range is shifted back by an hour after this date.  The screenshots below show an example with the shifted times highlighted in yellow.

```python
import pandas as pd
from datetime import datetime

start = datetime(2000, 1, 1)
end = datetime(2500, 1, 1)

idx = pd.period_range(start, end, freq=""6H"")
assert idx[-1].hour % 2 == 0  # raises

idx
# PeriodIndex(['2000-01-01 00:00', '2000-01-01 06:00', '2000-01-01 12:00',
#              '2000-01-01 18:00', '2000-01-02 00:00', '2000-01-02 06:00',
#              '2000-01-02 12:00', '2000-01-02 18:00', '2000-01-03 00:00',
#              '2000-01-03 06:00',
#              ...
#              '2499-12-29 17:00', '2499-12-30 23:00', '2499-12-30 05:00',
#              '2499-12-30 11:00', '2499-12-30 17:00', '2499-12-31 23:00',
#              '2499-12-31 05:00', '2499-12-31 11:00', '2499-12-31 17:00',
#              '2500-01-01 23:00'],
#             dtype='period[6H]', length=730489, freq='6H')

pd.__version__
# '1.1.0'
```"
706683021,36558,REF: de-duplicate Categorical validators,jbrockmendel,closed,2020-09-22T20:48:48Z,2020-09-22T22:18:07Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
706643025,36554,Call finalize in Series.dt,TomAugspurger,closed,2020-09-22T19:39:41Z,2020-09-22T22:18:11Z,xref #28283
705861876,36530,REF: Categorical.fillna match patterns in other methods,jbrockmendel,closed,2020-09-21T19:34:52Z,2020-09-22T22:23:11Z,"Also:

- always return a copy even if there is nothing to fill
- validate the fill_value even if there is nothing to fill"
492134439,28384,Categorical equality with NaN behaves unexpectedly,ageorgou,closed,2019-09-11T09:54:31Z,2020-09-22T23:21:38Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

cat_type = pd.CategoricalDtype(categories=[""a"", ""b"", ""c""])
s1_cat = pd.Series([""a"", ""b"", ""c""], dtype=cat_type)
s2_cat = pd.Series([np.nan, ""a"", ""b""], dtype=cat_type)
s1_cat != s2_cat  # expected all True
```
Output:
```
0    False
1     True
2     True
dtype: bool
```


#### Problem description
I would expect anything to compare `!=` to NaN.

Comparing the categorical series with `==` works as expected:
```python
s1_cat == s2_cat  # expect all False
```
Output:
```
0    False
1    False
2    False
dtype: bool
```

Element-wise comparison seems to work fine:
```python
for left, right in zip(s1_cat, s2_cat):
    print(left != right)  # expect all True
```
Output:
```
True
True
True
```

This also works as expected with other data types, e.g.:
```python
s1_int = pd.Series([1, 2, 3])
s2_int = pd.Series([np.nan, 1, 2])
s1_int != s2_int  # expect all True
```
gives
```
0    True
1    True
2    True
dtype: bool
```

Apologies if this is a duplicate; I have found various issues about categorical types and missing values, but not about comparing them in this way.

#### Expected Output
```python
s1_cat != s2_cat  # expected all True
```

```
0    True
1    True
2    True
dtype: bool
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 16.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.25.1
numpy            : 1.17.2
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.2
setuptools       : 41.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
703542734,36424,QST: Replace subset of DataFrame by another subset of DataFrame,zxdawn,closed,2020-09-17T12:29:07Z,2020-09-22T23:24:01Z,"- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

**Note**: If you'd still like to submit a question, please read [this guide](
https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your question.

```python
# Your code here, if applicable
import numpy as np
import pandas as pd
dic = {'A': [1, 4, 1, 4], 'B': [9, 2, 5, 3], 'C': [0, 0, 5, 3]}
df = pd.DataFrame(dic)
df.index = [0, 0, 1, 1]

df1 = df.copy()
df1[:] = np.nan

df1.loc[0]['A'] = df.loc[0]['A']
```

original `df`:
```
   A  B  C
0  1  9  0
0  4  2  0
1  1  5  5
1  4  3  3
```

I want to replace the subset of the copy with the subset of original `df`:
```
    A   B   C
0 1   NaN NaN
0 4   NaN NaN
1 NaN NaN NaN
1 NaN NaN NaN
```

However, it shows no change at all:
```
    A   B   C
0 NaN NaN NaN
0 NaN NaN NaN
1 NaN NaN NaN
1 NaN NaN NaN
```"
705010670,36486,add a test for loc method; check if a warning raise when replacing a …,samilAyoub,closed,2020-09-19T22:52:42Z,2020-09-22T23:24:07Z,"…subframe

- [x] closes #36424
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
358061232,22627,Series.reorder_levels docstring includes extra `axis` argument.,tschm,closed,2018-09-07T13:10:56Z,2020-09-22T23:24:18Z,"#### Code Sample, a copy-pastable example if possible

```python

    def reorder_levels(self, order):
        """"""
        Rearrange index levels using input order. May not drop or duplicate
        levels

        Parameters
        ----------
        order : list of int representing new level order.
               (reference level by number or key)
        axis : where to reorder levels

        Returns
        -------
        type of caller (new object)
        """"""
        if not isinstance(self.index, MultiIndex):  # pragma: no cover
            raise Exception('Can only reorder levels on a hierarchical axis.')

        result = self.copy()
        result.index = result.index.reorder_levels(order)
        return result


```
#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
352644692,22448,read_csv reference cites two different default delimiters,jazzlw,closed,2018-08-21T17:48:43Z,2020-09-23T01:05:57Z,"In the [reference for the pandas.read_csv function ](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html), the prototype function shows that the default value for sep is `sep=', ',` (notice the space after the comma).

In the detailed description, it says that the default is 

> `sep : str, default ‘,’`

with no space after the comma, which is how the function behaves (comma space separators lead to columns with leading whitespace).

I would have done this as a pull request but I couldn't find the source for the doc page."
705943885,36535,Regr/period range large value/issue 36430,nrebena,closed,2020-09-21T21:59:47Z,2020-09-23T10:34:58Z,"#  Checklist

- [x] closes #36430 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

# Solution

Culprit was the multiplication `unix_date * 24 * 3600 * 10**9 / factor`, for 
```
unix_date = 106752
np.log2(unix_date * 24 * 3600 * 10**9)
# 63.00000011936912
```
That probably lead to an integer overflow somewhere and the observed behaviours.

Splitting the multiplication did the trick.
"
686073187,35902,BUG: to_parquet does not accept pathlib.PosixPath if partition_cols are defined,vfilimonov,closed,2020-08-26T07:12:35Z,2020-09-23T11:20:13Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pathlib

df = pd.DataFrame({'A':[1,2,3,4], 'B':'C'})

df.to_parquet('tmp_path1.parquet')  # OK
df.to_parquet(pathlib.Path('tmp_path2.parquet'))  # OK

df.to_parquet('tmp_path3.parquet', partition_cols=['B'])  # OK
df.to_parquet(pathlib.Path('tmp_path4.parquet'), partition_cols=['B'])  # TypeError
```

#### Problem description

`to_parquet` method raises TypeError when using `pathlib.Path()` as an argument in case when `partition_cols` argument is not None. If no partition cols are provided, then `pathlib.Path()` is properly accepted

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-53-cae5a944d982> in <module>
      3 
      4 df.to_parquet('tmp_path3.parquet', partition_cols=['B']) # OK
----> 5 df.to_parquet(pathlib.Path('tmp_path4.parquet'), partition_cols=['B'])  # TypeError

~/miniconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs)
    197                 else:
    198                     kwargs[new_arg_name] = new_arg_value
--> 199             return func(*args, **kwargs)
    200 
    201         return cast(F, wrapper)

~/miniconda3/lib/python3.7/site-packages/pandas/core/frame.py in to_parquet(self, path, engine, compression, index, partition_cols, **kwargs)
   2370             index=index,
   2371             partition_cols=partition_cols,
-> 2372             **kwargs,
   2373         )
   2374 

~/miniconda3/lib/python3.7/site-packages/pandas/io/parquet.py in to_parquet(df, path, engine, compression, index, partition_cols, **kwargs)
    274         index=index,
    275         partition_cols=partition_cols,
--> 276         **kwargs,
    277     )
    278 

~/miniconda3/lib/python3.7/site-packages/pandas/io/parquet.py in write(self, df, path, compression, index, partition_cols, **kwargs)
    117                 compression=compression,
    118                 partition_cols=partition_cols,
--> 119                 **kwargs,
    120             )
    121         else:

~/miniconda3/lib/python3.7/site-packages/pyarrow/parquet.py in write_to_dataset(table, root_path, partition_cols, partition_filename_cb, filesystem, **kwargs)
   1790             subtable = pa.Table.from_pandas(subgroup, schema=subschema,
   1791                                             safe=False)
-> 1792             _mkdir_if_not_exists(fs, '/'.join([root_path, subdir]))
   1793             if partition_filename_cb:
   1794                 outfile = partition_filename_cb(keys)

TypeError: sequence item 0: expected str instance, PosixPath found
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.7.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Thu Jun 18 20:50:10 PDT 2020; root:xnu-4903.278.43~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 42.0.1.post20191125
Cython           : None
pytest           : 5.3.0
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : 1.1
pymysql          : 0.9.3
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.13.0
pandas_datareader: 0.9.0
bs4              : 4.6.3
bottleneck       : None
fsspec           : 0.6.0
fastparquet      : 0.3.2
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.1
sqlalchemy       : 1.3.13
tables           : 3.4.4
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.1.0
xlwt             : None
numba            : 0.46.0
</details>
"
707246635,36572,Backport PR #36535 on branch 1.1.x (Regr/period range large value/issue 36430),meeseeksmachine,closed,2020-09-23T10:35:46Z,2020-09-23T11:31:48Z,Backport PR #36535: Regr/period range large value/issue 36430
705624271,36523,DOC: a few sphinx fixes in release notes,simonjayhawkins,closed,2020-09-21T13:58:43Z,2020-09-23T11:37:14Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
703026382,36405,BUG: duplicated() returns rows that are not duplicated,bnojavan,closed,2020-09-16T19:43:19Z,2020-09-23T12:15:03Z,"I have a matrix of 1000*285 where 285 is my feature size which contain numerical and categorical features. The range of values are in (-15,32). When I use ""dups = df.duplicated()"" It is returning rows that are NOT duplicate."
705058248,36493,CI: Update version of 'black',MarcoGorelli,closed,2020-09-20T06:57:43Z,2020-09-23T12:17:25Z,"closes #35925 

Not many files left so am making a PR to close this off"
707305992,36574,QST: what are the semantics of the SettingWithCopyWarning?,PGryllos,closed,2020-09-23T12:10:05Z,2020-09-23T12:31:36Z,"Hi, on pandas `1.0.5` I get `SettingWithCopyWarning` when I do the following

```python
df['x'] = df['y'] - df['z']
```

1. Is the warning correct in this case? 
2. The suggestion is `Try using .loc[row_indexer,col_indexer] = value instead`

If I do something like

```
df.loc[~df.x.isna() , 'x'] = df['y'] - df['z']
```
I still get the same warning. I am not sure what is the proper way to do it?

Thanks
Prokopis
"
705044414,36491,TST: DataFrame.to_parquet accepts pathlib.Path with partition_cols defined,arw2019,closed,2020-09-20T04:44:49Z,2020-09-23T14:42:17Z,"- [x] closes #35902
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
707284260,36573,Backport PR #36523:  DOC: a few sphinx fixes in release notes,simonjayhawkins,closed,2020-09-23T11:36:25Z,2020-09-23T14:58:24Z,Backport PR #36523
706734647,36561,REF: share _reduce,jbrockmendel,closed,2020-09-22T22:35:00Z,2020-09-23T15:34:50Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
54679358,9287,Dataframe creation: Specifying dtypes with a dictionary,amelio-vazquez-reina,closed,2015-01-17T23:16:49Z,2020-09-23T16:15:29Z,"Apologies if this feature has been suggested before. Many of the IO functions (e.g. `read_csv`) allow use to easily specify the format for each column using a dictionary. As far as I understand, this is **not**
 possible with the regular dataframe construction, e.g:

``` python
df = pd.DataFrame(data=data, columns=columns, dtypes={'colname1': str, 'colname2': np.int})
```

**Even better**, it would be great if one could change the `dtypes` for the dataframe columns using a similar contruction, e.g.:

``` python
df.change_types({'colname1': str, 'colname2': np.int})
```

Is anything like this planned for already? 
"
330568641,21379,read_html - how to prevent the conversion of numerical fields,vzelen,open,2018-06-08T08:37:40Z,2020-09-23T16:58:58Z,"Hi everyone,

Is it possible to use pandas.read_html function in way so it won't convert the numbers in html tables and export them as they are (as strings)?

Assume that there is a html table which contains a value ""60,00"".

Reading that table using pandas.read_html will lead to an integer 6000.
Adding the flag _thousands='.'_ will result in a string ""60.00"".
Adding both flags _thousands='.'_ and _decimal=','_ will result in a float 60.0.

Is it possible to ask pandas.read_html to stop performing conversion of numbers by itself based on the logic of ""thousands"" and ""decimals""? Since the file may contain data in both EU and US formats. 

It would be amazing to use pandas as a tool that will just export the data from html to dataframe _as it is_ and leaves the logic of data postprocessing / conversion / etc to the further logic (which could be also implemented with a help of pandas).

A similar issue that I've described here: https://stackoverflow.com/questions/47327966/pandas-converting-numbers-to-strings-unexpected-results

Thank you a lot in advance, please let me know if it's reasonable to attach examples.
"
383358439,23853,REF/CLN: ops boilerplate,jbrockmendel,open,2018-11-22T02:31:37Z,2020-09-23T17:12:44Z,"A _ton_ of comparison and arithmetic operations do something combination of:

- [x] `other = lib.item_from_zerodim(other)`
- [ ] listlike but not arraylike --> wrap in ndarray
- [x] return `NotImplemented` if operating against a senior class
- [ ] raise `ValueError` if there is a length mismatch

But we don't _always_ do all of these, and we definitely don't use+test error messages as consistent as @gfyoung would like.

This behavior could all be collected+standardized in a decorator"
423112976,25798,TST: split up tests/frame/test_indexing.py,h-vetinari,closed,2019-03-20T07:53:39Z,2020-09-23T17:13:36Z,"Based on [review](https://github.com/pandas-dev/pandas/pull/25633#pullrequestreview-216493057) of @jreback in #25633:
> frame/test_indexing.py is actually pretty huge. so pls change according to my comments, and open an issue to split this to separate smaller files (after)"
453845993,26745,PLOT: Split plotting tests,datapythonista,closed,2019-06-08T23:39:44Z,2020-09-23T18:04:06Z,"#26414 splitted the pandas plotting code into two different modules:
- `pandas.plotting`: Generic plotting framework that defined an API (with docs) and is able to plot with a selectable backend (will be selectable with a pandas option)
- `pandas.plotting._matplotlib`: Matplotlib backend

Our tests in `pandas/tests/plotting` are currently testing both together (calling the pandas plotting framework, which will call the matplotlib backend). But ideally we'd like to have two different groups of tests:
- One with the current code but calling the matplotlib backend directly
- A test set of the generic pandas plotting framework, using a mock backend (needs to be implemented)

@jreback let me know if this makes sense to you"
705041686,36490,Commas Edited,Noahlq2657,closed,2020-09-20T04:16:21Z,2020-09-23T18:45:20Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
708595862,36623,"BUG: Categorical setitem, comparison with tuple category",jbrockmendel,closed,2020-09-25T02:39:06Z,2020-10-02T22:59:40Z,"- [x] closes #20439
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
713253453,36797,DOC: remove outdated doc closes #31487,jbrockmendel,closed,2020-10-01T23:45:15Z,2020-10-02T23:00:15Z,"- [x] closes #31487
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
709585587,36674,CLN: cleanups in DataFrame._reduce,jbrockmendel,closed,2020-09-26T18:18:46Z,2020-10-02T23:00:37Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Broken off from non-CLN work in the vicinity."
684045013,35858,BUG: Datetime MultiIndex Regression,matthewgilbert,closed,2020-08-22T18:48:49Z,2020-10-02T23:02:46Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas

date = pandas.Timestamp(""2000"")
x = pandas.DataFrame([
    [""a"", date, 1],
], columns=[""a"", ""b"", ""c""]).set_index([""a"", ""b""])[""c""]
x.loc[:, [date]]


InvalidIndexError: [Timestamp('2000-01-01 00:00:00')]
```

#### Problem description

The problem stems from `pandas.DatetimeIndex.get_loc` raising a `InvalidIndexError` in `1.*` instead of a `TypeError` as in `0.25.3`

#### 0.25.3

```python                                                                                                                                                                                        
import pandas
level_index = pandas.DatetimeIndex(['2000-01-01'], dtype='datetime64[ns]', name='b', freq=None)
key = [pandas.Timestamp('2000-01-01 00:00:00')]
level_index.get_loc(key)

...
TypeError: Cannot convert input [[Timestamp('2000-01-01 00:00:00')]] of type <class 'list'> to Timestamp
```

#### 1.1.0

```python                                                                                                                                                                                        
import pandas
level_index = pandas.DatetimeIndex(['2000-01-01'], dtype='datetime64[ns]', name='b', freq=None)
key = [pandas.Timestamp('2000-01-01 00:00:00')]
level_index.get_loc(key)

...
InvalidIndexError: [Timestamp('2000-01-01 00:00:00')]
```

Previously the `TypeError` was handled by https://github.com/pandas-dev/pandas/blob/23b1717a14f185e3512607d33cdba17bfddfd064/pandas/core/indexing.py#L1078

#### Expected Output

I would expect this to behave similar to `0.25.3` which is valid indexing synatx.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.6.11.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-42-generic
Version          : #46~18.04.1-Ubuntu SMP Fri Jul 10 07:21:24 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.16.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200814
Cython           : 0.29.21
pytest           : 5.3.5
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.12.0-RAY
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.1
sqlalchemy       : 1.3.19
tables           : 3.5.1
tabulate         : None
xarray           : 0.15.0
xlrd             : None
xlwt             : None
numba            : None



</details>
"
705005955,36484,ENH: PandasArray ops use core.ops functions,jbrockmendel,closed,2020-09-19T22:10:39Z,2020-10-02T23:06:30Z,"Needs tests, hopefully i can tweak fixture usage in tests.arithmetic"
269090134,18000,BENCH: Index.take benchmark is measuring wrong thing,jorisvandenbossche,closed,2017-10-27T12:28:20Z,2020-10-02T23:08:40Z,"The `take` benchmarks `indexing.IndexingMethods.time_take_intindex` is benchmarking `take` on a boolean list, which take interprets as a [0, 1, 0, 1, ...] values, which is a bit silly to benchmark. 
We should add some actual benchmark on integers instead."
93609154,10526,"read_json from url, Accept Header?",maxnoe,closed,2015-07-07T19:28:17Z,2020-10-02T23:24:44Z,"Today I tried to read json data from an url that checks the Accept-Header for 'application/json',
and only delivers json if this tag is higher ranked than 'text/html'.

See: [Flask: Handling Accept Headers](http://flask.pocoo.org/snippets/45/)
It seems, that the pandas request in '_url_open' has no such header at all.

This works:

``` python
import requests
import pandas
url = ""http://localhost:5000/foo""
r = requests.get(url, headers={'Accept': 'application/json'})
data = pandas.DataFrame(r.json())
print(data.head())
```

This does not, and I checked the header in the flask webapp of the received request, it is empty:

``` python
import pandas
url = ""http://localhost:5000/foo""
data = pandas.read_json(url)
print(data.head())
```

It would make sense if the `read_json` had `application/json` in its Accept-Header, correct?
"
713233554,36794,BUG: Series.groupby.rolling duplicates  index when grouping over index,phofl,closed,2020-10-01T22:50:39Z,2020-10-02T23:48:20Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
data = {
    'groupby_col': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', ],
    'agg_col': [1, 1, 0, 1, 0, 0, 0, 0, 1, 0],
}
df = pd.DataFrame(data).set_index(""groupby_col"")
grouped = df.groupby('groupby_col')
rolled = grouped.rolling(4)

result = rolled.mean()
print(result)

```
Output:
```
                         agg_col
groupby_col groupby_col         
A           A                NaN
            A                NaN
            A                NaN
            A               0.75
            A               0.50
B           B                NaN
            B                NaN
            B                NaN
            B               0.25
            B               0.25
```

#### Problem description

The duplicate ``groupby_col`` seems at least odd.

...

#### Expected Output

Would expect, that we get ``groupby_col`` only once in the index and a ``Series`` instead of a ``DataFrame``

Maybe related to #36507, but

```
data = {
    'groupby_col': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', ],
    'agg_col': [1, 1, 0, 1, 0, 0, 0, 0, 1, 0],
}
df = pd.DataFrame(data).set_index(""groupby_col"")
grouped = df.groupby('groupby_col')
result = grouped.apply(sum)
print(result)
```
works. So I am not sure.

#### Output of ``pd.show_versions()``

<details>

master

</details>
"
710678530,36718,EA: Tighten signature on DatetimeArray._from_sequence,jbrockmendel,closed,2020-09-29T01:22:42Z,2020-10-03T00:26:03Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref #33254

There are a few more steps to get DatetimeArray._from_sequence down to the ideal behavior, this is just trimming the signature down to what its supposed to be.

_from_sequence_not_strict could use a better name"
619709850,34225,BUG: Use of PeriodIndex with rolling transform aggregating function into cumulative aggregating?,yohplala,closed,2020-05-17T13:56:33Z,2020-10-03T00:59:34Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

---
#### Code Sample, a copy-pastable example

```python
import pandas as pd
start = '2020-01-01 08:00'
end = '2020-01-01 12:00'
intervals = pd.period_range(start=start, end=end, freq = '30T')
values = [i for i in range(0, len(intervals))]
ser = pd.Series(values, index = intervals)
offset = pd.tseries.frequencies.to_offset('1h')

test_min = ser.rolling(window = offset, closed='left').min()
test_sum = ser.rolling(window = offset, closed='left').sum()
test_max = ser.rolling(window = offset, closed='left').max()
```
#### Output
```python
# Problem 1
test_min
```
```python
2020-01-01 08:00    NaN
2020-01-01 08:30    0.0
2020-01-01 09:00    0.0
2020-01-01 09:30    0.0
2020-01-01 10:00    0.0
2020-01-01 10:30    0.0
2020-01-01 11:00    0.0
2020-01-01 11:30    0.0
2020-01-01 12:00    0.0
Freq: 30T, dtype: float64
```
```python
# Problem 1
test_sum
```
```python
2020-01-01 08:00    NaN
2020-01-01 08:30    0.0
2020-01-01 09:00    1.0
2020-01-01 09:30    3.0
2020-01-01 10:00    6.0
2020-01-01 10:30   10.0
2020-01-01 11:00   15.0
2020-01-01 11:30   21.0
2020-01-01 12:00   28.0
Freq: 30T, dtype: float64
```
```python
# Problem 2
test_max
```
```python
2020-01-01 08:00    NaN
2020-01-01 08:30    0.0
2020-01-01 09:00    1.0
2020-01-01 09:30    2.0
2020-01-01 10:00    3.0
2020-01-01 10:30    4.0
2020-01-01 11:00    5.0
2020-01-01 11:30    6.0
2020-01-01 12:00    7.0
Freq: 30T, dtype: float64
```
#### Problem descriptions
There are 2 problems:
  * problem 1:
     * min value on a `1H` window is not 0 given the input provided (see expected output)
     * 2nd example with sum helps understand what happens with min: rolling is actually operating cumulated sum and cumulated min?!
  * problem 2: the 0 at the second row in `test_max` shows that rolling with closed = 'left' is not able to handle PeriodIndex correctly. It seems it does not know that periods are themselves bins with a specific close parameter to be considered.

#### Expected Output
```python
# Problem 1
test_min
```
```python
2020-01-01 08:00    NaN
2020-01-01 08:30    0.0
2020-01-01 09:00    1.0
2020-01-01 09:30    2.0
2020-01-01 10:00    3.0
2020-01-01 10:30    4.0
2020-01-01 11:00    5.0
2020-01-01 11:30    6.0
2020-01-01 12:00    7.0
Freq: 30T, dtype: float64
```
```python
# Problem 1
test_sum
```
```python
2020-01-01 08:00    NaN
2020-01-01 08:30    1.0
2020-01-01 09:00    3.0
2020-01-01 09:30    5.0
2020-01-01 10:00    7.0
2020-01-01 10:30    9.0
2020-01-01 11:00   11.0
2020-01-01 11:30   13.0
2020-01-01 12:00   15.0
Freq: 30T, dtype: float64
```
```python
# Problem 2
test_max
```
```python
2020-01-01 08:00    NaN
2020-01-01 08:30    1.0
2020-01-01 09:00    2.0
2020-01-01 09:30    3.0
2020-01-01 10:00    4.0
2020-01-01 10:30    5.0
2020-01-01 11:00    6.0
2020-01-01 11:30    7.0
2020-01-01 12:00    8.0
Freq: 30T, dtype: float64
```
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-51-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : fr_FR.UTF-8
LOCALE           : fr_FR.UTF-8
pandas           : 1.0.3
numpy            : 1.16.3
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.2.0.post20200511
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.0.3
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.3.3
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.0.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
713242885,36795,DEPR: automatic alignment on frame.__cmp__(series),jbrockmendel,closed,2020-10-01T23:14:45Z,2020-10-03T01:16:39Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

xref #28759, may close it depending on if we want to do anything else on that front"
709421104,36656,ASV: used integer ndarray as indexer for Series.take indexing benchmark,hardikpnsp,closed,2020-09-26T04:19:56Z,2020-10-03T01:42:33Z,"- [x] closes #18000
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Series.take was using a list of booleans as indexers. Now, It is replaced with a random list of integers.
Here are the benchmark results with `asv run --bench indexing.Take`
### Benchmarks Results:

#### Before:

```
[100.00%] ··· indexing.Take.time_take                                                                                                                                                ok
[100.00%] ··· ========== =============
                index                 
              ---------- -------------
                 int      4.13±0.08ms 
               datetime   4.13±0.01ms 
              ========== =============
```

#### After:

```
[100.00%] ··· indexing.Take.time_take                                                                                                                                                ok
[100.00%] ··· ========== =============
                index                 
              ---------- -------------
                 int      6.36±0.08ms 
               datetime   6.29±0.03ms 
              ========== =============
```"
712524045,36763,DOC: Fix PR09 errors in several files,Iqrar99,closed,2020-10-01T06:13:25Z,2020-10-03T01:43:39Z,"- [x] closes #36764
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

As far as I can see, there are some PR09 pandas docstring errors. So with this PR I will resolve them. There are more commits soon.
```
PR09
Parameter description should finish with "".""
```"
713853919,36816,Rolling on DataFrameGroupBy duplicated index column when part of the grouping cols is from index,phofl,closed,2020-10-02T19:29:15Z,2020-10-03T02:26:21Z,"…grouping cols is from index

- [x] closes #36794
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Could not find a elegant way, to avoid adding the components in the for loop, so I dropped it from the resulting MultiIndex again.

Had to change a test, which depended on the wrong behavior.

cc @mroeschke "
559782259,31652,BUG: avoid specifying default coerce_timestamps in to_parquet,jorisvandenbossche,closed,2020-02-04T15:09:16Z,2020-10-03T05:07:49Z,"Looking into the usage question of https://github.com/pandas-dev/pandas/issues/31572, I noticed that specifying the version to allow writing nanoseconds to parquet worked in plain pyarrow code, but not with pandas' `to_parquet`. 
This is because we hardcode `coerce_timestamps=""ms""` while the default is `None`, which has version-dependent behaviour (eg if version=""2.0"", actually write the nanosecond data)"
712951294,36778,ENH: Implement FloatingArray reductions,dsaxton,closed,2020-10-01T15:28:36Z,2020-10-03T05:18:31Z,"Mostly copy / paste from https://github.com/pandas-dev/pandas/pull/36761

@jorisvandenbossche Would this require a whatsnew note? Seems the current v1.2.0 note is fairly high level and doesn't talk about specific methods that are supported. Should we talk about this or leave as-is?"
709753842,36683,bump pytables to 3.5.1 #24839,fangchenli,closed,2020-09-27T14:49:33Z,2020-10-03T05:35:12Z,"- [x] closes #24839
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

PyTables 3.5.1 was released on March 14, 2019. And it offers better support for np16.
"
709724310,36681,CI: debug file leak,fangchenli,closed,2020-09-27T11:46:12Z,2020-10-03T05:35:51Z,"Debug file leaks in cov build. Unrelated CI builds are commented out to save resource.
"
708383500,36607,CI: rerun flaky test in html #36467,fangchenli,closed,2020-09-24T18:24:54Z,2020-10-03T05:36:33Z,"Part of #36467
"
710577109,36717,CLN: use standard method to set unhashable object,fangchenli,closed,2020-09-28T21:04:22Z,2020-10-03T05:37:11Z,"#20589 mentioned lgtm. So I decided to test it. And it found this potential improvement. There is even a detailed reference with code examples. https://lgtm.com/rules/1780095/ Very interesting.

Just trying to see if their suggestion works.
"
710738944,36720,CLN: clean clipboard,fangchenli,closed,2020-09-29T03:52:17Z,2020-10-03T05:38:07Z,"
"
705248999,36514,CI/CLN: update travis,fangchenli,closed,2020-09-21T02:55:11Z,2020-10-03T05:41:40Z,"update and cleanup Travis 

closes #36601 
"
707650154,36584,TST: 32bit dtype compat #36579,fangchenli,closed,2020-09-23T20:14:59Z,2020-10-03T05:42:18Z,"Part of #36579

"
710432584,36707,"CI, CLN remove unnecessary noqa statements, add CI check",MarcoGorelli,closed,2020-09-28T17:00:05Z,2020-10-03T06:20:20Z,"- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

This changes loads of files, but most changes are quite trivial"
627563982,34466,BUG: Rolling count aggregation produces unexpected results,brandon-b-miller,open,2020-05-29T21:49:32Z,2020-10-03T06:31:40Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

```python
>>> pd.__version__
'1.1.0.dev0+1690.g70d7c04ff'
>>> pd.Series([1,1,1,None]).rolling(2, min_periods=2, center=True).count()
0    NaN
1    2.0
2    2.0
3    1.0
dtype: float64
```

#### Problem description

Apologies if this is expected behavior and not a bug. I noticed that prior to version 1.0, `rolling.count` ignored the `min_periods` parameter, as discussed in https://github.com/pandas-dev/pandas/pull/30923. However I'm having trouble understanding this output. Shouldn't the last element of the output be `NaN`, given that the final window includes the last and second to last elements, of which only one is valid?

#### Expected Output

```
0    NaN
1    2.0
2    2.0
3    NaN
dtype: float64
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 70d7c04ff585de361622e4fe1788480a7a4526b5
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-76-generic
Version          : #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0.dev0+1690.g70d7c04ff
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.4.0.post20200518
Cython           : 0.29.17
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
</details>
"
700341342,36310,REF: Back IntervalArray by array instead of Index,jbrockmendel,closed,2020-09-12T19:04:34Z,2020-10-03T07:21:22Z,"The benefit I have in mind here is that we could back it by a single 2xN array and a) avoid the kludge needed to make `__setitem__` atomic, b) do a view to get native types for e.g uniqueness checks, c) possibly share some methods with NDarrayBackedExtensionArray.

Also just in principle having EAs not depend on Index is preferable dependency-structure-wise.

cc @jschendel "
709601518,36675,REGR: Series.loc with a MultiIndex containing Timestamp raises InvalidIndexError,simonjayhawkins,closed,2020-09-26T19:59:34Z,2020-10-03T09:51:25Z,"ci testing fix in https://github.com/pandas-dev/pandas/issues/35858#issuecomment-678683559

- [ ] closes #35858
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
713944348,36818,Backport PR #36675 on branch 1.1.x (REGR: Series.loc with a MultiIndex containing Timestamp raises InvalidIndexError),meeseeksmachine,closed,2020-10-02T23:02:59Z,2020-10-03T11:50:24Z,Backport PR #36675: REGR: Series.loc with a MultiIndex containing Timestamp raises InvalidIndexError
714063264,36830,"QST: while trying to convert categorical variables which are ordinal , i ran the following code",senthilguna79,closed,2020-10-03T10:50:58Z,2020-10-03T12:17:16Z,"- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

**Note**: If you'd still like to submit a question, please read [this guide](
https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your question.

```python
# Your code here, if applicable

```
"
714061402,36829,DOC: update code style for remaining intro tutorial docs for #36777 computation.rst,Mikhaylov-yv,closed,2020-10-03T10:39:00Z,2020-10-03T14:00:04Z,
714075084,36832,DOC: update code style for remaining intro tutorial docs for #36777 computation.rst,Mikhaylov-yv,closed,2020-10-03T12:01:16Z,2020-10-03T14:00:50Z,update code style computation.rst  for #36777
714018864,36825,DOC: Fix extending.rst code style #36777,aniaan,closed,2020-10-03T06:31:44Z,2020-10-03T14:48:26Z,"ref #36777
I tried to format development/extending.rst, please review
cc @dsaxton "
714099055,36835,DOC: update code style for remaining intro tutorial docs for #36777 computation.rst,Mikhaylov-yv,closed,2020-10-03T14:12:21Z,2020-10-03T14:59:04Z,"…omputation.rst #36832

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
713954607,36821,DOC: update code style for development doc and user guide #36777,karasinski,closed,2020-10-02T23:39:23Z,2020-10-03T15:00:04Z,"Addresses part of #36777
Ran blacken-tools and checked for warnings from flake8-rst for 4 additional doc files."
713960179,36823,DOC: update code style for user guide for #36777,karasinski,closed,2020-10-03T00:03:01Z,2020-10-03T15:00:39Z,"Addresses part of #36777
Ran blacken-tools and checked for warnings from flake8-rst for 5 additional doc files."
501691148,28759,API: ops alignment behavior inconsistencies,jbrockmendel,closed,2019-10-02T19:50:49Z,2020-10-03T16:19:53Z,"In both Series and DataFrame ops, we have inconsistent behavior for when we call `self.align` vs when we raise.  Everything discussed here is for non-flex ops.

Case 1: consider `op(ser1, ser2)` for two Series with non-matching indexes.
- arithmetic ops call `self.align(other)`
- comparison ops raise `ValueError(""Can only compare identically-labeled Series objects"")`
- logical ops call `self.align(other)`

Case 2: consider `op(df1, df2)` for two DataFrames with non-matching axes
- arithmetic ops call `self.align(other)`
- comparison ops raise `ValueError(""Can only compare identically-labeled DataFrame objects"")`
- logical ops call `self.align(other)`

Case 3) consider `op(df, ser)`.  This always aligns, with comparison not being treated differently from the other two.

The policy (and code) would be simpler if we changed this so that either:
a) the comparison op in case 3 doesn't align, matching cases 1 and 2
b) comparison ops always align, matching arithmetic and logical ops"
708115174,36600,CI: silence codecov for unrelated lines,erfannariman,closed,2020-09-24T12:07:45Z,2020-10-03T17:33:56Z,
697701176,36265,Disabling ORC Support in pyarrow conda packages,xhochy,closed,2020-09-10T09:55:29Z,2020-10-03T20:00:01Z,"I'm fed up dealing with the build issues from the ORC C++ conda package and consider just disabling the support of it in the `pyarrow` conda package.

Main discussion should/can happen in the dask issue tracker as they seem to have the largest user base of this feature: https://github.com/dask/dask/issues/6615 There I also posted what needs to be done.

This issue is just a FYI, feel free to close anytime."
371563696,23223,[WIP] Add basic ExtensionIndex class,jorisvandenbossche,closed,2018-10-18T14:38:49Z,2020-10-03T22:17:00Z,"Explored this a bit 2 weeks ago, so thought could open it as a WIP PR in case it might serve discussion.

For me, the main question is how ""complete"" we want this to be, before we consider merging it. This WIP certainly already is able to preserve the EA in the Index (not convert to objects), and basic indexing works (but only tested the basics, and not all combinations of uniques/duplicated, sorted/non-sorted ... indexes)

Closes https://github.com/pandas-dev/pandas/issues/22861"
30436674,6732,groupby().first() skips NaN values,jwkvam,closed,2014-03-29T03:50:09Z,2020-10-04T00:39:33Z,"I do this

```
>>> df = pd.DataFrame([[1,np.nan,0],[1,1,1],[2,2,2],[2,3,3]], columns=list('abc'))
>>> print(df)
   a   b  c
0  1 NaN  0
1  1   1  1
2  2   2  2
3  2   3  3

[4 rows x 3 columns]
>>> print(df.groupby('a').first())
   b  c
a      
1  1  0
2  2  2

[2 rows x 2 columns]
```

but I expected this

```
>>> print(df.groupby('a').first())
     b  c
a      
1  NaN  0
2    2  2

[2 rows x 2 columns]
```

Is it possible to achieve my expected output? I get the same output in master and 0.13.1.
"
714218023,36846,Update documentation,trishitapingolia,closed,2020-10-04T02:40:08Z,2020-10-04T03:18:19Z,"Capitalization error

"
714200493,36844,DOC: Typo fix,lrjball,closed,2020-10-04T00:09:36Z,2020-10-04T03:27:58Z,"Noticed a minor typo when using the docs
"
495402718,28507,COMPAT: tzawareness check behavior is different from stdlib,jbrockmendel,closed,2019-09-18T19:05:05Z,2020-10-04T04:47:47Z,"`Timestamp` comparisons raise TypeError in some cases where `datetime`s do not:

```
ts = pd.Timestamp.now()
ts2 = ts.tz_localize(""UTC"")
dt = ts.to_pydatetime()
dt2 = ts2.to_pydatetime()

>>> ts == ts2   # <-- TypeError
>>> dt == dt2
False
```

The datetime comparison _will_ raise for inequalities, just not `==` or `!=`.

Changing this behavior would be an API change, but it would likely simplify a bunch of headaches (including a bug in array_equivalent)"
598261974,33478,CLN remove unreachable code in pandas/core/groupby/generic.py::DataFrameGroupBy::_wrap_applied_output,MarcoGorelli,closed,2020-04-11T13:02:10Z,2020-10-04T06:43:35Z,"NOTE: I'd originally [opened a PR](https://github.com/pandas-dev/pandas/pull/32583) to address this, but am now busy with other obligations + other PRs to respond to. So I'm opening it up as a good first issue for now, and will return to work on it if nobody takes it.

---------

In `pandas/core/groupby/generic.py::DataFrameGroupBy::_wrap_applied_output`, it seems this branch is never reached:

```
        else:
            # Handle cases like BinGrouper
            return self._concat_objects(keys, values, not_indexed_same=not_indexed_same)
```

Two things need to be done here:

1. refactor `_wrap_applied_output` by making a helper function out of some of its internals (it's currently very long, and changes are hard to review)
2. Remove this unused `else` branch"
520223655,29493,TYPING: Enable --check-untyped-defs for MyPy,simonjayhawkins,closed,2019-11-08T20:40:00Z,2020-10-04T08:09:58Z,"- [ ] closes #27568

I'll open a new issue for resolution of mypy errors per module, see https://gitter.im/pydata/pandas?at=5dc5cdf2091dd14a0e86103a and #28926 for process used for test modules.

cc @WillAyd @jbrockmendel "
708549398,36614,QST: Why have my data values been replaced by “NaN” after using qcut?,feryah,closed,2020-09-25T00:15:19Z,2020-10-04T10:34:43Z,"- [ ] I have searched the [[pandas] cut, qcut](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ https://stackoverflow.com/questions/63929143/why-have-my-data-values-been-replaced-by-nan-after-using-qcut] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

**Note**: If you'd still like to submit a question, please read [this guide](
https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your question.


I am working with pandas dataframe of 9000 rows and 6 columns. At this point, I am trying to convert the continuous variable 'Experience' years of a job into the categorical variable 'Level' of expertise (beginner - intermediate - advanced - expert) for each of the 4 jobs (Commercial Manager - Business Developer - Web Marketer - Traffic Manager).

Giving that years experience range are not the same for each job, I utilised ""qcut"" to divide data into 4 groups as follows:

(You can run the code below to get the dataframe sample)
```python
import pandas as pd


df = pd.DataFrame({'Job': ['Commercial Manager', 'Traffic Manager', 'Web Marketer', 'Commercial Manager', 'Commercial Manager', 'Web Marketer', 'Commercial Manager', 'Commercial Manager', 'Traffic Manager', 'Business Developer', 'Business Developer', 'Web Marketer', 'Traffic Manager', 'Traffic Manager', 'Commercial Manager', 'Business Developer', 'Traffic Manager', 'Commercial Manager', 'Business Developer', 'Business Developer', 'Web Marketer'], 
                   'Experience': [1.00000, 3.00000, 3.00000, 1.50000, 2.00000, 6.00000, 0.00000, 4.00000, 8.00000, 5.00000, 0.50000, 3.00000, 3.00000, 0.00000, 2.00000, 3.00000, 0.50000, 3.00000, 3.00000, 8.00000, 3.50000]})


levels = [""beginner"", ""intermediate"", ""advanced"", ""expert""]
jobs = [""Commercial Manager"", ""Business Developer"", ""Web Marketer"", ""Traffic Manager""]


def convert(levels, jobs):
  for j in jobs:
    df[""Level""] = pd.qcut(df.loc[df[""Job""] == j, ""Experience""].rank(method=""first""), q = 4, labels = levels, duplicates = ""drop"")
  return df

convert(levels, jobs)

```

It appears that it only worked for ""Traffic Manager"" and it replaced the other 'level' experience with NaN. I am really lost. Any help please?"
714276036,36856,DOC: blacken-docs doc/source/ecosystem.rst,meghanacosmos,closed,2020-10-04T09:41:10Z,2020-10-04T11:45:52Z,"- [x] xref #36777 
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
709047094,36638,PERF: fix long string representation,ivanovmg,closed,2020-09-25T15:55:36Z,2020-10-04T13:21:53Z,"- [x] closes #36636
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

- Fix long string representation for large dataframes.
- Eliminate for loop, which was filtering out the proper rows/columns to be displayed.
- Revert to the original implementation with concat-ing head+tail and left+right parts."
711246051,36726,CLN: private funcs in concat.py,ivanovmg,closed,2020-09-29T15:51:03Z,2020-10-04T13:22:33Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Refactor/cleanup ``_get_empty_dtype_and_na`` in ``pandas/core/internals/concat.py``
Extract functions, add typing.
"
707462683,36576,REF: refactor/cleanup of CSSToExcelConverter,ivanovmg,closed,2020-09-23T15:23:03Z,2020-10-04T13:24:27Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Refactor/clean up of ``CSSToExcelConverter`` in module ``pandas.io.formats.excel``.

- Move class variables to the top of the class
- Add font family mapping
- Extract methods
- Add missing typing
- Make color parsing cleaner"
707577767,36581,REF: refactor/cleanup CSSResolver,ivanovmg,closed,2020-09-23T18:17:15Z,2020-10-04T13:24:32Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Refactor ``__call__`` method of ``CSSResolver``.
- Extract methods
- Reorder class attributes
- Add type annotations"
710514630,36714,REF: rearrange test_to_latex.py,ivanovmg,closed,2020-09-28T19:18:21Z,2020-10-04T13:25:48Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Rearranged tests in ``test_to_latex.py`` to the corresponding classes.
Sorry for the messy diffs.
All I did (except to small commits cd92db9 and 2b66e2f) was rearrangement of the tests."
709361780,36647,ENH: match stdlib behavior for datetimelike comparisons,jbrockmendel,closed,2020-09-26T00:03:10Z,2020-10-04T18:05:57Z,"- [x] closes #28507
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
714375771,36866,BUG: rolling.apply substitutes for applied function,shaunc,closed,2020-10-04T18:40:45Z,2020-10-04T18:44:32Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python

import datetime
import pandas as pd
import numpy as np
index = pd.date_range(datetime.datetime.now(), periods=5, freq=""1s"")
ser = pd.Series(1, index=index)
ser.rolling(""2s"").apply(np.sum, kwargs={""initial"": 5})
```
results in:
```
ValueError: the 'initial' parameter is not supported in the pandas implementation of sum()
```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

When I explicitly specify the numpy version of sum, pandas should not substitute it for me.

#### Expected Output

Sums in window with 5 added.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.8.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Mon Aug 31 22:12:52 PDT 2020; root:xnu-6153.141.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.2
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
572451978,32316,DOC: Standardize references to pandas in the documentation,mroeschke,closed,2020-02-28T00:54:20Z,2020-10-04T20:13:32Z,"In our documentation (files in `doc/source/../*.rst`), we refer to pandas as `Pandas`, `pandas`, and `*pandas*` (sphinx italics).

For documentation consistency, we should standardize these to one form. `pandas` is the preferable form in our documentation files.

Unfortunately I don't think we create a lint rule for this pattern since we have to be flexible to references and hyperlinks."
714241114,36851,DOC: Standardize references to pandas in the documentation for #32316,Mikhaylov-yv,closed,2020-10-04T05:49:02Z,2020-10-04T21:05:49Z,"
Standardize references to pandas in the documentation for #32316"
714066730,36831,DOC: reformat doc groupby.rst,erfannariman,closed,2020-10-03T11:12:18Z,2020-10-04T22:33:14Z,"ref #36777 
"
714203723,36845,"DOC: normalize usage of word ""pandas""",LeviMatus,closed,2020-10-04T00:34:52Z,2020-10-04T23:16:35Z,"changes references to the library, pandas, to match the standard lowercase spelling. This changes applicable `.rst` files under the `doc/source` path.

References such as Pandas, **pandas**, *pandas*, and ``pandas`` have been replaced with pandas.

- [x] closes #32316 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
687700309,35943,CLN: resolve DeprecationWarning in `pandas/_testing.py` #35942,fangchenli,closed,2020-08-28T04:20:47Z,2020-10-05T00:16:32Z,"- [x] closes #35942
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
374089303,23335,'closed' parameter is not working for Rolling with duplicate Timestamp ,jiehuan,closed,2018-10-25T19:06:34Z,2020-10-05T00:41:02Z,"The 'closed' parameter is not really working when the date is duplicated.

Here is an example.

```Python
import pandas as pd

df = pd.DataFrame({'Date': ['2016-06-30', '2016-06-30', '2016-08-09'],
                   'amount': [100, 200, 10]
                  })

df['Date'] = pd.to_datetime(df['Date'])

df
```

|      |  Data| amount|
|----|--------|--------|
|0	|2016-06-30	|100|
|1	|2016-06-30	|200|
|2	|2016-08-09	|10|




```Python
df.rolling(window='90d', on='Date', closed='neither').sum()
```

Output

|      |  Data| amount|
|----|--------|--------|
|0	|2016-06-30	|NaN|
|1	|2016-06-30	|100|
|2	|2016-08-09	|300|

> 

Expected output:

|      |  Data| amount|
|----|--------|--------|
|0	|2016-06-30	|NaN|
|1	|2016-06-30	|NaN |
|2	|2016-08-09	|300|

Output of `pd.show_versions()`
<details>
 <summary>show_version</summary>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.0.final.0
python-bits: 64
OS: Darwin
OS-release: 17.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: en_US.UTF-8
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: None
pip: 18.1
setuptools: 40.4.3
Cython: None
numpy: 1.15.2
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 7.0.1
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>"
391370801,24295,Elegant way to generate indexable sliding window timeseries,spacegoing,closed,2018-12-15T11:44:22Z,2020-10-05T00:56:41Z,"To implement pytorch's `DataSet` class `__get_item__()` method, it requires to support the indexing such that `dataset[i]` can be used to get `ith` sample.

Say I have  a time-series `ser`:

    2017-12-29 14:44:00  69.90
    2017-12-29 14:45:00  69.91
    2017-12-29 14:46:00  69.87
    2017-12-29 14:47:00  69.85
    2017-12-29 14:48:00  69.86
    2017-12-29 14:49:00  69.92
    2017-12-29 14:50:00  69.90
    2017-12-29 14:51:00  70.00
    2017-12-29 14:52:00  69.97
    2017-12-29 14:53:00  69.99
    2017-12-29 14:54:00  69.99
    2017-12-29 14:55:00  69.85 

Since I need to index into the rolling window. I  generate a window length `3` time-series by using:

    l3_list = list()
    def t(x):
      l3_list.append(x.copy())
    ser.rolling(3).apply(t)

`l3_list` becomes:

    [array([69.9 , 69.91, 69.87]),
     array([69.91, 69.87, 69.85]),
     array([69.87, 69.85, 69.86]),
     array([69.85, 69.86, 69.92]),
     array([69.86, 69.92, 69.9 ]),
     array([69.92, 69.9 , 70.  ]),
     array([69.9 , 70.  , 69.97]),
     array([70.  , 69.97, 69.99]),
     array([69.97, 69.99, 69.99]),
     array([69.99, 69.99, 69.85])]

So that I can index in l3_list. Namely `l3_list[i]` is the `ith` sliding window. Is there a more memory efficient way to do this?"
419987601,25689,rolling apply gives different output when raw = True or raw = False,NGJROMO,closed,2019-03-12T13:31:22Z,2020-10-05T01:00:24Z,"Hi,
I am working with pandas.Series.apply() and got some unexpected output. I created the following  example code (I could not manage to reproduce the issue with any other values, maybe there is something that I am not seeing):

```python
import pandas as pd
import numpy as np

data=pd.Series([12.74,
 12.99,
 12.81,
 12.7,
 11.46,
 10.91,
 11.05,
 10.71,
 11.29,
 11.73,
 7.42,
 6.51,
 5.12,
 3.9,
 4.62,
 3.97,
 4.48,
 4.09,
 4.68,
 4.68,
 3.59,
 2.54,
 2.03,
 2.52,
 1.98,
 0.8,
 1.47,
 1.49,
 0.38,
 0.44,
 0.93,
 1.23,
 1.62,
 1.28,
 2.79,
 2.88,
 4.28,
 1.74,
 3.31,
 3.32,
 4.23,
 3.96,
 4.15,
 2.81,
 4.21,
 4.5,
 4.22,
 1.34,
 2.26,
 1.72,
 1.52,
 3.54,
 2.43,
 2.61,
 4.48,
 5.3,
 5.9,
 6.55,
 7.37,
 8.27,
 6.97,
 8.81,
 8.45,
 8.09,
 7.02,
 8.69,
 8.15,
 7.41,
 8.61,
 9.02,
 9.44,
 10.28,
 9.0,
 7.87,
 8.08,
 7.81,
 6.41,
 6.02,
 7.04,
 5.7,
 6.2,
 5.96,
 6.28,
 6.81,
 6.66,
 6.17,
 6.92,
 7.92,
 5.78,
 7.33,
 6.9,
 6.36,
 8.24,
 7.41,
 8.96,
 10.55,
 11.43,
 12.44,
 13.78,
 13.44,
 14.23,
 11.71,
 12.36,
 13.41,
 13.15,
 14.76,
 14.23,
 14.75,
 13.76,
 14.1,
 13.63,
 12.5,
 11.3,
 11.48,
 12.9,
 10.71,
 10.62,
 10.33,
 10.29,
 10.4,
 11.64,
 12.92,
 12.79,
 11.48,
 11.2,
 10.79,
 10.04,
 10.51,
 11.2,
 12.88,
 11.72,
 11.98,
 12.57,
 13.37,
 13.15,
 13.5,
 12.79,
 14.6,
 15.36,
 15.64,
 17.39,
 16.43,
 16.82,
 18.15,
 18.48,
 19.12,
 20.47,
 20.8,
 19.66,
 20.95,
 20.81,
 21.63,
 15.88,
 17.17,
 17.9,
 18.41,
 17.3,
 17.66,
 17.71,
 18.31,
 17.55,
 17.28,
 17.36,
 15.46,
 15.15,
 14.77,
 14.79,
 15.18,
 15.2,
 14.05,
 13.91,
 14.24,
 13.73,
 14.81,
 16.61,
 17.77,
 18.17,
 18.49,
 18.23,
 19.34,
 18.85,
 18.73,
 19.83,
 21.43,
 22.28,
 20.27,
 19.81,
 20.11,
 20.25,
 18.82,
 20.73,
 20.03,
 19.91,
 18.9,
 18.71,
 18.8,
 17.33,
 19.98,
 18.76,
 19.71,
 19.56,
 20.26,
 20.89,
 19.41,
 19.82,
 19.5,
 18.26,
 17.92,
 17.58,
 17.05,
 17.5,
 15.32,
 16.44,
 17.82,
 18.72,
 19.83,
 19.26,
 19.4,
 18.51,
 18.83,
 18.02,
 18.37,
 17.49,
 17.78,
 16.5,
 17.6,
 18.3,
 19.43,
 17.78,
 17.43,
 18.19,
 16.8,
 16.59,
 16.27,
 16.91,
 15.98,
 15.08,
 13.93,
 14.44,
 14.72,
 16.96,
 15.98,
 14.68,
 13.85,
 12.35,
 10.25,
 11.88,
 12.16,
 11.68,
 13.28,
 11.78,
 11.36,
 11.15,
 9.76,
 9.72,
 8.64,
 6.99,
 6.82,
 6.07,
 6.53,
 6.45,
 7.12,
 6.94,
 6.33,
 5.46,
 5.46,
 2.63,
 2.63,
 1.89,
 2.45,
 3.29,
 3.73,
 5.43,
 3.98,
 4.53,
 5.46,
 5.57,
 4.66,
 3.6,
 4.24,
 5.25,
 3.54,
 2.51,
 1.65,
 1.72,
 3.58,
 5.15,
 5.25,
 4.87,
 4.04,
 4.49,
 4.8,
 4.94,
 5.66,
 6.05,
 7.49,
 6.06,
 6.23,
 6.65,
 4.92,
 5.04,
 4.72,
 4.4,
 5.17,
 4.36,
 4.97,
 6.23,
 5.03,
 4.21,
 3.37,
 3.28,
 2.82,
 1.13,
 0.31,
 1.49,
 1.33,
 0.32,
 1.01,
 0.12,
 0.92,
 1.63,
 3.79,
 4.82,
 4.67,
 3.53,
 2.38,
 1.92,
 0.97,
 2.47,
 3.01,
 2.31,
 2.51,
 3.97,
 3.44,
 4.13,
 3.44,
 3.51,
 3.2,
 3.47,
 5.06,
 5.95,
 4.28,
 3.87,
 5.67,
 4.88,
 5.48,
 6.46,
 5.0,
 3.73,
 3.27,
 3.33,
 3.92,
 2.68,
 1.3,
 0.32,
 1.23,
 1.26,
 2.74,
 2.65,
 1.19,
 1.71,
 4.18,
 3.47,
 2.13,
 1.02,
 2.46,
 3.46,
 2.59,
 2.17,
 1.95,
 2.38,
 1.17,
 1.88,
 4.03,
 4.76,
 6.38,
 7.53,
 8.28,
 9.26,
 7.68,
 6.99,
 6.71,
 7.64,
 8.23,
 8.66,
 8.78,
 7.8,
 8.08,
 7.01,
 7.73,
 8.6,
 8.49,
 9.76,
 8.92,
 8.11,
 7.76,
 7.67,
 8.79,
 9.0,
 8.22,
 6.34,
 8.1,
 7.23,
 7.83,
 7.48,
 8.16,
 8.5,
 7.8,
 8.83,
 8.65])

ravg=data.rolling(600,min_periods=1).apply(np.mean,raw=True)
ravg2=data.rolling(600,min_periods=1).apply(np.mean,raw=False)

print('%.35f' % ravg.iloc[-1])
print('%.35f' % ravg2.iloc[-1])

```
#### Problem description

#### Expected Output
I expect the print statement in the end to return 9.34999999999999964472863211994990706

#### Output of ``pd.Series.apply()``
When setting raw=True, I get the following output:
```
9.34999999999999964472863211994990706
```

When setting raw=False, I get the following output:
```
9.35000000000001030286966852145269513
```

Is the difference intentional or a bug?

Kind regards,
Nick

"
553005242,31178,ewm NaN handling is wrong : simple example,nhuth500,open,2020-01-21T16:56:40Z,2020-10-05T01:13:14Z,"Hello,

It seems that the handling of NaN values returns bad values on this simple case (I'm using pandas 0.25.3)

import numpy as np
import pandas as pd
x = np.array([1, np.NaN, 5])
alpha = 0.5
x_ewm = pd.Series(x).ewm(alpha=alpha, adjust=False).mean()
x_ewm_simple = alpha * x[2] + ((1 - alpha) **2) * x[0]
print(x_ewm[2])
print(x_ewm_simple)

3.6666666666666665
2.75

According to pandas documentation https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html

When ignore_na is False (default), weights are based on absolute positions. For example, the weights of x and y used in calculating the final weighted average of [x, None, y] are (1-alpha)**2 and 1 (if adjust is True), and (1-alpha)**2 and alpha (if adjust is False).

Can you explain how 3.6666666666666665 is computed? Thank you."
636053820,34685,ENH: NamedAgg support for time windows/Expanding,ddofer,closed,2020-06-10T08:41:07Z,2020-10-05T01:21:47Z,"#### Is your feature request related to a problem?
I am using aggregations over data with groups over time, to create features. I had hoped to use the ""new"" namedAgg functionality to efficiently calculate and rename the many feature columns. 
I find that named Aggregations only works with the ""default""groupby.agg, when used with a rolling window or expanding, it's not supported. 
(I note that the functions documentation doesn't mention this,[ the named agg is mainly in the general documentation](https://pandas-docs.github.io/pandas-docs-travis/user_guide/groupby.html#groupby-aggregate-named), not method-level. i.e the [groupby.agg method's documentatio](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.aggregate.html)n doesn't mention or demonstrate this functionality at all). 

By ""named aggregations"" I refer to the functionality:
`animals.groupby(""kind"").agg(**min_height=('height', 'min')**)`

#### Describe the solution you'd like

Expand named aggregation support (NamedAgg) to the groupby aggregation used in [expanding.aggregate](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.window.expanding.Expanding.aggregate.html) and [rolling.aggregate](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.window.rolling.Rolling.aggregate.html)

#### API breaking implications
Should not affect it. Seems 1:1. 



#### Additional context
Example usage/errors:

```
df_actions.set_index(""age"").groupby('uuid').rolling(30).agg(unique_actions=('action', 'nunique'), 
                                total_actions=('counter', 'sum'))

TypeError: aggregate() missing 1 required positional argument: 'func'
```
When used without the window, we get the benefit of namedAggs. (The real code has many more columns and transformations and the columns the features are calculated in are dynamic, so setting a list of column names to use is not desirable. Additionally, data is time-sorted):

```
df_actions.set_index(""age"").groupby('uuid').agg(unique_actions=('action', 'nunique'), 
                               total_actions=('counter', 'sum'))

>>>

  | unique_actions | total_actions

0 | 0.0
0 | 0.0
...
```

"
709039619,36637,CI: pd.core.reshape.pivot.crosstab doctest is failing on unrelated PRs,arw2019,closed,2020-09-25T15:47:20Z,2020-10-05T04:25:41Z,"This doctest is failing on a few unrelated PR's:

```
=================================== FAILURES ===================================
_________________ [doctest] pandas.core.reshape.pivot.crosstab _________________
549     Examples
550     --------
551     >>> a = np.array([""foo"", ""foo"", ""foo"", ""foo"", ""bar"", ""bar"",
552     ...               ""bar"", ""bar"", ""foo"", ""foo"", ""foo""], dtype=object)
553     >>> b = np.array([""one"", ""one"", ""one"", ""two"", ""one"", ""one"",
554     ...               ""one"", ""two"", ""two"", ""two"", ""one""], dtype=object)
555     >>> c = np.array([""dull"", ""dull"", ""shiny"", ""dull"", ""dull"", ""shiny"",
556     ...               ""shiny"", ""dull"", ""shiny"", ""shiny"", ""shiny""],
557     ...              dtype=object)
558     >>> pd.crosstab(a, [b, c], rownames=['a'], colnames=['b', 'c'])
Differences (unified diff with -expected +actual):
    @@ -1,5 +1,6 @@
    -b   one        two
    +Index(['__dummy__'], dtype='object')
    +b    one        two      
     c   dull shiny dull shiny
    -a
    +a                        
     bar    1     2    1     0
     foo    2     2    1     2
```"
714232044,36849,DOC: black enhancingperf.rst and 10min.rst code style,aniaan,closed,2020-10-04T04:38:34Z,2020-10-05T05:57:39Z,ref #36777
712662700,36767,BUG: Index sortlevel ascending add type checking #32334,aniaan,closed,2020-10-01T09:22:49Z,2020-10-05T05:57:39Z,"- [x] closes #32334
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Add the check of ascending parameter in the sortlevel method, only allow bool value and single bool value list

cc @Dr-Irv 
"
714590639,36877,CLN: Remove the duplicate configuration of flake8-rst in setup.cfg,aniaan,closed,2020-10-05T07:24:23Z,2020-10-05T08:17:31Z,"Remove duplicate `ignore = E203` configuration in setup.cfg[flake8-rst]
"
714264805,36854,TYP: update setup.cfg,simonjayhawkins,closed,2020-10-04T08:35:04Z,2020-10-05T09:36:00Z,
714689290,36881,Why prompt me “name 'pd' is not defined”,syejing,closed,2020-10-05T09:46:37Z,2020-10-05T09:58:42Z,"### Question about pandas

I have customized a Python interpreter, on which I try to run pandas's apply method, but when executing this method, there will be an exception. I don't know why, who knows why

```python

[1] import pandas as pd
[2] titanic = pd.read_csv('../data/titanic.csv')
[3] def not_null_count(columns):
             return len(columns[pd.isnull(columns)])
[4] res = titanic.apply(not_null_count)
[5]print(res)

run_code:
comp_code = compile(code, '<zmq-kernel>', 'single')
            if comp_code:
                exec(comp_code, self.user_global_ns, self.user_ns)

```
File ""<zmq-kernel>"", line 1, in <module>
File ""/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py"", line 6878, in apply return op.get_result()
File ""/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py"", line 186, in get_result return self.apply_standard()
File ""/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py"", line 295, in apply_standard result = libreduction.compute_reduction(
File ""pandas/_libs/reduction.pyx"", line 620, in pandas._libs.reduction.compute_reduction
File ""pandas/_libs/reduction.pyx"", line 128, in pandas._libs.reduction.Reducer.get_result
File ""<zmq-kernel>"", line 2, in not_null_count
NameError: name 'pd' is not defined"
714620781,36879,Update flake8 to 3.8.4 in pre-commit-config.yaml,MarcoGorelli,closed,2020-10-05T08:09:45Z,2020-10-05T10:15:12Z,"Steps here are:

- upgrade flake8 in `.pre-commit-config.yaml`
- run `pre-commit run flake8 --all`

If everything passes, open a PR. If there's some error, let us know

Comment `take` if you want to work on this"
714371719,36864,CI: Update error message for np_dev,dsaxton,closed,2020-10-04T18:20:23Z,2020-10-05T10:47:02Z,https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=44153&view=logs&jobId=eab14f69-13b6-5db7-daeb-7b778629410b&j=eab14f69-13b6-5db7-daeb-7b778629410b&t=ce687173-08c6-5301-838d-71b2dda24510
651726042,35146,Move mark registration,TomAugspurger,closed,2020-07-06T18:06:03Z,2020-10-05T11:22:40Z,"Should avoid warnings like

```
  D:\a\1\s\test_venv\lib\site-packages\pandas\tests\plotting\test_series.py:655: PytestUnknownMarkWarning: Unknown pytest.mark.slow - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html
    @pytest.mark.slow

```

in https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=38672&view=logs&j=c0130b29-789d-5a3c-6978-10796a508a7f&t=e120bc6c-1f5e-5a41-8f0a-1d992cd2fbfb"
714729726,36886,Backport PR #36864 on branch 1.1.x (CI: Update error message for np_dev),meeseeksmachine,closed,2020-10-05T10:46:45Z,2020-10-05T12:03:15Z,Backport PR #36864: CI: Update error message for np_dev
714712529,36883,DOC: sync release notes on 1.1.x with master,simonjayhawkins,closed,2020-10-05T10:20:49Z,2020-10-05T12:04:47Z,xref #36845
714715216,36884,TYP: check_untyped_defs compat.pickle_compat,simonjayhawkins,closed,2020-10-05T10:24:51Z,2020-10-05T13:12:30Z,"pandas\compat\pickle_compat.py:277: error: Incompatible types in assignment (expression has type ""Callable[[bytes, DefaultNamedArg(bool, 'fix_imports'), DefaultNamedArg(str, 'encoding'), DefaultNamedArg(str, 'errors')], Any]"", variable has type ""Callable[[bytes, DefaultNamedArg(bool, 'fix_imports'), DefaultNamedArg(str, 'encoding'), DefaultNamedArg(str, 'errors'), DefaultNamedArg(Optional[Iterable[Any]], 'buffers')], Any]"")  [assignment]"
714721049,36885,TYP: check_untyped_defs core.arrays.base,simonjayhawkins,closed,2020-10-05T10:33:22Z,2020-10-05T13:15:12Z,"xref https://github.com/pandas-dev/pandas/issues/31160

from https://docs.python.org/3/library/functions.html?highlight=setattr#setattr

> For example, setattr(x, 'foobar', 123) is equivalent to x.foobar = 123.

The changes here, don't 'fix' the issue. The correct solution is to create these methods with a statically defined interface, see https://github.com/pandas-dev/pandas/issues/31160#issuecomment-699193016

In the meantime, the changes here allows untyped defs in this module to be checked. (without adding type ignores)

pandas\core\arrays\base.py:1179: error: Unsupported left operand type for + (""Type[ExtensionOpsMixin]"")  [operator]
pandas\core\arrays\base.py:1180: error: ""Type[ExtensionOpsMixin]"" has no attribute ""__radd__""  [attr-defined]
pandas\core\arrays\base.py:1181: error: Unsupported left operand type for - (""Type[ExtensionOpsMixin]"")  [operator]
pandas\core\arrays\base.py:1182: error: ""Type[ExtensionOpsMixin]"" has no attribute ""__rsub__""  [attr-defined]
pandas\core\arrays\base.py:1183: error: Unsupported left operand type for * (""Type[ExtensionOpsMixin]"")  [operator]
pandas\core\arrays\base.py:1184: error: ""Type[ExtensionOpsMixin]"" has no attribute ""__rmul__""  [attr-defined]
pandas\core\arrays\base.py:1185: error: Unsupported left operand type for ** (""Type[ExtensionOpsMixin]"")  [operator]
pandas\core\arrays\base.py:1186: error: ""Type[ExtensionOpsMixin]"" has no attribute ""__rpow__""  [attr-defined]
pandas\core\arrays\base.py:1187: error: Unsupported left operand type for % (""Type[ExtensionOpsMixin]"")  [operator]
pandas\core\arrays\base.py:1188: error: ""Type[ExtensionOpsMixin]"" has no attribute ""__rmod__""  [attr-defined]
pandas\core\arrays\base.py:1189: error: Unsupported left operand type for // (""Type[ExtensionOpsMixin]"")  [operator]
pandas\core\arrays\base.py:1190: error: ""Type[ExtensionOpsMixin]"" has no attribute ""__rfloordiv__""  [attr-defined]
pandas\core\arrays\base.py:1191: error: Unsupported left operand type for / (""Type[ExtensionOpsMixin]"")  [operator]
pandas\core\arrays\base.py:1192: error: ""Type[ExtensionOpsMixin]"" has no attribute ""__rtruediv__""  [attr-defined]
pandas\core\arrays\base.py:1193: error: Unsupported left operand type for divmod (""Type[ExtensionOpsMixin]"")  [operator]
pandas\core\arrays\base.py:1194: error: ""Type[ExtensionOpsMixin]"" has no attribute ""__rdivmod__""  [attr-defined]
pandas\core\arrays\base.py:1202: error: Cannot assign to a method  [assignment]
pandas\core\arrays\base.py:1203: error: Cannot assign to a method  [assignment]
pandas\core\arrays\base.py:1204: error: Unsupported left operand type for < (""Type[ExtensionOpsMixin]"")  [operator]
pandas\core\arrays\base.py:1205: error: Unsupported left operand type for > (""Type[ExtensionOpsMixin]"")  [operator]
pandas\core\arrays\base.py:1206: error: Unsupported left operand type for <= (""Type[ExtensionOpsMixin]"")  [operator]
pandas\core\arrays\base.py:1207: error: Unsupported left operand type for >= (""Type[ExtensionOpsMixin]"")  [operator]
pandas\core\arrays\base.py:1215: error: Unsupported left operand type for & (""Type[ExtensionOpsMixin]"")  [operator]
pandas\core\arrays\base.py:1216: error: ""Type[ExtensionOpsMixin]"" has no attribute ""__rand__""  [attr-defined]
pandas\core\arrays\base.py:1217: error: Unsupported left operand type for | (""Type[ExtensionOpsMixin]"")  [operator]
pandas\core\arrays\base.py:1218: error: ""Type[ExtensionOpsMixin]"" has no attribute ""__ror__""  [attr-defined]
pandas\core\arrays\base.py:1219: error: Unsupported left operand type for ^ (""Type[ExtensionOpsMixin]"")  [operator]
pandas\core\arrays\base.py:1220: error: ""Type[ExtensionOpsMixin]"" has no attribute ""__rxor__""  [attr-defined]"
714792791,36887,DOC: 1.1.3 release date,simonjayhawkins,closed,2020-10-05T12:24:21Z,2020-10-05T14:03:43Z,removed mention of py39 support @jreback  
714918492,36892,ti.csv,sazk07,closed,2020-10-05T14:59:25Z,2020-10-05T15:05:51Z,"pandas dataset for practice and understanding

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
714871201,36891,Backport PR #36887 on branch 1.1.x (DOC: 1.1.3 release date),meeseeksmachine,closed,2020-10-05T14:03:43Z,2020-10-05T15:17:12Z,Backport PR #36887: DOC: 1.1.3 release date
714464659,36872,PERF: Improve RollingGroupby.count,mroeschke,closed,2020-10-05T02:27:35Z,2020-10-05T16:52:34Z,"- [x] closes #35625
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

```
In [1]: import pandas as pd
   ...:
   ...: # Generate sample df
   ...: df = pd.DataFrame({'column1': range(600), 'group': 5*['l'+str(i) for i in range(120)]})
   ...:
   ...: # sort by group for easy/efficient joining of new columns to df
   ...: df=df.sort_values('group',kind='mergesort').reset_index(drop=True)

In [2]: %timeit df['mean']=df.groupby('group').rolling(3,min_periods=1)['column1'].mean().values
5.59 ms ± 310 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [3]: %timeit df['sum']=df.groupby('group').rolling(3,min_periods=1)['column1'].sum().values
   ...:
5.34 ms ± 343 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [4]: %timeit df['count']=df.groupby('group').rolling(3,min_periods=1)['column1'].count().values
   ...:
4.97 ms ± 51.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```"
187385852,14588,PERF: asv for select_dtypes,simonm3,closed,2016-11-04T16:57:29Z,2020-10-05T17:41:30Z,"Why is select_dtypes so slow?

`%timeit [col for col in df.columns if np.issubdtype(df[col].dtype, np.number)]`
453 microsecs per loop

`%timeit df.select_dtypes(include=[np.number])`
4.58 secs per loop"
714170553,36839,PERF: Add asv benchmarks for select_dtypes (14588),avinashpancham,closed,2020-10-03T20:31:48Z,2020-10-05T17:41:34Z,"- [x] closes #14588 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
713848619,36813,DOC: use black to fix code style in doc pandas-dev#36777,PrayagS,closed,2020-10-02T19:19:27Z,2020-10-05T19:18:15Z,Partially addresses #36777. Ran black on all the files under `doc/source/getting_started/comparison`.
709823582,36689,DOC: Start v1.1.4 release notes,dsaxton,closed,2020-09-27T21:55:44Z,2020-10-05T20:16:33Z,I think this requires the v1.1.3 tag in order to pass
157986198,13346,pd.Period off by hundred of years for dates past ~2263,eyurtsev,closed,2016-06-01T19:21:03Z,2020-10-05T20:52:52Z,"```
In [1]: import pandas as pd

In [2]: pd.Period(year=2300, month=1, day=1, freq='M').end_time
Out[2]: Timestamp('1715-07-14 00:25:26.290448383')
```

Input year is '2300', output shows '1715'
#### output of `pd.show_versions()`

In [3]: pd.show_versions()
## INSTALLED VERSIONS

commit: None
python: 2.7.11.final.0
python-bits: 64
OS: Linux
OS-release: 3.19.0-25-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.18.1
nose: None
pip: 7.1.2
setuptools: 21.0.0
Cython: None
numpy: 1.11.0
scipy: 0.17.0
statsmodels: 0.6.1
xarray: None
IPython: 4.2.0
sphinx: None
patsy: 0.4.1
dateutil: 2.5.3
pytz: 2016.4
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: 1.5.1
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: 4.4.1
html5lib: 0.9999999
httplib2: None
apiclient: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.8
boto: 2.39.0
pandas_datareader: 0.2.1
"
714361737,36863,CI: Show ipython directive errors,dsaxton,closed,2020-10-04T17:29:04Z,2020-10-06T00:07:04Z,"Current output of ipython directive errors in CI is essentially a blank line, so adding a bit more to try to show what error is happening.
```
(pandas-dev) ➜  ~ grep -B1 ""^<<<-------------------------------------------------------------------------$"" sphinx.log

<<<-------------------------------------------------------------------------
```
vs.
```
(pandas-dev) ➜  ~ grep -B10 ""^<<<-------------------------------------------------------------------------$"" sphinx.log


>>>-------------------------------------------------------------------------
Exception in /Users/danielsaxton/pandas/doc/source/user_guide/enhancingperf.rst at block ending on line 834
Specify :okexcept: as an option in the ipython:: block to suppress this message
  File ""<ipython-input-66-d3ab2b9618fd>"", line 1
    df.query(""strings == ""a"" and nums == 1"")
                          ^
SyntaxError: invalid syntax

<<<-------------------------------------------------------------------------
```"
712178547,36746,Standardize cast_str behavior in all datetimelike fill_value validators,jbrockmendel,closed,2020-09-30T18:33:31Z,2020-10-06T02:48:21Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This will in turn allow us to simplify/de-duplicate a bunch of code since we now pass cast_str in all cases."
714613524,36878,New branch,skorani,closed,2020-10-05T07:59:24Z,2020-10-06T08:12:48Z,"- [ ] ref #36777
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
714281126,36857,DOC: doc/source/whatsnew,meghanacosmos,closed,2020-10-04T10:13:38Z,2020-10-06T08:15:18Z,"- [x] xref #36777 
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
713373739,36802,DOC: ran blacken docs tool and checked output to improve formatting #36777,maria-ilie,closed,2020-10-02T06:09:05Z,2020-10-06T08:15:40Z,"For task #36777
Ran blacken-tools on user_guide/10min.rst, user_guide/advanced.rst, basics.rst
"
715125950,36899,Backport PR #36689 on branch 1.1.x (DOC: Start v1.1.4 release notes),meeseeksmachine,closed,2020-10-05T20:06:04Z,2020-10-06T10:28:43Z,Backport PR #36689: DOC: Start v1.1.4 release notes
714002422,36824,DOC: use black to fix code style in doc pandas-dev#36777,PrayagS,closed,2020-10-03T04:34:51Z,2020-10-06T12:21:29Z,"Partially addresses #36777. 

Fixed the following files:
- `doc/source/development/extending.rst`
- `doc/source/user_guide/duplicates.rst`
- `doc/source/user_guide/gotchas.rst`
- `doc/source/user_guide/scale.rst`"
715631703,36914,BUG: groupby dropna for multiindex dataframe ,AgnesBaud,closed,2020-10-06T12:36:56Z,2020-10-06T12:41:12Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

Hello,

I'm not sure if it's a bug report, a feature request, or if there's already another way to do it. In which case, I would be happy to hear it and close this issue.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

test_df = pd.DataFrame.from_dict({
'gene_1' : ['Bacteria', 'Lactobacillus', 11, 17],
'gene_2' : ['Bacteria', 'Lactobacillus', 9, 3],
'gene_3' : [np.nan, np.nan, 1, 2],
'gene_4' : [np.nan, np.nan, 3, 4],
'gene_5' : ['Bacteria', np.nan, 10, 11]
},
orient='index', columns=['kingdom', 'genus', 'sample_1', 'sample_2'])
test_df = test_df.set_index(['kingdom', 'genus'])
```
![test_df](https://user-images.githubusercontent.com/22837825/95197623-2cc05800-07da-11eb-9d9e-7bfead83ac54.png)

```python
test_df.groupby(level=[0,1], dropna = False).sum()
```
#### Current output

![groupby sum](https://user-images.githubusercontent.com/22837825/95197644-32b63900-07da-11eb-9dd4-603f0c35f367.png)

#### Problem description

I would like the dropna option of groupby to handle multipleindex dataframe

#### Expected Output

![whatiwant](https://user-images.githubusercontent.com/22837825/95197703-4b265380-07da-11eb-976f-d1c2f48ccb7d.png)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-48-generic
Version          : #52-Ubuntu SMP Thu Sep 10 10:58:49 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>

Related issues : #3729 #29716

Thank you,

Agnès
"
715206939,36902,CLN: standardize fixture usage in datetimelike array tests,jbrockmendel,closed,2020-10-05T22:34:11Z,2020-10-06T14:59:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
715170085,36901,REF: collect reduction tests,jbrockmendel,closed,2020-10-05T21:20:01Z,2020-10-06T14:59:50Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
715255705,36905,CLN: value -> key,jbrockmendel,closed,2020-10-06T00:45:29Z,2020-10-06T15:08:57Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
715852448,36919,fixed capitalization on ecosystem.rst,data-RanDan,closed,2020-10-06T16:47:34Z,2020-10-06T17:30:15Z,"Fixed capitalization on doc/source/ecosystem.rst
"
711402090,36727,BUG: Segmentation fault when doing pandas.core.window.rolling.RollingGroupBy.apply,geogunow,closed,2020-09-29T19:49:59Z,2020-10-06T18:04:00Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame(
    [
        [""A"", ""group_1"", pd.Timestamp(2019, 1, 1, 9)],
        [""B"", ""group_1"", pd.Timestamp(2019, 1, 2, 9)],
        [""C"", ""group_2"", pd.Timestamp(2019, 1, 3, 9)],
        [""D"", ""group_1"", pd.Timestamp(2019, 1, 6, 9)],
        [""E"", ""group_1"", pd.Timestamp(2019, 1, 7, 9)],
        [""F"", ""group_1"", pd.Timestamp(2019, 1, 10, 9)],
        [""G"", ""group_2"", pd.Timestamp(2019, 1, 20, 9)],
        [""H"", ""group_1"", pd.Timestamp(2019, 4, 8, 9)],
    ],
    columns=[""index"", ""group"", ""eventTime""],
).set_index(""index"")

groups = df.groupby(""group"")
df[""count_to_date""] = groups.cumcount()
rolling_groups = groups.rolling(""10d"", on=""eventTime"")
group_size = rolling_groups.apply(lambda df: df.shape[0])
print(group_size)
```

#### Problem description
The above code causes a segmentation fault inside pandas for versions *after* 1.0.5. Since I need the above code for a project, I am restricted to using pandas 1.0.5 until this is resolved. I am not sure what is causing the segmentation fault, but all the above circumstances are necessary to reproducing the bug (ie `DataFrame` with special index, a column set in the `DataFrame` after grouping, a rolling window on a group, etc).

I have reproduced this bug on a variety of machines and operating systems.

#### Expected Output

```
                        eventTime  count_to_date
group   index                                   
group_1 A     2019-01-01 09:00:00            1.0
        B     2019-01-02 09:00:00            2.0
        D     2019-01-06 09:00:00            3.0
        E     2019-01-07 09:00:00            4.0
        F     2019-01-10 09:00:00            5.0
        H     2019-04-08 09:00:00            1.0
group_2 C     2019-01-03 09:00:00            1.0
        G     2019-01-20 09:00:00            1.0
```

Note: This is indeed the output of versions 1.0.5 and prior.

#### Output of ``pd.show_versions()``
This is just one configuration but the bug has been reproduced on three different machines (both linux and mac), all exhibiting the same behavior.

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 17.7.0
Version          : Darwin Kernel Version 17.7.0: Thu Jun 18 21:21:34 PDT 2020; root:xnu-4570.71.82.5~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.2
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
</details>
"
715599638,36913,BUG: Fixed IntegerArray.__array_ufunc__ with nout,TomAugspurger,closed,2020-10-06T11:52:35Z,2020-10-06T18:42:14Z,"We forgot to return.
"

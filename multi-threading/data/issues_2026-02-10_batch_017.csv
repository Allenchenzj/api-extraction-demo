id,number,title,user,state,created_at,updated_at,body
798363011,39530,Backport PR #39512 on branch 1.2.x (CLN: Fix userguide deprecation warning new numpy),meeseeksmachine,closed,2021-02-01T13:42:41Z,2021-02-01T14:49:28Z,Backport PR #39512: CLN: Fix userguide deprecation warning new numpy
797553086,39494,CI: update for numpy 1.20,jbrockmendel,closed,2021-01-30T22:26:38Z,2021-02-01T15:19:08Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
792680834,39363,TST/WIP: stricter xfails,jbrockmendel,closed,2021-01-23T23:49:29Z,2021-02-01T15:53:30Z,
798448550,39532,Backport PR #39494: CI: update for numpy 1.20,simonjayhawkins,closed,2021-02-01T15:18:17Z,2021-02-01T16:11:03Z,Backport PR #39494
798476280,39535,Backport PR #39355: BUG: read_excel failing to check older xlrd versions properly,simonjayhawkins,closed,2021-02-01T15:48:58Z,2021-02-01T16:56:28Z,Backport PR #39355
798493775,39536,Backport PR #39482: ERR: Unify error message for bad excel sheetnames,simonjayhawkins,closed,2021-02-01T16:09:21Z,2021-02-01T17:49:24Z,Backport PR #39482
798471932,39534,CI: numpy warnings produced by pytables,jbrockmendel,closed,2021-02-01T15:43:58Z,2021-02-01T19:07:06Z,
798636971,39544,CI: pin numpy<1.20,jbrockmendel,closed,2021-02-01T19:18:01Z,2021-02-01T21:07:59Z,"- [x] closes #39541
- [ ] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
798628891,39540,Backport PR #39534 on branch 1.2.x (CI: numpy warnings produced by pytables),meeseeksmachine,closed,2021-02-01T19:06:12Z,2021-02-01T21:32:30Z,Backport PR #39534: CI: numpy warnings produced by pytables
798707979,39545,Backport PR #39544 on branch 1.2.x (CI: pin numpy<1.20),meeseeksmachine,closed,2021-02-01T21:07:06Z,2021-02-01T21:55:26Z,Backport PR #39544: CI: pin numpy<1.20
797474505,39482,ERR: Unify error message for bad excel sheetnames,rhshadrach,closed,2021-01-30T16:43:48Z,2021-02-01T23:09:03Z,"- [x] closes #39250
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

IMO the core issue in #39250 is upstream in openpyxl, providing a better error message is really tangential to it. Since the error message that openpyxl raises is not a regression, it seemed best to me to (a) target this for 1.3 and (b) unify all error messages across engines.

If, on the other hand, we really do want to improve the error message in 1.2.2 because of the switch from xlrd to openpyxl, then I will open a new PR for 1.2.2 that only impacts openpyxl.

Will do a followup to remove the duplicate tests.

cc @simonjayhawkins @jorisvandenbossche "
795599061,39446,REF: dont pass fill_value to algos.take_nd from internals.concat,jbrockmendel,closed,2021-01-28T01:58:37Z,2021-02-01T23:12:40Z,"The dtype-finding code in internals.concat is something else.  Tried untangling it, but now I think we can make it unnecessary altogether.  This is the first step in that process."
796345963,39454,BUG: incorrect casting in DataFrame.append,jbrockmendel,closed,2021-01-28T20:36:15Z,2021-02-01T23:14:09Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

I'm pretty sure we're going to end up with just using find_common_type instead of re-implementing similar stuff in internals.concat."
797802301,39508,TST: fix td64 arithmetic xfails,jbrockmendel,closed,2021-01-31T20:19:34Z,2021-02-01T23:49:59Z,
797583169,39495,TST/REF: collect index tests,jbrockmendel,closed,2021-01-31T00:05:40Z,2021-02-01T23:56:15Z,Moving away from creat_index and towards the index fixture
798716871,39546,"CI/TST: update exception message, xfail",jbrockmendel,closed,2021-02-01T21:19:53Z,2021-02-02T00:02:27Z,
692911411,36111,BUG: read_csv with custom date parser and na_filter=True results in ValueError,smith1401,closed,2020-09-04T08:46:14Z,2021-02-02T00:27:39Z,"```python
import numpy as np
import pandas as pd
from io import StringIO

def __custom_date_parser(time):
    time_temp = time.astype(np.float).astype(np.int) # convert float seconds to int type
    return pd.to_timedelta(time_temp, unit='s')

testdata = StringIO(""""""time    e   n   h
41047.00	-98573.7297	871458.0640	389.0089
41048.00	-98573.7299	871458.0640	389.0089
41049.00	-98573.7300	871458.0642	389.0088
41050.00	-98573.7299	871458.0643	389.0088
41051.00	-98573.7302	871458.0640	389.0086
    """""")

df = pd.read_csv(testdata, delim_whitespace=True, parse_dates=True, date_parser=__custom_date_parser, index_col='time')
```

I noticed this problem when I executed a piece of old code which has worked before (a few months ago). Normally this code would parse a text file with GPS seconds of week as time and convert it to a TimeDeltaIndex. Now when I execute this, it results in a **ValueError: unit abbreviation w/o a number**. (Full stack trace below) I tracked it down to the default option **na_filter=True** in pd.read_csv. When i set it to False everything is working. With a bit of digging I think i found the source of the error in algorithms.py -> _ensure_data -> line 142.

```python
    # datetimelike
    vals_dtype = getattr(values, ""dtype"", None)
    if needs_i8_conversion(vals_dtype) or needs_i8_conversion(dtype):
        if is_period_dtype(vals_dtype) or is_period_dtype(dtype):
            from pandas import PeriodIndex

            values = PeriodIndex(values)
            dtype = values.dtype
        elif is_timedelta64_dtype(vals_dtype) or is_timedelta64_dtype(dtype):
            from pandas import TimedeltaIndex

            values = TimedeltaIndex(values)  #This is line 142
            dtype = values.dtype
        else:
            # Datetime
            if values.ndim > 1 and is_datetime64_ns_dtype(vals_dtype):
                # Avoid calling the DatetimeIndex constructor as it is 1D only
                # Note: this is reached by DataFrame.rank calls GH#27027
                # TODO(EA2D): special case not needed with 2D EAs
                asi8 = values.view(""i8"")
                dtype = values.dtype
                return asi8, dtype

            from pandas import DatetimeIndex

            values = DatetimeIndex(values)
            dtype = values.dtype
```

Here the function tries to parse **values** as TimeDeltaIndex, but values is ['' 'n/a' '-nan' '#N/A' '1.#QNAN' 'nan' '#NA' 'NaN' '-1.#QNAN' '#N/A N/A', '-NaN' 'N/A' 'NULL' '<NA>' 'null' '1.#IND' 'NA' '-1.#IND'] in this case. It executes this if statement, because **is_timedelta64_dtype(dtype)** is true in this case. I can't believe that this is expected behaviour, as it has worked before.


``` python-traceback
Traceback (most recent call last):
  File ""...\lib\site-packages\pandas\io\parsers.py"", line 458, in _read
    data = parser.read(nrows)
  File ""...\lib\site-packages\pandas\io\parsers.py"", line 1186, in read
    ret = self._engine.read(nrows)
  File ""...\lib\site-packages\pandas\io\parsers.py"", line 2221, in read
    index, names = self._make_index(data, alldata, names)
  File ""...\lib\site-packages\pandas\io\parsers.py"", line 1667, in _make_index
    index = self._agg_index(index)
  File ""...\lib\site-packages\pandas\io\parsers.py"", line 1760, in _agg_index
    arr, _ = self._infer_types(arr, col_na_values | col_na_fvalues)
  File ""...\lib\site-packages\pandas\io\parsers.py"", line 1861, in _infer_types
    mask = algorithms.isin(values, list(na_values))
  File ""...\lib\site-packages\pandas\core\algorithms.py"", line 433, in isin
    values, _ = _ensure_data(values, dtype=dtype)
  File ""...\lib\site-packages\pandas\core\algorithms.py"", line 142, in _ensure_data
    values = TimedeltaIndex(values)
  File ""...\lib\site-packages\pandas\core\indexes\timedeltas.py"", line 157, in __new__
    data, freq=freq, unit=unit, dtype=dtype, copy=copy
  File ""...\lib\site-packages\pandas\core\arrays\timedeltas.py"", line 216, in _from_sequence
    data, inferred_freq = sequence_to_td64ns(data, copy=copy, unit=unit)
  File ""...\lib\site-packages\pandas\core\arrays\timedeltas.py"", line 930, in sequence_to_td64ns
    data = objects_to_td64ns(data, unit=unit, errors=errors)
  File ""...\lib\site-packages\pandas\core\arrays\timedeltas.py"", line 1040, in objects_to_td64ns
    result = array_to_timedelta64(values, unit=unit, errors=errors)
  File ""pandas\_libs\tslibs\timedeltas.pyx"", line 273, in pandas._libs.tslibs.timedeltas.array_to_timedelta64
  File ""pandas\_libs\tslibs\timedeltas.pyx"", line 268, in pandas._libs.tslibs.timedeltas.array_to_timedelta64
  File ""pandas\_libs\tslibs\timedeltas.pyx"", line 215, in pandas._libs.tslibs.timedeltas.convert_to_timedelta64
  File ""pandas\_libs\tslibs\timedeltas.pyx"", line 428, in pandas._libs.tslibs.timedeltas.parse_timedelta_string
ValueError: unit abbreviation w/o a number
python-BaseException
```

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.7.9.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.1.1
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200814
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
797583951,39496,TST: read_csv with custom date parser and na_filter=True results in ValueError,ftrihardjo,closed,2021-01-31T00:11:42Z,2021-02-02T00:27:43Z,"previous PR #39079

- [ ] closes #36111
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
798798260,39549,"Backport PR #39546 on branch 1.2.x (CI/TST: update exception message, xfail)",meeseeksmachine,closed,2021-02-01T23:41:05Z,2021-02-02T00:54:32Z,"Backport PR #39546: CI/TST: update exception message, xfail"
777138311,38865,REGR: Sorted pandas series is not plotted in the expected order in pandas 1.2.0,GajjarMihir,closed,2020-12-31T21:09:45Z,2021-02-02T01:42:42Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample
Note: The code was executed in Jupyter notebook.

```
import pandas as pd
print(pd.__version__)
d = {'col1': [4,2,3,1]}
df = pd.DataFrame(data=d)
df['col1'].sort_values().plot(kind='bar')
```

#### Problem description

In Pandas 1.2.0, the sorted values are not plotted in the correct order. Note that ```df['col1'].sort_values()``` does return sorted values.

![pandas 1.2.0](https://user-images.githubusercontent.com/25637736/103424892-eb430d00-4b7c-11eb-999d-0e525d122776.png)

#### Expected Output

In Pandas 1.1.5, this works as expected. This might be the case in older versions of pandas as well.

![pandas 1.1.5](https://user-images.githubusercontent.com/25637736/103424905-001fa080-4b7d-11eb-9577-0ff812d6d6fe.png)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 20.2.0
Version          : Darwin Kernel Version 20.2.0: Wed Dec  2 20:39:59 PST 2020; root:xnu-7195.60.75~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_CA.UTF-8
LOCALE           : en_CA.UTF-8

pandas           : 1.1.5
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.0.post20201006
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : 5.37.4
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.3
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.19
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.51.2
</details>"
797525427,39491,TST: Verify plot order of a Series (GH38865),avinashpancham,closed,2021-01-30T20:37:14Z,2021-02-02T01:42:46Z,"- [x] closes #38865
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
796399341,39457,BUG: fixing mixup of bucket and element counts in hash tables,realead,closed,2021-01-28T21:39:09Z,2021-02-02T02:00:00Z,"- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

The issue is that, `kh_resize_##name` expects the number of buckets in the map as input: https://github.com/pandas-dev/pandas/blob/39e126199b1d2082d9d084f7a37c9745ab9a95ee/pandas/_libs/src/klib/khash.h#L324

it is however used with the number of elements throughout the pandas code, e.g. https://github.com/pandas-dev/pandas/blob/39e126199b1d2082d9d084f7a37c9745ab9a95ee/pandas/core/algorithms.py#L412

as can be seen in the `__cinit__` function: https://github.com/pandas-dev/pandas/blob/39e126199b1d2082d9d084f7a37c9745ab9a95ee/pandas/_libs/hashtable_class_helper.pxi.in#L400-L404


Thus, for some sizes (like 10^3) there still could be a rehashing even if size-hint was used with the idea to avoid it (because at most 75% of buckets can be occupied)

I think it make sense to do the less surprising thing with the size-hint (i.e. the number of elements and not buckets).
"
796313289,39453,REF: simplify _get_empty_dtype_and_na,jbrockmendel,closed,2021-01-28T20:00:32Z,2021-02-02T02:46:09Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
23118344,5569,Feature request: Option to include NaNs in value_counts(),michaelaye,closed,2013-11-22T05:17:44Z,2021-02-02T06:27:18Z,"I find it highly valuable to also receive the information how many NaN values are in my Series.
Could we have an option in value_counts(), maybe `include_nans=True` that would add a count for those in the output of it?
"
798242810,39525,BUG: Groupby and apply fails quietly for str columns,lxbader,closed,2021-02-01T11:06:59Z,2021-02-02T08:15:19Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

The groupby - apply functionality breaks for string columns if within an apply a new column is assigned and a choice of columns is returned. Short example:

```python
import numpy as np
import pandas as pd
data = pd.DataFrame()
data[""foo""] = np.repeat([""person"", ""woman""], 3)
data[""bar""] = np.repeat([""camera"", ""TV""], 3)
data[""number""] = np.repeat([99,88], 3)
data[""id""] = np.arange(6)
```

```python
	foo	bar	number	id
0	person	camera	99	0
1	person	camera	99	1
2	person	camera	99	2
3	woman	TV	88	3
4	woman	TV	88	4
5	woman	TV	88	5
```

Let's define a test function which simply adds another column and returns the original DataFrame when applied:

```python
def testfunc1(df):
    df[""added""] = ""bazinga""
    return df

data.groupby(""bar"").apply(testfunc1)
```

```python
	foo	bar	number	id	added
0	person	camera	99	0	bazinga
1	person	camera	99	1	bazinga
2	person	camera	99	2	bazinga
3	woman	TV	88	3	bazinga
4	woman	TV	88	4	bazinga
5	woman	TV	88	5	bazinga
```

Behaves just like it should, this is the expected output. However, redefining this test function to return a specific set of columns returns this unexpected output:

```python
def testfunc2(df):
    df[""added""] = ""bazinga""
    return df[[""foo"", ""bar"", ""number"", ""id"", ""added""]]

data.groupby(""bar"").apply(testfunc2)
```
```python
	foo	bar	number	id	added
0	woman	TV	99	0	bazinga
1	woman	TV	99	1	bazinga
2	woman	TV	99	2	bazinga
3	woman	TV	88	3	bazinga
4	woman	TV	88	4	bazinga
5	woman	TV	88	5	bazinga
```

All information in string columns is overwritten with one specific value, while the behaviour for numeric columns carries the original values just as expected.

This was run on pandas version 1.2.1
"
798633816,39543,TYP: blocks,jbrockmendel,closed,2021-02-01T19:13:21Z,2021-02-02T08:52:01Z,
782679850,39067,TYP: prep _generate_range_overflow_safe for numpy 1.20,simonjayhawkins,closed,2021-01-09T20:30:00Z,2021-02-02T09:00:19Z,"difficult call, will replace

pandas/core/arrays/_ranges.py:153: error: Incompatible return value type (got ""signedinteger[_64Bit]"", expected ""int"")  [return-value]
pandas/core/arrays/_ranges.py:171: error: Incompatible return value type (got ""unsignedinteger[_64Bit]"", expected ""int"")  [return-value]

with

pandas/core/arrays/_ranges.py:129: error: Argument 1 to ""_generate_range_overflow_safe"" has incompatible type ""Union[int, number[Any]]""; expected ""Union[int, integer[Any]]""  [arg-type]
pandas/core/arrays/_ranges.py:129: error: Argument 2 to ""_generate_range_overflow_safe"" has incompatible type ""Union[int, number[Any]]""; expected ""Union[int, integer[Any]]""  [arg-type]
pandas/core/arrays/_ranges.py:137: error: Argument 2 to ""_generate_range_overflow_safe"" has incompatible type ""Union[int, number[Any]]""; expected ""Union[int, integer[Any]]""  [arg-type]
pandas/core/arrays/_ranges.py:138: error: Argument 2 to ""_generate_range_overflow_safe"" has incompatible type ""Union[int, number[Any]]""; expected ""Union[int, integer[Any]]""  [arg-type]

these are from arithmetic operations on np.integer (including floordiv) which AFAICT should return np.integer and not np.number

can't add the ignores yet, since we will get unused ignore mypy messages - active branch with fixes https://github.com/pandas-dev/pandas/compare/master...simonjayhawkins:numpy-fixes?expand=1"
782617816,39059,TYP: remove ignore in optional_args,simonjayhawkins,closed,2021-01-09T14:25:09Z,2021-02-02T09:01:04Z,xref #37715
745138564,37921,TYP: __getitem__ method of EA (2nd pass),simonjayhawkins,closed,2020-11-17T22:36:03Z,2021-02-02T09:02:44Z,"follow-on from #37898

remove casts and ignores added in #37898 (needed to use typevar in DatetimeLikeArrayMixin in conjunction with overload for slice to return correct DatetimeArray/PeriodArray)

another pass required to

more typevars for return type of type(self)
type \_\_getitem__ in concrete classes (Categorical, IntervalArray and SparseArray)
overloads with Any report overlapping overloads
NAType resolves to `Any` since implemented in cython without stub file
"
797541171,39492,DOC: Document how encoding errors are handled,twoertwein,closed,2021-01-30T21:37:13Z,2021-02-02T10:07:00Z,"xref #39450

This should probably go in 1.2.2.

<1.2.0: `engine=c` and `engine=python` handled encoding errors differently when `encoding=None` (python: ""replace"", c: ""strict""). The `engine=c` further supported to ignore lines (`skiprows`) with encoding errors iff `encoding=None`.

1.2.0 (shared file opening code between `engine=c` and `engine=python`): both engine mistakenly used ""strict"" when `encoding=None`. `engine=c`+`encoding=None` can no longer skip encoding errors.

1.2.1: both engine use ""replace"" when `encoding=None`. Both engines can ignore encoding errors in skipped rows iff `encoding=None`.

Option for 1.3: default to ""strict"" but expose `errors` in `read_csv` (and other read calls that might need it)?
"
799131600,39552,Backport PR #39492: DOC: Document how encoding errors are handled,simonjayhawkins,closed,2021-02-02T10:06:45Z,2021-02-02T10:57:48Z,Backport PR #39492
798629927,39541,CI: green builds,jreback,closed,2021-02-01T19:07:44Z,2021-02-02T12:54:29Z,"so the confluence of some pyarrow pinning and release of numpy 1.20 are causing pain rn. several PRs have addressed most of these. I think 38_locale is the remaining problem, needs to pin pyarrow. 

We need to get to green on master & 1.2.2 before any other PRs.

cc @pandas-dev/pandas-core "
794980769,39432,ENH: Add compression to read_stata and StataReader,bashtage,closed,2021-01-27T10:37:38Z,2021-02-02T13:19:25Z,"Add support for reading compressed dta files directly

xref #26599

- [X] closes #26599
- [X] tests added / passed
- [X] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [X] whatsnew entry
"
797718072,39500,DOC: add example to insert (#39313),nofarm3,closed,2021-01-31T14:00:36Z,2021-02-02T14:55:41Z,"- [x] closes #39313
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry - no 
"
797888910,39517,CLN: remove unnecessary name in stack_arrays,jbrockmendel,closed,2021-02-01T02:09:44Z,2021-02-02T15:10:15Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
797757380,39502,TST: trim fixture to avoid xfails/skips,jbrockmendel,closed,2021-01-31T16:57:47Z,2021-02-02T15:11:23Z,
796495335,39461,BUG: assert_attr_equal with numpy nat or pd.NA,jbrockmendel,closed,2021-01-29T01:12:28Z,2021-02-02T15:12:31Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

This started off with editing na_value_for_dtype so that it could be re-used in #39446, ended up snowballing into assert_attr_equal.  If requested, I'll split off the bugfix into its own PR."
797324915,39478,"BUG: Series[bool].__setitem__(scalar, non_bool)",jbrockmendel,closed,2021-01-30T03:35:16Z,2021-02-02T15:26:02Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
798462383,39533,CLN: localising scope of functions in `Styler`,attack68,closed,2021-02-01T15:32:42Z,2021-02-02T16:43:18Z,"- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

`Styler` has a mixture of function placements, e.g. outer scope, or as class methods or inner scope of class methods.

This PR has moved two outer scope functions to inner scope of class methods, where they are only used / relevant for that section, to be more consistent with the rest of the class.

"
799380274,39559,REF: move replace code out of Blocks,jbrockmendel,closed,2021-02-02T15:24:15Z,2021-02-02T19:22:16Z,
797818288,39512,CLN: Fix userguide deprecation warning new numpy,phofl,closed,2021-01-31T21:37:48Z,2021-02-02T21:14:21Z,This should get the ipython directives passing
797307255,39474,BUG: Regression cannot create Series with dtype 'S|' or 'bytes',TheNeuralBit,closed,2021-01-30T01:52:17Z,2021-02-02T22:16:55Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
s = pd.Series(['foo', 'bar', 'baz'])
assert s.astype(bytes).dtype.kind == 'S', ""kind is not S!""    # works in 1.1.x, 1.2.0, does not work in 1.2.1 (kind is 'O')
```

#### Problem description
Setting the type of a series to `bytes` changes str instances to `bytes`, but keeps the type as `object`. This seems to be a regression, in prior versions the type was changed to `bytesXX`. In pandas 1.2.1:

```py
In [1]: import pandas as pd                               
                                                                                                                                                                                                                                        
In [2]: pd.__version__                     
Out[2]: '1.2.1'                               
                                                                                                                    
In [3]: s = pd.Series(['foo', 'bar', 'baz'])                                                                                                                                                                                            
                                                                                                                    
In [4]: s.astype('|S')
Out[4]: 
0    b'foo'
1    b'bar'
2    b'baz'
dtype: object

In [5]: s.astype('bytes')
Out[5]: 
0    b'foo'
1    b'bar'
2    b'baz'
dtype: object

In [6]: s.astype(bytes)
Out[6]: 
0    b'foo'
1    b'bar'
2    b'baz'
dtype: object
```

#### Expected Output
I expected this output, produced with 1.2.0:

```py
In [1]: import pandas as pd

In [2]: pd.__version__
Out[2]: '1.2.0'

In [3]: s = pd.Series(['foo', 'bar', 'baz'])

In [4]: s.astype('|S')
Out[4]: 
0    b'foo'
1    b'bar'
2    b'baz'
dtype: bytes24

In [5]: s.astype('bytes')
Out[5]: 
0    b'foo'
1    b'bar'
2    b'baz'
dtype: bytes24

In [6]: s.astype(bytes)
Out[6]: 
0    b'foo'
1    b'bar'
2    b'baz'
dtype: bytes24
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS     
------------------     
commit           : 9d598a5e1eee26df95b3910e3f2934890d062caa
python           : 3.8.6.final.0
python-bits      : 64  
OS               : Linux
OS-release       : 5.7.17-1rodete4-amd64
Version          : #1 SMP Debian 5.7.17-1rodete4 (2020-10-01)
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.1
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 49.2.1
Cython           : 0.29.21
pytest           : 4.6.11
hypothesis       : None
sphinx           : 1.8.5
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.1
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.20
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
797500986,39484,BUG: Regression in astype not casting to bytes,phofl,closed,2021-01-30T18:35:59Z,2021-02-02T22:21:39Z,"- [x] closes #39474
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

Not sure if this is the right place"
639549497,34821,"BUG: read_excel(engine='openpyxl') with ""unsized"" XLSX issue",kuraga,closed,2020-06-16T10:21:39Z,2021-02-02T23:16:02Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Problem description

Consider code:
```python
import pandas as pd
pd.read_excel('test.xlsx', index_col=[0, 1], engine='openpyxl')
```
with some special `test.xlsx` (see below).

According to [openpyxl issue #1483](https://bitbucket.org/openpyxl/openpyxl/issues/1483) and [openpyxl documentation](https://openpyxl.readthedocs.io/en/stable/optimized.html#worksheet-dimensions), **user (i.e. Pandas) [have to](https://bitbucket.org/openpyxl/openpyxl/issues/1483#comment-57780692) call [`sheet.calculate_dimension(force=True)`](https://bitbucket.org/openpyxl/openpyxl/src/3.0.3/openpyxl/worksheet/_read_only.py#lines-133) if any workbook's worksheet is ""unsized""**.

(What is ""worksheet is unsized""? It means: the worksheet doesn't have `DIMENSION_TAG`, see [WorkSheetParser.parse_dimensions()](https://bitbucket.org/openpyxl/openpyxl/src/3.0.3/openpyxl/worksheet/_reader.py#lines-158). When it checked? On `ReadOnlyWorksheet` object construction.)

**But Pandas doesn't do it.**

So, if the worksheet is ""unsized"", on reading it by `read_excel(engine='openpyxl')` we try to [get sheet data](https://github.com/pandas-dev/pandas/blob/v1.0.3/pandas/io/excel/_base.py#L443). We  [iterate rows](https://github.com/pandas-dev/pandas/blob/v1.0.3/pandas/io/excel/_openpyxl.py#L539) falling down into openpyxl: [1](https://bitbucket.org/openpyxl/openpyxl/src/3.0.3/openpyxl/worksheet/_read_only.py#lines-29) -> [2](https://bitbucket.org/openpyxl/openpyxl/src/3.0.3/openpyxl/worksheet/worksheet.py#lines-454) -> [3](https://bitbucket.org/openpyxl/openpyxl/src/3.0.3/openpyxl/worksheet/_read_only.py#lines-27) -> [4](https://bitbucket.org/openpyxl/openpyxl/src/3.0.3/openpyxl/worksheet/worksheet.py#lines-401) (with `max_col == None`) -> [5](https://bitbucket.org/openpyxl/openpyxl/src/3.0.3/openpyxl/worksheet/worksheet.py#lines-433) -> [6](https://bitbucket.org/openpyxl/openpyxl/src/3.0.3/openpyxl/worksheet/_read_only.py#lines-185) (it returns `self._max_column == None`) -> [7](https://bitbucket.org/openpyxl/openpyxl/src/3.0.3/openpyxl/worksheet/worksheet.py#lines-436) -> [8](https://bitbucket.org/openpyxl/openpyxl/src/3.0.3/openpyxl/worksheet/_read_only.py#lines-57) (with `max_col == None`) -> [9](https://bitbucket.org/openpyxl/openpyxl/src/3.0.3/openpyxl/worksheet/_read_only.py#lines-89) ->  [10](https://bitbucket.org/openpyxl/openpyxl/src/3.0.3/openpyxl/worksheet/_read_only.py#lines-10) (with `max_col == None`). And in [these lines](https://bitbucket.org/openpyxl/openpyxl/src/ca7b1baf75f2fc6b270320ea91d82404f5039e1e/openpyxl/worksheet/_read_only.py#lines-107:108):
```python
max_col = max_col or  row[-1]['column']
row_width = max_col + 1 - min_col
```
we have: **row's cells number (`row_width`) is individual for each row (`row`)**.

Now consider an _unsized_ Excel table (I have a real file but it's private; after saving in LibreOffice it's become sized):
![image](https://user-images.githubusercontent.com/1063219/84760507-ce742d00-afd0-11ea-8caf-45031ba57a3b.png)
We get [sheet data](https://github.com/pandas-dev/pandas/blob/v1.0.3/pandas/io/excel/_base.py#L443):
```python
[ [`A1`],
  [ 'B1', 'B2' ] ]
```

But `index_col=[0, 1]` **suppose to have >=2 cols in each row**:
```
Traceback (most recent call last):
  File ""test.py"", line 6, in <module>
    pd.read_excel('test.xlsx', index_col=[0, 1], engine='openpyxl')
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 334, in read_excel
    **kwds,
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 888, in parse
    **kwds,
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 480, in parse
    last = data[offset][col]
IndexError: list index out of range
```

**IMHO, patch could be:**
```diff
diff --git a/pandas/io/excel/_openpyxl.py b/pandas/io/excel/_openpyxl.py
index c4327316d..3efbf4abc 100644
--- a/pandas/io/excel/_openpyxl.py
+++ b/pandas/io/excel/_openpyxl.py
@@ -536,6 +536,9 @@ class _OpenpyxlReader(_BaseExcelReader):
 
     def get_sheet_data(self, sheet, convert_float: bool) -> List[List[Scalar]]:
         data: List[List[Scalar]] = []
+
+        sheet.calculate_dimension(force=True)
+
         for row in sheet.rows:
             data.append([self._convert_cell(cell, convert_float) for cell in row])
```
(But it may provide [some drawbacks](https://bitbucket.org/openpyxl/openpyxl/issues/1483#comment-57780692)).
With this patch, sheet data would be:
```python
[ [`A1`, ''],
  [ 'B1', 'B2' ] ]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.42-calculate
machine          : x86_64
processor        : Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz
byteorder        : little
LC_ALL           : None
LANG             : ru_RU.utf8
LOCALE           : ru_RU.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.4.0.post20200518
Cython           : 0.29.17
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.8
numba            : None
</details>

Thanks!"
799770666,39569,CI: ipython tab completion tests,jbrockmendel,closed,2021-02-02T22:34:42Z,2021-02-02T23:41:21Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
799816924,39570,Backport PR #39569 on branch 1.2.x (CI: ipython tab completion tests),meeseeksmachine,closed,2021-02-02T23:34:59Z,2021-02-03T00:48:44Z,Backport PR #39569: CI: ipython tab completion tests
799834851,39571,CLN: re-use na_value_for_dtype,jbrockmendel,closed,2021-02-03T00:15:59Z,2021-02-03T02:08:35Z,"
"
797812445,39511,API: consistent default dropna for value_counts,jbrockmendel,closed,2021-01-31T21:08:27Z,2021-02-03T02:08:56Z,Also hopefully fixes some docbuild issues
797507836,39488,BUG: setting td64 value into numeric Series incorrectly casting to int,jbrockmendel,closed,2021-01-30T19:12:19Z,2021-02-03T02:11:38Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

We fixed this a few days ago for some of the setting-methods, but there were a few we missed.  Re-using the TestSetitemEquivalents pattern is much more thorough.

We'll be able to de-duplicate the test class following #39477"
797593715,39498,REF: Move agg helpers into apply,rhshadrach,closed,2021-01-31T01:27:22Z,2021-02-03T02:20:06Z,"- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
"
799607228,39562,pandas 1.2.0 - drop_duplicates(),ilyosbekkk,closed,2021-02-02T19:29:40Z,2021-02-03T02:30:39Z,"Hi everyone, I was using the .drop_duplicates() function to remove duplicate values from the .csv  file,  but unfortunately, it removed some non-duplicate values from my table.  So what would you recommend me to save non-duplicate values while dropping duplicates from the .csv  file using  drop_duplicates() function?
one small  part of  my  code:
            table = pd.read_csv(csv_path, low_memory=False, names=['timestamp', 'data'])
            table = table.drop_duplicates()
            table = table.sort_values(by=['timestamp'])
            table.to_csv(csv_path, index=False)
Thank you!

```
"
778450840,38960,TST: strict xfail,rhshadrach,closed,2021-01-04T23:37:53Z,2021-02-03T02:48:45Z,"- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

Part of #38902."
799945684,39573,BUG: groupby drops na values of categorical even if dropna=False is set,atkrueger,closed,2021-02-03T04:03:26Z,2021-02-03T04:11:51Z,"- [ x] I have checked that this issue has not already been reported.

- [x ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
df = pd.DataFrame({'a': [1,2,3]})
dtype = pd.api.types.CategoricalDtype([1,2])
df['b'] =df.a.astype(dtype)
df.groupby('b',dropna=False).size()


#### Problem description

observations for categorical variables with na values are dropped from groupby() even if dropna=False is specified.

#### Expected Output

b
nan    1
1        1
2        1

#### Actual output
b
1 1
2 1

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 9d598a5e1eee26df95b3910e3f2934890d062caa
python           : 3.9.0.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : AMD64 Family 25 Model 33 Stepping 0, AuthenticAMD
byteorder        : little
LC_ALL           : None
LANG             : en.UTF-8
LOCALE           : English_United States.1252

pandas           : 1.2.1
numpy            : 1.19.5
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.1
setuptools       : 50.3.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 3.0.0
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
798005725,39518,ENH: Using nrows option while processing xlsb files,code-R,closed,2021-02-01T06:16:56Z,2021-02-03T09:04:08Z,"#### Is your feature request related to a problem?

I wish pandas can use nrows eagerly while processing xlsx or xlsb files

https://github.com/pandas-dev/pandas/blob/master/pandas/io/excel/_pyxlsb.py#L73
If we see this code, this tries to process all the sheetdata and later apply nrows, which is very very slow in large files..in-fact in one of the xlsb files, the number of records are around 3k but, it was trying to loop around 100k records (because of xlsb metadata or something) even though I specify nrows as 3k, it has to wait processing all 100k records.

#### Describe the solution you'd like

If we can pass nrows to get_sheet_data and break the loop while it reaches the nrows number, then it will better.

#### API breaking implications

I am not really sure.

#### Describe alternatives you've considered

don't have alternatives at the moment, would like to hear suggestions about this.

#### Additional context

[add any other context, code examples, or references to existing implementations about the feature request here]
```python
df = pd.read_excel(""sample.xlsb"", nrows=100, engine=""pyxlsb"")
````

```python
def get_sheet_data(self, sheet, convert_float: bool, nrows: int) -> List[List[Scalar]]:
    res = []
    for index, r in enumerate(sheet.rows(sparse=False)):
        res.append([self._convert_cell(c, convert_float) for c in r])
        if index > nrows:
            break

    return res
```
We can set default value for nrows=infinity, incase if we dont want to break for others.
"
798211223,39524,STYLE Upgrade no-string-hints,MarcoGorelli,closed,2021-02-01T10:30:38Z,2021-02-03T13:14:26Z,"Precursor to #39501, in which the previous version of `no-string-hints` was removing the string literals even for `Literal[""foo""]`"
794358646,39412,BUG: make Index.where behavior mirror Index.putmask behavior,jbrockmendel,closed,2021-01-26T16:08:21Z,2021-02-03T15:05:22Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
799916379,39572,CLN: dtypes.concat,jbrockmendel,closed,2021-02-03T02:54:31Z,2021-02-03T15:06:34Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
798208922,39523,CI upgrade pyupgrade,MarcoGorelli,closed,2021-02-01T10:27:53Z,2021-02-03T18:08:05Z,"Issue here is:

1. in `.pre-commit-config.yaml`, change the `ref` of `pyupgrade` from v2.7.4 to v2.9.0
2. run `pre-commit run pyupgrade --all-files`
3. run `pre-commit run flake8 --all-files` - there will be lots of errors from unused imports, so you should remove them (or you could look into using [autoflake](https://github.com/myint/autoflake) to do it for you)
4. if both `pre-commit run flake8 --all-files` and `pre-commit run pyupgrade --all-files` both pass, then open a pull request :rocket: 

---

xref #39521"
798579426,39538,CI: Upgrade 'pyupgrade' (v2.7.4 --> v2.9.0),gunjan-solanki,closed,2021-02-01T17:55:27Z,2021-02-03T18:26:21Z,"- [x] closes #39523 
- [ ] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
------
This PR upgrades `pyupgrade` and also includes style/formatting changes required after upgrading `pyupgrade`, namely, removing unused imports (`flake8`), and running `black` and `isort`.
"
478575522,27821,Document security vulnerability reporting mechanism,TomAugspurger,closed,2019-08-08T16:32:59Z,2021-02-03T21:06:45Z,"Part of the tidelift setup. In https://github.com/numpy/numpy/issues/13475 / https://github.com/numpy/numpy/pull/13485, NumPy decided to just link to https://tidelift.com/docs/lifting/security. That should suffice to us.

Only thing to decide is who all gets notified with these reports."
800490690,39581,BUG: Dockerfile environment not working with latest pandas dev merge,ParfaitG,closed,2021-02-03T16:28:06Z,2021-02-03T21:23:54Z,"- [X] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

### Code Sample

**Bash** 

```git
docker build --tag pandas-parfaitg-env .
docker run -it --rm -v path-to-pandas-yourname:/home/pandas-parfaitg pandas-parfaitg-env

cd /home
ls
# pandas  pandas-parfaitg

cat > sandbox.py
# see script below

python sandbox.py
```
**Python** (sandbox.py)

```python
import sys
sys.path.remove('/home/pandas')
sys.path.append('/home/pandas-parfaitg')

import pandas as pd
```

### Problem Description

Currently, I am working on a pandas ENH PR and use a Docker container for my development environment, following exact instructions in [contributing docs](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#using-a-docker-container). Dockerfile builds image and runs container perfectly fine. However, after several weeks where above routine worked great, until yesterday evening after a `git fetch upstream` and `git merge upstream/master`, my development pandas `sandbox.py` code now errors out with the following message:

```python
Traceback (most recent call last):
  File ""sandbox.py"", line 5, in <module>
    import pandas as pd
  File ""/home/pandas-parfaitg/pandas/__init__.py"", line 51, in <module>
    from pandas.core.api import (
  File ""/home/pandas-parfaitg/pandas/core/api.py"", line 31, in <module>
    from pandas.core.groupby import Grouper, NamedAgg
  File ""/home/pandas-parfaitg/pandas/core/groupby/__init__.py"", line 1, in <module>
    from pandas.core.groupby.generic import DataFrameGroupBy, NamedAgg, SeriesGroupBy
  File ""/home/pandas-parfaitg/pandas/core/groupby/generic.py"", line 58, in <module>
    from pandas.core.aggregation import (
  File ""/home/pandas-parfaitg/pandas/core/aggregation.py"", line 41, in <module>
    from pandas.core.indexes.api import Index
  File ""/home/pandas-parfaitg/pandas/core/indexes/api.py"", line 24, in <module>
    from pandas.core.indexes.period import PeriodIndex
  File ""/home/pandas-parfaitg/pandas/core/indexes/period.py"", line 66, in <module>
    class PeriodIndex(DatetimeIndexOpsMixin):
  File ""/home/pandas-parfaitg/pandas/core/indexes/extension.py"", line 112, in wrapper
    meth = inherit_from_data(name, delegate, cache=cache, wrap=wrap)
  File ""/home/pandas-parfaitg/pandas/core/indexes/extension.py"", line 43, in inherit_from_data
    attr = getattr(delegate, name)
AttributeError: type object 'PeriodArray' has no attribute 'start_time'
````


If `sandbox.py` simply contains `import pandas` (the Docker build version), no error occurs. Does Dockerfile use last stable pandas build?

A few [commits have been merged yesterday evening](https://github.com/pandas-dev/pandas/commits/master/pandas/core) in the  `pandas/core` directory. Is this somehow related? I tried many `git` synches to be current with latest version to no avail. Is my forked repo or local branch the issue?

#### Expected Output

No error

#### Output of ``pd.show_versions()``

Unable to run since I am cannot `import pandas as pd` from `pandas/parfaitg`

<details>

...

</details>
"
787588643,39219,DOC/REF: Rolling/Expanding/EWM docstrings,mroeschke,closed,2021-01-17T01:10:52Z,2021-02-04T00:04:29Z,"- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them


- Create a new `window/doc.py` file for common template
- Using `@doc` decorator instead of `@Appender`
- Standardize descriptions and sections

<img width=""1255"" alt=""Screen Shot 2021-01-16 at 5 06 52 PM"" src=""https://user-images.githubusercontent.com/10647082/104828385-c7601680-5860-11eb-99b5-e73357719348.png"">
 
"
794594666,39426,BUG: sort_values create an unprintable object,gshimansky,closed,2021-01-26T22:05:21Z,2021-02-04T00:50:51Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd
import numpy as np

RAND_LOW = 0
RAND_HIGH = 100
NCOLS = 2 ** 5
NROWS = 2 ** 2

random_state = np.random.RandomState(seed=42)
data = {
    ""col{}"".format(int((i - NCOLS / 2) % NCOLS + 1)): random_state.randint(
        RAND_LOW, RAND_HIGH, size=(NROWS)
    )
    for i in range(NCOLS)
}
df1 = pd.DataFrame(data)
print(df1)
df2 = df1.sort_values(df1.index[0], axis=1, ignore_index=True)
print(df2)
```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]
Printing object returned by `sort_values` produces an exception because some internal metadata is wrong:

```
Traceback (most recent call last):
  File ""sort_values_test6.py"", line 19, in <module>
    print(df2)
  File ""/localdisk/gashiman/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py"", line 803, in __repr__
    self.to_string(
  File ""/localdisk/gashiman/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py"", line 939, in to_string
    return fmt.DataFrameRenderer(formatter).to_string(
  File ""/localdisk/gashiman/miniconda3/lib/python3.8/site-packages/pandas/io/formats/format.py"", line 1031, in to_string
    string = string_formatter.to_string()
  File ""/localdisk/gashiman/miniconda3/lib/python3.8/site-packages/pandas/io/formats/string.py"", line 23, in to_string
    text = self._get_string_representation()
  File ""/localdisk/gashiman/miniconda3/lib/python3.8/site-packages/pandas/io/formats/string.py"", line 47, in _get_string_representation
    return self._fit_strcols_to_terminal_width(strcols)
  File ""/localdisk/gashiman/miniconda3/lib/python3.8/site-packages/pandas/io/formats/string.py"", line 179, in _fit_strcols_to_terminal_width
    self.fmt.truncate()
  File ""/localdisk/gashiman/miniconda3/lib/python3.8/site-packages/pandas/io/formats/format.py"", line 700, in truncate
    self._truncate_horizontally()
  File ""/localdisk/gashiman/miniconda3/lib/python3.8/site-packages/pandas/io/formats/format.py"", line 718, in _truncate_horizontally
    self.tr_frame = concat((left, right), axis=1)
  File ""/localdisk/gashiman/miniconda3/lib/python3.8/site-packages/pandas/core/reshape/concat.py"", line 298, in concat
    return op.get_result()
  File ""/localdisk/gashiman/miniconda3/lib/python3.8/site-packages/pandas/core/reshape/concat.py"", line 520, in get_result
    new_data = concatenate_block_managers(
  File ""/localdisk/gashiman/miniconda3/lib/python3.8/site-packages/pandas/core/internals/concat.py"", line 89, in concatenate_block_managers
    return BlockManager(blocks, axes)
  File ""/localdisk/gashiman/miniconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py"", line 143, in __init__
    self._verify_integrity()
  File ""/localdisk/gashiman/miniconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py"", line 323, in _verify_integrity
    raise construction_error(tot_items, block.shape[1:], self.axes)
ValueError: Shape of passed values is (4, 16), indices imply (32, 16)
```

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]
INSTALLED VERSIONS
------------------
commit           : 9d598a5e1eee26df95b3910e3f2934890d062caa
python           : 3.8.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-56-generic
Version          : #62-Ubuntu SMP Mon Nov 23 19:20:19 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.1
numpy            : 1.19.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.3.1
setuptools       : 49.6.0.post20201009
Cython           : None
pytest           : 6.2.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : 0.4.1
xlsxwriter       : None
lxml.etree       : 4.6.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : None
fsspec           : 0.8.4
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : 0.14.1
pyarrow          : 2.0.0
pyxlsb           : None
s3fs             : 0.5.2
scipy            : 1.5.4
sqlalchemy       : 1.3.21
tables           : 3.6.1
tabulate         : None
xarray           : 0.16.2
xlrd             : 2.0.1
xlwt             : None
numba            : None

</details>
"
796552573,39464,BUG: Fix sort_values bug that creates unprintable object,zitorelova,closed,2021-01-29T03:46:02Z,2021-02-04T00:50:57Z,"- [x] closes #39426 
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

The problem occurs when sorting on columns and the `ignore_index` parameter is set to `True`. `sort_values` tries to refit the index after the columns have been sorted resulting in a corrupted dataframe."
600180431,33562,"BUG: df.style.apply(subset=...) argument only works for explicit slices but not "":""",dieterwang,closed,2020-04-15T10:08:52Z,2021-02-04T00:51:34Z,"Related to #25858 (if not the same?), reproduced on 0.25.3 and current version 1.0.3.

It seems that the `subset` argument for pd.DataFrame.style only works for slices with explicit labels, e.g. `['A',2]` but not for select-all labels, e.g. `[:,2]`. See MWE below

```python
import pandas as pd
pd.__version__ # 1.0.3

# Taken from 
# https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html
def color_negative_red(val):
    color = 'red' if val < 0 else 'black'
    return 'color: %s' % color

df = pd.DataFrame([
    [-1, 3, 6],
    [-2,-3, 2]
])
df.index = pd.MultiIndex.from_tuples([('A',1),('A',2)])

# Style works for applymap
df.style.applymap(color_negative_red)

# Works for explicit slice
slicer = pd.IndexSlice[pd.IndexSlice['A',2],:]
df.loc[slicer] # valid .loc slicer
df.style.applymap(color_negative_red, subset=slicer) # works

# Breaks for select-all slice
slicer = pd.IndexSlice[pd.IndexSlice[:,2],:]
df.loc[slicer] # valid .loc slicer
df.style.applymap(color_negative_red, subset=slicer)
# TypeError: unhashable type: 'slice'
```

"
800813494,39587,REGR: do not try to infer the excel type from a workbook object,twoertwein,closed,2021-02-04T00:14:51Z,2021-02-04T01:01:31Z,"- [ ] closes #39528
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
797450928,39481,BUG: Stack creates duplicate column,soerenwolfers,closed,2021-01-30T15:53:12Z,2021-02-04T01:52:52Z,"```
import numpy as np
import pandas as pd

df = pd.DataFrame(
    np.zeros([1, 5]), 
    columns=pd.MultiIndex.from_tuples(
        [
            (0, None, None), 
            (0, 0, 0),
            (0, 0, 1),
            (0, 1, 0),
            (0, 1, 1),
        ],
    )
)
```
![image](https://user-images.githubusercontent.com/16045385/106360891-efe11980-6312-11eb-8682-a5ed546b8d10.png)
```
df.stack(2)
```
![image](https://user-images.githubusercontent.com/16045385/106360914-05564380-6313-11eb-8ebb-b5a35a74f4b5.png)


```

INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.6.6.final.0
pandas           : 1.1.5
numpy            : 1.17.4
```"
800574446,39582,BUG: at/iat __setitem__ failing to cast,jbrockmendel,closed,2021-02-03T18:11:38Z,2021-02-04T02:33:52Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
800886125,39590,"BUG:When reading in a csv with MultiIndex columns, duplicated level names mangled",samgalen,closed,2021-02-04T03:08:17Z,2021-02-04T03:46:13Z,"- [X] I have checked that this issue has not already been reported. 

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.
 (I created a venv, and installed pandas version 1.2.1, but did not clone/build from source, not sure if that counts or not).
---

A csv with MultiIndexed columns, when read in appends a .1, .2, etc. unnecessarily. 

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

# creating a multi-Indexed csv

cols = [
    (""Left"", ""Tier 2"", ""Column 1""),
    (""Left"", ""Tier 2"", ""Column 2""),
    (""Left"", ""Tier 2"", ""Total""),
    (""Right"", ""Tier 2"", ""Total"")]

df = pd.DataFrame(np.arange(20).reshape((5,4)))
df.columns = pd.MultiIndex.from_tuples(cols)
print(df.head())

df.to_csv(""test.csv"")

read_df = pd.read_csv(""./test.csv"", header=[0,1], index_col=0)
print(read_df.head())

# output

# the original dataframe
      Left                 Right
    Tier 2                Tier 2
  Column 1 Column 2 Total  Total
0        0        1     2      3
1        4        5     6      7
2        8        9    10     11
3       12       13    14     15
4       16       17    18     19

# dataframe after being read back in from read_csv
         Left                     Right
       Tier 2  Tier 2.1 Tier 2.2 Tier 2
NaN  Column 1  Column 2    Total  Total
0.0         0         1        2      3
1.0         4         5        6      7
2.0         8         9       10     11
3.0        12        13       14     15

```

#### Problem description

In the example, the function read_csv appends a .1, .2 etc. to levels in a multi-indexed column. This is unnecessary, since the columns are uniquely identified by (""Left, ""Tier 2"", ""Column 1""), (""Left"", ""Tier 2"", ""Column 2""), so in theory there should be no column name overlap. This is additionally undesirable, as one of the main benefits to having a multi-index is the ability to access subsets of columns easily. 

#### Expected Output

The level names should not be mangled. Specifically, the expected output should be

```python
      Left                 Right
    Tier 2                Tier 2
  Column 1 Column 2 Total  Total
0        0        1     2      3
1        4        5     6      7
2        8        9    10     11
3       12       13    14     15
4       16       17    18     19
```

And 

```python
print(read_df.columns.values)
```

```python
array([(""Left"", ""Tier 2"", ""Column 1""),
       (""Left"", ""Tier 2"", ""Column 2""),
       (""Left"", ""Tier 2"", ""Total""),
       (""Right"", ""Tier 2"", ""Total"")])
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 9d598a5e1eee26df95b3910e3f2934890d062caa
python           : 3.9.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Tue Nov 10 00:07:31 PST 2020; root:xnu-4903.278.51~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.1
numpy            : 1.20.0
pytz             : 2021.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.2.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
147480645,12865,MetaIssue: Pandas tries too hard to be smart,DavidEscott,closed,2016-04-11T16:19:32Z,2021-02-04T03:52:35Z,"....and ends up doing things wrong.

I had the following task: Given a csv file A and a set of identically structured files X, Y, Z [which have keys in the first two columns, and values in subsequent columns]: I wanted to create a new file which has contents MAX(A.column_val, X.column_val + Y.column_val + Z.column_val).

I had previously written a pure python function to sort the individual columns, and then perform an merge-join on the columns. However conceptually this task is only a few lines of pandas code:

``` python
summed = pandas.concat([X, Y, Z]).groupby([""key_col1"", ""key_col2""]).sum()
output = pandas.concat([A, summed]).groupby([""key_col1"", ""key_col2""]).max()
output.to_csv(""something.csv"")
```

So surely switching to pandas would make the code easier to understand and maintain. Right? Nope!!!

All of the following were issues I had to work around:
1. The results of sum()/max() have heirarchical indexes and so I must liberally sprinkle `reset_index()` at particular points in my code.
2. Pandas tries to be smart and assumes that the empty string in my key_cols is a NULL string and converts it to NaN. This is annoying because now I have to add `na_rep=""""` to my `to_csv` command.
3. More significantly though, pandas thinks that a NaN in a groupby column is reason to exclude the entire row... so now I have to add a bunch of lines to `fillna('')` (and that doesn't modify the frame you call it on, but rather returns a copy so you have to `fillna('', inplace=True)`
4. Because pandas doesn't support columns with integer values and NA values it has cast all my ints to floats and my output now looks like `1.0, 2.0`. instead of `1, 2`. So I have to add a `float_format` directive to the `to_csv`.
5. Pandas also likes to output a row number, so `index=False` has to get added to `to_csv.`

The end result is that I am not of the opinion that the resulting pandas implementation is any more legible or maintainable. In three weeks am I going to remember why I have set `index=False` or what the hell that means? Am I going to understand why the calls to `reset_index` are necessary, or what they do? Probably not, which makes me concerned about using this code in production.
"
794559400,39421,BUG: Subset slicer on Styler failed on MultiIndex with slice(None) ,attack68,closed,2021-01-26T21:03:49Z,2021-02-04T06:42:32Z,"- [x] closes #33562
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

The problem with above issue was with the function `non_reducing_slice`. It didn't correctly re-format slicing tuples on multiindices when rows or cols also contained a `slice(None)`. An additional check is now done. This method is only used for `Styler` currently.

Tests Added / reformmatted."
799965998,39575,TST/REF: de-duplicate SetitemCastingEquivalents,jbrockmendel,closed,2021-02-03T04:51:24Z,2021-02-04T15:13:36Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
800854350,39589,TST: Remove duplicate invalid sheet tests from io.excel,rhshadrach,closed,2021-02-04T01:52:04Z,2021-02-04T15:51:47Z,"- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

Followup from #39482 - removed tests are duplicates of `test_readers.test_bad_sheetname_raises`
"
792922619,39382,Make get_loc with nan for FloatIndex consistent with other index types,phofl,closed,2021-01-24T22:59:18Z,2021-02-04T17:57:19Z,"- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

Have not added a whatsnew because I am not sure if 2 is expected here. You could also make a point for retruning nan. But to handle these cases consistently, we have to check for method here"
792351219,39342,TYP: Fix mypy ignores in parsers,phofl,closed,2021-01-22T22:39:38Z,2021-02-04T17:57:23Z,"- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

Tried fixing the Mypy Errors in parsers
"
797801270,39507,BUG: DataFrame.stack not handling NaN in MultiIndex columns correct,phofl,closed,2021-01-31T20:14:27Z,2021-02-04T23:12:37Z,"- [x] closes #39481
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
801604870,39598,TST: misplaced tests,jbrockmendel,closed,2021-02-04T20:17:47Z,2021-02-05T00:02:51Z,
778601697,38964,"BUG: Timedelta.min.ceil(""us"") returns NaT",jbrockmendel,closed,2021-01-05T05:38:31Z,2021-02-05T03:11:54Z,"
```
>>> Timedelta.min.ceil(""us"")
NaT

>>> pd.Timedelta.max.floor(""us"")
OverflowError: Python int too large to convert to C long
```"
778348310,38956,BUG: read_excel() using openpyxl engine header argument not working as expected,JuliaWilkinsSonos,closed,2021-01-04T20:35:24Z,2021-02-05T03:15:59Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example
I am happy to provide the file I am using privately, offline if necessary. This would be necessary to directly reproduce this error. 
```
import pandas as pd
df = pd.read_excel(myfile.xlsx, engine='openpyxl', header=2)
```
Throws error:
```...ValueError: Passed header=2 but only 2 lines in file```

#### Problem description
I changed my `read_excel()` code to be now using `engine='openpyxl` with the 1.2 `read_excel()` changes, but the `header` argument is no longer working as it was before. I am trying to read in a .xlsx file that has 2 initial rows that should be skipped, and the 3rd row contains the headers. Thus, I am passing `header=2`, which should automatically skip the first two rows and use the 3rd (0-indexed) row as the headers.  However, I am getting the following error: `ValueError: Passed header=2 but only 2 lines in file`. I have confirmed multiple times that this file contains more than 2 lines - it contains thousands (happy to provide this file offline if necessary). I was able to previously run this code with the same file (before the `xlrd` deprecation, using the same header specification. With the openpyxl changes in 1.2, should we be using the `header` argument any differently? Thank you.

This is related to changes in https://github.com/pandas-dev/pandas/pull/35029.

#### Expected Output
Intended behavior would be that the .xlsx file is successfully read in as a dataframe using headers in row 3 and does not throw the above error.

#### Output of ``pd.show_versions()``
<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.19.4
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 50.3.2
Cython           : None
pytest           : 4.3.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 4.3.1
pyxlsb           : None
s3fs             : 0.4.2
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 2.0.1
xlwt             : None
xlsxwriter       : None
numba            : None
```
</details>"
801735506,39601,BUG: Timedelta.round near implementation bounds,jbrockmendel,closed,2021-02-05T00:20:32Z,2021-02-05T04:39:24Z,"- [x] closes #38964
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
801947525,39608,Minor fix in read_csv docs,albertvillanova,closed,2021-02-05T08:04:27Z,2021-02-05T08:53:13Z,"Fix typo.
"
799640002,39563,"ENH: Styler accepts css-strings input args as well as list of (attr,value) pairs",attack68,closed,2021-02-02T20:09:27Z,2021-02-05T14:57:51Z,"#### Is your feature request related to a problem?

Currently the `styler` has the ability to `set_table_styles` provided they are characterised with a css-selector and list of attribute value pairs, e.g.

```
styler.set_table_styles([{'selector': 'th', 'props': [('color', 'red'), ('font-size', '1em'), ('font-weight',  'bold')]}])
```

Even the legacy docs describe this as a bit of a pain, and for someone used to css, they will be typically looking for a css-style solution:

```
styler.set_table_styles([{'selector': 'th', 'props': 'color:red;font-size:1em;font-weight:bold;'}])
```

#### Describe the solution you'd like

optionally use the input argument valid CSS format above.

#### API breaking implications

none if backwards compatible.


"
406672323,25154,"Multiindex slicing with NaNs, unexpected results",tunnij,closed,2019-02-05T08:21:14Z,2021-02-05T15:41:15Z,"#### Code Sample, a copy-pastable example if possible
```python
import pandas as pd
df = pd.DataFrame(
    pd.np.random.rand(2, 3), 
    columns=pd.MultiIndex.from_tuples([('a', 'foo'), ('b', 'bar'), ('b', pd.np.nan)], names=['first','second'])
)
# EXPECTED slicing everything on first level
df.loc[:, (['a', 'b'])]
Out[35]: 
first          a         b          
second       foo       bar       NaN
0       0.678021  0.383672  0.074164
1       0.738492  0.992545  0.661247

# EXPECTED just slicing one value from first level
df.loc[:, (['b'])]
Out[29]: 
first          b          
second       bar       NaN
0       0.383672  0.074164
1       0.992545  0.661247

# EXPECTED slicing out b, bar
df.loc[:, (['b'], ['bar'])]
Out[33]: 
first          b
second       bar
0       0.383672
1       0.992545

# UNEXPECTED slicing out b, nan
df.loc[:, (['b'], [pd.np.nan])]
Out[36]: 
Empty DataFrame
Columns: []
Index: [0, 1]

# UNEXPECTED slicing out b, [nan, 'bar']
df.loc[:, (['b'], ['bar', pd.np.nan])]
Out[39]: 
first          b
second       bar
0       0.383672
1       0.992545

# EXPECTED slicing out b, nan without the index
df.loc[:, ('b', pd.np.nan)]
Out[37]: 
0    0.074164
1    0.661247
Name: (b, nan), dtype: float64

```
#### Problem description
When trying to slice out multiple values from a particular level including levels with a nan value, the levels with nan are not retrieved. 

#### Expected Output
Both of these I expect to work:
```python
df.loc[:, (['b'], ['bar', pd.np.nan])]
Out[40]: 
first          b          
second       bar       NaN
0       0.383672  0.074164
1       0.992545  0.661247

df.loc[:, (['b'], [pd.np.nan])]
Out[40]: 
first          b          
second       NaN
0       0.074164
1       0.661247
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.15.final.0
python-bits: 64
OS: Linux
OS-release: 3.10.0-327.36.3.el7.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None
pandas: 0.22.0
pytest: 3.10.0
pip: 18.1
setuptools: 40.5.0
Cython: 0.28.5
numpy: 1.14.2
scipy: 1.0.1
pyarrow: None
xarray: 0.10.9
IPython: 5.8.0
sphinx: 1.8.1
patsy: 0.5.1
dateutil: 2.7.2
pytz: 2018.7
blosc: None
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.7
feather: None
matplotlib: 2.2.3
openpyxl: 2.5.9
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.1.2
lxml: 4.2.1
bs4: 4.6.3
html5lib: 1.0.1
sqlalchemy: 1.2.11
pymysql: None
psycopg2: 2.7.5 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
800983778,39591,PERF: Rolling/Expanding.cov/corr,mroeschke,closed,2021-02-04T06:40:51Z,2021-02-05T17:49:23Z,"- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

Also I think I stumbled on some bugs along the way; added whatsnew and modified the existing tests where necessary.

```
$ asv_bench % asv continuous -f 1.1 upstream/master HEAD -b rolling.Pairwise.time_groupby

       before           after         ratio
     [f79468b8]       [6a1e8592]
     <master>         <ref/groupby_rolling_pairwise>
-        299±50ms         178±20ms     0.59  rolling.Pairwise.time_groupby(1000, 'corr', True)
-         272±7ms         157±20ms     0.58  rolling.Pairwise.time_groupby(1000, 'cov', True)
-         262±1ms          146±4ms     0.56  rolling.Pairwise.time_groupby(None, 'cov', True)
-        303±10ms          158±6ms     0.52  rolling.Pairwise.time_groupby(10, 'cov', True)
-        279±20ms        139±0.7ms     0.50  rolling.Pairwise.time_groupby(None, 'corr', True)
-        315±10ms         149±10ms     0.47  rolling.Pairwise.time_groupby(10, 'corr', True)
-         137±2ms       34.0±0.8ms     0.25  rolling.Pairwise.time_groupby(10, 'cov', False)
-         127±5ms         28.2±1ms     0.22  rolling.Pairwise.time_groupby(None, 'cov', False)
-         134±5ms       28.1±0.4ms     0.21  rolling.Pairwise.time_groupby(1000, 'cov', False)
-         135±2ms         27.7±1ms     0.21  rolling.Pairwise.time_groupby(None, 'corr', False)
-        178±10ms         33.7±2ms     0.19  rolling.Pairwise.time_groupby(10, 'corr', False)
-        159±20ms         29.9±2ms     0.19  rolling.Pairwise.time_groupby(1000, 'corr', False)

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE INCREASED.
```"
799643193,39564,ENH: Styler css Str optional input arguments as well as List[Tuple],attack68,closed,2021-02-02T20:13:16Z,2021-02-05T20:07:59Z,"- [x] closes #39563 
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
799167809,39554,BUG: Rolling.count modifies 'min_periods' inplace since 1.2.0,dchigarev,closed,2021-02-02T10:52:53Z,2021-02-05T20:24:03Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import numpy as np
import pandas
from pandas.testing import assert_frame_equal

data = {
    ""col1"": [np.nan, 1, 2]
}

pd_df = pandas.DataFrame(data)
pd_rolled = pd_df.rolling(window=2, min_periods=None)

res1 = pd_rolled.sum()
pd_rolled.count()
res2 = pd_rolled.sum()

assert_frame_equal(res1, res2) # AssertionError

```

<details><summary>Output</summary>


```
AssertionError: DataFrame.iloc[:, 0] (column name=""col1"") are different

DataFrame.iloc[:, 0] (column name=""col1"") values are different (66.66667 %)
[index]: [0, 1, 2]
[left]:  [nan, nan, 3.0]
[right]: [0.0, 1.0, 3.0]
```


</details>

#### Problem description
Two sequential calls of `.sum` on the rolling object produces different results if we call `.count` between them and `min_periods=None`. The default behavior of `Rolling.sum` if `min_periods` is None is to consider `min_periods` to be equal to the window size. Currently, `Rolling.count` behaves differently, and considers `min_periods` to be 0 if it is None. #36649 brought a warning that this behavior is deprecated and also refactored `.count` implementation. Right after giving a warning, it modifies the original value of `min_periods` of the rolling object, so the future calls of `.sum` and other operations give incorrect results.
https://github.com/pandas-dev/pandas/blob/5e02681cb9edc6f8d9e5397e9bafcf940a9cd9dc/pandas/core/window/rolling.py#L2138-L2149

#### Expected Output
`Rolling.count` should not modify `min_periods` attribute of the rolling object, or if it is, revert back the original value of `min_periods` after performing `count`

#### Output of ``pd.show_versions()``

<details>

```

INSTALLED VERSIONS
------------------
commit           : 9d598a5e1eee26df95b3910e3f2934890d062caa
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-50-generic
Version          : #54-Ubuntu SMP Mon May 6 18:46:08 UTC 2019
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.1
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1.post20200622
Cython           : None
pytest           : 6.0.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : 0.4.1
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : 0.7.4
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : 0.13.2
pyarrow          : 1.0.1
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.1
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : None
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : None
numba            : None
```

</details>
"
494704477,28485,Matplotlib's ConciseDateFormatter-like formatter in pandas.plotting._matplotlib.converter.py,EBenkler,closed,2019-09-17T15:44:47Z,2021-02-05T20:40:32Z,"Matplotlib has a nice formatter class named matplotlib.dates.ConciseDateFormatter, which is a candidate to become Matplotlib's default date formatter in future versions (as of MPL 3.1.1, see 
[Matplotlib documentation](https://matplotlib.org/gallery/ticks_and_spines/date_concise_formatter.html)).

It would be nice if the converters defined in pandas.plotting._matplotlib.converter.py
could provide the same date formatting, especially regarding the pandas data types like Period."
738346536,37693,QST: read_stata encoding problem..,fdq09eca,closed,2020-11-08T00:48:40Z,2021-02-06T00:06:56Z,"I have a `.dta` file with Vietnamese categorial data.

`df = pd.read_stata('MucA.dta')` produces the following df:
```
                   tinh  huyen     xa  diaban  hoso  matv m1ac2     m1ac3  ...               m1ac12  m1ac13  m1ac14a m1ac14b m1ac15a m1ac15b m1ac15c m1ac15d
0      Thµnh phè Hµ Néi      1      4       8    13     2   Nam  Vî chång  ...      Trong x· ph­êng     NaN      NaN     NaN   Kh«ng   Kh«ng      Cã   Kh«ng
1      Thµnh phè Hµ Néi      1      4       8    13     1    N÷    Chñ hé  ...      Trong x· ph­êng     NaN      NaN     NaN   Kh«ng   Kh«ng      Cã   Kh«ng
2      Thµnh phè Hµ Néi      1      4       8    14     2   Nam      Kh¸c  ...  N¬i kh¸c trong tØnh     NaN      NaN     NaN   Kh«ng   Kh«ng   Kh«ng   Kh«ng
3      Thµnh phè Hµ Néi      1      4       8    14     1    N÷    Chñ hé  ...      Trong x· ph­êng     NaN      NaN     NaN   Kh«ng   Kh«ng   Kh«ng   Kh«ng
4      Thµnh phè Hµ Néi      1      4       8    15     2    N÷  Vî chång  ...      Trong x· ph­êng     NaN      NaN     NaN   Kh«ng      Cã   Kh«ng   Kh«ng
...                 ...    ...    ...     ...   ...   ...   ...       ...  ...                  ...     ...      ...     ...     ...     ...     ...     ...
36076       TØnh Cµ Mau    973  32248       2    15     4    N÷       Con  ...      Trong x· ph­êng     NaN      NaN     NaN     NaN     NaN     NaN     NaN
36077       TØnh Cµ Mau    973  32248       2    15     5    N÷       Con  ...      Trong x· ph­êng     NaN      NaN     NaN     NaN     NaN     NaN     NaN
36078       TØnh Cµ Mau    973  32248       2    15     1   Nam    Chñ hé  ...      Trong x· ph­êng     NaN      NaN     NaN   Kh«ng   Kh«ng   Kh«ng   Kh«ng
36079       TØnh Cµ Mau    973  32248       2    15     2    N÷  Vî chång  ...      Trong x· ph­êng     NaN      NaN     NaN   Kh«ng   Kh«ng   Kh«ng   Kh«ng
36080       TØnh Cµ Mau    973  32248       2    15     3    N÷       Con  ...      Trong x· ph­êng     NaN      NaN     NaN   Kh«ng   Kh«ng   Kh«ng   Kh«ng
```
I want to try other encodings to restore the Vietnamese characters, how may I do so?"
790662621,39312,REF/TST: Move Series/DataFrame apply tests to tests.apply,rhshadrach,closed,2021-01-21T03:46:29Z,2021-02-06T13:56:43Z,"- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

Ref: https://github.com/pandas-dev/pandas/pull/39212#issuecomment-763112628

Just a move. Will be following up with combining Series/DataFrame tests where appropriate."
802214573,39613,BUG: Inconsistency between Period.ordinal and Timestamp.fromordinal(),EBenkler,closed,2021-02-05T14:36:27Z,2021-02-06T16:39:51Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
ordinaldate = pd.Period(pd.Timestamp('2020-12-31'), freq='s').ordinal
pd.Timestamp.fromordinal(ordinaldate)
```

#### Problem description

The code snippet raises the error message:
*** ValueError: year 4406314 is out of range

#### Expected Output

Timestamp('2020-12-31 00:00:00')

Maybe I completely misunderstand the concept of Period and Timestamp, but I would expect that when I convert from a Period to an ordinal date, and subsequently convert back to a Timestamp, I should get the initial Timestamp.

I am specifically interested in the inverse operation of the conversion to ordinal in class PeriodConverter(dates.DateConverter)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 9d598a5e1eee26df95b3910e3f2934890d062caa
python           : 3.8.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 42 Stepping 7, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en
LOCALE           : de_DE.cp1252

pandas           : 1.2.1
numpy            : 1.20.0
pytz             : 2021.1
dateutil         : 2.8.1
pip              : 21.0.1
setuptools       : 49.6.0.post20210108
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.4.3
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.3
IPython          : 7.20.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.4
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 3.0.0
pyxlsb           : None
s3fs             : None
scipy            : 1.6.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
384212313,23920,Install numpy and cython when you run make  if they were not present ,srinivasreddy,closed,2018-11-26T07:57:23Z,2021-02-06T18:51:12Z,"I created a new virtual env  and run `make`, the installation exited with saying 
```
02:31 $ make
python setup.py build_ext --inplace
Traceback (most recent call last):
  File ""/home/dtdev/.virtualenvs/pandas/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 357, in get_provider
    module = sys.modules[moduleOrReq]
KeyError: 'numpy'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""setup.py"", line 727, in <module>
    ext_modules=maybe_cythonize(extensions, compiler_directives=directives),
  File ""setup.py"", line 470, in maybe_cythonize
    numpy_incl = pkg_resources.resource_filename('numpy', 'core/include')
  File ""/home/dtdev/.virtualenvs/pandas/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 1142, in resource_filename
    return get_provider(package_or_requirement).get_resource_filename(
  File ""/home/dtdev/.virtualenvs/pandas/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 359, in get_provider
    __import__(moduleOrReq)
ModuleNotFoundError: No module named 'numpy'
```
then installed `numpy`, then ran make again,

```
02:31 $ make
python setup.py build_ext --inplace
running build_ext
pandas._libs.algos: -> [['pandas/_libs/algos.c']]
Traceback (most recent call last):
  File ""setup.py"", line 738, in <module>
    **setuptools_kwargs)
  File ""/home/dtdev/.virtualenvs/pandas/lib/python3.6/site-packages/setuptools/__init__.py"", line 143, in setup
    return distutils.core.setup(**attrs)
  File ""/usr/lib64/python3.6/distutils/core.py"", line 148, in setup
    dist.run_commands()
  File ""/usr/lib64/python3.6/distutils/dist.py"", line 955, in run_commands
    self.run_command(cmd)
  File ""/usr/lib64/python3.6/distutils/dist.py"", line 974, in run_command
    cmd_obj.run()
  File ""/usr/lib64/python3.6/distutils/command/build_ext.py"", line 339, in run
    self.build_extensions()
  File ""setup.py"", line 366, in build_extensions
    self.check_cython_extensions(self.extensions)
  File ""setup.py"", line 363, in check_cython_extensions
    """""".format(src=src))
Exception: Cython-generated file 'pandas/_libs/algos.c' not found.
                Cython is required to compile pandas from a development branch.
                Please install Cython or download a release package of pandas.
```

Why not install numpy and cython if they were not present ?  

I think we should fix the Makefile ."
443414241,26368,corrwith in 0.24 is much slower than 0.23 (especially if corr axis is smaller than other axis),yavitzour,closed,2019-05-13T13:58:07Z,2021-02-06T18:57:55Z,"
Hi,

I've noticed that corrwith on pandas 0.24 is much slower than in 0.23, especially when trying to correlate dataframes where the length of the axis of correlation is much smaller than the length of the other axis.

Example:

```python
import pandas as pd
import numpy as np

df1 = pd.DataFrame(np.random.rand(10000, 100))
df2 = pd.DataFrame(np.random.rand(10000, 100))
df1.corrwith(df2, axis=1)
```
With pandas 0.23.4 the snippet above finishes in about 0.1 sec, whereas with pandas 0.24.1 it takes about 10 seconds (a 100 times slower...). 

If we increase the length of the correlation axis, 0.23.4 still performs much better, but the results are a bit less dramatic, for example with 10000 on both axes:

```python
df1 = pd.DataFrame(np.random.rand(10000, 10000))
df2 = pd.DataFrame(np.random.rand(10000, 10000))
df1.corrwith(df2, axis=1)
```
Pandas 0.23.4 finishes in ~10 seconds whereas pandas 0.24.1 finishes in about ~30 seconds (""only"" 3 times slower)

Thanks!"
668982753,35480,"BUG:weekofyear error,The last three days of 2008 are considered the first week",princelai,closed,2020-07-30T17:09:09Z,2021-02-07T08:07:44Z,"- [Yes ] I have checked that this issue has not already been reported.

- [ No] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
y = pd.Series(pd.date_range(start='2008-12-27',periods=7,freq='D'))
y.dt.isocalendar().week
```

#### Problem description
This is an obvious mistake, isn't it?
|    | d                   |   w |
|---:|:--------------------|----:|
|  0 | 2008-12-27 00:00:00 |  52 |
|  1 | 2008-12-28 00:00:00 |  52 |
|  2 | 2008-12-29 00:00:00 |   1 |
|  3 | 2008-12-30 00:00:00 |   1 |
|  4 | 2008-12-31 00:00:00 |   1 |
|  5 | 2009-01-01 00:00:00 |   1 |
|  6 | 2009-01-02 00:00:00 |   1 |

#### Expected Output
|    | d                   |   w |
|---:|:--------------------|----:|
|  0 | 2008-12-27 00:00:00 |  52 |
|  1 | 2008-12-28 00:00:00 |  52 |
|  2 | 2008-12-29 00:00:00 |   52 |
|  3 | 2008-12-30 00:00:00 |   52 |
|  4 | 2008-12-31 00:00:00 |   52 |
|  5 | 2009-01-01 00:00:00 |   1 |
|  6 | 2009-01-02 00:00:00 |   1 |

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.53-1-MANJARO
Version          : #1 SMP PREEMPT Wed Jul 22 15:04:56 UTC 2020
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2
setuptools       : 49.2.0
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : 0.10.0
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 1.0.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.50.1
</details>
"
797504755,39486,BUG: read_excel with openpyxl and missing dimension,rhshadrach,closed,2021-01-30T18:55:51Z,2021-02-07T12:05:14Z,"- [x] closes #38956
- [x] closes #39001
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

Targeted for 1.2.2 because this resolves an issue that occurs from the change of default engine from xlrd to openpyxl in 1.2."
801810685,39604,REGR: Rolling.count setting min_periods after call,mroeschke,closed,2021-02-05T03:28:52Z,2021-02-07T12:23:43Z,"- [x] closes #39554
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
798900238,39551,CI: numpy deprecation warnings,jbrockmendel,closed,2021-02-02T03:47:50Z,2021-02-07T12:33:51Z,
802936840,39633,Backport PR #39486: BUG: read_excel with openpyxl and missing dimension',simonjayhawkins,closed,2021-02-07T12:04:34Z,2021-02-07T13:08:23Z,Backport PR #39486
802940171,39634,Backport PR #39604: REGR: Rolling.count setting min_periods after call,simonjayhawkins,closed,2021-02-07T12:22:37Z,2021-02-07T13:14:37Z,Backport PR #39604
802942013,39635,Backport PR #39551: CI: numpy deprecation warnings,simonjayhawkins,closed,2021-02-07T12:33:16Z,2021-02-07T13:32:04Z,Backport PR #39551
790484893,39309,BUG: read_sas() value error related to datetime format,AshleyOboe,closed,2021-01-21T00:14:56Z,2021-02-07T15:17:10Z,"- [x ] I have checked that this issue has not already been reported.

- [x ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
def convert_encounter_chunks(file, inpath, outpath, param_encoding='latin1', param_chunksize=500000, debug=False):
    starttime=datetime.datetime.now()
    print(""start time:-"", starttime ) 
   
    filepath=f""{TNP_BUCKET}/{file}""
    print('converting ', filepath )
    dfreader=pd.read_sas(filepath,
                        encoding=param_encoding,
                        chunksize=param_chunksize,iterator=True)

    label=file.split('.')[0]
    outpath_base=f""{outpath}/{label}""

    iter=0
    for chunk in dfreader:

        chunk.to_parquet(f""{outpath_base}_{iter}"")
        itertime=datetime.datetime.now()
        print(""chunk time:-"", itertime ) 
        iterdiff=itertime-starttime
        cumminutes = iterdiff.seconds / 60
        print('Cumulative Time in minutes: ', cumminutes) 
        
        iter=iter+1

        if debug==True and iter>1:
            print('Debug mode on only running two iterations')
            break
    
    print(""end time:-"", endtime ) 
   #display(dfs[0])

```

#### Problem description

A value error is returned related to date formats.  I believe the data value meets the datetime criteria but then there is an error when actually implementing the datetime. 

Here's the stacktrace: 
```python-traceback
`---------------------------------------------------------------------------
OutOfBoundsDatetime                       Traceback (most recent call last)
/opt/conda/lib/python3.7/site-packages/pandas/io/sas/sas7bdat.py in _convert_datetimes(sas_datetimes, unit)
     51     try:
---> 52         return pd.to_datetime(sas_datetimes, unit=unit, origin=""1960-01-01"")
     53     except OutOfBoundsDatetime:

/opt/conda/lib/python3.7/site-packages/pandas/core/tools/datetimes.py in to_datetime(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)
    804         else:
--> 805             values = convert_listlike(arg._values, format)
    806             result = arg._constructor(values, index=arg.index, name=arg.name)

/opt/conda/lib/python3.7/site-packages/pandas/core/tools/datetimes.py in _convert_listlike_datetimes(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)
    345             result, tz_parsed = tslib.array_with_unit_to_datetime(
--> 346                 arg, unit, errors=errors
    347             )

pandas/_libs/tslib.pyx in pandas._libs.tslib.array_with_unit_to_datetime()

OutOfBoundsDatetime: cannot convert input with unit 'd'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-21-6b87792f6a22> in <module>
      1 myfile='nicuid_encounter_infant.sas7bdat'
----> 2 convert_encounter_chunks(myfile, TNP_BUCKET, TNP_BUCKET_CONVERTED, param_chunksize=2000000,  debug=False)

<ipython-input-20-9b8da485bcb0> in convert_encounter_chunks(file, inpath, outpath, param_encoding, param_chunksize, debug)
     21     dfs = [] # holds data chunks
     22     iter=0
---> 23     for chunk in dfreader:
     24         #dfs.append(chunk)
     25         #df=dfs[iter]

/opt/conda/lib/python3.7/site-packages/pandas/io/sas/sas7bdat.py in __next__(self)
    330 
    331     def __next__(self):
--> 332         da = self.read(nrows=self.chunksize or 1)
    333         if da is None:
    334             self.close()

/opt/conda/lib/python3.7/site-packages/pandas/io/sas/sas7bdat.py in read(self, nrows)
    721         p.read(nrows)
    722 
--> 723         rslt = self._chunk_to_dataframe()
    724         if self.index is not None:
    725             rslt = rslt.set_index(self.index)

/opt/conda/lib/python3.7/site-packages/pandas/io/sas/sas7bdat.py in _chunk_to_dataframe(self)
    769                 if self.convert_dates:
    770                     if self.column_formats[j] in const.sas_date_formats:
--> 771                         rslt[name] = _convert_datetimes(rslt[name], ""d"")
    772                     elif self.column_formats[j] in const.sas_datetime_formats:
    773                         rslt[name] = _convert_datetimes(rslt[name], ""s"")

/opt/conda/lib/python3.7/site-packages/pandas/io/sas/sas7bdat.py in _convert_datetimes(sas_datetimes, unit)
     58         elif unit == ""d"":
     59             return sas_datetimes.apply(
---> 60                 lambda sas_float: datetime(1960, 1, 1) + timedelta(days=sas_float)
     61             )
     62         else:

/opt/conda/lib/python3.7/site-packages/pandas/core/series.py in apply(self, func, convert_dtype, args, **kwds)
   4106             else:
   4107                 values = self.astype(object)._values
-> 4108                 mapped = lib.map_infer(values, f, convert=convert_dtype)
   4109 
   4110         if len(mapped) and isinstance(mapped[0], Series):

pandas/_libs/lib.pyx in pandas._libs.lib.map_infer()

/opt/conda/lib/python3.7/site-packages/pandas/io/sas/sas7bdat.py in <lambda>(sas_float)
     58         elif unit == ""d"":
     59             return sas_datetimes.apply(
---> 60                 lambda sas_float: datetime(1960, 1, 1) + timedelta(days=sas_float)
     61             )
     62         else:

ValueError: cannot convert float NaN to integer
`
```

#### Expected Output

No error / successful conversion


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.7.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.0-12-cloud-amd64
Version          : #1 SMP Debian 4.19.152-1 (2020-10-18)
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 49.6.0.post20201009
Cython           : 0.29.21
pytest           : 6.1.2
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.4
fastparquet      : 0.5.0
gcsfs            : 0.7.1
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 2.0.0
pyxlsb           : None
s3fs             : None
scipy            : 1.5.3
sqlalchemy       : 1.3.20
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.49.1

</details>
"
781631210,39027,REF: make FreqGroup an Enum,jbrockmendel,closed,2021-01-07T21:44:54Z,2021-01-08T15:53:46Z,
778592650,38963,DEPR: Rolling.win_type returning freq & is_datetimelike,mroeschke,closed,2021-01-05T05:17:07Z,2021-01-08T17:26:42Z,"- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

xref: https://github.com/pandas-dev/pandas/pull/38641#issuecomment-754118989
xref: https://github.com/pandas-dev/pandas/pull/38664/files#r551419130"
776572455,38823,CLN: Simplify inspect_excel_format,rhshadrach,closed,2020-12-30T18:02:22Z,2021-01-08T21:39:23Z,"ref: #38819

The function `io.excel._base.inspect_excel_format` does not need to take two parameters `path` and `content` since either is passed to get_handle. It should instead take a single parameter, `path_or_buf`."
780846072,39008,CLN: inspect_excel_format,lithomas1,closed,2021-01-06T21:05:55Z,2021-01-08T22:11:48Z,"- [x] closes #38823 
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

cc @rhshadrach "
781709761,39030,"CLN: add typing to dtype arg in core/internals, core/reshape and core (GH38808)",avinashpancham,closed,2021-01-07T23:30:44Z,2021-01-08T23:09:19Z,"incremental PR for issue #38808

- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
781589850,39025,BUG: Missing keys using aggregation dictionary that are unsortable raise TypeError instead of  SpecificationError,simonjayhawkins,closed,2021-01-07T20:30:39Z,2021-01-08T23:17:59Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
>>> import numpy as np
>>> import pandas as pd
>>>
>>> pd.__version__
'1.3.0.dev0+358.g68db2d26dd'
>>>
>>> df = pd.DataFrame(
...     np.random.randn(1000, 3),
...     index=pd.date_range(""1/1/2012"", freq=""S"", periods=1000),
...     columns=[1, ""foo"", None],
... )
>>> r = df.resample(""3T"")
>>> r.agg({1: ""mean"", ""foo"": ""sum""})
                            1        foo
2012-01-01 00:00:00 -0.070340   9.142790
2012-01-01 00:03:00 -0.048538  31.041777
2012-01-01 00:06:00 -0.058046  -0.394660
2012-01-01 00:09:00  0.014268  -8.947485
2012-01-01 00:12:00  0.040080  -6.869409
2012-01-01 00:15:00  0.020587   0.225159
>>>
>>> r.agg({2: ""mean"", ""bar"": ""sum""})
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\simon\pandas\pandas\core\resample.py"", line 298, in aggregate
    result, how = aggregate(self, func, *args, **kwargs)
  File ""C:\Users\simon\pandas\pandas\core\aggregation.py"", line 566, in aggregate
    return agg_dict_like(obj, arg, _axis), True
  File ""C:\Users\simon\pandas\pandas\core\aggregation.py"", line 741, in agg_dict_like
    cols = sorted(set(keys) - set(selected_obj.columns.intersection(keys)))
TypeError: '<' not supported between instances of 'str' and 'int'
>>>
>>> df = pd.DataFrame(
...     np.random.randn(1000, 3),
...     index=pd.date_range(""1/1/2012"", freq=""S"", periods=1000),
...     columns=[""A"", ""B"", ""C""],
... )
>>>
>>> r = df.resample(""3T"")
>>> r.agg({""r1"": ""mean"", ""r2"": ""sum""})
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\simon\pandas\pandas\core\resample.py"", line 298, in aggregate
    result, how = aggregate(self, func, *args, **kwargs)
  File ""C:\Users\simon\pandas\pandas\core\aggregation.py"", line 566, in aggregate
    return agg_dict_like(obj, arg, _axis), True
  File ""C:\Users\simon\pandas\pandas\core\aggregation.py"", line 742, in agg_dict_like
    raise SpecificationError(f""Column(s) {cols} do not exist"")
pandas.core.base.SpecificationError: Column(s) ['r1', 'r2'] do not exist
>>>
```

code sample based on test_agg_consistency in pandas/tests/resample/test_resample_api.py

#### Problem description

with mypy 0.790

pandas/core/aggregation.py:741: error: Value of type variable ""_LT"" of ""sorted"" cannot be ""Optional[Hashable]""  [type-var]

#### Expected Output

pandas.core.base.SpecificationError: Column(s) [2, 'bar'] do not exist

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
781638674,39028,BUG: Resample.aggregate raising TypeError instead of SpecificationError with missing keys dtypes,phofl,closed,2021-01-07T21:58:05Z,2021-01-08T23:18:38Z,"- [x] closes #39025
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
782407195,39042,REF: Prep for moving Series.apply into apply,rhshadrach,closed,2021-01-08T21:31:27Z,2021-01-09T01:57:25Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

Will be moving Series.apply into apply next. The plan is to have a base class Apply which is parent to both FrameApply and SeriesApply."
781628504,39026,TYP/CLN: Use futures annotations in apply,rhshadrach,closed,2021-01-07T21:39:52Z,2021-01-09T01:57:48Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
739552393,37732,CLN: clean setup.py,fangchenli,closed,2020-11-10T04:04:44Z,2021-01-09T17:10:58Z,an attempt to clean setup.py
782449195,39045,Update awkward phrasing,davidschlachter,closed,2021-01-08T23:10:20Z,2021-01-09T21:21:54Z,"Docs: change

""The builtin options available in each of the pandas plot functions that are worthwhile to have a look.""

to

""The builtin options available in each of the pandas plot functions are worth reviewing."""
779312013,38974,BUG: pd.to_numeric does not copy _mask for ExtensionArrays,arw2019,closed,2021-01-05T17:15:32Z,2021-01-09T21:22:56Z,"In #38746 while implementing `to_numeric` for ExtensionArrays I do not copy the mask of the original input. This means that the (potentially) cast array shares a mask with the input. We should copy the mask.

```python
In [9]: import pandas as pd
   ...: import pandas._testing as tm
   ...: 
   ...: arr = pd.array([1, 2, pd.NA], dtype=""Int64"")
   ...: 
   ...: result = pd.to_numeric(arr, downcast=""integer"")
   ...: expected = pd.array([1, 2, pd.NA], dtype=""Int8"")
   ...: tm.assert_extension_array_equal(result, expected)
   ...: 
   ...: arr[1] = pd.NA # should not modify result
   ...: tm.assert_extension_array_equal(result, expected)
   ...: 
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-9-f72c43e18273> in <module>
      9 
     10 arr[1] = pd.NA
---> 11 tm.assert_extension_array_equal(result, expected)

~/repos/pandas/pandas/_testing/asserters.py in assert_extension_array_equal(left, right, check_dtype, index_values, check_less_precise, check_exact, rtol, atol)
    794     left_na = np.asarray(left.isna())
    795     right_na = np.asarray(right.isna())
--> 796     assert_numpy_array_equal(
    797         left_na, right_na, obj=""ExtensionArray NA mask"", index_values=index_values
    798     )

    [... skipping hidden 1 frame]

~/repos/pandas/pandas/_testing/asserters.py in _raise(left, right, err_msg)
    699             diff = diff * 100.0 / left.size
    700             msg = f""{obj} values are different ({np.round(diff, 5)} %)""
--> 701             raise_assert_detail(obj, msg, left, right, index_values=index_values)
    702 
    703         raise AssertionError(err_msg)

~/repos/pandas/pandas/_testing/asserters.py in raise_assert_detail(obj, message, left, right, diff, index_values)
    629         msg += f""\n[diff]: {diff}""
    630 
--> 631     raise AssertionError(msg)
    632 
    633 

AssertionError: ExtensionArray NA mask are different

ExtensionArray NA mask values are different (33.33333 %)
[left]:  [False, True, True]
[right]: [False, False, True]
```

Thanks @jorisvandenbossche for pointing this out!"
782511645,39049,BUG: pd.to_numeric does not copy _mask for ExtensionArrays ,arw2019,closed,2021-01-09T04:42:40Z,2021-01-09T21:22:59Z,"- [x] closes #38974 
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry (not needed as bug on master only)

Fixing a bug I introduced in  #38746"
782627913,39061,REF: Move Series.apply/agg into apply,rhshadrach,closed,2021-01-09T15:22:44Z,2021-01-09T21:25:23Z,"- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

Will do follow up to share list/dict/str code in Apply for how=apply; they are essentially identical."
777470877,38900,"REGR: Buffer dtype mismatch, expected 'Python object' but got a string",forgetso,closed,2021-01-02T14:42:12Z,2021-01-09T22:05:13Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
>>> import pandas as pd
>>> df = pd.DataFrame(['o',""o'doul's"",'oad','oaf','oafish'])
>>> df = df.astype('|S')
>>> df
             0
0         b'o'
1  b""o'doul's""
2       b'oad'
3       b'oaf'
4    b'oafish'
>>> import numpy as np
>>> df.replace({None:np.nan})


```

#### Problem description

This was working in 1.1.5

```
    df[c].replace({None: np.NaN}, inplace=True)
  File ""/home/user/anaconda3/envs/uniqueness/lib/python3.8/site-packages/pandas/core/series.py"", line 4479, in replace
    return super().replace(
  File ""/home/user/anaconda3/envs/uniqueness/lib/python3.8/site-packages/pandas/core/generic.py"", line 6840, in replace
    return self.replace(
  File ""/home/user/anaconda3/envs/uniqueness/lib/python3.8/site-packages/pandas/core/series.py"", line 4479, in replace
    return super().replace(
  File ""/home/user/anaconda3/envs/uniqueness/lib/python3.8/site-packages/pandas/core/generic.py"", line 6889, in replace
    new_data = self._mgr.replace_list(
  File ""/home/user/anaconda3/envs/uniqueness/lib/python3.8/site-packages/pandas/core/internals/managers.py"", line 664, in replace_list
    bm = self.apply(
  File ""/home/user/anaconda3/envs/uniqueness/lib/python3.8/site-packages/pandas/core/internals/managers.py"", line 427, in apply
    applied = getattr(b, f)(**kwargs)
  File ""/home/user/anaconda3/envs/uniqueness/lib/python3.8/site-packages/pandas/core/internals/blocks.py"", line 910, in _replace_list
    [b.convert(numeric=False, copy=True) for b in result]
  File ""/home/user/anaconda3/envs/uniqueness/lib/python3.8/site-packages/pandas/core/internals/blocks.py"", line 910, in <listcomp>
    [b.convert(numeric=False, copy=True) for b in result]
  File ""/home/user/anaconda3/envs/uniqueness/lib/python3.8/site-packages/pandas/core/internals/blocks.py"", line 2558, in convert
    values = f(None, self.values.ravel(), None)
  File ""/home/user/anaconda3/envs/uniqueness/lib/python3.8/site-packages/pandas/core/internals/blocks.py"", line 2542, in f
    values = soft_convert_objects(
  File ""/home/user/anaconda3/envs/uniqueness/lib/python3.8/site-packages/pandas/core/dtypes/cast.py"", line 1139, in soft_convert_objects
    values = lib.maybe_convert_objects(values, convert_datetime=True)
  File ""pandas/_libs/lib.pyx"", line 2125, in pandas._libs.lib.maybe_convert_objects
ValueError: Buffer dtype mismatch, expected 'Python object' but got a string
```

#### Expected Output

Nones are replaced with nans

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-128-generic
Version          : #131-Ubuntu SMP Wed Dec 9 06:57:35 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.5
numpy            : 1.19.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.2
Cython           : 3.0a6
pytest           : 6.1.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 3.0.0a1
IPython          : 7.17.0
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.20
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.52.0
</details>



"
634895809,34655,read_csv cannot use dtype and true_values/false_values,JonnyWaffles,closed,2020-06-08T20:00:40Z,2021-01-09T22:18:23Z,"Hi friends,

Not sure if this is working as intended, but it appears you cannot use both dtype and true_values or false_values kwargs when reading csv. 

```python
from io import StringIO
from csv import writer

import pandas as pd
import pytest


def test_pandas_read_write():
    df = pd.DataFrame({'A': ['yes', 'no'], 'B': ['yes', 'no']})
    out_io = StringIO(newline='')
    df.to_csv(out_io, index=False)

    out_io.seek(0)
    pd.read_csv(out_io)

    out_io.seek(0)
    kwargs = dict(dtype={'A': 'boolean', 'B': 'boolean'})

    with pytest.raises(ValueError):
        pd.read_csv(out_io, **kwargs)

    kwargs.update({'true_values': ['yes'], 'false_values': ['no']})
    out_io.seek(0)

    with pytest.raises(ValueError):
        pd.read_csv(out_io, **kwargs)

    out_io.seek(0)
    # pop dtype so true/false values work
    kwargs.pop('dtype')
    pd.read_csv(out_io, **kwargs)
```
Using converters kwarg to get the data how I like will fix my problem, but we may want to update the docs to let users know true/false user defined values will not work in conjugation with providing the boolean type."
780923971,39012,BUG: read_csv raising ValueError for tru_values/false_values and boolean dtype,phofl,closed,2021-01-07T00:06:13Z,2021-01-09T22:22:25Z,"- [x] closes #34655
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

I am not really experienced with cython, so I would appreciate feedback on the switiching function. This was not done previously in case of ea boolean dtype, hence why this was failing before."
782672773,39065,Regression in replace raising ValueError for bytes object,phofl,closed,2021-01-09T19:42:51Z,2021-01-09T22:23:17Z,"- [x] closes #38900
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
778286338,38952,BUG: DataFrame.iloc.setitem with IntegerArray values,jbrockmendel,closed,2021-01-04T18:45:33Z,2021-01-09T22:26:22Z,"```
arr = pd.array([4, 5, 6])
df = pd.DataFrame(np.arange(3))

>>> df.iloc[:2] = arr[:2]
ValueError: could not broadcast input array from shape (2) into shape (2,1)

>>> df.iloc[:2] = arr[:2].to_numpy()  # <-- works fine
```

This bug would not occur with 2D EAs."
782610951,39055,QST: How to print specific element from dataframe?,debasisc-opa,closed,2021-01-09T13:46:14Z,2021-01-09T22:58:38Z,"- [ ] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

**Note**: If you'd still like to submit a question, please read [this guide](
https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your question.

```python
# Your code here, if applicable

https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html

```
df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list(""ABCD""))
print(""Print the dataframe which is like a matrix"")
print(df)
```

This works. But how do I print specific element of the dataframe? What is the syntax?

print(df[0:1] prints one row

print(df[0:2] prints two rows.

How do I print exact element (ex: row1, value of column A)?
Also length of a dataframe. Here, we know length=6 because we have created.

```
"
782343218,39040,BUG: DataFrame.iloc.__setitem__ with IntegerArray values,jbrockmendel,closed,2021-01-08T19:23:03Z,2021-01-09T23:27:40Z,"- [x] closes #38952
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
291109989,19371,API: Add closed to IntervalDtype,jschendel,closed,2018-01-24T07:32:31Z,2021-01-10T00:38:28Z,"This was brought up by @shoyer in a different issue, and it makes sense to me

Adding `closed` to `IntervalDtype` makes sense to me, as in my mind `closed` is inherently part of the dtype.  Since differing `closed` makes two `IntervalIndex` incompatible, I'd expect their dtypes to not be equal, but they currently are, which seems a little strange:
```python
In [2]: ii1 = pd.interval_range(0, 3, closed='left')

In [3]: ii2 = pd.interval_range(0, 3, closed='right')

In [4]: ii1.dtype == ii2.dtype
Out[4]: True
```

There's also a larger discussion as to if adding `closed` to `IntervalDtype` would allow us to remove `closed` as a parameter to the `IntervalIndex` constructor.  My preference would be to keep the `closed` parameter for user convenience, similar to how `CategoricalIndex` accepts `categories` and `ordered` parameters despite also accepting `CategoricalDtype` (though maybe this is just for legacy reasons).  Willing to be convinced otherwise though.

xref https://github.com/pandas-dev/pandas/issues/19263#issuecomment-359986104 (original comment)
xref #19370 (might be rendered moot)

cc @shoyer"
782695414,39076,CI: Benchmarks in CI are failing because xlrd was updated to 2.0.1,phofl,closed,2021-01-09T22:16:05Z,2021-01-10T00:41:50Z,"Raises in Benchmark run (for example https://github.com/pandas-dev/pandas/runs/1674714425) because it seems xlrd was updated on conda-forge
https://anaconda.org/conda-forge/xlrd"
782561545,39050,BUG: .diff() not working on int8 and int16 dtype,nrcjea001,closed,2021-01-09T08:27:47Z,2021-01-10T00:42:30Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

```python
# Your code here
df=pd.DataFrame({'a':[1,2,3,3,2],'b':[0,1,1,0,0]})
df['b']=df['b'].astype(np.int8)
df.groupby('a')['b'].diff()

---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-23-f6026edf0092> in <module>
      1 df=pd.DataFrame({'a':[1,2,3,3,2],'b':[0,1,1,0,0]})
      2 df['b']=df['b'].astype(np.int8)
----> 3 df.groupby('a')['b'].diff()

~\Anaconda3\lib\site-packages\pandas\core\groupby\groupby.py in wrapper(*args, **kwargs)
    815                 return self.apply(curried)
    816 
--> 817             return self._python_apply_general(curried, self._obj_with_exclusions)
    818 
    819         wrapper.__name__ = name

~\Anaconda3\lib\site-packages\pandas\core\groupby\groupby.py in _python_apply_general(self, f, data)
    926             data after applying f
    927         """"""
--> 928         keys, values, mutated = self.grouper.apply(f, data, self.axis)
    929 
    930         return self._wrap_applied_output(

~\Anaconda3\lib\site-packages\pandas\core\groupby\ops.py in apply(self, f, data, axis)
    235             # group might be modified
    236             group_axes = group.axes
--> 237             res = f(group)
    238             if not _is_indexed_like(res, group_axes, axis):
    239                 mutated = True

~\Anaconda3\lib\site-packages\pandas\core\groupby\groupby.py in curried(x)
    804 
    805             def curried(x):
--> 806                 return f(x, *args, **kwargs)
    807 
    808             # preserve the name so we can detect it when calling plot methods,

~\Anaconda3\lib\site-packages\pandas\core\series.py in diff(self, periods)
   2436         {examples}
   2437         """"""
-> 2438         result = algorithms.diff(self.array, periods)
   2439         return self._constructor(result, index=self.index).__finalize__(
   2440             self, method=""diff""

~\Anaconda3\lib\site-packages\pandas\core\algorithms.py in diff(arr, n, axis, stacklevel)
   2000         # TODO: can diff_2d dtype specialization troubles be fixed by defining
   2001         #  out_arr inside diff_2d?
-> 2002         algos.diff_2d(arr, out_arr, n, axis, datetimelike=is_timedelta)
   2003     else:
   2004         # To keep mypy happy, _res_indexer is a list while res_indexer is

pandas\_libs\algos.pyx in pandas._libs.algos.diff_2d()

NotImplementedError: 
```

#### Problem description

I try to take the difference of dataframe on columns with dtype int8 and int16 but this yields an error. This was working in previous version of pandas, 1.1.5, but not in current version, 1.2.0.

#### Expected Output

```python
0    NaN
1    NaN
2    NaN
3   -1.0
4   -1.0
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 11, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.2.0
numpy            : 1.19.5
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 49.6.0.post20210108
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.6.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : None
fsspec           : 0.8.5
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 2.0.0
pyxlsb           : None
s3fs             : None
scipy            : 1.6.0
sqlalchemy       : None
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.2
xlrd             : None
xlwt             : None
numba            : 0.52.0

</details>
"
782716287,39080,Auto backport of pr 39069 on 1.2.x,jreback,closed,2021-01-10T00:56:13Z,2021-01-10T00:56:31Z,
780048512,38994,DOC: elaborate on copies vs in place operations in comparison docs,afeld,closed,2021-01-06T05:20:24Z,2021-01-10T01:13:26Z,"<img width=""782"" alt=""Screen Shot 2021-01-06 at 12 15 05 AM"" src=""https://user-images.githubusercontent.com/86842/103731907-437e7280-4fb4-11eb-9e6d-e702f117656c.png"">

- [ ] ~~closes #xxxx~~
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] ~~whatsnew entry~~
"
760769542,38394,"ENH: make ""closed"" part of IntervalDtype",jbrockmendel,closed,2020-12-10T00:08:54Z,2021-01-10T02:07:08Z,"- [x] closes #19371
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Most of the work here is in accomodating backwards-compat."
782714746,39078,Backport PR #39077 on branch 1.2.x (Remove xlrd benchmark after xlrd was updated to 2.0.1 on conda-forge),meeseeksmachine,closed,2021-01-10T00:42:17Z,2021-01-10T02:32:08Z,Backport PR #39077: Remove xlrd benchmark after xlrd was updated to 2.0.1 on conda-forge
782694002,39075,Backport PR #39065 on branch 1.2.x (Regression in replace raising ValueError for bytes object),meeseeksmachine,closed,2021-01-09T22:06:34Z,2021-01-10T02:32:32Z,Backport PR #39065: Regression in replace raising ValueError for bytes object
782716452,39081,"Backport PR #39069: REGR: diff_2d raising for int8, int16",jreback,closed,2021-01-10T00:57:30Z,2021-01-10T02:37:20Z,
782692427,39074,Fixed #39073 Add STUMPY to Pandas ecosystem docs,seanlaw,closed,2021-01-09T21:56:28Z,2021-01-10T02:39:08Z,"- [ ] closes #39073
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
777510890,38906,BUG: reindexing empty CategoricalIndex fails if target contains duplicates,batterseapower,closed,2021-01-02T19:16:35Z,2021-01-10T02:39:53Z,"- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of pandas. (Tested version 1.2.0)
- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

This fails:

```python
pd.DataFrame(columns=pd.CategoricalIndex([]), index=['K']).reindex(columns=pd.CategoricalIndex(['A', 'A']))
```

But these succeed:

```python
pd.DataFrame(columns=pd.Index([]), index=['K']).reindex(columns=pd.CategoricalIndex(['A', 'A']))
pd.DataFrame(columns=pd.CategoricalIndex([]), index=['K']).reindex(columns=pd.CategoricalIndex(['A', 'B']))
pd.DataFrame(columns=pd.CategoricalIndex([]), index=['K']).reindex(columns=pd.CategoricalIndex([]))
```

The error is:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\mboling\Anaconda3\envs\pandastest\lib\site-packages\pandas\util\_decorators.py"", line 312, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\mboling\Anaconda3\envs\pandastest\lib\site-packages\pandas\core\frame.py"", line 4173, in reindex
    return super().reindex(**kwargs)
  File ""C:\Users\mboling\Anaconda3\envs\pandastest\lib\site-packages\pandas\core\generic.py"", line 4806, in reindex
    return self._reindex_axes(
  File ""C:\Users\mboling\Anaconda3\envs\pandastest\lib\site-packages\pandas\core\frame.py"", line 4013, in _reindex_axes
    frame = frame._reindex_columns(
  File ""C:\Users\mboling\Anaconda3\envs\pandastest\lib\site-packages\pandas\core\frame.py"", line 4055, in _reindex_columns
    new_columns, indexer = self.columns.reindex(
  File ""C:\Users\mboling\Anaconda3\envs\pandastest\lib\site-packages\pandas\core\indexes\category.py"", line 448, in reindex
    new_target, indexer, _ = result._reindex_non_unique(np.array(target))
  File ""C:\Users\mboling\Anaconda3\envs\pandastest\lib\site-packages\pandas\core\indexes\base.py"", line 3589, in _reindex_non_unique
    new_indexer = np.arange(len(self.take(indexer)))
  File ""C:\Users\mboling\Anaconda3\envs\pandastest\lib\site-packages\pandas\core\indexes\base.py"", line 751, in take
    taken = algos.take(
  File ""C:\Users\mboling\Anaconda3\envs\pandastest\lib\site-packages\pandas\core\algorithms.py"", line 1657, in take
    result = arr.take(indices, axis=axis)
IndexError: cannot do a non-empty take from an empty axes.
```

#### Problem description

It is unexpected that `CategoricalIndex` behaves differently than `Index` in this regard. A problem similar to this was already reported and solved in #16770, but it looks like there is a remaining bug in the edge case where the target index contains duplicates.

#### Expected Output

The failing code should return a dataframe with two columns and one row.

#### Output of ``pd.show_versions()``

<details>
pd.show_versions() fails because I dont' have numba installed in this environment and it's currently impossible to install it simultaneously with Pandas 1.2.0, so this is the output of ""conda list"" instead:

blas                      1.0                         mkl
bottleneck                1.3.2            py39h7cc1a96_1
ca-certificates           2020.12.8            haa95532_0
certifi                   2020.12.5        py39haa95532_0
et_xmlfile                1.0.1                   py_1001
icc_rt                    2019.0.0             h0cc432a_1
intel-openmp              2020.3             h57928b3_311    conda-forge
jdcal                     1.4.1                      py_0
libblas                   3.9.0                     5_mkl    conda-forge
libcblas                  3.9.0                     5_mkl    conda-forge
liblapack                 3.9.0                     5_mkl    conda-forge
mkl                       2020.4             hb70f87d_311    conda-forge
mkl-service               2.3.0            py39h196d8e1_0
numpy                     1.19.4           py39h6635163_1    conda-forge
openpyxl                  3.0.5                      py_0
openssl                   1.1.1i               h2bbff1b_0
pandas                    1.2.0            py39h2e25243_0    conda-forge
pip                       20.3.3             pyhd8ed1ab_0    conda-forge
python                    3.9.1           h7840368_2_cpython    conda-forge
python-dateutil           2.8.1                      py_0    conda-forge
python_abi                3.9                      1_cp39    conda-forge
pytz                      2020.5             pyhd8ed1ab_0    conda-forge
scipy                     1.5.2            py39h14eb087_0
setuptools                49.6.0           py39h467e6f4_2    conda-forge
six                       1.15.0             pyh9f0ad1d_0    conda-forge
sqlite                    3.34.0               h8ffe710_0    conda-forge
tzdata                    2020f                he74cb21_0    conda-forge
vc                        14.2                 hb210afc_2    conda-forge
vs2015_runtime            14.28.29325          h5e1d092_0    conda-forge
wheel                     0.36.2             pyhd3deb0d_0    conda-forge
wincertstore              0.2             py39hde42818_1005    conda-forge
xlrd                      2.0.1              pyhd3eb1b0_0
</details>
"
782459711,39046,BUG: reindexing empty CategoricalIndex would fail if target had duplicates,batterseapower,closed,2021-01-08T23:44:42Z,2021-01-10T02:39:57Z,"- [x] closes #38906
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
778547629,38962,BUG: DataFrame.any() with not returning Boolean series with skipna=False across columns with numeric and string types,rmwenzel,closed,2021-01-05T03:33:00Z,2021-01-10T04:42:03Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

If the dataframe has columns have different numeric types 
```python
import pandas as pd
import numpy as np
df = pd.DataFrame([[1, 0], [np.nan, 1]], columns=list('ab'))
df.any(axis=1, skipna=False)
```
the behavior is as expected

```python
0    True
1    True
dtype: bool
```

But if the columns have numeric and string types 

```python
df = pd.DataFrame([[1, 's'], [np.nan, 't']], columns=list('ab'))
df.any(axis=1, skipna=False)
```
it looks like the first column is returned

```python
0      1
1    NaN
dtype: object
```

The mixed types across rows doesn't seem to be a problem, if I do

```python
df = pd.DataFrame([['s', 1], [np.nan, 't']], columns=list('ab'))
df.any(axis=0, skipna=False)
```

I get

```python
a    True
b    True
dtype: bool
```

Also don't have any issues if `skipna=True`.

Expected behavior is to get a series of booleans either way.


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.9.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Mon Aug 31 22:12:52 PDT 2020; root:xnu-6153.141.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.5
numpy            : 1.19.2
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.0.0.post20201207
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
782675966,39066,TYP: ALL_NUMPY_DTYPES mypy errors with numpy-1.20,simonjayhawkins,closed,2021-01-09T20:03:59Z,2021-01-10T09:23:31Z,"prevents mypy errors

pandas/_testing/__init__.py:122: error: Unsupported operand types for + (""List[Union[ExtensionDtype, Union[str, dtype[Any]], Type[str], Type[float], Type[int], Type[complex], Type[bool], Type[object]]]"" and ""List[object]"")  [operator]
pandas/_testing/__init__.py:123: error: Unsupported operand types for + (""List[Union[ExtensionDtype, Union[str, dtype[Any]], Type[str], Type[float], Type[int], Type[complex], Type[bool], Type[object]]]"" and ""List[object]"")  [operator]
pandas/_testing/__init__.py:124: error: Unsupported operand types for + (""List[Union[ExtensionDtype, Union[str, dtype[Any]], Type[str], Type[float], Type[int], Type[complex], Type[bool], Type[object]]]"" and ""List[object]"")  [operator]"
782613649,39056,TYP: remove ignore from makeMissingDataframe,simonjayhawkins,closed,2021-01-09T14:01:40Z,2021-01-10T09:24:15Z,xref #37715
782614217,39057,TYP: remove ignore from makeCustomIndex,simonjayhawkins,closed,2021-01-09T14:04:45Z,2021-01-10T09:25:27Z,xref #37715
782588096,39052,TYP: remove ignores from set_testing_mode and reset_testing_mode,simonjayhawkins,closed,2021-01-09T11:29:12Z,2021-01-10T09:26:01Z,"xref #37715

The error message is self-explanatory although the original code passing a tuple seemed to work also, although not documented.

"
782684671,39070,TYP: PeriodArray._unbox_scalar,simonjayhawkins,closed,2021-01-09T21:02:08Z,2021-01-10T09:32:13Z,"mypy errors with numpy 1.20

pandas/core/arrays/period.py:273: error: Return type ""int"" of ""_unbox_scalar"" incompatible with return type ""Union[signedinteger[_64Bit], datetime64, timedelta64]"" in supertype ""DatetimeLikeArrayMixin""  [override]
pandas/core/arrays/period.py:277: error: Incompatible return value type (got ""signedinteger[_64Bit]"", expected ""int"")  [return-value]
pandas/core/arrays/period.py:280: error: Incompatible return value type (got ""signedinteger[_64Bit]"", expected ""int"")  [return-value]"
781147185,39014,DOC: Add whatsnew,vangorade,closed,2021-01-07T08:58:40Z,2021-01-10T11:00:28Z,"- [x] closes #38878
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
780463061,38998,DOC: Update contributing.rst,vangorade,closed,2021-01-06T10:52:38Z,2021-01-10T11:03:10Z,"- [x] closes #38938
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
778769340,38967,TST: add missing iloc label indexing tests,vangorade,closed,2021-01-05T09:00:38Z,2021-01-10T11:05:52Z,"- [x] closes #38967
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
782604417,39054,TYP: remove ignore from all_timeseries_index_generator,simonjayhawkins,closed,2021-01-09T13:06:04Z,2021-01-10T12:34:34Z,xref #37715
782086004,39036,"CLN, TYP use from __future__ import annotations",MarcoGorelli,closed,2021-01-08T12:30:21Z,2021-01-10T13:59:02Z,"Seeing as pandas is Python3.7+ we can clean up some annotations. Not yet figured out how to write a good automated check for this, but here's a start"
720796025,37108,"TYP: fix typing errors for mypy==0.790, bump mypy version",fangchenli,closed,2020-10-13T20:31:00Z,2021-01-10T15:29:51Z,"- [x] closes  #37104
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
774712126,38694,DOC add command to run pre-commit checks to pull request template,MarcoGorelli,closed,2020-12-25T11:13:22Z,2021-01-10T18:30:49Z,"I think we could add
```bash
passes `pre-commit run --from-ref=upstream/master --to-ref=HEAD --all-files`
```
to the pull request template (maybe it could even replace the current `black` and `flake8` ones).

The file to change would be
```
.github/PULL_REQUEST_TEMPLATE.md
```

Some of the contributing guide may also need updating if it mentions the `flake8 --diff` command."
782817840,39090,CI: Skip numpy dev failing tests,phofl,closed,2021-01-10T13:23:02Z,2021-01-10T20:12:33Z,"- [x] xref #39089
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

"
782692077,39073,DOC: Add STUMPY to pandas ecosystem,seanlaw,closed,2021-01-09T21:54:13Z,2021-01-10T20:25:46Z,"#### Location of the documentation

[Pandas Ecosystem - Stats & ML](https://pandas.pydata.org/docs/ecosystem.html#statistics-and-machine-learning)

#### Documentation problem

This isn't a problem, just an addition to the current ecosystem listing.

#### Suggested fix for documentation

This is a simple addition to the `ecosystem.rst` file for adding [STUMPY](https://github.com/TDAmeritrade/stumpy) to the existing docs"
782695921,39077,Remove xlrd benchmark after xlrd was updated to 2.0.1 on conda-forge,phofl,closed,2021-01-09T22:19:47Z,2021-01-10T20:27:25Z,"- [x] closes #39076
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

Is pinning here the right thing to do in environment.yml?

cc @jreback "
782153435,39038,BUG: Setting closed='neither' for rolling function of Series leads to mean() producing nan values,Safeturn,closed,2021-01-08T14:23:25Z,2021-01-11T07:31:23Z,"- [x] I have checked that this issue has not already been reported.

- [x ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
#### Code Sample, a copy-pastable example

```python
import numpy as np
import pandas as pd

series = pd.Series(np.arange(20), index=pd.date_range(""2018-01-01"", periods=20, freq=""M""))
series.rolling(window=5, closed='neither').mean()
```

#### Problem description

In the current version of pandas, the call to mean() produces a series of nan-values. For pandas 1.1.5, the output was different providing valid values starting from the fifth entry of the series.

#### Expected Output

2018-01-31     NaN
2018-02-28     NaN
2018-03-31     NaN
2018-04-30     NaN
2018-05-31     2.0
2018-06-30     3.0
2018-07-31     4.0
2018-08-31     5.0
2018-09-30     6.0
2018-10-31     7.0
2018-11-30     8.0
2018-12-31     9.0
2019-01-31    10.0
2019-02-28    11.0
2019-03-31    12.0
2019-04-30    13.0
2019-05-31    14.0
2019-06-30    15.0
2019-07-31    16.0
2019-08-31    17.0
Freq: M, dtype: float64

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.128-microsoft-standard
Version          : #1 SMP Tue Jun 23 12:58:10 UTC 2020
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0
numpy            : 1.19.5
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.0
Cython           : 0.29.17
pytest           : 6.0.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.6.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
782615608,39058,TYP: series apply method adds type hints,aniaan,closed,2021-01-09T14:13:02Z,2021-01-11T12:21:21Z,"
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
"
714080327,36833,REF: NDFrame describe,ivanovmg,closed,2020-10-03T12:31:44Z,2021-01-11T12:27:21Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Refactor ``NDFrame.describe`` method.

- Create module ``pandas/io/formats/describe.py``
- Delegate ``NDFrame.describe`` to function ``describe_ndframe`` in the new module
- Implement polymorphism for describing series and dataframe
- Implement strategy pattern for describing series of different kinds (categorical, numeric, timestamp)

Benefits:
- Reduce complexity in ``pandas/core/generic.py``
- Straightforward logic how to treat each datatype (reduced if/elif/else workflow)
- Enable potential for further extension"
783181277,39099,BUG: DataFrame.to_csv with compression method xz does not consider compresslevel,jor-,closed,2021-01-11T08:46:48Z,2021-01-11T13:03:02Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pdv
df = pd.DataFrame(np.random.randint(100,size=(10**5, 3)),columns=['A','B','C'])
for compresslevel in range(1, 10):
    df.to_csv(f'/tmp/test.csv.{compresslevel}.xz', compression={'method': 'xz', 'compresslevel': compresslevel})
```


#### Output of checking file size
```
jore@mac /tmp % ls -l /tmp/test.csv.*
-rw-r--r--  1 jore  wheel  419804 11 Jan 09:29 /tmp/test.csv.1.xz
-rw-r--r--  1 jore  wheel  419804 11 Jan 09:29 /tmp/test.csv.2.xz
-rw-r--r--  1 jore  wheel  419804 11 Jan 09:29 /tmp/test.csv.3.xz
-rw-r--r--  1 jore  wheel  419804 11 Jan 09:29 /tmp/test.csv.4.xz
-rw-r--r--  1 jore  wheel  419804 11 Jan 09:29 /tmp/test.csv.5.xz
-rw-r--r--  1 jore  wheel  419804 11 Jan 09:29 /tmp/test.csv.6.xz
-rw-r--r--  1 jore  wheel  419804 11 Jan 09:29 /tmp/test.csv.7.xz
-rw-r--r--  1 jore  wheel  419804 11 Jan 09:29 /tmp/test.csv.8.xz
-rw-r--r--  1 jore  wheel  419804 11 Jan 09:30 /tmp/test.csv.9.xz
```

#### Problem description

The `compresslevel` is not considered during the compression with `xz`. However for `gzip`and `bz2` the `compresslevel` is correctly considered during compression.

#### Expected Output

The file size should decrease when the compresslevel is increased.

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.9.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 20.2.0
Version          : Darwin Kernel Version 20.2.0: Wed Dec  2 20:39:59 PST 2020; root:xnu-7195.60.75~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : de_DE.UTF-8
LANG             : de_DE.UTF-8
LOCALE           : de_DE.UTF-8

pandas           : 1.2.0
numpy            : 1.19.5
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 49.6.0.post20201009
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : Nonev
```

</details>
"
782737321,39084,Backport PR #39071 on branch 1.2.x (Regression in loc.setitem raising ValueError with unordered MultiIndex columns and scalar indexer),meeseeksmachine,closed,2021-01-10T04:01:18Z,2021-01-11T13:18:58Z,Backport PR #39071: Regression in loc.setitem raising ValueError with unordered MultiIndex columns and scalar indexer
680178696,35768,ExtensionBlock.take_nd crashes in 1.1.0,vmarkovtsev,closed,2020-08-17T11:57:26Z,2021-01-11T13:24:25Z,"https://github.com/pandas-dev/pandas/blob/934e9f840ebd2e8b5a5181b19a23e033bd3985a5/pandas/core/internals/blocks.py#L1718

I've got `self.values` of type `pd.Series` with `pd.Timestamp`-s, and that does not have `fill_value` and `allow_fill`, so the kwargs check fails.

```
athenian/api/controllers/miners/github/branches.py:34: in extract_branches
    for repo, repo_branches in branches.groupby(Branch.repository_full_name.key, sort=False):
/usr/local/lib/python3.8/dist-packages/pandas/core/groupby/ops.py:133: in get_iterator
    for key, (i, group) in zip(keys, splitter):
/usr/local/lib/python3.8/dist-packages/pandas/core/groupby/ops.py:935: in __iter__
    sdata = self._get_sorted_data()
/usr/local/lib/python3.8/dist-packages/pandas/core/groupby/ops.py:948: in _get_sorted_data
    return self.data.take(self.sort_idx, axis=self.axis)
/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py:3341: in take
    new_data = self._mgr.take(
/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py:1414: in take
    return self.reindex_indexer(
/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py:1251: in reindex_indexer
    new_blocks = [
/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py:1252: in <listcomp>
    blk.take_nd(
/usr/local/lib/python3.8/dist-packages/pandas/core/internals/blocks.py:1720: in take_nd
    new_values = self.values.take(indexer, fill_value=fill_value, allow_fill=True)
/usr/local/lib/python3.8/dist-packages/pandas/core/series.py:829: in take
    nv.validate_take(tuple(), kwargs)
/usr/local/lib/python3.8/dist-packages/pandas/compat/numpy/function.py:68: in __call__
    validate_kwargs(fname, kwargs, self.defaults)
/usr/local/lib/python3.8/dist-packages/pandas/util/_validators.py:148: in validate_kwargs
    _check_for_invalid_keys(fname, kwargs, compat_args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fname = 'take', kwargs = {'allow_fill': True, 'fill_value': numpy.datetime64('NaT')}, compat_args = OrderedDict([('out', None), ('mode', 'raise')])

    def _check_for_invalid_keys(fname, kwargs, compat_args):
        """"""
        Checks whether 'kwargs' contains any keys that are not
        in 'compat_args' and raises a TypeError if there is one.
        """"""
        # set(dict) --> set of the dictionary's keys
        diff = set(kwargs) - set(compat_args)
    
        if diff:
            bad_arg = list(diff)[0]
>           raise TypeError(f""{fname}() got an unexpected keyword argument '{bad_arg}'"")
E           TypeError: take() got an unexpected keyword argument 'allow_fill'

/usr/local/lib/python3.8/dist-packages/pandas/util/_validators.py:122: TypeError
```

```
(Pdb++) self.values
0   2019-11-01 09:08:16+00:00
1   2017-01-30 18:04:00+00:00
2   2016-12-05 10:59:00+00:00
3   2019-05-16 11:16:00+00:00
Name: commit_date, dtype: datetime64[ns, UTC]
```

It appeared due to
```
-> fc.replace(0, pd.NaT, inplace=True)
[50]   /usr/local/lib/python3.8/dist-packages/pandas/core/series.py(4563)replace()
-> return super().replace(
[51]   /usr/local/lib/python3.8/dist-packages/pandas/core/generic.py(6583)replace()
-> return self._update_inplace(result)
[52]   /usr/local/lib/python3.8/dist-packages/pandas/core/generic.py(3955)_update_inplace()
-> self._maybe_update_cacher(verify_is_copy=verify_is_copy)
[53]   /usr/local/lib/python3.8/dist-packages/pandas/core/generic.py(3235)_maybe_update_cacher()
-> ref._maybe_cache_changed(cacher[0], self)
[54]   /usr/local/lib/python3.8/dist-packages/pandas/core/generic.py(3196)_maybe_cache_changed()
-> self._mgr.iset(loc, value)
[55]   /usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py(1066)iset()
-> blk.set(blk_locs, value_getitem(val_locs))
[56] > /usr/local/lib/python3.8/dist-packages/pandas/core/internals/blocks.py(1593)set()
-> self.values = values
```"
782680314,39069,"REGR: diff_2d raising for int8, int16",mzeitlin11,closed,2021-01-09T20:32:57Z,2021-01-11T13:44:50Z,"- [x] closes #39050
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
777794983,38936,CLN: Consolidate raise_on_missing and on_version into a single errors parameter in import_optional_dependency,lithomas1,closed,2021-01-04T03:50:52Z,2021-01-11T13:59:41Z,"xref [#38925 comment](https://github.com/pandas-dev/pandas/pull/38925#discussion_r551048418)
The new parameter should errors should have options of either ""raise"", ""warn"", or ""ignore"". 
"
771108473,38566,"BUG: Reindexing two tz-aware indices drops tz on the target index when tolerance and method is specified for only ""ffill"" and ""bfill""",ketozhang,closed,2020-12-18T19:07:55Z,2021-01-11T14:11:58Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample
```python
df = pd.DataFrame({'value': [0, 1, 2, 3]},
                  index=[pd.Timestamp('2020-01-01 05:00:00+0000', tz='UTC'),
                         pd.Timestamp('2020-01-01 06:00:00+0000', tz='UTC'),
                         pd.Timestamp('2020-01-01 07:00:00+0000', tz='UTC'),
                         pd.Timestamp('2020-01-01 08:00:00+0000', tz='UTC')
                         ]
                  )

new_index = pd.Series([pd.Timestamp('2020-01-01 5:30:00+0000', tz='UTC'),
                       pd.Timestamp('2020-01-01 6:30:00+0000', tz='UTC'),
                       pd.Timestamp('2020-01-01 7:30:00+0000', tz='UTC'),
                       pd.Timestamp('2020-01-01 8:30:00+0000', tz='UTC'),
                       pd.Timestamp('2020-01-01 9:30:00+0000', tz='UTC')]
                      )

new_df = df.reindex(new_index, method=""ffill"", tolerance=pd.Timedelta(""1 hour""))
```

#### Problem description
The following exception is raised when `method` is `""ffill""` and `""bfill""` but not `""nearest""` (see #32740) **AND** `tolerance` is specified
```
TypeError: DatetimeArray subtraction must have the same timezones or no timezones
```

I found the timezone was dropped when reaching this function on lines 3024 and 3036

https://github.com/pandas-dev/pandas/blob/b5958ee1999e9aead1938c0bba2b674378807b3d/pandas/core/indexes/base.py#L3024-L3036

where `target` is the target index that's tz-aware. However once converted to `target_values`, the tz info disappears from the numpy array.

I found a working solution but unsure if this behavior affects any other parts functionalities
```
- self._filter_indexer_tolerance(target_values, indexer, tolerance)
+ self._filter_indexer_tolerance(target, indexer, tolerance)
```


#### Expected Output

```
                           value
2020-01-01 05:30:00+00:00    0.0
2020-01-01 06:30:00+00:00    1.0
2020-01-01 07:30:00+00:00    2.0
2020-01-01 08:30:00+00:00    3.0
2020-01-01 09:30:00+00:00    NaN
```

#### Output of ``pd.show_versions()``

```
INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.0-11-cloud-amd64
Version          : #1 SMP Debian 4.19.146-1 (2020-09-17)
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.5
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : 6.1.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.4
fastparquet      : 0.4.1
gcsfs            : 0.7.1
matplotlib       : 3.3.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.3
sqlalchemy       : 1.3.20
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.51.2
```
</details>
"
783334334,39101,TST: add test for describe include/exclude,ivanovmg,closed,2021-01-11T12:33:50Z,2021-01-11T14:15:30Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

Added test for checking that if include is 'all' then setting exclude will raise ValueError.
This case was not covered."
782724490,39082,REF: Remove Apply.get_result,rhshadrach,closed,2021-01-10T02:05:25Z,2021-01-11T14:32:14Z,"- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

Slightly simpler and allows typing of the result that we get out of Apply. Having both apply and agg go through get_result would mean typing the result as `Union[FrameOrSeriesUnion, Tuple[bool, Optional[FrameOrSeriesUnion]]]`"
783382795,39106,TYP: fix mypy error in pandas/core/common.py,ivanovmg,closed,2021-01-11T13:44:27Z,2021-01-11T14:52:28Z,"- [ ] xref #37715
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

Handle ignored mypy error in ``pandas/core/common.py``."
783381295,39105,sync release notes on 1.2.x from master,simonjayhawkins,closed,2021-01-11T13:42:27Z,2021-01-11T14:56:25Z,"PR direct against 1.2.x

syncing release notes between 1.2.x and master"
782752976,39085,IntervalIndex/IntervalArray __repr__ remove redundant closed,jbrockmendel,closed,2021-01-10T06:16:13Z,2021-01-11T16:12:58Z,
782686356,39071,Regression in loc.setitem raising ValueError with unordered MultiIndex columns and scalar indexer,phofl,closed,2021-01-09T21:13:36Z,2021-01-11T17:06:34Z,"- [x] closes #38601
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

@jbrockmendel Would it be faster to use arange too in the line above of my fix instead of Index?"
722242366,37134,Remove the Label alias in pandas._typing.,simonjayhawkins,closed,2020-10-15T11:20:19Z,2021-01-11T17:07:38Z,"in mypy 0.790 bugfix (open PR to update #37108)

> Make None compatible with Hashable (PR 9371)

we have an alias in pandas._typing for Label to work around this bug.

I propose that we now remove the Label alias for two reasons.

1. It has been the source of some confusion, especially when using Optional[Label]
2. for the API, users would know what Hashable is without needing to look up the alias."
782760440,39086,BUG: setting td64 value into Series[numeric],jbrockmendel,closed,2021-01-10T07:19:15Z,2021-01-11T17:22:34Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
782731363,39083,CLN: Simplify optional dependency,lithomas1,closed,2021-01-10T03:06:44Z,2021-01-11T18:08:54Z,"- [x] closes #38936 
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
783404734,39107,TYP: replace Label alias from pandas._typing with Hashable,simonjayhawkins,closed,2021-01-11T14:13:46Z,2021-01-11T18:12:15Z,"- [ ] closes #37134
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
783604444,39112,DOC: minor cleanup of 1.2.1 release notes,simonjayhawkins,closed,2021-01-11T18:36:11Z,2021-01-11T20:03:50Z,
782627057,39060,BUG: in network decorator,simonjayhawkins,closed,2021-01-09T15:18:04Z,2021-01-11T20:45:34Z,
782900078,39094,REF: implement dtypes.cast.can_hold_element,jbrockmendel,closed,2021-01-10T20:24:47Z,2021-01-11T21:52:28Z,"Passes the tests added by #39086, gets us close to collapsing the NumericBlock subclasses and just using NumericBlock directly."
783665532,39114,Backport PR #39112 on branch 1.2.x (DOC: minor cleanup of 1.2.1 release notes),meeseeksmachine,closed,2021-01-11T20:02:12Z,2021-01-12T00:00:27Z,Backport PR #39112: DOC: minor cleanup of 1.2.1 release notes
783753148,39116,DataFrame.apply doesn't handle numpy ops or DataFrame properties,rhshadrach,closed,2021-01-11T22:30:14Z,2021-01-12T01:15:07Z,"```
s = pd.Series([1, 2])
df = pd.DataFrame([1, 2])

print(s.agg(""sqrt""))
print(df.agg(""sqrt""))
print(s.apply(""sqrt""))
print(df.apply(""sqrt""))

print(s.agg(""size""))
print(df.agg(""size""))
print(s.apply(""size""))
print(df.apply(""size""))
```

In each of the two blocks, first three lines all give the expected output and the last raises."
777492018,38904,TST: skips in extension.test_categorical,rhshadrach,closed,2021-01-02T17:03:19Z,2021-01-12T01:17:22Z,"ref: https://github.com/pandas-dev/pandas/pull/38901#discussion_r550899659

At least some of the tests that are skipped here should instead be xfailed."
783783393,39118,ENH: Allow numpy ops and DataFrame properties as str arguments to DataFrame.apply,rhshadrach,closed,2021-01-11T23:35:03Z,2021-01-12T01:27:48Z,"- [x] closes #39116
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

"
782639465,39062,TST: skips in extension.test_categorical,rhshadrach,closed,2021-01-09T16:25:36Z,2021-01-12T01:28:00Z,"- [x] closes #38904  
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

cc @simonjayhawkins 

For the searchsorted test, the categorical data starts off ordered as ""C"" < ""A"" < ""B"" but then gets reordered in the test by these lines:

> b, c, a = data_for_sorting
        arr = type(data_for_sorting)._from_sequence([a, b, c])

so the ordering becomes ""A"" < ""B"" < ""C"". This causes failure when ""C"" is said to be inserted at index 3 (new order) to preserve order rather than the expected 0 (original order).

"
783352257,39102,MOVE: describe to pandas/core/describe.py,ivanovmg,closed,2021-01-11T13:01:20Z,2021-01-12T06:26:12Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

Move ``describe`` functionality to a separate module and prepare for the further refactor/simplification (xref #36833).

 - Created new module ``pandas/core/describe.py``
 - Delegate the functionality of ``NDFrame.describe`` to function ``describe_ndframe`` in the new module"
780649767,39003,TST: add note about scope of base extension tests to all files,jorisvandenbossche,closed,2021-01-06T15:47:35Z,2021-01-12T08:05:04Z,"We already had this note in about half of the files in this directory, copied it to include in the other files as well."
781978512,39035,API: concatting DataFrames does not skip empty objects,jorisvandenbossche,closed,2021-01-08T09:28:29Z,2021-01-12T08:36:34Z,"See discussion in https://github.com/pandas-dev/pandas/pull/38843

To repeat the main argument: when not skipping empty objects, the resulting dtype of a concat-operation only depends on the input *dtypes*, and not on the exact content (the exact values, how many values (shape)). In general we want to get rid of value-dependent behaviour. In the past we discussed this in the context of the certain values (eg presence of NaNs or not), but I think also the shape should not matter (eg when slicing dataframes before contatting, you can get empties or not depending on *values*).

This starts with reverting the change for DataFrame. If we opt for this behaviour, in addition we should also discuss how to change the Series behaviour.

cc @jbrockmendel"
784097915,39124,DOC: fixed a minor mistake worksheet -> workbook,pilyavetsb,closed,2021-01-12T10:12:41Z,2021-01-12T13:52:02Z,"A small typo related to #38990
A worksheet can't have multiple worksheets, but a workbook can. Minor fix but may confuse the newcomers

- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
472963962,27589,Add NumPy Stubs,WillAyd,closed,2019-07-25T16:37:11Z,2021-01-12T17:54:36Z,"We use ndarray often in our annotations, but it always just resolves to `Any` since it doesn't have any annotations natively. It looks like numpy has a separate Numpy-stubs repo under development:

https://github.com/numpy/numpy-stubs

Might be worth considering that as a development dependency. Certainly would like to find issues with ndarray usage sooner than later

@shoyer "
508782326,29064,Remove numpy references from pandas._typing,WillAyd,closed,2019-10-18T00:08:57Z,2021-01-12T17:54:54Z,"Because we don't have type information from numpy AND because we specifically ignore_missing_imports for that in our setup, any numpy types currently resolve to `Any`. This may cause surprising behavior if not scrutinized...

To illustrate, we currently define `DType` in pandas._typing as `Dtype = Union[str, np.dtype, ""ExtensionDtype""]`. The numpy item here basically makes this a `Union[..., Any]` so it will almost never fail static analysis. If you remove the np reference therein you will get errors like the following:

```sh
pandas/tests/arrays/sparse/test_dtype.py:74: error: Argument 1 to ""SparseDtype"" has incompatible type ""Type[int]""; expected ""Union[str, ExtensionDtype]""
```

In this particular case we probably want to replace `np.dtype` with `Type[Union[float, bool, int, str, object]]` to account for legitimate `dtype=object` calls (@angelaambroz this was mentioned in #29046). 

The appearances in TypeVar items may not be as big of a deal, so open to thoughts on whomever wants to review this

@simonjayhawkins "
486449011,28195,TYPING: add skeleton stub for Timestamp,simonjayhawkins,closed,2019-08-28T15:19:45Z,2021-01-12T18:20:27Z,"xref #28133

This PR adds just the skeleton so that the attributes, methods and arguments available are known.

in this PR, the methods starting with an underscore have been omitted and the attributes typed with Any. This follows default stubgen output. https://mypy.readthedocs.io/en/latest/stubgen.html#automatic-stub-generation-stubgen

fix for `pandas\core\arrays\datetimes.py:275: error: Incompatible types in assignment (expression has type ""Type[Timestamp]"", base class ""AttributesMixin"" defined the type as ""Type[DatetimeLikeScalar]"")` arising from adding type information for Timestamp.

relevant diff for `mypy pandas --disallow-any-unimported`...

```
< pandas\_typing.py:23: error: Constraint 2 becomes ""Any"" due to an unfollowed import
7d5
< pandas\core\dtypes\dtypes.py:653: error: Type of variable becomes ""Type[Any]"" due to an unfollowed import
44,45d41
< pandas\core\arrays\_ranges.py:15: error: Argument 1 to ""generate_regular_range"" becomes ""Any"" due to an unfollowed import
< pandas\core\arrays\_ranges.py:15: error: Argument 2 to ""generate_regular_range"" becomes ""Any"" due to an unfollowed import
129c125
< pandas\io\formats\format.py:1550: error: Argument 1 to ""_format_datetime64"" becomes ""Union[Any, Any]"" due to an unfollowed import
---
> pandas\io\formats\format.py:1550: error: Argument 1 to ""_format_datetime64"" becomes ""Union[Any, Timestamp]"" due to an unfollowed import
131c127
< pandas\io\formats\format.py:1567: error: Argument 1 to ""_format_datetime64_dateonly"" becomes ""Union[Any, Any]"" due to an unfollowed import
---
> pandas\io\formats\format.py:1567: error: Argument 1 to ""_format_datetime64_dateonly"" becomes ""Union[Any, Timestamp]"" due to an unfollowed import
```"
784508583,39129,DOC: getting started under coming from sql,mattwelke,closed,2021-01-12T18:59:46Z,2021-01-12T19:08:25Z,"#### Location of the documentation

[this should provide the location of the documentation, e.g. ""pandas.read_csv"" or the URL of the documentation, e.g. ""https://dev.pandas.io/docs/reference/api/pandas.read_csv.html""]

**Note**: You can check the latest versions of the docs on `master` [here](https://pandas.pydata.org/docs/dev/).

#### Documentation problem

[this should provide a description of what documentation you believe needs to be fixed/improved]

#### Suggested fix for documentation

[this should explain the suggested fix and **why** it's better than the existing documentation]
"
782921632,39095,BUG: reindex raising TypeError with timezone aware index and tolerance for ffill and bfill,phofl,closed,2021-01-10T22:20:57Z,2021-01-12T21:54:49Z,"- [x] closes #38566
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

Is there a better place for these tests?

EDIT: Technically a regression, but since it persists since 1.0.0 is it worth backporting?"
773526474,38654,[BUG] Concat duplicates errors (or lack there of),ivirshup,closed,2020-12-23T06:38:13Z,2021-01-12T23:04:12Z,"- [ ] closes #6963
- [ ]  xref #36263, #36290
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

@jreback

So here is a basic proposal for making `concat` throw a more useful error when there are duplicate values on the index not being concatenated along. Below are some examples of how this error message is improved. I'm including a comparison to 1.2.0rc1, since it includes a pr which was supposed to change some of these errors to working cases, but those cases are extremely limited. That's why the first commit of this PR reverts that change.

I haven't gone too far with this since it a) reverts a merged PR and b) affects the release candidate, so should be conservative. I would like to get feedback on whether I should proceed with this approach.

My thinking right now is that a better error and revert should go into 1.2 – no behaviour change from 1.1, just better errors. A question of whether `pd.concat(..., join=""inner"")` means an intersection of indices (the current [and documented](https://pandas.pydata.org/docs/user_guide/merging.html#concatenating-objects) behaviour) or an actual inner join seems like something that could benefit from more discussion.

If they are set operations, it should probably error on duplicates. If they are relational algebra operations, they should work with duplicates. It would probably also be useful to then add more arguments to `concat`, like `merge`s `validate`.

# One dataframe has repeated column names (str)

```python
pd.concat([
    pd.DataFrame(np.ones((4, 4)), columns=list(""aabc"")),
    pd.DataFrame(np.ones((4, 3)), columns=list(""abc"")),
])
```

This PR

```pytb
InvalidIndexError: Reindexing only valid with uniquely valued Index objects
```

Pandas v1.1.5

```pytb
ValueError: Plan shapes are not aligned
```

pandas 1.2.0rc1

```pytb
ValueError: Plan shapes are not aligned
```

-------------------------

# One dataframe has repeated column names (int)

```python
pd.concat([  # One dataframe has repeated column names
    pd.DataFrame(np.ones((4, 4)), columns=[1, 1, 2, 3]),
    pd.DataFrame(np.ones((4, 3)), columns=[1, 2, 3]),
)
```

This PR

```pytb
InvalidIndexError: Reindexing only valid with uniquely valued Index objects
```

Pandas v1.1.5

```pytb
ValueError: Plan shapes are not aligned
```

pandas 1.2.0rc1

```pytb
ValueError: Plan shapes are not aligned
```


--------------------------

# Repeated columns (same amount) different column ordering

```python
pd.concat([
    pd.DataFrame(np.ones((2, 4)), columns=list(""aabc"")),
    pd.DataFrame(np.ones((2, 4)), columns=list(""abca"")),
])
``` 

This PR:

```pytb
InvalidIndexError: Reindexing only valid with uniquely valued Index objects
```

In pandas v1.15

```pytb
AssertionError: Number of manager items must equal union of block items
# manager items: 3, # tot_items: 4
```

In pandas v1.2.0rc1

```pytb
AssertionError: Number of manager items must equal union of block items
# manager items: 3, # tot_items: 4
```

----------------------------

# Repeated columns of different values in each

This point is what the reverted PR was trying to fix. This demonstrates that it didn't seem to fix what it set out to, and that the intended fix introduced strange behaviour anyways. To do this, I'll use the reverted test case.

```python
# Test case from previous PR
a_int = pd.DataFrame([1, 2, 3, 4], index=[0, 1, 1, 4], columns=[""a""])
b_int = pd.DataFrame([6, 7, 8, 9], index=[0, 0, 1, 3], columns=[""b""])

# Same, but with string indexes
a_str = a_int.set_index(letters[a_int.index])
b_str = b_int.set_index(letters[b_int.index])
```

## Int index

```python
pd.concat([a_int, b_int], axis=1)
```

This PR

```pytb
InvalidIndexError: Reindexing only valid with uniquely valued Index objects
```

Pandas v1.1.5

```pytb
ValueError: Shape of passed values is (8, 2), indices imply (6, 2)
```

pandas 1.2.0rc1

```pytb
     a    b
0  1.0  6.0
0  1.0  7.0
1  2.0  8.0
1  3.0  8.0
3  NaN  9.0
4  4.0  NaN
```

<details>
<summary> This behaviour is a bit strange </summary>

First, the `concat` documentation [defines the options here as set operations, like intersect and union](https://pandas.pydata.org/docs/user_guide/merging.html#concatenating-objects). Is this a union, or is this an outer join?

If these are joins, then inner joins only seem to work when they are equivalent to intersections:

```python
>>> pd.concat([a_int, b_int], axis=1, join=""inner"")
...
ValueError: Shape of passed values is (3, 2), indices imply (2, 2)

>>> pd.merge(a_int, b_int, how=""inner"", left_index=True, right_index=True)
   a  b
0  1  6
0  1  7
1  2  8
1  3  8
```

</details>


## str index

This is mostly just to show the behaviour in 1.2.0rc1 is different for string and int indices

```python
pd.concat([a_str, b_str], axis=1)
```

This PR

```pytb
InvalidIndexError: Reindexing only valid with uniquely valued Index objects
```

Pandas v1.1.5

```pytb
ValueError: Shape of passed values is (5, 2), indices imply (4, 2)
```

pandas 1.2.0rc1

```pytb
ValueError: Shape of passed values is (5, 2), indices imply (4, 2)
```
"
779687296,38980,REGR: ExtensionArray aggregation on non-numeric types fails,BryanCutler,closed,2021-01-05T22:51:01Z,2021-01-13T13:18:40Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame({'key': ['a', 'a', 'b', 'b'], 'val': ['a', 'b', 'c', 'd']}, dtype=""string"")
df.groupby(""key"").agg({'val': 'first'})
```

Expected:
```
    val
key    
a     a
b     c
```

Actual fails with error:
```
lib/python3.8/site-packages/pandas/core/groupby/ops.py in _ea_wrap_cython_operation(self, kind, values, how, axis, min_count, **kwargs)
    538             return result
    539 
--> 540         raise NotImplementedError(values.dtype)
    541

NotImplementedError: string
```

#### Problem description

Calling groupby on an ExtensionArray and aggregation on a string column fails with the error: `NotImplementedError: string`. This is because an optimized cython aggregation is attempted and when fails, the error string is improperly checked and does not fallback to a non-cython aggregation.

This was found with the extension arrays from https://github.com/CODAIT/text-extensions-for-pandas/blob/master/text_extensions_for_pandas/io/bert.py#L217  and full error log can be seen here https://github.com/CODAIT/text-extensions-for-pandas/pull/157/checks?check_run_id=1646927381#step:5:1438

#### Expected Output

Pandas should handle the NotImplementedError and fallback to a non-cython aggregation implementation.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-58-generic
Version          : #64-Ubuntu SMP Wed Dec 9 08:16:25 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 49.6.0.post20201009
Cython           : None
pytest           : 6.1.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 2.0.0
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
762441346,38412,REF: move __array_ufunc__ to base NumericArray,jorisvandenbossche,closed,2020-12-11T15:02:30Z,2021-01-13T13:59:37Z,"xref https://github.com/pandas-dev/pandas/issues/38110

The implementations for integer and float array is almost identical, so move to base class. In addition, this also enables that ufuncs on IntegerArray can return FloatingArray (instead of float numpy array)

I should probably consolidate the tests as well, and move those to `tests/arrays/masked/test_ufunc.py` or so.

In addition, `BooleanArray.__array_ufunc__` is actually also very similar, but didn't include that here, yet. I should probably do that, but wondering if I should move the common `__array_ufunc__` implementation to BaseMaskedArray, or rather make BooleanArray subclass NumericArray."
785098602,39145,Backport PR #38982 on branch 1.2.x (REGR: Bug fix for ExtensionArray groupby aggregation on non-numeric types),meeseeksmachine,closed,2021-01-13T13:20:16Z,2021-01-13T14:00:07Z,Backport PR #38982: REGR: Bug fix for ExtensionArray groupby aggregation on non-numeric types
689244209,36010,POC: ArrayManager -- array-based data manager for columnar store ,jorisvandenbossche,closed,2020-08-31T14:17:54Z,2021-01-13T14:10:37Z,"Related to the discussion in https://github.com/pandas-dev/pandas/issues/10556, and following up on the mailing list discussion *""A case for a simplified (non-consolidating) BlockManager with 1D blocks""* ([archive](https://mail.python.org/pipermail/pandas-dev/2020-May/001219.html)).

This branch experiments with an *""array manager""*, storing a list of 1D arrays instead of blocks. 
The idea is that this `ArrayManager` could optionally be used instead of `BlockManager`. If we ensure the ""DataManager"" has a clear interface for the rest of pandas (and thus parts outside of the internals don't rely on details like block layout, xref https://github.com/pandas-dev/pandas/issues/34669), this should be possible without much changes outside of /core/internals.

Some notes on this experiment:

- This is not a complete POC, not every aspect and behaviour of the BlockManager has already been replicated, and there are still places in pandas that rely on the blocks being present, so lots of tests are still failing (although changes in behaviour are also desired). That said, a *lot* of the basic operations do work. Two illustrations of this:
  - An updated version of the notebook I showed in the mailing list discussion as well: with a certain setup, comparing a set of operations between block vs array manager: https://nbviewer.jupyter.org/gist/jorisvandenbossche/f917d4301d21069e2be2e3b7c7aa4d07
  - I ran the arithmetic.py benchmark file, comparing against master, see below for the results.

- For now, I focused on an ArrayManager storing a list of *numpy arrays*. Of course we need to expand that to support ExtensionArrays as well (or ExtensionArrays only?), but the reason I limited to numpy arrays for now: besides making it a bit simpler to experiment with, this also gives a fairer comparison with the consolidated BlockManager (because it focuses on the numpy array being 1D vs 2D, and doesn't mix in performance/implementation differences of numpy array vs ExtensionArray).

- Personally, I think this looks promising. Many of the methods are a *lot* simpler than the BlockManager equivalent (although not every aspect is implemented yet, that's correct). And for the case I showed in the notebook, performance looks also good. For the benchmark suite I ran, there are obviously slowdowns for the ""wide dataframe"" benchmarks.
  There is still a lot of work needed to make this fully working with the rest of pandas, though ;)

- Given the early proof of concept stage, detailed code feedback is not yet needed, but I would find it very useful to discuss the following aspects:

  - High-level feedback on the approach: does the approach of the two subclasses look interesting? The approach of the ArrayManager itself storing a list of arrays? ...

  - What to do with Series, which now is a SingleBlockManager inheriting from BlockManager (should we also have a ""SingleArrayManager""?)

  - *If* we find this interesting, how can we go from here? How do we decide on this? (what aspects already need to work, how fast does it need to be?) I don't think getting a fully complete implementation passing all tests is is possible in a single PR. Are we fine with merging something partial in master and continue from there? Or a shared feature branch in upstream? ...


<details>
<summary>Benchmark results for asv_bench/arithmetic.py</summary>

As an example, I ran `asv continuous -f 1.1 upstream/master HEAD -b arithmetic`. 

The benchmarks with a slowdown bigger than a factor 2 can basically be brought back to two cases:

- Benchmarks for ""wide"" dataframes (eg `FrameWithFrameWide` using a case with n_cols > n_rows)
- Benchmarks from the `IntFrameWithScalar` class: from a quick profile, it seems that the usage of numexpr is the cause, and disabling this seems to reduce the slowdown to a factor 2. The numexpr code (and checking if it should be used etc) apparently has a high overhead per call, which I assume is something that can be solved (moving those checks a level higher up, so we don't need to repeat it for each column)

```
       before           after         ratio
     [b45327f5]       [047f9091]
     <master>                   
!        40.6±6ms           failed      n/a  arithmetic.Ops.time_frame_multi_and(False, 'default')
!        32.7±2ms           failed      n/a  arithmetic.Ops.time_frame_multi_and(False, 1)
!        26.5±1ms           failed      n/a  arithmetic.Ops.time_frame_multi_and(True, 'default')
!        37.7±2ms           failed      n/a  arithmetic.Ops.time_frame_multi_and(True, 1)
+      1.06±0.3ms         93.5±7ms    88.57  arithmetic.FrameWithFrameWide.time_op_same_blocks(<built-in function gt>)
+      1.51±0.2ms         80.6±3ms    53.34  arithmetic.FrameWithFrameWide.time_op_same_blocks(<built-in function add>)
+     1.22±0.08ms         55.1±5ms    45.19  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0('le')
+     1.30±0.07ms        55.6±20ms    42.83  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0('ne')
+      2.12±0.4ms         90.1±4ms    42.47  arithmetic.FrameWithFrameWide.time_op_different_blocks(<built-in function gt>)
+     1.17±0.04ms         49.4±4ms    42.38  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0('gt')
+     1.28±0.07ms         52.9±3ms    41.28  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0('lt')
+      1.29±0.2ms       52.5±0.6ms    40.63  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0('ge')
+     1.44±0.02ms         56.8±7ms    39.56  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0('eq')
+      2.08±0.3ms        78.9±10ms    37.90  arithmetic.Ops2.time_frame_float_mod
+      2.34±0.1ms         78.3±4ms    33.51  arithmetic.FrameWithFrameWide.time_op_different_blocks(<built-in function add>)
+      1.66±0.2ms         46.6±1ms    28.00  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0('mul')
+      1.78±0.2ms         48.2±5ms    27.02  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0('truediv')
+     1.14±0.04ms         26.8±4ms    23.49  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function le>)
+      1.83±0.2ms         42.9±1ms    23.39  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0('add')
+      1.94±0.3ms         45.1±4ms    23.29  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0('sub')
+     1.23±0.07ms         23.0±3ms    18.65  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function ge>)
+     1.33±0.08ms         22.8±1ms    17.14  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function eq>)
+     1.03±0.05ms         17.6±2ms    17.13  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function ge>)
+      1.65±0.5ms         28.1±7ms    17.00  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function eq>)
+     1.21±0.05ms         20.1±3ms    16.67  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function gt>)
+     1.18±0.03ms       19.4±0.9ms    16.54  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function eq>)
+     1.08±0.07ms         17.8±1ms    16.53  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function lt>)
+     1.22±0.05ms         20.0±2ms    16.41  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function gt>)
+     1.30±0.06ms         21.2±3ms    16.28  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function ne>)
+     1.15±0.06ms         18.6±3ms    16.18  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function lt>)
+      1.42±0.1ms         22.6±1ms    15.96  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function lt>)
+     1.11±0.01ms       17.6±0.4ms    15.85  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function ne>)
+      5.30±0.8ms        81.7±20ms    15.40  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis1('lt')
+      1.37±0.2ms         20.7±3ms    15.09  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function gt>)
+     1.22±0.05ms         18.0±6ms    14.72  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function ge>)
+      1.28±0.1ms         18.6±3ms    14.55  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function gt>)
+     1.17±0.08ms         17.0±3ms    14.54  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function gt>)
+      1.22±0.1ms       17.6±0.8ms    14.44  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function eq>)
+      1.35±0.1ms         19.4±2ms    14.35  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function le>)
+      1.35±0.1ms         19.2±4ms    14.21  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function ge>)
+      4.36±0.3ms         61.8±8ms    14.17  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis1('le')
+      1.31±0.1ms         18.5±2ms    14.09  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function lt>)
+      4.48±0.5ms         62.9±5ms    14.06  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis1('ge')
+      1.15±0.1ms         16.1±1ms    14.01  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function ne>)
+      1.33±0.1ms         18.6±2ms    14.00  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function ge>)
+      4.37±0.4ms         58.9±2ms    13.48  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis1('ne')
+      1.22±0.2ms         16.2±3ms    13.25  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function le>)
+      1.25±0.1ms         16.5±1ms    13.13  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function le>)
+      1.44±0.2ms         18.6±4ms    12.90  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function ge>)
+      1.75±0.3ms         22.3±2ms    12.74  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function eq>)
+      1.42±0.3ms         18.0±7ms    12.68  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function gt>)
+      1.36±0.1ms         17.2±1ms    12.67  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function ne>)
+        440±30μs       5.57±0.1ms    12.65  arithmetic.Ops2.time_frame_series_dot
+      1.63±0.2ms         20.6±2ms    12.65  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function lt>)
+     1.35±0.07ms         17.0±3ms    12.58  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function le>)
+      1.34±0.2ms         16.7±1ms    12.46  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function eq>)
+      1.50±0.1ms         18.6±5ms    12.43  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function ge>)
+     1.35±0.07ms         16.8±1ms    12.42  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function ge>)
+      1.35±0.1ms         16.7±2ms    12.37  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function le>)
+      1.55±0.3ms         18.9±2ms    12.20  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function le>)
+      1.67±0.3ms         20.3±5ms    12.17  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function ne>)
+      1.55±0.2ms       18.5±0.7ms    11.94  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function le>)
+      5.05±0.5ms         59.1±3ms    11.70  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis1('gt')
+      1.51±0.2ms         17.6±2ms    11.66  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function lt>)
+     1.33±0.08ms         15.3±1ms    11.50  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function ne>)
+      4.47±0.1ms         51.2±1ms    11.45  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis1('eq')
+      1.35±0.1ms         15.4±2ms    11.45  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function lt>)
+      1.76±0.5ms         19.8±2ms    11.28  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function lt>)
+     1.55±0.09ms       16.8±0.3ms    10.86  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function ne>)
+      1.71±0.1ms         18.2±2ms    10.58  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function eq>)
+      1.51±0.2ms         15.9±3ms    10.54  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function eq>)
+      1.53±0.2ms       15.6±0.3ms    10.19  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function ne>)
+      1.95±0.2ms         19.7±5ms    10.08  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function gt>)
+     2.22±0.08ms         21.6±4ms     9.73  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function add>)
+     1.77±0.08ms         16.7±1ms     9.48  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function gt>)
+      2.19±0.1ms         19.9±2ms     9.08  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function mul>)
+     1.91±0.04ms         17.0±2ms     8.88  arithmetic.Ops.time_frame_comparison(True, 'default')
+      2.18±0.1ms         19.0±1ms     8.73  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function add>)
+     2.23±0.08ms         19.1±1ms     8.59  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function sub>)
+     2.24±0.07ms         19.0±3ms     8.47  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function mul>)
+     2.34±0.06ms         19.5±2ms     8.31  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function truediv>)
+      2.52±0.2ms         20.3±6ms     8.06  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function truediv>)
+      2.39±0.2ms         19.2±2ms     8.05  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function truediv>)
+      3.07±0.4ms         24.4±5ms     7.94  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function mod>)
+      2.24±0.1ms         17.5±2ms     7.85  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function add>)
+      2.24±0.2ms       17.4±0.7ms     7.79  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function sub>)
+      2.33±0.1ms         18.0±2ms     7.73  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function mul>)
+      2.15±0.1ms         16.4±4ms     7.60  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function sub>)
+     2.10±0.05ms         15.9±2ms     7.57  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function add>)
+      2.27±0.1ms         16.8±1ms     7.39  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function add>)
+      3.59±0.1ms         26.1±5ms     7.27  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function mod>)
+      2.32±0.1ms         16.8±3ms     7.25  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function sub>)
+     2.36±0.08ms       17.1±0.7ms     7.23  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function truediv>)
+      2.42±0.2ms         17.4±2ms     7.17  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function sub>)
+     2.31±0.09ms       16.4±0.9ms     7.11  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function add>)
+      7.34±0.9ms         52.2±2ms     7.10  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis1('add')
+      2.32±0.1ms       16.4±0.9ms     7.07  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function add>)
+      2.25±0.2ms         15.8±2ms     7.03  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function sub>)
+      2.51±0.5ms         17.3±2ms     6.91  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function add>)
+      2.43±0.1ms       16.7±0.8ms     6.84  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function mul>)
+      2.24±0.1ms         15.2±2ms     6.81  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function mul>)
+        7.81±1ms         52.9±4ms     6.78  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis1('sub')
+      2.48±0.2ms         16.4±2ms     6.62  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function mul>)
+        6.82±1ms       44.4±0.7ms     6.51  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis1('mul')
+     2.25±0.05ms       14.6±0.8ms     6.48  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function sub>)
+      3.14±0.7ms         19.8±2ms     6.30  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function mod>)
+      2.57±0.2ms         15.9±2ms     6.19  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function sub>)
+      2.57±0.1ms         15.8±2ms     6.16  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function truediv>)
+        7.70±1ms         47.2±3ms     6.13  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis1('truediv')
+      3.02±0.1ms         18.4±3ms     6.08  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function mod>)
+      2.79±0.2ms       16.8±0.8ms     6.04  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function truediv>)
+      3.16±0.3ms       19.1±0.7ms     6.04  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function mod>)
+      2.51±0.2ms       14.9±0.5ms     5.92  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function mul>)
+      2.71±0.1ms       15.9±0.8ms     5.86  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function mul>)
+      2.72±0.3ms         15.9±1ms     5.83  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function truediv>)
+        11.9±1ms         64.0±5ms     5.39  arithmetic.Ops2.time_frame_int_mod
+      3.59±0.4ms         19.1±5ms     5.33  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function mod>)
+      6.23±0.4ms         32.7±6ms     5.25  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function mod>)
+      3.28±0.2ms         17.2±2ms     5.23  arithmetic.Ops.time_frame_add(True, 'default')
+        23.7±6ms          112±7ms     4.70  arithmetic.FrameWithFrameWide.time_op_same_blocks(<built-in function floordiv>)
+      3.51±0.4ms       16.5±0.6ms     4.70  arithmetic.Ops.time_frame_mult(True, 'default')
+        3.61±2ms         16.3±1ms     4.52  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function truediv>)
+        45.8±4ms         194±20ms     4.25  arithmetic.FrameWithFrameWide.time_op_different_blocks(<built-in function floordiv>)
+      5.64±0.6ms         21.9±1ms     3.89  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function mod>)
+      3.13±0.1ms       11.4±0.5ms     3.63  arithmetic.Ops.time_frame_comparison(True, 1)
+      12.2±0.8ms         42.5±4ms     3.47  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function pow>)
+      4.03±0.7ms       11.2±0.3ms     2.79  arithmetic.Ops.time_frame_add(True, 1)
+        53.0±6ms         143±10ms     2.69  arithmetic.Ops2.time_frame_float_floor_by_zero
+      4.11±0.2ms         11.1±1ms     2.69  arithmetic.Ops.time_frame_mult(True, 1)
+        54.9±4ms          125±9ms     2.28  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0('floordiv')
+      25.0±0.6ms         55.9±5ms     2.24  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function pow>)
+      2.42±0.2ms       5.21±0.6ms     2.16  arithmetic.Ops.time_frame_comparison(False, 'default')
+        16.2±1ms         31.9±3ms     1.97  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function pow>)
+        30.9±3ms        58.1±10ms     1.88  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function pow>)
+      3.36±0.3ms       5.76±0.4ms     1.71  arithmetic.Ops.time_frame_add(False, 'default')
+      3.10±0.3ms       5.03±0.3ms     1.62  arithmetic.Ops.time_frame_comparison(False, 1)
+        30.5±3ms         49.2±9ms     1.61  arithmetic.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function pow>)
+      3.42±0.3ms       5.51±0.4ms     1.61  arithmetic.Ops.time_frame_mult(False, 1)
+      3.52±0.2ms       5.63±0.1ms     1.60  arithmetic.Ops.time_frame_add(False, 1)
+      3.60±0.2ms       5.74±0.5ms     1.59  arithmetic.Ops.time_frame_mult(False, 'default')
+        57.9±1ms         89.7±6ms     1.55  arithmetic.Ops2.time_frame_float_div
+      32.1±0.5ms         48.7±2ms     1.52  arithmetic.Ops2.time_frame_dot
+     2.96±0.06ms       4.32±0.4ms     1.46  arithmetic.DateInferOps.time_add_timedeltas
+        65.9±2ms         93.8±1ms     1.42  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis1('pow')
+         106±2ms          132±3ms     1.25  arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0('pow')
+     1.33±0.01ms       1.64±0.2ms     1.24  arithmetic.OffsetArrayArithmetic.time_add_series_offset(<YearEnd: month=12>)
+      7.09±0.2ms       8.49±0.5ms     1.20  arithmetic.DateInferOps.time_subtract_datetimes
+        1.13±0ms      1.33±0.09ms     1.18  arithmetic.OffsetArrayArithmetic.time_add_series_offset(<YearBegin: month=1>)
+     1.25±0.02ms       1.47±0.1ms     1.18  arithmetic.OffsetArrayArithmetic.time_add_series_offset(<SemiMonthEnd: day_of_month=15>)
+     2.52±0.04ms       2.97±0.2ms     1.18  arithmetic.OffsetArrayArithmetic.time_add_series_offset(<BusinessDay>)
+     1.16±0.01ms      1.32±0.06ms     1.13  arithmetic.OffsetArrayArithmetic.time_add_series_offset(<QuarterBegin: startingMonth=3>)
-      1.67±0.2ms      1.42±0.02ms     0.85  arithmetic.OffsetArrayArithmetic.time_add_dti_offset(<MonthEnd>)
-        282±20μs          230±5μs     0.81  arithmetic.NumericInferOps.time_subtract(<class 'numpy.int8'>)
-      4.36±0.2ms       3.54±0.3ms     0.81  arithmetic.NumericInferOps.time_modulo(<class 'numpy.uint16'>)
-      1.29±0.1ms      1.03±0.06ms     0.80  arithmetic.NumericInferOps.time_multiply(<class 'numpy.int64'>)
-     1.77±0.09ms      1.39±0.03ms     0.79  arithmetic.OffsetArrayArithmetic.time_add_dti_offset(<SemiMonthBegin: day_of_month=15>)
-      1.54±0.2ms      1.13±0.02ms     0.74  arithmetic.NumericInferOps.time_divide(<class 'numpy.int8'>)
-        301±40μs          221±4μs     0.73  arithmetic.OffsetArrayArithmetic.time_add_series_offset(<Day>)
-      3.85±0.5ms       2.58±0.2ms     0.67  arithmetic.OffsetArrayArithmetic.time_add_dti_offset(<BusinessDay>)

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE DECREASED.
```

</details>"
777092947,38858,DOC/CI: Fix building docs with --no-api,rhshadrach,closed,2020-12-31T17:28:16Z,2021-01-13T15:03:03Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Fixes running docs via

     python make.py --no-api

A few notes:

 - The removed check `if os.path.exists(path):`; will incorrectly raise if docs are being rebuilt.
 - The value `dirname` returned by `os.walk` is an absolute path, but the check was treating it as relative.
 - If docs are built with api and then rebuilt with --no-api, we want to exclude everything in the `reference` directory; in particular `reference/api`."
784625056,39134,BUG: Close resources opened by pyxlsb,twoertwein,closed,2021-01-12T22:01:14Z,2021-01-13T15:20:28Z,"Spin-off from #39047:
- Close resources opened by pyxlsb
- Use context manager when opening non-fsspec URLs (I'm not sure whether that reduced any `ResourceWarning`s but it can't hurt)
"
784366239,39127,CLN: remove redundant consolidation,jbrockmendel,closed,2021-01-12T15:58:47Z,2021-01-13T15:35:15Z,We only get here with nblocks == 1
784648893,39137,ASV: Add xlrd engine for xls file,phofl,closed,2021-01-12T22:44:37Z,2021-01-13T16:55:12Z,"- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

Is one parametrized function more efficient than multiple functions during setup and teardown? If not I would make a separate function for the xls benchmark

cc @jorisvandenbossche 
"
772410761,38621,BUG: join not working correctly with MultiIndex and one dimension categorical,phofl,closed,2020-12-21T19:31:30Z,2021-01-13T16:55:33Z,"- [x] closes #38502
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
771759560,38599,BUG:Issue Concerning Ambiguous Numpy Array Shape in Column,tlplayer,closed,2020-12-21T00:47:25Z,2021-01-13T17:26:54Z,"I spent multiple hours wondering why my dataframe was not fitting well into my tf model but it was the fact that the shape of the numpy array was not inferred and thus left ambiguous. I suggest, having a warning or a strict option for what shape is allowed within the column.

https://stackoverflow.com/questions/65383794/tensorflow-fails-to-interpret-what-is-stored-in-numpy-ndarray/65384804?noredirect=1#comment115597094_65384804"
781560705,39024,BUG: read_csv - file left open after UnicodeDecodeError when sep=None,davemfish,closed,2021-01-07T19:40:37Z,2021-01-13T18:03:45Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import tempfile
import textwrap
import pandas
import os

workspace_dir = tempfile.mkdtemp()
csv_file = os.path.join(workspace_dir, 'non-utf8.csv')
# encode with ISO Cyrillic, include a non-ASCII character to achieve UnicodeDecodeError
with open(csv_file, 'w', encoding='iso8859_5') as file_obj:
    file_obj.write(textwrap.dedent(
        """"""
        header,
        fЮЮ,
        bar
        """"""
    ).strip())

try:
    dataframe = pandas.read_csv(csv_file, sep=None)
except UnicodeDecodeError as error:
    os.remove(csv_file)
    raise

```

#### Problem description
`os.remove` raises a PermissionError on Windows because apparently the file handle is still open. This only happens when the `sep=None` kwarg is used. Leaving out that kwarg gets the expected output.

#### Expected Output
```python
Traceback (most recent call last):
  File ""..\scratch\pandas_file_handle.py"", line 19, in <module>
    dataframe = pandas.read_csv(csv_file, sep=None)
  File ""C:\Users\dmf\projects\invest\env\lib\site-packages\pandas\io\parsers.py"", line 605, in read_csv
    return _read(filepath_or_buffer, kwds)
  File ""C:\Users\dmf\projects\invest\env\lib\site-packages\pandas\io\parsers.py"", line 457, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File ""C:\Users\dmf\projects\invest\env\lib\site-packages\pandas\io\parsers.py"", line 814, in __init__
    self._engine = self._make_engine(self.engine)
  File ""C:\Users\dmf\projects\invest\env\lib\site-packages\pandas\io\parsers.py"", line 1045, in _make_engine
    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]
  File ""C:\Users\dmf\projects\invest\env\lib\site-packages\pandas\io\parsers.py"", line 2291, in __init__
    self._make_reader(self.handles.handle)
  File ""C:\Users\dmf\projects\invest\env\lib\site-packages\pandas\io\parsers.py"", line 2412, in _make_reader
    line = f.readline()
  File ""C:\Users\dmf\projects\invest\env\lib\codecs.py"", line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xce in position 10: invalid continuation byte
```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.7.9.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : AMD64 Family 23 Model 1 Stepping 1, AuthenticAMD
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : None.None

pandas           : 1.2.0
numpy            : 1.19.2
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 49.6.0.post20201009
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : None

</details>
"
782686910,39072,TST GH26807 Break up pandas/tests/io/pytables/test_store.py,moink,closed,2021-01-09T21:17:33Z,2021-01-13T18:04:39Z,"This PR addresses xref #26807 for pandas/tests/io/pytables/test_store.py, which was more than 5000 lines before this attempt. I made a new directory pandas/tests/io/pytables/store/ to put all the new modules in. I tried to 1) get all the new modules below 1000 lines and 2) break it up in a somewhat logical way, but let's see if I succeeded in the opinion of the reviewer.

There were 194 tests in pandas/tests/io/pytables/test_store.py, and now there are 194 tests in pandas/tests/io/pytables/store so I haven't gained or lost any. One is skipped on my dev machine.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
784787244,39142,BUG: .dt.isocalendar().week results in the last day of the year as week 1,trenton3983,closed,2021-01-13T04:55:33Z,2021-01-13T18:23:56Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

## Issue 1
```python
import pandas as pd

# sample dataframe
d = pd.DataFrame({'v': [pd.Timestamp('2018-12-03 00:00:00'), pd.Timestamp('2018-12-10 00:00:00'), pd.Timestamp('2018-12-17 00:00:00'), pd.Timestamp('2018-12-24 00:00:00'), pd.Timestamp('2018-12-31 00:00:00')]})

# display(d)
           v
0 2018-12-03
1 2018-12-10
2 2018-12-17
3 2018-12-24
4 2018-12-31

# get the week number
wn = d.v.dt.isocalendar().week

# display(wn)
0    49
1    50
2    51
3    52
4     1
Name: week, dtype: UInt32
```

#### Problem description

- The last day of the week should not be 1, for a given year.
- `d.v.dt.week` also generates the incorrect output, with a 1 
- This is an issue for a number of reasons, but in terms of plotting with week number on the x-axis, the plot line goes from 52, back to 1.

#### Expected Output

- `d.v.dt.strftime('%W').astype(int)` will generate the expected output

```python
0    49
1    50
2    51
3    52
4    53
Name: week, dtype: UInt32
```

## Issue 2:

```python
import pandas as pd

# sample data
data = {'date': pd.bdate_range('2020-11-01', freq='D', periods=93)}
df = pd.DataFrame(data)

# get week number
df['week_strf'] = df.date.dt.strftime('%W').astype('int')
df['week_isocal'] = df.date.dt.isocalendar().week

# display(df.head(10))
        date  week_strf  week_isocal
0 2020-11-01         43           44
1 2020-11-02         44           45
2 2020-11-03         44           45
3 2020-11-04         44           45
4 2020-11-05         44           45
5 2020-11-06         44           45
6 2020-11-07         44           45
7 2020-11-08         44           45
8 2020-11-09         45           46
9 2020-11-10         45           46
```

#### Problem description

- The two methods for getting the week number do not produce the same result

#### Expected Output

- `week_strf` and `week_isocal` should have the same week number

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.2.0
numpy            : 1.19.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.1.2.post20210110
Cython           : 0.29.21
pytest           : 6.2.1
hypothesis       : None
sphinx           : 3.4.3
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: 0.9.0
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.3
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.2
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.21
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : None
xlrd             : 2.0.1
xlwt             : 1.3.0
numba            : 0.51.2

</details>
"
783800821,39119,Pandas strftime function prints unix epoch time in local TZ instead of UTC,blwells,closed,2021-01-12T00:16:50Z,2021-01-13T19:05:09Z,"I'm trying to convert a given date and time to unix epoch time using Pandas. This is normally a very straightforward thing to do, merely converting the string of date and time to a datetime object, and then printing the object as a unix epoch timestamp using strftime, for example:

`pd.to_datetime('2021-01-01 00:00:00').strftime('%s')`

Running the above example on a remote linux machine I have set up, whose system date is recorded in UTC, gets me the expected output of ""1609459200"" which is 2020-01-01 00:00:00 in unix epoch time. However, when pulling my code onto my Mac laptop and running locally has resulted in what was unexpected to me at first: that the above code prints off a unix epoch time 7 hours in advance, or a string reading ""1609484400"". 

Running the date command on my Mac OS terminal gives me the time in MST, which is where I live, instead of UTC like the remote linux machine I pulled my code from. MST is 7 hours behind UTC so I assume that Pandas is trying to account for the MST time recorded on my laptop and doing some kind of internal conversion. I figure that I need to specify that the time I'm inputing is UTC, so I tried that both with ""utc=True"" for `pd.to_datetime()` as well as ""tz='UTC'"" for `pd.Timestamp()`. No dice though, as I still get a return value of ""1609484400"". 

Output when running with pd.to_datetime():
> dt = pd.to_datetime('2021-01-01 00:00:00', utc=True)
> dt
> ->Timestamp('2021-01-01 00:00:00+0000', tz='UTC')
> 
> dt.strftime('%Y-%m-%d %H:%M:%S')
> -> '2021-01-01 00:00:00'  # Converting back to the original format gives me the original string
> 
> dt.strftime('%B %d, %Y, %r')
> -> 'January 01, 2021, 12:00:00 AM'  # Expected output
> 
> dt.strftime('%s')
> -> '1609484400'  # This is '2021-01-01 07:00:00+00:00' in unix epoch time (7 hours ahead)
> 
> dt.tz_convert('MST').strftime('%s')
> -> '1609484400'  # Converting the timezone to MST before strftime strangely produces the same result

The exact same outputs occurred using `pd.Timestamp()` with the timezone set to either UTC or MST. Again, the expected output for `dt.strfime('%s')` should be ""1609484400"" in UTC, and I'm confused why I can't produce this value on my local machine. I know I could try to futz around with my Mac's system date, or hardcode a solution into my script by subtracting 7 hours from the dt object, but I don't want to change my Mac's system date, nor do I want to hardcode a solution into the script residing on my local machine. Am I doing something wrong or fundamentally misunderstanding something here, or is this a bug for the epoch time printoff for strftime? 

I am using:
Pandas version 1.2.0
Mac OS version 10.13.6

Thanks in advance for your help!


"
385017049,23959,"tz_compare fails to consider string ""UTC"" and pytz UTC object equal",TomAugspurger,closed,2018-11-27T22:33:59Z,2021-01-19T22:43:13Z,"Nothing user-facing here, but this surprised me.

```python
In [5]: pd._libs.tslibs.timezones.tz_compare(""UTC"", pytz.timezone(""UTC""))
Out[5]: False
```

Other timezones behave as expected

```
In [6]: pd._libs.tslibs.timezones.tz_compare(""US/Eastern"", pytz.timezone(""US/Eastern""))
Out[6]: True
```

Is this user error, or a bug?"
775112753,38733,BUG: GroupBy.idxmax/idxmin with EA dtypes,jbrockmendel,closed,2020-12-27T21:40:23Z,2021-01-19T23:28:59Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

@jorisvandenbossche thoughts on how/where to handle the skipna kwarg?"
787571925,39216,API/BUG: treat different UTC tzinfos as equal,jbrockmendel,closed,2021-01-16T23:09:44Z,2021-01-19T23:31:14Z,"- [x] closes #23959
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
788571301,39261,BUG: iloc assignment in Pandas 1.2.0,quant-dc,closed,2021-01-18T22:03:53Z,2021-01-20T00:13:08Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

# %% Create data

data = pd.DataFrame([
    [1,2,3,4],
    [1,2,3,4]
], columns=['A', 'B', 'C', 'D'])

# %% Check missing data assignment
addition = pd.DataFrame([
    [5,6,7],
    [1,2,3]
], columns=['A', 'B', 'C'])

data.iloc[0, :] = addition.iloc[0, :]  # Fails

# %% Check misaligned assignment
data = pd.DataFrame([
    [1,2,3,4],
    [1,2,3,4]
], columns=['A', 'B', 'C', 'D'])

addition_misaligned = pd.DataFrame([
    [5,6,7,8],
    [1,2,4,3]
], columns=['A', 'B', 'D', 'C'])

data.iloc[0, :] = addition_misaligned.iloc[0, :]  # Works, result incorrect
pd.testing.assert_frame_equal(data, addition_misaligned, check_like=True)
```

#### Problem description

My apologies if this is two reports in one.

Previously (pandas 1.1.5), assignment of partial series to rows in a DataFrame would complete and the alignment would be such that a missing value was applied to the element that was missing in the assignment. This now errors with a broadcast shape error.

Related, assignment of a fully specified series works, but the labels are not assigned correctly and instead assigned by their matrix position. Again, the values were aligned in pandas 1.1.5.

Reading the discussion on #39004 I think this may be related. And the error is similar to [this comment](https://github.com/pandas-dev/pandas/issues/39004#issuecomment-756685730).

```python-traceback
Traceback (most recent call last):
  File ""venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3331, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-c0c0cf213f59>"", line 13, in <module>
    data.iloc[0, :] = addition.iloc[0, :]  # Fails
  File ""venv/lib/python3.7/site-packages/pandas/core/indexing.py"", line 695, in __setitem__
    iloc._setitem_with_indexer(indexer, value, self.name)
  File ""venv/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1644, in _setitem_with_indexer
    self._setitem_single_block(indexer, value, name)
  File ""venv/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1872, in _setitem_single_block
    self.obj._mgr = self.obj._mgr.setitem(indexer=indexer, value=value)
  File ""venv/lib/python3.7/site-packages/pandas/core/internals/managers.py"", line 565, in setitem
    return self.apply(""setitem"", indexer=indexer, value=value)
  File ""venv/lib/python3.7/site-packages/pandas/core/internals/managers.py"", line 431, in apply
    applied = getattr(b, f)(**kwargs)
  File ""venv/lib/python3.7/site-packages/pandas/core/internals/blocks.py"", line 994, in setitem
    values[indexer] = value
ValueError: could not broadcast input array from shape (3) into shape (4)
```


[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output

Assignment of the data to DataFrame respecting the labels of the assigning data.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Tue Nov 10 00:10:30 PST 2020; root:xnu-6153.141.10~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_GB.UTF-8
pandas           : 1.2.0
numpy            : 1.19.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.0.3
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
558738694,31582,Dispatch IntervalIndex.argsort to IntervalArray,jbrockmendel,closed,2020-02-02T18:47:50Z,2021-01-20T00:13:22Z,"cc @jschendel thoughts on if we can do better in the non-ascending/""quicksort"" case?"
789507207,39284,BUG: read_excel fails to retrieve complete data / returns empty dataframe,thuwon,closed,2021-01-20T00:31:26Z,2021-01-20T00:34:20Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd 
import numpy as np
import os

df = pd.read_excel('BinanceExport.xlsx')
df

```

#### Problem description

pandas.read_excel doesn't correctly read certain binance.com xlsx exports (Microsoft Workbook format)

Trying to my binance exports returns an empty dataframe
```python
df = pd.read_excel('BinanceExport.xlsx')
df

>>> Date(UTC)
    ---------
```
or 
```python
df = pd.read_excel('BinanceExport.xlsx')
df

>>> Empty DataFrame
    Columns: [Date(UTC)]
  

  Index: []
```

Renaming the file doesn't help, but manually resaving does

It seems related to this issue https://github.com/pandas-dev/pandas/issues/39250 
but mine isn't a corrupted file nor does it return an error

I posted a question on stackoverflow and the export is downloadable down below

#### Expected Output

return complete dataframe

#### Output of ``pd.show_versions()``

<details>

[INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.9.0.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Tue Nov 10 00:10:30 PST 2020; root:xnu-6153.141.10~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : None
LOCALE           : en_US.UTF-8

pandas           : 1.2.0
numpy            : 1.19.2
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 49.6.0.post20201009
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.6
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 2.0.1
xlwt             : None
numba            : None]

</details>

[BinanceExport.xlsx](https://github.com/pandas-dev/pandas/files/5839398/BinanceExport.xlsx)"
758281851,38340,ENH: implement fast isin() for nullable dtypes,jorisvandenbossche,closed,2020-12-07T08:19:19Z,2021-01-20T01:44:33Z,"Currently, you can get quite a slowdown:

```
In [41]: arr = np.random.randint(0, 10, 1_000_001)

In [42]: s = pd.Series(arr)

In [43]: %timeit s.isin([1, 2, 3, 20])
2.71 ms ± 175 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [44]: s = pd.Series(arr, dtype=""Int64"")

In [45]: %timeit s.isin([1, 2, 3, 20])
22.9 ms ± 96.7 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```"
786849849,39186,REF: extract classes in pandas/core/describe.py,ivanovmg,closed,2021-01-15T12:12:34Z,2021-01-20T03:31:05Z,"- [ ] xref #36833
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

Extract classes ``SeriesDescriber`` and ``DataFrameDescriber``.
In the next steps I am going to enable strategy pattern and move corresponding describe functions to the concrete strategy classes (for each datatype)."
788745506,39268,CLN: remove pd import in pandas/core/computation,ivanovmg,closed,2021-01-19T06:27:46Z,2021-01-20T03:31:15Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

Remove ``import pandas as pd`` in ``pandas/core/computation``."
786568460,39184,"BUG: Numpy ufuncs e.g. np.[op](df1, df2) aligns columns in pandas 1.2.0 where it did not before",WhistleWhileYouWork,closed,2021-01-15T05:41:59Z,2021-01-20T07:22:57Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample

```python
df = pd.DataFrame({k: [1,2,3,4,5] for k in 'abcd'})
np.add(df[['a', 'b']], df[['c', 'd']])
```

#### Problem description

This is a regression from pandas 1.1.5 (both versions are using numpy 1.19.5).

Normally if we want to add, subtract, multiply or divide df columns with different names we get NaNs because the column names don't match. E.g:
```python
>>> df[['a', 'b']] + df[['c', 'd']]
    a   b   c   d
0 NaN NaN NaN NaN
1 NaN NaN NaN NaN
2 NaN NaN NaN NaN
3 NaN NaN NaN NaN
4 NaN NaN NaN NaN
```

To get around this, we would use np.[op](df1, df2).
However, we get the same output as above.
```python
>>> np.add(df[['a', 'b']], df[['c', 'd']])
    a   b   c   d
0 NaN NaN NaN NaN
1 NaN NaN NaN NaN
2 NaN NaN NaN NaN
3 NaN NaN NaN NaN
4 NaN NaN NaN NaN
```



#### Expected Output

```python
# Using pandas 1.1.5:
>>> np.add(df[['a', 'b']], df[['c', 'd']])
    a   b
0   2   2
1   4   4
2   6   6
3   8   8
4  10  10
```

#### Temporary solution
```python
# This may have a potential copy penalty with the conversion to numpy
>>> df[['a', 'b']] + df[['c', 'd']].to_numpy()
    a   b
0   2   2
1   4   4
2   6   6
3   8   8
4  10  10
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.9.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.2.0
numpy            : 1.19.5
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 49.6.0.post20210108
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
</details>

 Just my 2 cents: I was more than willing to test this on a nightly / master release, but it doesn't appear you release those. It could be quite beneficial to publish nightlies to PyPl so we don't report issues that have already been fixed. For some, it might be easier to test a nightly than peruse recent open and closed issues."
656402511,35270,TST: Add test to ensure DF describe does not throw an error (#32409),luckyvs1,closed,2020-07-14T07:45:02Z,2021-01-20T07:38:37Z,"- [x] closes #32409
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Notes:
- Tested using `pytest pandas/tests/frame/methods/test_describe.py::TestDataFrameDescribe::test_describe_does_not_raise_error` and tested error format by calling a raise error line in the try block `raise TypeError(""Test message"")`"
789716088,39288,Backport PR #39239 on branch 1.2.x (DEPR: raise deprecation warning in numpy ufuncs on DataFrames if not aligned + fallback to <1.2.0 behaviour),meeseeksmachine,closed,2021-01-20T07:28:05Z,2021-01-20T08:15:48Z,Backport PR #39239: DEPR: raise deprecation warning in numpy ufuncs on DataFrames if not aligned + fallback to <1.2.0 behaviour
787785446,39239,DEPR: raise deprecation warning in numpy ufuncs on DataFrames if not aligned + fallback to <1.2.0 behaviour,jorisvandenbossche,closed,2021-01-17T20:15:01Z,2021-01-20T09:42:33Z,"Closes #39184

This is obviously a last-minute change, but if people agree on the deprecation, I think we should try to include it in v1.2.1. I *think* my patch is relatively safe, since converting the input to numpy arrays (what I am doing now manually as fallback) is what happened before adding `DataFrame.__array_ufunc__` as well. 

It adds quite some lines of code, but it's mostly some simple checking of the exact case which is a bit verbose.

The specific tests I added were verified to pass on pandas 1.1.5, so they codify the previous behaviour (minus the warnings).

cc @TomAugspurger 


"
787757141,39234,DOC: Start v1.2.2 release notes,simonjayhawkins,closed,2021-01-17T17:44:59Z,2021-01-20T13:23:52Z,
789975873,39294,Backport PR #39234 on branch 1.2.x (DOC: Start v1.2.2 release notes),meeseeksmachine,closed,2021-01-20T13:24:12Z,2021-01-20T14:40:23Z,Backport PR #39234: DOC: Start v1.2.2 release notes
789708307,39287,REF: split describe categorical function,ivanovmg,closed,2021-01-20T07:14:41Z,2021-01-20T15:17:03Z,"- [ ] xref #36833
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

Moved handing timestamp data from ``describe_categorical_1d`` into ``describe_timestamp_as_categorical_1d``.
Extracted function ``select_describe_func`` for selecting the proper function, suitable for a given data type.

Pros:
- Simplified logic on which function is selected for which data type.
- Improved way of issuing ``FutureWarning``.
- Next logical step in enabling the strategy pattern.

Cons:
- Unnecessary argument ``percentiles_ignored`` passed into functions for handling categorical types. This is done to unify the interface across all functions for describing a series. The problem will be handled when enabling strategy pattern as the percentiles will become an instance attribute for the strategy class.
- Slight duplication in ``describe_categorical_1d`` into ``describe_timestamp_as_categorical_1d``, which will be handled as the strategy class for categorical data is created (it will be encapsulated there)."
788727483,39267,ENH: cast instead of raise for IntervalIndex setops with differnet closed,jbrockmendel,closed,2021-01-19T05:49:05Z,2021-01-20T15:54:31Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
790137182,39300,STYLE: Add a precommit rule that validates there is no usage in pd.api.types.<method>,nofarm3,closed,2021-01-20T16:34:11Z,2021-01-20T17:29:44Z,"Followup to  issue #39203 

Add a precommit rule that validates there is no usage in pd.api.types.<method>

For example:

Wrong:
```pd.api.types.is_number(1)```

Right:

```
from pandas.api.types import is_number

is_number(1)
```
"
734542308,37586,CLN refactor rest of core,MarcoGorelli,closed,2020-11-02T14:17:39Z,2021-01-20T17:36:20Z,"Some refactorings found by Sourcery https://sourcery.ai/

I've removed the ones of the kind
```diff
- if param:
-     var = a
- else:
-     var = b
+ var = a if param else b
```"
574639058,32409,pandas1.0.1 has trouble with certain column names,jeremy-rutman,closed,2020-03-03T12:37:59Z,2021-01-20T18:57:32Z,"#### Code Sample, a copy-pastable example if possible

```python

df = pd.read_json(file_path,lines=True)

TypeError: unhashable type: ‘dict’ .  `

```
#### Problem description

Pandas1.0.1 with python3.7 hits the above for json containing dicts containing the key ""last_status_change_at"" and possibly others, while pandas0.25.3 with python3.7 does not. I haven't checked yet with python3.6. 

#### Expected Output
no unhashableness
#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS (pandas 1.0.1 which hits error)
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None


INSTALLED VERSIONS  (pandas0.25.3 which doesnt hit error)
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
789329954,39277,REF: share PeriodArray.asfreq with Period.asfreq,jbrockmendel,closed,2021-01-19T20:13:25Z,2021-01-20T19:24:12Z,"Slightly faster for PeriodArray, no change for Period.

```
In [3]: pi = pd.period_range(""2016-01-01"", periods=100, freq=""D"")

In [4]: %timeit pi.asfreq(""W"")
43.5 µs ± 696 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- master
36.3 µs ± 1.06 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- PR

In [5]: per = pi[0]

In [6]: %timeit per.asfreq(""W"")
10.5 µs ± 88.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)  #  <-- master
10.5 µs ± 160 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)  # <-- PR
```"
789183430,39274,ENH: Period comparisons with mismatched freq use py3 behavior,jbrockmendel,closed,2021-01-19T16:45:56Z,2021-01-20T20:25:02Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
490562789,28329,"DataFrame.corr(method=""kendall"") calculation is slow",dsaxton,closed,2019-09-06T23:51:58Z,2021-01-20T21:08:10Z,"```python
import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.randn(1000, 300))

df.corr(method=""kendall"")
# 21.6 s ± 686 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

`DataFrame.corr(method=""kendall"")` doesn't scale particularly well, perhaps because it's the only named correlation method that isn't Cythonized at the moment (we just call `kendalltau` from `scipy` repeatedly in a Python for loop: https://github.com/pandas-dev/pandas/blob/master/pandas/core/frame.py#L7454).  It may be worthwhile to try to implement something more efficient within `_libs/algos.pyx`.

Relevant discussion: https://github.com/pandas-dev/pandas/pull/28151"
775051286,38721,RLS: 1.2.1,simonjayhawkins,closed,2020-12-27T14:50:15Z,2021-01-20T21:29:11Z,"Tracking issue for the 1.2.1 release.

https://github.com/pandas-dev/pandas/milestone/81

Currently scheduled for January 18, 2021 (date flexible on severity of regressions)

List of open regressions: https://github.com/pandas-dev/pandas/issues?q=is%3Aopen+is%3Aissue+label%3ARegression"
790181476,39301,CI consistently put `|` at start of line in pygrep hooks,MarcoGorelli,closed,2021-01-20T17:35:04Z,2021-01-20T22:15:15Z,"Currently, the pygrep hooks have the ""or"" `|` symbol at the end of the line, which makes diffs longer than necessary and can cause bugs. It would be better to put that symbol at the beginning of the line.

For example, instead of
```
    -   id: non-standard-numpy.random-related-imports
        name: Check for non-standard numpy.random-related imports excluding pandas/_testing.py
        language: pygrep
        exclude: pandas/_testing.py
        entry: |
            (?x)
            # Check for imports from np.random.<method> instead of `from numpy import random` or `from numpy.random import <method>`
            from\ numpy\ import\ random|
            from\ numpy.random\ import
```

it should be
```
    -   id: non-standard-numpy.random-related-imports
        name: Check for non-standard numpy.random-related imports excluding pandas/_testing.py
        language: pygrep
        exclude: pandas/_testing.py
        entry: |
            (?x)
            # Check for imports from np.random.<method> instead of `from numpy import random` or `from numpy.random import <method>`
            from\ numpy\ import\ random
            |from\ numpy.random\ import
```

There's even an `|` symbol missing in the `non-standard-imports-in-tests`, which should be fixed (having the symbol at the beginning of the line would probably have made this easier to spot)

The file that needs changing is `.pre-commit-config.yaml`"
790293069,39304,CLN: consistently put '|' at start of line in pygrep hooks #39301,gustavocmaciel,closed,2021-01-20T20:20:43Z,2021-01-20T22:15:36Z,"- [x] closes #39301
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them


I moved the `|` symbols from the end of the line to the beginning of the line in pygrep hooks.

I also added the `|` symbol in the `non-standard-imports-in-tests` at the beginning of the line 93, which was missing as @MarcoGorelli pointed.
"
787800547,39241,Included reading from Excel example as suggested via #38990,bsun94,closed,2021-01-17T21:38:17Z,2021-01-21T00:45:52Z,"- In the ""Comparison to"" documentation for Excel, added in example of writing to and reading from an Excel file as suggested in #38990, which is based on #38554."
388038976,24124,DatetimeIndex and Timestamp have different implementation limits,jbrockmendel,closed,2018-12-06T03:14:37Z,2021-01-21T00:50:18Z,"`Timestamp`'s minimum value is bounded away from `np.iinfo(np.int64).min` ""to allow overflow free conversion with a microsecond resolution"", but `DatetimeIndex` is not:

```
>>> dti = pd.date_range(end=pd.Timestamp.min, periods=2, freq='ns')
>>> dti
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pandas/core/base.py"", line 77, in __repr__
    return str(self)
  File ""pandas/core/base.py"", line 57, in __str__
    return self.__bytes__()
  File ""pandas/core/base.py"", line 69, in __bytes__
    return self.__unicode__().encode(encoding, 'replace')
  File ""pandas/core/indexes/base.py"", line 927, in __unicode__
    data = self._format_data()
  File ""pandas/core/indexes/base.py"", line 970, in _format_data
    is_justify=is_justify, name=name)
  File ""pandas/io/formats/printing.py"", line 348, in format_object_summary
    first = formatter(obj[0])
  File ""pandas/core/arrays/datetimelike.py"", line 333, in __getitem__
    return self._box_func(val)
  File ""pandas/core/arrays/datetimes.py"", line 327, in <lambda>
    return lambda x: Timestamp(x, freq=self.freq, tz=self.tz)
  File ""pandas/_libs/tslibs/timestamps.pyx"", line 736, in pandas._libs.tslibs.timestamps.Timestamp.__new__
    ts = convert_to_tsobject(ts_input, tz, unit, 0, 0, nanosecond or 0)
  File ""pandas/_libs/tslibs/conversion.pyx"", line 324, in pandas._libs.tslibs.conversion.convert_to_tsobject
    check_dts_bounds(&obj.dts)
  File ""pandas/_libs/tslibs/np_datetime.pyx"", line 120, in pandas._libs.tslibs.np_datetime.check_dts_bounds
    raise OutOfBoundsDatetime(
pandas._libs.tslibs.np_datetime.OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 1677-09-21 00:12:43

```

"
790440084,39308,BUG: date_range hitting iNaT,jbrockmendel,closed,2021-01-20T23:21:12Z,2021-01-21T01:56:01Z,"- [x] closes #24124
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
787546831,39212,BUG: DataFrame.apply axis=1 for str ops with no axis argument,rhshadrach,closed,2021-01-16T20:31:42Z,2021-01-21T02:11:51Z,"- [x] closes #39211
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

"
675693396,35643,ENH: Styler tooltips feature,attack68,closed,2020-08-09T12:41:11Z,2021-01-21T06:46:55Z,"- [x] closes #21266 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This is a draft PR to address 21266: Tooltips for DataFrames in Styler.

This PR has the following objectives:

1) Add functionality whilst being completely (almost) decoupled from the rest of the HTML rendering process
2) Be fully backwards compatible and have no impact at all on previous Styler's that have never used tooltips.
3) Use pseudo CSS class rendering via `table_styles` to control the visualisation of the tooltips.

To address 1) the architecture was simple:

- If `tooltips` are detected then an additional HTML element is added to every data cell: `<span class=""pd-t""></span>`. If `tooltips` are not detected (by default) nothing is done.
- Add generic table level CSS for the tooltip class, hiding tooltips by default but positioning, sizing, and coloring them.
- Loop through a tooltips DataFrame and based on the row and col index add additional content to the <span> element using the ::after pseudo-element targeting the exact cell id. This is table level also. Rendering is performed as the last step.

To address 2) default values ensure the functions make no changes unless `set_tooltips` has been called.

## Extensions

The very simple architecture requires a DataFrame of tooltips to be supplied. This requires the user to have constructed it privately (possibly using the regular DataFrame.apply and DataFrame.applymap) methods. These could be incorporated.

It would also be better to only add extra HTML <span> elements to data cells that have tooltips. This requires conditional looping of the render dict, which I haven't included here to save on initial complexity. For small dataframes it is also irrelevant.

However, given my time constraints I would rather leave this to a more available and more competent developer!!

Comments welcome..
"
788385928,39254,Backport PR #39202: REGR: to_stata tried to remove file before closing it,simonjayhawkins,closed,2021-01-18T16:04:43Z,2021-01-21T12:06:05Z,Backport PR #39202
790110365,39299,"CLN, TYP Remove string type hints",MarcoGorelli,closed,2021-01-20T16:02:21Z,2021-01-21T12:15:35Z,"- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

For some background: there was some consensus on doing this [here](https://github.com/pandas-dev/pandas/pull/36034#issuecomment-685186742)

Then [here](https://github.com/pandas-dev/pandas/pull/39270#pullrequestreview-571190631) there was the suggestion to use `ast.parse`, rather than regular expressions. I had a go at that, and made this into its own package: https://github.com/MarcoGorelli/no-string-hints , which I've included here as a pre-commit hook

The changes here are just a result of running `pre-commit run no-string-hints -a`, and then adding `from __future__ import annotations` to some files."
768298455,38514,CI: un-xfail,jbrockmendel,closed,2020-12-15T23:30:15Z,2021-01-21T14:50:03Z,"No idea why this stopped failing, but not going to look a gift horse in the mouth."
719016911,37067,CI/TST: xfail in `test_parquet` has passed,fangchenli,closed,2020-10-12T03:44:39Z,2021-01-21T14:51:21Z,"The xfail test `test_s3_roundtrip_for_dir` in `io/test_parquet` has passed on my machine. But I don't see it on our CI.

<details>

INSTALLED VERSIONS
------------------
commit           : 4d3b197c98913e894bcea9edca9ee7bad2964d84
python           : 3.8.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-48-generic
Version          : #52-Ubuntu SMP Thu Sep 10 10:58:49 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.2.0.dev0+745.g4d3b197c9
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.6.0.post20201009
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : 5.37.1
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.6
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.3
fastparquet      : 0.4.1
gcsfs            : 0.7.1
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.2
sqlalchemy       : 1.3.19
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.51.2

</details>
"
787315913,39203,STYLE: dont' use pd.api.types anywhere in tests,jreback,closed,2021-01-16T01:28:21Z,2021-01-21T15:04:52Z,"example here: https://github.com/pandas-dev/pandas/pull/37367/files#diff-69c698518ca64a05f8b137ff9bb5445ce9fd21139b396e9b0d4ce7338b2a3984R132

we should just ban this with a precommit style rule (and import things directly). the doc-strings also should be updated to do the import once.

```
grep -r pd.api.types ~/pandas-dev/pandas
/home/jreback/pandas-dev/pandas/_libs/lib.pyx:    >>> pd.api.types.is_scalar(dt)
/home/jreback/pandas-dev/pandas/_libs/lib.pyx:    >>> pd.api.types.is_scalar([2, 3])
/home/jreback/pandas-dev/pandas/_libs/lib.pyx:    >>> pd.api.types.is_scalar({0: 1, 2: 3})
/home/jreback/pandas-dev/pandas/_libs/lib.pyx:    >>> pd.api.types.is_scalar((0, 2))
/home/jreback/pandas-dev/pandas/_libs/lib.pyx:    >>> pd.api.types.is_scalar(Fraction(3, 5))
/home/jreback/pandas-dev/pandas/tests/frame/test_ufunc.py:    if pd.api.types.is_extension_array_dtype(dtype) or isinstance(dtype, dict):
/home/jreback/pandas-dev/pandas/tests/frame/test_ufunc.py:        pd.api.types.is_extension_array_dtype(dtype_a)
/home/jreback/pandas-dev/pandas/tests/frame/test_ufunc.py:        or pd.api.types.is_extension_array_dtype(dtype_b)
/home/jreback/pandas-dev/pandas/tests/frame/test_ufunc.py:    if pd.api.types.is_extension_array_dtype(dtype) or isinstance(dtype, dict):
/home/jreback/pandas-dev/pandas/tests/extension/base/dtype.py:        return not pd.api.types.is_string_dtype(dtype)
/home/jreback/pandas-dev/pandas/tests/extension/base/dtype.py:        return not pd.api.types.is_object_dtype(dtype)
/home/jreback/pandas-dev/pandas/tests/extension/arrow/arrays.py:        if pd.api.types.is_scalar(item):
/home/jreback/pandas-dev/pandas/tests/extension/arrow/test_bool.py:    assert pd.api.types.is_bool_dtype(data)
/home/jreback/pandas-dev/pandas/tests/extension/json/array.py:            if pd.api.types.is_bool_dtype(item.dtype):
/home/jreback/pandas-dev/pandas/tests/extension/decimal/array.py:        if pd.api.types.is_list_like(value):
/home/jreback/pandas-dev/pandas/tests/extension/decimal/array.py:            if pd.api.types.is_scalar(key):
/home/jreback/pandas-dev/pandas/tests/extension/list/array.py:        elif pd.api.types.is_string_dtype(dtype) and not pd.api.types.is_object_dtype(
/home/jreback/pandas-dev/pandas/tests/extension/test_integer.py:                and pd.api.types.is_integer_dtype(other.dtype)
/home/jreback/pandas-dev/pandas/tests/extension/test_floating.py:                and pd.api.types.is_float_dtype(other.dtype)
/home/jreback/pandas-dev/pandas/core/dtypes/inference.py:    >>> pd.api.types.is_number(1)
/home/jreback/pandas-dev/pandas/core/dtypes/inference.py:    >>> pd.api.types.is_number(7.15)
/home/jreback/pandas-dev/pandas/core/dtypes/inference.py:    >>> pd.api.types.is_number(False)
/home/jreback/pandas-dev/pandas/core/dtypes/inference.py:    >>> pd.api.types.is_number(""foo"")
/home/jreback/pandas-dev/pandas/core/dtypes/inference.py:    >>> pd.api.types.is_number(""5"")
/home/jreback/pandas-dev/pandas/core/generic.py:        >>> cat_dtype = pd.api.types.CategoricalDtype(
```"
772419770,38622,BUG: dt64 <-> dt64tz roundup,jbrockmendel,closed,2020-12-21T19:47:42Z,2021-01-21T15:15:19Z,"xref #33401 which discusses astype, #24559 which discusses constructors.  I think these need to be considered jointly.

Two big-picture questions.

1) when we pass `constructor(dt64values, tz=tz)`, do we interpret dt64values as UTC or wall-times?
2) when we pass constructor(dt64values).astype(""M8[ns, tz]"")`, do we interpret dt64values as UTC or wall-times?

Let's check the current behavior on master for Timestamp, DatetimeIndex, and Series (see code block below if you want to check my work)

 cls                                        | Timestamp  | DatetimeIndex | Series  |
------------------------ | ------------- | ------------- | ------- |
cls(dt64, dtype=dt64tz)  | UTC  | Wall  | UTC |
cls(dt64).astype(dt64tz) | N/A  | Wall  | UTC |
cls(dt64tz).astype(""M8[ns]"")  | N/A  | UTC | UTC |

If we were starting fresh, I would probably lean towards making these all interpret dt64 as wall-times.  But because these are already mostly-UTC, it would be an easier change to bring the DatetimeIndex behavior in line with the others.

Thoughts?  cc @jorisvandenbossche @mroeschke @jreback 

```
values = (3600 * 10**9 * np.arange(4)).astype(""datetime64[ns]"")
dt64 = values[-1]

tz = ""US/Pacific""
dtype = pd.DatetimeTZDtype(tz=tz)

ts = pd.Timestamp(dt64, tz=tz)
dti = pd.DatetimeIndex(values, dtype=dtype)
ser = pd.Series(values, dtype=dtype)

if_wall = pd.DatetimeIndex(values).tz_localize(tz)
if_utc = pd.DatetimeIndex(values).tz_localize(""UTC"").tz_convert(tz)

assert ts == if_utc[-1]
assert (dti == if_wall).all()
assert (ser == if_utc).all()

dti_naive = pd.DatetimeIndex(values)
ser_naive = pd.Series(values)

res_dti = dti_naive.astype(dtype)
res_ser = ser.astype(dtype)

assert (res_dti == dti).all()
assert (res_ser == ser).all()

# astype from tzaware -> tznaive
assert (dti.astype(dti_naive.dtype) == dti.tz_convert(""UTC"").tz_localize(None)).all()
assert (ser_naive.astype(ser.dtype) == ser).all()
```
"
790638028,39310,CI: fix flaky tests,jbrockmendel,closed,2021-01-21T02:52:50Z,2021-01-21T15:21:43Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
788451506,39258,DEPR: astype dt64<->dt64tz,jbrockmendel,closed,2021-01-18T17:49:00Z,2021-01-21T15:23:36Z,"- [x] closes #38622
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
790383537,39306,ENH: PeriodIndex setops with incompatible freq cast instead of raise,jbrockmendel,closed,2021-01-20T22:23:46Z,2021-01-21T15:24:33Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

Did the same thing for IntervalIndex earlier in #39267"
789376408,39279,REF: share start_time/end_time between Period/PeriodArray,jbrockmendel,closed,2021-01-19T21:15:22Z,2021-01-21T15:25:28Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

One thing that might make this undesirable is that the annotations on this are accurate for Period but not for PeriodArray.  Thoughts @simonjayhawkins 
"
789893341,39291,DOC: Fix inaccurate statement in 1.2.0 release notes,rs2,closed,2021-01-20T11:27:14Z,2021-01-21T15:44:04Z,"#### Location of the documentation

https://pandas.pydata.org/docs/whatsnew/v1.2.0.html

#### Suggested fix for documentation

It would be more accurate for [this line](https://github.com/pandas-dev/pandas/blame/ff628b17ccfcaf14ceefeb438aafa56d3849937c/doc/source/whatsnew/v1.2.0.rst#L749) to reference https://github.com/pandas-dev/pandas/pull/37983 (itself) as #27101 does _not_ deal with the `xarray` upgrade, whereas #37983 does.

"
768845519,38521,BUG: Setting values to slice fails with duplicated column name,leonarduschen,closed,2020-12-16T13:05:45Z,2021-01-21T15:53:35Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

Originally posted on [StackOverflow](https://stackoverflow.com/questions/65255166/interesting-results-with-duplicate-columns-in-pandas-dataframe).

Possibly related to #15695 (the traceback looks different though)

---

#### Code Sample, a copy-pastable example
```python
import pandas as pd
df = pd.DataFrame(columns=['a', 'b', 'b'])
df.loc[:, 'a'] = list(range(5))  # raise ValueError
```

Traceback:
```python
Traceback (most recent call last):
  File ""c:\Users\leona\pandas\main.py"", line 3, in <module>
    df.loc[:, 'a'] = list(range(5))
  File ""c:\Users\leona\pandas\pandas\core\indexing.py"", line 691, in __setitem__
    iloc._setitem_with_indexer(indexer, value, self.name)
  File ""c:\Users\leona\pandas\pandas\core\indexing.py"", line 1636, in _setitem_with_indexer
    self._setitem_single_block(indexer, value, name)
  File ""c:\Users\leona\pandas\pandas\core\indexing.py"", line 1862, in _setitem_single_block
    self.obj._mgr = self.obj._mgr.setitem(indexer=indexer, value=value)
  File ""c:\Users\leona\pandas\pandas\core\internals\managers.py"", line 565, in setitem
    return self.apply(""setitem"", indexer=indexer, value=value)
  File ""c:\Users\leona\pandas\pandas\core\internals\managers.py"", line 428, in apply
    applied = getattr(b, f)(**kwargs)
  File ""c:\Users\leon\pandas\pandas\core\internals\blocks.py"", line 1022, in setitem
    values[indexer] = value
ValueError: cannot copy sequence with size 5 to array axis with dimension 0
```

#### Problem description

It works with no duplicated column:
```python
df = pd.DataFrame(columns=['a', 'b', 'c'])
df.loc[:, 'a'] = list(range(5))
```

These work even with duplicated column names:
```python
df = pd.DataFrame(columns=['a', 'b', 'b'])
df['a'] = list(range(5))  # Same as expected output below

df = pd.DataFrame(columns=['a', 'b', 'b'])
df.a = list(range(5))  # Same as expected output below
```

Setting on new column name is okay:
```python
df = pd.DataFrame(columns=['a', 'b', 'b'])
df.loc[:, 'c'] = list(range(5))

#      a    b    b  c
# 0  NaN  NaN  NaN  0
# 1  NaN  NaN  NaN  1
# 2  NaN  NaN  NaN  2
# 3  NaN  NaN  NaN  3
# 4  NaN  NaN  NaN  4
```


#### Expected Output

```
   a    b    b
0  0  NaN  NaN
1  1  NaN  NaN
2  2  NaN  NaN
3  3  NaN  NaN
4  4  NaN  NaN
```

#### Output of ``pd.show_versions()``

<details><summary>Output:</summary>


INSTALLED VERSIONS
------------------
commit           : 122d50246bcffcf8c3f252146340ac02676a5bf6
python           : 3.9.0.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 11, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : English_Singapore.1252

pandas           : 1.3.0.dev0+83.g122d50246.dirty
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.2.1
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None



</details>
"
214554403,15695,DataFrame.__setitem__ fails with multiple values and duplicate column names,toobaz,closed,2017-03-15T23:17:46Z,2021-01-21T15:54:42Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: df = pd.DataFrame(index=range(3), columns=['A', 'B', 'C', 'D', 'E', 'F'])

In [3]: df.loc[0, ['A', 'D']] = (1,2)

In [4]: df.loc[:, ['B', 'E']] = (1,2)

In [5]: df[['C', 'F']] = (1,2)

In [6]: df
Out[6]: 
     A  B  C    D  E  F
0    1  1  1    2  2  2
1  NaN  1  1  NaN  2  2
2  NaN  1  1  NaN  2  2

In [7]: dfdup = pd.DataFrame(index=range(3), columns=['A', 'B', 'C']*2)

In [8]: dfdup.loc[0, 'A'] = (1,2) # Works

In [9]: dfdup.loc[:, 'B'] = (1,2) # Works

In [10]: dfdup['C'] = (1,2) # Fails
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-10-17d5611af828> in <module>()
----> 1 dfdup['C'] = (1,2)

/home/pietro/nobackup/repo/pandas/pandas/core/frame.py in __setitem__(self, key, value)
   2421         else:
   2422             # set column
-> 2423             self._set_item(key, value)
   2424 
   2425     def _setitem_slice(self, key, value):

/home/pietro/nobackup/repo/pandas/pandas/core/frame.py in _set_item(self, key, value)
   2487 
   2488         self._ensure_valid_index(value)
-> 2489         value = self._sanitize_column(key, value)
   2490         NDFrame._set_item(self, key, value)
   2491 

/home/pietro/nobackup/repo/pandas/pandas/core/frame.py in _sanitize_column(self, key, value, broadcast)
   2658 
   2659             # turn me into an ndarray
-> 2660             value = _sanitize_index(value, self.index, copy=False)
   2661             if not isinstance(value, (np.ndarray, Index)):
   2662                 if isinstance(value, list) and len(value) > 0:

/home/pietro/nobackup/repo/pandas/pandas/core/series.py in _sanitize_index(data, index, copy)
   2847 
   2848     if len(data) != len(index):
-> 2849         raise ValueError('Length of values does not match length of ' 'index')
   2850 
   2851     if isinstance(data, PeriodIndex):

ValueError: Length of values does not match length of index

In [11]: dfdup
Out[11]: 
     A  B    C    A  B    C
0    1  1  NaN    2  2  NaN
1  NaN  1  NaN  NaN  2  NaN
2  NaN  1  NaN  NaN  2  NaN
```
#### Problem description

While ``loc`` correctly treats ``A`` as referring to two columns, ``DataFrame[.]`` sees one value only and raises. Note that ``Series[.]`` behaves correctly:

``` python
In [12]: s = pd.Series(index=['A', 'B']*2)

In [13]: s.loc['A'] = (1,2)

In [14]: s
Out[14]: 
A    1.0
B    NaN
A    2.0
B    NaN
dtype: float64

In [15]: s['B'] = (1,2)

In [16]: s
Out[16]: 
A    1.0
B    1.0
A    2.0
B    2.0
dtype: float64
```

#### Expected Output

``Out[6]`` (except for column names, obviously)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.7.0-1-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: it_IT.utf8
LOCALE: it_IT.UTF-8

pandas: 0.19.0+605.gc081d5782
pytest: 3.0.6
pip: 9.0.1
setuptools: 33.1.1
Cython: 0.25.2
numpy: 1.12.0
scipy: 0.18.1
xarray: None
IPython: 5.1.0.dev
sphinx: 1.4.9
patsy: 0.3.0-dev
dateutil: 2.5.3
pytz: 2016.7
blosc: None
bottleneck: 1.2.0
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.0.0
openpyxl: 2.3.0
xlrd: 1.0.0
xlwt: 1.1.2
xlsxwriter: 0.9.6
lxml: 3.7.1
bs4: 4.5.3
html5lib: 0.999999999
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.8
s3fs: None
pandas_gbq: None
pandas_datareader: 0.2.1


</details>
"
789380958,39280,BUG: DataFrame.__setitem__ raising ValueError when setting multiple values to dup columns,phofl,closed,2021-01-19T21:21:25Z,2021-01-21T16:12:55Z,"- [x] closes #15695
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

Setting with a scalar key which is duplicated in columns should be the same as using a listlike key"
789636048,39286,TST: test_to_csv_compression_encoding_gcs fails sometimes,twoertwein,closed,2021-01-20T05:07:00Z,2021-01-21T17:23:23Z,"only seems to happen sometimes in ""Database / Linux_py37_cov""
```python
_______________ test_to_csv_compression_encoding_gcs[zip-cp1251] _______________
[gw1] linux -- Python 3.7.9 /usr/share/miniconda/envs/pandas-dev/bin/python

gcs_buffer = <_io.BytesIO object at 0x7f4959d0b650>, compression_only = 'zip'
encoding = 'cp1251'

    @td.skip_if_no(""gcsfs"")
    @pytest.mark.parametrize(""encoding"", [""utf-8"", ""cp1251""])
    def test_to_csv_compression_encoding_gcs(gcs_buffer, compression_only, encoding):
        """"""
        Compression and encoding should with GCS.
    
        GH 35677 (to_csv, compression), GH 26124 (to_csv, encoding), and
        GH 32392 (read_csv, encoding)
        """"""
        from fsspec import registry
    
        registry.target.clear()  # remove state
        df = tm.makeDataFrame()
    
        # reference of compressed and encoded file
        compression = {""method"": compression_only}
        if compression_only == ""gzip"":
            compression[""mtime""] = 1  # be reproducible
        buffer = BytesIO()
        df.to_csv(buffer, compression=compression, encoding=encoding, mode=""wb"")
    
        # write compressed file with explicit compression
        path_gcs = ""gs://test/test.csv""
        df.to_csv(path_gcs, compression=compression, encoding=encoding)
>       assert gcs_buffer.getvalue() == buffer.getvalue()
E       AssertionError: assert b'PK\x03\x04\...0\x00\x00\x00' == b'PK\x03\x04\...0\x00\x00\x00'
E         At index 10 diff: b'\xa6' != b'\xa5'
E         Use -v to get the full diff

```"
789347101,39278,BUG: loc.setitem raising ValueError when df has duplicate columns,phofl,closed,2021-01-19T20:36:45Z,2021-01-21T17:28:21Z,"- [x] closes #38521
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
789974984,39293,STYLE: dont use pd api types in tests,nofarm3,closed,2021-01-20T13:23:01Z,2021-01-21T17:30:12Z,"- [x] closes #39203 
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [] whatsnew entry

I didn't add a whatsnew entry since it's only style fixes.
I'll open a new issue regarding @jreback suggestion to ban this with a precommit style rule.
I prefer it to be done in a different PR."
791243829,39319,Backport PR #39316 on branch 1.2.x ([DOC] Update PR link in 1.2.0 release notes),meeseeksmachine,closed,2021-01-21T15:45:36Z,2021-01-21T17:35:29Z,Backport PR #39316: [DOC] Update PR link in 1.2.0 release notes
786670488,39185,DOC: set the documentation language,afeld,closed,2021-01-15T07:22:26Z,2021-01-21T17:46:42Z,"Ensures the language is specified in the generated HTML. [Documentation on the configuration option.](https://www.sphinx-doc.org/en/master/usage/configuration.html#confval-language)

Before:

```
$ curl -s https://pandas.pydata.org/docs/dev/getting_started/comparison/comparison_with_spreadsheets.html | grep ""<html""
<html>
```

After:

```
$ python make.py --single getting_started/comparison/comparison_with_spreadsheets.rst
$ grep ""<html"" build/html/getting_started/comparison/comparison_with_spreadsheets.html 
<html lang=""en"">
```

- [ ] ~~closes #xxxx~~
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] ~~whatsnew entry~~
"
790919394,39314,DOC: clarify in which version the excel engine default changed,jorisvandenbossche,closed,2021-01-21T10:31:33Z,2021-01-21T18:00:02Z,"Small clarification, because ""previously"" only made sense in the actual v1.2.0 release notes, and not in the main docs"
787812939,39244,BUG: Timestamp.round floating point error,jbrockmendel,closed,2021-01-17T22:46:46Z,2021-01-21T18:22:40Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

xref #38964 analogous bug in Timedelta (separate PR upcoming)

```
ts = pd.Timestamp.max - pd.Timedelta(304)

>>> ts.ceil(""s"")
Timestamp('1677-09-21 00:12:43.290448384')
```"
782870090,39092,CLN: Numpy compat functions namespace,phofl,closed,2021-01-10T17:49:13Z,2021-01-21T20:18:17Z,"- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them


Should we do something about the private variables ``_np_version_under1p19`` and ``_np_version_under1p20`` too?

cc @jreback"
791398527,39324,BUG: Series.to_json returning a native python float from a column of type int64,mbiokyle29,closed,2021-01-21T18:34:41Z,2021-01-21T20:51:20Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas
example_df = pandas.DataFrame({'a': [1], 'b': [1.0]})

# this passes (DataFrame.to_dict is called)
assert type(example_df.to_dict('records')[0]['a']) is int

# this passes (Series.to_dict is called but on a Series with no floats)
assert type(example_df[['a']].iloc[0].to_dict()['a']) is int

# this fails
assert type(example_df.iloc[0].to_dict()['a']) is int

# this fails
assert type(example_df.apply(pandas.Series.to_dict, axis=1).iloc[0]['a']) is int
```

#### Problem description

I have found another issue which involves type conversion / handling with the `DataFrame`/`Series` `to_dict` function. There are a number of other issues in this area that I found, but I think this one is unique.

Related issues:
- https://github.com/pandas-dev/pandas/issues/24908: this is an issue where the output is not being converted to the native python data type
- https://github.com/pandas-dev/pandas/issues/25969: again, this is ""numpy -> native"" type conversion not happening
- https://github.com/pandas-dev/pandas/issues/27616: numpy.boolean being returned


In most/all the cases I could find, the issue was that `to_dict` was returning the underlying `numpy` type and not converting to the native. In this case, I am getting a native python type, just the wrong one. It only happens when both are true:
- Series.to_dict is called
- The Series in question contains an int64 and a float64 column

#### Expected Output
I would expect all 3 assert statements to pass, which is to say that I should get data back with int type for the 'a' column. Visually speaking:
```python
In [7]: example_df.to_dict('records')[0]
Out[7]: {'a': 1, 'b': 1.0}

In [8]: example_df.iloc[0].to_dict()
Out[8]: {'a': 1.0, 'b': 1.0}. # a should not be a float here

In [11]: type(example_df.iloc[0].to_dict()['a'])
Out[11]: float  # this should be int
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 9d598a5e1eee26df95b3910e3f2934890d062caa
python           : 3.9.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 20.1.0
Version          : Darwin Kernel Version 20.1.0: Sat Oct 31 00:07:11 PDT 2020; root:xnu-7195.50.7~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.1
numpy            : 1.19.5
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.1.1
Cython           : None
pytest           : 6.2.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
787805137,39242,Updated Comparison to Excel documentation with code example of read_excel and to_excel for #38990,bsun94,closed,2021-01-17T22:02:49Z,2021-01-21T23:41:12Z,"- This is my first time contributing - please let me know if any other detail is needed! Essentially I just swapped the order of the ""Excel"" and ""CSV"" paragraphs in the data importing section, so that the examples given for to_excel and read_excel would flow from the CSV example without needing repeated code/codeblocks.
"
789300973,39276,BUG: is_utc(dateutil_utc),jbrockmendel,closed,2021-01-19T19:35:55Z,2021-01-21T23:48:17Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
777719887,38930,TST/REF: splitting pandas/io/parsers.py into multiple files,arw2019,closed,2021-01-03T21:29:21Z,2021-01-21T23:48:43Z,"`pandas/io/parsers.py` is close to 4k LOC (3986 on 1.3 master). Would it be reasonable to split it into multiple files (located, say, in a `pandas/io/parsers` directory)?

AFAICT the file contains 4 logical pieces:
- `read_...` methods (~700 LOC)
-  `ParserBase` and related module-level methods ( ~1100 LOC)
- `CParser` and related module-level methods (~400LOC)
- `PythonParser`, its derived classes and related module-level methods (~1800 LOC)

Once #38370 goes in these would be joined by 
- `PyarrowParser` (~100 LOC right now)

IMO this kind of refactor would make the module somewhat easier to grok
"
791375691,39322,Backport PR #39314 on branch 1.2.x (DOC: clarify in which version the excel engine default changed),meeseeksmachine,closed,2021-01-21T17:59:53Z,2021-01-21T23:50:43Z,Backport PR #39314: DOC: clarify in which version the excel engine default changed
791196908,39318,TST: Benchmarks for CSV writing for various index variations,DanielFEvans,closed,2021-01-21T15:08:00Z,2021-01-21T23:58:19Z,"Benchmarks derived from GH37484 (https://github.com/pandas-dev/pandas/issues/37484). This captures a peculiar difference in timings between a DataFrame originating from `df.set_index(...).head(...)` and one from `df.head(...).set_index(...)`.

- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

I've not included a whatsnew entry, as no section seems particularly relevant to internal-only test/benchmark changes. If it should go in, shout and I'll add a note somewhere."
788648884,39264,CI/STYLE Fix misspellings exposed by codespell,zitorelova,closed,2021-01-19T02:08:34Z,2021-01-21T23:59:18Z,"- [ ] partial fix for  #38802 
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
791010841,39316,[DOC] Update PR link in 1.2.0 release notes,rs2,closed,2021-01-21T12:09:49Z,2021-01-22T00:04:13Z,"- [x] closes #39291
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
~whatsnew entry~
"
782680133,39068,BUG/API: DTI/TDI/PI.insert cast to object on failure,jbrockmendel,closed,2021-01-09T20:31:41Z,2021-01-22T00:07:29Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

ATM DTI/TDI/PI/insert are pretty idiosyncratic.  This changes them to match the base Index.insert by a) always trying _validate_fill_value (this changes the behavior for strings that can be parsed to datetime/timedelta/period) and b) always fall back to object in cases where this validation fails (this changes behavior for non-strings where the validation fails).

AFAICT the current behavior was implemented in #5819 and the only discussion I see about castable strings or non-castable non-strings is this [comment](https://github.com/pandas-dev/pandas/pull/5819#issuecomment-31454545), which I don't think is relevant any longer.

One corner case is mismatched tzs, which raise in master.  In the PR they cast to object.  They could also reasonably cast to the index's tz (xref #37605)

cc @rhshadrach IIRC you had a question about the insert tests in test_coercion.  I think this addresses what you were asking about.  Let me know if I missed something."
787607165,39221,BUG: Timestamp comparison with barely-out-of-bounds datetime64,jbrockmendel,closed,2021-01-17T03:35:59Z,2021-01-22T00:08:11Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
700467863,36321,BUG: SeriesGroupBy.transform should raise with axis=1,arw2019,closed,2020-09-13T02:57:47Z,2021-01-22T02:40:36Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example
The `dropna=True` case runs
```python
In [7]: df = pd.DataFrame({""A"": [0, 0, 7, 1], ""B"": [1, 2, 4, 3]}, index=list('abcd')) 
   ...: gb = df.groupby('B', dropna=True, axis=1) 
   ...: gb['B'].transform(len)                                                                         
Out[7]: Series([], Name: B, dtype: int64)
```
but with `dropna=False` the call to `transform` throws with a somewhat unfriendly message
```
In [9]: df = pd.DataFrame({""A"": [0, 0, 7, 1], ""B"": [1, 2, 4, 3]}, index=list('abcd')) 
   ...: gb = df.groupby('B', dropna=False, axis=1) 
   ...: gb['B'].transform(len)                    
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-2-c7271aa13f24> in <module>
      1 df = pd.DataFrame({""A"": [0, 0, 7, 1], ""B"": [1, 2, 4, 3]}, index=list('abcd'))
      2 gb = df.groupby('B', dropna=False, axis=1)
----> 3 gb['B'].transform(len)

/workspaces/pandas-arw2019/pandas/core/groupby/generic.py in transform(self, func, engine, engine_kwargs, *args, **kwargs)
    506 
    507         if not isinstance(func, str):
--> 508             return self._transform_general(func, *args, **kwargs)
    509 
    510         elif func not in base.transform_kernel_allowlist:

/workspaces/pandas-arw2019/pandas/core/groupby/generic.py in _transform_general(self, func, *args, **kwargs)
    546 
    547             concatenated = concat(results)
--> 548             result = self._set_result_index_ordered(concatenated)
    549         else:
    550             result = self.obj._constructor(dtype=np.float64)

/workspaces/pandas-arw2019/pandas/core/groupby/groupby.py in _set_result_index_ordered(self, result)
    691             result = result.sort_index(axis=self.axis)
    692 
--> 693         result.set_axis(self.obj._get_axis(self.axis), axis=self.axis, inplace=True)
    694         return result
    695 

/workspaces/pandas-arw2019/pandas/core/series.py in set_axis(self, labels, axis, inplace)
   4365     @Appender(generic.NDFrame.set_axis.__doc__)
   4366     def set_axis(self, labels, axis: Axis = 0, inplace: bool = False):
-> 4367         return super().set_axis(labels, axis=axis, inplace=inplace)
   4368 
   4369     @doc(

/workspaces/pandas-arw2019/pandas/core/generic.py in set_axis(self, labels, axis, inplace)
    657         """"""
    658         self._check_inplace_and_allows_duplicate_labels(inplace)
--> 659         return self._set_axis_nocheck(labels, axis, inplace)
    660 
    661     def _set_axis_nocheck(self, labels, axis: Axis, inplace: bool):

/workspaces/pandas-arw2019/pandas/core/generic.py in _set_axis_nocheck(self, labels, axis, inplace)
    662         # NDFrame.rename with inplace=False calls set_axis(inplace=True) on a copy.
    663         if inplace:
--> 664             setattr(self, self._get_axis_name(axis), labels)
    665         else:
    666             obj = self.copy()

/workspaces/pandas-arw2019/pandas/core/generic.py in __setattr__(self, name, value)
   5384         try:
   5385             object.__getattribute__(self, name)
-> 5386             return object.__setattr__(self, name, value)
   5387         except AttributeError:
   5388             pass

/workspaces/pandas-arw2019/pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__()
     64 
     65     def __set__(self, obj, value):
---> 66         obj._set_axis(self.axis, value)

/workspaces/pandas-arw2019/pandas/core/series.py in _set_axis(self, axis, labels, fastpath)
    425         if not fastpath:
    426             # The ensure_index call above ensures we have an Index object
--> 427             self._mgr.set_axis(axis, labels)
    428 
    429     # ndarray compatibility

/workspaces/pandas-arw2019/pandas/core/internals/managers.py in set_axis(self, axis, new_labels)
    217 
    218         if new_len != old_len:
--> 219             raise ValueError(
    220                 f""Length mismatch: Expected axis has {old_len} elements, new ""
    221                 f""values have {new_len} elements""

ValueError: Length mismatch: Expected axis has 2 elements, new values have 4 elements
                                                     
```


#### Expected Output
Since there are no missing values in the input `dropna=True` and `dropna=False` should give the same results.

#### Problem description 
(Edited)
As discussed below and in #35751 `SeriesGroupBy` with `axis=1` never makes sense. We should have a more explicit error message.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 0cdea22618723e4f4e2f1a29f98eab8084db9183
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-47-generic
Version          : #51-Ubuntu SMP Fri Sep 4 19:50:52 UTC 2020
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+354.g0cdea2261
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.1.0.post20200704
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : 5.19.0
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : 0.4.0
gcsfs            : 0.6.2
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.1
</details>
"
761008433,38401,BUG: .groupby().min() .max() .agg('min') .agg('max') ERROR,ghost9023,closed,2020-12-10T08:03:48Z,2021-01-22T10:17:41Z,"- [O] I have checked that this issue has not already been reported.
  I checked it but this bug reported that already solved. but in my pandas, still occur.
  (https://github.com/pandas-dev/pandas/issues/31522)

- [O] I have confirmed this bug exists on the latest version of pandas.
  I've checked latest version at anaconda

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python

# data given from https://www.kaggle.com/hesh97/titanicdataset-traincsv/data
import pandas as pd

df = pd.read_csv('./train.csv')
df.head()

class_group = df.groupby('Pclass')
print(class_group.groups)  # pass
print(class_group.mean()['Survived'])  # pass
print(class_group.median())  # pass
print(class_group.agg('min'))  # error occur. also .agg('max'), .min(), .max()


```

#### Problem description

I found same error occurred at older version, 1.0.0, 1.0.1. (https://github.com/pandas-dev/pandas/issues/31522)

At that ticket, after 1.0.3, this bug solved.

But I tested, Windows10 64bit python3.6 pandas 1.0.1, 1.0.3, 1.0.5, 1.1.3 and CentOS7.5 python3.8 pandas 1.0.5, same error occured.

Just like this. Same code, same data at Windows 10 and CentOS 7

* Windows
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
Traceback (most recent call last):
  File ""E:/PythonProjects/Deepphi/local-dev/pandaserror.py"", line 11, in <module>
    print(class_group.agg('min'))  # error occur. also .agg('max'), .min(), .max()
  File ""C:\Users\deepnoid-workstation\Anaconda3_64bit\envs\dev\lib\site-packages\pandas\core\groupby\generic.py"", line 951, in aggregate
    result, how = self._aggregate(func, *args, **kwargs)
  File ""C:\Users\deepnoid-workstation\Anaconda3_64bit\envs\dev\lib\site-packages\pandas\core\base.py"", line 307, in _aggregate
    return self._try_aggregate_string_function(arg, *args, **kwargs), None
  File ""C:\Users\deepnoid-workstation\Anaconda3_64bit\envs\dev\lib\site-packages\pandas\core\base.py"", line 263, in _try_aggregate_string_function
    return f(*args, **kwargs)
  File ""C:\Users\deepnoid-workstation\Anaconda3_64bit\envs\dev\lib\site-packages\pandas\core\groupby\groupby.py"", line 1552, in min
    numeric_only=numeric_only, min_count=min_count, alias=""min"", npfunc=np.min
  File ""C:\Users\deepnoid-workstation\Anaconda3_64bit\envs\dev\lib\site-packages\pandas\core\groupby\groupby.py"", line 1000, in _agg_general
    how=alias, alt=npfunc, numeric_only=numeric_only, min_count=min_count,
  File ""C:\Users\deepnoid-workstation\Anaconda3_64bit\envs\dev\lib\site-packages\pandas\core\groupby\generic.py"", line 1022, in _cython_agg_general
    how, alt=alt, numeric_only=numeric_only, min_count=min_count
  File ""C:\Users\deepnoid-workstation\Anaconda3_64bit\envs\dev\lib\site-packages\pandas\core\groupby\generic.py"", line 1135, in _cython_agg_blocks
    assert len(locs) == result.shape[1]
AssertionError

Process finished with exit code 1
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

CentOS
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
Traceback (most recent call last):
  File ""pandaserror.py"", line 24, in <module>
    print(class_group.agg('min'))
  File ""/conda/anaconda3/lib/python3.8/site-packages/pandas/core/groupby/generic.py"", line 928, in aggregate
    result, how = self._aggregate(func, *args, **kwargs)
  File ""/conda/anaconda3/lib/python3.8/site-packages/pandas/core/base.py"", line 311, in _aggregate
    return self._try_aggregate_string_function(arg, *args, **kwargs), None
  File ""/conda/anaconda3/lib/python3.8/site-packages/pandas/core/base.py"", line 267, in _try_aggregate_string_function
    return f(*args, **kwargs)
  File ""/conda/anaconda3/lib/python3.8/site-packages/pandas/core/groupby/groupby.py"", line 1372, in f
    return self._cython_agg_general(alias, alt=npfunc, **kwargs)
  File ""/conda/anaconda3/lib/python3.8/site-packages/pandas/core/groupby/generic.py"", line 993, in _cython_agg_general
    agg_blocks, agg_items = self._cython_agg_blocks(
  File ""/conda/anaconda3/lib/python3.8/site-packages/pandas/core/groupby/generic.py"", line 1100, in _cython_agg_blocks
    assert len(locs) == result.shape[1]
AssertionError
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

* Windows
INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.6.10.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.4.0.post20200518
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.49.1
None


* CentOS
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1062.12.1.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : ko_KR.UTF-8
LOCALE           : ko_KR.UTF-8

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : None
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.2
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.9
numba            : 0.50.1
None


</details>
"
783533318,39110,"BUG: `get_dummies` works on a single integer value column, but fails silently when applied to multiple columns. ",wcneill,closed,2021-01-11T16:50:00Z,2021-01-22T12:49:10Z,"- [x] I could not find this issue under open issues when searching: ""bug get_dummies""

- [X] I have confirmed this bug exists on the latest version of pandas (1.2.0)

---

#### Code Sample, a copy-pastable example

```python
# A sample dataframe. Last column has integer values.
df = pd.DataFrame({'A': ['a', 'b', 'c'], 'B': ['1', '2', '3'], 'C': [1, 2, 3]})

# Encoding the integer values fine. 
df2 = pd.get_dummies(df['C'], prefix='C')

# Encoding the string values works fine.:
df3 = pd.get_dummies(df[['A', 'B']], prefix=['a', 'b'])

# Encoding both this fails silently:
df4 = pd.get_dummies(df)

# Supplying prefixes makes the issue apparent:
df5 = pd.get_dummies(df, prefix=['A', 'B', 'C'])
```

#### Problem description
Pandas `get_dummies` will one-hot encode a single integer valued categorical column. **However, it silently ignores any further columns containing numeric categories.** 

The last line of the minimal working example (with prefix list argument) results in:
```
ValueError: Length of 'prefix' (3) did not match the length of the columns being encoded (2).
```

#### Why the current behavior a problem and why the expected output is a better solution:
The expected output matches what the documentation advertises. 

#### Expected Output
All columns passed to `get_dummies` should be considered categorical and encoded, including those containing integers.

**or**

No integer columns should be allowed. Produce a warning/error/update the docs. 

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.2.0
numpy            : 1.19.2
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.1.2.post20210110
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
782781118,39087,BUG:,quanbingDG,closed,2021-01-10T09:42:48Z,2021-01-22T14:23:45Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here

```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
791714616,39333,TYP: @final for more Index methods,jbrockmendel,closed,2021-01-22T05:38:55Z,2021-01-22T15:15:43Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
791644989,39330,REF: de-duplicate algos.try_sort calls,jbrockmendel,closed,2021-01-22T02:28:12Z,2021-01-22T15:17:23Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
787584424,39217,REF: Split up parsers into smaller files,phofl,closed,2021-01-17T00:38:48Z,2021-01-22T18:44:52Z,"- [x] closes #38930
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them


Refactored them into 4 files, similar to described in the issue. If this is accepted I will try to clean this up a bit more through unifying the calls a bit
"
792067821,39339,BUG / Question: Cant loop over Full/Complete DataFrame Index,helloworldnoob,closed,2021-01-22T14:53:46Z,2021-01-22T19:02:07Z,"I have been looping over dataframes from selected indexes. Works fine for the most part, however, when trying to loop over the entire dataframe (rather than using .query() and then saving the queried index), I get KeyError: 4393 


Why I think this is a bug is because I can make it work by doing:
```
dfDbi = dfD.head(4390)  # dataframe has 4393 keys, tried .copy() but no luck with the for loop
dfDbi = dfDbi.query('OC<0').index #saving index to later loop through

# pseudocode
for x in dfDbi:
    if dfD.loc[x] and dfD.loc[x+1] condition true:
        (do something)
```

The above works. Changing dfD.head to (4391) or higher does not. 
What I want to do is:

```
dfDi = dfD.index 

for x in dfDi:
    if dfD[x] and dfD.loc[x+1] condition true:
        (do something)
```

If I have butchered my explanation, please let me know so I can try to further explain."
142239573,12679,"ENH: guarantee pandas.Series.value_counts ""sort=False"" to be original ordering",rchurt,closed,2016-03-21T02:38:57Z,2021-01-22T21:42:34Z,"Hello,

I'm trying to make a new DataFrame that contains the value counts of a column of an existing DataFrame ([spreadsheet.xlsx](https://github.com/pydata/pandas/files/181629/spreadsheet.xlsx)), but I want the rows in the new DataFrame to be in the same order as the old one.

When I do:

``` python
import pandas as pd
df = pd.read_excel('./spreadsheet.xlsx')
print(df[0].value_counts(sort=False))
```

I get the DataFrame:

``` python
h4  8
ct1 6
f2  2
s1  2
EST2    2
f5  2
E4  8
h2  8
hd2 7
f3  2
ART1    2
s2  2
f1  2
h3  8
EST1    2
s3  2
E6  8
ART2    2
DGT2    2
ct2 6
s4  2
ct3 6
f4  2
DGT1    2
s5  2
```

When what I really want is:

``` python
h2  8
h3  8
h4  8
hd2 7
E4  8
E6  8
ct1 6
.
.
.
```

...because that's the order in which the values occur in the original DataFrame.

I can't tell how it's sorting them, but it is somehow. Is this the expected behavior?

Thanks

``` python
Installed versions:
commit: None
python: 3.5.1.final.0
python-bits: 64
OS: Darwin
OS-release: 15.3.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: None

pandas: 0.18.0
nose: 1.3.7
pip: 8.1.1
setuptools: 20.3
Cython: 0.23.4
numpy: 1.10.4
scipy: 0.17.0
statsmodels: None
xarray: None
IPython: 4.1.2
sphinx: 1.3.5
patsy: 0.4.0
dateutil: 2.5.0
pytz: 2016.1
blosc: None
bottleneck: 1.0.0
tables: 3.2.2
numexpr: 2.4.6
matplotlib: 1.5.1
openpyxl: 2.3.2
xlrd: 0.9.4
xlwt: 1.0.0
xlsxwriter: 0.8.4
lxml: 3.6.0
bs4: 4.4.1
html5lib: None
httplib2: 0.9.2
apiclient: 1.5.0
sqlalchemy: 1.0.12
pymysql: None
psycopg2: None
jinja2: 2.8
boto: 2.39.0
```
"
109629267,11227,COMPAT: different orderings in value_counts on 32-bit platforms,jreback,closed,2015-10-03T15:47:50Z,2021-01-22T21:42:34Z,"This occurs on 32-bit linux, a slightly different ordering is returned from the hashtable. Only guess is that it is because the indexing is `Py_ssize_t` and this is hashed and has differing values. So the test should be slightly different for those platforms.

see test skipping here: https://github.com/pydata/pandas/commit/d6c7a3a884232948a198ba4a9475325cacb5ca98

Not a big deal, but here's the question. Should we guarantee these types of orderings, IOW, use a `int64` instead of `Py_ssize_t` for indexing (on all platforms)?
"
786108545,39172,BUG: SeriesGroupBy.value_counts() raises on an empty Series,FilippoBovo,closed,2021-01-14T16:00:03Z,2021-01-22T23:26:52Z,"- [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

Using the `value_counts` method after grouping an empty DataFrame and selecting a column raises an error.

Example:

```python
>>> pd.DataFrame(columns=[""A"", ""B""]).groupby(""A"")[""B""].value_counts()
```

Error:

```python-traceback
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""/Users/username/.virtualenvs/my_project/lib/python3.8/site-packages/pandas/core/groupby/generic.py"", line 736, in value_counts
    codes = [rep(level_codes) for level_codes in codes] + [llab(lab, inc)]
  File ""/Users/username/.virtualenvs/my_project/lib/python3.8/site-packages/pandas/core/groupby/generic.py"", line 705, in <lambda>
    llab = lambda lab, inc: lab[inc]
IndexError: boolean index did not match indexed array along dimension 0; dimension is 0 but corresponding boolean dimension is 1
```

Instead, I would expect that `value_counts` returns an empty series, like when using the `max` method, like the following,

```python
>>> pd.DataFrame(columns=[""A"", ""B""]).groupby(""A"")[""B""].max()
```

which gives this result:

```
Series([], Name: B, dtype: object)
```

If it is normal to have an error when using `value_counts` for an empty `SeriesGroupBy`, why is it so?

Thank you.
"
791419837,39326,BUG: SeriesGroupBy.value_counts() raising error on an empty Series,AnnaDaglis,closed,2021-01-21T19:09:19Z,2021-01-22T23:26:57Z,"- [x] closes #39172
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
791645607,39331,REF: Fix PeriodIndex._outer_indexer -> share remaining set methods,jbrockmendel,closed,2021-01-22T02:30:01Z,2021-01-22T23:35:53Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
791517768,39327,BUG: pandas.DataFrame.apply gives different results between pandas versions 1.1.4 and 1.2.0,brandonlind,closed,2021-01-21T21:48:09Z,2021-01-23T00:04:05Z,"- [ x] I have checked that this issue has not already been reported.

- [ x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
df2 =pd.DataFrame(dict(one=[str((i/10)*100)+""%"" for i in range(5)],
                       two=[str((i/20)*100)+""%"" for i in range(10,15)],
                       three=[str((i/30)*100)+""%"" for i in range(20,25)]))

# the following two lines give the same result as the other in 1.1.4 but different results in 1.2.0
df2.apply(lambda series: series.str.replace(""%"","""").astype(float)/100, axis=1)  
df2.apply(lambda series: series.str.replace(""%"","""").astype(float)/100, axis=0)
```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

The two versions of pandas (1.1.4 and 1.2.0) give different results when `axis=1`

#### Expected Output

They should be the same when `axis=1`

#### Output of ``pd.show_versions()``

<details>

even though my ""pandas 1.2.0 environment"" is python 3.8.5 while my ""pandas 1.1.4 environment"" is python 3.7.9, the problem is resolved if I revert the python 3.8.5 environment to pandas 1.1.4.

pandas 1.1.4 environment
```
INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.7.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1160.11.1.el7.x86_64
Version          : #1 SMP Mon Nov 30 13:05:31 EST 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.4
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.0.0.post20201207
Cython           : None
pytest           : 6.2.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.6.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.4
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
```

pandas 1.2.0 environment
```
INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1160.11.1.el7.x86_64
Version          : #1 SMP Mon Nov 30 13:05:31 EST 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0
numpy            : 1.19.4
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.0.0.post20201207
Cython           : None
pytest           : 6.2.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.6.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.4
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
```

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
784575912,39132,PERF: cythonize kendall correlation,lithomas1,closed,2021-01-12T20:39:16Z,2021-01-23T20:26:53Z,"- [x] closes #28329 
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

ASV's. 

       before           after         ratio
     [a6bc6ecc]       [1eb86441]
     <perf-cythonize-kendall~2^2>       <perf-cythonize-kendall>
-             81M            57.6M     0.71  stat_ops.Correlation.peakmem_corr_wide('kendall')
-      1.41±0.02s          362±7ms     0.26  stat_ops.Correlation.time_corr_wide_nans('kendall')
-      1.57±0.04s          398±4ms     0.25  stat_ops.Correlation.time_corr_wide('kendall')
-        34.6±4ms      8.75±0.06ms     0.25  stat_ops.Correlation.time_corr('kendall')

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE INCREASED."
792691964,39366,CI: parallelize the documentation build,afeld,closed,2021-01-24T01:00:48Z,2021-01-24T01:54:32Z,"Testing if this makes a difference in the build time, relative to `auto` (https://github.com/pandas-dev/pandas/pull/39364). 

- [ ] ~~closes #xxxx~~
- [ ] tests ~~added /~~ passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] ~~whatsnew entry~~
"
754775262,38219,Update Dockerfile to use conda-forge compilers,xhochy,closed,2020-12-01T22:29:33Z,2021-01-24T05:32:54Z,"Additionally I switched to the miniforge3 image as this bases its installation on conda-forge
and I also have used `mamba` to install the environment as this massively saves time.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
753049504,38163,DOC: Add examples for filling missing data,chrystalchan-academic,closed,2020-11-29T23:59:38Z,2021-01-24T11:36:00Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
789971944,39292,Default behaviour for return_type = None corrected.,rajat315315,closed,2021-01-20T13:18:48Z,2021-01-24T12:32:53Z,"- [x] closes #35518
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
792635805,39354,TST: remove incorrect xfails,jbrockmendel,closed,2021-01-23T19:44:23Z,2021-01-24T21:22:28Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
792675438,39361,TST: fix fillna test for PandasArray,jbrockmendel,closed,2021-01-23T23:35:28Z,2021-01-24T21:25:32Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
792608783,39350,"TST: fix xfailed tests for IntegerArray, FloatingArray",jbrockmendel,closed,2021-01-23T17:14:52Z,2021-01-24T21:44:43Z,
771798731,38604,BUG: surprising non-error when assigning a multi-column DataFrame to a single column,iamlemec,closed,2020-12-21T03:07:03Z,2021-01-24T22:01:02Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

Assigning a multi-column DataFrame to a single column does not raise an error and simply assigns the first column of the DataFrame to the column. Below is an example:

```python
df1 = pd.DataFrame({'a': [0, 1, 2, 3], 'b': [4, 5, 6, 7]}) 
df2 = pd.DataFrame({'c': [8, 9, 10, 11], 'd': [12, 13, 14, 15]}) 
df1['b'] = df2
```

This seems like one of the instances where not raising an error might ease some short-term pain but could mask underlying issues with a user's code. This behavior seems to have been around for a while. It occurs in master and as far back as 1.0.3."
755740791,38254,BUG: incorrect EA casting in groubpy.agg,jbrockmendel,closed,2020-12-03T01:38:03Z,2021-01-24T22:02:34Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The existing code is implicitly assuming that _from_sequence is strict, so that maybe_cast_to_extension_array will only return an EA when correct.  Since that assumption is untrue, the current code will return incorrect results, as in the test this adds.

If we just removed L729-L741 in groupby.ops, we would have 12ish test failures.  A few of those would be for float64 result failing to cast back to Float64, and the rest would be for `ndarray[Decimal objects]` failing to cast back to DecimalArray.  Until `_from_sequence` is reliable, I would rather remove these few lines and return correct-but-suboptimally-casted results than have these kludges.

cc @jorisvandenbossche "
792680207,39362,BUG: incorrect casting ints to Period in GroupBy.agg,jbrockmendel,closed,2021-01-23T23:44:17Z,2021-01-24T22:03:13Z,"- [x] closes #38254
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

This is pretty much throwing in the towel on #38254."
792708710,39370,TST: tighten PandasArray tests,jbrockmendel,closed,2021-01-24T02:58:38Z,2021-01-24T22:15:25Z,xfail instead of skip where appropriate
792429604,39344,TST: fix incorrect extension test,jbrockmendel,closed,2021-01-23T03:32:16Z,2021-01-24T23:31:00Z,
791024971,39317,BUG+DOC: Recent Styler Enhancements,attack68,closed,2021-01-21T12:26:29Z,2021-01-25T06:34:17Z,"**DOC**: this edits the `style.ipynb` file to include examples of the 3 recent enhancements to `Styler`:

 - Tooltips
 - External CSS classes to datacells
 - Row and Column styling via table styles

It also addresses 3 minor bugs that I detected when wirting the documentation:

**BUG1** (lines 696-703): `set_td_classes` only attached classes to 'diagonal elements' of the DataFrame due a misaligned comprehension loop:

```
{
r: {
   c: [str(classes.iloc[r, c])] 
   for c, cn in enumerate(classes.columns) if not mask.iloc[r, c]
   }
for r, rn in enumerate(classes.index)
}
```
replaces:
```
{
r: {
    c: [str(classes.iloc[r, c])]
    }
for r, rn in enumerate(classes.index)
for c, cn in enumerate(classes.columns)
if not mask.iloc[r, c]
}
```

**BUG2** (lines 513 to 537): external css class names from the set_td_classes were appended to the HTML id tag, as well as the HTML class tag, due to a premature variable update relating to the `row_dict`. The solution was to re-order so that 'id' tags were generated first and then 'class' was updated with information from the new methods.

**BUG3** (lines 1920-1925): Tooltips were generated for the wrong indexes/columns, since the null rows/columns were dropped and then the interger locations of rows/columns was mapped. The solution was to avoid dropping null rows/columns.


"
792873533,39379,BUG: pd.read_clipboard() not working in WSL2,pandichef,closed,2021-01-24T19:27:27Z,2021-01-25T10:57:03Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
#### Code Sample, a copy-pastable example
```python
pd.read_clipboard()
```

#### Problem description
I recently started using WSL2 on Windows 10 Pro / Ubuntu 20.04 / Python 3.8.5.  In general, It works incredibly well today (unlike in 2018).  Unfortunately, pd.read_clipboard() and df.to_clipboard() don't work.  I believe the problem is pandas because I got the desired functionality to work using pyperclip directly.

#### Error Message
>>> pd.read_clipboard()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/mickeymouse/.local/lib/python3.8/site-packages/pandas/io/clipboards.py"", line 38, in read_clipboard
    text = clipboard_get()
  File ""/home/mickeymouse/.local/lib/python3.8/site-packages/pandas/io/clipboard/__init__.py"", line 649, in lazy_load_stub_paste
    return paste()
  File ""/home/mickeymouse/.local/lib/python3.8/site-packages/pandas/io/clipboard/__init__.py"", line 287, in __call__
    raise PyperclipException(EXCEPT_MSG)
pandas.io.clipboard.PyperclipException:
    Pyperclip could not find a copy/paste mechanism for your system.
    For more information, please visit
    https://pyperclip.readthedocs.io/en/latest/introduction.html#not-implemented-error"
793138578,39391,⬆️ UPGRADE: Autoupdate pre-commit config,github-actions[bot],closed,2021-01-25T08:00:24Z,2021-01-25T14:34:15Z,"<!-- START pr-commits -->
<!-- END pr-commits -->

## Base PullRequest

default branch (https://github.com/pandas-dev/pandas/tree/master)

## Command results
<details>
<summary>Details: </summary>

<details>
<summary><em>add path</em></summary>

```Shell
/home/runner/work/_actions/technote-space/create-pr-action/v2/node_modules/npm-check-updates/bin
```



</details>
<details>
<summary><em>pip install pre-commit</em></summary>

```Shell
Collecting pre-commit
  Downloading pre_commit-2.9.3-py2.py3-none-any.whl (184 kB)
Collecting cfgv>=2.0.0
  Using cached cfgv-3.2.0-py2.py3-none-any.whl (7.3 kB)
Collecting identify>=1.0.0
  Downloading identify-1.5.13-py2.py3-none-any.whl (97 kB)
Collecting nodeenv>=0.11.1
  Using cached nodeenv-1.5.0-py2.py3-none-any.whl (21 kB)
Collecting pyyaml>=5.1
  Downloading PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)
Collecting toml
  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)
Collecting virtualenv>=20.0.8
  Downloading virtualenv-20.4.0-py2.py3-none-any.whl (5.7 MB)
Collecting appdirs<2,>=1.4.3
  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)
Collecting distlib<1,>=0.3.1
  Using cached distlib-0.3.1-py2.py3-none-any.whl (335 kB)
Collecting filelock<4,>=3.0.0
  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)
Collecting six<2,>=1.9.0
  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)
Installing collected packages: six, filelock, distlib, appdirs, virtualenv, toml, pyyaml, nodeenv, identify, cfgv, pre-commit
Successfully installed appdirs-1.4.4 cfgv-3.2.0 distlib-0.3.1 filelock-3.0.12 identify-1.5.13 nodeenv-1.5.0 pre-commit-2.9.3 pyyaml-5.4.1 six-1.15.0 toml-0.10.2 virtualenv-20.4.0
```

### stderr:

```Shell
WARNING: You are using pip version 20.3.1; however, version 21.0 is available.
You should consider upgrading via the '/opt/hostedtoolcache/Python/3.9.1/x64/bin/python -m pip install --upgrade pip' command.
```

</details>
<details>
<summary><em>pre-commit autoupdate || (exit 0);</em></summary>

```Shell
Updating https://github.com/python/black ... already up to date.
Updating https://gitlab.com/pycqa/flake8 ... already up to date.
Updating https://github.com/PyCQA/isort ... [INFO] Initializing environment for https://github.com/PyCQA/isort.
already up to date.
Updating https://github.com/asottile/pyupgrade ... [INFO] Initializing environment for https://github.com/asottile/pyupgrade.
already up to date.
Updating https://github.com/pre-commit/pygrep-hooks ... already up to date.
Updating https://github.com/asottile/yesqa ... already up to date.
Updating https://github.com/pre-commit/pre-commit-hooks ... [INFO] Initializing environment for https://github.com/pre-commit/pre-commit-hooks.
already up to date.
Updating https://github.com/codespell-project/codespell ... [INFO] Initializing environment for https://github.com/codespell-project/codespell.
already up to date.
Updating https://github.com/MarcoGorelli/no-string-hints ... [INFO] Initializing environment for https://github.com/MarcoGorelli/no-string-hints.
updating v0.1.5 -> v0.1.6.
```



</details>
<details>
<summary><em>pre-commit run -a || (exit 0);</em></summary>

```Shell
[INFO] Installing environment for https://github.com/python/black.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://gitlab.com/pycqa/flake8.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://gitlab.com/pycqa/flake8.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/PyCQA/isort.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/asottile/pyupgrade.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for local.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for local.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for local.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/asottile/yesqa.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/pre-commit/pre-commit-hooks.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/codespell-project/codespell.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/MarcoGorelli/no-string-hints.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
black..................................................................................................Passed
flake8.................................................................................................Passed
flake8 (cython)........................................................................................Passed
flake8 (cython template)...............................................................................Passed
isort..................................................................................................Passed
pyupgrade..............................................................................................Passed
rst ``code`` is two backticks..........................................................................Passed
rst directives end with two colons.....................................................................Passed
rst ``inline code`` next to normal text................................................................Passed
Generate pip dependency from conda.....................................................................Passed
flake8-rst.............................................................................................Passed
Check for non-standard imports.........................................................................Passed
Check for non-standard numpy.random-related imports excluding pandas/_testing.py.......................Passed
Check for non-standard imports in test suite...........................................................Passed
Check for incorrect code block or IPython directives...................................................Passed
Check for use of not concatenated strings..............................................................Passed
Check for strings with wrong placed spaces.............................................................Passed
Check for import of private attributes across modules..................................................Passed
Check for use of private functions across modules......................................................Passed
Check for use of bare pytest raises....................................................................Passed
Check for inconsistent use of pandas namespace in tests................................................Passed
Check for use of Union[Series, DataFrame] instead of FrameOrSeriesUnion alias..........................Passed
Check for use of foo.__class__ instead of type(foo)....................................................Passed
Check for outdated annotation syntax and missing error codes...........................................Passed
Check for use of np.bool instead of np.bool_...........................................................Passed
Check code for instances of os.remove..................................................................Passed
Check code for instances of pd.api.types...............................................................Passed
Strip unnecessary `# noqa`s............................................................................Passed
Fix End of Files.......................................................................................Passed
Trim Trailing Whitespace...............................................................................Passed
codespell..............................................................................................Passed
no-string-hints........................................................................................Passed
```



</details>

</details>

## Changed files
<details>
<summary>Changed file: </summary>

- .pre-commit-config.yaml

</details>

<hr>

[:octocat: Repo](https://github.com/technote-space/create-pr-action) | [:memo: Issues](https://github.com/technote-space/create-pr-action/issues) | [:department_store: Marketplace](https://github.com/marketplace/actions/create-pr-action)"
792957513,39385,REF: move PandasDtype to dtypes.dtypes,jbrockmendel,closed,2021-01-25T01:34:14Z,2021-01-25T15:08:25Z,This will allow us to import it in e.g. core.construction without a circular import
792942173,39384,CI: fix PandasArray test,jbrockmendel,closed,2021-01-25T00:39:21Z,2021-01-25T15:08:46Z,
790029906,39296,BUG: of apply function of DataFrame reported in issues #39166,jmishra01,closed,2021-01-20T14:31:57Z,2021-01-25T16:27:28Z,"- [x] closes #39166 
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
600966586,33589,"BUG: KeyError on slicing with datetime, when datetime contains microseconds ",PaulVoigt,closed,2020-04-16T11:26:41Z,2021-01-25T16:33:13Z,"#### Problem description

Slicing a DataFrame with a datetime index by datetime results in a KeyError when the string contains microseconds. 

 `df['2017-10-25T16:25:04.252':'2017-10-25T16:50:05.237']`

```
During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
<ipython-input-20-482a9c5e8c58> in <module>
----> 1 df['2017-10-25T16:25:04.252':'2017-10-25T16:50:05.237']

/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key)
   2777 
   2778         # Do we have a slicer (on rows)?
-> 2779         indexer = convert_to_index_sliceable(self, key)
   2780         if indexer is not None:
   2781             # either we have a slice or we have a string that can be converted

/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py in convert_to_index_sliceable(obj, key)
   2265     idx = obj.index
   2266     if isinstance(key, slice):
-> 2267         return idx._convert_slice_indexer(key, kind=""getitem"")
   2268 
   2269     elif isinstance(key, str):

/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py in _convert_slice_indexer(self, key, kind)
   2960             indexer = key
   2961         else:
-> 2962             indexer = self.slice_indexer(start, stop, step, kind=kind)
   2963 
   2964         return indexer

/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/datetimes.py in slice_indexer(self, start, end, step, kind)
    823                 mask = True
    824                 if start is not None:
--> 825                     start_casted = self._maybe_cast_slice_bound(start, ""left"", kind)
    826                     mask = start_casted <= self
    827 

/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/datetimes.py in _maybe_cast_slice_bound(self, label, side, kind)
    761             freq = getattr(self, ""freqstr"", getattr(self, ""inferred_freq"", None))
    762             _, parsed, reso = parsing.parse_time_string(label, freq)
--> 763             lower, upper = self._parsed_string_to_bounds(reso, parsed)
    764             # lower, upper form the half-open interval:
    765             #   [parsed, parsed + 1 freq)

/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/datetimes.py in _parsed_string_to_bounds(self, reso, parsed)
    517         }
    518         if reso not in valid_resos:
--> 519             raise KeyError
    520         if reso == ""year"":
    521             start = Timestamp(parsed.year, 1, 1)

KeyError: 
```


I think that the function .parse_time_string in pandas._libs.tslibs.parsing.pyx is not supporting microseconds and returns `None`

#### Output of ``pd.show_versions()``

pandas           : 1.0.3



"
792869863,39378,BUG: Slicing DatetimeIndex with strings containing microseconds raising KeyError,phofl,closed,2021-01-24T19:09:30Z,2021-01-25T16:38:42Z,"- [x] closes #33589
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

Is there a reason not supporting milliseconds resolution when supporting microseconds?"
783486912,39109,DOC: NDFrame fillna method add use case,aniaan,closed,2021-01-11T15:49:45Z,2021-01-25T16:40:42Z,"- [x] closes #38917 
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

cc @MarcoGorelli"
792907175,39381,TST: tighten Decimal tests,jbrockmendel,closed,2021-01-24T21:36:20Z,2021-01-25T16:49:34Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
791702499,39332,PERF: DatetimeIndex.get_indexer with mismatched tz,jbrockmendel,closed,2021-01-22T05:08:55Z,2021-01-25T16:50:03Z,"Avoid casting to object dtype.

```
In [2]: dti = pd.date_range(""2016-01-01"", periods=10000, tz=""US/Pacific"")
In [3]: dti2 = dti.tz_convert(""UTC"")

In [4]: %timeit dti.get_indexer(dti2)
127 ms ± 4.69 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)  # <-- master
352 µs ± 10.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  # <-- PR

```"
792971548,39387,ENH: PandasArray.value_counts,jbrockmendel,closed,2021-01-25T02:22:58Z,2021-01-25T17:03:48Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
792994102,39388,PERF: Rolling.cov/corr,mroeschke,closed,2021-01-25T03:32:10Z,2021-01-25T17:50:21Z,"- [ ] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry


Will attempt to improve the performance of `groupby.rolling` next.

```
       before           after         ratio
     [37b5800a]       [ee9a56c1]
     <master>         <ref/rolling_cov_corr>
-        10.4±2ms       8.90±0.2ms     0.86  rolling.Pairwise.time_pairwise(10, 'cov', True)
-        10.2±1ms       8.52±0.1ms     0.83  rolling.Pairwise.time_pairwise(None, 'cov', True)
-      2.48±0.1ms      1.07±0.01ms     0.43  rolling.Pairwise.time_pairwise(1000, 'cov', False)
-      2.55±0.2ms      1.08±0.02ms     0.42  rolling.Pairwise.time_pairwise(10, 'cov', False)
-     2.33±0.04ms          918±6μs     0.39  rolling.Pairwise.time_pairwise(None, 'cov', False)
-      4.02±0.6ms      1.46±0.02ms     0.36  rolling.Pairwise.time_pairwise(10, 'corr', False)
-      4.08±0.4ms       1.42±0.1ms     0.35  rolling.Pairwise.time_pairwise(1000, 'corr', False)
-     3.69±0.07ms      1.17±0.03ms     0.32  rolling.Pairwise.time_pairwise(None, 'corr', False)

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE INCREASED.
```

Added a `ddof` argument to `corr` which was implicitly accepted (and used) by `kwargs` before; making it an explicit kwargs in this PR. Will document it after https://github.com/pandas-dev/pandas/pull/39219 is in."
792285704,39341,BUG: DataFrame.__setitem__ not raising ValueError when rhs is df and has wrong number of columns,phofl,closed,2021-01-22T20:29:19Z,2021-01-25T19:01:19Z,"- [x] closes #38604
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

Have to check how many columns key references before raising (duplicates or MultiIndex may reference more than one column)"
793471137,39398,BUG: `DateTimeIndex + DateTimeIndex.freq` giving incorrect result if timezone is set.,rwijtvliet,closed,2021-01-25T15:18:39Z,2021-01-25T19:02:58Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
#### Code Sample, a copy-pastable example

```python
import pandas as pd
i = pd.date_range('2020-03-27', freq='D', periods=4, tz='Europe/Berlin')
i2 = i + i.freq
i2.freq = 'D' # ValueError: Inferred frequency None from passed values does not conform to passed frequency D
```

#### Problem description

`i` is as expected.
```
DatetimeIndex(['2020-03-27 00:00:00+01:00', '2020-03-28 00:00:00+01:00',
               '2020-03-29 00:00:00+01:00', '2020-03-30 00:00:00+02:00'],
              dtype='datetime64[ns, Europe/Berlin]', freq='D') #good
```
but `i2` has two issues: 
```
DatetimeIndex(['2020-03-29 00:00:00+01:00', '2020-03-30 01:00:00+02:00',
               '2020-03-31 00:00:00+02:00', '2020-04-01 00:00:00+02:00'],
              dtype='datetime64[ns, Europe/Berlin]', freq='D') 
#issue: 2020-03-30 timestamp not at midnight
#issue: i2 has attribute `freq='D'`, but pd.infer_freq returns None
```

#### Expected Output

`i2` to be 
```
DatetimeIndex(['2020-03-29 00:00:00+01:00', '2020-03-30 00:00:00+02:00',
               '2020-03-31 00:00:00+02:00', '2020-04-01 00:00:00+02:00'],
              dtype='datetime64[ns, Europe/Berlin]', freq='D') 
```
(`2020-03-30 00:00:00+02:00` is only change),
with `i2.freq` not returning `None`

#### Related
May be related to [#38243](https://github.com/pandas-dev/pandas/issues/38243)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en
LOCALE           : de_DE.cp1252

pandas           : 1.2.0
numpy            : 1.19.5
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.1.2.post20210112
Cython           : None
pytest           : 6.2.1
hypothesis       : None
sphinx           : 3.4.3
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.6.0
sqlalchemy       : 1.3.22
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
</details>
"
680227798,35769,BUG: Raise ValueError instead of bare Exception in sanitize_array,micahjsmith,closed,2020-08-17T13:16:03Z,2021-01-25T21:15:38Z,"- [x] closes #35744  
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
792656869,39357,"BUG: Series(list_of_tuples, dtype=PandasDtype(object))",jbrockmendel,closed,2021-01-23T21:51:22Z,2021-01-25T23:35:21Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
787710345,39227,BUG: Series.resample fails on NaT PeriodIndex,rhshadrach,closed,2021-01-17T13:51:28Z,2021-01-26T00:01:50Z,"Failure is only on Series and only when the index is entirely `NaT`.

```
freq = ""D""

df = DataFrame([1, 2], index=PeriodIndex([NaT, NaT], freq=freq))
print(df.resample(freq).sum())

ser = Series([1, 2], index=PeriodIndex([NaT, NaT], freq=freq))
print(ser.resample(freq).sum())
```

Second raises, first returns the expected

```
Empty DataFrame
Columns: []
Index: []
```"
792958306,39386,TST: remove test_error,jbrockmendel,closed,2021-01-25T01:37:14Z,2021-01-26T00:16:54Z,It makes no sense to me that we'd test unwanted behavior by default and then override it for working arrays.
793808965,39408,REF: implement Index._find_common_type_compat,jbrockmendel,closed,2021-01-25T23:48:33Z,2021-01-26T03:04:30Z,Before long im going to need to flesh this out to handle complex dtype.
748197780,37998,DOC: add a link to new styler method,attack68,closed,2020-11-22T08:44:20Z,2021-01-26T08:08:42Z,"Adds a documentation link for the new `Styler` method from #36159
"
675002862,35607,ENH: Styler column and row styles,attack68,closed,2020-08-07T13:05:09Z,2021-01-26T08:10:55Z,"- [x] closes #35605 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
793315354,39395,BUG: `Styler.apply` and `applymap` duplicate CSS on re-render.,attack68,closed,2021-01-25T11:51:10Z,2021-01-26T14:15:14Z,"
#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame((np.random.rand(1, 1)*100))
s = df.style\
    .applymap(lambda x: 'background-color: black;')\
    .apply(lambda x: pd.DataFrame([['color: red;']]), axis=None)
print(s.render())
print(s.ctx)
print(s.render())
print(s.ctx)

...

<style  type=""text/css"" >
#T_fda50_row0_col0{
            background-color:  black;
            color:  red;
        }</style>

defaultdict(<class 'list'>, {(0, 0): ['background-color: black', 'color: red']})

<style  type=""text/css"" >
#T_fda50_row0_col0{
            background-color:  black;
            color:  red;
            background-color:  black;
            color:  red;
        }</style>

defaultdict(<class 'list'>, {(0, 0): ['background-color: black', 'color: red', 'background-color: black', 'color: red']})

```

#### Problem description

The private `_todo` list is not cleared after initial execution so repeats the execution on second render.
There does not seem to be any reason any reason to maintain the `_todo` list after it has been done and the `ctx` object has been updated with necessary css-styles.

#### Expected Output

Same on each render.

"
787719218,39229,BUG: Series.resample fails on NaT index,rhshadrach,closed,2021-01-17T14:37:26Z,2021-01-26T15:00:28Z,"- [x] closes #39227
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

This will also resolve the core issue of https://github.com/pandas-dev/pandas/pull/34998#discussion_r556463043. cc @jorisvandenbossche 

The added test:

```
    s = series.copy()
    s.index = PeriodIndex([NaT] * len(s), freq=freq)
    result = getattr(s.resample(freq), resample_method)()
```

currently fails for `freq=""M""`, where upsample is used instead of downsample leading to failure since the index is not unique. I can't tell if this is a bug or an operation that should raise. It also fails for `DataFrame`, so I think is unrelated to the issue here.

cc @jbrockmendel "
793323676,39396,BUG: re-render CSS with `styler.apply` and `applymap` non-cleared ` _todo`,attack68,closed,2021-01-25T12:03:24Z,2021-01-26T17:01:17Z,"- [ x] closes #39395
- [ x] tests added / passed
- [ x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
780466642,38999,BUG:,bharat-ics,closed,2021-01-06T10:58:25Z,2021-01-26T17:38:09Z,"- [Checked ] I have checked that this issue has not already been reported.

- [ Checked] I have confirmed this bug exists on the latest version of pandas.

- [Not Checked ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
df.to_csv(""test.csv"")

ImportError: cannot import name 'CompressionOptions' from 'pandas._typing
```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

Breaking error on printing csvs right after upgrading to 1.1.5

#### Expected Output
CSV output of dataframe


"
450987680,26599,Compression keyword for Stata and others?,ozak,closed,2019-05-31T21:26:46Z,2021-01-26T18:17:49Z,"Hi,

I was trying to open zipped Stata files and thought one could do it as in ``read_csv``using  the ``compression='zip'`` keyword option. Is this not implemented? Saw #15644 and follow ups, but at the time it seems the discussion was about output not input files. In many cases providers give Stata files in zip format, which e.g. in my case may mean 1000's of different zipped archives containing Stata + other files. #12103 seems to have added the functionality more generally, but at least in ``pd.__version__=0.23.4`` it still complains when passing the ``compression`` keyword. I do not see the option in ``pandas/io/stata.py`` although in ``common.py`` the option seems to be there. Any pointers? I am happy to contribute if I can figure out where and how this is implemented for other formats."
793568854,39399,TYP upgrade and pin mypy to 0.800,MarcoGorelli,closed,2021-01-25T17:18:04Z,2021-01-26T20:46:42Z,"mypy 0.800 came out 3 days ago https://mypy-lang.blogspot.com/2021/01/mypy-0800-released.html , so it would be good to try upgrading"
792612760,39351,BUG: read_hdf bad filtering in case of categorical string columns,nofarm3,closed,2021-01-23T17:37:25Z,2021-01-26T20:58:57Z,"- [x] closes #39189
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry


This bug happens when filtering on categorical string columns and  choose a value which doesn't exist in the column.
Instead of returning an empty dataframe, we get some records. 
It happens because of the usage in numpy `searchsorted(v, side=""left"")` that find indices where elements should be inserted to maintain order (and not 0 in case that the value doesn't exist), like was assumed in the code.

I changed it to first the for the value, and use `searchsorted` only if value exists, I also added a test for this specific use case.
I think in the long run, maybe we should refactor this area in the code since one function covers multiple use cases which makes it more complex to test."
793801023,39407,TYP: Make mypy 0.800 compatible,phofl,closed,2021-01-25T23:28:47Z,2021-01-26T21:02:16Z,"- [x] closes #39399
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
"
793752356,39405,ENH: implement EA.delete,jbrockmendel,closed,2021-01-25T21:52:23Z,2021-01-26T21:58:07Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
792567812,39347,STYLE: cpplint (unintentionally?) skips pandas/_libs/src/ujson/,MarcoGorelli,closed,2021-01-23T14:13:25Z,2021-01-26T22:37:11Z,"In code_checks.sh we have
```
cpplint --quiet --extensions=c,h --headers=h --recursive --filter=-readability/casting,-runtime/int,-build/include_subdir pandas/_libs/src/*.h pandas/_libs/src/parser pandas/_libs/ujson pandas/_libs/tslibs/src/datetime pandas/_libs/*.cpp
```

However, `pandas/_libs/ujson` doesn't exist.

I presume `cpplint` should be running on `pandas/_libs/src/ujson/` instead?"
794558214,39419,DOC: Update mypy version in whatsnew,phofl,closed,2021-01-26T21:01:52Z,2021-01-26T22:50:22Z,"- [x] whatsnew entry

Forgot this in #39407"
794639952,39429,Backport PR #39406 on branch 1.2.x (DOC: link to correct PR),meeseeksmachine,closed,2021-01-26T23:33:09Z,2021-01-27T08:39:00Z,Backport PR #39406: DOC: link to correct PR
793783996,39406,DOC: link to correct PR,erfannariman,closed,2021-01-25T22:50:40Z,2021-01-27T09:21:39Z,"xref #39171, #18262, #35224
"
794639849,39428,Backport PR #39376 on branch 1.2.x (REGR: write compressed pickle files with protocol=5),meeseeksmachine,closed,2021-01-26T23:32:57Z,2021-01-27T10:15:20Z,Backport PR #39376: REGR: write compressed pickle files with protocol=5
766967248,38474,New Pandas Assertion Error ,andrewswimm3395,closed,2020-12-14T22:03:08Z,2021-01-27T12:07:13Z,I am running into an assertion error when using group_by.min() which was never a problem when I was using the older version of pandas
779689697,38981,to_csv - parameter decimal,dornech,closed,2021-01-05T22:53:23Z,2021-01-27T12:09:24Z,"I think that pandas.to_csv does not properly process the decimal parameter. It seems it is ignored whenever the dataframe to be exported contains a NaN value. This means it is not possible to export pandas in European CSV format.

I am aware that problems with to_csv have been discussed earlier and from these discussions I found the hint to check for NaN values. I am afraid I do not understand the final outcome of alle these previous discussions but I think the current behaviour is not quite correct. 

"
723648968,37187,"BUG:When SeriesGroupBy.apply to return DataFrame for each group, the result is incorrect",CrystalWindSnake,closed,2020-10-17T03:47:13Z,2021-01-27T12:11:26Z,"- [x] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

df = pd.DataFrame(
{
    'name':'a a b b'.split(),
    'n1': [1,20,1,30],
})

def ap_gp(s):
    des = np.where(s>10,'over','not over')
    res = s.to_frame()
    res['des']=des
    return res

df.groupby('name')['n1'].apply(ap_gp)
```

output:
|    |   a | des      |   b |
|---:|----:|:---------|----:|
|  0 |   1 | not over | nan |
|  1 |  20 | over     | nan |
|  2 | nan | not over |   1 |
|  3 | nan | over     |  30 |


#### Expected Output
|    |   n1 | des      |
|---:|-----:|:---------|
|  0 |    1 | not over |
|  1 |   20 | over     |
|  2 |    1 | not over |
|  3 |   30 | over     |


#### Problem description
I think the reason for this error is to modify the name attribute of Series before concat



#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : 0.29.15
pytest           : 5.3.5
hypothesis       : 5.5.4
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.48.0
</details>
"
668336003,35474,ENH: data frame min/max and idxmin/idxmax,chrisvoncsefalvay,closed,2020-07-30T03:29:06Z,2021-01-27T12:12:10Z,"#### Is your feature request related to a problem?

I wish I could use an idxmax-like function to obtain the row and column coordinates of the minimum or maximum value.

#### Describe the solution you'd like

For a `DataFrame` object `df`, the proposed function `df.dfmin` would yield a tuple `(idx, col)` where `idx` would be the index of the minimum value (or a tuple of the hierarchical index's levels) and `col` the name of the column. For multiple equal values, a tuple of values could be returned.

#### API breaking implications

None.
"
780840365,39007,BUG: Inconsistent results of value_counts and mode for different nan-values,realead,closed,2021-01-06T20:54:29Z,2021-01-27T14:03:52Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.


```python
import pandas as pd
import numpy as np
s=pd.Series([True, pd.NA, np.nan])
s.value_counts(dropna=False)
print(s.value_counts(dropna=False))
print(s.mode(dropna=False))
```

#### Problem description

For `s.value_counts(...)` the result is

```
NaN     2
True    1
dtype: int64
```

but for `s.mode(...)` the output is:

```
0     NaN
1    True
2    <NA>
dtype: object
```

#### Expected Output

for the sake of consistancy one would expect `mode` to be only `NaN`, as it was counted twice with `value_counts`, i.e.

```
0     NaN
dtype: object
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-53-generic
Version          : #74-Ubuntu SMP Fri Dec 2 15:59:10 UTC 2016
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.5
numpy            : 1.19.2
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.0.0.post20201207
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : 5.36.0
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.50.1

</details>
"
555729423,31357,Using matplotlib scatter legends?,amueller,closed,2020-01-27T17:41:20Z,2021-01-27T15:47:43Z,"Matplotlib now allows doing legends for categorical data in scatter:
https://matplotlib.org/gallery/lines_bars_and_markers/scatter_with_legend.html

Is that something that pandas could/should use?
I feel like there's a tricky issue in deciding whether the color variable is categorical but basically matplotlib allows us now to treat the case and I'm not sure if treating all cases as continuous makes the most sense."
134509853,12380,Scatter plot should have a discrete colorbar when 'c' is integer,s-celles,closed,2016-02-18T08:12:17Z,2021-01-27T15:47:43Z,"``` python
import pandas as pd
pd.set_option('max_rows', 10)
import sklearn.datasets as datasets
import pandas_ml as pdml  # https://github.com/pandas-ml/pandas-ml
import matplotlib.pyplot as plt
df = pdml.ModelFrame(datasets.load_iris())
print(df)
     .target  sepal length (cm)  sepal width (cm)  petal length (cm)  \
0          0                5.1               3.5                1.4
1          0                4.9               3.0                1.4
2          0                4.7               3.2                1.3
3          0                4.6               3.1                1.5
4          0                5.0               3.6                1.4
..       ...                ...               ...                ...
145        2                6.7               3.0                5.2
146        2                6.3               2.5                5.0
147        2                6.5               3.0                5.2
148        2                6.2               3.4                5.4
149        2                5.9               3.0                5.1

     petal width (cm)
0                 0.2
1                 0.2
2                 0.2
3                 0.2
4                 0.2
..                ...
145               2.3
146               1.9
147               2.0
148               2.3
149               1.8

[150 rows x 5 columns]

print(df.dtypes)
.target                int64
sepal length (cm)    float64
sepal width (cm)     float64
petal length (cm)    float64
petal width (cm)     float64
dtype: object
df.plot.scatter(x='sepal length (cm)', y='sepal width (cm)', c='.target')

plt.show()
```

![iris](https://cloud.githubusercontent.com/assets/109167/13136974/2a1ca272-d61e-11e5-9463-a7fa40fb28f2.png)
#### Expected Output

A scatter plot with a colobar with a discrete colormap
#### Additional information

Converting `.target` column to a category doesn't fix the problem.
Maybe I should open an other issue about it.

``` python
df['.target'] = df['.target'].astype('category')
df.plot.scatter(x='sepal length (cm)', y='sepal width (cm)', c='.target')
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
//anaconda/lib/python3.5/site-packages/matplotlib/colors.py in to_rgb(self, arg)
    305                     else:
--> 306                         fl = float(argl)
    307                         if fl < 0 or fl > 1:

ValueError: could not convert string to float: '.target'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
//anaconda/lib/python3.5/site-packages/matplotlib/colors.py in to_rgba(self, arg, alpha)
    369             else:
--> 370                 r, g, b = self.to_rgb(arg)
    371             if alpha is None:

//anaconda/lib/python3.5/site-packages/matplotlib/colors.py in to_rgb(self, arg)
    327             raise ValueError(
--> 328                 'to_rgb: Invalid rgb arg ""%s""\n%s' % (str(arg), exc))
    329             # Error messages could be improved by handling TypeError

ValueError: to_rgb: Invalid rgb arg "".target""
could not convert string to float: '.target'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
//anaconda/lib/python3.5/site-packages/matplotlib/colors.py in to_rgba_array(self, c, alpha)
    398             # Single value? Put it in an array with a single row.
--> 399             return np.array([self.to_rgba(c, alpha)], dtype=np.float)
    400         except ValueError:

//anaconda/lib/python3.5/site-packages/matplotlib/colors.py in to_rgba(self, arg, alpha)
    375             raise ValueError(
--> 376                 'to_rgba: Invalid rgba arg ""%s""\n%s' % (str(arg), exc))
    377

ValueError: to_rgba: Invalid rgba arg "".target""
to_rgb: Invalid rgb arg "".target""
could not convert string to float: '.target'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
//anaconda/lib/python3.5/site-packages/matplotlib/colors.py in to_rgb(self, arg)
    305                     else:
--> 306                         fl = float(argl)
    307                         if fl < 0 or fl > 1:

ValueError: could not convert string to float: '.'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
//anaconda/lib/python3.5/site-packages/matplotlib/colors.py in to_rgba(self, arg, alpha)
    369             else:
--> 370                 r, g, b = self.to_rgb(arg)
    371             if alpha is None:

//anaconda/lib/python3.5/site-packages/matplotlib/colors.py in to_rgb(self, arg)
    327             raise ValueError(
--> 328                 'to_rgb: Invalid rgb arg ""%s""\n%s' % (str(arg), exc))
    329             # Error messages could be improved by handling TypeError

ValueError: to_rgb: Invalid rgb arg "".""
could not convert string to float: '.'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-8-19145e105567> in <module>()
----> 1 df.plot.scatter(x='sepal length (cm)', y='sepal width (cm)', c='.target')

//anaconda/lib/python3.5/site-packages/pandas/tools/plotting.py in scatter(self, x, y, s, c, **kwds)
   3847         axes : matplotlib.AxesSubplot or np.array of them
   3848         """"""
-> 3849         return self(kind='scatter', x=x, y=y, c=c, s=s, **kwds)
   3850
   3851     def hexbin(self, x, y, C=None, reduce_C_function=None, gridsize=None,

//anaconda/lib/python3.5/site-packages/pandas/tools/plotting.py in __call__(self, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)
   3669                           fontsize=fontsize, colormap=colormap, table=table,
   3670                           yerr=yerr, xerr=xerr, secondary_y=secondary_y,
-> 3671                           sort_columns=sort_columns, **kwds)
   3672     __call__.__doc__ = plot_frame.__doc__
   3673

//anaconda/lib/python3.5/site-packages/pandas/tools/plotting.py in plot_frame(data, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)
   2554                  yerr=yerr, xerr=xerr,
   2555                  secondary_y=secondary_y, sort_columns=sort_columns,
-> 2556                  **kwds)
   2557
   2558

//anaconda/lib/python3.5/site-packages/pandas/tools/plotting.py in _plot(data, x, y, subplots, ax, kind, **kwds)
   2382         plot_obj = klass(data, subplots=subplots, ax=ax, kind=kind, **kwds)
   2383
-> 2384     plot_obj.generate()
   2385     plot_obj.draw()
   2386     return plot_obj.result

//anaconda/lib/python3.5/site-packages/pandas/tools/plotting.py in generate(self)
    985         self._compute_plot_data()
    986         self._setup_subplots()
--> 987         self._make_plot()
    988         self._add_table()
    989         self._make_legend()

//anaconda/lib/python3.5/site-packages/pandas/tools/plotting.py in _make_plot(self)
   1557             label = None
   1558         scatter = ax.scatter(data[x].values, data[y].values, c=c_values,
-> 1559                              label=label, cmap=cmap, **self.kwds)
   1560         if cb:
   1561             img = ax.collections[0]

//anaconda/lib/python3.5/site-packages/matplotlib/__init__.py in inner(ax, *args, **kwargs)
   1810                     warnings.warn(msg % (label_namer, func.__name__),
   1811                                   RuntimeWarning, stacklevel=2)
-> 1812             return func(ax, *args, **kwargs)
   1813         pre_doc = inner.__doc__
   1814         if pre_doc is None:

//anaconda/lib/python3.5/site-packages/matplotlib/axes/_axes.py in scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)
   3891                 offsets=offsets,
   3892                 transOffset=kwargs.pop('transform', self.transData),
-> 3893                 alpha=alpha
   3894                 )
   3895         collection.set_transform(mtransforms.IdentityTransform())

//anaconda/lib/python3.5/site-packages/matplotlib/collections.py in __init__(self, paths, sizes, **kwargs)
    829         """"""
    830
--> 831         Collection.__init__(self, **kwargs)
    832         self.set_paths(paths)
    833         self.set_sizes(sizes)

//anaconda/lib/python3.5/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs)
    115
    116         self.set_edgecolor(edgecolors)
--> 117         self.set_facecolor(facecolors)
    118         self.set_linewidth(linewidths)
    119         self.set_linestyle(linestyles)

//anaconda/lib/python3.5/site-packages/matplotlib/collections.py in set_facecolor(self, c)
    610             c = mpl.rcParams['patch.facecolor']
    611         self._facecolors_original = c
--> 612         self._facecolors = mcolors.colorConverter.to_rgba_array(c, self._alpha)
    613         self.stale = True
    614

//anaconda/lib/python3.5/site-packages/matplotlib/colors.py in to_rgba_array(self, c, alpha)
    420             result = np.zeros((nc, 4), dtype=np.float)
    421             for i, cc in enumerate(c):
--> 422                 result[i] = self.to_rgba(cc, alpha)
    423             return result
    424

//anaconda/lib/python3.5/site-packages/matplotlib/colors.py in to_rgba(self, arg, alpha)
    374         except (TypeError, ValueError) as exc:
    375             raise ValueError(
--> 376                 'to_rgba: Invalid rgba arg ""%s""\n%s' % (str(arg), exc))
    377
    378     def to_rgba_array(self, c, alpha=None):

ValueError: to_rgba: Invalid rgba arg "".""
to_rgb: Invalid rgb arg "".""
could not convert string to float: '.'

```
#### output of `pd.show_versions()`

```
INSTALLED VERSIONS
------------------
commit: None
python: 3.5.1.final.0
python-bits: 64
OS: Darwin
OS-release: 14.5.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: fr_FR.UTF-8

pandas: 0.17.1
nose: 1.3.7
pip: 8.0.2
setuptools: 19.6.2
Cython: 0.23.4
numpy: 1.10.4
scipy: 0.17.0
statsmodels: 0.6.1
IPython: 4.0.1
sphinx: 1.3.1
patsy: 0.4.0
dateutil: 2.4.2
pytz: 2015.7
blosc: None
bottleneck: 1.0.0
tables: 3.2.2
numexpr: 2.4.4
matplotlib: 1.5.1
openpyxl: 2.2.6
xlrd: 0.9.4
xlwt: 1.0.0
xlsxwriter: 0.7.7
lxml: 3.4.4
bs4: 4.4.1
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 1.0.9
pymysql: None
psycopg2: None
Jinja2: 2.8
```

Pinging @sinhrks 

See also http://stanford.edu/~mwaskom/software/seaborn/examples/scatterplot_matrix.html
"
622571691,34293,ENH: categorical scatter plot,MarcoGorelli,closed,2020-05-21T15:16:01Z,2021-01-27T16:04:56Z,"- [ ] closes #12380 (this idea was mentioned as a comment here). Also, closes #31357 (I hadn't noticed found that issue when I first opened this)
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Here's what this would look like:

![image](https://user-images.githubusercontent.com/33491632/82671464-cd9ce500-9c36-11ea-9f1d-eb901d209ab9.png)


Was just hoping to get initial feedback on whether this would be welcome

Somethings I still need to do:
- [ ] add docstring / annotations to `_plot_colorbar`
- [x] `test_fash.sh` passes locally but there are some CI failures, need to reproduce / figure out why (those tests pass now, failure was probably unrelated)
- [ ] add an example of this functionality to the docs
- [ ] if ints are passed, treat same way as categorical"
792629002,39352,STYLE: Fix incorrect path to ujson directory,gustavocmaciel,closed,2021-01-23T19:04:06Z,2021-01-27T16:44:13Z,"- [x] closes #39347 

I fixed the path to `ujson` directory so `cpplint` will run on `pandas/_libs/src/ujson/` instead of `pandas/_libs/ujson`, which doesn't exist.

But the title of the issue says `cpplint (unintentionally?) skips pandas/_libs/src/ujson/`, so I understand there's a chance the `pandas/_libs/src/ujson/` is getting *intentionally* skipped.

Assuming this was **unintentional**, I believe this small fix solves the issue."
297378494,19710,DOC: develop a set of standard example DataFrames for use in docstring examples,jorisvandenbossche,open,2018-02-15T09:31:51Z,2021-01-27T17:28:04Z,"Related to https://github.com/pandas-dev/pandas/pull/19704. I didn't find an existing open issue, only a discussion mentioning this in https://github.com/pandas-dev/pandas/pull/16520 (@datapythonista it was actually you then! I didn't realize that :-))

I think it would be good to have a set of standard DataFrames that we reuse throughout our docs (to start with in the docstrings, but we could actually also use a standardized set for the user guide):

- Some small, more ""realistic"" dataframes would make it is easier to reason about than dummy random data + adds familiarity when reading multiple docstrings
- Makes it easier for contributors to add examples to the docstring as they don't have to invent their own data each time

I don't think there will be ""one example dataframe to rule them all"", but it would be nice to have a set of them that can cover most of the use cases. 
So we can post some ideas here and discuss them, trying to get to a list.

Side question is whether we want to always define them with code in the docstring, or want to have some example data loading capabilities (eg like seaborn, it examples always start with a `iris = sns.load_dataset(""iris"")` or other dataset). It can also be a mixture of both of course."
792691531,39365,REF: Unify _set_noconvert_dtype_columns for parsers,phofl,closed,2021-01-24T00:57:26Z,2021-01-27T18:36:35Z,"- [x] xref #39345
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

I unified the code which led to a bugfix. The test failed for the python parser case previously"
792629949,39353, ENH: making mode stable/keeping original ordering,realead,closed,2021-01-23T19:09:47Z,2021-01-27T19:04:33Z,"- [x] closes #39007
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

This is follow up of #39009 but for `mode`: now keys returned by `mode` are in the original order. This is also improvement of the code quality as `mode` now uses the same code path as `value_counts`.

Fixes a performance regression of #39009.
"
780846647,39009,ENH: making value_counts stable/keeping original ordering,realead,closed,2021-01-06T21:07:00Z,2021-01-27T19:05:04Z,"closes #12679
closes #11227
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

The order of the returned keys for `value_counts` aren't arbitrary (i.e. depending on the used hash function) but are original ordering (when sorted this applies for the keys with the same number of values).
"
744205819,37900,quantile function failure ,enigdata,closed,2020-11-16T21:33:21Z,2021-01-27T19:34:26Z,"If I create a dataframe like this:

```
gravity_df = pd.DataFrame(columns=[""accel_time_seconds"", ""gravity_x"", ""gravity_y"", ""gravity_z""])
gravity_df['accel_time_seconds'] = list(range(100))
gravity_df.loc[:70, 'gravity_x'] = -0.145438
gravity_df.loc[:70, 'gravity_y'] = -0.355129
gravity_df.loc[:70, 'gravity_z'] = -0.923434
gravity_df.loc[70:, 'gravity_x'] = -0.114892
gravity_df.loc[70:, 'gravity_y'] = -0.428598
gravity_df.loc[70:, 'gravity_z'] = -0.89616
```

quantile function would not work properly for multiple columns:
```
gravity_df[['gravity_x', 'gravity_y']].quantile(0.5)
```

Error message as follows:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-25-17d112cd6897> in <module>
----> 1 gravity_df[['gravity_x']].quantile(0.5)

~/.local/share/virtualenvs/crashes-ETafZQlO/lib/python3.7/site-packages/pandas/core/frame.py in quantile(self, q, axis, numeric_only, interpolation)
   8279 
   8280         result = data._data.quantile(
-> 8281             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed
   8282         )
   8283 

~/.local/share/virtualenvs/crashes-ETafZQlO/lib/python3.7/site-packages/pandas/core/internals/managers.py in quantile(self, axis, consolidate, transposed, interpolation, qs, numeric_only)
    533 
    534         # single block, i.e. ndim == {1}
--> 535         values = _concat._concat_compat([b.values for b in blocks])
    536 
    537         # compute the orderings of our original data

~/.local/share/virtualenvs/crashes-ETafZQlO/lib/python3.7/site-packages/pandas/core/dtypes/concat.py in _concat_compat(to_concat, axis)
    172                 to_concat = [x.astype(""object"") for x in to_concat]
    173 
--> 174     return np.concatenate(to_concat, axis=axis)
    175 
    176 

ValueError: need at least one array to concatenate
```

"
661485596,35349,Non-intuitive behavior when multi-indexing a Series,konstantinmiller,closed,2020-07-20T07:37:09Z,2021-01-28T00:59:52Z,"Most likely that is not a bug but intended behavior but I find it quite confusing.

Consider the following example
```
idx = pd.IndexSlice
s = pd.Series(index=pd.MultiIndex.from_tuples([('A', '0'), ('A', '1'), ('B', '0')]), 
              data=[21, 22, 23])
s.loc[idx['A', :], :]
```
which gives
```
A  0    21
   1    22
B  0    23
dtype: int64
```
while intuitively, one would expect
```
A  0    21
   1    22
dtype: int64
```
I would expect the latter because that's what you would get with a `pd.DataFrame` where the second `:` is referring to the column index. Not paying attention that the object is a series, gives you a completely different result than if you would have a data frame."
795592123,39445,TST: strictify xfails in sparse tests,jbrockmendel,closed,2021-01-28T01:39:54Z,2021-01-28T02:22:36Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
793989942,39410,"BUG: pd.testing.assert_frame_equal(..., check_exact=True) raises AssertionError when comparing numeric ExtensionDtypes",apriha,closed,2021-01-26T07:03:35Z,2021-01-28T02:34:12Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

d = {""x"": [1, 2]}

df1 = pd.DataFrame(data=d, dtype=pd.UInt32Dtype())
df2 = pd.DataFrame(data=d, dtype=pd.UInt32Dtype())

pd.testing.assert_frame_equal(df1, df2, check_exact=True)
```

#### Problem description

The above code raises the following exception:

```
AssertionError: ndarray Expected type <class 'numpy.ndarray'>, found <class 'pandas.core.arrays.integer.IntegerArray'> instead
```

Instead, the usage of `pd.testing.assert_frame_equal(..., check_exact=True)` should not raise an exception when comparing numeric `ExtensionDtype`s.

#### Expected Output

No exception.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 9d598a5e1eee26df95b3910e3f2934890d062caa
python           : 3.8.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 17.7.0
Version          : Darwin Kernel Version 17.7.0: Fri Oct 30 13:34:27 PDT 2020; root:xnu-4570.71.82.8~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.1
numpy            : 1.19.5
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.2.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
794627214,39427,REF: reuse can_hold_element for NumericIndex._validate_fill_value,jbrockmendel,closed,2021-01-26T23:05:12Z,2021-01-28T02:35:29Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
795249196,39437,REF: Index.insert maybe_promote -> find_common_type,jbrockmendel,closed,2021-01-27T16:24:56Z,2021-01-28T02:35:57Z,
794577671,39425,REF: de-duplicate MultiIndex validation,jbrockmendel,closed,2021-01-26T21:35:11Z,2021-01-28T02:36:50Z,
757505461,38303,BUG: Series.loc setitem with Series raises ValueError,ma3da,closed,2020-12-05T01:33:26Z,2021-01-28T02:37:06Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

ser = pd.Series([0, 0, 0], dtype=object)

ser.loc[0] = pd.Series([42])  # ValueError: No axis named 1 for object type Series

```

#### Problem description
<details>

```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/sources/official.clone/pandas/pandas/core/generic.py in _get_axis_number(cls, axis)
    456         try:
--> 457             return cls._AXIS_TO_AXIS_NUMBER[axis]
    458         except KeyError:

KeyError: 1

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-24-7c4b5d72ca25> in <module>
----> 1 ser.loc[0] = pd.Series([0])

~/sources/official.clone/pandas/pandas/core/indexing.py in __setitem__(self, key, value)
    689 
    690         iloc = self if self.name == ""iloc"" else self.obj.iloc
--> 691         iloc._setitem_with_indexer(indexer, value, self.name)
    692 
    693     def _validate_key(self, key, axis: int):

~/sources/official.clone/pandas/pandas/core/indexing.py in _setitem_with_indexer(self, indexer, value, name)
   1634             self._setitem_with_indexer_split_path(indexer, value, name)
   1635         else:
-> 1636             self._setitem_single_block(indexer, value, name)
   1637 
   1638     def _setitem_with_indexer_split_path(self, indexer, value, name: str):

~/sources/official.clone/pandas/pandas/core/indexing.py in _setitem_single_block(self, indexer, value, name)
   1848             # setting for extensionarrays that store dicts. Need to decide
   1849             # if it's worth supporting that.
-> 1850             value = self._align_series(indexer, Series(value))
   1851 
   1852         elif isinstance(value, ABCDataFrame) and name != ""iloc"":

~/sources/official.clone/pandas/pandas/core/indexing.py in _align_series(self, indexer, ser, multiindex_indexer)
   2018 
   2019         elif is_scalar(indexer):
-> 2020             ax = self.obj._get_axis(1)
   2021 
   2022             if ser.index.equals(ax):

~/sources/official.clone/pandas/pandas/core/generic.py in _get_axis(self, axis)
    467     @final
    468     def _get_axis(self, axis: Axis) -> Index:
--> 469         axis_number = self._get_axis_number(axis)
    470         assert axis_number in {0, 1}
    471         return self.index if axis_number == 0 else self.columns

~/sources/official.clone/pandas/pandas/core/generic.py in _get_axis_number(cls, axis)
    457             return cls._AXIS_TO_AXIS_NUMBER[axis]
    458         except KeyError:
--> 459             raise ValueError(f""No axis named {axis} for object type {cls.__name__}"")
    460 
    461     @final

ValueError: No axis named 1 for object type Series
```
</details>

#### Expected Output
The same as with

```python
ser.iloc[0] = pd.Series([42])
```

```
0    0    42
dtype: int64
1                       0
2                       0
dtype: object
```
#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : 03e58585036c83ca3d4c86d7d3d7ede955c15130
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.8.0-0.bpo.2-amd64
Version          : #1 SMP Debian 5.8.10-1~bpo10+1 (2020-09-26)
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+1465.g03e585850
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 40.8.0
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.3
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None```
</details>
"
642578152,34921,BUG: `read_excel` function auto-fills `NaN` values in index-columns with recent not-NaN values in the column,subhrm,open,2020-06-21T14:53:33Z,2021-01-28T03:21:24Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
>>> # Notice for col2 cells are populated for rows 2,3 & 5, where as they are emply in the excel file and csv file
>>> pd.read_excel(""test.xlsx"", index_col=[0,1]) 
           data
col1 col2
1    2        5
2    2       10
3    2       15
4    8       20
5    8       25
6    12      30

>>> # Notice how read_csv works correctly and has NaN in col2 for rows 2,3 & 5
>>> pd.read_csv(""test.csv"", index_col=[0,1]) 
           data
col1 col2
1    2.0      5
2    NaN     10
3    NaN     15
4    8.0     20
5    NaN     25
6    12.0    30

>>> pd.read_excel(""test.xlsx"", index_col=[0,1], engine=""xlrd"") # same issue with 'xlrd' engine
           data
col1 col2
1    2        5
2    2       10
3    2       15
4    8       20
5    8       25
6    12      30

>>> pd.read_excel(""test.xlsx"", index_col=[0,1], engine=""openpyxl"") # same issue with 'openpyxl' engine
           data
col1 col2
1    2        5
2    2       10
3    2       15
4    8       20
5    8       25
6    12      30
```

#### Problem description

The `read_excel` function auto-fills `NaN` values in index columns with prev not `NaN` values. where as `read_csv` funtion works correctly.

Here is a screenshot of the excel file
![Annotation 2020-06-21 202246](https://user-images.githubusercontent.com/850012/85227780-0b8b5580-b3fd-11ea-9d55-84bd733f5321.png)


Here are the two test files in one zip archive. [test.zip](https://github.com/pandas-dev/pandas/files/4809648/test.zip)


#### Expected Output

The `NaN` cells should be left untouched as correctly read by `read_csv`.

#### Output of ``pd.show_versions()``

<details>

```

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_India.1252

pandas           : 1.0.4
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.0.post20200616
Cython           : 0.29.20
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.9
numba            : 0.49.1
```

</details>
"
774620221,38692,API: setting np.nan into Series[bool],jbrockmendel,closed,2020-12-25T04:06:58Z,2021-01-28T03:29:55Z,"```
ser = pd.Series([True, False, True, False])
ser[0] = np.nan

>>> ser
0    NaN
1    0.0
2    1.0
3    0.0
dtype: float64
```

It would be more consistent (and would simplify the code) if this cast to object and did not convert True/False to 1.0/0.0.  ATM we have 2 tests that such a change would break."
794570808,39423,BUG: Assert_frame_equal always raising AssertionError when comparing extension dtypes,phofl,closed,2021-01-26T21:23:03Z,2021-01-28T08:48:38Z,"- [x] closes #39410
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

"
795796893,39449,Backport PR #39423: BUG: Assert_frame_equal always raising AssertionError when comparing extension dtypes,phofl,closed,2021-01-28T08:46:11Z,2021-01-28T10:58:38Z,#39423
340799085,21881,Series with dtype=object does unexpected type conversion,AllenDowney,closed,2018-07-12T21:09:09Z,2021-01-28T14:59:30Z,"#### Code Sample, a copy-pastable example if possible

```python
# Example 1

timestamp = pd.Timestamp(1412526600000000000)
series = pd.Series([], dtype=object)
series['timestamp'] = timestamp
type(series.timestamp)

# Example 2

series = pd.Series([], dtype=object)
series['anything'] = 300.0
series['timestamp'] = timestamp
type(series.timestamp)

```
#### Problem description

In the first example, the timestamp is still a Timestamp.

In the second example, the timestamp gets converted to int.

#### Expected Output

I expected the timestamp to continue to be a Timestamp, especially because the dtype of the Series is object.  Why are the types of the values getting converted?

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.5.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-128-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.utf8
LOCALE: en_US.UTF-8

pandas: 0.23.0
pytest: 3.5.1
pip: 10.0.1
setuptools: 39.1.0
Cython: 0.28.2
numpy: 1.14.3
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.4.0
sphinx: 1.7.4
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: 1.2.1
tables: 3.4.3
numexpr: 2.6.5
feather: None
matplotlib: 2.2.2
openpyxl: 2.5.3
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.4
lxml: 4.2.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.7
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
789563802,39285,Series with dtype=object does unexpected type conversion,ftrihardjo,closed,2021-01-20T02:17:21Z,2021-01-28T14:59:33Z,"- [ ] closes #21881
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
795261982,39439,BUG: RangeIndex concatenating incorrectly a single object of length 1,AnnaDaglis,closed,2021-01-27T16:40:30Z,2021-01-28T15:04:29Z,"- [x] closes #39401
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
408601476,25261,plotting: subplots_adjust prevents use of constrained_layout=True,michaelaye,closed,2019-02-11T00:15:51Z,2021-01-28T15:07:14Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
import matplotlib.pyplot as plt
import pandas as pd

fig, axes = plt.subplots(2, constrained_layout=True)
times = pd.date_range(start='now', periods=10)
pd.DataFrame({'a': np.arange(10)}, index=times).plot(style='x', ax=axes[0])
```
#### Problem description
Plotting time-series uses pandas-internally a subplots_adjust, but due to this I am unable to use the new matplotlib constrained_layout that would take care of these things automatically.

#### Expected Output
A good layout that respects my constrained_layout setting to plt.subplots()

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.1.final.0
python-bits: 64
OS: Linux
OS-release: 3.10.0-862.el7.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.1
pytest: 4.2.0
pip: 19.0.2
setuptools: 40.7.3
Cython: 0.29.5
numpy: 1.16.1
scipy: 1.2.0
pyarrow: None
xarray: 0.11.3
IPython: 7.1.1
sphinx: 1.8.4
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: None
tables: 3.4.4
numexpr: 2.6.9
feather: None
matplotlib: 3.0.2
openpyxl: None
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.3
lxml.etree: 4.3.1
bs4: None
html5lib: None
sqlalchemy: 1.2.17
pymysql: None
psycopg2: 2.7.7 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
790106861,39298,REF: add skeleton for strategy pattern in describe,ivanovmg,closed,2021-01-20T15:58:06Z,2021-01-28T15:30:09Z,"- [ ] xref #36833
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

Added a skeleton for the strategy pattern in ``pandas/core/describe.py``.
Currently the methods ``describe`` simply delegate to the corresponding ``describe_`` functions.
In the next steps I plan on moving the ``describe`` functions into each of the strategy classes.
Finally, these functions can be unified, since a lot there has a common pattern."
788709687,39266,BUG: setting dt64 values into Series[int] incorrectly casting dt64->int,jbrockmendel,closed,2021-01-19T05:04:54Z,2021-01-28T15:33:32Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
795590247,39444,REF: shallow_copy->simple_new/rename,jbrockmendel,closed,2021-01-28T01:34:43Z,2021-01-28T20:54:20Z,
788104795,39247,BUG: V1.2 DataFrame.to_csv() fails to write a file with codecs,Kazzz-S,closed,2021-01-18T09:43:13Z,2021-01-28T20:55:16Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
#! /usr/bin/env python
# -*- coding: utf-8 -*-

import pandas as pd
import codecs

x    = [ 1, 2, 3, 4, 5  ]
y    = [ 6, 7, 8, 9, 10 ]
z    = [ 'a', 'b', 'c', 'd', 'e' ]
data = { ""X"": x, ""Y"":y, ""Z"":z }
df   = pd.DataFrame( data, columns=[ ""X"", ""Y"", ""Z"" ] )
print( ""Pandas version = %s"" % pd.__version__ )
print(df)

fp = codecs.open( ""out-testPD12.csv"", ""w"", ""utf-8"" )
fp.write( ""Pandas version = %s\n"" % pd.__version__  )
df.to_csv( fp, index=False, header=True )
fp.close()

```

#### Problem description
When saving the file, `TypeError: utf_8_encode() argument 1 must be str, not bytes`  flags on.

#### Expected Output
The output below has been obtained by downgrading and pinned Pandas to V1.1.5.
V1.1.2 is also tested OK.
```
Pandas version = 1.1.5
X,Y,Z
1,6,a
2,7,b
3,8,c
4,9,d
5,10,e
```

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.2.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Tue Nov 10 00:10:30 PST 2020; root:xnu-6153.141.10~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.2.0
numpy            : 1.19.2
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.1.2.post20210112
Cython           : 0.29.21
pytest           : 6.2.1
hypothesis       : None
sphinx           : 3.4.3
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.3
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.2
odfpy            : None
openpyxl         : 3.0.6
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.21
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 2.0.1
xlwt             : 1.3.0
numba            : 0.51.2

```
(base) Catalina1{kazzz-s} temp (1)% python testPD12.py 
Pandas version = 1.2.0
   X   Y  Z
0  1   6  a
1  2   7  b
2  3   8  c
3  4   9  d
4  5  10  e
Traceback (most recent call last):
  File ""testPD12.py"", line 52, in <module>
    df.to_csv( fp, index=False, header=True )
  File ""/Users/kazzz-s/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py"", line 3384, in to_csv
    return DataFrameRenderer(formatter).to_csv(
  File ""/Users/kazzz-s/opt/anaconda3/lib/python3.8/site-packages/pandas/io/formats/format.py"", line 1083, in to_csv
    csv_formatter.save()
  File ""/Users/kazzz-s/opt/anaconda3/lib/python3.8/site-packages/pandas/io/formats/csvs.py"", line 248, in save
    self._save()
  File ""/Users/kazzz-s/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py"", line 104, in __exit__
    self.close()
  File ""/Users/kazzz-s/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py"", line 89, in close
    self.handle.flush()
  File ""/Users/kazzz-s/opt/anaconda3/lib/python3.8/codecs.py"", line 721, in write
    return self.writer.write(data)
  File ""/Users/kazzz-s/opt/anaconda3/lib/python3.8/codecs.py"", line 377, in write
    data, consumed = self.encode(object, self.errors)
TypeError: utf_8_encode() argument 1 must be str, not bytes
```
</details>
"
693200790,36115,REGR: append tz-aware DataFrame with tz-naive values,jorisvandenbossche,closed,2020-09-04T14:00:20Z,2021-01-28T22:36:17Z,"Closes #35460
"
790651395,39311,REF: Move part of groupby.agg to apply,rhshadrach,closed,2021-01-21T03:22:50Z,2021-01-28T23:00:37Z,"- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

pre-commit adjusted to account for the false positive on `Union[SeriesGroupBy, DataFrameGroupBy]`.

Moves part of groupby.agg implementation from `pandas.core.aggregation` to `pandas.core.apply`. The only difference in the code is that in `apply`, the error message raised is different when the argument is a string (e.g. ""sum"") and the kwarg axis=1 is provided. Added a test to check for error message.

After this will be resample and rolling, then much of the implementation code in core.aggregation code can be moved into apply."
777144619,38867,REF: Move aggregation into apply,rhshadrach,closed,2020-12-31T21:51:41Z,2021-01-28T23:02:00Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This is a step toward #35725. We can't simply ban agg from not aggregating because it's used by apply for series, dataframe, groupby, resampler, and rolling. There are two other related issues. First, apply, agg, and transform are all doing similar things but with different implementations leading to unintended inconsistencies. Second, we would also like to ban mutation in apply (and presumably agg and transform for the same reasons).

It seems best to me to handle all of these by first combining the implementation into apply, cleaning up and sharing code between the implementations, and then we would be in a good place to ban agg from not aggregating.

This starts the process by moving aggregation.aggregate into apply, but it is only utilized for frames. Once this is done for the remaining objects (series, groupby, resampler, rolling), the rest of the functions in aggregation can be moved into apply as well."
796305643,39452,Backport PR #39440 on branch 1.2.x (REGR: prefer user-provided mode),meeseeksmachine,closed,2021-01-28T19:52:02Z,2021-01-29T02:02:39Z,Backport PR #39440: REGR: prefer user-provided mode
796378815,39455,TST: use unique ports in test_user_agent.py ,twoertwein,closed,2021-01-28T21:13:12Z,2021-01-29T02:03:03Z,Should take care of the occasional `OSError: [Errno 98] Address already in use` in `test_server_and_default_headers`.
776456184,38808,CLN: Add typing for dtype argument in codebase,avinashpancham,closed,2020-12-30T13:28:45Z,2021-01-29T02:04:26Z,"Follow up from #37546 and #38680 

Add typing for the `dtype` argument throughout the pandas codebase. Part of the code might need an update after adding the type since the mypy type checks can fail.

 In #38680 it is already done for `io/sql.py`, from a directory search I found that the list of files below still have untyped dtype arguments. 

Would be great if people can work on a few files at a time in separate PRs and mention this issue. I will already start with the remaining files in the io directory

- [x] io/parsers.py
- [x] io/excel/_base.py
- [x] io/json/_json.py
- [x] io/pytables.py
- [x] core/arrays/datetimelike.py
- [x] core/arrays/base.py
- [x] core/arrays/boolean.py
- [x] core/arrays/categorical.py
- [x] core/arrays/integer.py
- [x] core/arrays/interval.py
- [x] core/arrays/masked.py
- [x] core/arrays/numpy_.py
- [x] core/arrays/period.py
- [x] core/arrays/sparse/array.py
- [x] core/arrays/string_.py
- [x] core/arrays/string_arrow.py
- [x] core/arrays/timedeltas.py
- [x] core/indexes/category.py
- [x] core/indexes/datetimes.py
- [x] core/indexes/interval.py
- [x] core/indexes/numeric.py
- [x] core/indexes/period.py
- [x] core/indexes/range.py
- [x] core/internals/block.py
- [x] core/internals/managers.py
- [x] core/reshape/reshape.py
- [x] core/strings/object_array.py
- [x] core/base.py
- [x] core/common.py
- [x] core/frame.py
- [x] core/generic.py
- [x] core/series.py
"
796410851,39458,CLN: add typing to dtype arg in core/frame.py (GH38808),avinashpancham,closed,2021-01-28T21:59:25Z,2021-01-29T02:04:31Z,"- [x] closes #38808
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
796452808,39459,REF: Move Resampler/Window.agg into core.apply,rhshadrach,closed,2021-01-28T23:24:35Z,2021-01-29T02:52:40Z,"- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

Last move from aggregation.aggregate to apply. Will follow up with moving helper methods from aggregation into apply."
796460218,39460,REF: avoid using Block attributes,jbrockmendel,closed,2021-01-28T23:41:55Z,2021-01-29T03:03:44Z,Moves this chunk of code incrementally towards being BlockManager/ArrayManager agnostic
793607430,39401,"ValueError when concatenating a length-one, reversed object",ahawryluk,closed,2021-01-25T18:08:48Z,2021-01-29T04:24:47Z,"The following code should return a new Series with a single value, but raises a ValueError:
```python
import pandas as pd
s = pd.Series([100])
pd.concat([s.iloc[::-1]])
```

The corresponding example with a DataFrame also raises a ValueError:
```python
import pandas as pd
df = pd.DataFrame([100])
pd.concat([df.iloc[::-1]])
```

The DataFrame version can be reproduced with multiple columns, but any of the following modifications will prevent the bug:

- create Series or DataFrame with len > 1
- concatenate more than one object
- remove the reversing operation `.iloc[::-1]`
- pass ignore_index=True to pd.concat

This bug appeared as an edge case in a function that usually handles DataFrames with multiple rows.

<details>
<summary>Traceback for the Series example</summary>

```
C:\Anaconda3\lib\site-packages\pandas\core\reshape\concat.py in concat(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    296     )
    297
--> 298     return op.get_result()
    299
    300

C:\Anaconda3\lib\site-packages\pandas\core\reshape\concat.py in get_result(self)
    483
    484                 res = concat_compat(arrs, axis=0)
--> 485                 result = cons(res, index=self.new_axes[0], name=name, dtype=res.dtype)
    486                 return result.__finalize__(self, method=""concat"")
    487

C:\Anaconda3\lib\site-packages\pandas\core\series.py in __init__(self, data, index, dtype, name, copy, fastpath)
    320                     if len(index) != len(data):
    321                         raise ValueError(
--> 322                             f""Length of passed values is {len(data)}, ""
    323                             f""index implies {len(index)}.""
    324                         )

ValueError: Length of passed values is 1, index implies 0.
```

</details>

<details>
<summary>Traceback for the DataFrame example</summary>

```
C:\Anaconda3\lib\site-packages\pandas\core\reshape\concat.py in concat(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    296     )
    297
--> 298     return op.get_result()
    299
    300

C:\Anaconda3\lib\site-packages\pandas\core\reshape\concat.py in get_result(self)
    519
    520             new_data = concatenate_block_managers(
--> 521                 mgrs_indexers, self.new_axes, concat_axis=self.bm_axis, copy=self.copy
    522             )
    523             if not self.copy:

C:\Anaconda3\lib\site-packages\pandas\core\internals\concat.py in concatenate_block_managers(mgrs_indexers, axes, concat_axis, copy)
     87         blocks.append(b)
     88
---> 89     return BlockManager(blocks, axes)
     90
     91

C:\Anaconda3\lib\site-packages\pandas\core\internals\managers.py in __init__(self, blocks, axes, do_integrity_check)
    141
    142         if do_integrity_check:
--> 143             self._verify_integrity()
    144
    145         # Populate known_consolidate, blknos, and blklocs lazily

C:\Anaconda3\lib\site-packages\pandas\core\internals\managers.py in _verify_integrity(self)
    321         for block in self.blocks:
    322             if block.shape[1:] != mgr_shape[1:]:
--> 323                 raise construction_error(tot_items, block.shape[1:], self.axes)
    324         if len(self.items) != tot_items:
    325             raise AssertionError(

ValueError: Shape of passed values is (1, 1), indices imply (0, 1)
```

</details>

<details>
<summary>pd.show_versions()</summary>

```
INSTALLED VERSIONS
------------------
commit           : 9d598a5e1eee26df95b3910e3f2934890d062caa
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.17763
machine          : AMD64
processor        : Intel64 Family 6 Model 63 Stepping 2, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_Canada.1252

pandas           : 1.2.1
numpy            : 1.19.2
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.3.3.post20210118
Cython           : 0.29.21
pytest           : 6.2.1
hypothesis       : None
sphinx           : 3.4.3
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.3
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.2
odfpy            : None
openpyxl         : 3.0.6
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.21
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 2.0.1
xlwt             : 1.3.0
numba            : 0.51.2
```

</details>"
793268816,39394,ENH: Fix support for matplotlib's constrained_layout (#25261),mdruiter,closed,2021-01-25T10:46:58Z,2021-01-29T13:34:27Z,"- [x] closes #25261
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
796550792,39463,"CLN: assorted cleanups, ported tests",jbrockmendel,closed,2021-01-29T03:40:31Z,2021-01-29T15:55:49Z,
796894173,39468,DOC: Add header to `test_writing.rst`,gustavocmaciel,closed,2021-01-29T13:45:53Z,2021-01-29T18:44:11Z,"#### Location of the documentation

https://github.com/pandas-dev/pandas/blob/master/doc/source/development/test_writing.rst

#### Documentation problem

The `test_writitng.rst` file doesn't have the standard header.

#### Suggested fix for documentation

This can be fixed by adding `{{ header }}` at the beginning of the file."
793703254,39403,Clean up DataFrame.setitem behavior for duplicate columns,phofl,closed,2021-01-25T20:30:25Z,2021-01-29T21:03:02Z,"- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

cc  @jbrockmendel This cleans the edgy behavior up for duplicate columns.

Previous test was wrong."
797156791,39471,TST: fixturize in test_coercion,jbrockmendel,closed,2021-01-29T20:11:28Z,2021-01-29T21:27:43Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
796499803,39462,BUG: accepting ndarray[object] of dt64(nat) in TimedeltaIndex,jbrockmendel,closed,2021-01-29T01:23:12Z,2021-01-29T21:29:31Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
797136964,39470,"CLN: remove NaT.__int__, __long__",jbrockmendel,closed,2021-01-29T19:42:37Z,2021-01-29T21:33:31Z,
797229117,39472,"TST/REF: directories for test_period, test_datetimes, test_timedeltas",jbrockmendel,closed,2021-01-29T21:49:59Z,2021-01-30T00:12:39Z,
797029951,39469,DOC: correct ipython code block in v0.8.0.rst file,jorisvandenbossche,closed,2021-01-29T16:47:52Z,2021-01-30T08:34:35Z,Tiny follow-up on https://github.com/pandas-dev/pandas/pull/39043/files#r558333164
795402180,39442,BUG: DataFrame constructor reordering elements with ndarray from datetime dtype not datetime64[ns],phofl,closed,2021-01-27T20:00:15Z,2021-01-30T08:36:59Z,"- [x] closes #39422
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

The ravel(""K"") was a problem when passing in a view on the underlying object, which was done here."
794568102,39422,"BUG: pd.DataFrame is created inconsistently from numpy.ndarray for non-native pandas dtypes such as: datetime64[ms], etc..",mheydel,closed,2021-01-26T21:18:25Z,2021-01-30T08:37:13Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

a = np.array(
    [
        [""2015-01-01"", ""2015-01-02"", ""2015-01-03""],
        [""2017-01-01"", ""2017-01-02"", ""2017-02-03""],
    ],
    dtype=""datetime64[ms]"",
)

print(pd.DataFrame(a))
           0          1          2
0 2015-01-01 2015-01-03 2017-01-02
1 2015-01-02 2017-01-01 2017-02-03


b = np.array(
    [
        [""2015-01-01"", ""2015-01-02"", ""2015-01-03""],
        [""2017-01-01"", ""2017-01-02"", ""2017-02-03""],
    ],
    dtype=""datetime64[M]"",
)

print(pd.DataFrame(b))
           0          1          2
0 2015-01-01 2015-01-03 2017-01-02
1 2015-01-02 2017-01-01 2017-02-03

c = np.array(
    [
        [""2015-01-01"", ""2015-01-02"", ""2015-01-03""],
        [""2017-01-01"", ""2017-01-02"", ""2017-02-03""],
    ],
    dtype=""datetime64[ns]"",
)

print(pd.DataFrame(c))
           0          1          2
0 2015-01-01 2015-01-02 2015-01-03
1 2017-01-01 2017-01-02 2017-02-03

```

#### Problem description
The exists an inconsistency how pd.DataFrame() is created from 2D numpy.ndarray for not pandas native type. The dataframe should be transposed for datetime64[M], datetime64[ms] etc.


#### Expected Output
```python
print(pd.DataFrame(a))
           0          1          2
0 2015-01-01 2015-01-02 2015-01-03
1 2017-01-01 2017-01-02 2017-02-03

print(pd.DataFrame(b))
0 2015-01-01 2015-01-02 2015-01-03
1 2017-01-01 2017-01-02 2017-02-03
```

#### Possible Hint
```python

d = np.array(
    [
        [""2015-01-01"", ""2015-01-02"", ""2015-01-03""],
        [""2017-01-01"", ""2017-01-02"", ""2017-02-03""],
    ],
    dtype=""datetime64[ms]"",
    order=""f"",
)

print(pd.DataFrame(d))
0 2015-01-01 2015-01-02 2015-01-03
1 2017-01-01 2017-01-02 2017-02-03

```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United Kingdom.1252

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 21.0
setuptools       : 49.6.0.post20210108
Cython           : None
pytest           : 6.2.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.6.0
sqlalchemy       : 1.3.22
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
752398252,38118,BUG: parsed datetime string resolution incorrect,boringow,closed,2020-11-27T17:45:38Z,2021-01-30T15:32:34Z,"I was working today with datetime index, you have the reso (resolutions) defined from year to nanosecond, but then you had the lines 560-570 repeating 'minute' and 'second', and working with millisecond or nanosecond raises a KeyIndex error.


- [ ] closes #38121
- [ ] closes #38077 
- [ ]  tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
797506867,39487,CI: Pin pyarrow to 0.15.1 in 37 macos and linux,phofl,closed,2021-01-30T19:07:06Z,2021-01-30T20:29:02Z,"Pyarrow 2.0.0 instead of 0.15.1 is fetched on these builds since today, which causes test failures"
797323151,39477,TST/REF: SetitemCastingEquivalents,jbrockmendel,closed,2021-01-30T03:25:49Z,2021-01-30T20:44:09Z,Separating this out in preparation of using this pattern for more of the setitem tests (e.g. test_setitem_td64_into_complex further down in this file)
797503029,39485,TST/REF: collect index tests,jbrockmendel,closed,2021-01-30T18:46:39Z,2021-01-30T20:44:30Z,
794558279,39420,Read hdf returns unexpected values for categorical,nofarm3,closed,2021-01-26T21:01:58Z,2021-01-31T06:26:55Z,"- [V] closes #39189
- [V] tests added / passed
- [V] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [V] whatsnew entry

This bug happens when filtering on categorical string columns and choose a value which doesn't exist in the column. Instead of returning an empty dataframe, we get some records. It happens because of the usage in numpy `searchsorted(v, side=""left"")` that find indices where elements should be inserted to maintain order (and not 0 in case that the value doesn't exist), like was assumed in the code.
I changed it to first the for the value, and use `searchsorted` only if value exists, I also added a test for this specific use case. I think in the long run, maybe we should refactor this area in the code since one function covers multiple use cases which makes it more complex to test.

In addition, I moved the logic to a new method to keep the single-responsibility principle and to make it easier to test."
478117160,27808,VIS: Fix DataFrame.plot() produces incorrect legend markers when plotting multiple series on the same axis,charlesdong1991,closed,2019-08-07T20:10:10Z,2021-01-31T10:02:37Z,"I found out that the reason this happens is because `subplots` is not updated accordingly when `plt.subplots()` is used and assign to `ax`, e.g. `fig, ax = plt.subplots(nrows=1, ncols=3)`. I tested my solution and things all get solved, and figure looks very correct like below:
![Screen Shot 2019-08-07 at 10 03 39 PM](https://user-images.githubusercontent.com/9269816/62654665-95a30a80-b960-11e9-94ad-ab43a0e8a3cf.png)

BTW, i am not very sure how to properly test this, and would be nice to know!

- [x] closes #18222
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
785314883,39152,CLN remove 'import pandas as pd' from pandas/core/generic.py,MarcoGorelli,closed,2021-01-13T17:55:04Z,2021-01-31T13:28:25Z,"- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

---

will do the rest (excluding `pandas/tests`) and add a check if this is the kind of thing we want"
784438820,39128,QST:,Vijaya-Sankar-Challa,closed,2021-01-12T17:32:21Z,2021-01-31T13:48:57Z,"Hi, 
I am new to pandas. I was facing issue with openpyxl in append mode

```
import pandas as pd

col = ['Expiry','pnl']
df = pd.DataFrame([], columns=col)
writer = pd.ExcelWriter('asd.xlsx', engine='openpyxl', mode='a')

df.to_excel(writer, ""Sheet1"")

writer.save()
writer.close()
```

It runs successfully but when I open excel file it says
**We found a problem with some content. Do you want us to try to recover?**
I have searched for solution in internet. But nothing was working. Could you please help me?

- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

**Note**: If you'd still like to submit a question, please read [this guide](
https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your question.

```python
# Your code here, if applicable

```
"
777109840,38862,CI speed-up @github-actions pre-commit by only running it on changed files,MarcoGorelli,closed,2020-12-31T18:45:37Z,2021-01-31T19:36:22Z,"From #38858 it seems the actions works fine, though is perhaps a bit slow as it has to run on all files

If we apply the diff
```diff
diff --git a/.github/workflows/comment_bot.yml b/.github/workflows/comment_bot.yml
index 98b8e10f6..dc396be75 100644
--- a/.github/workflows/comment_bot.yml
+++ b/.github/workflows/comment_bot.yml
@@ -29,7 +29,7 @@ jobs:
       - name: Install-pre-commit
         run: python -m pip install --upgrade pre-commit
       - name: Run pre-commit
-        run: pre-commit run --all-files || (exit 0)
+        run: pre-commit run --from-ref=origin/master --to-ref=HEAD --all-files || (exit 0)
       - name: Commit results
         run: |
           git config user.name ""$(git log -1 --pretty=format:%an)""
```

then it should run much faster"
797800232,39506,Remove repeated keywords (#39503),Rasori,closed,2021-01-31T20:09:12Z,2021-01-31T21:15:47Z,"- [x] closes #39503
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
797764665,39503,BUG: Repeated key words on datetime indexing methods (_parsed_string_to_bounds()),boringow,closed,2021-01-31T17:30:06Z,2021-01-31T22:14:11Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. 
Readed! I will know for next time 

#### Code Sample, a copy-pastable example

```python

There is no need for code, as the error is on the function having repeated keywords.
```

#### Problem description

There are two repeated keywords on the master/pandas/core/indexes/datetimes.py#L560 and #L561. 
I closed the issue #38121, as it is working fine, but as said, there are repeated keywords.

        valid_resos = {
            ""year"",
            ""month"",
            ""quarter"",
            ""day"",
            ""hour"",
            ""minute"",
            ""second"",
            ""minute"",
            ""second"",
            ""millisecond"",
            ""microsecond"",
        }

it should be:

        valid_resos = {
            ""year"",
            ""month"",
            ""quarter"",
            ""day"",
            ""hour"",
            ""minute"",
            ""second"",
            ""millisecond"",
            ""microsecond"",
        }

Reference here:

https://github.com/pandas-dev/pandas/blob/0b16fb308b7808f193a5a8d86d71b6e57d99d4e7/pandas/core/indexes/datetimes.py#L560-L561

#### Expected Output

        valid_resos = {
            ""year"",
            ""month"",
            ""quarter"",
            ""day"",
            ""hour"",
            ""minute"",
            ""second"",
            ""millisecond"",
            ""microsecond"",
        }

#### Output of ``pd.show_versions()``

 There is no need for versions. Pandas code in this case is fine. Just two repeated lines.

"
782714993,39079,BUG: read_csv with custom date parser and na_filter=True results in ValueError,ftrihardjo,closed,2021-01-10T00:44:25Z,2021-02-01T10:00:48Z,"- [ ] closes #36111 
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
798057121,39521,⬆️ UPGRADE: Autoupdate pre-commit config,github-actions[bot],closed,2021-02-01T07:29:54Z,2021-02-01T10:54:13Z,"<!-- START pr-commits -->
<!-- END pr-commits -->

## Base PullRequest

default branch (https://github.com/pandas-dev/pandas/tree/master)

## Command results
<details>
<summary>Details: </summary>

<details>
<summary><em>add path</em></summary>

```Shell
/home/runner/work/_actions/technote-space/create-pr-action/v2/node_modules/npm-check-updates/bin
```



</details>
<details>
<summary><em>pip install pre-commit</em></summary>

```Shell
Collecting pre-commit
  Downloading pre_commit-2.10.0-py2.py3-none-any.whl (185 kB)
Collecting pyyaml>=5.1
  Downloading PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)
Collecting virtualenv>=20.0.8
  Downloading virtualenv-20.4.1-py2.py3-none-any.whl (7.2 MB)
Collecting toml
  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)
Collecting cfgv>=2.0.0
  Using cached cfgv-3.2.0-py2.py3-none-any.whl (7.3 kB)
Collecting identify>=1.0.0
  Downloading identify-1.5.13-py2.py3-none-any.whl (97 kB)
Collecting nodeenv>=0.11.1
  Using cached nodeenv-1.5.0-py2.py3-none-any.whl (21 kB)
Collecting six<2,>=1.9.0
  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)
Collecting distlib<1,>=0.3.1
  Using cached distlib-0.3.1-py2.py3-none-any.whl (335 kB)
Collecting appdirs<2,>=1.4.3
  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)
Collecting filelock<4,>=3.0.0
  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)
Installing collected packages: six, filelock, distlib, appdirs, virtualenv, toml, pyyaml, nodeenv, identify, cfgv, pre-commit
Successfully installed appdirs-1.4.4 cfgv-3.2.0 distlib-0.3.1 filelock-3.0.12 identify-1.5.13 nodeenv-1.5.0 pre-commit-2.10.0 pyyaml-5.4.1 six-1.15.0 toml-0.10.2 virtualenv-20.4.1
```

### stderr:

```Shell
WARNING: You are using pip version 20.3.4; however, version 21.0.1 is available.
You should consider upgrading via the '/opt/hostedtoolcache/Python/3.9.1/x64/bin/python -m pip install --upgrade pip' command.
```

</details>
<details>
<summary><em>pre-commit autoupdate || (exit 0);</em></summary>

```Shell
Updating https://github.com/python/black ... already up to date.
Updating https://gitlab.com/pycqa/flake8 ... already up to date.
Updating https://github.com/PyCQA/isort ... [INFO] Initializing environment for https://github.com/PyCQA/isort.
already up to date.
Updating https://github.com/asottile/pyupgrade ... [INFO] Initializing environment for https://github.com/asottile/pyupgrade.
updating v2.7.4 -> v2.8.0.
Updating https://github.com/pre-commit/pygrep-hooks ... already up to date.
Updating https://github.com/asottile/yesqa ... already up to date.
Updating https://github.com/pre-commit/pre-commit-hooks ... [INFO] Initializing environment for https://github.com/pre-commit/pre-commit-hooks.
already up to date.
Updating https://github.com/codespell-project/codespell ... [INFO] Initializing environment for https://github.com/codespell-project/codespell.
already up to date.
Updating https://github.com/MarcoGorelli/no-string-hints ... [INFO] Initializing environment for https://github.com/MarcoGorelli/no-string-hints.
updating v0.1.6 -> v0.1.7.
```



</details>
<details>
<summary><em>pre-commit run -a || (exit 0);</em></summary>

```Shell
[INFO] Installing environment for https://github.com/python/black.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://gitlab.com/pycqa/flake8.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://gitlab.com/pycqa/flake8.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/PyCQA/isort.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/asottile/pyupgrade.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for local.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for local.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for local.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/asottile/yesqa.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/pre-commit/pre-commit-hooks.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/codespell-project/codespell.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/MarcoGorelli/no-string-hints.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
black..................................................................................................Passed
flake8.................................................................................................Passed
flake8 (cython)........................................................................................Passed
flake8 (cython template)...............................................................................Passed
isort..................................................................................................Passed
pyupgrade..............................................................................................Failed
- hook id: pyupgrade
- exit code: 1
- files were modified by this hook

Rewriting pandas/plotting/_core.py
Rewriting pandas/io/excel/_base.py
Rewriting pandas/core/algorithms.py
Rewriting pandas/core/indexes/datetimes.py
Rewriting pandas/core/reshape/melt.py
Rewriting pandas/plotting/_matplotlib/core.py
Rewriting pandas/core/reshape/merge.py
Rewriting pandas/tests/extension/arrow/test_timestamp.py
Rewriting pandas/core/reshape/reshape.py
Rewriting pandas/core/resample.py
Rewriting pandas/core/arrays/string_arrow.py
Rewriting pandas/io/stata.py
Rewriting pandas/core/indexes/multi.py
Rewriting pandas/core/tools/datetimes.py
Rewriting pandas/core/internals/managers.py
Rewriting pandas/core/frame.py
Rewriting pandas/core/arrays/timedeltas.py
Rewriting pandas/core/computation/ops.py
Rewriting pandas/core/dtypes/base.py
Rewriting pandas/core/dtypes/dtypes.py
Rewriting pandas/io/excel/_openpyxl.py
Rewriting pandas/core/arrays/base.py
Rewriting pandas/core/apply.py
Rewriting pandas/tests/extension/decimal/array.py
Rewriting pandas/core/missing.py
Rewriting pandas/core/generic.py
Rewriting pandas/core/sorting.py
Rewriting pandas/io/formats/format.py
Rewriting pandas/io/sas/sasreader.py
Rewriting pandas/core/arrays/interval.py
Rewriting pandas/core/internals/array_manager.py
Rewriting pandas/core/arrays/masked.py
Rewriting pandas/core/window/rolling.py
Rewriting pandas/core/arrays/datetimes.py
Rewriting pandas/io/json/_normalize.py
Rewriting pandas/core/groupby/grouper.py
Rewriting pandas/io/formats/csvs.py
Rewriting pandas/core/groupby/groupby.py
Rewriting pandas/io/pytables.py
Rewriting pandas/plotting/_matplotlib/timeseries.py
Rewriting pandas/core/series.py
Rewriting pandas/plotting/_matplotlib/style.py
Rewriting pandas/core/construction.py
Rewriting pandas/core/indexes/base.py
Rewriting pandas/core/aggregation.py
Rewriting pandas/core/internals/blocks.py
Rewriting pandas/core/internals/ops.py
Rewriting pandas/core/ops/__init__.py
Rewriting pandas/core/groupby/ops.py
Rewriting pandas/core/reshape/concat.py
Rewriting pandas/tests/extension/json/array.py
Rewriting pandas/core/arrays/_mixins.py
Rewriting pandas/core/indexes/period.py
Rewriting pandas/core/arrays/categorical.py
Rewriting pandas/io/formats/info.py
Rewriting pandas/io/parquet.py
Rewriting pandas/core/arrays/string_.py
Rewriting pandas/core/indexes/range.py
Rewriting pandas/_testing/__init__.py
Rewriting pandas/core/arrays/floating.py
Rewriting pandas/core/arrays/datetimelike.py
Rewriting pandas/core/computation/scope.py
Rewriting pandas/tests/extension/arrow/arrays.py
Rewriting pandas/plotting/_matplotlib/__init__.py
Rewriting pandas/tests/extension/list/array.py
Rewriting pandas/core/computation/align.py
Rewriting pandas/core/dtypes/cast.py
Rewriting pandas/core/reshape/pivot.py
Rewriting pandas/plotting/_matplotlib/tools.py
Rewriting pandas/core/arrays/sparse/dtype.py
Rewriting pandas/core/window/ewm.py
Rewriting pandas/core/internals/concat.py
Rewriting pandas/core/internals/construction.py
Rewriting pandas/core/arrays/numeric.py
Rewriting pandas/core/arrays/boolean.py
Rewriting pandas/plotting/_matplotlib/misc.py
Rewriting pandas/core/arrays/period.py
Rewriting pandas/tests/plotting/common.py
Rewriting pandas/io/common.py
Rewriting pandas/core/arrays/numpy_.py
Rewriting pandas/core/arrays/integer.py
Rewriting pandas/core/indexing.py
Rewriting pandas/core/arrays/sparse/array.py
Rewriting pandas/io/formats/style.py
Rewriting pandas/core/describe.py
Rewriting pandas/compat/pickle_compat.py
Rewriting pandas/io/gbq.py
Rewriting pandas/core/computation/pytables.py
Rewriting pandas/core/groupby/generic.py
Rewriting pandas/core/indexes/interval.py
Rewriting pandas/io/orc.py

rst ``code`` is two backticks..........................................................................Passed
rst directives end with two colons.....................................................................Passed
rst ``inline code`` next to normal text................................................................Passed
Generate pip dependency from conda.....................................................................Passed
flake8-rst.............................................................................................Passed
Check for non-standard imports.........................................................................Passed
Check for non-standard numpy.random-related imports excluding pandas/_testing.py.......................Passed
Check for non-standard imports in test suite...........................................................Passed
Check for incorrect code block or IPython directives...................................................Passed
Check for use of not concatenated strings..............................................................Passed
Check for strings with wrong placed spaces.............................................................Passed
Check for import of private attributes across modules..................................................Passed
Check for use of private functions across modules......................................................Passed
Check for use of bare pytest raises....................................................................Passed
Check for inconsistent use of pandas namespace in tests................................................Passed
Check for use of Union[Series, DataFrame] instead of FrameOrSeriesUnion alias..........................Passed
Check for use of foo.__class__ instead of type(foo)....................................................Passed
Check for outdated annotation syntax and missing error codes...........................................Passed
Check for use of np.bool instead of np.bool_...........................................................Passed
Check code for instances of os.remove..................................................................Passed
Check code for instances of pd.api.types...............................................................Passed
Strip unnecessary `# noqa`s............................................................................Passed
Fix End of Files.......................................................................................Passed
Trim Trailing Whitespace...............................................................................Passed
codespell..............................................................................................Passed
no-string-hints........................................................................................Passed
```



</details>

</details>

## Changed files
<details>
<summary>Changed 92 files: </summary>

- .pre-commit-config.yaml
- pandas/_testing/__init__.py
- pandas/compat/pickle_compat.py
- pandas/core/aggregation.py
- pandas/core/algorithms.py
- pandas/core/apply.py
- pandas/core/arrays/_mixins.py
- pandas/core/arrays/base.py
- pandas/core/arrays/boolean.py
- pandas/core/arrays/categorical.py
- pandas/core/arrays/datetimelike.py
- pandas/core/arrays/datetimes.py
- pandas/core/arrays/floating.py
- pandas/core/arrays/integer.py
- pandas/core/arrays/interval.py
- pandas/core/arrays/masked.py
- pandas/core/arrays/numeric.py
- pandas/core/arrays/numpy_.py
- pandas/core/arrays/period.py
- pandas/core/arrays/sparse/array.py
- pandas/core/arrays/sparse/dtype.py
- pandas/core/arrays/string_.py
- pandas/core/arrays/string_arrow.py
- pandas/core/arrays/timedeltas.py
- pandas/core/computation/align.py
- pandas/core/computation/ops.py
- pandas/core/computation/pytables.py
- pandas/core/computation/scope.py
- pandas/core/construction.py
- pandas/core/describe.py
- pandas/core/dtypes/base.py
- pandas/core/dtypes/cast.py
- pandas/core/dtypes/dtypes.py
- pandas/core/frame.py
- pandas/core/generic.py
- pandas/core/groupby/generic.py
- pandas/core/groupby/groupby.py
- pandas/core/groupby/grouper.py
- pandas/core/groupby/ops.py
- pandas/core/indexes/base.py
- pandas/core/indexes/datetimes.py
- pandas/core/indexes/interval.py
- pandas/core/indexes/multi.py
- pandas/core/indexes/period.py
- pandas/core/indexes/range.py
- pandas/core/indexing.py
- pandas/core/internals/array_manager.py
- pandas/core/internals/blocks.py
- pandas/core/internals/concat.py
- pandas/core/internals/construction.py
- pandas/core/internals/managers.py
- pandas/core/internals/ops.py
- pandas/core/missing.py
- pandas/core/ops/__init__.py
- pandas/core/resample.py
- pandas/core/reshape/concat.py
- pandas/core/reshape/melt.py
- pandas/core/reshape/merge.py
- pandas/core/reshape/pivot.py
- pandas/core/reshape/reshape.py
- pandas/core/series.py
- pandas/core/sorting.py
- pandas/core/tools/datetimes.py
- pandas/core/window/ewm.py
- pandas/core/window/rolling.py
- pandas/io/common.py
- pandas/io/excel/_base.py
- pandas/io/excel/_openpyxl.py
- pandas/io/formats/csvs.py
- pandas/io/formats/format.py
- pandas/io/formats/info.py
- pandas/io/formats/style.py
- pandas/io/gbq.py
- pandas/io/json/_normalize.py
- pandas/io/orc.py
- pandas/io/parquet.py
- pandas/io/pytables.py
- pandas/io/sas/sasreader.py
- pandas/io/stata.py
- pandas/plotting/_core.py
- pandas/plotting/_matplotlib/__init__.py
- pandas/plotting/_matplotlib/core.py
- pandas/plotting/_matplotlib/misc.py
- pandas/plotting/_matplotlib/style.py
- pandas/plotting/_matplotlib/timeseries.py
- pandas/plotting/_matplotlib/tools.py
- pandas/tests/extension/arrow/arrays.py
- pandas/tests/extension/arrow/test_timestamp.py
- pandas/tests/extension/decimal/array.py
- pandas/tests/extension/json/array.py
- pandas/tests/extension/list/array.py
- pandas/tests/plotting/common.py

</details>

<hr>

[:octocat: Repo](https://github.com/technote-space/create-pr-action) | [:memo: Issues](https://github.com/technote-space/create-pr-action/issues) | [:department_store: Marketplace](https://github.com/marketplace/actions/create-pr-action)"
759905200,38379,fix series.isin slow issue with Dtype IntegerArray,tushushu,closed,2020-12-09T01:08:46Z,2021-02-01T11:30:00Z,"- [ ] closes #38340 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
797775669,39505,CI: Run pre-commit gh action on only changed files,gunjan-solanki,closed,2021-01-31T18:18:56Z,2021-02-01T13:36:32Z,"
- [x] closes #38862
- [ ] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
-----
Since pre-commit in comment-bot runs on all files, it's a bit slow. Running it only on diff should speed it up significantly."
797523833,39490,Backport PR #39487 on branch 1.2.x (CI: Pin pyarrow to 0.15.1 in 37 macos and linux),meeseeksmachine,closed,2021-01-30T20:27:21Z,2021-02-01T13:43:31Z,Backport PR #39487: CI: Pin pyarrow to 0.15.1 in 37 macos and linux
778327498,38955,BUG: read_excel() fails when checking __version__ of older xlrd versions,kcharlie2,closed,2021-01-04T20:00:06Z,2021-02-01T13:45:48Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### When xlrd is installed but is an ""older"" version (e.g. 1.1.0):

```python
pandas.read_excel(""file.xlsx"", engine=""openpyxl"")
```

raises

```python
AttributeError: module 'xlrd' has no attribute '__version__'
```

This specifically occurs in ``pandas/io/excel/_base.py:336``

#### Problem description

This bug occurs regardless of the ``engine`` input because ``pandas.read_excel()`` _always_ checks ``xlrd.__version__`` if any version of xlrd is installed. Older versions of xlrd use ``__VERSION__`` instead of ``__version__`` instead, which results in the AttributeError.

This exception does not occur and everything behaves as expected if:

- xlrd is not installed
- A ""newer"" version of xlrd is installed (that has the ``__version__`` attribute)


#### Expected Output

According to the documentation, 
```
Changed in version 1.2.0: When engine=None, the following logic will be used to determine the engine:
    
- If path_or_buffer is an OpenDocument format (.odf, .ods, .odt), then odf will be used.
- Otherwise if path_or_buffer is an xls format, xlrd will be used.
- Otherwise if openpyxl is installed, then openpyxl will be used.
- Otherwise if xlrd >= 2.0 is installed, a ValueError will be raised.
- Otherwise xlrd will be used and a FutureWarning will be raised. This case will raise a ValueError in a future version of pandas.

```

Based on this, I believe the intended behavior is:
- read_excel() should _not_ check the xlrd version if engine is ``""openpyxl""``
- If read_excel() does need to know the version of xlrd, it should try both ``__version__`` and ``__VERSION__``

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1062.4.1.el7.x86_64
Version          : #1 SMP Wed Sep 25 09:42:57 EDT 2019
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US
LOCALE           : en_US.ISO8859-1

pandas           : 1.2.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 40.8.0
Cython           : None
pytest           : 6.0.1
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.1.0
xlwt             : None
numba            : None


</details>
"
798274607,39526,CI: pin numpy for CI / Checks github action,simonjayhawkins,closed,2021-02-01T11:49:21Z,2021-02-01T13:48:52Z,"failures with numpy 1.20

https://github.com/pandas-dev/pandas/pull/39517/checks?check_run_id=1803471267

xref #39513 and https://github.com/pandas-dev/pandas/pull/38379#discussion_r567753143

conda-forge should pick up 1.19.5 with these changes."
788218893,39250,BUG: read_excel fails with IndexError: list index out of range,YarShev,closed,2021-01-18T12:14:22Z,2021-02-01T13:49:12Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas
df = pandas.read_excel(""excel_sheetname_title.xlsx"")
IndexError: list index out of range
```
<details>

pandas           : 1.2.0
numpy            : 1.19.2
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.1.2.post20210112
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.6
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 2.0.1
xlwt             : None
numba            : None

</details>

The file which read_excel fails on.
[excel_sheetname_title.xlsx](https://github.com/pandas-dev/pandas/files/5830079/excel_sheetname_title.xlsx)"
798360791,39529,Backport PR #39526 on branch 1.2.x (CI: pin numpy for CI / Checks github action),meeseeksmachine,closed,2021-02-01T13:39:58Z,2021-02-01T14:48:39Z,Backport PR #39526: CI: pin numpy for CI / Checks github action
785514736,39156,CI: Mark network test as xfail,phofl,closed,2021-01-13T22:52:12Z,2021-01-13T23:41:29Z,"- [x] xref #39155
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

https://github.com/pandas-dev/pandas/issues/39155#issuecomment-759783358
"
785536063,39157,Backport PR #39156 on branch 1.2.x (CI: Mark network test as xfail),meeseeksmachine,closed,2021-01-13T23:40:30Z,2021-01-14T00:29:42Z,Backport PR #39156: CI: Mark network test as xfail
783965192,39121,REF: eliminate inner functions in describe,ivanovmg,closed,2021-01-12T06:50:56Z,2021-01-14T07:07:42Z,"- [ ] xref #36833
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

Remove inner functions in ``describe_ndframe``, while passing the required arguments."
781647839,39029,BUG: read_csv does not close file during an error in _make_reader,twoertwein,closed,2021-01-07T22:14:19Z,2021-01-14T11:05:23Z,"- [x] closes #39024
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

I don't understand why `td.check_file_leaks` doesn't complain about the left opened file (at least for me locally). I commented the close call on purpose to see whether the test fails at least for the CI.

@jbrockmendel I think you have debugged ResourceWarnings in the past. Do you know why the test doesn't fail? Even putting a `open(""foo"", mode=""w"")` in the test doesn't make it fail.

[The test case is different from  #39024 but the symptoms are the same. Unless the except clause is narrowed down to specific exceptions, this PR will fix #39024]"
784124131,39125,January 2021 Developer Meeting,jorisvandenbossche,closed,2021-01-12T10:48:13Z,2021-01-14T14:22:36Z,"The monthly dev meeting is Wednesday January 13th, at 18:00 UTC (12 am Central). Our calendar is at https://pandas.pydata.org/docs/development/meeting.html#calendar to check your local time.

Video Call:  https://zoom.us/j/942410248?pwd=T2l2Qi9vaC82Z294ZEtFczYxMVM2dz09
Minutes: https://docs.google.com/document/u/1/d/1tGbTiYORHiSPgVMXawiweGJlBw5dOkVJLY-licoBmBU/edit?ouid=102771015311436394588&usp=docs_home&ths=true

Please add items you'd like to see discussed to the agenda. All are welcome to attend.

@pandas-dev/pandas-core @pandas-dev/pandas-triage "
785241107,39149,REF: implement is_exact_match,jbrockmendel,closed,2021-01-13T16:14:38Z,2021-01-14T15:33:08Z,Working to smooth out some inconsistencies in setitem between code paths.
785411446,39153,BUG: Minor fix setitem_cache_updating tests,rosagold,closed,2021-01-13T20:16:26Z,2021-01-14T15:56:19Z,"BUG: tiny fix in tests, i guess from rapid development ;) -> https://github.com/pandas-dev/pandas/issues/5424#issuecomment-27658793
"
785268948,39150,DOC: 1.3 release notes,simonjayhawkins,closed,2021-01-13T16:49:36Z,2021-01-14T15:58:26Z,
786108608,39173,ENH: Undeprecate and reimplement DataFrame.lookup,impredicative,closed,2021-01-14T16:00:07Z,2021-01-14T16:04:27Z,"#### Is your feature request related to a problem?

I seriously get the impression that Pandas is actively being sabotaged. As a case in point, `DataFrame.lookup` was deprecated in v1.2.0. The problems with this are:

1. The [doc for `DataFrame.lookup`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.lookup.html) says to see itself for an example. Well, there is no example in the doc page. The only example I'm aware of is [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#looking-up-values-by-index-column-labels) instead which is a different page.
2. The [changelog](https://pandas.pydata.org/docs/whatsnew/v1.2.0.html#deprecations) for this deprecation points to [GH18682](https://github.com/pandas-dev/pandas/pull/18682) which looks to be wholly irrelevant to this change. A lot has gone wrong here.
3. There is significant user code that already uses `lookup`. Why break it? If the current implementation of `lookup` is suboptimal, shouldn't it be optimized instead?
4. It is extremely complicated to use `melt` when `lookup` works quite simply. For example, compare [this simple answer using `lookup`](https://stackoverflow.com/a/45487394/) with [this complicated answer using `melt`](https://stackoverflow.com/a/65722008/).

Does nobody review changes, docs, and release notes anymore prior to release? It looks this way.

#### Describe the solution you'd like

1. Undeprecate `lookup`.
2. Optimize `lookup` if attainable using `melt`.
"
435546924,26181,pandas.rolling.apply skips calling function if window contains any NaN,AHG123,closed,2019-04-21T20:53:16Z,2021-01-14T18:15:23Z,"This issue has been raised several times, but I do not think has been adequately addressed. 

The solution given is always min_periods, but that only includes the obvious issue where the NaN is at the beginning of the series. If there is any NaN then .apply will not call the function.

There should be a skipna=True option that works. Despite this question being asked in a dozen places, it has not been fixed. 

I see that @jreback has closed this issue multiple times with a reference to min_periods. Again... this is NOT an adequate answer. (See the followup here https://github.com/pandas-dev/pandas/issues/19218 that was ignored once the issue was marked closed.)

There should simply be a skipna=True flag in the rolling window function. As-is, it will not call the function in .apply() if there are ANY NaNs at any point in the window."
625868961,34411,BUG: pd.read_sql returns empty list if query has no results and chunksize is set,JohanKahrstrom,closed,2020-05-27T17:02:36Z,2021-01-14T18:52:47Z,"- [ x] I have checked that this issue has not already been reported.

- [ x] I have confirmed this bug exists on the latest version of pandas.

- [x ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import sqlite3

# Create empty test table in memory
conn = sqlite3.connect(':memory:')
conn.cursor().execute('CREATE TABLE test (column_1 INTEGER);')

# Run the query without chunksize, works as expected
pd.read_sql('select * from test', conn)

# Run the query with chunksize, returns generator as expected
pd.read_sql('select * from test', conn, chunksize=5)

# However, the generator is empty
list(pd.read_sql('select * from test', conn, chunksize=5))

# I would expect, that for all cases where chunksize isn't necessary,
# than the following two lines would return exactly the same
# result, but the second throws ""ValueError: No objects to concatenate""
pd.read_sql('select * from test', conn)

pd.concat(pd.read_sql('select * from test', conn, chunksize=5))
```


#### Problem description

In many cases, returning zero rows is an expected result, and the code should run fine on the returned DataFrame (iterating over it, getting all values in a row, etc.).

The current behaviour instead returns an empty list, with no information about for example the columns in the dataframe.

#### Expected Output

The expected output would be a list containing a single empty dataframe, with the correct column metadata. I would expect that for all queries that run fine without chunkbite being set, the following equality should hold:

```
pd.testing.assert_frame_equal(
    pd.read_sql(query, conn),
    pd.concat(pd.read_sql(query, conn, chunksize=5))
)
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 333db4b765f8e88c0c2392943cb7d6c6013dc6e8
python           : 3.8.2.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Thu Jan 23 06:52:12 PST 2020; root:xnu-4903.278.25~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.0.dev0+1685.g333db4b76.dirty
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 46.4.0.post20200518
Cython           : 0.29.19
pytest           : 5.4.2
hypothesis       : 5.16.0
sphinx           : 3.0.4
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.14.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fastparquet      : 0.4.0
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0

</details>
"
626447697,34429,Fix read_sql empty result with chunksize bug GH34411,JohanKahrstrom,closed,2020-05-28T11:52:29Z,2021-01-14T18:52:51Z,"- [x ] closes #34411
- [ x] tests added / passed
- [ x] passes `black pandas`
- [ x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ x] whatsnew entry

`pd.read_sql` was returning an empty generator when chunksize is set and the query returns zero results. Now it correctly returns a generator with a single empty DataFrame (:issue:`34411`)."
645472762,34987,BUG: Cannot create third-party ExtensionArrays for datetime types (xfail),xhochy,closed,2020-06-25T11:05:58Z,2021-01-14T18:59:14Z,"- [x] closes #34986
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
779698674,38982,REGR: Bug fix for ExtensionArray groupby aggregation on non-numeric types,BryanCutler,closed,2021-01-05T23:02:23Z,2021-01-14T19:08:56Z,"- [X] closes #38980 
- [x] tests added / passed
- [X] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
784757676,39140,ENH: Allow apply/agg to accept list-like and dict-like,rhshadrach,closed,2021-01-13T03:32:56Z,2021-01-14T20:48:36Z,"```
s = pd.Series([1, 2])
df = pd.DataFrame([1, 2])

# Reducers: list-like
ops = np.array([np.sum, np.mean])
print(df.agg(ops))
print(s.agg(ops))
print(df.apply(ops))
print(s.apply(ops))  # Raises

# Reducers: dict-like
ops = pd.Series({0: np.mean})
print(df.agg(ops))  # Raises
print(s.agg(ops))  # Raises
print(df.apply(ops))  # Raises
print(s.apply(ops))  # Raises

# Transforms - list-like
ops = np.array([np.abs, np.sqrt])
print(df.transform(ops))
print(df.apply(ops))
print(s.transform(ops))
print(s.apply(ops))  # Raises

# Transforms - dict-like
ops = pd.Series({0: np.abs})
print(df.transform(ops))
print(s.transform(ops))
print(s.apply(ops))  # Raises
print(df.apply(ops))  # Raises
```

All lines that don't raise give the expected output."
784638411,39136,BUG: setting categorical values into object dtype DataFrame,jbrockmendel,closed,2021-01-12T22:24:38Z,2021-01-14T21:54:22Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

```
df = pd.DataFrame({""A"": range(3)}, dtype=object)
cat = pd.Categorical([""alpha"", ""beta"", ""gamma""])

df.iloc[range(3), 0] = cat

>>> df
     A
0  NaN
1     
2     

```"
786361908,39177,CI: Pin nbformat to 5.0.8,phofl,closed,2021-01-14T21:44:04Z,2021-01-14T22:29:08Z,"- [x] xref #39176

cc @jreback nbformat should be enoguh I hope."
786386460,39179,Backport PR #39177 on branch 1.2.x (CI: Pin nbformat to 5.0.8),meeseeksmachine,closed,2021-01-14T22:28:58Z,2021-01-14T23:24:37Z,Backport PR #39177: CI: Pin nbformat to 5.0.8
784777672,39141,ENH: Allow Series.apply to accept list-like and dict-like,rhshadrach,closed,2021-01-13T04:30:13Z,2021-01-15T01:27:53Z,"- [x] closes #39140
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

I only added the Series.apply list/dict-like addition to the whatsnew. For both Series and DataFrame .agg, this PR also enables passing a Series as a dict-like, but other dict-likes already work. It didn't seem notable to add this to the whatsnew."
786088425,39170,REF: extract more functions in pandas/core/describe.py,ivanovmg,closed,2021-01-14T15:35:23Z,2021-01-15T10:16:56Z,"- [ ] xref #36833
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

Extracted ``describe_series``, ``describe_frame`` and ``select_column`` in ``pandas/core/describe.py``.
The next time I think I will introduce the classes ``SeriesDescriber`` and ``DataFrameDescriber`` and will start moving the functions into the methods incrementally, if that is OK."
785761275,39165,REF/TYP: extract function/type args in describe.py,ivanovmg,closed,2021-01-14T07:40:16Z,2021-01-15T12:14:26Z,"- [ ] xref #36833
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

Extracted function ``_refine_percentiles`` and typed function arguments in ``pandas/core/describe.py``."
782874163,39093,"TYP: NDFrame.pipe, GroupBy.pipe etc.",topper-123,closed,2021-01-10T18:10:43Z,2021-01-15T14:05:49Z,"Type up `pipe` function and methods. This allows type checkers to understand the return value type from pipes, which is nice.
"
785171933,39147,REGR: Assigning multiple new columns with loc fails when index is a MultiIndex,gdiepen,closed,2021-01-13T14:52:49Z,2021-01-15T14:14:18Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

rows = [(1,2), (3,4)]
initial_cols = [""a"", ""b""]

df = pd.DataFrame(42, index=pd.MultiIndex.from_tuples(rows), columns=initial_cols)

new_cols = [""c"", ""d""]
df.loc[:, new_cols] = None
```

#### Problem description

When running the above code in pandas 1.1.5, you will get the following error:
```
  File ""foo.py"", line 11, in <module>
    df.loc[:, new_cols] = None
  File ""/home/guido/anaconda3/envs/pandas_115_bug/lib/python3.7/site-packages/pandas/core/indexing.py"", line 666, in __setitem__
    indexer = self._get_setitem_indexer(key)
  File ""/home/guido/anaconda3/envs/pandas_115_bug/lib/python3.7/site-packages/pandas/core/indexing.py"", line 609, in _get_setitem_indexer
    return self._convert_tuple(key, is_setter=True)
  File ""/home/guido/anaconda3/envs/pandas_115_bug/lib/python3.7/site-packages/pandas/core/indexing.py"", line 734, in _convert_tuple
    idx = self._convert_to_indexer(k, axis=i, is_setter=is_setter)
  File ""/home/guido/anaconda3/envs/pandas_115_bug/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1198, in _convert_to_indexer
    return self._get_listlike_indexer(key, axis, raise_missing=True)[1]
  File ""/home/guido/anaconda3/envs/pandas_115_bug/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1254, in _get_listlike_indexer
    self._validate_read_indexer(keyarr, indexer, axis, raise_missing=raise_missing)
  File ""/home/guido/anaconda3/envs/pandas_115_bug/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1298, in _validate_read_indexer
    raise KeyError(f""None of [{key}] are in the [{axis_name}]"")
KeyError: ""None of [Index(['c', 'd'], dtype='object')] are in the [columns]""
```
Till version 1.1.4, the above code would result in the dataframe being extended with the two new columns c and d. With the release of pandas 1.1.5, the code results in the above error. It might be related to the fix for #37711

Single column assignments do still work, i.e. replacing the single assignment line with the following two separate assignment lines does work:
```python
df.loc[:, ""c""] = None
df.loc[:, ""d""] = None
```

The problem also only occurs whenever the index is a multiindex. If the index is not a multiindex, the error will not occur.

#### Expected Output
Expected output is that no error occurs and that the two columns c and d are added with values None for all rows.


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.7.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.8.0-33-generic
Version          : #36-Ubuntu SMP Wed Dec 9 09:14:40 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.5
numpy            : 1.19.2
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.1.2.post20210112
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None


</details>
"
783844819,39120,BUG: Series.__setitem__ with mismatched IntervalDtype,jbrockmendel,closed,2021-01-12T02:00:14Z,2021-01-15T15:18:04Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
786927565,39187,Backport PR #39161 on branch 1.2.x (Fix regression in loc setitem raising KeyError when enlarging df with multiindex),meeseeksmachine,closed,2021-01-15T14:15:10Z,2021-01-15T15:21:15Z,Backport PR #39161: Fix regression in loc setitem raising KeyError when enlarging df with multiindex
785545073,39161,Fix regression in loc setitem raising KeyError when enlarging df with multiindex,phofl,closed,2021-01-14T00:03:38Z,2021-01-15T16:04:31Z,"- [x] closes #39147
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

The previous fix only viewed row indexers not column indexers"
36085695,7509,combine_first not retaining dtypes,altaurog,closed,2014-06-19T14:53:55Z,2021-01-15T16:30:22Z,"I found a number of issues that seemed related, all closed over a year ago, but there still seem to be some inconsistencies here:

``` python
In [1]: from datetime import datetime
In [2]: import pandas as pd
In [3]: pd.__version__
Out[3]: '0.13.1'
In [4]: dfa = pd.DataFrame([[datetime.now(), 2]], columns=['a','b'])
In [5]: dfb = pd.DataFrame([[4],[5]], columns=['b'])
In [6]: dfa.dtypes
Out[6]: 
a    datetime64[ns]
b             int64
dtype: object
In [7]: dfb.dtypes
Out[7]: 
b    int64
dtype: object
In [8]: # int64 becomes float64 when combining the two frames
In [9]: dfa.combine_first(dfb).dtypes
Out[9]: 
a    datetime64[ns]
b           float64
dtype: object
In [10]: # datetime64[ns] becomes float64 if the first frame is empty
In [11]: dfa.iloc[:0].combine_first(dfb).dtypes
Out[11]: 
a    float64
b      int64
dtype: object
```
"
782561585,39051,ENH: try to preserve the dtype on combine_first for the case where the two DataFrame objects have the same columns,danielhrisca,closed,2021-01-09T08:27:59Z,2021-01-15T16:30:28Z,"…e two DataFrame objects have the same columns

- [ ] closes #7509 
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
745325097,37933,ENH: make closed part of IntervalDtype,jbrockmendel,closed,2020-11-18T04:45:09Z,2021-01-15T18:09:57Z,"- [x] closes #19371
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @jschendel

Still need to figure out what to do for backwards compat for unpickled IntervalDtype objects; ATM this just sets closed=None.

This causes IntervalIndex.append to cast to object dtype instead of raising if we try to append a non-matching closed.  I think thats the right thing to do, but we haven't fully discussed that xref #37774"
785211760,39148,"BUG: Cells containing ""NA"" (string) in excel is imported as NaN",michamich,closed,2021-01-13T15:40:18Z,2021-01-15T19:13:02Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

test_dict = {
    ""TestInput"": [""NA"", ""na"", ""N.A.""],
    ""TestCol2"": [1234, ""1234"", """"]
}

test_df = pd.DataFrame(test_dict)
print(test_df)

file_path = ""./test-input.xlsx""
test_df.to_excel(file_path, index=False)
input_df = pd.read_excel(file_path, dtype=""object"")
print(input_df)
```

#### Problem description
The string ""NA"" is imported as NaN instead of a string.

During DataFrame creation, the top-left cell contains the string ""NA"". However, when the same DataFrame was imported from excel, it is imported as NaN, as if the cell was blank.

Screenshot of output from jupyter below:
<img width=""526"" alt=""Screenshot 2021-01-13 at 11 36 59 PM"" src=""https://user-images.githubusercontent.com/35208548/104473723-40106a00-55f8-11eb-8b1e-958ae39a675d.png"">

#### Expected Output

The imported DataFrame should be identical to the DataFrame that was initially created with ""NA"" imported a string and not NaN.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 20.2.0
Version          : Darwin Kernel Version 20.2.0: Wed Dec  2 20:40:21 PST 2020; root:xnu-7195.60.75~1/RELEASE_ARM64_T8101
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.2.0
numpy            : 1.19.2
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.1.2.post20210112
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
787041369,39190,"BUG: pd.to_csv(..., compression=""zip"") creates multiple files in zip archive that cannot be read by pd.read_csv()",tomstesco,closed,2021-01-15T16:54:39Z,2021-01-15T19:15:50Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

# enough data to cause chunking into multiple files
n_data = 100000
df = pd.DataFrame(
    {'name': [""Raphael""]*n_data,
    'mask': [""red""]*n_data,
    'weapon': [""sai""]*n_data,
    }
)

compression_opts = dict(method='zip', archive_name='out.csv')
df.to_csv('out.csv.zip', index=False, compression=compression_opts)

# reading back the data produces an error
r_df = pd.read_csv(""out.csv.zip"")

# passing in compression_opts doesn't work either
r_df = pd.read_csv(""out.csv.zip"", compression=compression_opts)
```

How pandas was installed:

```bash
~/projects/testing$ ~/.pyenv/versions/3.8.6/bin/python3.8 -m venv text_venv
~/projects/testing$ . text_venv/bin/activate
(text_venv) ~/projects/testing$ pip install pandas==1.2.0
```

#### Problem description

Introduced in 1.2.0, `to_csv` with compression=""zip"" has an issue with chunking of data into multiple data files within the zip archive, when reading back the data this produces an error. When the data is very small (for example if `n_data` above was 2), this produces only one data file in the archive and no error when reading the archive.

This has been reported by other users on StackOverflow here: https://stackoverflow.com/questions/65689941/when-using-pandas-dataframe-to-csv-with-compression-zip-it-creates-a-zip-f

#### Expected Output

The `to_csv` should work with `read_csv` using zip compression. The behaviour with small data and medium sized data should be the same. The archive should always contain a single file or be read into a single df.

#### Output of ``pd.show_versions()``

<details>

Python 3.8.6 (default, Nov 25 2020, 11:35:37)
[Clang 10.0.1 (clang-1001.0.46.4)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas as pd
>>> pd.show_versions()
/Users/tom.s/projects/testing/text_venv/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.
  warnings.warn(

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Tue Nov 10 00:07:31 PST 2020; root:xnu-4903.278.51~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0
numpy            : 1.19.5
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 49.2.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
562162741,31819,pandas 1.0.1 read_csv() is broken for some file-like objects,sasanquaneuf,closed,2020-02-09T11:33:52Z,2021-01-15T20:42:10Z,"#### Code Sample

```python
import os
import pandas
import tempfile
import traceback

# pandas.show_versions()

fname = ''
with tempfile.NamedTemporaryFile(delete=False) as f:
    f.write('てすと\nこむ'.encode('shift-jis'))
    f.seek(0)
    fname = f.name

    try:
        result = pandas.read_csv(f, encoding='shift-jis')
        print('read shift-jis')
        print(result)

    except Exception as e:
        print(e)
        print(traceback.format_exc())

os.unlink(fname)

```
#### Problem description

Pandas 1.0.1, this sample does not work. But pandas 0.25.3, this sample works fine.
As stated in [issue #31575](https://github.com/pandas-dev/pandas/issues/31575), the encode of file-like object is ignored when its class is not io.BufferedIOBase neither RawIOBase.
However, some file-like objects are NOT inherited one of them, although the ""actual"" inner object is one of them.
In this code sample case, [according to the cpython implementation](https://github.com/python/cpython/blob/master/Lib/tempfile.py#L450), they has file as their attribute `self.file = file`, and `__getattr__()` returns the file's attribute as their attribute.
So the code is not work. The identic problems are in other file-like objects, for example, tempfile.*File class, werkzeug's FileStorage class, and so on.

**Note**: I first recognized this problem with using pandas via flask's posted file. The file-like object is an instance of werkzeug's FileStorage. I avoided this problem with following code:

```python
pandas.read_csv(request.files['file'].stream._file, encoding='shift-jis')
```

#### Expected Output

```
read shift-jis
  てすと
0  こむ
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.138-89.102.amzn1.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : ja_JP.UTF-8
LOCALE           : ja_JP.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 9.0.3
setuptools       : 36.2.7
Cython           : None
pytest           : 3.6.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.0.5
lxml.etree       : 4.2.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : None
pandas_datareader: None
bs4              : 4.6.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.1
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 3.6.2
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.4
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.1.0
xlwt             : None
xlsxwriter       : 1.0.5
numba            : None

</details>
"
776991851,38851,"REGR: fillna on datetime64[ns, UTC] column hits RecursionError",akrherz,closed,2020-12-31T13:34:39Z,2021-01-16T01:09:21Z,"#### Code Sample

```python
from datetime import datetime, timezone
import pandas as pd

df = pd.DataFrame(range(31))
df[""dt""] = pd.date_range(""2020/12/01"", ""2020/12/31"", tz=""UTC"")
df[""dt""].iloc[5] = pd.NaT
df[""dt""] = df[""dt""].fillna(datetime(1980, 1, 1, tzinfo=timezone.utc))
```

#### Problem description

pandas 1.2.0 is throwing a RecursionError while attempting to `fillna` on a `datetime64[ns, UTC]` column.  Same code worked in prior releases.

```
raceback (most recent call last):
  File ""recursion.py"", line 7, in <module>
    df[""dt""] = df[""dt""].fillna(utc(1980, 1, 1, tzinfo=timezone.utc))
  File ""/opt/miniconda3/envs/prod/lib/python3.8/site-packages/pandas/core/series.py"", line 4433, in fillna
    return super().fillna(
  File ""/opt/miniconda3/envs/prod/lib/python3.8/site-packages/pandas/core/generic.py"", line 6403, in fillna
    new_data = self._mgr.fillna(
  File ""/opt/miniconda3/envs/prod/lib/python3.8/site-packages/pandas/core/internals/managers.py"", line 621, in fillna
    return self.apply(
  File ""/opt/miniconda3/envs/prod/lib/python3.8/site-packages/pandas/core/internals/managers.py"", line 427, in apply
    applied = getattr(b, f)(**kwargs)
  File ""/opt/miniconda3/envs/prod/lib/python3.8/site-packages/pandas/core/internals/blocks.py"", line 461, in fillna
    return self.split_and_operate(None, f, inplace)
  File ""/opt/miniconda3/envs/prod/lib/python3.8/site-packages/pandas/core/internals/blocks.py"", line 516, in split_and_operate
    nv = f(mask, new_values, None)
  File ""/opt/miniconda3/envs/prod/lib/python3.8/site-packages/pandas/core/internals/blocks.py"", line 459, in f
    return block.fillna(value, limit=limit, inplace=inplace, downcast=None)
  File ""/opt/miniconda3/envs/prod/lib/python3.8/site-packages/pandas/core/internals/blocks.py"", line 461, in fillna
    return self.split_and_operate(None, f, inplace)
  File ""/opt/miniconda3/envs/prod/lib/python3.8/site-packages/pandas/core/internals/blocks.py"", line 516, in split_and_operate
    nv = f(mask, new_values, None)
  File ""/opt/miniconda3/envs/prod/lib/python3.8/site-packages/pandas/core/internals/blocks.py"", line 459, in f
    return block.fillna(value, limit=limit, inplace=inplace, downcast=None)
...snipped...
  File ""/opt/miniconda3/envs/prod/lib/python3.8/site-packages/pandas/core/internals/blocks.py"", line 441, in fillna
    if self._can_hold_element(value):
  File ""/opt/miniconda3/envs/prod/lib/python3.8/site-packages/pandas/core/internals/blocks.py"", line 2267, in _can_hold_element
    tipo = maybe_infer_dtype_type(element)
  File ""/opt/miniconda3/envs/prod/lib/python3.8/site-packages/pandas/core/dtypes/cast.py"", line 924, in maybe_infer_dtype_type
    elif is_list_like(element):
  File ""pandas/_libs/lib.pyx"", line 1033, in pandas._libs.lib.is_list_like
  File ""pandas/_libs/lib.pyx"", line 1038, in pandas._libs.lib.c_is_list_like
  File ""/opt/miniconda3/envs/prod/lib/python3.8/abc.py"", line 98, in __instancecheck__
    return _abc_instancecheck(cls, instance)
RecursionError: maximum recursion depth exceeded in comparison

```

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

pd.show_versions()
/opt/miniconda3/envs/prod/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.18.0-240.8.1.el8_3.x86_64
Version          : #1 SMP Fri Dec 4 12:24:03 EST 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0
numpy            : 1.19.4
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 50.0.0.post20200830
Cython           : 0.29.21
pytest           : 6.2.1
hypothesis       : None
sphinx           : 3.4.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.2
html5lib         : 1.1
pymysql          : None
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : None
fsspec           : 0.8.5
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.3
sqlalchemy       : 1.3.22
tables           : None
tabulate         : None
xarray           : 0.16.2
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.52.0

</details>

Thank you."
779544403,38979,BUG: Different results from DataFrame.apply and str accessor,pLeBlanc93,closed,2021-01-05T20:40:23Z,2021-01-16T01:12:46Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd

print(pd.__version__)

df = pd.DataFrame(zip('abc', 'def'))
print(df.apply(lambda f: ""/"".join(f), axis=1))
print(df.apply(lambda f: ""/"".join(f.str.upper()), axis=1))
```

#### Problem description
Using the `str` accessor gives different results on 1.1.5 and 1.2.x:
```
1.1.5
0    a/d
1    b/e
2    c/f
dtype: object
0    A/D
1    B/E
2    C/F
dtype: object

1.2.0+22.g6cdb4e7fb
0    a/d
1    b/e
2    c/f
dtype: object
0    A/D
1    A/D
2    A/D
dtype: object

```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 6cdb4e7fbf4ba9a8fc83af550d866765f64ecdd9
python           : 3.7.9.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 63 Stepping 2, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.2.0+22.g6cdb4e7fb
numpy            : 1.19.5
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.0.0.post20201207
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
787250816,39196,REGR: NaT.__richmp__(dateobj),jbrockmendel,closed,2021-01-15T22:24:18Z,2021-01-16T01:49:33Z,"- [x] closes #39151
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
787226648,39195,TST/REF: split large categorical indexing test,jbrockmendel,closed,2021-01-15T21:52:18Z,2021-01-16T02:03:13Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
787312277,39198,"Backport PR #39191 on branch 1.2.x (Revert ""BUG/REG: RollingGroupby MultiIndex levels dropped (#38737)"")",meeseeksmachine,closed,2021-01-16T01:11:24Z,2021-01-16T02:14:58Z,"Backport PR #39191: Revert ""BUG/REG: RollingGroupby MultiIndex levels dropped (#38737)"""
787312698,39199,Backport PR #39188 on branch 1.2.x (REGR: Different results from DataFrame.apply and str accessor),meeseeksmachine,closed,2021-01-16T01:13:14Z,2021-01-16T02:50:59Z,Backport PR #39188: REGR: Different results from DataFrame.apply and str accessor
775132360,38738,WIP/REF: setitem_with_indexer always use same path for 2D,jbrockmendel,closed,2020-12-28T00:07:50Z,2021-01-16T03:43:21Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

@phofl I'm hoping you're interested in picking up the torch to get this across the finish line.  There are two main places where I think this still needs work

1) being systematic about whether/when we issue a SettingWithCopyWarning
2) perf, as operating column-wise is going to take a toll.  We need to operate block-wise."
450320379,26574,CI: build documentation for Windows,simonjayhawkins,open,2019-05-30T13:55:42Z,2021-01-16T03:50:34Z,"xref https://github.com/pandas-dev/pandas/pull/26499#issuecomment-495074526

documentation is a good place to start for new contributors.

should a doc build be included in CI to ensure that the documentation builds smoothly on Windows?

if so: 
- should a build failure be an allowed failure?
- should it be a separate job or included in the one of the two current windows jobs?"
787062338,39191,"Revert ""BUG/REG: RollingGroupby MultiIndex levels dropped (#38737)""",simonjayhawkins,closed,2021-01-15T17:26:31Z,2021-01-16T09:58:33Z,"This reverts commit a37f1a45e83d7f803d7fcab7d384da28e9c1e714.

reverts PR #38737

I think we ended up deciding to revert on master and backport to keep master and 1.2.x in sync and not release this change in 1.2.1 to allow for further discussion.

once this revert PR merged:
- [x] backport
- [x] the blocker tag can then be removed from #38787
- [x] #38523 will be reopen temporarily
- [ ] @mroeschke will open a new PR with these changes (in draft? milestoned 1.2.2?)

if not these changes, the mutliindex with a single level may still need resolution (and backport) to close #38523

cc @mroeschke @jorisvandenbossche 

add 1.2.1 milestone for this revert for backport.

"
786972726,39188,REGR: Different results from DataFrame.apply and str accessor,simonjayhawkins,closed,2021-01-15T15:17:31Z,2021-01-16T10:02:08Z,"- [ ] closes #38979
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry


@jbrockmendel despite my comment https://github.com/pandas-dev/pandas/issues/38979#issuecomment-760470446, I had no joy trying to locate the caching issue.
"
785292017,39151,BUG: comparison of pandas NaT with datetime.date evaluating to True,arc12,closed,2021-01-13T17:21:11Z,2021-01-16T10:04:01Z,"#### Code Sample, a copy-pastable example
```python
import pandas
import datetime
pandas.NaT < datetime.datetime.now().date()
```

#### Problem description
I understood the convention is for NA-types to evaluate to False in all inequalities. The above code evaluates to True.

These DO evaluate to False, as expected.  
`pandas.NaT < datetime.datetime.now()`  
`pandas.NaT > datetime.datetime.now().date()`

This seems to have been introduced with 1.2.0, and to not have been the behaviour under 1.1.5.

#### Output of ``pd.show_versions()``
```
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.7.9.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.2.0
numpy            : 1.19.2
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.1.2.post20210112
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : 1.3.2
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.22
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.52.0
```"
787313831,39201,Backport PR #39196 on branch 1.2.x (REGR: NaT.__richmp__(dateobj)),meeseeksmachine,closed,2021-01-16T01:18:22Z,2021-01-16T10:06:32Z,Backport PR #39196: REGR: NaT.__richmp__(dateobj)
777248323,38874,REGR: incorrect results with std on rolling window since 1.2.0,cragod,closed,2021-01-01T09:08:54Z,2021-01-16T10:16:50Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample

```python
import pandas as pd
import numpy as np

df = pd.DataFrame({'A': [1e-8, -1.1e-8, 1.2e-8, -1.1e-8] * 5})
df['std'] = df['A'].rolling(10).std(ddof=0)
print(df)
```

#### Problem description

when i ran above code in 1.2.0, the output is:

```
               A               std
0   1.000000e-08  NaN
1  -1.100000e-08  NaN
2   1.200000e-08  NaN
3  -1.100000e-08  NaN
4   1.000000e-08  NaN
5  -1.100000e-08  NaN
6   1.200000e-08  NaN
7  -1.100000e-08  NaN
8   1.000000e-08  NaN
9  -1.100000e-08  0.0
10  1.200000e-08  0.0
11 -1.100000e-08  0.0
12  1.000000e-08  0.0
13 -1.100000e-08  0.0
14  1.200000e-08  0.0
15 -1.100000e-08  0.0
16  1.000000e-08  0.0
17 -1.100000e-08  0.0
18  1.200000e-08  0.0
19 -1.100000e-08  0.0
```

#### Expected Output

ran in 1.1.5:

```
               A           std
0   1.000000e-08           NaN
1  -1.100000e-08           NaN
2   1.200000e-08           NaN
3  -1.100000e-08           NaN
4   1.000000e-08           NaN
5  -1.100000e-08           NaN
6   1.200000e-08           NaN
7  -1.100000e-08           NaN
8   1.000000e-08           NaN
9  -1.100000e-08  1.092200e-08
10  1.200000e-08  1.112160e-08
11 -1.100000e-08  1.112160e-08
12  1.000000e-08  1.092200e-08
13 -1.100000e-08  1.092200e-08
14  1.200000e-08  1.112160e-08
15 -1.100000e-08  1.112160e-08
16  1.000000e-08  1.092200e-08
17 -1.100000e-08  1.092200e-08
18  1.200000e-08  1.112160e-08
19 -1.100000e-08  1.112160e-08
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.7.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 20.2.0
Version          : Darwin Kernel Version 20.2.0: Wed Dec  2 20:39:59 PST 2020; root:xnu-7195.60.75~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : zh_CN.UTF-8
LOCALE           : zh_CN.UTF-8

pandas           : 1.2.0
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 41.2.0
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : 1.8.5
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.15
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
786148073,39174,"CLN,TYP Remove string return annotations",MarcoGorelli,closed,2021-01-14T16:49:32Z,2021-01-16T11:33:14Z,"
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

this was just done with
```python
    parser = argparse.ArgumentParser()
    parser.add_argument('paths', nargs='*', type=Path)
    args = parser.parse_args()
    for path in args.paths:
        text = path.read_text()
        text = re.sub(r'-> ""(\w+)""', '-> \\1', text)
        path.write_text(text)
```

There's other to do (which I'll get to in other PRs), but this already gets rid of a big load of them"
787313206,39200,Backport PR #39193 on branch 1.2.x (CI: Set xfail to strict=False for network tests),meeseeksmachine,closed,2021-01-16T01:15:19Z,2021-01-16T11:37:35Z,Backport PR #39193: CI: Set xfail to strict=False for network tests
787169388,39194,"REGR: fillna on datetime64[ns, UTC] column hits RecursionError",simonjayhawkins,closed,2021-01-15T20:20:22Z,2021-01-16T12:32:09Z,"- [ ] closes #38851
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry

@jbrockmendel this reverts #37040 in it's entirety for backport

feel free to push to this branch if we **need** any of the ancillary cleanup on 1.2.x otherwise can open a new PR to add those cleanups back to master."
787449973,39206,"Backport PR #39194: REGR: fillna on datetime64[ns, UTC] column hits RecursionError",simonjayhawkins,closed,2021-01-16T11:50:02Z,2021-01-16T12:55:13Z,Backport PR #39194
787326196,39204, BUG: assert_frame_equal raising TypeError with check_like and mixed dtype in Index or columns,phofl,closed,2021-01-16T02:18:32Z,2021-01-16T14:59:56Z,"- [x] closes #39168
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

Worked on 1.1.5 for assert_frame_equal and was not supported for assert_index_equal"
787156255,39193,CI: Set xfail to strict=False for network tests,phofl,closed,2021-01-15T19:55:06Z,2021-01-16T15:01:19Z,"- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

This one is ugly. Botocore and boto3 were upgraded tonight to 1.19.55 and 1.16.55. Since aiobotocore requires them to be .52 this is no longer installed, but aiobotocore is an required dependency from s3fs 0.5.*. So this get's downgraded to 0.4.2 were only botocore was a required dependency (no version specification). As soon as the requirements of aiobotocore are updated again this will switch back to s3fs=0.5.2 which will cause the tests to fail again. Hence setting them to strict=False

sorry for the long explanation :)"
787481366,39208,Backport PR #39204:  BUG: assert_frame_equal raising TypeError with check_like and mixed dtype in Index or columns,simonjayhawkins,closed,2021-01-16T14:47:42Z,2021-01-16T15:48:54Z,"
Backport PR #39204"
787552742,39214,QST:compute score across columns meeting a specific conditions,fooms2512,closed,2021-01-16T21:08:12Z,2021-01-16T21:22:27Z,"I have the following dataframe
   Q11_1  Q11_2  Q11_3  Q11_4  Q11_5  Q11_6
      2      2      2      1      1      5   
      2      2      2      3      3      3   
      2      2      2      2      3      3   
      2      2      2      2      4      3   
      4      4      2      4      5      2   

How could I compute the score according to the following rule
if Q11_1 >=3:
score =1 else score =0
then if Q11_2 >=3 then we add 1 else score stay the same
then if Q11_3 >=3 then we add 1 else score stay the same
then if Q11_4 >=4 then we add 1 else score stay the same
then if Q11_5 >=4 then we add 1 else score stay the same
then if Q11_6 >=4 then we add 1 else score stay the same

This would give
   Q11_1  Q11_2  Q11_3  Q11_4  Q11_5  Q11_6  score
      2      2      2      1      1      5   1
      2      2      2      3      3      3   0
      2      2      2      2      3      3   0
      2      2      2      2      4      3   1
      4      4      2      4      5      2   4

thank you for your help. I couldn't find how to handle this


```
"
787534963,39210,TYP: misc annotations,jbrockmendel,closed,2021-01-16T19:21:00Z,2021-01-16T21:29:03Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
718263798,37006,BLD: remove blockslider #34014,fangchenli,closed,2020-10-09T16:07:22Z,2021-01-17T02:37:31Z,"Part of #34014

xref:
https://github.com/ivirshup/pandas/commit/2c17e72a9deffd8e0d8999b88a2e31c0a58c746c
#34997

Impact on groupby
```
       before           after         ratio
     [97877442]       [a82d3773]
     <master^2>       <libreduction-cython>
+     5.07±0.06ms         43.8±1ms     8.65  groupby.Apply.time_scalar_function_single_col
+      15.5±0.2ms        108±0.6ms     6.97  groupby.Apply.time_scalar_function_multi_col
+         453±3ms          508±1ms     1.12  groupby.Apply.time_copy_overhead_single_col
+      1.44±0.03s       1.61±0.01s     1.12  groupby.Apply.time_copy_function_multi_col
-      26.0±0.7ms       23.4±0.2ms     0.90  groupby.Nth.time_frame_nth_any('float64')
```

Impact on index_cached_properties
```
       before           after         ratio
     [97877442]       [a82d3773]
     <master^2>       <libreduction-cython>
+        401±10ns        862±400ns     2.15  index_cached_properties.IndexCache.time_inferred_type('Int64Index')
+        397±10ns        649±200ns     1.64  index_cached_properties.IndexCache.time_is_unique('Int64Index')
+     1.01±0.03μs       1.31±0.2μs     1.30  index_cached_properties.IndexCache.time_values('DatetimeIndex')
+      3.51±0.2μs       4.55±0.8μs     1.29  index_cached_properties.IndexCache.time_shape('CategoricalIndex')
+      1.97±0.2μs       2.30±0.2μs     1.17  index_cached_properties.IndexCache.time_inferred_type('MultiIndex')
+     1.75±0.04μs       2.02±0.2μs     1.15  index_cached_properties.IndexCache.time_shape('DatetimeIndex')
-      1.99±0.3μs       1.58±0.2μs     0.79  index_cached_properties.IndexCache.time_values('UInt64Index')
-      2.33±0.6μs       1.77±0.5μs     0.76  index_cached_properties.IndexCache.time_inferred_type('Float64Index')
```

No significant changes on frame_methods
"
785801092,39168,BUG: assert_frame_equal() with check_like=True errors with non-comparable types,khaeru,closed,2021-01-14T08:48:04Z,2021-01-17T11:24:06Z,"- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of pandas.
- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code sample

```python
import pandas as pd
import pandas.testing as pdt

# Note that df.columns contains both str and int
df = pd.DataFrame([[0, 1, 2]], columns=[""foo"", ""bar"", 42])

pdt.asset_frame_equal(df, df, check_like=True)

```

#### Problem description

This code raises:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-10-05cc1ba40d40> in <module>
----> 1 pdt.assert_frame_equal(df, df, check_like=True)

    [... skipping hidden 2 frame]

~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py in sort_values(self, return_indexer, ascending, na_position, key)
   4664         # ignore na_position for MultiIndex
   4665         if not isinstance(self, ABCMultiIndex):
-> 4666             _as = nargsort(
   4667                 items=idx, ascending=ascending, na_position=na_position, key=key
   4668             )

~/.local/lib/python3.8/site-packages/pandas/core/sorting.py in nargsort(items, kind, ascending, na_position, key, mask)
    365
    366     if is_extension_array_dtype(items):
--> 367         return items.argsort(ascending=ascending, kind=kind, na_position=na_position)
    368     else:
    369         items = np.asanyarray(items)

~/.local/lib/python3.8/site-packages/pandas/core/arrays/base.py in argsort(self, ascending, kind, na_position, *args, **kwargs)
    584
    585         values = self._values_for_argsort()
--> 586         return nargsort(
    587             values,
    588             kind=kind,

~/.local/lib/python3.8/site-packages/pandas/core/sorting.py in nargsort(items, kind, ascending, na_position, key, mask)
    377         non_nans = non_nans[::-1]
    378         non_nan_idx = non_nan_idx[::-1]
--> 379     indexer = non_nan_idx[non_nans.argsort(kind=kind)]
    380     if not ascending:
    381         indexer = indexer[::-1]

TypeError: '<' not supported between instances of 'int' and 'str'
```

The cause is PR #37479, which added the following to `assert_index_equal()`:
```python
    # If order doesn't matter then sort the index entries
    if not check_order:
        left = left.sort_values()
        right = right.sort_values()
```
This is code is triggered by `assert_frame_equal(…, check_like=True)`. `.sort_order()` does not work when an index contains non-comparable types, like `str` and `int`.

Detected via iiasa/ixmp#390.

#### Expected output

In pandas < 1.2.0, the last line above returned `True`.

The description of the `check_like` argument is:
https://github.com/pandas-dev/pandas/blob/25110a92b291de6688c3accad4c34d84837445e8/pandas/_testing/asserters.py#L1127-L1130
…i.e. this does not indicate that the columns index may only contain comparable types, so the function should not raise an exception.


#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.8.0-36-generic
Version          : #40-Ubuntu SMP Tue Jan 5 21:54:35 UTC 2021
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_CA.UTF-8
LOCALE           : en_CA.UTF-8

pandas           : 1.2.0
numpy            : 1.19.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 50.3.2
Cython           : 0.29.21
pytest           : 6.1.2
hypothesis       : None
sphinx           : 3.3.0
blosc            : 1.8.1
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.6.1
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.4
sqlalchemy       : 1.3.19
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.51.2

</details>
"
787688485,39223,Setting diagonal to 1 in corr function,dhalpern,closed,2021-01-17T11:47:20Z,2021-01-17T11:49:36Z,"I often want to use the corr function to also compute distances between columns (e.g. df.corr(method = spicy.spatial.distance.euclidean). However, I noticed that the .corr function always sets the i=j case to 1 (which is incorrect for the distance case) (see [here](https://github.com/pandas-dev/pandas/blob/3e89b4c4b1580aa890023fc550774e63d499da25/pandas/core/frame.py#L8394-L8395)).

I realize using this function to compute distances is sort of a non-standard usage but is there any reason to hardcode it? Would it make more sense as a parameter that could be set?"
787689021,39224,ENH: Setting diagonal to 1 in corr function,dhalpern,closed,2021-01-17T11:50:41Z,2021-01-17T12:01:56Z,"#### Is your feature request related to a problem?

I often want to use the corr function to also compute distances between columns (e.g. df.corr(method = spicy.spatial.distance.euclidean). However, I noticed that the .corr function always sets the i=j case to 1 (which is incorrect for the distance case) (see here).

#### Describe the solution you'd like

I realize using this function to compute distances is sort of a non-standard usage but is there any reason to hardcode it? Would it make more sense as a parameter that could be set?

"
787331064,39205,CI: failing numpy_dev build on 1.2.x,jreback,closed,2021-01-16T02:48:17Z,2021-01-17T13:23:14Z,"this is passing on master, but not on 1.2.1

https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=52437&view=logs&j=eab14f69-13b6-5db7-daeb-7b778629410b&t=ce687173-08c6-5301-838d-71b2dda24510

```
2021-01-16T02:31:05.2414654Z =================================== FAILURES ===================================
2021-01-16T02:31:05.2415844Z _ TestComparison.test_index_series_compat[eq-IntervalIndex-array-assert_numpy_array_equal] _
2021-01-16T02:31:05.2416708Z [gw1] linux -- Python 3.8.5 /home/vsts/miniconda3/envs/pandas-dev/bin/python
2021-01-16T02:31:05.2417059Z 
2021-01-16T02:31:05.2417505Z self = <pandas.tests.arithmetic.test_interval.TestComparison object at 0x7f7984759fa0>
2021-01-16T02:31:05.2420975Z op = <built-in function eq>
2021-01-16T02:31:05.2447033Z constructor = <class 'pandas.core.indexes.interval.IntervalIndex'>
2021-01-16T02:31:05.2461011Z expected_type = <built-in function array>
2021-01-16T02:31:05.2461472Z assert_func = <function assert_numpy_array_equal at 0x7f79d19fb280>
2021-01-16T02:31:05.2461911Z 
2021-01-16T02:31:05.2462190Z     @pytest.mark.parametrize(
2021-01-16T02:31:05.2462598Z         ""constructor, expected_type, assert_func"",
2021-01-16T02:31:05.2462923Z         [
2021-01-16T02:31:05.2463268Z             (IntervalIndex, np.array, tm.assert_numpy_array_equal),
2021-01-16T02:31:05.2463755Z             (Series, Series, tm.assert_series_equal),
2021-01-16T02:31:05.2464080Z         ],
2021-01-16T02:31:05.2464384Z     )
2021-01-16T02:31:05.2464794Z     def test_index_series_compat(self, op, constructor, expected_type, assert_func):
2021-01-16T02:31:05.2465386Z         # IntervalIndex/Series that rely on IntervalArray for comparisons
2021-01-16T02:31:05.2465772Z         breaks = range(4)
2021-01-16T02:31:05.2466225Z         index = constructor(IntervalIndex.from_breaks(breaks))
2021-01-16T02:31:05.2466569Z     
2021-01-16T02:31:05.2466845Z         # scalar comparisons
2021-01-16T02:31:05.2467209Z         other = index[0]
2021-01-16T02:31:05.2467523Z         result = op(index, other)
2021-01-16T02:31:05.2467991Z         expected = expected_type(self.elementwise_comparison(op, index, other))
2021-01-16T02:31:05.2468402Z         assert_func(result, expected)
2021-01-16T02:31:05.2468765Z     
2021-01-16T02:31:05.2469036Z         other = breaks[0]
2021-01-16T02:31:05.2469532Z         result = op(index, other)
2021-01-16T02:31:05.2469988Z         expected = expected_type(self.elementwise_comparison(op, index, other))
2021-01-16T02:31:05.2470394Z         assert_func(result, expected)
2021-01-16T02:31:05.2470739Z     
2021-01-16T02:31:05.2471260Z         # list-like comparisons
2021-01-16T02:31:05.2471642Z         other = IntervalArray.from_breaks(breaks)
2021-01-16T02:31:05.2472056Z         result = op(index, other)
2021-01-16T02:31:05.2472452Z         expected = expected_type(self.elementwise_comparison(op, index, other))
2021-01-16T02:31:05.2472923Z         assert_func(result, expected)
2021-01-16T02:31:05.2473207Z     
2021-01-16T02:31:05.2473879Z         other = [index[0], breaks[0], ""foo""]
2021-01-16T02:31:05.2474269Z         result = op(index, other)
2021-01-16T02:31:05.2474668Z         expected = expected_type(self.elementwise_comparison(op, index, other))
2021-01-16T02:31:05.2475135Z >       assert_func(result, expected)
2021-01-16T02:31:05.2475399Z 
2021-01-16T02:31:05.2475764Z pandas/tests/arithmetic/test_interval.py:287: 
2021-01-16T02:31:05.2476208Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2021-01-16T02:31:05.2476557Z 
2021-01-16T02:31:05.2476986Z left = array([False, False, False]), right = array([ True, False, False])
2021-01-16T02:31:05.2477357Z err_msg = None
2021-01-16T02:31:05.2477641Z 
2021-01-16T02:31:05.2477946Z     def _raise(left, right, err_msg):
2021-01-16T02:31:05.2478307Z         if err_msg is None:
2021-01-16T02:31:05.2478738Z             if left.shape != right.shape:
2021-01-16T02:31:05.2479107Z                 raise_assert_detail(
2021-01-16T02:31:05.2479563Z                     obj, f""{obj} shapes are different"", left.shape, right.shape
2021-01-16T02:31:05.2479942Z                 )
2021-01-16T02:31:05.2480247Z     
2021-01-16T02:31:05.2480506Z             diff = 0
2021-01-16T02:31:05.2480879Z             for left_arr, right_arr in zip(left, right):
2021-01-16T02:31:05.2481340Z                 # count up differences
2021-01-16T02:31:05.2481806Z                 if not array_equivalent(left_arr, right_arr, strict_nan=strict_nan):
2021-01-16T02:31:05.2482323Z                     diff += 1
2021-01-16T02:31:05.2482597Z     
2021-01-16T02:31:05.2482891Z             diff = diff * 100.0 / left.size
2021-01-16T02:31:05.2483349Z             msg = f""{obj} values are different ({np.round(diff, 5)} %)""
2021-01-16T02:31:05.2483814Z >           raise_assert_detail(obj, msg, left, right, index_values=index_values)
2021-01-16T02:31:05.2484450Z E           AssertionError: numpy array are different
2021-01-16T02:31:05.2484795Z E           
2021-01-16T02:31:05.2485284Z E           numpy array values are different (33.33333 %)
2021-01-16T02:31:05.2485649Z E           [left]:  [False, False, False]
2021-01-16T02:31:05.2486175Z E           [right]: [True, False, False]
2021-01-16T02:31:05.2486482Z 
2021-01-16T02:31:05.2486757Z pandas/_testing.py:1159: AssertionError
2021-01-16T02:31:05.2487496Z _ TestComparison.test_index_series_compat[eq-Series-Series-assert_series_equal] _
2021-01-16T02:31:05.2488191Z [gw1] linux -- Python 3.8.5 /home/vsts/miniconda3/envs/pandas-dev/bin/python
2021-01-16T02:31:05.2488567Z 
2021-01-16T02:31:05.2488904Z self = <pandas.tests.arithmetic.test_interval.TestComparison object at 0x7f79840c7730>
2021-01-16T02:31:05.2489563Z op = <built-in function eq>, constructor = <class 'pandas.core.series.Series'>
2021-01-16T02:31:05.2490223Z expected_type = <class 'pandas.core.series.Series'>
2021-01-16T02:31:05.2490622Z assert_func = <function assert_series_equal at 0x7f79d19fb3a0>
2021-01-16T02:31:05.2490958Z 
2021-01-16T02:31:05.2491206Z     @pytest.mark.parametrize(
2021-01-16T02:31:05.2491526Z         ""constructor, expected_type, assert_func"",
2021-01-16T02:31:05.2491885Z         [
2021-01-16T02:31:05.2492201Z             (IntervalIndex, np.array, tm.assert_numpy_array_equal),
2021-01-16T02:31:05.2492660Z             (Series, Series, tm.assert_series_equal),
2021-01-16T02:31:05.2492958Z         ],
2021-01-16T02:31:05.2493334Z     )
2021-01-16T02:31:05.2493713Z     def test_index_series_compat(self, op, constructor, expected_type, assert_func):
2021-01-16T02:31:05.2494196Z         # IntervalIndex/Series that rely on IntervalArray for comparisons
2021-01-16T02:31:05.2494601Z         breaks = range(4)
2021-01-16T02:31:05.2515866Z         index = constructor(IntervalIndex.from_breaks(breaks))
2021-01-16T02:31:05.2516454Z     
2021-01-16T02:31:05.2516755Z         # scalar comparisons
2021-01-16T02:31:05.2517121Z         other = index[0]
2021-01-16T02:31:05.2517451Z         result = op(index, other)
2021-01-16T02:31:05.2518053Z         expected = expected_type(self.elementwise_comparison(op, index, other))
2021-01-16T02:31:05.2518530Z         assert_func(result, expected)
2021-01-16T02:31:05.2518818Z     
2021-01-16T02:31:05.2519159Z         other = breaks[0]
2021-01-16T02:31:05.2519477Z         result = op(index, other)
2021-01-16T02:31:05.2519960Z         expected = expected_type(self.elementwise_comparison(op, index, other))
2021-01-16T02:31:05.2520457Z         assert_func(result, expected)
2021-01-16T02:31:05.2520751Z     
2021-01-16T02:31:05.2521443Z         # list-like comparisons
2021-01-16T02:31:05.2521826Z         other = IntervalArray.from_breaks(breaks)
2021-01-16T02:31:05.2522244Z         result = op(index, other)
2021-01-16T02:31:05.2522645Z         expected = expected_type(self.elementwise_comparison(op, index, other))
2021-01-16T02:31:05.2523109Z         assert_func(result, expected)
2021-01-16T02:31:05.2523399Z     
2021-01-16T02:31:05.2523698Z         other = [index[0], breaks[0], ""foo""]
2021-01-16T02:31:05.2524100Z         result = op(index, other)
2021-01-16T02:31:05.2524497Z         expected = expected_type(self.elementwise_comparison(op, index, other))
2021-01-16T02:31:05.2524958Z >       assert_func(result, expected)
2021-01-16T02:31:05.2525215Z 
2021-01-16T02:31:05.2525528Z pandas/tests/arithmetic/test_interval.py:287: 
2021-01-16T02:31:05.2526028Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2021-01-16T02:31:05.2526680Z pandas/_libs/testing.pyx:46: in pandas._libs.testing.assert_almost_equal
2021-01-16T02:31:05.2527142Z     cpdef assert_almost_equal(a, b,
2021-01-16T02:31:05.2527546Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2021-01-16T02:31:05.2527956Z 
2021-01-16T02:31:05.2528320Z >   raise_assert_detail(obj, msg, lobj, robj, index_values=index_values)
2021-01-16T02:31:05.2528734Z E   AssertionError: Series are different
2021-01-16T02:31:05.2529231Z E   
2021-01-16T02:31:05.2529534Z E   Series values are different (33.33333 %)
2021-01-16T02:31:05.2529923Z E   [index]: [0, 1, 2]
2021-01-16T02:31:05.2530368Z E   [left]:  [False, False, False]
2021-01-16T02:31:05.2530715Z E   [right]: [True, False, False]
2021-01-16T02:31:05.2531028Z 
2021-01-16T02:31:05.2531331Z pandas/_libs/testing.pyx:161: AssertionError
2021-01-16T02:31:05.2532134Z _ TestComparison.test_index_series_compat[ne-IntervalIndex-array-assert_numpy_array_equal] _
2021-01-16T02:31:05.2532992Z [gw1] linux -- Python 3.8.5 /home/vsts/miniconda3/envs/pandas-dev/bin/python
2021-01-16T02:31:05.2533317Z 
2021-01-16T02:31:05.2533705Z self = <pandas.tests.arithmetic.test_interval.TestComparison object at 0x7f7984d5c5e0>
2021-01-16T02:31:05.2534243Z op = <built-in function ne>
2021-01-16T02:31:05.2534860Z constructor = <class 'pandas.core.indexes.interval.IntervalIndex'>
2021-01-16T02:31:05.2535420Z expected_type = <built-in function array>
2021-01-16T02:31:05.2535864Z assert_func = <function assert_numpy_array_equal at 0x7f79d19fb280>
2021-01-16T02:31:05.2536160Z 
2021-01-16T02:31:05.2536407Z     @pytest.mark.parametrize(
2021-01-16T02:31:05.2536807Z         ""constructor, expected_type, assert_func"",
2021-01-16T02:31:05.2537102Z         [
2021-01-16T02:31:05.2537468Z             (IntervalIndex, np.array, tm.assert_numpy_array_equal),
2021-01-16T02:31:05.2537854Z             (Series, Series, tm.assert_series_equal),
2021-01-16T02:31:05.2538200Z         ],
2021-01-16T02:31:05.2538511Z     )
2021-01-16T02:31:05.2538887Z     def test_index_series_compat(self, op, constructor, expected_type, assert_func):
2021-01-16T02:31:05.2606624Z         # IntervalIndex/Series that rely on IntervalArray for comparisons
2021-01-16T02:31:05.2607151Z         breaks = range(4)
2021-01-16T02:31:05.2687364Z         index = constructor(IntervalIndex.from_breaks(breaks))
2021-01-16T02:31:05.2687833Z     
2021-01-16T02:31:05.2688104Z         # scalar comparisons
2021-01-16T02:31:05.2688414Z         other = index[0]
2021-01-16T02:31:05.2688719Z         result = op(index, other)
2021-01-16T02:31:05.2689117Z         expected = expected_type(self.elementwise_comparison(op, index, other))
2021-01-16T02:31:05.2689504Z         assert_func(result, expected)
2021-01-16T02:31:05.2689783Z     
2021-01-16T02:31:05.2690033Z         other = breaks[0]
2021-01-16T02:31:05.2690325Z         result = op(index, other)
2021-01-16T02:31:05.2690722Z         expected = expected_type(self.elementwise_comparison(op, index, other))
2021-01-16T02:31:05.2691115Z         assert_func(result, expected)
2021-01-16T02:31:05.2691394Z     
2021-01-16T02:31:05.2692145Z         # list-like comparisons
2021-01-16T02:31:05.2692512Z         other = IntervalArray.from_breaks(breaks)
2021-01-16T02:31:05.2692846Z         result = op(index, other)
2021-01-16T02:31:05.2693221Z         expected = expected_type(self.elementwise_comparison(op, index, other))
2021-01-16T02:31:05.2693623Z         assert_func(result, expected)
2021-01-16T02:31:05.2693887Z     
2021-01-16T02:31:05.2694180Z         other = [index[0], breaks[0], ""foo""]
2021-01-16T02:31:05.2694546Z         result = op(index, other)
2021-01-16T02:31:05.2694934Z         expected = expected_type(self.elementwise_comparison(op, index, other))
2021-01-16T02:31:05.2695322Z >       assert_func(result, expected)
2021-01-16T02:31:05.2695556Z 
2021-01-16T02:31:05.2695863Z pandas/tests/arithmetic/test_interval.py:287: 
2021-01-16T02:31:05.2696276Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2021-01-16T02:31:05.2696602Z 
2021-01-16T02:31:05.2696918Z left = array([ True,  True,  True]), right = array([False,  True,  True])
2021-01-16T02:31:05.2697265Z err_msg = None
2021-01-16T02:31:05.2697480Z 
2021-01-16T02:31:05.2697762Z     def _raise(left, right, err_msg):
2021-01-16T02:31:05.2698114Z         if err_msg is None:
2021-01-16T02:31:05.2698459Z             if left.shape != right.shape:
2021-01-16T02:31:05.2698803Z                 raise_assert_detail(
2021-01-16T02:31:05.2699198Z                     obj, f""{obj} shapes are different"", left.shape, right.shape
2021-01-16T02:31:05.2699761Z                 )
2021-01-16T02:31:05.2700009Z     
2021-01-16T02:31:05.2700245Z             diff = 0
2021-01-16T02:31:05.2700598Z             for left_arr, right_arr in zip(left, right):
2021-01-16T02:31:05.2700997Z                 # count up differences
2021-01-16T02:31:05.2701434Z                 if not array_equivalent(left_arr, right_arr, strict_nan=strict_nan):
2021-01-16T02:31:05.2701881Z                     diff += 1
2021-01-16T02:31:05.2702133Z     
2021-01-16T02:31:05.2702418Z             diff = diff * 100.0 / left.size
2021-01-16T02:31:05.2702802Z             msg = f""{obj} values are different ({np.round(diff, 5)} %)""
2021-01-16T02:31:05.2703257Z >           raise_assert_detail(obj, msg, left, right, index_values=index_values)
2021-01-16T02:31:05.2703684Z E           AssertionError: numpy array are different
2021-01-16T02:31:05.2704002Z E           
2021-01-16T02:31:05.2704325Z E           numpy array values are different (33.33333 %)
2021-01-16T02:31:05.2704694Z E           [left]:  [True, True, True]
2021-01-16T02:31:05.2705054Z E           [right]: [False, True, True]
2021-01-16T02:31:05.2705306Z 
2021-01-16T02:31:05.2705579Z pandas/_testing.py:1159: AssertionError
2021-01-16T02:31:05.2706269Z _ TestComparison.test_index_series_compat[ne-Series-Series-assert_series_equal] _
2021-01-16T02:31:05.2706975Z [gw1] linux -- Python 3.8.5 /home/vsts/miniconda3/envs/pandas-dev/bin/python
```

"
598801766,33517,BUG: Fails and or weird aggregation results when using agg with custom functions,mfcabrera,closed,2020-04-13T10:10:38Z,2021-01-17T13:41:03Z,"- [x] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame(
        [
            (1, ""a"", ""f""),
            (2, ""b"", ""d""),
            (3, None, ""f""),
            (4, ""a"", None),
            (5, ""a"", ""f""),
            (6, None, ""d""),
            (7, None, ""d""),
            (8, ""b"", None),
            (9, ""a"", ""f""),
            (10, None, None),
            (11, None, ""f""),
            (12, None, ""d""),
             ],
             columns=[""item"", ""att1"", ""att2""],
        )

df2 = pd.DataFrame(
                    [
                        (1, ""a"", ""f""),
                        (2, ""b"", ""d""),
                        (3, None, ""f""),]
                 , columns=[""item"", ""att1"", ""att2""]
)



def count_not_null(series: pd.Series) -> int:
    return series.notnull().astype(int).sum()

def count_all(series: pd.Series) -> int:
    """""" count all the values (regardless if they are null or nan) """"""
    return len(series)

df.agg(""count"")
# item    12
# att1     6
# att2     9
# dtype: int64

df.agg([""count"", ])
# item  att1  att2
# count    12     6     9

df.agg(count_all)
# item    12
# att1    12
# att2    12
# dtype: int64

df.agg([count_all,])
# item  att1  att2
# count_all    12    12    12

df.query(""item==1"").agg([""count"",]) # works fine
# item  att1  att2
# count     1     1     1

# All seems to work well however same aggregation gives weird results on the other dataframe
df2.aggregate([count_all,])
#          item att1      att2
#          item att1 count_all
# count_all  3.0  3.0       NaN
# 0          NaN  NaN       1.0
# 1          NaN  NaN       1.0
# 2          NaN  NaN       1.0

#  I get the same issues on the other df when I filter some column for exampe
df.query(""item==1"").agg([count_all,])  # **weird result** ->
#           item      att1      att2
# item count_all count_all
# count_all  1.0       NaN       NaN
# 0          NaN       1.0       1.0

#  And it fails if I use more than one custom aggregations 

df.query(""item==1"").agg([""sum"", ""count""]) # works with standard aggregations
#       item att1 att2
# sum       1    a    f
# count     1    1    1

# fails with custom aggregations
df.query(""item==1"").agg([count_all, count_not_null])

/usr/local/anaconda3/envs/hooqu/lib/python3.8/site-packages/pandas/core/base.py in _aggregate_multiple_funcs(self, arg, _axis)
    553             result = Series(results, index=keys, name=self.name)
    554             if is_nested_object(result):
--> 555                 raise ValueError(""cannot combine transform and aggregation operations"")
    556             return result
    557

ValueError: cannot combine transform and aggregation operations

# df[df[""item""]==1].agg([count_all, count_not_null]) also fails

# It works well if I don't use any filtering via query
df.agg([count_all, count_not_null])
#                 item  att1  att2
# count_all         12    12    12
# count_not_null    12     6     9

# It also fails without filtering on the second dataframe

df2.agg([count_all, count_not_null])

---> 1 df2.agg([count_all, count_not_null])

/usr/local/anaconda3/envs/hooqu/lib/python3.8/site-packages/pandas/core/frame.py in aggregate(self, func, axis, *args, **kwargs)
   6704         result = None
   6705         try:
-> 6706             result, how = self._aggregate(func, axis=axis, *args, **kwargs)
   6707         except TypeError:
   6708             pass

/usr/local/anaconda3/envs/hooqu/lib/python3.8/site-packages/pandas/core/frame.py in _aggregate(self, arg, axis, *args, **kwargs)
   6718             result = result.T if result is not None else result
   6719             return result, how
-> 6720         return super()._aggregate(arg, *args, **kwargs)
   6721
   6722     agg = aggregate

/usr/local/anaconda3/envs/hooqu/lib/python3.8/site-packages/pandas/core/base.py in _aggregate(self, arg, *args, **kwargs)
    475         elif is_list_like(arg):
    476             # we require a list, but not an 'str'
--> 477             return self._aggregate_multiple_funcs(arg, _axis=_axis), None
    478         else:
    479             result = None

/usr/local/anaconda3/envs/hooqu/lib/python3.8/site-packages/pandas/core/base.py in _aggregate_multiple_funcs(self, arg, _axis)
    521                 colg = self._gotitem(col, ndim=1, subset=obj.iloc[:, index])
    522                 try:
--> 523                     new_res = colg.aggregate(arg)
    524                 except (TypeError, DataError):
    525                     pass

/usr/local/anaconda3/envs/hooqu/lib/python3.8/site-packages/pandas/core/series.py in aggregate(self, func, axis, *args, **kwargs)
   3686         # Validate the axis parameter
   3687         self._get_axis_number(axis)
-> 3688         result, how = self._aggregate(func, *args, **kwargs)
   3689         if result is None:
   3690

/usr/local/anaconda3/envs/hooqu/lib/python3.8/site-packages/pandas/core/base.py in _aggregate(self, arg, *args, **kwargs)
    475         elif is_list_like(arg):
    476             # we require a list, but not an 'str'
--> 477             return self._aggregate_multiple_funcs(arg, _axis=_axis), None
    478         else:
    479             result = None

/usr/local/anaconda3/envs/hooqu/lib/python3.8/site-packages/pandas/core/base.py in _aggregate_multiple_funcs(self, arg, _axis)
    553             result = Series(results, index=keys, name=self.name)
    554             if is_nested_object(result):
--> 555                 raise ValueError(""cannot combine transform and aggregation operations"")
    556             return result
    557



```

#### Problem description

I would have expected the output of a custom aggregation upon filtering to be very similar to the one standard ones. Furthermore there seems to be a small bug when passing a single custom aggregation into a collection to the `agg` DataFrame method. 

I have narrow down the problem to the call to `_aggregate_multiple_funcs` that works differently based on the size of the dataframe and the number of functions. 

In particular when executing the aggregation on the columns (series) different column behave differently. Example:

if I defined:

```python
def count_all(series):
    print(type(series))
    return len(series)
```
And then called `aggregate` on different columns the function is passed different set of paramters:

```python

df2['att1'].agg(count_all)
# <class 'str'>
# <class 'str'>
# <class 'NoneType'>
# <class 'pandas.core.series.Series'>

# and

df2['att2'].agg(count_all)
# <class 'str'>
# <class 'str'>
# <class 'str'>

```
I don't understand this behaviour. I would expect that both functions receive only the full series data.

#### Workaround
After reading the code I found this line (not sure if it has to do with the problem):
https://github.com/pandas-dev/pandas/blob/v1.0.3/pandas/core/series.py#L3706

So I reimplemented one of the custom aggregation like this:
```
def count_all(series):
    if not isinstance(series, pd.Series):
        raise TypeError
    return len(series)
```
And then all aggregations worked. Was I using a bad implementation of a custom aggregation function? From the docs it was not obvious that the aggregation function is required to check the input type.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.0.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : de_DE.UTF-8
LOCALE           : de_DE.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : 5.4.1
hypothesis       : 5.8.0
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
787712171,39228,DOC: Added notes section in dropna method,complexsum,closed,2021-01-17T14:01:10Z,2021-01-17T14:17:10Z,"In case of specifying both the optional parameters how and thresh then thresh takes the precedence

- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

![Screenshot from 2021-01-17 19-12-34](https://user-images.githubusercontent.com/51440666/104845272-7997ec80-58fa-11eb-9d42-5f2d212e440c.png)"
782809496,39089,CI: Numpy changed dtype inference,phofl,closed,2021-01-10T12:37:38Z,2021-01-17T14:17:38Z,"Numpy changed the dtype inference (https://github.com/numpy/numpy/pull/17863), which causes tests to fail. Problem description as follows:

```
other = [pd.Interval(0, 1, closed=""right""), 0, ""foo""]
arr = np.asarray(other)
result = pd.array(arr)
```

On a regular Numpy version the numpy array has dtype object and this returns
```
<PandasArray>
[Interval(0, 1, closed='right'), 0, 'foo']
Length: 3, dtype: object
```

while on numpy dev the numpy array has dtype ``<U64`` and this returns 
```
<StringArray>
['(0, 1]', '0', 'foo']
Length: 3, dtype: string
```
This leads to a different evaluation of ``is_object_dtype(other_dtype)``"
787693542,39225,"Revert ""CI: Skip numpy dev failing tests (#39090)""",phofl,closed,2021-01-17T12:16:27Z,2021-01-17T14:18:41Z,"This reverts commit 3bd3d1ec

- [x] xref #39205 can close this now probably
- [x] closes #39089 
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

"
775956004,38781,DOC: Undocumented change in .isin behavior from 1.1.5 to 1.2.0,StefanBRas,closed,2020-12-29T15:40:50Z,2021-01-17T15:36:01Z,"#### Location of the documentation
https://pandas.pydata.org/docs/whatsnew/v1.2.0.html

#### Documentation problem
There seems to be an undocumented change with how `isin` works.
pandas 1.1.5:
```python
import pandas as pd
pd.Series([0]).isin(['0']) 
# 0    True
# dtype: bool
pd.Series([1]).isin(['1']) 
# 0    True
# dtype: bool
pd.Series([1.1]).isin(['1.1']) 
# 0    True
# dtype: bool
```
But in pandas 1.2.0 (and 1.2.0rc):
```python
import pandas as pd
pd.Series([0]).isin(['0']) 
# 0    False
# dtype: bool
pd.Series([1]).isin(['1']) 
# 0    False
# dtype: bool
pd.Series([1.1]).isin(['1.1']) 
# 0    False
# dtype: bool
```

But the release notes mentions nothing about it.
I have not posted this issue as an bug,  because I think the newer implementation is the better implementation.
However I propose adding the change to the release notes for other people like me where the change broke tests in a pipeline.

#### Suggested fix for documentation

Add a line mentioning that `isin` no longer returns true when comparing floats to a string of the float.
"
782662176,39064,Added docs for the change of behavior of isin,omarafifii,closed,2021-01-09T18:35:05Z,2021-01-17T15:37:27Z,"This is to close issue #38781, I added documentation to explain the change of behavior of the isin function.

- [x] closes #38781 
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
327955081,21266,[Enhancement Request] Styler support for tooltips.,jeremysalwen,closed,2018-05-31T01:06:33Z,2021-01-17T15:46:56Z,"#### Problem description
It would be nice if pandas stlying allowed you to set a tooltip for table cells, in the same way that it allows you to set the CSS style using apply, applymap, etc.

Currently it is possible to create a hacky workaround using css-only tooltips and set_table_styles, but it adds substantial complexity to achieve something that is conceptually very simple.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
pd.show_versions()


INSTALLED VERSIONS
------------------
commit: None
python: 2.7.13.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.0-6-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.19.2
nose: None
pip: 9.0.1
setuptools: 36.0.1
Cython: 0.28.3
numpy: 1.14.3
scipy: 0.19.1
statsmodels: 0.8.0
xarray: None
IPython: 5.1.0
sphinx: None
patsy: 0.4.1+dev
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: 3.4.2
numexpr: 2.6.2
matplotlib: 2.0.0
openpyxl: 2.3.0
xlrd: 1.1.0
xlwt: 0.7.5
xlsxwriter: None
lxml: 4.0.0
bs4: 4.6.0
html5lib: 0.999999999
httplib2: 0.9.2
apiclient: 1.6.5
sqlalchemy: 1.1.3
pymysql: 0.6.1.None
psycopg2: None
jinja2: 2.8
boto: 2.48.0
pandas_datareader: 0.2.0

</details>
"
783666485,39115,Documentation showing placeholders instead of text,MicaelJarniac,closed,2021-01-11T20:03:48Z,2021-01-17T15:59:39Z,"#### Location of the documentation

[pandas.DataFrame.mad](https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.mad.html)

#### Documentation problem

The documentation is showing placeholders instead of the actual text.
![Screenshot](https://i.imgur.com/inPHZSf.png)
`{desc}`, `{axis_descr}`, `{name1}`, `{name2}`, `{see_also}`, `{examples}`

https://github.com/pandas-dev/pandas/blob/3e89b4c4b1580aa890023fc550774e63d499da25/pandas/core/generic.py#L10890-L10901"
784734759,39139,BUG: Placeholders not being filled on docstrings,MicaelJarniac,closed,2021-01-13T02:29:00Z,2021-01-17T16:00:43Z,"- [x] closes #39115 
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
"
787731529,39230,Backport PR #39064 on branch 1.2.x (Added docs for the change of behavior of isin),meeseeksmachine,closed,2021-01-17T15:37:17Z,2021-01-17T16:35:58Z,Backport PR #39064: Added docs for the change of behavior of isin
787736056,39231,Backport PR #39139 on branch 1.2.x (BUG: Placeholders not being filled on docstrings),meeseeksmachine,closed,2021-01-17T15:59:53Z,2021-01-17T17:24:05Z,Backport PR #39139: BUG: Placeholders not being filled on docstrings
787739659,39232,CI: pre-commit fixup,simonjayhawkins,closed,2021-01-17T16:17:35Z,2021-01-17T17:47:55Z,
787773621,39237,Backport PR #39233 on branch 1.2.x (DOC: 1.2.1 release date),meeseeksmachine,closed,2021-01-17T19:10:46Z,2021-01-17T20:50:36Z,Backport PR #39233: DOC: 1.2.1 release date
787755846,39233,DOC: 1.2.1 release date,simonjayhawkins,closed,2021-01-17T17:38:51Z,2021-01-18T09:19:55Z,
788084943,39246,DOC: clean-up of v1.2.1 whatsnew,jorisvandenbossche,closed,2021-01-18T09:16:39Z,2021-01-18T10:56:28Z,"First commit is fixing links to the reference docs (many were not actually working links), the second commit is just a reorder to group related ones together (but together it makes the diff a bit difficult to interpret)

cc @simonjayhawkins "
788162607,39248,Backport PR #39246 on branch 1.2.x (DOC: clean-up of v1.2.1 whatsnew),meeseeksmachine,closed,2021-01-18T10:55:54Z,2021-01-18T11:48:13Z,Backport PR #39246: DOC: clean-up of v1.2.1 whatsnew
788209599,39249,BUG: read_excel fails with `IndexError: list index out of range`,YarShev,closed,2021-01-18T12:01:03Z,2021-01-18T12:14:37Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas
df = pandas.read_excel(""excel_sheetname_title.xlsx"")
IndexError: list index out of range
```

#### Output of ``pd.show_versions()``

<details>

pandas           : 1.2.0
numpy            : 1.19.2
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.1.2.post20210112
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.6
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 2.0.1
xlwt             : None
numba            : None

</details>
"
501029011,28733,Inconsistent indexes for tick label plotting,nrebena,closed,2019-10-01T17:08:33Z,2021-01-18T15:21:15Z,"The tick position for BarPlot can be define using the convert tool from matplotlib.

The main advantage is that it will reuse the same position when you have text as axis values. Previously, the tick position was determined by the order of the given element, so that ['A', 'B'] where given label [0, 1], and if updating the plot with order ['B', 'A'], you will not draw at the right position.
### 
The resulting plot for each issue would be:
![issue26186](https://user-images.githubusercontent.com/49879400/65983815-b24e5100-e47e-11e9-87c5-8c536744da28.png)
![issue11465](https://user-images.githubusercontent.com/49879400/65983825-b8443200-e47e-11e9-954d-76919e96c38f.png)
- [x] closes #26186
- [x] closes #11465
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
787768109,39235,"Revert ""Inconsistent indexes for tick label plotting (#28733)""",simonjayhawkins,closed,2021-01-17T18:42:05Z,2021-01-18T15:36:57Z,"This reverts commit fb379d8266492f917ed880f7619f3d0d9bc7c8db.

reverts #28733

cc @nrebena @jreback @WillAyd @charlesdong1991

just the complete revert for now for ci testing, will add commits for release notes. will NOT add regression tests see https://github.com/pandas-dev/pandas/issues/38721#issuecomment-761840221"
788353092,39252,"Backport PR #39235 on branch 1.2.x (Revert ""Inconsistent indexes for tick label plotting (#28733)"")",meeseeksmachine,closed,2021-01-18T15:20:00Z,2021-01-18T16:10:44Z,"Backport PR #39235: Revert ""Inconsistent indexes for tick label plotting (#28733)"""
520002936,29482,Dataframe assignment breaks Timestamp series (possibly due to rounding),coells,closed,2019-11-08T13:02:10Z,2021-01-18T17:06:08Z,"#### Code Sample

```python
import pandas as pd

print(pd.__version__)

df = pd.DataFrame({
    't': [pd.Timestamp('2017-05-22 22:28:35.489689'),
          pd.Timestamp('2017-05-22 22:28:36.086933'),
          pd.Timestamp('2017-05-22 22:28:37.000001')]
})

print(df)
print()

mask = [True, True, True]

t = df['t'].values

print(t)
print()

df.loc[mask, 't'] = t

print(df)
print()
```

#### Problem description

Upgrade from Pandas 0.24.1 to 0.25.3 causes a bug in how a Timestamp series/ndarray is handled. The bug seems to be present at least since 0.25.1.

The code above takes a subset of dataframe column and stores it as separate series or ndarray. After the assignment back to the dataframe, nanosecond precision is broken as shows the script output.

```
0.25.3
                           t
0 2017-05-22 22:28:35.489689
1 2017-05-22 22:28:36.086933
2 2017-05-22 22:28:37.000001

['2017-05-22T22:28:35.489689000' '2017-05-22T22:28:36.086933000'
 '2017-05-22T22:28:37.000001000']

                              t
0 2017-05-22 22:28:35.489689088
1 2017-05-22 22:28:36.086932992
2 2017-05-22 22:28:37.000001024
```

#### Expected Output

The ""made up"" nanoseconds are not random, hence it seems to be some kind of rounding. I have tried to provide as-simple-as-possible case based on our code that combines further data handling. 
We do use nanoseconds, but sometimes Timestamp is rounded to microseconds and we expect Pandas to keep it so.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.113-el6.x86_64.lime.1
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.14.1
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 18.1
setuptools       : 39.2.0
Cython           : 0.29.6
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.2.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 7.4.0
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.1
matplotlib       : 2.2.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
788386647,39255,Backport PR #39202 on branch 1.2.x,twoertwein,closed,2021-01-18T16:05:47Z,2021-01-18T21:07:13Z,
787588112,39218,REF: implement ensure_iterable_indexer,jbrockmendel,closed,2021-01-17T01:06:09Z,2021-01-19T02:26:06Z,Split off from a bigger branch
788418813,39256,DOC: keep color for note admonitions in sphinx theme,jorisvandenbossche,closed,2021-01-18T16:54:05Z,2021-01-19T08:33:20Z,"This doesn't change anything about the appearance in the docs, but is meant to keep the current look anticipating a change in the default color in the theme upstream (https://github.com/pandas-dev/pydata-sphinx-theme/pull/281)"
784632772,39135,ENH: Add support for `date_unit` to be specified per column in `to_json`,galipremsagar,open,2021-01-12T22:14:27Z,2021-01-19T09:13:41Z,"#### Is your feature request related to a problem?

I would want to encode my dataframe consisting of `datetime64` & `timedelta64` columns with different units of precision per column in `to_json`.

#### Describe the solution you'd like

Currently in `to_json`, the `date_unit` param achieves this for the entire dataframe, but I would like to encode each column with different precision. Hence a `dict-like` support for `date_unit` would be good to have.


#### Describe alternatives you've considered

None

"
788818823,39269,Backport PR #39256 on branch 1.2.x (DOC: keep color for note admonitions in sphinx theme),meeseeksmachine,closed,2021-01-19T08:34:41Z,2021-01-19T09:50:09Z,Backport PR #39256: DOC: keep color for note admonitions in sphinx theme
741462684,37784,RLS: 1.2,simonjayhawkins,closed,2020-11-12T10:09:04Z,2021-01-19T10:29:12Z,"Tracking issue for the 1.2 release. https://github.com/pandas-dev/pandas/milestone/73

Planning a release candidate for sometime towards the end of the month, 1-2 weeks before the final release.

List of open regressions: https://github.com/pandas-dev/pandas/issues?q=is%3Aopen+is%3Aissue+label%3ARegression (includes regressions that may be included in 1.1.5)

List of open blockers: https://github.com/pandas-dev/pandas/issues?q=is%3Aopen+is%3Aissue+label%3Ablocker+"
789046778,39271,Backport PR #39253 on branch 1.2.x (REGR: codecs.open() is always opened in text mode),meeseeksmachine,closed,2021-01-19T14:00:24Z,2021-01-19T15:00:02Z,Backport PR #39253: REGR: codecs.open() is always opened in text mode
787314990,39202,REGR: to_stata tried to remove file before closing it,twoertwein,closed,2021-01-16T01:24:02Z,2021-01-19T15:54:08Z,"Spin-off from #39047:

If an error occurred during `to_stata`, the code tried to first remove the file and then close the file handle:

1. removing the file failed on Windows (it is still opened)
2. It created an empty file when closing the handle
"
785542940,39160,Backport PR #39029: BUG: read_csv does not close file during an error in _make_reader,twoertwein,closed,2021-01-13T23:58:39Z,2021-01-19T15:54:13Z,Backport PR #39029
787594031,39220,DEPR: EWM.vol,mroeschke,closed,2021-01-17T01:50:39Z,2021-01-19T16:16:31Z,"- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

`vol` is undocumented and was just `std` which is more standard in our API"
788540003,39259,BUG: Numpy ufunc accumulate doesn't run correctly on Pandas 1.2.0,quant-dc,closed,2021-01-18T20:51:12Z,2021-01-19T16:19:24Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample

```python
import pandas as pd
import numpy as np

# %% Create random data

np.random.seed(50)
number_of_observations = 100
values = np.random.randn(number_of_observations)

# %% Form DataFrame

data = pd.DataFrame(values, columns=['A'])
data = data.cumsum()  # Get more local maximums for comparison

# %% Accumulate the maximum

pandas_accumulate = np.maximum.accumulate(data).to_numpy()
numpy_accumulate = np.maximum.accumulate(data.to_numpy())

assert np.allclose(pandas_accumulate, numpy_accumulate)

```

#### Problem description

Numpy maximum accumulate does nothing to the data compared with the same function applied to the numpy array (confirmed with axis argument specified too). Assert statement passed on pandas version 1.1.5. Broken in master as of writing also.

#### Expected Output

As previous version, the result of the numpy function in the pandas DataFrame.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Tue Nov 10 00:10:30 PST 2020; root:xnu-6153.141.10~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_GB.UTF-8
pandas           : 1.2.0
numpy            : 1.19.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.0.3
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
788600908,39262,CI: skip checking freq on flaky pytables test,jbrockmendel,closed,2021-01-18T23:27:56Z,2021-01-19T16:23:39Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
789162911,39273,Backport PR #39260 on branch 1.2.x (REGR: fix numpy accumulate ufuncs for DataFrame),meeseeksmachine,closed,2021-01-19T16:19:59Z,2021-01-19T17:38:33Z,Backport PR #39260: REGR: fix numpy accumulate ufuncs for DataFrame
788559340,39260,REGR: fix numpy accumulate ufuncs for DataFrame,jorisvandenbossche,closed,2021-01-18T21:33:49Z,2021-01-19T18:00:19Z,"Closes #39259

So we shouldn't call non-`__call__` ufuncs block-by-block, since they can work along a certain axis, like `accumulate`. In this case there were 2 problems: 1) we were applying the ufunc on the Block values, which is 2D but transposed compared to the DataFrame (so the accumulation was done in the wrong direction) and 2) the `axis` keyword was also not passed through in case the user specified it."
787773700,39238,CLN: remove unused axis kwarg from Block.putmask,jbrockmendel,closed,2021-01-17T19:11:11Z,2021-01-19T18:15:08Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
787772526,39236,"REF: roll IntBlock, BoolBlock, ComplexBlock into NumericBlock",jbrockmendel,closed,2021-01-17T19:05:02Z,2021-01-19T18:15:29Z,
787543144,39211,BUG: DataFrame.apply axis=1 for str ops with no axis argument,rhshadrach,closed,2021-01-16T20:09:14Z,2021-01-19T20:29:25Z,"E.g.

```
df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})
print(df.apply('pct_change', axis=0))
print(df.apply('pct_change', axis=1))
```

both give the output

```
     a         b
0  NaN       NaN
1  1.0  0.333333
```

This should instead raise rather than silently give incorrect results.
"
787567846,39215,TST GH26807 Break up test_strings,moink,closed,2021-01-16T22:42:10Z,2021-01-19T20:44:11Z,"This PR partially addresses xref #26807 in the case of pandas/tests/test_strings.py. It deletes that file and makes a new subpackage pandas/tests/strings/ with 8 new test modules plus a conftest file. 

I have only moved tests and flattened the structure by moving the tests from being methods to functions -  I haven't changed them in any other way. There are 1814 tests that were in pandas/tests/test_strings.py and are now in pandas/tests/strings/. I tried to keep each test module logically cohesive and less than 1000 lines long - test_strings.py still contains all the tests that don't really fit anywhere else.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
787552204,39213,CLN: use getvalue() instead of seek(0)+read(),twoertwein,closed,2021-01-16T21:04:45Z,2021-01-19T21:19:24Z,`BytesIO`'s `getvalue()` is the same as `seek(0)` followed by `read()` if the buffer is not used in any other place (`getvalue` doesn't change the position within the buffer). getvalue is slightly faster then seek+read.
788367141,39253,REGR: codecs.open() is always opened in text mode,twoertwein,closed,2021-01-18T15:38:54Z,2021-01-19T21:50:22Z,"- [x] closes #39247
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"

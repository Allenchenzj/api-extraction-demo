id,number,title,user,state,created_at,updated_at,body
732439822,37487,TST/REF: loc/iloc/at/iat tests go in tests.indexing,jbrockmendel,closed,2020-10-29T15:54:34Z,2020-10-30T14:44:14Z,"not tests.(series|frame).indexing

`(Series|DataFrame).__(getitem|setitem)__` tests go in the tests.(series|frame).indexing

"
732708569,37504,TST: pct_change from generic to frame,jbrockmendel,closed,2020-10-29T22:34:09Z,2020-10-30T14:49:07Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
732671889,37498,TST: tz_localize/tz_convert unclear test assertions,jbrockmendel,closed,2020-10-29T21:21:54Z,2020-10-30T14:53:02Z,kill off tests.series.test_timezones.
732835481,37511,CLN: remove _vendored/typing_extensions,jbrockmendel,closed,2020-10-30T02:38:57Z,2020-10-30T14:56:51Z,"- [x] closes #37119
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

As discussed in #37119, the vendored version does not serve its intended purpose.  Moreover, #37137 shows a way to get by without it."
732758151,37508,REGR: inplace Series op not actually operating inplace,jbrockmendel,closed,2020-10-29T23:52:07Z,2020-10-30T15:00:52Z,"- [x] closes #36373
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

alternative to #37497, ensures that Series inplace ops are actually inplace whenever possible.  The whatsnew is copied verbatim from #37497, the new test is copied with an additional assertion about `series._values`"
730022904,37436,ENH: std for dt64 dtype,jbrockmendel,closed,2020-10-27T01:25:48Z,2020-10-30T17:00:08Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
732830773,37509,CLN: assorted cleanups,jbrockmendel,closed,2020-10-30T02:25:01Z,2020-10-30T17:01:42Z,
731024393,37458,CLN: de-duplicate validator,jbrockmendel,closed,2020-10-28T02:21:57Z,2020-10-30T17:02:31Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
731951288,37481,REF: separate out cases in setitem_with_indexer,jbrockmendel,closed,2020-10-29T02:50:57Z,2020-10-30T17:10:45Z,Preparatory to fixing a multiple-paths issue in setitem_with_indexer
732747016,37507,TST/REF: collect indexing tests by method,jbrockmendel,closed,2020-10-29T23:40:42Z,2020-10-30T17:16:48Z,
730447263,37448,BUG: Inconsistent correlation between constant series (varies with number of rows),anders-kiaer,closed,2020-10-27T13:23:34Z,2020-10-30T19:57:22Z,"- [X] I have checked that this issue has not already been reported (might be another variant of #20954).

- [X] I have confirmed this bug exists on the latest version of pandas (`1.1.3`).

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

for length in [2, 3, 5, 10, 20]:
    print(pd.DataFrame(length*[[0.42, 0.1]], columns=[""A"", ""B""]).corr())
```
gives
```
    A   B
A NaN NaN
B NaN NaN
    A    B
A NaN  NaN
B NaN  1.0
     A   B
A  1.0 NaN
B  NaN NaN
     A    B
A  1.0 -1.0
B -1.0  1.0
     A    B
A  1.0  1.0
B  1.0  1.0
```

#### Problem description

Inconsistent output with slightly varying number of rows. Would expect correlation between series where at least one of them is constant, to be `NaN`.

This makes e.g. code dependent on `dropna()` usage after calculating `corr()` difficult/error prone, as behaviour is inconsistent.

#### Expected Output

Either consistent `NaN` output when calculating correlation with constant data, or a warning in [`pandas.DataFrame.corr` documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) stating that returned correlation between constant series can be anything from `[1.0, -1.0, NaN]`."
730894507,37453,ENH: Improve numerical stability for Pearson corr() and cov(),phofl,closed,2020-10-27T22:19:44Z,2020-10-30T20:03:38Z,"- [x] closes #37448
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Floating number issues when summing the same number often enough..."
733479978,37522,TST: suppress warnings we cant do anything about,jbrockmendel,closed,2020-10-30T20:01:48Z,2020-10-30T22:08:34Z,and remove redundant filterwarnings from excel.test_readers
723999175,37218,BUG: Wrong results and SegFaults when indexing with uint arrays,HagaiHargil,closed,2020-10-18T12:44:37Z,2020-10-30T22:45:57Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

ser = pd.Series([1, 2, 3])
arr8 = np.array([4], dtype=np.uint8)
arr16 = np.array([4], dtype=np.uint16)
arr32 = np.array([4], dtype=np.uint32)
arr64 = np.array([4], dtype=np.uint64)

print(ser[arr8])  # prints random number, probably out-of-bounds memory address
print(ser[arr16])  # prints random number, probably out-of-bounds memory address
print(ser[arr32])  # segfault
print(ser[arr64])  # correctly returns a KeyError
```

#### Problem description

Indexing into a Series without `.iloc` or `.loc` using an unsigned integer that points outside of the Series sometimes results in pandas returning some bogus value, and sometimes causes a segfault. I wasn't able to reproduce this behavior with signed integer arrays.

#### Expected Output

I assume that these queries should return a KeyError.

#### Output of ``pd.show_versions()``

I confirmed the bug with the following two versions:

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.147-1-MANJARO
machine          : x86_64
processor        :
byteorder        : little
LC_ALL           : None
LANG             : en_IL.UTF-8
LOCALE           : en_IL.UTF-8

pandas           : 1.0.3
numpy            : 1.17.1
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : 0.29.19
pytest           : 5.1.2
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.1.2
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.45.1

</details>

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.147-1-MANJARO
machine          : x86_64
processor        :
byteorder        : little
LC_ALL           : None
LANG             : en_IL.UTF-8
LOCALE           : en_IL.UTF-8

pandas           : 1.0.3
numpy            : 1.17.1
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : 0.29.19
pytest           : 5.1.2
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.1.2
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.45.1

</details>
"
732633839,37495,BUG: Series[uintarray] failing to raise KeyError,jbrockmendel,closed,2020-10-29T20:24:18Z,2020-10-30T22:49:43Z,"- [x] closes #37218
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
722849514,37153,BUG: rolling.median/quantile does not work with (some) BaseIndexer subclasses,mroeschke,closed,2020-10-16T03:48:16Z,2020-10-30T23:11:48Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

Using the example from `computation.rst`.

```
In [1]: In [70]: use_expanding = [True, False, True, False, True]
   ...:

In [2]: In [72]: df = pd.DataFrame({'values': range(5)})
   ...:

In [3]: In [2]: from pandas.api.indexers import BaseIndexer
   ...:

In [4]: ...: class CustomIndexer(BaseIndexer):
   ...: ...:
   ...: ...:    def get_window_bounds(self, num_values, min_periods, center, closed):
   ...: ...:        start = np.empty(num_values, dtype=np.int64)
   ...: ...:        end = np.empty(num_values, dtype=np.int64)
   ...: ...:        for i in range(num_values):
   ...: ...:            if self.use_expanding[i]:
   ...: ...:                start[i] = 0
   ...: ...:                end[i] = i + 1
   ...: ...:            else:
   ...: ...:                start[i] = i
   ...: ...:                end[i] = i + self.window_size
   ...: ...:        return start, end
   ...:

In [5]: indexer = CustomIndexer(window_size=1, use_expanding=use_expanding)

# Each window
In [7]: for window in df.rolling(indexer):
   ...:     print(window)
   ...:
   values
0       0
   values
1       1
   values
0       0
1       1
2       2
   values
3       3
   values
0       0
1       1
2       2
3       3
4       4

# Should be [0, 1, 1, 3, 2]
In [8]: df.rolling(indexer).median()
Out[8]:
   values
0     0.0
1     1.0
2     1.5
3     NaN
4     3.0

# Should be [0, 1, 1, 3, 2]
In [10]: df.rolling(indexer).quantile(0.5)
Out[10]:
   values
0     0.0
1     1.0
2     1.5
3     NaN
4     3.0
```

The cython aggregations probably assume some sort of property of the windows that may not hold with the custom indexers
"
723459489,37166,BUG: Bug in quantile() and median() returned wrong result for non monotonic window borders,phofl,closed,2020-10-16T19:02:58Z,2020-10-30T23:17:52Z,"- [x] closes #37153
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

cc @mroeschke 

Values were not added to the window again if starting point was moved back and values were not removed from the window, when ending point was moved back

Should I create functions for the deletion and addition?"
729035801,37401,CI/CLN: Lint more class references in tests,dsaxton,closed,2020-10-25T14:43:14Z,2020-10-31T00:42:12Z,
730890608,37452,CLN: Breakup agg,rhshadrach,closed,2020-10-27T22:15:30Z,2020-10-31T02:28:57Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Extracted dictionary parts to `agg_dict_like`, and changed from `isinstance(arg, dict)` to `is_dict_like(arg)`. Also renamed `aggregate_multiple_funcs` to `agg_list_like`."
687264722,35922,BUG: Index.sort_values and Series.sort_values reverse duplicate order when ascending=False,AlexKirko,closed,2020-08-27T13:57:02Z,2020-10-31T14:49:14Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
IN:
import pandas as pd
ind = pd.Index([1, 1, 2, 3])

ind, indexer = ind.sort_values(ascending=False, return_indexer=True)
indexer
OUT:
array([3, 2, 1, 0], dtype=int64)
```

#### Problem description

Came across this one while fixing #35584 in #35604. `sort_values` reverses duplicate order when `ascending=False`. This is clearest when calling `Index.sort_values`, because it can return an indexer, but it's also true for `Series.sort_values` and it propagates to `DataFrame.sort_values`.

#35604 will make sorting in descending order stable for most `Index` types (leveraging nargsort from `sorting.py`), but the problem will remain for datetime-like index types and for Series and will require fixing.

#### Expected Output

```python
array([3, 2, 0, 1], dtype=int64)
```

Duplicates should maintain order when `descending=False`. This will also let us leverage the same sorting algorithm both for `Index` and `Series`.

#### Additional use cases

Some additional use cases from the PR.

```python
s = pd.Series([""A"", ""AA"", ""BB"", ""CAC""], dtype=""object"")
s.sort_values(ascending=False, key=lambda ser: ser.str.len())

OUT:
3    CAC
2     BB
1     AA
0      A
dtype: object
```
I don't think that swapping is expected here.

Then consider that you might be sorting a DataFrame with several columns, and a column with duplicates might be the first one. In this case you likely wouldn't expect a descending sort to change duplicate order. Or you could be using something like `nlargest` and get weirdness because there is a descending sort in there and it swaps elements.

Obviously, we could get by with a convention that we always revert duplicate order with a descending sort by being careful, but I believe keeping duplicate order is cleaner. In cases where it doesn't matter, it's the same, and when it does matter (as in `nlargest` and the like), you don't need to remember that you need extra reversals.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d0ca4b347b060bc4a49b2d31a818d5a28d1e665f
python           : 3.7.8.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : ru_RU.UTF-8
LOCALE           : None.None

pandas           : 0.26.0.dev0+4054.gd0ca4b347
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : 5.23.3
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : None
gcsfs            : 0.6.2
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.3.1
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0

</details>
"
310888421,20594,BUG: HDFStore failures on timezone-aware data,adshieh,closed,2018-04-03T15:35:04Z,2020-10-31T14:59:42Z,"#### Code Sample, a copy-pastable example if possible

```python
In [1]: import pandas as pd
   ...: 
   ...: def check_roundtrip(obj):
   ...:     with pd.HDFStore('test.h5', 'w') as store:
   ...:         store['obj'] = obj
   ...:         retrieved = store['obj']
   ...:         return obj.equals(retrieved)
   ...: 
   ...: s = pd.Series([], dtype='datetime64[ns, UTC]')
   ...: t = pd.Series([0], dtype='datetime64[ns, UTC]')
   ...: df = pd.DataFrame({'A': s})

In [2]: print(check_roundtrip(s))
False

In [3]: print(check_roundtrip(t))
False

In [4]: print(check_roundtrip(df))

------------------------------------------------------------------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-4-57f1e9853bf6> in <module>()
----> 1 print(check_roundtrip(df))

<ipython-input-1-1d7fb84453e9> in check_roundtrip(obj)
      4     with pd.HDFStore('test.h5', 'w') as store:
      5         store['obj'] = obj
----> 6         retrieved = store['obj']
      7         return obj.equals(retrieved)
      8 

/home/ashieh/.local/lib/python2.7/site-packages/pandas/io/pytables.pyc in __getitem__(self, key)
    481 
    482     def __getitem__(self, key):
--> 483         return self.get(key)
    484 
    485     def __setitem__(self, key, value):

/home/ashieh/.local/lib/python2.7/site-packages/pandas/io/pytables.pyc in get(self, key)
    669         if group is None:
    670             raise KeyError('No object named %s in the file' % key)
--> 671         return self._read_group(group)
    672 
    673     def select(self, key, where=None, start=None, stop=None, columns=None,

/home/ashieh/.local/lib/python2.7/site-packages/pandas/io/pytables.pyc in _read_group(self, group, **kwargs)
   1347         s = self._create_storer(group)
   1348         s.infer_axes()
-> 1349         return s.read(**kwargs)
   1350 
   1351 

/home/ashieh/.local/lib/python2.7/site-packages/pandas/io/pytables.pyc in read(self, start, stop, **kwargs)
   2902             blk_items = self.read_index('block%d_items' % i)
   2903             values = self.read_array('block%d_values' % i,
-> 2904                                      start=_start, stop=_stop)
   2905             blk = make_block(values,
   2906                              placement=items.get_indexer(blk_items))

/home/ashieh/.local/lib/python2.7/site-packages/pandas/io/pytables.pyc in read_array(self, key, start, stop)
   2464             if shape is not None:
   2465                 # length 0 axis
-> 2466                 ret = np.empty(shape, dtype=dtype)
   2467             else:
   2468                 ret = node[start:stop]

TypeError: Invalid datetime unit in metadata string ""[ns, UTC]""
```
#### Problem description

HDFStore fails to save empty/non-empty Series and empty DataFrames with timezone-aware data correctly. The issue is that the timezone information is not being saved correctly. For Series, there is no timezone handling at all. For DataFrames, the timezone handling is skipped when empty.

#### Expected Output

The checks should pass.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.12.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-1049-aws
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.22.0
pytest: 3.5.0
pip: 9.0.3
setuptools: 39.0.1
Cython: 0.28.1
numpy: 1.14.2
scipy: 1.0.1
pyarrow: 0.9.0
xarray: None
IPython: 5.6.0
sphinx: None
patsy: None
dateutil: 2.7.2
pytz: 2018.3
blosc: None
bottleneck: None
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 2.2.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: 0.8.0
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
721653048,37117,CI: check for numpy.random-related imports,onshek,closed,2020-10-14T17:37:22Z,2020-10-31T15:11:51Z,"~~- [x] closes #37053 (keep this issue open until both #37492 and #37117 are completed)~~
- [x] tests added / passed ~~(once #37492is merged)~~

# update 2020-10-31
now use a pre-commit check instead of the previous code_check

*** 
~~code_check.sh will be used to locate where numpy.random-related imports are.~~ See the separate script to do the whole clean-up in https://github.com/pandas-dev/pandas/pull/37103#issue-502582592."
731316291,37465,BUG: crosstab fails with categoricals on master,bashtage,closed,2020-10-28T10:31:34Z,2020-10-31T15:16:56Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

crosstab no longer works correctly with categorical inputs. This is a regression from 1.1.x.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

g = np.random.default_rng(840812492384587325982704)
a = pd.Series(g.integers(0,3,size=100)).astype(""category"")
b = pd.Series(g.integers(0,2,size=100)).astype(""category"")
pd.crosstab(a,b,margins=True,dropna=False)
```

Produces

```
Traceback (most recent call last):
  File ""C:\git\pandas\pandas\core\indexes\base.py"", line 2965, in get_loc
    return self._engine.get_loc(casted_key)
  File ""pandas\_libs\index.pyx"", line 70, in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
  File ""pandas\_libs\index.pyx"", line 98, in pandas._libs.index.IndexEngine.get_loc
    self._check_type(val)
  File ""pandas\_libs\index_class_helper.pxi"", line 93, in pandas._libs.index.Int64Engine._check_type
    raise KeyError(val)
KeyError: 'All'
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""pandas\_libs\index.pyx"", line 705, in pandas._libs.index.BaseMultiIndexCodesEngine.get_loc
    indices = [0 if checknull(v) else lev.get_loc(v) + 1
  File ""C:\git\pandas\pandas\core\indexes\base.py"", line 2963, in get_loc
    casted_key = self._maybe_cast_indexer(key)
  File ""C:\git\pandas\pandas\core\indexes\category.py"", line 527, in _maybe_cast_indexer
    return self._data._unbox_scalar(key)
  File ""C:\git\pandas\pandas\core\arrays\categorical.py"", line 1728, in _unbox_scalar
    code = self.categories.get_loc(key)
  File ""C:\git\pandas\pandas\core\indexes\base.py"", line 2967, in get_loc
    raise KeyError(key) from err
KeyError: 'All'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""C:\git\pandas\pandas\core\generic.py"", line 3776, in _set_item
    loc = self._info_axis.get_loc(key)
  File ""C:\git\pandas\pandas\core\indexes\multi.py"", line 2795, in get_loc
    return self._engine.get_loc(key)
  File ""pandas\_libs\index.pyx"", line 708, in pandas._libs.index.BaseMultiIndexCodesEngine.get_loc
    raise KeyError(key)
KeyError: ('__dummy__', 'All')
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""C:\Anaconda\lib\site-packages\IPython\core\interactiveshell.py"", line 3417, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-3-ba4a70b3d31b>"", line 7, in <module>
    pd.crosstab(a,b,margins=True,dropna=False)
  File ""C:\git\pandas\pandas\core\reshape\pivot.py"", line 623, in crosstab
    **kwargs,
  File ""C:\git\pandas\pandas\core\frame.py"", line 6972, in pivot_table
    observed=observed,
  File ""C:\git\pandas\pandas\core\reshape\pivot.py"", line 180, in pivot_table
    fill_value=fill_value,
  File ""C:\git\pandas\pandas\core\reshape\pivot.py"", line 242, in _add_margins
    table, data, values, rows, cols, aggfunc, observed, margins_name
  File ""C:\git\pandas\pandas\core\reshape\pivot.py"", line 331, in _generate_marginal_results
    piece[all_key] = margin[key]
  File ""C:\git\pandas\pandas\core\frame.py"", line 3083, in __setitem__
    self._set_item(key, value)
  File ""C:\git\pandas\pandas\core\frame.py"", line 3160, in _set_item
    NDFrame._set_item(self, key, value)
  File ""C:\git\pandas\pandas\core\generic.py"", line 3779, in _set_item
    self._mgr.insert(len(self._info_axis), key, value)
  File ""C:\git\pandas\pandas\core\internals\managers.py"", line 1173, in insert
    new_axis = self.items.insert(loc, item)
  File ""C:\git\pandas\pandas\core\indexes\multi.py"", line 3667, in insert
    level = level.insert(lev_loc, k)
  File ""C:\git\pandas\pandas\core\indexes\category.py"", line 695, in insert
    code = self._data._validate_insert_value(item)
  File ""C:\git\pandas\pandas\core\arrays\categorical.py"", line 1186, in _validate_insert_value
    return self._validate_fill_value(value)
  File ""C:\git\pandas\pandas\core\arrays\categorical.py"", line 1222, in _validate_fill_value
    f""'fill_value={fill_value}' is not present ""
ValueError: 'fill_value=All' is not present in this Categorical's categories
```


#### Problem description

Previously produced the correct output. A simple regression.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d89331b96e919acc2a83f7f6201830d246018e36
python           : 3.7.9.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.2.0.dev0+948.gd89331b96
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.0.post20201006
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: v0.10.0dev0+30.gadb67b2
bs4              : 4.8.0
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.51.2

</details>
"
733470109,37521,REF: _setitem_with_indexer_split_path,jbrockmendel,closed,2020-10-30T19:44:23Z,2020-10-31T15:18:38Z,"Splitting this off to get a cleaner diff in an upcoming PR(s) that make setitem_with_indexer go through split_path for _all_ DataFrame cases, giving us less special-casing to worry about."
733489844,37523,TST/REF: collect Series accessor tests,jbrockmendel,closed,2020-10-30T20:19:29Z,2020-10-31T15:19:29Z,
730937391,37455,CI: Troubleshoot PY38 windows build,jbrockmendel,closed,2020-10-27T23:07:06Z,2020-10-31T15:20:28Z,"Some of the error messages look like a OOM, so this is just trying cutting down the number of tests collected/run by ~40%"
630207360,34556,ENH: Rolling Calculations Ignore Extended Window Specification,daskol,closed,2020-06-03T18:02:48Z,2020-10-31T15:34:38Z,"#### Is your feature request related to a problem?

Rolling windows API is not coherent to `scipy.signal` API.

#### Describe the solution you'd like

Support for weighted rolling window functions in `pandas` is based on `scipy.signal`. Precisely, `pandas` uses `scipy.signal.get_window` in order to get weights for a specific window specification. The issue is that `scipy` allows more accurate window specification. Namely, one can use non-symetric exponential window as following.
```python
scipy.signal.windows.exponential(window_size, sym=False)
```
Moreover, `pandas` API specification requires paramter `tau` what is not necesaray in fact. So, API could be even simpler.

A possible solution to use `win_type` in order to`getattrib` of `scipy.signal.windows` module. If there is  a function named as `win_type` then `pandas` should invoke the function with all passed paramters to aggregation function over rolling window. In the snippet below, `.sum()` implies invocation of `scipy.signal.windows.exponential(2, sym=False)`.

```python
df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})
df.rolling(2, win_type='exponential').sum(sym=True)
```

#### API breaking implications

API won't be broken. However, I would like to relax some contraints. Namely, aggregations over rolling window should be calculated without explicit window parameter specification.

#### Describe alternatives you've considered

Alternative solution looks like the following. It is simple and a bit verbose.
```python
window = sp.signal.windows.exponential(2, sym=False)

df.rolling(2, min_periods=2) \
    .apply(lambda x: (x * window).sum())
```"
733498971,37525,TST/REF: collect tests from test_api into method-specific files,jbrockmendel,closed,2020-10-30T20:31:52Z,2020-10-31T16:03:15Z,"I was surprised we didnt already have a test_copy

Also an easter egg:

```
    def test_keys(self, datetime_series):	    
        # HACK: By doing this in two stages, we avoid 2to3 wrapping the call
        # to .keys() in a list()	
        getkeys = datetime_series.keys	
        assert getkeys() is datetime_series.index
```"
641860976,34870,BUG: AssertionError when slicing MultiIndex and setting value of pandas.Series,epizzigoni,closed,2020-06-19T10:12:56Z,2020-10-31T17:19:06Z,"
#### Code Sample

```python
>>> import numpy as np
... import pandas as pd
... 
... arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],
...           ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]
... tuples = list(zip(*arrays))
... index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])
... 
... s = pd.Series(np.random.randn(8), index=index)
>>>s
first  second
bar    one      -0.592576
       two      -0.054856
baz    one      -0.169856
       two      -0.712601
foo    one       2.180688
       two      -0.032646
qux    one      -0.729950
       two       0.883029
dtype: float64

>>> s[('baz', 'one'):('foo', 'two')] = 4  # This works!
>>> s
first  second
bar    one      -0.592576
       two      -0.054856
baz    one       4.000000
       two       4.000000
foo    one       4.000000
       two       4.000000
qux    one      -0.729950
       two       0.883029
dtype: float64

>>> s.loc[('baz', 'one'):('foo', 'two')] = 4  # This gives the error!
Traceback (most recent call last):
  File ""/Users/edoardo/miniconda3/envs/my37/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 2646, in get_loc
    return self._engine.get_loc(key)
  File ""pandas/_libs/index.pyx"", line 111, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/index.pyx"", line 138, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: ('baz', 'one')
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/Users/edoardo/miniconda3/envs/my37/lib/python3.7/site-packages/pandas/core/indexes/multi.py"", line 2890, in _get_level_indexer
    start = level_index.get_loc(key.start)
  File ""/Users/edoardo/miniconda3/envs/my37/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File ""pandas/_libs/index.pyx"", line 111, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/index.pyx"", line 138, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: ('baz', 'one')
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""/Users/edoardo/miniconda3/envs/my37/lib/python3.7/site-packages/pandas/core/indexing.py"", line 670, in __setitem__
    indexer = self._get_setitem_indexer(key)
  File ""/Users/edoardo/miniconda3/envs/my37/lib/python3.7/site-packages/pandas/core/indexing.py"", line 641, in _get_setitem_indexer
    return ax.get_loc(key)
  File ""/Users/edoardo/miniconda3/envs/my37/lib/python3.7/site-packages/pandas/core/indexes/multi.py"", line 2662, in get_loc
    loc = self._get_level_indexer(key, level=0)
  File ""/Users/edoardo/miniconda3/envs/my37/lib/python3.7/site-packages/pandas/core/indexes/multi.py"", line 2903, in _get_level_indexer
    key.start, key.stop, key.step, kind=""loc""
  File ""/Users/edoardo/miniconda3/envs/my37/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 4717, in slice_indexer
    raise AssertionError(""Start slice bound is non-scalar"")
AssertionError: Start slice bound is non-scalar

```

#### Problem description

I'm trying to slice and set values of a pandas Series but using the loc function does not work. I can do it by simply using [] and using loc if the Series is first converted into a DataFrame.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.5.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8
pandas           : 1.0.4
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.0.post20200616
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
732856323,37513,TST: setting value at MultiIndex slice using .loc,arw2019,closed,2020-10-30T03:38:24Z,2020-10-31T17:19:09Z,"- [x] closes  #34870
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
728972158,37397,RLS: 1.1.4,simonjayhawkins,closed,2020-10-25T08:44:41Z,2020-10-31T17:46:52Z,"[![Tag Release](https://github.com/simonjayhawkins/pandas-release/workflows/Tag%20Release/badge.svg)](https://github.com/simonjayhawkins/pandas-release/actions?query=workflow%3A%22Tag+Release%22)

Tracking issue for the 1.1.4 release. Scheduled for release October 30, 2020

https://github.com/pandas-dev/pandas/milestone/78

Please do not remove/change milestones from these issues without a note explaining the reasoning (changing milestones doesn't trigger notification)

During issue triage, regressions from 1.0.5 onwards should be milestoned 1.1.4 in the first instance. cc @pandas-dev/pandas-triage 
"
701189586,36360,BUG: MutliIndex names lost during pivot,r7sa,closed,2020-09-14T15:07:52Z,2020-10-31T17:51:55Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
d = pd.DataFrame([[1,2,3],[2,2,3],[3,2,3],[4,2,3],[5,2,3]], 
                 columns=pd.MultiIndex.from_tuples([[0, 0, 0], [0, 0, 1], [0, 0, 2]], names=['c1', 'c2', 'c3']),
                 index=pd.MultiIndex.from_tuples([[0, 0, 0, 0, 0, 0, 0],
                                                  [0, 0, 1, 0, 0, 0, 1],
                                                  [0, 1, 0, 0, 0, 1, 0],
                                                  [0, 1, 1, 0, 0, 1, 1],
                                                  [1, 0, 0, 0, 1, 0, 0]
                                                  ], 
                                                 names=['i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7']))
e = d.pivot_table(index=['i1',], columns=['i2', 'i3',], values=[(0, 0, 1), (0, 0, 2)], aggfunc=lambda v: v.values[0])
e.columns
```
result is:
```MultiIndex([(0, 0, 1, 0, 0),
            (0, 0, 1, 0, 1),
            (0, 0, 1, 1, 0),
            (0, 0, 1, 1, 1),
            (0, 0, 2, 0, 0),
            (0, 0, 2, 0, 1),
            (0, 0, 2, 1, 0),
            (0, 0, 2, 1, 1)],
           names=[None, None, None, 'i2', 'i3'])
```
some names is None, but must be
```names=['c1', 'c2', 'c3', 'i2', 'i3'])```

#### Problem description
Problems seems in  pandas/core/groupby/generic.py in class DataFrameGroupBy in function  _wrap_aggregated_output
Line
```python
name = self._obj_with_exclusions._get_axis(1 - self.axis).name
```
seems to must be
```python
name_axis = self._obj_with_exclusions._get_axis(1 - self.axis)
name = name_axis.names if isinstance(name_axis, MultiIndex) else name_axis.name
```
"
709414568,36655,BUG: DataFrame.pivot drops column level names when both rows and columns are multiindexed,arw2019,closed,2020-09-26T03:35:11Z,2020-10-31T17:52:00Z,"- [x] closes #36360
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Fixed by checking for multi-indexed columns in `DataFrameGroupBy._wrap_aggregated_output`"
733600283,37529,"REF: share delete, putmask, insert between ndarray-backed EA indexes",jbrockmendel,closed,2020-10-31T01:27:09Z,2020-10-31T18:09:02Z,
733741287,37537,Backport PR #37520 on branch 1.1.x (DOC: Start v1.1.5 release notes),meeseeksmachine,closed,2020-10-31T15:13:59Z,2020-10-31T18:25:26Z,Backport PR #37520: DOC: Start v1.1.5 release notes
709536510,36666,DEPR deprecate pd.to_timedelta('1 M') and pd.to_timedelta('1 Y'),MarcoGorelli,closed,2020-09-26T13:20:03Z,2020-10-31T18:52:07Z,"We have that `pd.to_timedelta(1, unit='M')` and `pd.to_timedelta(1, unit='Y')` are deprecated, and so so should these (xref https://github.com/pandas-dev/pandas/pull/34979#issuecomment-699494617 ).

Once deprecated, the note added in #34979 should be removed"
714138116,36838,"DEPR: Deprecate use of strings denoting units with 'M', 'Y' or 'y' in pd.to_timedelta (36666)",avinashpancham,closed,2020-10-03T17:35:09Z,2020-10-31T18:52:13Z,"- [x] closes #36666
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
733739149,37536,#37535 pandas-CI: revert disable of windows window tests,nagesh-chowdaiah,closed,2020-10-31T15:03:06Z,2020-10-31T18:54:39Z,"- [x] closes #37535 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
removed the restrictions on windows test
"
520047615,29485,groupby.quantile() fails on Timedelta columns,dokteurwho,closed,2019-11-08T14:33:54Z,2020-10-31T18:57:02Z,"#### Code Sample, groupby quantile example

```python
# groupby quantile example
import pandas as pd

print(f""Pandas {pd.__version__}"")

df = pd.DataFrame({
    'idx': [1, 1, 2],
    'start_date': ['2019-10-02', '2019-10-04', '2019-10-05'],
    'arrival_date': ['2019-11-02', '2019-10-14', '2019-10-15']})

for c in ['start_date', 'arrival_date']:
    df[c] = pd.to_datetime(df[c])

df['delta'] = df['arrival_date'] - df['start_date']

print(df.groupby('idx')[['delta']].quantile(.9))
```
#### Problem description

The code example used to work in 0.21.1.
In 0.25 I obtain a ""TypeError: No matching signature found""

#### Expected Output
Pandas 0.21.1

               delta
idx
1   28 days 21:36:00
2   10 days 00:00:00


#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-693.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 18.1
setuptools       : 40.8.0.post20191016
Cython           : 0.29.14
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
694260668,36158,BUG: indices and get_group raise exception with GroupBy initialized using lambda or named function with tuple values as group names (Multi-dimensional groups),dimithras,closed,2020-09-06T05:49:12Z,2020-10-31T19:11:34Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

**UPD**: bug does not exist anymore in latest master branch, yet still present in 1.1.3 from pip.  
Updated version details:

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.3
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.0
pip              : 20.2.3
setuptools       : 40.8.0
Cython           : 0.29.21
pytest           : 6.0.1
hypothesis       : 5.36.0
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.3.4
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.19
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

**Edit**: removed everything related to categorical rhetoric, updated with findings. Corrected several mistypes.

---

#### Code Sample, a copy-pastable example

**UPD**: A clearer sample [provided](https://github.com/pandas-dev/pandas/issues/36158#issuecomment-697977693) by @AlexanderCecile which addresses that bug appears on group names inside lambda, not due to tuples in cell values.
```python
import random

import pandas as pd

df = pd.DataFrame({""col_1"": [random.randint(0, 5) for _ in range(5)]})

print(df, end=""\n\n"")

grp_tuples = [(""a"", ""b""), (""c"", ""d""), (""e"", ""f"")]

# grp_res = df.groupby(by=""col_1"")
grp_res = df.groupby(by=lambda x: grp_tuples[x % len(grp_tuples)])

for key, grp in grp_res:
    print(f""key: {key}\ngroup:\n{grp}\n"")

print(grp_res.indices)
print(grp_res.grouper.indices)
```

##### Original sample

```python
df = pd.DataFrame({'Tuples': ((x, y)
                              for x in [0, 1]
                              for y in np.random.randint(3, 5, 5))})
df.groupby('Tuples').grouper.indices
df.groupby(lambda x: df.iloc[x,0]).indices
```

||0|1|2|3|4|5|6|7|8|9|
|-|-|-|-|-|-|-|-|-|-|-|
|'Tuples'|(0,3)|(0,3)|(0,3)|(0,3)|(0,3)|(1,4)|(1,4)|(1,4)|(1,3)|(1,4)|

---

### Problem description

**Problem**: .indices do not work on lambda made group by with tuples

In the sample above two different **.groupby** in crude terms do the same thing and provide groups with same characteristics - grouping DataFrame by column values which are represented by tuples. Technically single column **DF** is a **Series** object, but issue remains the same if some other columns are added. However these groups share some weird behavior: 

- Both have same output for **.groups**:
> {(0, 3): [0, 1, 2, 3, 4], (1, 3): [8], (1, 4): [5, 6, 7, 9]}

- Both show same .**head**(), .**size**() some other methods [listed here](https://pandas.pydata.org/pandas-docs/stable/reference/groupby.html). Groups can be selected with tuple key `(0, 3)` with all same output.

And here come the differences:
```diff
gb.transform('size')
+ just shows the index of a DF
- IndexError: Column(s) Tuples already selected 

gb1.sum() / gb1.agg(sum)
+ shows column 'Tuples' with recurring tuples, e.g. (0, 3), (1, 3)
+ in case GroupBy is prepared with as_index=False, also shows index
- show index and 'Tuples' column where tuples are added per group
- on each line: (0, 3, 0, 3, 0, 3), (1, 3, 1, 3)

gb.all()
+ index with tuples per line, e.g. (0, 3), (1, 3)
+ if as_index=False:
+ ValueError: Length mismatch: Expected axis has 0 elements, new values have 3 elements 
- table with tuples (indexes if as_index=False) and True values

gb.count()
+ Tuples column and no counts
+ if as_index=False:
+ IndexError: list index out of range
- table with tuples (indexes if as_index=False) and actual count of groups
```

Basically all aggregated methods have same result as above. However if **GroupBy** is made on Series `df['Tuples'].groupby(df['Tuples'])` majority of aggregated functions start working as for lambda made **GroupBy**, output is the same, but does not show as table in jupyter, just plain-text, yet I guess that's an unexpected behavior. That's __sapid__, but whatever, these are the differences one can live with.

## Main issue

**Tuple** group names in **lambda** or named function ***GroupBy*** break method **.get_group()** and attribute **.indices**.

As mentioned in [docs](https://pandas.pydata.org/docs/user_guide/groupby.html#piping-function-calls) **GroupBy** works pretty well with two-dimensional keys, although missing handful level selectors that MultiIndex has. Perhaps that's a topic for __feature-request__, not an issue.

Sample in [docs](https://pandas.pydata.org/docs/user_guide/groupby.html#piping-function-calls) produces same tuple groups `dict_keys( [ ('Store_1', 'Product_1'), ... () ]`, this should not be a problem. Also **GroupBy** made with lambda works all fine with numeric / string labels.

In case with tuples, **.indices** and **.get_group** fall with error `NotImplementedError: isna is not defined for MultiIndex`.

</br>

**.get_group()**
<details><summary>.get_group() Traceback</summary><blockquote>

<details><summary>ERROR</summary><blockquote>

```python
df.groupby(lambda x: df.iloc[x,0]).get_group((0,3))
```

```diff
-   ---------------------------------------------------------------------------
-   NotImplementedError                       Traceback (most recent call last)
-   <ipython-input-3-c37a9ee93781> in <module>
-   ----> 1 df.groupby(lambda x: df.iloc[x,0]).get_group((0,3))
```

</details><details><summary>Part 1 => .get_indices</summary><blockquote>

#####     c:\python37\lib\site-packages\pandas\core\groupby\groupby.py in get_group(self, name, obj)
        806             obj = self._selected_obj
        807 
    --> 808         inds = self._get_index(name)
        809         if not len(inds):
        810             raise KeyError(name)
    

#####     c:\python37\lib\site-packages\pandas\core\groupby\groupby.py in _get_index(self, name)
        628         Safe get index, translate keys for datelike to underlying repr.
        629         """"""
    --> 630         return self._get_indices([name])[0]
        631 
        632     @cache_readonly
    

#####     c:\python37\lib\site-packages\pandas\core\groupby\groupby.py in _get_indices(self, names)
        593             return []
        594 
    --> 595         if len(self.indices) > 0:
        596             index_sample = next(iter(self.indices))
        597         else:
    

#####     c:\python37\lib\site-packages\pandas\core\groupby\groupby.py in indices(self)
        572         """"""
        573         self._assure_grouper()
    --> 574         return self.grouper.indices
        575 
        576     def _get_indices(self, names):
    

####    pandas\_libs\properties.pyx in pandas._libs.properties.CachedProperty.__get__()

</details><details><summary>Part 2 => Categories nan</summary><blockquote>

#####     c:\python37\lib\site-packages\pandas\core\groupby\ops.py in indices(self)
        222         """""" dict {group name -> group indices} """"""
        223         if len(self.groupings) == 1:
    --> 224             return self.groupings[0].indices
        225         else:
        226             codes_list = [ping.codes for ping in self.groupings]
    

####    pandas\_libs\properties.pyx in pandas._libs.properties.CachedProperty.__get__()
    

#####     c:\python37\lib\site-packages\pandas\core\groupby\grouper.py in indices(self)
        558             return self.grouper.indices
        559 
    --> 560         values = Categorical(self.grouper)
        561         return values._reverse_indexer()
        562 
    

#####     c:\python37\lib\site-packages\pandas\core\arrays\categorical.py in __init__(self, values, categories, ordered, dtype, fastpath)
        360 
        361             # we're inferring from values
    --> 362             dtype = CategoricalDtype(categories, dtype.ordered)
        363 
        364         elif is_categorical_dtype(values.dtype):
    

#####     c:\python37\lib\site-packages\pandas\core\dtypes\dtypes.py in __init__(self, categories, ordered)
        161 
        162     def __init__(self, categories=None, ordered: Ordered = False):
    --> 163         self._finalize(categories, ordered, fastpath=False)
        164 
        165     @classmethod
    

#####     c:\python37\lib\site-packages\pandas\core\dtypes\dtypes.py in _finalize(self, categories, ordered, fastpath)
        315 
        316         if categories is not None:
    --> 317             categories = self.validate_categories(categories, fastpath=fastpath)
        318 
        319         self._categories = categories
    

#####     c:\python37\lib\site-packages\pandas\core\dtypes\dtypes.py in validate_categories(categories, fastpath)
        487         if not fastpath:
        488 
    --> 489             if categories.hasnans:
        490                 raise ValueError(""Categorical categories cannot be null"")
        491 

</details><details><summary>Part 3 => nan, Multiindex</summary><blockquote>

####    pandas\_libs\properties.pyx in pandas._libs.properties.CachedProperty.__get__()
    

#####     c:\python37\lib\site-packages\pandas\core\indexes\base.py in hasnans(self)
       2046         """"""
       2047         if self._can_hold_na:
    -> 2048             return bool(self._isnan.any())
       2049         else:
       2050             return False
    

####    pandas\_libs\properties.pyx in pandas._libs.properties.CachedProperty.__get__()
    

#####     c:\python37\lib\site-packages\pandas\core\indexes\base.py in _isnan(self)
       2026         """"""
       2027         if self._can_hold_na:
    -> 2028             return isna(self)
       2029         else:
       2030             # shouldn't reach to this condition by checking hasnans beforehand
    

#####     c:\python37\lib\site-packages\pandas\core\dtypes\missing.py in isna(obj)
        122     Name: 1, dtype: bool
        123     """"""
    --> 124     return _isna(obj)
        125 
        126 
    

#####     c:\python37\lib\site-packages\pandas\core\dtypes\missing.py in _isna(obj, inf_as_na)
        151     # hack (for now) because MI registers as ndarray
        152     elif isinstance(obj, ABCMultiIndex):
    --> 153         raise NotImplementedError(""isna is not defined for MultiIndex"")
        154     elif isinstance(obj, type):
        155         return False
    

### NotImplementedError: isna is not defined for MultiIndex

**>>> End of Traceback <<<**

</details></details>

**.indices**
<details>
<summary>.indices Traceback</summary><blockquote>

<details><summary>ERROR</summary><blockquote>

```python
df.groupby(lambda x: df.iloc[x,0]).indices
```

```diff
-   ---------------------------------------------------------------------------
-   NotImplementedError                       Traceback (most recent call last)
-   <ipython-input-5-fd5a9f33be4e> in <module>
-   ----> 1 df.groupby(lambda x: df.iloc[x,0]).indices
```

</details><details><summary>Part 1 => groupings, Categorical</summary><blockquote>

#####     c:\python37\lib\site-packages\pandas\core\groupby\groupby.py in indices(self)
        572         """"""
        573         self._assure_grouper()
    --> 574         return self.grouper.indices
        575 
        576     def _get_indices(self, names):
    

####    pandas\_libs\properties.pyx in pandas._libs.properties.CachedProperty.__get__()
    

#####     c:\python37\lib\site-packages\pandas\core\groupby\ops.py in indices(self)
        222         """""" dict {group name -> group indices} """"""
        223         if len(self.groupings) == 1:
    --> 224             return self.groupings[0].indices
        225         else:
        226             codes_list = [ping.codes for ping in self.groupings]
    

####    pandas\_libs\properties.pyx in pandas._libs.properties.CachedProperty.__get__()
    

#####     c:\python37\lib\site-packages\pandas\core\groupby\grouper.py in indices(self)
        558             return self.grouper.indices
        559 
    --> 560         values = Categorical(self.grouper)
        561         return values._reverse_indexer()
        562 
    

#####     c:\python37\lib\site-packages\pandas\core\arrays\categorical.py in __init__(self, values, categories, ordered, dtype, fastpath)
        360 
        361             # we're inferring from values
    --> 362             dtype = CategoricalDtype(categories, dtype.ordered)
        363 
        364         elif is_categorical_dtype(values.dtype):
    

#####     c:\python37\lib\site-packages\pandas\core\dtypes\dtypes.py in __init__(self, categories, ordered)
        161 
        162     def __init__(self, categories=None, ordered: Ordered = False):
    --> 163         self._finalize(categories, ordered, fastpath=False)
        164 
        165     @classmethod
    

#####     c:\python37\lib\site-packages\pandas\core\dtypes\dtypes.py in _finalize(self, categories, ordered, fastpath)
        315 
        316         if categories is not None:
    --> 317             categories = self.validate_categories(categories, fastpath=fastpath)
        318 
        319         self._categories = categories

</details><details><summary>Part 2 => Categories nan</summary><blockquote>

#####     c:\python37\lib\site-packages\pandas\core\dtypes\dtypes.py in validate_categories(categories, fastpath)
        487         if not fastpath:
        488 
    --> 489             if categories.hasnans:
        490                 raise ValueError(""Categorical categories cannot be null"")
        491 
    

####    pandas\_libs\properties.pyx in pandas._libs.properties.CachedProperty.__get__()
    

#####     c:\python37\lib\site-packages\pandas\core\indexes\base.py in hasnans(self)
       2046         """"""
       2047         if self._can_hold_na:
    -> 2048             return bool(self._isnan.any())
       2049         else:
       2050             return False
    

####    pandas\_libs\properties.pyx in pandas._libs.properties.CachedProperty.__get__()
    

#####     c:\python37\lib\site-packages\pandas\core\indexes\base.py in _isnan(self)
       2026         """"""
       2027         if self._can_hold_na:
    -> 2028             return isna(self)
       2029         else:
       2030             # shouldn't reach to this condition by checking hasnans beforehand
    

#####     c:\python37\lib\site-packages\pandas\core\dtypes\missing.py in isna(obj)
        122     Name: 1, dtype: bool
        123     """"""
    --> 124     return _isna(obj)
        125 
        126 
    

#####     c:\python37\lib\site-packages\pandas\core\dtypes\missing.py in _isna(obj, inf_as_na)
        151     # hack (for now) because MI registers as ndarray
        152     elif isinstance(obj, ABCMultiIndex):
    --> 153         raise NotImplementedError(""isna is not defined for MultiIndex"")
        154     elif isinstance(obj, type):
        155         return False
    

###    NotImplementedError: isna is not defined for MultiIndex

**>>> End of Traceback <<<**

</details></details></br>

~~Seems like in both cases problem starts on **Categorical(self.grouper)** and no categories are found, that's just my doubt.~~  
Problem starts during grouper init ([line 413](https://github.com/pandas-dev/pandas/blob/master/pandas/core/groupby/grouper.py#L413)), if lambda or function is used, grouper is assigned lambda or named function respectively and [line 440](https://github.com/pandas-dev/pandas/blob/master/pandas/core/groupby/grouper.py#L440) is skipped as it does not see MultiIndex behind it.  
Later on [line 512](https://github.com/pandas-dev/pandas/blob/master/pandas/core/groupby/grouper.py#L512) changes are triggered and [line 518](https://github.com/pandas-dev/pandas/blob/master/pandas/core/groupby/grouper.py#L518) assigns actual value (MultiIndex) instead of a function, however this case skips [line 440](https://github.com/pandas-dev/pandas/blob/master/pandas/core/groupby/grouper.py#L440) casting and bug appears.

Most of the methods listed in grouper.py for lambda made **GroupBy grouper** are working, to name a few: **codes**, **ngroups**, **result_index**. Everything the _BaseGrouper_ has. Only indices and get_group do not work.

Pandas is a great module and I'm happy to use it without those indices especially that I can get the same output performing a `for` loop on .groups __PrettyDict__, yet that's a bug and usage of multiple-level group names is a potential growth area for the module.

I will be happy to continue digging on this problem further on my own and will appreciate any input on the course of this error.

#### Expected Output

```
>>> df.groupby(lambda x: df.iloc[x,0]).indices
{(0, 3): array([0, 1, 2, 3, 4], dtype=int64),
 (1, 3): array([8], dtype=int64),
 (1, 4): array([5, 6, 7, 9], dtype=int64)}
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.0
pip              : 20.2.2
setuptools       : 40.8.0
Cython           : None
pytest           : 6.0.1
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.3.4
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.19
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
708297814,36605,BUG: handle multi-dimensional grouping better,dimithras,closed,2020-09-24T16:08:07Z,2020-10-31T19:11:38Z,"As described in issue #36158 multi-dimensional groups prepared with lambda or named function raise error on indices as isnan() is not defined for MultiIndex.

Grouping already has the check for MultiIndex on line 440, but it's not triggered on __init__ when grouper is represented by **lambda** function and gets casted to MultiIndex later where MultiIndex stays as is.

~~Tested on the code provided in #36158, yet not sure if significant testing is needed as this is a relatively small change to code.  
Grouper code has not been changed for a while, should be save to merge.~~  
Tests for this particular issue provided.

~~**Concerns**: probably it will be better to make `isinstance(grouper, MultiIndex):` a function as after this change this piece of code occurs twice in __init__.~~  
Those operate on different grouper object. First one is passed on init, second one is `self`, where grouper is changed from lambda to MultiIndex and needs further change added in PR.

- [X] closes #36158
- [X] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
640405665,34849,WIP: TST: split up tests/plotting/test_frame.py into subdir & modules #34769,iahsanujunda,closed,2020-06-17T12:33:32Z,2020-10-31T19:17:09Z,"Moved `tests/plotting/test_frame.py` to `tests/plotting/frame/test_frame.py`.

Worked towards #34769 but not closing it as this has not split the test suite.

- [ ] xref #34769
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
733491043,37524,BUG: slice_canonize incorrectly raising,jbrockmendel,closed,2020-10-30T20:21:29Z,2020-10-31T19:32:58Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Doesn't appear to affect anything in master, but stumbled on it when implementing the PR do always return views when indexing on columns."
733792728,37542,DOC Add note about virtualenv and conda,MarcoGorelli,closed,2020-10-31T19:33:36Z,2020-10-31T20:39:14Z,xref https://github.com/pandas-dev/pandas/pull/37023#issuecomment-716591356
726062129,37299,API: require timezone match in DatetimeArray.shift,jbrockmendel,closed,2020-10-21T01:46:18Z,2020-10-31T20:52:33Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
729764272,37423,"REF: avoid special case in DTA/TDA.median, flesh out tests",jbrockmendel,closed,2020-10-26T17:30:04Z,2020-10-31T20:53:18Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Also fixes currently-incorrect empty case."
722660668,37145,BUG: Fix bug in quantile() for resample and groupby with Timedelta,phofl,closed,2020-10-15T20:38:45Z,2020-10-31T21:26:00Z,"- [x] closes #29485
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
728757093,37378,DOC:Pandas groupby agg example for k-hot encoding,Teut2711,closed,2020-10-24T10:11:08Z,2020-10-31T21:39:52Z,"#### Location of the documentation
https://pandas.pydata.org/pandas-docs/version/0.22.0/generated/pandas.DataFrame.aggregate.html#pandas.DataFrame.aggregate

[this should provide the location of the documentation, e.g. ""pandas.read_csv"" or the URL of the documentation, e.g. ""https://dev.pandas.io/docs/reference/api/pandas.read_csv.html""]

**Note**: You can check the latest versions of the docs on `master` [here](https://pandas.pydata.org/docs/dev/).

#### Documentation problem
It is strange that even on stackoverflow reverse explode has only a single answer(that too is asked very recently). So adding a few more examples should not be bad.
![image](https://user-images.githubusercontent.com/40588378/97079156-51586480-160f-11eb-8545-2efa6c384784.png)

[this should provide a description of what documentation you believe needs to be fixed/improved]

#### Suggested fix for documentation
The following example tells how to perform k-hot encoding with agg. 

![image](https://user-images.githubusercontent.com/40588378/97079098-ddb65780-160e-11eb-9db3-0577a034352e.png)


```
maximum = df.groupby(""ImageId"").agg({""ClassId"":lambda x:x.tolist()})[""ClassId""].max()[0]
df = df.groupby(""ImageId"").agg({""ClassId"":lambda x:x.tolist()})
df[""ClassId""] = df[""ClassId""].apply(lambda x:[1 if i+1 in x else 0 for i in range(maximum)] )
```

![image](https://user-images.githubusercontent.com/40588378/97079113-09394200-160f-11eb-90ad-5b1c38d86859.png)

[this should explain the suggested fix and **why** it's better than the existing documentation]
"
723825216,37204,"ENH: Support all Scipy window types in rolling(..., win_type)",mroeschke,closed,2020-10-17T18:49:01Z,2020-10-31T22:57:52Z,"- [x] closes #34556
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Additionally cleans up some helper methods"
705233260,36512,DOC: default value for pd.DateOffset() is 24 hours instead of 1 calendar day,GYHHAHA,closed,2020-09-21T01:43:17Z,2020-10-31T23:00:18Z,"```python
>>>pd.__version__
1.1.1
>>>ts = pd.Timestamp('2011-3-27 00:00:00', tz='Europe/Helsinki')
>>>ts + pd.DateOffset(days=1)
Timestamp('2011-03-28 00:00:00+0300', tz='Europe/Helsinki')
>>>ts + pd.DateOffset()
Timestamp('2011-03-28 01:00:00+0300', tz='Europe/Helsinki')
>>>ts + pd.DateOffset(hours=24)
Timestamp('2011-03-28 01:00:00+0300', tz='Europe/Helsinki')
```

The default action for pd.DateOffset seems to be equal to pd.DateOffset(hours=24) instead of pd.DateOffset(days=1).
But the user guide says DateOffset object defaults to 1 calendar day.

<img width=""611"" alt=""捕获"" src=""https://user-images.githubusercontent.com/41546976/93727690-db677480-fbee-11ea-9439-24979ebb85ce.PNG"">"
705271026,36516,DOC: Correct inconsistent description on default DateOffset setting,GYHHAHA,closed,2020-09-21T04:20:50Z,2020-10-31T23:50:19Z,"correct inconsistent doc description on default DateOffset setting

- [x] closes #36512 
"
733792134,37541,"REF: make EA reductions, nanops funcs keyword-only",jbrockmendel,closed,2020-10-31T19:30:48Z,2020-11-01T00:03:02Z,
730990457,37457,BUG: BusinessDay.apply_index with offset,jbrockmendel,closed,2020-10-28T01:05:48Z,2020-11-01T02:16:39Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
719087896,37072,BUG: preserve timezone info when writing empty tz-aware series to HDF5 ,arw2019,closed,2020-10-12T06:21:28Z,2020-11-01T02:24:22Z,"- [x] closes #20594 (coupled with #37069 for empty tz-aware dataframes)
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref #20595 (stale PR that looked into this)"
580127049,32666,implemented additionally functionality of formatters_col in to_latex,wgranados,closed,2020-03-12T18:11:02Z,2020-11-01T04:37:40Z,"Not sure what I should do for the whatsnew entry, bit confused about versioning. 

- [ x ] closes #26278
- [ x] tests added / passed
- [ x] passes `black pandas`
- [ x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
733802908,37543,Added example to the aggregate function,Teut2711,closed,2020-10-31T20:36:47Z,2020-11-01T06:10:25Z,"Added example of k-hot encoding to the aggregate function

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
718950223,37063,TST: Verify functioning of histtype argument (GH23992),avinashpancham,closed,2020-10-11T23:27:19Z,2020-11-01T06:31:43Z,"- [x] closes #23992 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
385694852,23992,step option does not work for hist,orena1,closed,2018-11-29T11:40:00Z,2020-11-01T06:31:43Z,"#### Code Sample, a copy-pastable example if possible

```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
N = 1000
df = pd.DataFrame({
  'a': np.random.randint(0, 2, N),
  'b': np.random.random(N)
})
plt.figure()
df.groupby('a').hist(histtype='step', ax=plt.gca())
plt.legend()
plt.show()
```

This is what I get:
![image](https://user-images.githubusercontent.com/8983713/49219292-7d471d00-f3db-11e8-9b7a-a9d6c67c2dcf.png)

#### Problem description

This function should create a step hist which is unfilled

>     - 'step' generates a lineplot that is by default
>       unfilled.
> 

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.15.final.0
python-bits: 64
OS: Linux
OS-release: 4.14.65-gentoo
machine: x86_64
processor: Intel(R) Core(TM) i9-7900X CPU @ 3.30GHz
byteorder: little
LC_ALL: None
LANG: en_IL.US-ASCII
LOCALE: None.None

pandas: 0.23.4
pytest: 3.8.0
pip: 10.0.1
setuptools: 40.2.0
Cython: 0.28.5
numpy: 1.15.1
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 5.8.0
sphinx: 1.7.9
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.8
feather: None
matplotlib: 2.2.3
openpyxl: 2.5.6
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.1.0
lxml: 4.2.5
bs4: 4.6.3
html5lib: 1.0.1
sqlalchemy: 1.2.11
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>"
728121959,37362,CI autoupdate pre-commit versions,MarcoGorelli,closed,2020-10-23T10:57:19Z,2020-11-01T10:32:02Z,"Rather than manually updating versions of hooks when new ones come out, we can make a GitHub action to open a PR to do this automatically every Monday.

For this to work, we will need whoever owns the repo to [create an access token](https://github.com/technote-space/create-pr-action#github_token) and add it as secret `ACTION_TRIGGER_TOKEN` to the repo - see screenshot below for an example

![image](https://user-images.githubusercontent.com/33491632/96995872-c9972b00-1526-11eb-9c9f-0e38d754590c.png)
"
733557228,37527,TYP: use typing.final in indexes.base,jbrockmendel,closed,2020-10-30T22:42:01Z,2020-11-01T14:47:59Z,cc @simonjayhawkins this imports final conditionally following how you did it with Literal
733300918,37520,DOC: Start v1.1.5 release notes,simonjayhawkins,closed,2020-10-30T15:56:23Z,2020-11-01T16:34:23Z,
138851133,12549,TST: Add more period tests,sinhrks,closed,2016-03-06T23:45:36Z,2020-11-01T22:12:55Z,"- [x] tests added / passed
- [x] passes `git diff upstream/master | flake8 --diff`
- Added some `Period` related tests in preparation to add period dtype.
- Moved some tests `test_timeseries` to correct test class.
"
734026829,37563,CLN: remove redundant variable in setitem_with_indexer,jbrockmendel,closed,2020-11-01T18:04:59Z,2020-11-02T00:26:12Z,
733819294,37548,CLN: de-duplicate recode_for_categories,jbrockmendel,closed,2020-10-31T22:32:38Z,2020-11-02T00:26:48Z,Step towards figuring out the small differences between Categorical's _validate_foo methods
733819674,37549,TST/REF: collect base tests by method,jbrockmendel,closed,2020-10-31T22:35:44Z,2020-11-02T00:27:05Z,
734024735,37562,TST: fix xfailing indexing test,jbrockmendel,closed,2020-11-01T17:54:23Z,2020-11-02T00:27:21Z,"When moving to the always-go-split_path, a bunch of tests fail without this fix."
733842837,37551,"REF: simplify Index.take, MultiIndex.take",jbrockmendel,closed,2020-11-01T01:29:42Z,2020-11-02T00:28:09Z,
733140910,37519,BUG: DataFrame.combine_first() works incorrect with with string indexes,sfilatov96,closed,2020-10-30T12:42:12Z,2020-11-02T00:42:21Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---
#### Code Sample, a copy-pastable example

```python
def test_astype(self):
        df_data = {'a': {4: '162', 7: '85'}, 'b': {4: NA, 7: NA}}
        df2_data = {'a': {139: '85'}, 'b': {139: NA}}
        df = DataFrame(df_data).astype({
            ""a"": 'string',
            ""b"": 'string',
        })
        df2 = DataFrame(df2_data).astype({
            ""a"": 'string',
            ""b"": 'string',
        })
        sum_df = df.set_index(['a', 'b']).combine_first(
            df2.set_index(['a', 'b']),
        ).reset_index()
        assert sum_df

```

#### Problem description
You can see that in resulting `sum_df` there are duplicate rows like ('85', nan). This behavior is incorrect. combine_first() method should join such rows


"
712403101,36758,DEPR: |&^ as set operations for Index,jbrockmendel,closed,2020-10-01T01:34:03Z,2020-11-02T01:02:40Z,"It isn't that burdensome to write `left.union(right)` instead of `left | right`, and deprecating this special-case behavior allow us to fully share the implementation with Series."
734079730,37570,TST: avoid/suppress DeprecationWarnings,jbrockmendel,closed,2020-11-01T22:43:18Z,2020-11-02T01:09:51Z,"After this, all thats left for me locally is plotting warnings about multiple subplots"
734429815,37579,refactor core,MarcoGorelli,closed,2020-11-02T11:31:33Z,2020-11-02T11:43:04Z,"Some refactorings found by Sourcery https://sourcery.ai/

I've removed the ones of the kind
```diff
- if param:
-     var = a
- else:
-     var = b
+ var = a if param else b
```"
731904942,37478,ENH: Unhelpful output from assert_frame_equal when indexes differ and check_like=True,amilbourne,closed,2020-10-29T00:36:41Z,2020-11-02T13:43:42Z,"#### Problem:

Calling ```testing.assert_frame_equal``` with mismatched indexes and ```check_like=True``` generates unhelpful output.

If you run:
```python
import pandas as pd
df1 = pd.DataFrame({""A"": [1.0, 2.0, 3.0], ""B"": [4.0, 5.0, 6.0]}, index=[""a"", ""b"", ""c""])
df2 = pd.DataFrame({""A"": [1.0, 2.0, 3.0], ""B"": [4.0, 5.0, 6.0]}, index=[""a"", ""b"", ""d""])
pd.testing.assert_frame_equal(df1, df2, check_like=True)
```
The output will be:
```
AssertionError: DataFrame.iloc[:, 0] (column name=""A"") are different

DataFrame.iloc[:, 0] (column name=""A"") values are different (33.33333 %)
[index]: [a, b, d]
[left]:  [1.0, 2.0, nan]
[right]: [1.0, 2.0, 3.0]
```
The data of the input DataFrames are not actually different (there is no nan), but when ```check_like=True``` the code calls ```left.reindex_like(right)``` before comparing indexes (and columns), in order to ensure that both frames are ordered the same.
However, if the indexes contain different values (rather than the same values in a different order),
the ```reindex_like``` function fills the data values (row or column) for the mismatched index entries with NaNs.
This results in the subsequent index checks passing, but the ```assert_frame_equals``` function failing 
with a data not equal error (as above).

Even more confusingly, if the values being compared are not floats then you get a dtype not equal error:
```
AssertionError: Attributes of DataFrame.iloc[:, 0] (column name=""A"") are different

Attribute ""dtype"" are different
[left]:  float64
[right]: int64
```

These messages are quite unhelpful, as the mismatch is in the index, and the error should logically be the same as you would get if you ran with ```check_like=False```.

#### Applies to:

The code above was run against the latest code from master.
```python
>>> print(pd.__version__)
1.2.0.dev0+950.gd321be6
```

#### Solution:

The message for the above assertion failure should be something like:
```
AssertionError: DataFrame.index are different

DataFrame.index values are different (33.33333 %)
[left]:  Index(['a', 'b', 'c'], dtype='object')
[right]: Index(['a', 'b', 'd'], dtype='object')
```
Which is what you get if you run with ```check_like=False```.

"
731914224,37479,ENH: Fix output of assert_frame_equal if indexes differ and check_like=True,amilbourne,closed,2020-10-29T01:04:40Z,2020-11-02T13:43:46Z,"- [x] closes #37478 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

As described in #37478 This PR fixes some very misleading output when ```assert_frame_equal``` is called with differing index values and ```check_like=True```

I have added a ```check_order``` parameter to the ```assert_index_equal``` function.  This does essentially the same thing as the ```check_like``` parmeter on ```assert_frame_equal``` (but for indexes).  I think ```check_order``` gives a much clearer idea of what the parameter does, but it is a break from the previous naming.  I am unsure whether I should have stuck with ```check_like``` for the new parameter."
734095740,37573,TST/REF: collect tests by method,jbrockmendel,closed,2020-11-02T00:07:52Z,2020-11-02T14:29:53Z,"This is something of an outlier in that it collects iter/iteritems/iterrows/itertuples into a test_iteration file.  I think this is a sufficiently clear grouping, akin to test_arithmetic/test_reductions/test_unary, LMK if you disagree."
734069895,37567,CLN: de-duplicate asof_locs,jbrockmendel,closed,2020-11-01T21:48:15Z,2020-11-02T14:30:26Z,"We might want to deprecate allowing DatetimeIndex `where` in `PeriodIndex.asof_locs`.  We only have one test for it, and allowing it is inconsistent with how we treat other comparisons (unless we want to start allowing dt64 in PeriodArray.searchsorted...)"
734414591,37578,sourcery refactoring,MarcoGorelli,closed,2020-11-02T11:09:14Z,2020-11-02T14:56:58Z,"I tried applying https://sourcery.ai/github/ , seems like there's some nice suggestions here. I removed the ones where they change things like
```diff
- if param:
-     var = a
- else:
-     var = b
+ var = a if param else b
```"
728596673,37374,"DEPR: Index.__and__, __or__, __xor__ behaving as set ops",jbrockmendel,closed,2020-10-23T23:25:38Z,2020-11-02T15:00:08Z,"- [x] closes #36758
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

xref #30228"
734017302,37560,REF: move _wrap_joined_index up to NDarrayBackedExtensionIndex,jbrockmendel,closed,2020-11-01T17:17:32Z,2020-11-02T15:12:42Z,"Small steps towards

a) supporting arbitrary ExtensionIndex
b) using `@final` more in index code"
734575617,37587,TST: catch FutureWarnings from Index.__and__ deprecation,jbrockmendel,closed,2020-11-02T14:59:50Z,2020-11-02T17:30:19Z,
734023911,37561,REF: move IntervalIndex.equals up to ExtensionIndex.equals,jbrockmendel,closed,2020-11-01T17:50:25Z,2020-11-02T17:45:30Z,
720969764,37110,BLD: extract GH Actions check function to avoid duplication in code_checks.sh,plammens,closed,2020-10-13T23:13:43Z,2020-11-02T19:42:09Z,"This PR extracts a function `if_gh_actions` in `ci/code_checks.sh` to avoid command/arguments duplication. Originally part of #36386 (cherry-picked from 8611fe68); extracted to make the changes more modular (see the cross-linked comment). 

- [N/A] closes #xxxx (xref https://github.com/pandas-dev/pandas/pull/36386#issuecomment-707115169)
- [N.A] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [N/A] whatsnew entry
"
734578228,37588,REF: prelims for single-path setitem_with_indexer,jbrockmendel,closed,2020-11-02T15:03:04Z,2020-11-02T21:46:43Z,Removes a pytest.skip that was supposed to have been removed along with an xfail in a previous step.
723445657,37164,ENH: __repr__ for 2D DTA/TDA,jbrockmendel,closed,2020-10-16T18:38:01Z,2020-11-02T21:47:01Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
734605180,37590,TST/REF: collect indexing tests by method,jbrockmendel,closed,2020-11-02T15:36:20Z,2020-11-02T21:47:52Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
734792323,37595,CLN: de-duplicate _validate_where_value with _validate_setitem_value,jbrockmendel,closed,2020-11-02T20:20:39Z,2020-11-02T22:23:07Z,
734594821,37589,TST/REF: collect tests by method,jbrockmendel,closed,2020-11-02T15:23:23Z,2020-11-02T22:40:14Z,
733809628,37545,REF: Categorical.is_dtype_equal -> categories_match_up_to_permutation,jbrockmendel,closed,2020-10-31T21:21:37Z,2020-11-02T22:48:23Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

There are too many notions of CategoricalDtype equality.  This gives a clearer name to one of them."
734882524,37596,TST/REF: share method tests between DataFrame and Series,jbrockmendel,closed,2020-11-02T22:35:47Z,2020-11-03T02:51:11Z,"By default, a test using the frame_or_series fixture goes in tests.frame"
734614289,37591,BUG: Index.where casting ints to str,jbrockmendel,closed,2020-11-02T15:48:14Z,2020-11-03T02:51:44Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Two things going on here (can separate if asked, but they got mixed together in the process).

1) don't re-raise in `DatetimeLikeIndexMixin.where`, so we end up giving the more standard exception message.
2) Use validate_fill_value in Index.where, and have that disallow strs for numeric.  This fixes the following in master:

```
>>> idx = pd.Index(range(3))
>>> mask = np.array([True, False, True])
>>> 
>>> idx.where(mask, ""2"")
Index(['0', '2', '2'], dtype='object')
```
"
735003399,37598,WIP: Debug high memory failure window tests,mroeschke,closed,2020-11-03T04:18:05Z,2020-11-03T04:25:38Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
735008129,37599,CI: Use conda deactivate instead of source deactivate,mroeschke,closed,2020-11-03T04:31:07Z,2020-11-03T04:37:03Z,"Seen in the logs when running `setup_env.sh`

```
DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
```"
734540348,37585,CLN refactor core/computation,MarcoGorelli,closed,2020-11-02T14:14:55Z,2020-11-03T08:12:52Z,"Some refactorings found by Sourcery https://sourcery.ai/

I've removed the ones of the kind
```diff
- if param:
-     var = a
- else:
-     var = b
+ var = a if param else b
```"
734431388,37580,CLN refactor non-core,MarcoGorelli,closed,2020-11-02T11:33:59Z,2020-11-03T08:14:17Z,"Some refactorings found by Sourcery https://sourcery.ai/

I've removed the ones of the kind
```diff
- if param:
-     var = a
- else:
-     var = b
+ var = a if param else b
```"
715067561,36895,BUG: Merging two data frames on a datetime index where one index is Object data type,PCerles,closed,2020-10-05T18:31:47Z,2020-11-03T13:30:04Z,"- [X ] I have checked that this issue has not already been reported.

- [ X] I have confirmed this bug exists on the latest version of pandas.

- [X ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import numpy as np
import pandas as pd

df = pd.DataFrame(np.zeros((5, 5)))
df.columns = list('abcde')
df2 = pd.DataFrame(columns=list('abcfg'))

df['a'] = pd.Timestamp(""2020-05-05"")

df = df.set_index(['a', 'b', 'c'])
df2 = df2.set_index(['a', 'b', 'c'])

# error
df.merge(df2, on=['a'], how='left')

# no error
out = df.reset_index().merge(df2.reset_index(), how='left', on=['a'])

# no error
df.join(df2, how='left')


```

#### Problem description

We pull a lot of data from SQL into data frames. The current pandas.from_sql does not automatically set column data types, and they get reset anyway when we call a `set_index`. So when our getters return an empty result, none of the data types are set in the multi-index.

We have a lot of instances in our production codebase where we merge two dataframes in the above pattern. In pandas 0.25.3 (last used version), a DF missing values in a level of the datetime index was not a problem. However, in pandas 1.1.2, we get an error thrown whenever one of the levels in the multi-index is a datetime index, and one of the DFs in the join has a missing dtype.

#### Expected Output

Same as df.join(df2) or the other example shown.

#### Output of ``pd.show_versions()``

<details>

commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.2
numpy            : 1.17.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 41.2.0
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : 0.4.1
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.17
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : None

</details>
"
734273273,37576,QST: Seems inconsistent behavior in MultiIndex.is_monotonic ??,itholic,closed,2020-11-02T07:45:03Z,2020-11-03T13:30:38Z,"#### Question about pandas

In the example below, I have no idea why their results are different in pandas 1.1.4.

```python
>>> pd.MultiIndex.from_tuples([('x', 'a'), ('y', 'b'), ('y', 'b'), ('z', 'a')]).is_monotonic
True
>>> pd.MultiIndex.from_tuples([('x', 'a'), ('y', 'b'), ('y', 'a'), ('z', 'a')]).is_monotonic
False
```

Is it intended behavior in pandas 1.1.4. or seems like bug ??

If this is a bug, which one is correct behavior ??

Thanks :)"
702179908,36386,CI/BLD: Restrict ci/code_checks.sh to tracked repo files,plammens,closed,2020-09-15T19:05:11Z,2020-11-03T14:42:09Z,"- [x] closes #36368 
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Previously, some of the checks in `code_checks.sh` ran unrestricted on all the
contents of the repository root (recursively), so that if any files extraneous
to the repo were present (e.g. a virtual environment directory, or generated source files), they were
checked too, potentially causing many false positives when a developer runs
`./ci/code_checks.sh` locally to check that the code is ready to be put in a PR.

The checker invocations that were already scoped (i.e. they were already
restricted, in one way or another, to the actual pandas code, e.g. by
restricting the search to the `pandas` subfolder) have been left as-is,
while those that weren't are now given an explicit list of files that are
tracked in the repo.
"
721850139,37124,REF: IntervalArray comparisons,jbrockmendel,closed,2020-10-14T23:02:47Z,2020-11-03T15:11:29Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
715097788,36897,regression fix for merging DF with datetime index with empty DF,PCerles,closed,2020-10-05T19:20:49Z,2020-11-03T18:34:08Z,"- [X ] closes https://github.com/pandas-dev/pandas/issues/36895
- [X ] tests added / passed
- [X ] passes `black pandas`
- [ X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ X] whatsnew entry
"
735392083,37602,ERR: fix error message in Period for invalid frequency,jorisvandenbossche,closed,2020-11-03T14:59:02Z,2020-11-03T19:18:30Z,"Small fix of the error message, it was missing the actual freq to fill into the message. 
On master, you get:

```
In [1]: pd.Period(""2012-01-02"", freq=""WOM-1MON"")
...
ValueError: Invalid frequency: {0}
```"
734714563,37594,API: Should Index.where/insert raise or cast on invalid other?,jbrockmendel,closed,2020-11-02T18:07:35Z,2020-11-03T20:50:25Z,"Index.where

ATM `index.where(mask, something_incompatible)` raises for EA-backed indexes and MultiIndex and casts (sometimes buggily xref #37591) for everything else.

These should all do the same thing.  I lean towards raising rather than casting.  Doing that breaks a handful of tests, but AFAICT they are all directly testing Index.where, so not used by other parts of the code.

----
Index.insert

CategoricalIndex and IntervalIndex raise on invalid, PeriodIndex casts to object, DatetimeIndex and TimedeltaIndex cast to object _only_ for strings and otherwise raise.  The base class casts but uses _coerce_scalar_to_index instead of any of our more standard patterns.

If changed to always raise, 25 tests fail.  Most of these are directly testing Index.insert so could be changed if the API changed.  The main troubling one is test_pivot_periods_with_margins, which I haven't tracked down all the way.

---
Index.putmask I'm still getting a handle on"
726367987,37306,CI: pin some builds to use pyarrow==1.0,jorisvandenbossche,closed,2020-10-21T10:54:21Z,2020-11-14T02:22:23Z,"Now that pyarrow 2.0 is released, it will get picked by the CI builds that don't pin the pyarrow version. It would be good to pin some builds to use pyarrow 1.0.

See this overview table of @simonjayhawkins at https://github.com/pandas-dev/pandas/pull/35259#issuecomment-712966527"
642493426,34918,BUG: DataFrame.(any|all) inconsistency,jbrockmendel,closed,2020-06-21T04:37:54Z,2020-11-14T02:59:30Z,
740862811,37760,DOC: test organization,jbrockmendel,closed,2020-11-11T15:38:45Z,2020-11-14T03:00:21Z,"Retry of #37637, reverted by #37756"
742900520,37825,CLN: Old shell scripts,mroeschke,closed,2020-11-14T01:57:17Z,2020-11-14T03:24:33Z,The scripts were not documented and don't seem to be used anywhere.
740575040,37755,BUG: Series.groupby() fails in pandas 1.1.4 when index has tuple name.,burk,closed,2020-11-11T08:34:09Z,2020-11-14T03:31:46Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

print(""Test 1"")
a = pd.Series([1,2,3,4], index=[1,1,2,2], name=(""a"", ""a""))
a.index.name = (""b"", ""b"")
print(a)
print(a.index)
print(a.groupby(level=0).last())

print(""Test 2"")
a = pd.Series([1,2,3,4], index=[2,3,4,5], name=(""a"", ""a""))
b = pd.Series([1,1,2,2], index=[2,3,4,5], name=(""b"", ""b""))
a.index = b.reindex(a.index)
print(a)
print(a.index)
print(a.groupby(level=0).last())
```

#### Problem description

In pandas 1.1.2 this works fine. While it crashes in pandas 1.1.4. The problem seems related to the tuple index names. The output is:
``` 
Test 1
(b, b)
1    1
1    2
2    3
2    4
Name: (a, a), dtype: int64
Int64Index([1, 1, 2, 2], dtype='int64', name=('b', 'b'))
Traceback (most recent call last):
  File ""testcase.py"", line 8, in <module>
    print(a.groupby(level=0).last())
  File ""/home/burk/.local/share/virtualenvs/timeseries-Gj3kjmJv/lib/python3.8/site-packages/pandas/core/series.py"", line 1735, in groupby
    return SeriesGroupBy(
  File ""/home/burk/.local/share/virtualenvs/timeseries-Gj3kjmJv/lib/python3.8/site-packages/pandas/core/groupby/groupby.py"", line 525, in __init__
    grouper, exclusions, obj = get_grouper(
  File ""/home/burk/.local/share/virtualenvs/timeseries-Gj3kjmJv/lib/python3.8/site-packages/pandas/core/groupby/grouper.py"", line 773, in get_grouper
    if is_in_obj(gpr):  # df.groupby(df['name'])
  File ""/home/burk/.local/share/virtualenvs/timeseries-Gj3kjmJv/lib/python3.8/site-packages/pandas/core/groupby/grouper.py"", line 765, in is_in_obj
    return gpr is obj[gpr.name]
  File ""/home/burk/.local/share/virtualenvs/timeseries-Gj3kjmJv/lib/python3.8/site-packages/pandas/core/series.py"", line 888, in __getitem__
    result = self._get_value(key)
  File ""/home/burk/.local/share/virtualenvs/timeseries-Gj3kjmJv/lib/python3.8/site-packages/pandas/core/series.py"", line 989, in _get_value
    loc = self.index.get_loc(label)
  File ""/home/burk/.local/share/virtualenvs/timeseries-Gj3kjmJv/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 2895, in get_loc
    return self._engine.get_loc(casted_key)
  File ""pandas/_libs/index.pyx"", line 70, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/index.pyx"", line 96, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/index.pyx"", line 118, in pandas._libs.index.IndexEngine._get_loc_duplicates
TypeError: only integer scalar arrays can be converted to a scalar index
```

#### Expected Output
This is the output in 1.1.2:
```
Test 1
(b, b)
1    1
1    2
2    3
2    4
Name: (a, a), dtype: int64
Int64Index([1, 1, 2, 2], dtype='int64', name=('b', 'b'))
(b, b)
1    2
2    4
Name: (a, a), dtype: int64

Test 2
(b, b)
1    1
1    2
2    3
2    4
Name: (a, a), dtype: int64
Int64Index([1, 1, 2, 2], dtype='int64', name=('b', 'b'))
(b, b)
1    2
2    4
Name: (a, a), dtype: int64
```

#### Output of ``pd.show_versions()``

<details>
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.8.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.8.0-26-generic
Version          : #27-Ubuntu SMP Wed Oct 21 22:29:16 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.4
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 44.0.0
Cython           : 0.29.17
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : 0.4.1
xlsxwriter       : None
lxml.etree       : 4.6.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : None
IPython          : 7.19.0
pandas_datareader: 0.9.0
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 2.0.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.20
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.51.2
</details>
"
742837512,37824,CLN: avoid incorrect usages of values_for_argsort,jbrockmendel,closed,2020-11-13T22:55:56Z,2020-11-14T03:41:00Z,
742013580,37799,BUG: DataFrame.all(bool_only=True) inconsistency with object dtype,jbrockmendel,closed,2020-11-12T23:52:01Z,2020-11-14T04:05:53Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Discussed on the dev call yesterday."
739557894,37733,API: consistently raise TypeError for invalid-typed fill_value,jbrockmendel,closed,2020-11-10T04:18:21Z,2020-11-14T04:22:12Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This makes us consistent about raising TypeError when passing a wrong-typed fill_value to e.g. `take`.  The downside is that we changed `Categorical.take` from TypeError to ValueError back in 1.1.0 #33660, so theres a bit of whiplash.

The upside (besides this just being More Correct) is that we will be just about fully sharing setitem-like validator methods."
742229413,37808,DOC: capitalize Python as proper noun,erictleung,closed,2020-11-13T07:37:05Z,2020-11-14T07:08:50Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
742644932,37816,TYP: copy method of EAs,simonjayhawkins,closed,2020-11-13T17:40:38Z,2020-11-14T09:35:48Z,
465144225,27286,Assignment via .loc,cruzzoe,closed,2019-07-08T09:13:58Z,2020-11-14T10:16:32Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

df = pd.DataFrame({'Data': ['afoo abc: agsegsegs', 'b def: eafsegsg', 'c ghi:']})
df2 = df

df2.loc[:, 'foo2']  = df2.Data.fillna('').astype(str).str.extract(r'.*(\w{3}).*')
print df2['foo2']
```
#### Problem description

When printing df2['foo2'] I get:
0   NaN
1   NaN
2   NaN

If instead I change the line assigning to 'foo2' to:
```python
df2['foo2']  = df2.Data.fillna('').astype(str).str.extract(r'.*(\w{3}).*')
```
When printing df2['foo2'] I get:

0    egs
1    gsg
2    ghi
  
I would have expected df.loc[:, 'foo2'] and df['foo2'] assignments to behave the same way.
 
#### Output of ``pd.show_versions()``

<details>

pandas: 0.23.4
numpy: 1.15.4
python: 2.7.14.final.0
</details>
"
742968790,37829,Backport PR #37801 on branch 1.1.x: REGR: SeriesGroupBy where index has a tuple name fails,simonjayhawkins,closed,2020-11-14T09:33:03Z,2020-11-14T10:30:29Z,Backport PR #37801
741156555,37776,BUG: Bug in xs ignored droplevel,phofl,closed,2020-11-11T23:43:44Z,2020-11-14T13:08:26Z,"- [x] closes #19056
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
742781973,37822,CI: pin build to use pyarrow==1.0,simonjayhawkins,closed,2020-11-13T20:51:04Z,2020-11-14T13:55:01Z,"closes #37306

@jorisvandenbossche picked build at random. we can discuss here what the preferences are? cc @jreback "
489934620,28303,Extreme performance difference between int and float factorization,ghost,closed,2019-09-05T18:43:23Z,2020-11-14T16:15:26Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np
from time import time


df = pd.date_range(start=""1/1/2018"", end=""1/2/2018"", periods=1e6).to_frame()

start = time()
dfr = df.resample(""1s"").last()
print(time() - start)
print(""Length:"", len(dfr))
print()

group_index = np.round(df.index.astype(int) / 1e9)
start = time()
dfr = df.groupby(group_index).last()
print(time() - start)
print(""Length:"", len(dfr))
```
#### Problem description

In my current project, I use groupby as well as resample for the same data frames with the same aggregations. I have noticed that resample is way quicker than groupby. While I understand that groupby is more flexible, it would still be nice if the performance was comparable. In the example above, resample is more than 50 times faster:

```
Length: 86401
0.023558616638183594

Length: 86401
1.264981746673584
```

I am aware that they don't result in the exact same data frames, but this does not matter for this discussion.

#### Expected Output

Better performance for groupby.

I haven't looked at the groupby implementation and therefore I don't know if there is a good reason for the difference. If there is a good reason, some common cases could still be improved a lot. For example, in this case, we could just check first if the by-argument is monotonic increasing or decreasing. In this case, the operation can be implemented even without a hash map.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-17134-Microsoft
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.1
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: 0.7.4
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.14.1
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
711461744,36729,PERF: using murmur hash for float64 khash-tables,realead,closed,2020-09-29T21:24:54Z,2020-11-14T16:16:15Z,"- [x] closes #28303
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The currently used hash-function can lead to many collisions (see #28303 or [this comment](https://github.com/pandas-dev/pandas/pull/36611#issuecomment-699551943)) for series like 0.0, 1.0, 2.0, ... n.

This PR uses a specialization (for a simple double-value) of [murmur2-hash](https://github.com/aappleby/smhasher/blob/61a0530f28277f2e850bfc39600ce61d02b518de/src/MurmurHash2.cpp#L37), which is used in [stdc++](https://github.com/gcc-mirror/gcc/blob/41d6b10e96a1de98e90a7c0378437c3255814b16/libstdc%2B%2B-v3/libsupc%2B%2B/hash_bytes.cc#L25) and [libc++](https://github.com/llvm/llvm-project/blob/1cfde143e82aeb47cffba436ba7b5302d8e14193/libcxx/include/utility#L977) and more or less state of the art.

An alternative would be to use Python's `_Py_HashDouble`, but because it has the property: `hash(1.0)=1`, `hash(2.0)=2` and so on: there is no desirable [avalanche effect ](https://en.wikipedia.org/wiki/Avalanche_effect), which is not an issue for Python's `dict` implementation, but problematic for khash as it uses a different strategy for collision handling. See #13436 as `_Py_HashDouble` was replaced through the simple hash-function used until now."
700266726,36308,BUG: DataFrameGroupBy.transform with axis=1 fails,rhshadrach,closed,2020-09-12T15:00:54Z,2020-11-14T16:50:19Z,"The following seems to happen on all transform groupby kernels:

```
df = DataFrame({""A"": [1, 2, 3], ""B"": [4, 5, 6]})
df.groupby([1, 1], axis=1).transform(""shift"")
```

results in `ValueError: Length mismatch: Expected axis has 2 elements, new values have 3 elements`.

I think the issue is in `pandas.core.groupby.generic._wrap_transformed_output`; this method does not take into account `self.axis` when wrapping the output. All that needs to happen is `result = result.T` and to use the index labels rather than the column labels for `columns` there."
700693433,36350,BUG: DataFrameGroupBy.transform with axis=1 fails (#36308),ethanator,closed,2020-09-14T01:21:44Z,2020-11-14T16:50:23Z,"- [x] closes #36308 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Updated `pandas.core.groupby.generic._wrap_transformed_output` as suggested by @rhshadrach but also had to modify `pandas.core.groupby.groupby.pct_change` to get 2 tests passed where the operation passed to `transform` is `pct_change`.

All tests passed except for `pandas/tests/io/test_clipboard.py` and `pandas/tests/io/test_parquet.py` that are already broken on the latest `master` as of September 14, 2020 01:15 UTC.
"
739941159,37742,"BUG: rolling.apply() with engine=""numba"" causes memory leak.",blackbox-tech,closed,2020-11-10T14:09:25Z,2020-11-14T18:36:46Z,"Apologies if this is a duplicate, I searched the outstanding bugs and could find this one.

I running pandas version **1.1.4** and numba version **0.51.2**

If I use the 'numba' engine in the rolling.apply() the process leaks memory.

Take this example function:

```python
def rolling_mad(values):
    @nb.njit(nogil=True)
    def _mad(x):
        return np.fabs(x - x.mean()).mean()

    df = pd.DataFrame(values)
    rolling = df.rolling(180, min_periods=1, center=True)
    return rolling.apply(_mad, engine=""numba"", raw=True).values
```

This snippet demonstrates the issue:
```python

import numpy as np
import numba as nb
import pandas as pd
import resource

values = np.random.rand(1200)

for i in range(20):
    print(f""Memory usage before call {resource.getrusage(resource.RUSAGE_SELF)[2] / 1024:.3f}KB"")
    m = rolling_mad(values)
    m = None
    print(f""Memory usage after call {resource.getrusage(resource.RUSAGE_SELF)[2] / 1024:.3f}KB"")
```

If I use engine == 'numba' there is a leak
```
Memory usage before call 121.801MB
Memory usage after call 148.547MB
Memory usage before call 148.547MB
Memory usage after call 149.320MB
Memory usage before call 149.320MB
Memory usage after call 151.625MB
Memory usage before call 151.625MB
Memory usage after call 155.672MB
Memory usage before call 155.672MB
Memory usage after call 159.473MB
Memory usage before call 159.473MB
Memory usage after call 159.988MB
Memory usage before call 159.988MB
Memory usage after call 161.535MB
Memory usage before call 161.535MB
Memory usage after call 162.566MB
Memory usage before call 162.566MB
Memory usage after call 163.512MB
Memory usage before call 163.512MB
Memory usage after call 164.543MB
Memory usage before call 164.543MB
Memory usage after call 165.316MB
Memory usage before call 165.316MB
Memory usage after call 167.121MB
Memory usage before call 167.121MB
Memory usage after call 169.184MB
Memory usage before call 169.184MB
Memory usage after call 169.957MB
Memory usage before call 169.957MB
Memory usage after call 170.730MB
Memory usage before call 170.730MB
Memory usage after call 171.762MB
Memory usage before call 171.762MB
Memory usage after call 172.277MB
Memory usage before call 172.277MB
Memory usage after call 173.051MB
Memory usage before call 173.051MB
Memory usage after call 174.082MB
Memory usage before call 174.082MB
Memory usage after call 177.500MB
```

However, if I do not use engine=""numba"" in rolling.apply() there is no memory leak.

```
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
Memory usage before call 121.801MB
Memory usage after call 121.801MB
```
"
741102719,37771,Fix pivot index bug,Jacob-Stevens-Haas,closed,2020-11-11T22:05:50Z,2020-11-14T18:46:23Z,"Previously, the variable ""cols"" was created solely to concatenate
the lists ""index"" and ""columns"" when the ""values"" parameter was
left as None.  It did so in a way that it modified the variable
passed as ""index"" using list.extend(), affecting the caller's
namespace.

This change simply uses an anonymous list in place of the
variable ""cols""

Had some trouble setting up the dev environment to build pandas from source,
so no tests checked or docs built.  However, it's fairly minor change, so hoping 
that it builds green regardless...

- [X] closes #37635 
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
742721896,37818,ENH: storage_options for to_excel,twoertwein,closed,2020-11-13T19:06:20Z,2020-11-14T18:47:34Z,"- [ ] ref #33987 (follow up to #37639)
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry: not added, already implied in whatsnew of #37639?"
741585104,37787,Fix regression for loc and __setitem__ when one-dimensional tuple was given for MultiIndex,phofl,closed,2020-11-12T13:06:14Z,2020-11-14T19:06:11Z,"- [x] closes #37711
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
691263138,36076,BUG: ser.to_frame().all() inconsistent with ser.all() with CategoryDtype,jbrockmendel,closed,2020-09-02T17:42:26Z,2020-11-14T19:38:31Z,"Based on tests.frame.test_analytics.test_any_all_np_func

```
ser = pd.Series([0, 1], dtype=""category"", name=""A"")
df = ser.to_frame()

>>> ser.all().
TypeError: Categorical cannot perform the operation all

>>> df.all().item()
False
```

Whats happening here is the DataFrame case is calling `self.values` which casts to int64 (avoidable with 2D EAs...).

If instead we refactor (xref #35881) to operate blockwise, this block is ignored, so we end up getting the result we'd get on an empty DataFrame, i.e. True.  (xref #28900 ignoring failures is a footgun).

Expected Behavior: as long as have ignore_failures, `df.all()` should be `True`"
506257396,28949,BUG: np.min on DataFrame with a non-ordered Categorical col,jbrockmendel,closed,2019-10-13T00:49:41Z,2020-11-14T19:38:31Z,"Discovered while debugging the try/excepts in groupby:

```
cat = pd.Categorical(['a', 'b', 'c', 'b'], ordered=False)
ser = pd.Series(cat)
df = ser.to_frame()
>>> np.min(cat)   # <-- correctly raises
>>> np.min(ser)   # <-- correctly raises
>>> np.min(df)     # <-- incorrectly returns Series(['a'])
```
"
322595231,21020,"Series[categorical] median raises, but DataFrame doesn't",TomAugspurger,closed,2018-05-13T13:44:51Z,2020-11-14T19:38:31Z,"This is a bit strange.

```python
In [3]: pd.DataFrame({""A"": pd.Categorical([1, 2, 2, 2, 3])})
Out[3]:
   A
0  1
1  2
2  2
3  2
4  3

In [4]: df = pd.DataFrame({""A"": pd.Categorical([1, 2, 2, 2, 3])})

In [5]: df.median()
Out[5]:
A    2.0
dtype: float64

In [6]: df.A.median()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-6-15196eaacc89> in <module>()
----> 1 df.A.median()

~/sandbox/pandas/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs)
   9587                                       skipna=skipna)
   9588         return self._reduce(f, name, axis=axis, skipna=skipna,
-> 9589                             numeric_only=numeric_only)
   9590
   9591     return set_function_name(stat_func, name, cls)

~/sandbox/pandas/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)
   3220         return delegate._reduce(op=op, name=name, axis=axis, skipna=skipna,
   3221                                 numeric_only=numeric_only,
-> 3222                                 filter_type=filter_type, **kwds)
   3223
   3224     def _reindex_indexer(self, new_index, indexer, copy):

~/sandbox/pandas/pandas/core/arrays/categorical.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)
   2065         if func is None:
   2066             msg = 'Categorical cannot perform the operation {op}'
-> 2067             raise TypeError(msg.format(op=name))
   2068         return func(numeric_only=numeric_only, **kwds)
   2069

TypeError: Categorical cannot perform the operation median
```

Anyone know whether that's intentional?"
742909883,37827,BUG: DataFrame reductions inconsistent with Series counterparts,jbrockmendel,closed,2020-11-14T02:58:28Z,2020-11-14T19:40:53Z,"- [x] closes #36076
- [x] closes #28949
- [x] closes #21020
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
743049014,37838,Fix cases of inconsistent namespacing in tests,lpkirwin,closed,2020-11-14T17:52:35Z,2020-11-14T19:42:33Z,"Sometimes e.g. Series and pd.Series are used
in the same test file. This fixes some of these
cases, generally by using the explicitly
imported class.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
743047200,37836,Backport PR #37787 on branch 1.1.x (Fix regression for loc and __setitem__ when one-dimensional tuple was given for MultiIndex),meeseeksmachine,closed,2020-11-14T17:41:48Z,2020-11-14T19:49:07Z,Backport PR #37787: Fix regression for loc and __setitem__ when one-dimensional tuple was given for MultiIndex
740673410,37758,BUG: read_fwf() does not obey skip_blank_lines,dnicolodi,closed,2020-11-11T10:57:26Z,2020-11-14T20:13:01Z,"The `read_fwf()` function does not obey the `skip_blank_lines` argument:

```python
data = io.StringIO(""""""\
 1 2 3
 4 5 6
"""""")

d = pandas.read_fwf(data, header=None, colspec=[(0, 2), (2, 4), (4, 6)])
```
results in:
```
   0  1  2
0  1  2  3
1  4  5  6
```
while
```python
data = io.StringIO(""""""\
 1 2 3

 4 5 6
"""""")

d = pandas.read_fwf(data, header=None, colspec=[(0, 2), (2, 4), (4, 6)])
```
results in
```
     0    1    2
0  1.0  2.0  3.0
1  NaN  NaN  NaN
2  4.0  5.0  6.0
```

The `skip_blank_lines` argument defaults to `True`, but also explicitly setting it results in the same behavior. The `read_fwf()` function documentation says that it takes all arguments supported by `TextFileReader` thus this is unexpected.

I have tested this with Pandas 1.1.3 as distributed by conda, however, the relevant code does not seem to have changed on master.
"
742018585,37803,BUG: skip_blank_lines ignored by read_fwf ,ma3da,closed,2020-11-13T00:05:02Z,2020-11-14T20:57:44Z,"- [x] closes #37758
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
734170204,37575,DOC: Add windows.rst,mroeschke,closed,2020-11-02T04:30:53Z,2020-11-14T21:16:14Z,"Adding a dedicated `user_guide/windows.rst` for `rolling/expanding/ewm` operations.

Mainly ported over from `computation.rst`. 
"
275711707,18409,int64 Series: += with boolean mask yielding wrong resultss,aldanor,open,2017-11-21T13:28:49Z,2020-11-14T23:20:35Z,"It looks like `__iadd__` for a view of int64 series obtained by applying a boolean mask (*without* `.loc`) works fine if mask is only composed of `True` (i.e. if `mask.all() is True`); otherwise, it seems to convert the values to float midway, and then back to int64 -- which loses precision and yields wrong results, as seen in the example below. Any ideas on the underlying reason for this? If it's not meant to be used without `.loc` indexer, should it fire a warning?

Thanks.

(I realize this may be a more generic problem, but this is the one I can report in detail)

```python
>>> import pandas as pd
>>> pd.__version__
'0.20.3'

>>> x = 123456789012345678

>>> s = pd.Series([0, 1])
>>> s += x
>>> s
0    123456789012345678
1    123456789012345679
dtype: int64
# fine, as expected

>>> s = pd.Series([0, 1])
>>> s[[True, True]] += x
>>> s
0    123456789012345678
1    123456789012345679
dtype: int64
# also fine

>>> s = pd.Series([0, 1])
>>> s[[False, True]] += x
>>> s[[True, False]] += x
>>> s
0    123456789012345680  # ????
1    123456789012345680  # ????
dtype: int64
# not fine

# 123456789012345680?
>>> int(float(x))
123456789012345680
```"
242916850,16920,Regression in s.loc._getitem_iterable(...),toobaz,closed,2017-07-14T07:15:35Z,2020-11-14T23:48:45Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: s = pd.Series(range(9), index=pd.MultiIndex.from_product([['A', 'B', 'C'], ['foo', 'bar', 'baz']],names=['one', 'two']))

In [3]: s.loc._getitem_iterable(['A', 'E'], 0)
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-3-62e87e14f902> in <module>()
----> 1 s.loc._getitem_iterable(['A', 'E'])

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_iterable(self, key, axis)
   1095             # if it cannot handle; we only act on all found values
   1096             indexer, keyarr = labels._convert_listlike_indexer(
-> 1097                 key, kind=self.name)
   1098             if indexer is not None and (indexer != -1).all():
   1099                 return self.obj.take(indexer, axis=axis)

/home/pietro/nobackup/repo/pandas/pandas/core/indexes/multi.py in _convert_listlike_indexer(self, keyarr, kind)
   1760             mask = check == -1
   1761             if mask.any():
-> 1762                 raise KeyError('%s not in index' % keyarr[mask])
   1763 
   1764         return indexer, keyarr

KeyError: ""['E'] not in index""
```
#### Problem description

(Unless/until we change our general approach to this - related discussion is in #15747,) indexing with a list containing at least one valid key should not raise an error. Indeed, before https://github.com/pandas-dev/pandas/commit/05d70f4e617a274813bdb02db69143b5554aa106 , 

```python
In [2]: s = pd.Series(range(9), index=pd.MultiIndex.from_product([['A', 'B', 'C'], ['foo', 'bar', 'baz']],names=['one', 'two']))

In [3]: s.loc._getitem_iterable(['A', 'E'])
Out[3]: 
one  two
A    foo    0
     bar    1
     baz    2
dtype: int64
```

#### Expected Output

See above. This had already been reported [here](https://github.com/pandas-dev/pandas/pull/15615#issuecomment-287826997), opening an independent issue for clarity, as suggested by @gfyoung .

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.0-3-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: it_IT.UTF-8
LOCALE: it_IT.UTF-8

pandas: 0.21.0.dev+231.g03cec7563
pytest: 3.0.6
pip: 9.0.1
setuptools: None
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.19.0
xarray: None
IPython: 5.1.0.dev
sphinx: 1.5.6
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.0.2
openpyxl: None
xlrd: 1.0.0
xlwt: 1.1.2
xlsxwriter: 0.9.6
lxml: None
bs4: 4.5.3
html5lib: 0.999999999
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
pandas_gbq: None
pandas_datareader: 0.2.1

</details>
"
743057337,37841,CI: Skip test when no xlwt or openpyxl,alimcmaster1,closed,2020-11-14T18:42:45Z,2020-11-14T23:50:40Z,"Ref: https://github.com/pandas-dev/pandas/pull/37818

Seeing error in CI: https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=47726&view=logs&j=acc1347f-8235-55b0-95c7-0e2189e61659&t=486f34f1-eda6-516a-fc5d-d8195128dacd

Skip logic same as `test_to_excel` in this file

cc @jreback "
738848452,37711,"BUG: errors with `.loc[(slice(...), ), ]`  when modifying a subset of rows in a pandas dataframe/series in 1.1.4",taozuoqiao,closed,2020-11-09T09:20:16Z,2020-11-15T03:36:03Z,"I often use `.loc[(slice(...),),]`  for selection and modifying when dealing with both series and dataframe: see 
[advanced-indexing-with-hierarchical-index](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#advanced-indexing-with-hierarchical-index)
> In [43]: df.loc['bar']
Out[43]: 
               A         B         C
second                              
one     0.895717  0.410835 -1.413681
two     0.805244  0.813850  1.607920

> This is a shortcut for the slightly more verbose notation df.loc[('bar',),] (equivalent to df.loc['bar',] in this example).

Every thinkg is OK in `0.25.4`,  but in `1.1.4` (also `1.0.3` and `1.1.3`), only selection is valid  and modifying will raise errors:

```python
import pandas as pd
s = pd.Series([1,2,3], index=pd.MultiIndex.from_tuples([('a','A'),  ('b','B'), ('c', 'C')]))
df = s.to_frame()
print(s.loc[('a', ), ])
print(df.loc[('a', ), ])
```
but when I try to modify a subset of rows using `.loc[(slice(...),),]` 
```python
In [2]: df.loc[('a', ), ] = 0
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-2-480fc1763dde> in <module>
----> 1 df.loc[('a', ), ] = 0

~/opt/anaconda3/envs/edge/lib/python3.8/site-packages/pandas/core/indexing.py in __setitem__(self, key, value)                                                                    
    664         else:
    665             key = com.apply_if_callable(key, self.obj)
--> 666         indexer = self._get_setitem_indexer(key)
    667         self._has_valid_setitem_indexer(key)
    668

~/opt/anaconda3/envs/edge/lib/python3.8/site-packages/pandas/core/indexing.py in _get_setitem_indexer(self, key)                                                                  
    591         """"""
    592         if self.name == ""loc"":
--> 593             self._ensure_listlike_indexer(key)
    594
    595         if self.axis is not None:

~/opt/anaconda3/envs/edge/lib/python3.8/site-packages/pandas/core/indexing.py in _ensure_listlike_indexer(self, key, axis)                                                        
    645             # key may be a tuple if we are .loc
    646             # in that case, set key to the column part of key
--> 647             key = key[column_axis]
    648             axis = column_axis
    649

IndexError: tuple index out of range
```
and
```python
In [3]: s.loc[('a', ), ] = 0
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-3-c3a2e38a0417> in <module>
----> 1 s.loc[('a', ), ] = 0

~/opt/anaconda3/envs/edge/lib/python3.8/site-packages/pandas/core/indexing.py in __setitem__(self, key, value)
    668 
    669         iloc = self if self.name == ""iloc"" else self.obj.iloc
--> 670         iloc._setitem_with_indexer(indexer, value)
    671 
    672     def _validate_key(self, key, axis: int):

~/opt/anaconda3/envs/edge/lib/python3.8/site-packages/pandas/core/indexing.py in _setitem_with_indexer(self, indexer, value)
   1633         if take_split_path:
   1634             # Above we only set take_split_path to True for 2D cases
-> 1635             assert self.ndim == 2
   1636             assert info_axis == 1
   1637 

AssertionError:
```

Note that “partial” indexing `.loc[slice(...)]` is still valid in ``1.1.4`:
```python
In [4]: df.loc['a'] = 0

In [5]: s.loc['a'] = 0

In [6]: df
Out[6]: 
     0
a A  0
b B  2
c C  3

In [7]: s
Out[7]: 
a  A    0
b  B    2
c  C    3
dtype: int64
```
Of course, `.loc[('a', )]` is valid for series  and  `.loc[('a', ), : ]`  is vaild for dataframe, but I have to use different codes for series and dataframe.

Update on 20201113:
-----------------------
- For series, `s.loc[('a')] = 0`, `s.loc[('a', )] = 0` and  `s.loc[('a'), ] = 0` are vaild,  but `s.loc[('a', ), ] = 0` will raise similar AssertionError like above.
- For dataframe, `df.loc[('a')] = 0`, `df.loc[('a'), :] = 0` and  `df.loc[('a',), :] = 0` are vaild, but `df.loc[('a', )] = 0` , `df.loc[('a'), ] = 0` and `df.loc[('a', ), ] = 0` will raise similar IndexError like above."
743157863,37857,CLN: remove something.xlsx,twoertwein,closed,2020-11-15T01:31:32Z,2020-11-15T04:17:10Z,"`test_register_writer` in `pandas/tests/io/excel/test_writers.py` wasn't expecting that `something.xls` and `something.xlsx` will ever be created.

Since #37639, `ExcelWriter` opens the file which will create it."
743060324,37843,namespace consistency for test_dtype.py,reshamas,closed,2020-11-14T19:01:18Z,2020-11-15T09:03:51Z,"- [x ] references #37838
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
743059848,37842,Fix cases of inconsistent namespacing in tests,jan-mue,closed,2020-11-14T18:58:32Z,2020-11-15T09:09:07Z,"Additional fixes for MarcoGorelli/PyDataGlobal2020-sprint#1.
Continues the work of lpkirwin in #37838.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
291788131,19406,Boolean indexing and assignment from python list broken,Dobatymo,closed,2018-01-26T03:14:51Z,2020-11-15T09:12:13Z,"There is a problem with assigning lists to indexed Series
```python
from math import nan
import pandas as pd
s1 = pd.Series([nan, nan, ""c""])
s1[s1.isnull()] = [""a"",""b""]
s2 = pd.Series([nan, ""b"", nan])
s2[s2.isnull()] = [""a"",""c""]
#s1: ""a"", ""b"", ""c""
#s2: ""a"", ""b"", ""a""
```

`s1` shows the correct result, whereas `s2` is obviously wrong. In pandas `0.19.2` the result is correct, but in `0.21.0` and `0.23.0.dev0+97.g24c07b075` I get the above behavior. It can be fixed if a numpy array instead of a python list is assigned.

```python
from math import nan
import pandas as pd
import numpy as np
s1 = pd.Series([nan, nan, ""c""])
s1[s1.isnull()] = np.array([""a"",""b""])
s2 = pd.Series([nan, ""b"", nan])
s2[s2.isnull()] = np.array([""a"",""c""])
#s1: ""a"", ""b"", ""c""
#s2: ""a"", ""b"", ""c""
```

There probably should be a test case to catch that...

#### Output of ``pd.show_versions()``
<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.4.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en_US
LOCALE: None.None

pandas: 0.23.0.dev0+97.g24c07b075
pytest: 3.2.3
pip: 9.0.1
setuptools: 38.2.4
Cython: 0.27.3
numpy: 1.14.0
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2017.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.0.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: 4.6.0
html5lib: 0.9999999
sqlalchemy: 1.1.13
pymysql: 0.7.11.None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>"
743067660,37849,Backport PR #37787 on branch 1.1.x (Fix regression for loc and __setitem__ when one-dimensional tuple was given for MultiIndex),phofl,closed,2020-11-14T19:48:25Z,2020-11-15T11:16:31Z,"Backport #37787

``frame_or_series`` not know here.

cc @simonjayhawkins "
742693998,37817,TYP: _concat_same_type method of EA,simonjayhawkins,closed,2020-11-13T18:35:17Z,2020-11-15T11:36:16Z,some overlap with #37816
743055262,37840,TST: Add test for setting item using python list (#19406),paxcodes,closed,2020-11-14T18:29:33Z,2020-11-15T13:44:22Z,"- [X] closes #19406 
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [NA] whatsnew entry
"
743165546,37858,CI: xfail a windows test,jbrockmendel,closed,2020-11-15T02:41:45Z,2020-11-15T15:15:14Z,At least we can fix a subset of the windows builds.
718681211,37037,Fixed metadata propagation in DataFrame.__getitem__,TomAugspurger,closed,2020-10-10T18:58:19Z,2020-11-15T17:27:53Z,xref #28283
743048677,37837,CLN: remove unnecessary close calls and add a few necessary ones,twoertwein,closed,2020-11-14T17:50:45Z,2020-11-15T17:29:32Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Follow up to #37639. Close file handles as early as possible.

Remove `close` calls that are already within a try-except/finally block that closes the file handles.

Add `close` call for `read_json` when we read the entire content in one go.

Add close call in `read_csv` when reaching the end of the iterator.

Put `to/read_excel` in a try-finally block to make sure that file handles are closed even in case of an exception."
711736654,36738,BUG: to_timedelta drops decimals from input if precision is greater than nanoseconds,Gvxy2,closed,2020-09-30T08:16:52Z,2020-11-15T17:31:47Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

I am trying to convert a time to timedelta. The input data is in string format. When number of decimals is too big, it resets all decimals to zero.

```python
import pandas as pd
d = {'Time':['8:53:08.26','8:53:08.71800000001', '8:53:09.729']}
df = pd.DataFrame(data=d)
pd.to_timedelta(df[""Time""])
```
The result of the above pice of code gives:
```
0   0 days 08:53:08.260000
1          0 days 08:53:08
2   0 days 08:53:09.729000
Name: Time, dtype: timedelta64[ns]
```

As it can be seen, all the decimals from second data is lost.

#### Problem description

With the present behavior, a sorted array of data returns a wrong and unsorted array.

#### Expected Output
```
0   0 days 08:53:08.260000
1   0 days 08:53:08.718000
2   0 days 08:53:09.729000
Name: Time, dtype: timedelta64[ns]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 7
Version          : 6.1.7601
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : es_ES.cp1252

pandas           : 1.1.1
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200925

</details>
"
712781329,36771,BUG: Timedelta drops decimals if precision is greater than nanoseconds,phofl,closed,2020-10-01T12:09:11Z,2020-11-15T17:32:28Z,"- [x] closes #36738
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I am not quite sure, if we should round to nanoseconds or cut if off after the 9th decimal. I implemented a simple cut off, but would be happy to change if rounding is better.

I would also add a note to the user guide if the future behavior is clear"
277174751,18531,Inconsistent handling of empty slices with non-monotonic indexes,TomAugspurger,closed,2017-11-27T21:26:58Z,2020-11-15T17:39:59Z,"These two should probably behave the same

`df1` has a non-monotonic datetimeindex, and we slice with values that don't fall in the index.
`df2` has a non-monotonic index of strings, which we slice values outside the index.

```python
In [9]: import pandas as pd
   ...:
   ...: df1 = pd.DataFrame({""A"": [1, 2, 3]},
   ...:                    index=[pd.Timestamp('2017'),
   ...:                           pd.Timestamp('2019'),
   ...:                           pd.Timestamp('2018')])
   ...: df2 = pd.DataFrame({""A"": [1, 2, 3]},
   ...:                    index=['a', 'c', 'b'])
   ...:

In [10]: df1.loc['2020':'2022']
Out[10]:
Empty DataFrame
Columns: [A]
Index: []

In [11]: df2.loc['d':'e']
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/indexes/base.py in get_slice_bound(self, label, side, kind)
   3664             try:
-> 3665                 return self._searchsorted_monotonic(label, side)
   3666             except ValueError:

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/indexes/base.py in _searchsorted_monotonic(self, label, side)
   3623
-> 3624         raise ValueError('index must be monotonic increasing or decreasing')
   3625

ValueError: index must be monotonic increasing or decreasing

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
<ipython-input-11-e86df68316ba> in <module>()
----> 1 df2.loc['d':'e']

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/indexing.py in __getitem__(self, key)
   1367
   1368             maybe_callable = com._apply_if_callable(key, self.obj)
-> 1369             return self._getitem_axis(maybe_callable, axis=axis)
   1370
   1371     def _is_scalar_access(self, key):

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/indexing.py in _getitem_axis(self, key, axis)
   1575         if isinstance(key, slice):
   1576             self._has_valid_type(key, axis)
-> 1577             return self._get_slice_axis(key, axis=axis)
   1578         elif is_bool_indexer(key):
   1579             return self._getbool_axis(key, axis=axis)

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/indexing.py in _get_slice_axis(self, slice_obj, axis)
   1400         labels = obj._get_axis(axis)
   1401         indexer = labels.slice_indexer(slice_obj.start, slice_obj.stop,
-> 1402                                        slice_obj.step, kind=self.name)
   1403
   1404         if isinstance(indexer, slice):

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/indexes/base.py in slice_indexer(self, start, end, step, kind)
   3529         """"""
   3530         start_slice, end_slice = self.slice_locs(start, end, step=step,
-> 3531                                                  kind=kind)
   3532
   3533         # return a slice

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/indexes/base.py in slice_locs(self, start, end, step, kind)
   3730         start_slice = None
   3731         if start is not None:
-> 3732             start_slice = self.get_slice_bound(start, 'left', kind)
   3733         if start_slice is None:
   3734             start_slice = 0

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/indexes/base.py in get_slice_bound(self, label, side, kind)
   3666             except ValueError:
   3667                 # raise the original KeyError
-> 3668                 raise err
   3669
   3670         if isinstance(slc, np.ndarray):

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/indexes/base.py in get_slice_bound(self, label, side, kind)
   3660         # we need to look up the label
   3661         try:
-> 3662             slc = self._get_loc_only_exact_matches(label)
   3663         except KeyError as err:
   3664             try:

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/indexes/base.py in _get_loc_only_exact_matches(self, key)
   3629         get_slice_bound.
   3630         """"""
-> 3631         return self.get_loc(key)
   3632
   3633     def get_slice_bound(self, label, side, kind):

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2529                 return self._engine.get_loc(key)
   2530             except KeyError:
-> 2531                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2532
   2533         indexer = self.get_indexer([key], method=method, tolerance=tolerance)

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()
    137             util.set_value_at(arr, loc, value)
    138
--> 139     cpdef get_loc(self, object val):
    140         if is_definitely_invalid_key(val):
    141             raise TypeError(""'{val}' is an invalid key"".format(val=val))

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()
    159
    160         try:
--> 161             return self.mapping.get_item(val)
    162         except (TypeError, ValueError):
    163             raise KeyError(val)

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()
   1263                                        sizeof(uint32_t)) # flags
   1264
-> 1265     cpdef get_item(self, object val):
   1266         cdef khiter_t k
   1267         if val != val or val is None:

~/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()
   1271             return self.table.vals[k]
   1272         else:
-> 1273             raise KeyError(val)
   1274
   1275     cpdef set_item(self, object key, Py_ssize_t val):

KeyError: 'd'
```

cc @jreback @shughes-uk"
742725927,37819,Deprecate partial slicing of unordered DatetimeIndex when both keys are not present,phofl,closed,2020-11-13T19:13:00Z,2020-11-15T17:41:54Z,"- [x] xref #18531
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

cc @jreback @jorisvandenbossche 

This would deprecate #37796 before removing it. "
743068668,37851,CLN: remove no-longer-used ignore_failures in frame_apply,jbrockmendel,closed,2020-11-14T19:54:49Z,2020-11-15T18:14:33Z,
743083165,37853,CLN: indexing plane_indexer -> pi,jbrockmendel,closed,2020-11-14T21:30:51Z,2020-11-15T18:15:00Z,"We know plane_indexer is a 1-tuple, can simplify a bit."
743217946,37861,BUG: Prevent Series.isin from unwantedly casting isin values from float to integer (GH21804),avinashpancham,closed,2020-11-15T09:26:58Z,2020-11-15T18:26:09Z,"- [x] closes #21804 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
743086128,37855,DOC: Add a reference to window.rst in computation.rst,mroeschke,closed,2020-11-14T21:52:54Z,2020-11-15T19:44:11Z,"xref https://github.com/pandas-dev/pandas/pull/37575#issuecomment-727235030
"
743062815,37845,DOC: SS01 docstrings errors fixed,lucasrodes,closed,2020-11-14T19:16:28Z,2020-11-15T20:55:14Z,"xref #27977
- [ ] closes part of [MarcoGorelli/PyDataGlobal2020-sprint#4](https://github.com/MarcoGorelli/PyDataGlobal2020-sprint/issues/4)
- [ ] tests added / passed
- [X] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
743352266,37876,CLN: Adding comment/more info about a possible alternative approach to murmur2-hash,realead,closed,2020-11-15T21:47:40Z,2020-11-15T23:12:32Z,"Small comment as requested here: https://github.com/pandas-dev/pandas/pull/36729#discussion_r522956690
"
743361996,37879,CI: DataFrame.apply(np.any),jbrockmendel,closed,2020-11-15T22:35:51Z,2020-11-16T00:41:01Z,Should fix these failures https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=47831&view=logs&j=f016abb9-7827-5fa2-935a-22bd9b1477b6&t=c86edbe1-8c1d-5e5c-5b6f-d970fa4acf6d
743166226,37859,"QST: Is the behavior between ""Index.astype"" and ""Series.astype"" different ??",itholic,closed,2020-11-15T02:48:48Z,2020-11-16T07:48:12Z,"#### Question about pandas

When I cast the type with `astype` for `Index` or `Series`,

I noticed that their behavior for `bool` casting is slightly different.

```python
>>> pd.Series([1, None]).astype(bool)
0    True
1    True
dtype: bool

>>> pd.Index([1, None]).astype(bool)
Index([True, False], dtype='object')
```

As shown above, `None` is casted `True` from `Series.astype`, but casted `False` from `Index.astype`.

Is this normal for some reason??

I used pandas 1.1.4.

Thanks :)"
743068932,37852,making namespace usage more consistent,christopherhadley,closed,2020-11-14T19:56:38Z,2020-11-16T10:00:16Z,"- [x] closes #37838
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
651677941,35144,"BUG: pd.crosstab fails when passed multiple columns, margins True and normalize True",HughKelley,closed,2020-07-06T16:41:57Z,2020-11-16T16:48:16Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame({""A"": [""foo"", ""foo"", ""foo"", ""foo"", ""foo"",
          ""bar"", ""bar"", ""bar"", ""bar""],
                    ""B"": [""one"", ""one"", ""one"", ""two"", ""two"",
                          ""one"", ""one"", ""two"", ""two""],
                    ""C"": [""small"", ""large"", ""large"", ""small"",
                          ""small"", ""large"", ""small"", ""small"",
                          ""large""],
                    ""D"": [1, 2, 2, 3, 3, 4, 5, 6, 7],
                    ""E"": [2, 4, 5, 5, 6, 6, 8, 9, 9]})

pd.crosstab(index=df.C,
            columns=[df.A,df.B],  
            margins=True,
            margins_name='Sub-Total',
            normalize='all')

# returns ValueError: Sub-Total not in pivoted DataFrame
```

#### Problem description

#27663 found that `pd.crosstab` failed when `margins` and `normalize` were true.  This continues to be the case when more than one column is passed. 

This is not the case when a single column and multiple rows is passed.


#### Output of ``pd.show_versions()``


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 47.1.1.post20200604
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : 1.2.9
numba            : None

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
743431477,37886,REF: Use more memory views in rolling aggregations,mroeschke,closed,2020-11-16T03:02:33Z,2020-11-16T17:02:48Z,- [x] tests added / passed
743061014,37844,DOC: add examples to docstrings of assert_ functions,nguevara,closed,2020-11-14T19:05:17Z,2020-11-16T20:39:36Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
343072623,21989,iloc calls should not reach indexes code,toobaz,closed,2018-07-20T11:27:40Z,2020-11-17T00:15:16Z,"Related to #21982, which however only fixes one particular case: there are more.

Will soon push a branch with more substantial changes - which however is still not ready (breaks some tests)."
744273272,37903,CLN: Unused file 'path_to_file.zip',lucasrodes,closed,2020-11-16T23:07:46Z,2020-11-17T01:10:54Z,"New file [path_to_file.zip](https://github.com/pandas-dev/pandas/blob/master/scripts/path_to_file.zip) was added in #37844.

I believe this is file is (accidentally?) generated when running [scripts/validate_docstrings.py](https://github.com/pandas-dev/pandas/blob/master/scripts/validate_docstrings.py) and probably is not needed in the repository."
641973038,34871,BUG: Inconsistent behavior for df.replace over pd.Period columns,justinessert,closed,2020-06-19T13:33:41Z,2020-11-17T01:17:29Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame({
    'Int': [1, 2, 3],
    'Flo': [1.0, 6.0, 1.0],
    'Per': [pd.Period('2020-01')]*3,
    'Tim': [pd.Timestamp('2020-01')]*3
})

# TypeError: 'value' should be a 'Period', 'NaT', or array of those. Got 'float' instead.
df.replace(1.0, 0.0)

# Works
df[['Int', 'Flo', 'Tim']].replace(1.0, 0.0)

# Works
df['Per'].replace(1.0, pd.Period('2020-02'))

```

#### Problem description

`df.replace(1.0, 0.0)` fails if any columns of `df` are Period types. The error is saying that 0.0 is an invalid value to put in a Pandas Period column. This should never be an issue though, 1.0 will never exist in a Pandas Period column, so nothing will ever be replaced for 0.0.

#### Expected Output

The expected behavior would be consistent with the `df[['Int', 'Flo', 'Tim']].replace(1.0, 0.0)` in the example above. This does work, the 'Flo' column has all of it's 1.0s replaced with 0.0s and the 'Int' and 'Tim' columns (which are not Float types) remain unaffected. I would expect that a Pandas Period column would also be skipped in the same way. 

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.5.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.4
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.0
pip              : 20.1.1
setuptools       : 47.3.0.post20200616
Cython           : None
pytest           : 4.4.1
hypothesis       : None
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.1
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 4.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.9
numba            : None

</details>
"
446038212,26468,BUG: setitem with boolean mask and series as value is broken for Series with EA type,jorisvandenbossche,closed,2019-05-20T10:23:40Z,2020-11-17T01:21:38Z,"Consider the following example (on master, 0.25.0dev) where the value being assigned to `s[boolean_mask] = value` is a Series itself (of the correct length + matching index):

```
In [1]: df = pd.DataFrame({'a': [0, 0, np.nan, np.nan], 'b': pd.array(range(4), dtype='Int64')})

In [2]: s = df['b'].copy()

In [3]: s[df['a'].isna()] = df.loc[df['a'].isna(), 'b']         
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-04e8761e7ad9> in <module>
----> 1 s[df['a'].isna()] = df.loc[df['a'].isna(), 'b']

~/scipy/pandas/pandas/core/series.py in __setitem__(self, key, value)
   1055         # do the setitem
   1056         cacher_needs_updating = self._check_is_chained_assignment_possible()
-> 1057         setitem(key, value)
   1058         if cacher_needs_updating:
   1059             self._maybe_update_cacher()

~/scipy/pandas/pandas/core/series.py in setitem(key, value)
   1046                 key = check_bool_indexer(self.index, key)
   1047                 try:
-> 1048                     self._where(~key, value, inplace=True)
   1049                     return
   1050                 except InvalidIndexError:

~/scipy/pandas/pandas/core/generic.py in _where(self, cond, other, inplace, axis, level, errors, try_cast)
   8819             new_data = self._data.putmask(mask=cond, new=other, align=align,
   8820                                           inplace=True, axis=block_axis,
-> 8821                                           transpose=self._AXIS_REVERSED)
   8822             self._update_inplace(new_data)
   8823 

~/scipy/pandas/pandas/core/internals/managers.py in putmask(self, **kwargs)
    511 
    512     def putmask(self, **kwargs):
--> 513         return self.apply('putmask', **kwargs)
    514 
    515     def diff(self, **kwargs):

~/scipy/pandas/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)
    393                                             copy=align_copy)
    394 
--> 395             applied = getattr(b, f)(**kwargs)
    396             result_blocks = _extend_blocks(applied, result_blocks)
    397 

~/scipy/pandas/pandas/core/internals/blocks.py in putmask(self, mask, new, align, inplace, axis, transpose)
   1593         mask = _safe_reshape(mask, new_values.shape)
   1594 
-> 1595         new_values[mask] = new
   1596         new_values = self._try_coerce_result(new_values)
   1597         return [self.make_block(values=new_values)]

~/scipy/pandas/pandas/core/arrays/integer.py in __setitem__(self, key, value)
    399             mask = mask[0]
    400 
--> 401         self._data[key] = value
    402         self._mask[key] = mask
    403 

ValueError: NumPy boolean array indexing assignment cannot assign 4 input values to the 2 output values where the mask is true
```

The reason this fails is because under the hood, the assigned value is aligned with the Series, turning it into a 4-len series, and then trying to assign this to an array with a boolean mask for 2 values.

"
466018291,27315,some argument combinations with reindex fails on an empty dataframe,ajspera,closed,2019-07-09T22:50:21Z,2020-11-17T01:26:21Z,"```python
import pandas as pd
from datetime import datetime, timedelta

end = datetime.utcnow()
begin = end - timedelta(minutes=1)
data_interval = 10
date_index = pd.date_range(start=begin,
                           end=end,
                           freq='{} s'.format(data_interval))
df = pd.DataFrame([], columns=['time','a','b'])
df = df.set_index('time', drop=True)
tol = timedelta(seconds=9)
df = df.reindex(date_index, method='pad', tolerance=tol)

# IndexError: index -1 is out of bounds for axis 0 with size 0

df = pd.DataFrame([], columns=['time','a','b'])
df = df.reindex(date_index, method='nearest')

# IndexError: index -1 is out of bounds for axis 0 with size 0

```
You get an index error when a dataframe is empty using the `tolerance=` or `method='nearest'` .

This is not something that happens with other usages of reindex and can come up as a surprise when reindexing an empty window of data. I would expect it to behave the same as it does without tolerance here.

#### Expected Output
Should be same as reindex with no args in this case which returns...
```
                              a    b
2019-07-09 22:35:05.165640  NaN  NaN
2019-07-09 22:35:15.165640  NaN  NaN
2019-07-09 22:35:25.165640  NaN  NaN
2019-07-09 22:35:35.165640  NaN  NaN
2019-07-09 22:35:45.165640  NaN  NaN
2019-07-09 22:35:55.165640  NaN  NaN
2019-07-09 22:36:05.165640  NaN  NaN
```

#### Temp Solution
Simple user solution is to check length... but this is a problem that might surprise someone at a bad time like it did for us.
``` python
if(len(df) is 0):
    df = df.reindex(date_index)
else:
    df = df.reindex(date_index, method='pad', tolerance=tol)
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.5.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-54-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 5.0.1
pip: 18.0
setuptools: 40.4.1
Cython: None
numpy: 1.16.4
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: 1.1.8
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: 1.3.5
pymysql: 0.9.3
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: 0.6.1
pandas_datareader: None
gcsfs: None

</details>
"
743428172,37885,REF: Index._intersection,jbrockmendel,closed,2020-11-16T02:51:39Z,2020-11-17T01:26:47Z,"Follow the same pattern as Index._union.  Motivated by trying to get stricter typing in CategoricalIndex._shallow_copy, which is blocked by pinning down return types for set ops."
743376837,37881,REF: simplify setitem_with_indexer,jbrockmendel,closed,2020-11-15T23:54:02Z,2020-11-17T01:28:59Z,Good news: this is the last pure-cleanup before we start actually changing the logic.
714379641,36867,BUG: df.replace over pd.Period columns (#34871),samc1213,closed,2020-10-04T19:01:03Z,2020-11-17T01:31:42Z,"- [x] closes #34871
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This commit ensures that PeriodArrays return False for _can_hold_element for any element that is not a pd.Period. This prevents upstream code from casting the dtype to object. Also un-xfail test written in #34904"
743063208,37846,Added docs to fix GL08 errors in CustomBusinessHours,mohdkashif93,closed,2020-11-14T19:18:58Z,2020-11-17T01:32:36Z,"- [ ] xref #27977
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Currently even after adding the docstrings the command `python scripts/validate_docstrings.py --errors=GL08` does not pick up the changes."
743427584,37884,REF: make casting explicit in CategoricalIndex,jbrockmendel,closed,2020-11-16T02:49:43Z,2020-11-17T01:32:52Z,Motivated by trying to get stricter typing in CategoricalIndex._shallow_copy.
743181923,37860,CLN: use putmask_simple instead of putmask,jbrockmendel,closed,2020-11-15T04:59:22Z,2020-11-17T01:34:21Z,
743332521,37871,REF: share IntervalIndex._check_method with CategoricalIndex,jbrockmendel,closed,2020-11-15T20:00:07Z,2020-11-17T01:35:55Z,
743510869,37888,"BUG: df.str.contains(XX) return False when XX contains ""+"" character",TakuNishiumi,closed,2020-11-16T06:25:42Z,2020-11-17T02:28:12Z,"- [ y] I have checked that this issue has not already been reported.

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame([""ABC"", ""ABC+534""], index=[""AA"",""BB""]).T
df[""BB""].str.contains(""ABC+5"")
```
returns 
```
0    False
Name: BB, dtype: bool
```

#### Problem description
When I ran the `df.str.contains(XX)`, I found a new bug.
I run the below notebook, it returns False, despite it is definitely include keyword with in the cell.


"
743336774,37872,REF: share _simple_new,jbrockmendel,closed,2020-11-15T20:22:34Z,2020-11-17T03:14:50Z,"@simonjayhawkins two questions: 1) is there a nice way to annotate the `values` arg (maybe a Protocol thing?) and 2) is the use of `cls.__annotations__[""_data""]` an egregious antipattern?"
723031135,37158,BUG: unexpected behaviour using `.sample` without replacement and with weights as a series,theMogget,closed,2020-10-16T08:48:11Z,2020-11-17T04:24:28Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample

```python
import pandas as pd

elems = [0, 1, 2, 3, 4]
series_test = pd.Series(elems)
long_bad_weights = [0.1, 0.2] * 3
long_bad_weights_series = pd.Series([0.1, 0.2] * 3)
short_bad_weights_series = pd.Series([0.1, 0.2])

### Case 1: Weights passed in as a list
# throws pandas error - if weight list is too long or short, also throws with replace=True and replace=False
series_test.sample(weights=long_bad_weights)

### Case 2: Weights passed in as a series, with replace=True
series_test.sample(weights=long_bad_weights_series)  # throws no error
series_test.sample(n=3, weights=short_bad_weights_series, replace=True)  # throws no error
series_test.sample(n=3, weights=long_bad_weights_series, replace=True)  # throws no error

### Case 3: Weights passed in as a series, with replace=False
series_test.sample(n=3, weights=long_bad_weights_series, replace=False)  # throws no error
series_test.sample(n=3, weights=short_bad_weights_series, replace=False)  # throws numpy error
```

#### Problem description

Passing in weights that do not match the length of the axis to be sampled throws a pandas error (Case 1). However, if passing in weights as a series, `.sample` accepts it despite unequal lengths. 

This behaviour seems rather inconsistent to me. It results an error possibly being thrown by numpy, _if the weights are too short_, depending on whether `replace=True` or `replace=False` (Case 2, Case 3). 

In the code for `.sample` with weights passed in as a series, we reindex the weights, leading to `np.nan`s if the weights were too short.

https://github.com/pandas-dev/pandas/blob/c2a96c644e51f4d164b657240b7bed5f39fd349c/pandas/core/generic.py#L5200-L5202

This can later throw an error from numpy, since random.choice doesn't accept sampling with replacement if `np.nan` is in the weights.


**Case 1 pandas error**
```
Traceback (most recent call last):
  File ""/python-scratchpad/main.py"", line 11, in <module>
    df_test.sample(weights=long_bad_weights)
  File ""/python-scratchpad/venv/lib/python3.8/site-packages/pandas/core/generic.py"", line 4950, in sample
    raise ValueError(
ValueError: Weights and axis to be sampled must be of same length
```
**Case 3 numpy error**
```
Traceback (most recent call last):
  File ""/repositories/python-scratchpad/main.py"", line 20, in <module>
    series_test.sample(n=3, weights=short_bad_weights_series, replace=False)  # throws numpy error
  File ""/repositories/python-scratchpad/venv/lib/python3.8/site-packages/pandas/core/generic.py"", line 4993, in sample
    locs = rs.choice(axis_length, size=n, replace=replace, p=weights)
  File ""mtrand.pyx"", line 961, in numpy.random.mtrand.RandomState.choice
ValueError: Fewer non-zero entries in p than size
```

#### Expected Output

I'd expect pandas to throw a weight length error, no matter the dtype the weights are passed in as. Is there a reason for accepting weights as a series regardless of length? 

#### Output of ``pd.show_versions()``

<details>

```INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.0.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8
pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
```

</details>
"
743349007,37874,BUG in reindex raises IndexingError when df is empty sometimes,phofl,closed,2020-11-15T21:28:31Z,2020-11-17T08:35:37Z,"- [x] closes #27315
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Build on top of #27326"
738087686,37676,BUG: setitem with boolean mask and series as value is broken for Series with EA type,phofl,closed,2020-11-06T22:51:37Z,2020-11-17T08:35:51Z,"- [x] closes #26468
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Was fixed already, added a test"
743654734,37891,CI upgrade pyupgrade to v2.7.4,MarcoGorelli,closed,2020-11-16T09:02:25Z,2020-11-17T09:23:09Z,"This should just be a matter of changing the version number to v2.7.4 for pyupgrade in .pre-commit-config.yaml, and checking that all still passes:
```
pre-commit run pyupgrade --all-files
```
If it does, please open a PR to upgrade - else, report on any failures"
708447021,36611,"PERF: Always using panda's hashtable approach, dropping np.in1d",realead,closed,2020-09-24T20:16:34Z,2020-11-17T12:59:27Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

numpy's `in1d` uses look-up in `O(log n)` in `in1d` compared to panda's `O(1)` so for a large number of unique elements in `values` pandas solution will become better.

Here is comparison in the performance benchmark:

```
       before           after         ratio
     [27aae225]       [796a7dbc]
+     11.4±0.07ms       48.8±0.3ms     4.29  series_methods.IsInLongSeries.time_isin('float64', 1)
+      19.4±0.2ms       59.5±0.6ms     3.06  series_methods.IsInLongSeries.time_isin('float64', 2)
+      13.1±0.1ms       39.2±0.5ms     2.98  series_methods.IsInLongSeries.time_isin('int64', 1)
+      26.9±0.4ms       64.0±0.7ms     2.38  series_methods.IsInLongSeries.time_isin('float32', 1)
+      34.5±0.2ms       74.9±0.9ms     2.17  series_methods.IsInLongSeries.time_isin('float32', 2)
+      23.2±0.1ms       44.5±0.3ms     1.92  series_methods.IsInLongSeries.time_isin('int64', 2)
+      28.1±0.4ms       53.8±0.4ms     1.91  series_methods.IsInLongSeries.time_isin('int32', 1)
+      42.7±0.2ms       70.3±0.3ms     1.64  series_methods.IsInLongSeries.time_isin('float64', 5)
+      38.2±0.3ms       59.1±0.5ms     1.55  series_methods.IsInLongSeries.time_isin('int32', 2)
+      57.4±0.3ms       85.3±0.5ms     1.49  series_methods.IsInLongSeries.time_isin('float32', 5)
-        83.2±1ms       76.6±0.5ms     0.92  series_methods.IsInLongSeries.time_isin('float64', 10)
-         118±6ms       69.8±0.2ms     0.59  series_methods.IsInLongSeries.time_isin('int32', 10)
-         102±1ms       54.9±0.3ms     0.54  series_methods.IsInLongSeries.time_isin('int64', 10)
-         515±3ms          216±1ms     0.42  series_methods.IsInLongSeries.time_isin('int32', 100000)
-         498±3ms          201±1ms     0.40  series_methods.IsInLongSeries.time_isin('int64', 100000)
-         529±2ms          154±7ms     0.29  series_methods.IsInLongSeries.time_isin('float32', 100000)
-         529±3ms        131±0.7ms     0.25  series_methods.IsInLongSeries.time_isin('float64', 100000)
-         519±5ms        107±0.6ms     0.21  series_methods.IsInLongSeries.time_isin('int32', 1000)
-         499±3ms       91.3±0.3ms     0.18  series_methods.IsInLongSeries.time_isin('int64', 1000)
-         534±4ms         87.6±3ms     0.16  series_methods.IsInLongSeries.time_isin('float32', 1000)
-         519±2ms       72.6±0.9ms     0.14  series_methods.IsInLongSeries.time_isin('float64', 1000)
```

It looks as if for more than 10 `values`-elements, panda's approach is faster.

See also this comment (https://github.com/pandas-dev/pandas/issues/22205#issuecomment-410519409).

The question is whether it really makes sense to keep numpy's approach for less than 10 `values`-elements."
744607470,37911,CLN: Add *.zip to .gitignore,lucasrodes,closed,2020-11-17T10:26:39Z,2020-11-17T13:01:31Z,"- [X] xref #37903
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
744367998,37907,REF: simplify dt64 formatter functions,jbrockmendel,closed,2020-11-17T03:03:30Z,2020-11-17T15:21:59Z,
744401375,37908,CLN: defer CategoricalIndex.astype to Categorical.astype,jbrockmendel,closed,2020-11-17T04:32:45Z,2020-11-17T15:23:55Z,
744225530,37901,BUG: obj.loc[listlike] with missing keys and CategoricalIndex,jbrockmendel,closed,2020-11-16T22:02:37Z,2020-11-17T15:25:12Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This behavior was deprecated for all other Index subclasses a few years ago and finally removed in 1.0.  AFAICT not removing it for CategoricalIndex was an oversight. xref https://github.com/pandas-dev/pandas/commit/8ab61851eeae0d16deaa2f6eafcf59393b06aaeb#r44233424"
696187840,36230,TST GH 29699 multilevel column is called after multiple appends,TAJD,closed,2020-09-08T21:25:03Z,2020-11-17T17:07:12Z,"

- [ ] closes #29699
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Not sure where the best place to put the test is. 

Also, this code generates a warning which doesn't seem to disappear when following the guidance of the warning i.e: set `sort=False`. As the default behaviour is meant to return a warning this is tested for.
"
743085221,37854,CLN: Another unused script,mroeschke,closed,2020-11-14T21:46:32Z,2020-11-17T17:09:20Z,"xref https://github.com/pandas-dev/pandas/pull/37825

Another script that looks unused and undocumented
"
724664988,37249,DOC: Replace pandas on Ray in ecosystem.rst with Modin,devin-petersohn,closed,2020-10-19T14:16:46Z,2020-11-17T17:17:49Z,"* Resolves #37247

Signed-off-by: Devin Petersohn <devin.petersohn@gmail.com>

- [x] closes #37247
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
733895056,37555,Fixed the documentation for pandas.DataFrame.set_index inplace parameter,PurushothamanSrikanth,closed,2020-11-01T06:33:52Z,2020-11-17T17:27:47Z,"Fixed the documentation for `pandas.DataFrame.set_index` 's `inplace` parameter.
Added proper explanation for True case.
"
527380102,29799,API: Infer extension types in array,TomAugspurger,closed,2019-11-22T19:40:57Z,2020-11-17T18:07:53Z,"* string
* integer

Closes #29791 (though we'll want to add BooleanArray)."
745025580,37913,CI: fix mypy error,simonjayhawkins,closed,2020-11-17T19:28:25Z,2020-11-17T19:53:21Z,cc @jbrockmendel 
744065223,37898,TYP: __getitem__ method of EA,simonjayhawkins,closed,2020-11-16T18:31:16Z,2020-11-17T19:56:05Z,"as a precursor to https://github.com/pandas-dev/pandas/pull/35259#discussion_r522951940

there are more changes/additions needed but this PR might get too big to review and push through in a timely manor.

overload is required to avoid casts
Any is used for any type, whereas should use object.
only the abstract classes have been typed.
could create a scalar type alias in each of the EA classes similar to DTScalarOrNaT in DatetimeLikeArrayMixin
import and expose pandas._libs.missing.NAType in pandas._typing 

any of these could be done here or as follow-on"
605062622,33730,DOC: add hvplot as option to plotting.backend,raybellwaves,closed,2020-04-22T20:45:22Z,2020-11-17T22:37:28Z,"#### Location of the documentation

Couple of sections here
https://pandas.pydata.org/pandas-docs/stable/ecosystem.html#visualization
https://dev.pandas.io/docs/development/extending.html#plotting-backends

#### Documentation problem

Add information that [`hvplot`](https://hvplot.holoviz.org/) can be used as a backend
`pd.set_option(""plotting.backend"", ""hvplot"")`

#### Suggested fix for documentation

Not sure will this could go. Could add a hvplot section to https://github.com/pandas-dev/pandas/blob/master/doc/source/ecosystem.rst#visualization but perhaps want to make an explicit list of all the plotting backend options at https://dev.pandas.io/docs/development/extending.html#plotting-backends
"
740074746,37744,DOC: add hvplot,raybellwaves,closed,2020-11-10T16:50:25Z,2020-11-17T22:37:44Z,"- [x] closes #33730 
- [NA] tests added / passed
- [NA] passes `black pandas`
- [NA] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [NA?] whatsnew entry
"
222210812,16037,ENH: support .groupby().ewm() directly,jreback,closed,2017-04-17T19:39:48Z,2020-11-18T00:36:48Z,"would be nice to support ``.groupby().ewm()`` directly.
```
In [2]: df = pd.DataFrame({'tid': np.random.randint(0, 10, size=100), 'value': np.random.randn(100)})

In [3]: df.groupby('tid').rolling(3).value.mean()
Out[3]: 
tid    
0    3          NaN
     6          NaN
     10   -0.596818
     16   -0.619943
     23   -0.688623
     24   -0.167816
             ...   
9    74    0.575155
     80    1.046240
     81    0.279683
     85    0.296154
     86   -0.267774
     92    0.320419
Name: value, dtype: float64

In [4]: df.groupby('tid').apply(lambda x: x.ewm(5).value.mean())
Out[4]: 
tid    
0    3    -1.182125
     6    -0.118510
     10   -0.616044
     16   -0.820601
     23   -0.435397
     24   -0.279619
             ...   
9    74    0.505610
     80    0.657754
     81    0.451367
     85    0.372517
     86    0.226676
     92    0.429832
Name: value, dtype: float64

```"
730918989,37454,ValueError raised when plottting reversed fixed-frequency TimedeltaIndex ,theavey,closed,2020-10-27T22:43:56Z,2020-11-18T00:55:36Z,"When plotting with a `TimedeltaIndex` with a fixed frequency and `xmin > xmax`, a ValueError gets raised.
```python
>>> pd.Series([1,2], index=pd.timedelta_range(start=0, periods=2, freq='D')).plot(xlim=(3, 1))
```
> C:\ProgramData\Miniconda3\envs\env_name\lib\site-packages\pandas\plotting\_matplotlib\converter.py in __call__(self, x, pos)
>   1066     def __call__(self, x, pos=0) -> str:
>   1067         (vmin, vmax) = tuple(self.axis.get_view_interval())
>-> 1068         n_decimals = int(np.ceil(np.log10(100 * 1e9 / (vmax - vmin))))
>   1069         if n_decimals > 9:
>   1070             n_decimals = 9
>
> ValueError: cannot convert float NaN to integer

Seems that there should just be an `abs` in [this line](https://github.com/pandas-dev/pandas/blob/master/pandas/plotting/_matplotlib/converter.py#L1075).

I would expect this to be able to output a plot successfully.

Works fine with a `TimedeltaIndex` with `freq=None`:
```python
pd.Series([1,2], index=[pd.Timedelta(days=1), pd.Timedelta(days=2)]).plot(xlim=(3, 1))
```


INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.6.0.post20201009
Cython           : None
pytest           : 6.1.1
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.20
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None"
731428675,37469,BUG: Fix for #37454: allow reversed axis when plotting with TimedeltaIndex,theavey,closed,2020-10-28T13:11:42Z,2020-11-18T00:55:41Z,"- [x] closes #37454
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
675495799,35622,BUG: error: may be used uninitialized. In alpine,NikSays,closed,2020-08-08T09:35:02Z,2020-11-18T01:16:38Z,"- [ x ] I have checked that this issue has not already been reported. (Not as a separate issue)

- [ x ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Problem description
In an alpine contarner, running `pip3 install pandas=1.1.0`, 
results in error: 
```
'__pyx_v_year' may be used uninitialized in this function`
     7787 |             __pyx_v_year = (__pyx_v_year - 1);
          |             ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~
    cc1: all warnings being treated as errors
    error: command 'gcc' failed with exit status 1
```

#### Expected Output

Successful install"
745210883,37925,CI: mypy error in pandas/core/indexes/datetimelike.py::_simple_new,arw2019,closed,2020-11-18T01:19:32Z,2020-11-18T01:22:04Z,"Seeing this in a bunch of unrelated PRs (my own and others'):
```
pandas/core/indexes/datetimelike.py:776: error: Argument 1 to ""_simple_new"" of ""DatetimeIndexOpsMixin"" has incompatible type ""Union[ExtensionArray, Any]""; expected ""Union[DatetimeArray, TimedeltaArray, PeriodArray]""  [arg-type]
Found 1 error in 1 file (checked 1119 source files)
Performing static analysis using mypy DONE
```
I wonder if it's a bug on master?"
743357225,37878,ENH: Support groupby.ewm operations,mroeschke,closed,2020-11-15T22:12:32Z,2020-11-18T01:38:45Z,"- [x] closes #16037
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Additionally enables a Numba engine for `groupby(...).ewm(...).mean(...)`


```
In [1]: df = pd.DataFrame({""A"": range(10_000), ""B"": range(10_000)})

In [2]: gb_ewm = df.groupby(""A"").ewm(com=1.0)

--cache time first
In [3]: %timeit -r 1 -n 1 gb_ewm.mean(engine='numba')
1.02 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)

In [4]: %timeit gb_ewm.mean(engine='numba')
578 ms ± 28.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [5]: %timeit df.groupby('A').apply(lambda x: x.ewm(com=1.0).mean())
3.43 s ± 208 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [6]: %timeit gb_ewm.mean(engine='cython')
4.19 s ± 204 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```"
745139651,37922,"CLN: NDFrame._where, comment typos",jbrockmendel,closed,2020-11-17T22:38:13Z,2020-11-18T02:52:16Z,
735514493,37608,CLN: remove rebox_native,jbrockmendel,closed,2020-11-03T17:38:29Z,2020-11-03T23:14:41Z,
706754469,36562,BUG: TypeError: '<' not supported between instances of 'float' and 'str' when using `.combine_first` with categorical index containing nan,ssche,closed,2020-09-22T23:29:51Z,2020-11-04T01:55:12Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
In [79]: x = ['b', 'b', 'c', 'a', 'b', np.nan]
    ...: y = ['a', 'b', 'c', 'a', 'b', 'd']
    ...: mi1 = pd.MultiIndex.from_arrays(
    ...:     [x, [1, 2, 3, 4, 5, 6]],
    ...:     names=['a', 'b']
    ...: )
    ...: df = pd.DataFrame({'c': [1, 1, 1, 1, 1, 1]}, index=mi1)
    ...: mi2 = pd.MultiIndex.from_arrays(
    ...:     [y, [1, 1, 1, 1, 1, 1]],
    ...:     names=['a', 'b']
    ...: )
    ...: s = pd.Series([1, 2, 3, 4, 5, 6], index=mi2)
    ...: df.combine_first(pd.DataFrame({'some_col': s}))
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/algorithms.py in safe_sort(values, codes, na_sentinel, assume_unique, verify)
   2060         try:
-> 2061             sorter = values.argsort()
   2062             ordered = values.take(sorter)

TypeError: '<' not supported between instances of 'float' and 'str'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-79-de018ddfae29> in <module>
     11 )
     12 s = pd.Series([1, 2, 3, 4, 5, 6], index=mi2)
---> 13 df.combine_first(pd.DataFrame({'some_col': s}))

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/frame.py in combine_first(self, other)
   6239             return expressions.where(mask, y_values, x_values)
   6240 
-> 6241         return self.combine(other, combiner, overwrite=False)
   6242 
   6243     def update(

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/frame.py in combine(self, other, func, fill_value, overwrite)
   6104         other_idxlen = len(other.index)  # save for compare
   6105 
-> 6106         this, other = self.align(other, copy=False)
   6107         new_index = this.index
   6108 

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/frame.py in align(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis, broadcast_axis)
   3955         broadcast_axis=None,
   3956     ) -> ""DataFrame"":
-> 3957         return super().align(
   3958             other,
   3959             join=join,

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/generic.py in align(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis, broadcast_axis)
   8542             axis = self._get_axis_number(axis)
   8543         if isinstance(other, ABCDataFrame):
-> 8544             return self._align_frame(
   8545                 other,
   8546                 join=join,

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/generic.py in _align_frame(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis)
   8589         if axis is None or axis == 0:
   8590             if not self.index.equals(other.index):
-> 8591                 join_index, ilidx, iridx = self.index.join(
   8592                     other.index, how=join, level=level, return_indexers=True
   8593                 )

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/indexes/base.py in join(self, other, how, level, return_indexers, sort)
   3491                 )
   3492             else:
-> 3493                 return self._join_non_unique(
   3494                     other, how=how, return_indexers=return_indexers
   3495                 )

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/indexes/base.py in _join_non_unique(self, other, how, return_indexers)
   3618         rvalues = other._get_engine_target()
   3619 
-> 3620         left_idx, right_idx = _get_join_indexers(
   3621             [lvalues], [rvalues], how=how, sort=True
   3622         )

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/reshape/merge.py in _get_join_indexers(left_keys, right_keys, sort, how, **kwargs)
   1326         for n in range(len(left_keys))
   1327     )
-> 1328     zipped = zip(*mapped)
   1329     llab, rlab, shape = [list(x) for x in zipped]
   1330 

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/reshape/merge.py in <genexpr>(.0)
   1323     # get left & right join labels and num. of levels at each location
   1324     mapped = (
-> 1325         _factorize_keys(left_keys[n], right_keys[n], sort=sort, how=how)
   1326         for n in range(len(left_keys))
   1327     )

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/reshape/merge.py in _factorize_keys(lk, rk, sort, how)
   1978     if sort:
   1979         uniques = rizer.uniques.to_array()
-> 1980         llab, rlab = _sort_labels(uniques, llab, rlab)
   1981 
   1982     # NA group

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/reshape/merge.py in _sort_labels(uniques, left, right)
   2003     labels = np.concatenate([left, right])
   2004 
-> 2005     _, new_labels = algos.safe_sort(uniques, labels, na_sentinel=-1)
   2006     new_labels = ensure_int64(new_labels)
   2007     new_left, new_right = new_labels[:llength], new_labels[llength:]

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/algorithms.py in safe_sort(values, codes, na_sentinel, assume_unique, verify)
   2063         except TypeError:
   2064             # try this anyway
-> 2065             ordered = sort_mixed(values)
   2066 
   2067     # codes:

~/envs/pandas-test/lib/python3.8/site-packages/pandas/core/algorithms.py in sort_mixed(values)
   2046         # order ints before strings, safe in py3
   2047         str_pos = np.array([isinstance(x, str) for x in values], dtype=bool)
-> 2048         nums = np.sort(values[~str_pos])
   2049         strs = np.sort(values[str_pos])
   2050         return np.concatenate([nums, np.asarray(strs, dtype=object)])

<__array_function__ internals> in sort(*args, **kwargs)

~/envs/pandas-test/lib/python3.8/site-packages/numpy/core/fromnumeric.py in sort(a, axis, kind, order)
    989     else:
    990         a = asanyarray(a).copy(order=""K"")
--> 991     a.sort(axis=axis, kind=kind, order=order)
    992     return a
    993 

TypeError: '<' not supported between instances of 'float' and 'str'
```

#### Problem description

I use `df.combine_first(...)` to add a column to a dataframe while extending the index in case an index value does not exist in the target dataframe. However, if the MultIindex of the dataframe/series contains mixed np.nan/str values in their index value, the above `TypeError` is raised. I originally noticed this for categorical types (`x` and `y`), but the problem can be simplified to plain string types.

The reproduction of this error also seems to depend on the length/order of `x` and `y` (I tried to reduce it to fewer elements and kept the `np.nan`, but that didn't reproduce the error).

#### Expected Output

No exception and the dataframe containing the new column of series.

#### Output of ``pd.show_versions()``

<details>

In [84]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_AU.UTF-8
LOCALE           : en_AU.UTF-8

pandas           : 1.1.2
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 46.4.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
735625825,37612,DOC: Fix punctuation,MicaelJarniac,closed,2020-11-03T20:49:38Z,2020-11-04T01:58:17Z,"Fixed punctuation to match others.
"
679172099,35719,REGR: pd.to_hdf(dropna=True) not dropping all nan rows,XiaozhanYang,closed,2020-08-14T13:52:25Z,2020-11-04T01:59:22Z,"#### Location of the documentation

[this should provide the location of the documentation, e.g. ""pandas.read_csv"" or the URL of the documentation, e.g. ""https://dev.pandas.io/docs/reference/api/pandas.read_csv.html""]

**Note**: You can check the latest versions of the docs on `master` [here](https://pandas.pydata.org/docs/dev/).

https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html

#### Documentation problem

[this should provide a description of what documentation you believe needs to be fixed/improved]

On the section:

""HDFStore will by default not drop rows that are all missing. This behavior can be changed by setting dropna=True.""

The last example:

In [370]: pd.read_hdf('file.h5', 'df_with_missing')
Out[370]: 
   col1  col2
0   0.0   1.0
1   NaN   NaN
2   2.0   NaN

should output:

   col1  col2
0   0.0   1.0
1   2.0   NaN

which does not have rows that are all missing.


#### Suggested fix for documentation

[this should explain the suggested fix and **why** it's better than the existing documentation]

This has been provided above.
"
723819617,37203,CLN: clean color selection in _matplotlib/style,ivanovmg,closed,2020-10-17T18:18:43Z,2020-11-04T02:18:10Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

There was a comment in the top of the module that the code was a bit too dynamic.
The logic in the function was too complex.

I refactored the code
 - Extract functions
 - Simplify logic
 - Add type annotations

Note: there is one issue with mypy, which I do not know how to handle. Currently I suggested to ignore the error."
675827015,35646,"BUG: groupby(..., dropna=False).indices with single group key does not include nan group",mroeschke,closed,2020-08-10T02:31:36Z,2020-11-04T02:59:03Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
# Your code here
In [9]: data = {'group':['g1', 'g1', 'g1', np.nan, 'g1', 'g1', 'g2', 'g2', 'g2', 'g2', np.nan],
   ...:                     'A':[3, 1, 8, 2, 6, -1, 0, 13, -4, 0, 1],
   ...:                     'B':[5, 2, 3, 7, 11, -1, 4,-1, 1, 0, 2]}
   ...: df = pd.DataFrame(data)
   ...: df.groupby('group',dropna=True).indices
Out[9]: {'g1': array([0, 1, 2, 4, 5]), 'g2': array([6, 7, 8, 9])}

In [10]: df.groupby('group',dropna=False).indices
Out[10]: {'g1': array([0, 1, 2, 4, 5]), 'g2': array([6, 7, 8, 9])}

In [11]: pd.__version__
Out[11]: '1.2.0.dev0+67.gaefae55e1'
```

#### Problem description

The grouping codes + indices are determined for a single group by key here

https://github.com/pandas-dev/pandas/blob/aefae55e1960a718561ae0369e83605e3038f292/pandas/core/groupby/grouper.py#L559

And `Categorical` does not support `nan` as a label (only a missing -1 code)

This works correctly if multiple group keys are passed

Once this issue is addressed, https://github.com/pandas-dev/pandas/issues/35542 will be fixed
#### Expected Output

```
In [10]: df.groupby('group',dropna=False).indices
Out[10]: {'g1': array([0, 1, 2, 4, 5]), 'g2': array([6, 7, 8, 9]), np.nan: array([3, 10]}
```
"
672765849,35542,BUG: Groupby dropna=False argument unexpected behaviour with rolling window,nrcjea001,closed,2020-08-04T12:47:16Z,2020-11-04T02:59:03Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
data = {'group':['g1', 'g1', 'g1', np.nan, 'g1', 'g1', 'g2', 'g2', 'g2', 'g2', np.nan], 
                    'A':[3, 1, 8, 2, 6, -1, 0, 13, -4, 0, 1], 
                    'B':[5, 2, 3, 7, 11, -1, 4,-1, 1, 0, 2]} 
df = pd.DataFrame(data)
df.groupby('group',dropna=False)['A'].rolling(1,min_periods=1).mean()

#### Output
group   
g1     0     3.0
       1     1.0
       2     8.0
       4     6.0
       5    -1.0
g2     6     0.0
       7    13.0
       8    -4.0
       9     0.0
Name: A, dtype: float64
```

#### Problem description

In Pandas 1.1.0, dropna=False is introduced as argument in groupby to allow for NA in group keys. However, we do not see this behaviour with rolling groupby. NA is not added to group key.

#### Expected Output

Nan group key

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 11, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.1.0
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 49.2.1.post20200802
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.48.0

</details>
"
735672320,37616,TST/REF: share tests across Series/DataFrame,jbrockmendel,closed,2020-11-03T22:19:50Z,2020-11-04T03:41:58Z,
734993909,37597,REF: re-use _validate_setitem_value in Categorical.fillna,jbrockmendel,closed,2020-11-03T03:47:54Z,2020-11-04T03:42:36Z,
735697596,37617,TST: collect tests by method,jbrockmendel,closed,2020-11-03T23:16:38Z,2020-11-04T03:53:42Z,
735717286,37618,TST/REF: tests.generic,jbrockmendel,closed,2020-11-04T00:09:51Z,2020-11-04T04:01:56Z,
735724792,37619,TST/REF: parametrize set_axis tests,jbrockmendel,closed,2020-11-04T00:30:58Z,2020-11-04T04:02:09Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
717303778,36980,BUG: Pandas closes user-provided file handles that it doesn't own.,tpllaha,closed,2020-10-08T12:33:43Z,2020-11-04T04:09:17Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
`read_csv` closes user-provided file handles in specific cases. In particular, if we pass a `BytesIO` or a file opened in binary mode, **and** pass an `encoding` kwarg. If the `encoding` kwarg is not passed, pandas does not close the file handle (as expected).
Much earlier (in 2016) a related [issue](https://github.com/pandas-dev/pandas/issues/14418) has been reported and fixed. The test cases that were mentioned there still pass today, but only in case the file handle is opened in binary mode & the `encoding` kwarg is passed. (version `1.1.2` & python `3.8.5`)


#### Code Sample, a copy-pastable example

```python
import sys
import pandas
from io import BytesIO, StringIO

print(f'Python version: {sys.version}')
print(f'pandas version: {pandas.__version__}')

string_io = StringIO('a,b\n1,2')
bytes_io_1 = BytesIO(b'a,b\n1,2')
bytes_io_2 = BytesIO(b'a,b\n1,2')

pandas.read_csv(string_io)
print(f'Was StringIO closed? - {string_io.closed}')

pandas.read_csv(bytes_io_1)
print(f'Was BytesIO closed when encoding is NOT passed? - {bytes_io_1.closed}')

pandas.read_csv(bytes_io_2, encoding='utf-8')
print(f'Was BytesIO closed when encoding is passed? - {bytes_io_2.closed}')
```

prints:

```
Python version: 3.8.5 (v3.8.5:580fbb018f, Jul 20 2020, 12:11:27) 
[Clang 6.0 (clang-600.0.57)]
pandas version: 1.1.2
Was StringIO closed? - False
Was BytesIO closed when encoding is NOT passed? - False
Was BytesIO closed when encoding is passed? - True
                                              ^
```

#### Problem description

Expectation is that pandas should never close user-provided handles. If pandas didn't open the file, then it doesn't own it & therefore shouldn't close it.


#### Expected Output

I would expect the script above to output:
```
Python version: 3.8.5 (v3.8.5:580fbb018f, Jul 20 2020, 12:11:27) 
[Clang 6.0 (clang-600.0.57)]
pandas version: 1.1.2
Was StringIO closed? - False
Was BytesIO closed when encoding is NOT passed? - False
Was BytesIO closed when encoding is passed? - False
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.17.2
pytz             : 2020.1
dateutil         : 2.7.3
pip              : 20.2.3
setuptools       : 40.7.1
Cython           : 0.29.7
pytest           : 4.6.5
hypothesis       : 4.55.2
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.2
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : 0.3.5
scipy            : 1.5.2
sqlalchemy       : 1.3.10
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
717770495,36997,REF/BUG/TYP: read_csv shouldn't close user-provided file handles,twoertwein,closed,2020-10-09T01:04:08Z,2020-11-04T04:13:30Z,"- [x] closes #36980
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

REF/BUG: de-duplicate all file handling in `TextReader` by calling `get_handle` in `CParserWrapper`. ~~When `TextReader` gets a string it uses memory mapping (it is given a file object in all other cases).~~

REF/TYP: The second commit adds a new return value to `get_handle` (whether the buffer is wrapped inside a TextIOWrapper: in that case we cannot close it, we need to detach it (and flush it if we wrote to it)). I made `get_handle` return a typed dataclass `HandleArgs` and made sure that all created handles are in `HandleArgs.created_handles` there is no need to close `HandleArgs.handle` (unless it is created by `get_filename_or_buffer`).

I used asserts for mypy when I'm 100% certain about the type, otherwise I added mypy ignore statements.

In the future it might be good to merge `get_handle` and `get_filename_or_buffer`."
734041069,37564,"REGR: pd.to_hdf(..., dropna=True) not dropping missing rows",arw2019,closed,2020-11-01T19:19:55Z,2020-11-04T08:07:23Z,"- [x] closes #35719
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
651670957,35142,improved exception message,nickmccullum,closed,2020-07-06T16:32:20Z,2020-11-04T08:29:21Z,"for ValueError: cannot reindex from a duplicate axis

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
642158229,34874,Doc: Dates being plotted wrong_x-compact parameter #20688,gauravchandok,closed,2020-06-19T18:42:08Z,2020-11-04T08:40:55Z,"- [x] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
631190223,34584,DOC: DataFrame.reindex improve summary and return type,gauravchandok,closed,2020-06-04T22:38:23Z,2020-11-04T08:44:21Z,"- [x] closes #33888 
- [x] tests added / passed (passes `scripts/validate_docstrings.py pandas.DataFrame.reindex`)
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry (Not Required)"
630486347,34565,BUG: Fix na_rep when using pd.NA in _format_strings (#33950),k-fillmore,closed,2020-06-04T03:39:27Z,2020-11-04T08:46:52Z,"- [x] closes #33950
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
735279219,37601,DEPR: DataFrame/Series.slice_shift,erfannariman,closed,2020-11-03T12:28:07Z,2020-11-04T09:23:51Z,"- [x] ref #18262
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
730330788,37442,BUG: read csv with enclosed double quotes,zer0link,closed,2020-10-27T10:42:48Z,2020-11-04T09:50:28Z,"- [ ✔] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
def read_as_df(self, file_path):
        return pd.read_csv(
            file_path,
            sep='|',
            quoting=csv.QUOTE_ALL,
            encoding='utf-16'
        )
```

#### Problem description
I have a csv with some data with separator ""|"" and all columns are enclosed with double quotes:
```
""Code""|""Owner""|""OwnerName""|""Age""|""Description""|""AdditionalRemarks""
""122""|""100""|""Random owner 1""|""52""|""’Some random description""""|""Some addtitional remarks""
""123""|""101""|""Owner 2""|""32""|""Data1 | Data2 | Data3""|""Addtitional remarks ++""
```

When I read the csv using the code above: I got this result:
#### Shown Output:
![image](https://user-images.githubusercontent.com/7539963/97289933-29077a80-1848-11eb-9de2-73dc221c0787.png)

#### Expected Output
For data row 1: the addtional remarks column is read as part of the description column
For data row 2: the data looks fine

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]
INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.17763
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : None.None

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 20.2.3
setuptools       : 40.8.0
Cython           : None
pytest           : 5.3.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.0.1
pymysql          : 0.10.1
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 6.3.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.4
fastparquet      : 0.3.2
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : 0.3.4
scipy            : None
sqlalchemy       : 1.3.20
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.45.1
</details>
"
733776076,37539,CI Move unwanted typing checks to pre-commit,MarcoGorelli,closed,2020-10-31T18:03:26Z,2020-11-04T11:03:01Z,"There's still some grep-based checks left, but running doctests / running mypy can stay in code_checks"
727594339,37343,BUG: groupby __iter__ on pandas 1.1.x not propagating _metadata on DataFrame subclasses,ryandvmartin,closed,2020-10-22T17:43:41Z,2020-11-04T13:22:55Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python

In [1]: import pandas as pd
   ...:
   ...: class SubclassedDataFrame2(pd.DataFrame):
   ...:
   ...:     # temporary properties
   ...:     _internal_names = pd.DataFrame._internal_names + ['internal_cache']
   ...:     _internal_names_set = set(_internal_names)
   ...:
   ...:     # normal properties
   ...:     _metadata = ['added_property']
   ...:
   ...:     @property
   ...:     def _constructor(self):
   ...:         return SubclassedDataFrame2
   ...:

In [2]: df = SubclassedDataFrame2({'A': [1, 2, 3], 'B': [1, 1, 2], 'C': [7, 8, 9]})
   ...: df.added_property = ""hello""
   ...: for i, d in df.groupby(""B""):
   ...:     assert d.added_property == ""hello""
   ...:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-2-bf64fc505324> in <module>
      2 df.added_property = ""hello""
      3 for i, d in df.groupby(""B""):
----> 4     assert d.added_property == ""hello""
      5

~\RMS-Python\lib\site-packages\pandas\core\generic.py in __getattr__(self, name)
   5133             or name in self._accessors
   5134         ):
-> 5135             return object.__getattribute__(self, name)
   5136         else:
   5137             if self._info_axis._can_hold_identifiers_and_holds_name(name):

AttributeError: 'SubclassedDataFrame2' object has no attribute 'added_property'


```

#### Problem description

Under pandas 1.0.x the above is working, on 1.1.x, the _metadata is not propagated to the split frames. I think this is a known issue with groupby and `__finalize__`, but this specific behavior with `__iter__` is a regression from 1.0.x behavior on DataFrame subclasses.

#### Expected Output

Assert statements above are passing, with metadata propagated to the split dataframe

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : 0.29.17
pytest           : 5.4.1
hypothesis       : 5.36.1
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : 1.3.2
fsspec           : 0.8.0
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.16
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.49.0

</details>
"
640954927,34857,BUG: read-only buffer failures in datetime parsing,jorisvandenbossche,closed,2020-06-18T06:49:43Z,2020-11-04T13:43:46Z,"PR https://github.com/pandas-dev/pandas/pull/22147 caused several ""read-only buffer"" errors (latest: https://github.com/pandas-dev/pandas/issues/34843). I quickly skimmed that PR, and most changes there have been fixed in the mean-time (either converted back to use ndarray or added `const`), a few are remaining, like:

`array_strptime` (and also `array_to_timedelta64` -> `to_timedelta`)
```
In [4]: arr = np.array(['15/10/2020'], dtype=object) 

In [5]: arr.flags.writeable = False   

In [6]: pd.to_datetime(arr, format=""%d/%m/%Y"")    
...
ValueError: buffer source array is read-only
```

and then a few in parsing.pyx, but I suppose those are generally not a problem, if the arrays are coming from out parsing code, they will never be read-only.

_Originally posted by @jorisvandenbossche in https://github.com/pandas-dev/pandas/issues/34843#issuecomment-645259205_"
726558492,37312,BUG: comparison/indexing causes ValueError: buffer source array is read-only,dmitra79,closed,2020-10-21T14:41:31Z,2020-11-04T13:43:47Z,"- [X ] I have checked that this issue has not already been reported.

- [ X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
di=np.array(['2016-12-27 02:02:09', '2016-12-27 02:02:09.017000',
               '2016-12-27 02:02:09.033000', '2016-12-27 02:02:09.050000',
               '2016-12-27 02:02:09.066000'],dtype='datetime64[ns]')
di.setflags(write=False)
v1=[95.746, 95.735, 95.722, 95.709,    10]
v2=[-86.474, -86.491, -86.505, -86.513,   11]
temp=pd.DataFrame({'v1':v1,'v2':v2}, index=di)
temp=temp.astype('float64')
temp2=temp.v1-temp.v2
temp2[temp2>180]=temp2[temp2>180]-360
```

This code throws an error:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/series.py in __setitem__(self, key, value)
    999         try:
-> 1000             self._set_with_engine(key, value)
   1001         except (KeyError, ValueError):

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/series.py in _set_with_engine(self, key, value)
   1032         # fails with AttributeError for IntervalIndex
-> 1033         loc = self.index._engine.get_loc(key)
   1034         validate_numeric_casting(self.dtype, value)

pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

TypeError: '2016-12-27 02:02:09.000     True
2016-12-27 02:02:09.017     True
2016-12-27 02:02:09.033     True
2016-12-27 02:02:09.050     True
2016-12-27 02:02:09.066    False
dtype: bool' is an invalid key

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-3-05bd4183267d> in <module>
      8 temp=temp.astype('float64')
      9 temp2=temp.v1-temp.v2
---> 10 temp2[temp2>180]=temp2[temp2>180]-360

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/series.py in __setitem__(self, key, value)
   1018                 key = np.asarray(key, dtype=bool)
   1019                 try:
-> 1020                     self._where(~key, value, inplace=True)
   1021                 except InvalidIndexError:
   1022                     self.iloc[key] = value

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/generic.py in _where(self, cond, other, inplace, axis, level, errors, try_cast)
   8778             if other.ndim <= self.ndim:
   8779 
-> 8780                 _, other = self.align(
   8781                     other, join=""left"", axis=axis, level=level, fill_value=np.nan
   8782                 )

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/series.py in align(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis, broadcast_axis)
   4272         broadcast_axis=None,
   4273     ):
-> 4274         return super().align(
   4275             other,
   4276             join=join,

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/generic.py in align(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis, broadcast_axis)
   8557             )
   8558         elif isinstance(other, ABCSeries):
-> 8559             return self._align_series(
   8560                 other,
   8561                 join=join,

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/generic.py in _align_series(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis)
   8660                 join_index, lidx, ridx = None, None, None
   8661             else:
-> 8662                 join_index, lidx, ridx = self.index.join(
   8663                     other.index, how=join, level=level, return_indexers=True
   8664                 )

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/indexes/datetimelike.py in join(self, other, how, level, return_indexers, sort)
    884 
    885         this, other = self._maybe_utc_convert(other)
--> 886         return Index.join(
    887             this,
    888             other,

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/indexes/base.py in join(self, other, how, level, return_indexers, sort)
   3500         elif self.is_monotonic and other.is_monotonic:
   3501             try:
-> 3502                 return self._join_monotonic(
   3503                     other, how=how, return_indexers=return_indexers
   3504                 )

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/indexes/base.py in _join_monotonic(self, other, how, return_indexers)
   3800                 join_index = self
   3801                 lidx = None
-> 3802                 ridx = self._left_indexer_unique(sv, ov)
   3803             elif how == ""right"":
   3804                 join_index = other

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/indexes/datetimelike.py in wrapper(left, right)
     62             right = right.view(""i8"")
     63 
---> 64         results = joinf(left, right)
     65         if with_indexers:
     66             # dtype should be timedelta64[ns] for TimedeltaIndex

pandas/_libs/join.pyx in pandas._libs.join.left_join_indexer_unique()

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/_libs/join.cpython-38-x86_64-linux-gnu.so in View.MemoryView.memoryview_cwrapper()

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/_libs/join.cpython-38-x86_64-linux-gnu.so in View.MemoryView.memoryview.__cinit__()

ValueError: buffer source array is read-only

```


#### Problem description

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
commit : db08276
python : 3.8.6.final.0
python-bits : 64
OS : Linux
OS-release : 4.15.0-118-generic
Version : #119-Ubuntu SMP Tue Sep 8 12:30:01 UTC 2020
machine : x86_64
processor : x86_64
byteorder : little
LC_ALL : None
LANG : en_US.UTF-8
LOCALE : en_US.UTF-8

pandas : 1.1.3
numpy : 1.19.2
pytz : 2020.1
dateutil : 2.8.1
pip : 20.2.3
setuptools : 49.6.0.post20201009
Cython : 0.29.21
lxml.etree : 4.5.2
jinja2 : 2.11.2
IPython : 7.18.1
fsspec : 0.8.4
fastparquet : 0.4.1
matplotlib : 3.3.2
pyarrow : 0.17.1
scipy : 1.5.2
xarray : 0.16.1
numba : 0.51.2

</details>
"
724949034,37264,BUG: another ValueError: buffer source array is read-only,dmitra79,closed,2020-10-19T20:24:12Z,2020-11-04T13:43:47Z,"- [ ] I have checked that this issue has not already been reported.

- [X ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

This seems related to: https://github.com/pandas-dev/pandas/issues/37174 
however it shows up under different conditions - appears to be triggered by comparison involving NaN.

#### Code Sample, a copy-pastable example

```python
print(temp)
temp[temp>180]=temp[temp>180]-360
```

Output:
```
2016-12-27 02:02:09.000    182.220
2016-12-27 02:02:09.017    182.226
2016-12-27 02:02:09.033    182.227
2016-12-27 02:02:09.050    182.222
2016-12-27 02:02:09.066        NaN
dtype: float64
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/series.py in __setitem__(self, key, value)
    999         try:
-> 1000             self._set_with_engine(key, value)
   1001         except (KeyError, ValueError):

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/series.py in _set_with_engine(self, key, value)
   1032         # fails with AttributeError for IntervalIndex
-> 1033         loc = self.index._engine.get_loc(key)
   1034         validate_numeric_casting(self.dtype, value)

pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.DatetimeEngine.get_loc()

TypeError: '2016-12-27 02:02:09.000     True
2016-12-27 02:02:09.017     True
2016-12-27 02:02:09.033     True
2016-12-27 02:02:09.050     True
2016-12-27 02:02:09.066    False
dtype: bool' is an invalid key

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-33-85e39ae5f3f1> in <module>
      3 temp=temp.head(5)
      4 print(temp.head())
----> 5 temp[temp>180]=temp[temp>180]-360

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/series.py in __setitem__(self, key, value)
   1018                 key = np.asarray(key, dtype=bool)
   1019                 try:
-> 1020                     self._where(~key, value, inplace=True)
   1021                 except InvalidIndexError:
   1022                     self.iloc[key] = value

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/generic.py in _where(self, cond, other, inplace, axis, level, errors, try_cast)
   8778             if other.ndim <= self.ndim:
   8779 
-> 8780                 _, other = self.align(
   8781                     other, join=""left"", axis=axis, level=level, fill_value=np.nan
   8782                 )

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/series.py in align(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis, broadcast_axis)
   4272         broadcast_axis=None,
   4273     ):
-> 4274         return super().align(
   4275             other,
   4276             join=join,

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/generic.py in align(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis, broadcast_axis)
   8557             )
   8558         elif isinstance(other, ABCSeries):
-> 8559             return self._align_series(
   8560                 other,
   8561                 join=join,

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/generic.py in _align_series(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis)
   8660                 join_index, lidx, ridx = None, None, None
   8661             else:
-> 8662                 join_index, lidx, ridx = self.index.join(
   8663                     other.index, how=join, level=level, return_indexers=True
   8664                 )

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/indexes/datetimelike.py in join(self, other, how, level, return_indexers, sort)
    884 
    885         this, other = self._maybe_utc_convert(other)
--> 886         return Index.join(
    887             this,
    888             other,

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/indexes/base.py in join(self, other, how, level, return_indexers, sort)
   3500         elif self.is_monotonic and other.is_monotonic:
   3501             try:
-> 3502                 return self._join_monotonic(
   3503                     other, how=how, return_indexers=return_indexers
   3504                 )

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/indexes/base.py in _join_monotonic(self, other, how, return_indexers)
   3800                 join_index = self
   3801                 lidx = None
-> 3802                 ridx = self._left_indexer_unique(sv, ov)
   3803             elif how == ""right"":
   3804                 join_index = other

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/indexes/datetimelike.py in wrapper(left, right)
     62             right = right.view(""i8"")
     63 
---> 64         results = joinf(left, right)
     65         if with_indexers:
     66             # dtype should be timedelta64[ns] for TimedeltaIndex

pandas/_libs/join.pyx in pandas._libs.join.left_join_indexer_unique()

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/_libs/join.cpython-38-x86_64-linux-gnu.so in View.MemoryView.memoryview_cwrapper()

~/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/_libs/join.cpython-38-x86_64-linux-gnu.so in View.MemoryView.memoryview.__cinit__()

ValueError: buffer source array is read-only
```

#### Problem description

See error above

#### Expected Output

I would have expected:
- the 'NA' value to be ignored in comparison without causing an error
- If an error is due to NA, I expect to see appropriate message, instead of  'read-only' being thrown

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
commit : db08276
python : 3.8.6.final.0
python-bits : 64
OS : Linux
OS-release : 4.15.0-118-generic
Version : #119-Ubuntu SMP Tue Sep 8 12:30:01 UTC 2020
machine : x86_64
processor : x86_64
byteorder : little
LC_ALL : None
LANG : en_US.UTF-8
LOCALE : en_US.UTF-8

pandas : 1.1.3
numpy : 1.19.2
pytz : 2020.1
dateutil : 2.8.1
pip : 20.2.3
setuptools : 49.6.0.post20201009
Cython : 0.29.21
lxml.etree : 4.5.2
jinja2 : 2.11.2
IPython : 7.18.1
fsspec : 0.8.4
fastparquet : 0.4.1
matplotlib : 3.3.2
pyarrow : 0.17.1
scipy : 1.5.2
xarray : 0.16.1
numba : 0.51.2

</details>
"
731183629,37461,BUG: Metadata propagation for groupby iterator,Japanuspus,closed,2020-10-28T07:30:05Z,2020-11-04T14:19:10Z,"Addresses part of #28283

This PR ensures that `__finalize__` is called for objects returned by iterator over `.groupby`-results.
Based on the discussion and benchmarks in PR #35688 (see https://github.com/pandas-dev/pandas/pull/35688#issuecomment-675367517), `__finalize__` is _not_ called for intermediate objects used by `.apply` (and maybe other code paths).

The implemented solution is the safe choice with regard to performance: An alternative solution would be to call `__finalize__` immediately after initialization (in `DataSplitter._chop()`).
The downside to doing so would be the performance hit due to the overhead of finalizing intermediate objects in `.apply` (and maybe other code paths).  In the benchmarks referenced above, this amounted to around 4%.
The upside of finalizing immediately after initialization would be reduced complexity and that it would allow `.apply` (and any other code paths using `DataSplitter._chop` to access metadata)..

- [x] closes #37343 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
734432838,37581,CLN refactor core/arrays,MarcoGorelli,closed,2020-11-02T11:36:17Z,2020-11-04T14:23:02Z,"Some refactorings found by Sourcery https://sourcery.ai/

I've removed the ones of the kind
```diff
- if param:
-     var = a
- else:
-     var = b
+ var = a if param else b
```"
736152016,37628,Backport PR #37461 on branch 1.1.x: BUG: Metadata propagation for groupby iterator,simonjayhawkins,closed,2020-11-04T14:18:37Z,2020-11-04T15:54:30Z,Backport PR #37461
721958024,37128,TYP: add Shape alias to pandas._typing,arw2019,closed,2020-10-15T04:15:08Z,2020-11-04T16:53:11Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref https://github.com/pandas-dev/pandas/pull/37024#discussion_r503488032

cc @simonjayhawkins "
668855598,35478,DatetimeIndex get_loc validates date object,knabben,closed,2020-07-30T15:17:43Z,2020-11-04T16:55:11Z,"- [ ] closes #35466
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Adding the date object try cast in get_loc."
736168003,37630,DOC: Fix typo,MicaelJarniac,closed,2020-11-04T14:39:25Z,2020-11-04T16:58:27Z,"I'm not entirely sure about this one, but ""If an ndarray is passed, the values are used as-is determine the groups."" does sound odd to me, so I do believe it was missing the ""to"".
"
736225643,37632,Added dataclass into dataframe API reference,taytzehao,closed,2020-11-04T15:53:54Z,2020-11-04T17:34:04Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
Added dataclass constructor for dataframe into the Pandas in pandas/init.py line 363 and 426."
714196080,36842,BUG: Groupy dropped nan groups from result when grouping over single column,phofl,closed,2020-10-03T23:31:55Z,2020-11-04T21:12:38Z,"- [x] closes #35646
- [x] closes #35542
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

When grouping over 2 columns with only ``nans`` this raised an error previously. Realised this, when running tests for my fix, because this case was tests with one grouping column. Added test to cover fixed behavior for more than one column.

cc @mroeschke  Takes same path as two or more grouping columns now."
453081056,26692,If tuples used as index pd.read_json( orient='split') does not read file saved by df.to_json(orient='split),achatrian,open,2019-06-06T15:00:11Z,2020-11-04T21:51:51Z,"#### Code Sample, a copy-pastable example if possible
```python
a = pd.DataFrame()
a['example'] = [0, 1 ,2, 3 ,4 ,5 ,6]
a.index = [(i, i) for i in range(7)]
a.to_json('test2.json', orient='split')
b = pd.read_json('test2.json', orient='split')
```
#### Problem description

After using tuples as index and saving DataFrame with orient='split', error is thrown during loading via pd.read_json( orient='split').

Full traceback:
--------------------------------------------------------
ValueError             Traceback (most recent call last)
~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in create_block_manager_from_arrays(arrays, names, axes)
   1666         blocks = form_blocks(arrays, names, axes)
-> 1667         mgr = BlockManager(blocks, axes)
   1668         mgr._consolidate_inplace()

~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in __init__(self, blocks, axes, do_integrity_check)
    113         if do_integrity_check:
--> 114             self._verify_integrity()
    115 

~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in _verify_integrity(self)
    310             if block._verify_integrity and block.shape[1:] != mgr_shape[1:]:
--> 311                 construction_error(tot_items, block.shape[1:], self.axes)
    312         if len(self.items) != tot_items:

~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in construction_error(tot_items, block_shape, axes, e)
   1690     raise ValueError(""Shape of passed values is {0}, indices imply {1}"".format(
-> 1691         passed, implied))
   1692 

ValueError: Shape of passed values is (7, 1), indices imply (3, 1)

During handling of the above exception, another exception occurred:

ValueError             Traceback (most recent call last)
<ipython-input-36-f5b064e2d9e4> in <module>
      3 a.index = [(i, i, i) for i in range(7)]
      4 a.to_json('test2.json', orient='split')
----> 5 b = pd.read_json('test2.json', orient='split')

~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py in read_json(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)
    425         return json_reader
    426 
--> 427     result = json_reader.read()
    428     if should_close:
    429         try:

~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py in read(self)
    535             )
    536         else:
--> 537             obj = self._get_object_parser(self.data)
    538         self.close()
    539         return obj

~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py in _get_object_parser(self, json)
    554         obj = None
    555         if typ == 'frame':
--> 556             obj = FrameParser(json, **kwargs).parse()
    557 
    558         if typ == 'series' or obj is None:

~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py in parse(self)
    650 
    651         else:
--> 652             self._parse_no_numpy()
    653 
    654         if self.obj is None:

~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py in _parse_no_numpy(self)
    874                 loads(json, precise_float=self.precise_float))}
    875             self.check_keys_split(decoded)
--> 876             self.obj = DataFrame(dtype=None, **decoded)
    877         elif orient == ""index"":
    878             self.obj = DataFrame(

~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy)
    446 
    447                     mgr = arrays_to_mgr(arrays, columns, index, columns,
--> 448                                         dtype=dtype)
    449                 else:
    450                     mgr = init_ndarray(data, index, columns, dtype=dtype,

~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype)
     59     axes = [ensure_index(columns), index]
     60 
---> 61     return create_block_manager_from_arrays(arrays, arr_names, axes)
     62 
     63 

~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in create_block_manager_from_arrays(arrays, names, axes)
   1669         return mgr
   1670     except ValueError as e:
-> 1671         construction_error(len(arrays), arrays[0].shape, axes, e)
   1672 
   1673 

~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in construction_error(tot_items, block_shape, axes, e)
   1689         raise ValueError(""Empty data passed with indices specified."")
   1690     raise ValueError(""Shape of passed values is {0}, indices imply {1}"".format(
-> 1691         passed, implied))
   1692 
   1693 

ValueError: Shape of passed values is (7, 1), indices imply (2, 1)

2 in last tuple is number of elements of tuples I use as index.

#### Expected Output

b = a -- dataframe with same content as a

        example
(0, 0)    0
(1, 1)    1
(2, 2)    2
(3, 3)    3
(4, 4)    4
(5, 5)    5
(6, 6)    6

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.18.0-20-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.24.2
pytest: 4.5.0
pip: 19.1.1
setuptools: 41.0.1
Cython: 0.29.7
numpy: 1.16.3
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 7.5.0
sphinx: 2.0.1
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.9
feather: None
matplotlib: 3.1.0
openpyxl: 2.6.2
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.8
lxml.etree: 4.3.3
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.3.3
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
733805809,37544,BUG: Agg and Apply with custom function improperly ignore column labels when function returns a dict-like,aevangeline,closed,2020-10-31T20:56:10Z,2020-11-04T23:47:59Z,"Hello folks! I've discovered an odd corner case regression that I hope can be fixed.

The problem is that if you are running an aggregation/apply on a dataframe using a custom function - if that function returns a dict-like object, it doesn't use the appropriate column labels in the original dataframe.

As a simple replication 
```{python} 
df = pd.DataFrame([[1, 2, 3],
                   [4, 5, 6],
                   [7, 8, 9],
                   [np.nan, np.nan, np.nan]],
                  columns=['A', 'B', 'C'])

print(df.agg(lambda x : {""sum"" :np.sum(x)}))
```

prints 
```
0    {'sum': 12.0}
1    {'sum': 15.0}
2    {'sum': 18.0}
```

instead of the expected
```
A    {'sum': 12.0}
B    {'sum': 15.0}
C    {'sum': 18.0}
```

This is a regression/breaking change since in prior versions of pandas (I believe 1.0.X and earlier) the expected results were produced.
"
734087574,37571,BUG: to_dict should return a native datetime object for NumPy backed dataframes,arw2019,closed,2020-11-01T23:27:18Z,2020-11-04T23:50:47Z,"- [x] closes #21256
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Added a fix for datetime. Added tests for datetime and for bool"
735806542,37621,ENH: memory_map for compressed files,twoertwein,closed,2020-11-04T04:43:10Z,2020-11-05T00:47:18Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Apply memory mapping first and then compression. The existing order didn't work.

This adds support for memory mapping in `read_csv` when compression is specified (python and c engine)."
734435938,37583,CLN refactor core/groupby,MarcoGorelli,closed,2020-11-02T11:40:44Z,2020-11-05T07:12:21Z,"Some refactorings found by Sourcery https://sourcery.ai/

I've removed the ones of the kind
```diff
- if param:
-     var = a
- else:
-     var = b
+ var = a if param else b
```"
736409515,37636,DOC: Fix typo,MicaelJarniac,closed,2020-11-04T20:41:02Z,2020-11-05T14:00:30Z,"""columns(s)"" sounded odd, I believe it was supposed to be just ""column(s)"".
"
736911250,37646,Order not preserved in OrderedDict when using to_dict with orient='records',muellermichel,closed,2020-11-05T12:57:54Z,2020-11-05T14:14:16Z,"version: pandas 1.1.2

reproducer:

    DataFrame([{""B"": 1, ""A"": 2}]).to_dict(orient='records', into=OrderedDict)
    > [OrderedDict([('A', 2), ('B', 1)])]

expected behavior:

    DataFrame([{""B"": 1, ""A"": 2}]).to_dict(orient='records', into=OrderedDict)
    > [OrderedDict([('B', 1), ('A', 2)])]
"
737015700,37649,Addressed dataclass comment,taytzehao,closed,2020-11-05T15:10:19Z,2020-11-05T15:26:32Z,Addressed comments for adding dataclass into pandas dataframe reference API
737033205,37651,Merge pull request #1 from pandas-dev/master,taytzehao,closed,2020-11-05T15:30:12Z,2020-11-05T15:30:43Z,"Merge original repo to update forked repository

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
729663683,37421,TST/REF: collect tests by method from generic,jbrockmendel,closed,2020-10-26T15:26:37Z,2020-11-05T18:33:32Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
728520593,37370,DOC: add searchsorted examples #36411,Mikhaylov-yv,closed,2020-10-23T20:59:52Z,2020-11-05T20:55:07Z,DOC: add searchsorted examples #36411
733746235,37538,TST: split up tests/plotting/test_frame.py into subdir & modules #34769,Mikhaylov-yv,closed,2020-10-31T15:38:12Z,2020-11-05T21:18:37Z,"- [ ] closes #34769
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I moved the file to a new directory.
What is the best structure for this folder?"
737063594,37652,BUG: don't suppress OSError in tzlocal,jbrockmendel,closed,2020-11-05T16:06:59Z,2020-11-06T00:11:38Z,Seeing a bunch of suppressed exceptions in the py38 windows tests.  Hopefully this will cause them to surface correctly.
736567839,37638,TST/REF: collect indexing tests by method,jbrockmendel,closed,2020-11-05T02:54:40Z,2020-11-06T00:27:00Z,
736335378,37634,TST/REF: collect tests for get_numeric_data,jbrockmendel,closed,2020-11-04T18:32:46Z,2020-11-06T00:27:33Z,
736576519,37640,REF: de-duplicate _validate_insert_value with _validate_scalar,jbrockmendel,closed,2020-11-05T03:19:43Z,2020-11-06T00:39:03Z,"There is only one place where `_validate_insert_value` isnt identical to `_validate_scalar`, and that is in `DatetimeLikeArrayMixin._validate_scalar`, which ATM does not unbox (which all the other `_validate_scalar` methods do).  This updates `_validate_scalar` to unbox.

If we stopped there, `TimedeltaIndex.get_loc` would take a pretty significant performance hit, so I added a `unbox=True` kwarg to `_validate_scalar` that TDI.get_loc uses to keep perf flat.

Made `_validate_scalar` keyword-only past the first (non-self) argument.

Last, renamed `Index._validate_scalar` to `Index._require_scalar` to disambiguate the names.  (one of the two uses of `Index._validate_scalar` is wrong and will be removed, but thats a whole different can of worms)"
736943744,37647,ENH: Allow support for dtype datetime64[ms] or other units or numpy.datetime64 cols.,isichei,closed,2020-11-05T13:44:06Z,2020-11-06T01:11:24Z,"#### Is your feature request related to a problem?

I would like to be able to use Pandas datetime datatype column for datetimes beyond the bounds for `ns` units (which are  `1677-09-21 00:12:43.145225` and `2262-04-11 23:47:16.854775807`).

As will often be dealing with dates above and below these thresholds. Apologies if already open on another issue but couldn't quite find this. Not sure if tied in with [this issue](https://github.com/pydata/xarray/issues/789)

#### Describe the solution you'd like

```python
df = pd.DataFrame({""col"": [""2020-01-01"", ""2999-01-01"", ""1990-01-01"", None]})
```

One of the following:

```python
import pandas as pd

df = pd.DataFrame({""col"": [""2020-01-01"", ""2999-01-01"", ""1990-01-01"", None]})
df.col = pd.to_datetime(df.col, base_unit=""ms"")
df.col.dtype # datetime64[ms]
```

or 

```python
import pandas as pd
import numpy as np

df = pd.DataFrame()
df[""col""] = np.array([""2020-01-01"", ""2999-01-01"", ""1990-01-01"", None], dtype='datetime64[ms]')
df.col.dtype # datetime64[ms]
```

#### API breaking implications

Not sure, I assume this might be a big ask depending on how the column casting internals work.

#### Describe alternatives you've considered

At the moment the only workaround is having an object column with datetimes. However I believe this stops you from using the `.dt` method on the column which is really handy.

```
"
605222487,33739,PERF: fix #32976 slow group by for categorical columns,rtlee9,closed,2020-04-23T04:05:52Z,2020-11-06T01:53:02Z,"Aggregate categorical codes with fast cython aggregation for select `how` operations. Added new ASV benchmark copied from 32976 indicating > 99% improvement in performance for this case.

- [x] closes #32976
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
273670053,18277,CLN: Remove unnecessary uses of pd. in tests,jschendel,closed,2017-11-14T05:12:50Z,2020-11-06T02:08:58Z,"- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

- Mostly edits along the lines of `pd.Series` -> `Series` for things that were already imported and had redundant uses of `pd`.  
- I cleaned some of the `import` statements, but I didn't import anything new, though that could be done in the future for further cleaning. 
- Was able to shorten some multi-line statements for a net deletion of ~100 lines without changing the content of any tests.
- Only did the base level `tests` and `tests/indexing` so far. 
"
17863233,4522,BUG: GH4516 Fixed issue with sorting a duplicate multi-index that has multiple dtypes,jreback,closed,2013-08-09T13:42:27Z,2020-11-06T03:13:44Z,"closes #4516
"
713851997,36814,DOC: add example & prose of slicing with labels when index has duplicate labels ,junjunjunk,closed,2020-10-02T19:25:58Z,2020-11-06T06:13:08Z,- [x] closes #36251
723769657,37193,BUG: record warnings related to DatetimeIndex,ivanovmg,closed,2020-10-17T14:22:59Z,2020-11-06T15:32:27Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This PR handles warnings emitted when running tests in ``pandas/tests/plotting/test_datetimelike.py``.
Warnings looked like this:

```
pandas/tests/plotting/test_datetimelike.py::TestTSPlot::test_irreg_dtypes
  /workspaces/pandas/pandas/core/frame.py:3216: FutureWarning: Automatically casting object-dtype Index of datetimes to DatetimeIndex is deprecated and will be removed in a future version.  Explicitly cast to DatetimeIndex instead.
    return klass(values, index=self.index, name=name, fastpath=True)
```

Presumably it started after implementing this: https://github.com/pandas-dev/pandas/pull/36697/

The reason for not using ``tm.assert_produces_warning`` is that when I use it,
then I get an error that the warning is raised with the incorrect stacklevel."
676813966,35668,ENH: Enable short_caption in to_latex,ivanovmg,closed,2020-08-11T11:51:27Z,2020-11-06T15:34:10Z,"- [x] closes #36267
- [x] tests added / passed
- [x] passes black pandas
- [x] passes git diff upstream/master -u -- ""*.py"" | flake8 --diff
- [x] whatsnew entry


Enable short_caption for ``DataFrame.to_latex`` by expanding the meaning of kwarg ``caption``.
Optionally ``caption`` can be a ``Tuple[str, str] = full_caption, short_caption``.

The final caption macro would look like this:
```
\caption[short_caption]{full_caption}
```
"
684718601,35872,REF: simplify latex formatting,ivanovmg,closed,2020-08-24T14:32:13Z,2020-11-06T15:34:44Z,"- [x] closes https://github.com/pandas-dev/pandas/issues/35790
- [x] tests added ``pandas/tests/io/formats/test_latex.py``
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Refactor ``to_latex`` using polymorphism and the builder design pattern.

Polymorphism
--------------

Previously there was a complicated logic in multiple methods
for either longtable or regular table.
This PR implements various builders:
  - ``RegularTableBuilder``
  - ``LongTableBuilder``
  - ``TabularBuilder``

Selection of the appropriate builder is carried out in ``LatexFormatter``,
depending on the kwargs provided.

Each builder must implement construction of all the table's components:
  - Beginning and end of the environment,
  - Separators (top, middle, bottom),
  - Header,
  - Body.

This allows one eliminate complex logic throughout multiple methods.

Separate Concerns
------------------

Separated concerns: row formatting into string (separate class ``RowStringConverter``)
is carried out independently of the table builder.
Derived from ``RowStringConverter`` are two classes used to iterate over headers or body of the tables:
  - ``RowHeaderIterator``
  - ``RowBodyIterator``

Extract Methods
----------------

Multiple methods were extracted to improve code readability.
"
690332272,36046,REF: simplify CSVFormatter,ivanovmg,closed,2020-09-01T17:49:29Z,2020-11-06T15:35:00Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Refactor CSVFormatter
----------------------
1. Put data validation in setters
2. Extract helper methods and properties
"
695815026,36218,REF: eliminate method _write() in json writers,ivanovmg,closed,2020-09-08T12:09:13Z,2020-11-06T15:35:25Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Attempted to figure out what is wrong with the issue https://github.com/pandas-dev/pandas/issues/36211.
I noticed that the private methods ``_write`` are defined in each child of class ``Writer``, which simply re-define ``obj`` and call the parent ``_write`` method.
It turns out that the source of the recursion does not lie there, but I refactored the module slightly by removing the mediator ``_write`` method. Instead I put the differences between the writers into their ``__init__`` method.

It seems that source of the problem with the recursion mentioned in the bug report is in ``pandas/_libs/src/ujson/python/objToJSON.c``."
723784735,37195,CLN: parametrize test_nat_comparisons,ivanovmg,closed,2020-10-17T15:35:50Z,2020-11-06T15:36:53Z,"Parametrize ``TestDatetime64SeriesComparison.test_nat_comparisons``
using ``operator`` module.

If this kind of cleanup is considered reasonable,
then I will go through the remaining tests in this test module.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
735870320,37623,TST: 32bit dtype compat test_groupby_dropna,ivanovmg,closed,2020-11-04T07:07:07Z,2020-11-06T15:38:24Z,"Part of #36579

Related to ``test_groupby_nan_included``."
727131680,37332,REF: extract dialect validation,ivanovmg,closed,2020-10-22T07:36:10Z,2020-11-06T15:38:43Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Following https://github.com/pandas-dev/pandas/pull/36852, I extracted validation of dialect into a separate function."
724893682,37263,ENH: implement matching warning message,ivanovmg,closed,2020-10-19T18:54:36Z,2020-11-06T15:38:58Z,"- [ ] closes #37261
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Implement matching warning message.
New kwarg ``match`` in ``assert_produces_warning``."
627595291,34471,BUG: Fix read_json shape error on multi-indexed DF/SR with orient='split' #4889,LucasG0,closed,2020-05-29T23:26:51Z,2020-11-06T16:54:12Z,"
- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
737902019,37670,BUG: DataFrame constructor with 2d list input to index,arw2019,open,2020-11-06T16:47:59Z,2020-11-06T17:16:59Z,"xref  #34471

The `DataFrame` constructor doesn't appear to be working correctly with 

```python
In [3]: pd.DataFrame([[1], [2]], index=[[""a"", ""b""], [""c"", ""d""]])
Out[3]: 
     0
a c  1
b d  2
```

I *think* we might expect
```python
In [3]: pd.DataFrame([[1], [2]], index=[[""a"", ""b""], [""c"", ""d""]])
Out[3]: 
     0
a b  1
c d  2
```

FTR I'm not versant with the context here (whether this is in fact expected and we should maybe update the docs or if there is something to change)"
737413810,37659,CI: catch windows py38 OSError,jbrockmendel,closed,2020-11-06T02:50:11Z,2020-11-07T01:30:23Z,"Still seeing these following #37652, so catching again one level higher."
738116515,37679,REF/TST: share some api tests,jbrockmendel,closed,2020-11-07T00:27:48Z,2020-11-07T01:31:41Z,
642587580,34922,Completely rework the capitalization check algo in 'validate_rst_title_capitalization.py',cleconte987,closed,2020-06-21T15:45:37Z,2020-11-07T16:12:44Z,"- [ ] Modify file scripts/validate_rst_title_capitalization
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

PR on rework of the script 'validate_rst_title_capitalization.py' to better handle the capitalization, the capitalization exceptions, with the use of exceptions in the form of tuples to allow different writings for a particular word. The idea is to be able to use two different types inside 'CAPITALIZATION_EXCEPTIONS': one-word string, or tuple containing a number of word elements, with different capitalization.
You can experiment it by replacing some exceptions inside 'CAPITALIZATION_EXCEPTIONS' by tuples containing several times the same exception with different capitalisations. I already did it for 'categorical' with ('categorical', 'Categorical')"
738268286,37685,"BUG: DataFrame.groupby(., dropna=True, axis=0) incorrectly throws ShapeError [RESUBMIT]",arw2019,closed,2020-11-07T15:59:45Z,2020-11-07T16:26:09Z,"- [x] closes #35612
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Resubmitting #35751 to see if I can get to green here"
427212367,25927,pd.Series.loc.__getitem__ promotes to float64 instead of raising KeyError,allComputableThings,closed,2019-03-29T23:25:23Z,2020-11-07T23:47:12Z,"#### Code Sample, a copy-pastable example if possible

For:
```

result = a2b.loc[vals]       # pd.Series()[ np.array ]
```
If a2b is a series that maps `{int64:int64}` and vals is an `int64` array, the result should be a series that maps  `{int64:int64}`, or a KeyError should be thrown

Pasteable repo:
```
       import pandas as pd
       import numpy as np
       a2b = pd.Series(
           index = np.array([ 9724501000001103,  9724701000001109,  9725101000001107,
                     9725301000001109,  9725601000001103,  9725801000001104,
                     9730701000001104, 10049011000001109, 10328511000001105]),
           data = np.array([999000011000001104, 999000011000001104, 999000011000001104,
                        999000011000001104, 999000011000001104, 999000011000001104,
                        999000011000001104, 999000011000001104, 999000011000001104])
       )
       assert a2b.dtype==np.int64
       assert a2b.index.dtype==np.int64
       key = np.array([ 9724501000001103,  9724701000001109,  9725101000001107,
                             9725301000001109,  9725601000001103,  9725801000001104,
                             9730701000001104,
                             10047311000001102, # Misin in a2b.index
                             10049011000001109,
                             10328511000001105])
       result = a2b.loc[key]
       result
       assert result.dtype==np.int64
       assert result.index.dtype==np.int64
```

What happens:
```python
In [2]:         import pandas as pd
   ...:         import numpy as np
   ...:         a2b = pd.Series(
   ...:             index = np.array([ 9724501000001103,  9724701000001109,  9725101000001107,
   ...:                       9725301000001109,  9725601000001103,  9725801000001104,
   ...:                       9730701000001104, 10049011000001109, 10328511000001105]),
   ...:             data = np.array([999000011000001104, 999000011000001104, 999000011000001104,
   ...:                          999000011000001104, 999000011000001104, 999000011000001104,
   ...:                          999000011000001104, 999000011000001104, 999000011000001104])
   ...:         )
   ...:         assert a2b.dtype==np.int64
   ...:         assert a2b.index.dtype==np.int64
   ...:         key = np.array([ 9724501000001103,  9724701000001109,  9725101000001107,
   ...:                               9725301000001109,  9725601000001103,  9725801000001104,
   ...:                               9730701000001104,
   ...:                               10047311000001102, # Misin in a2b.index
   ...:                               10049011000001109,
   ...:                               10328511000001105])
   ...:         result = a2b.loc[key]
   ...:         result
   ...: 
Out[2]: 
 9.990000e+17   NaN
 9.990000e+17   NaN
 9.990000e+17   NaN
 9.990000e+17   NaN
 9.990000e+17   NaN
 9.990000e+17   NaN
 9.990000e+17   NaN
NaN             NaN
 9.990000e+17   NaN
 9.990000e+17   NaN
dtype: float64

In [3]:         assert result.dtype==np.int64
   ...:         assert result.index.dtype==np.int64
   ...: 
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-3-be86ec17a393> in <module>()
----> 1 assert result.dtype==np.int64
      2 assert result.index.dtype==np.int64

AssertionError: 

```

#### Problem description

I don't like this behavior because:
 - I have quietly lost all my data due to cast to float64
 - in other calls to __getitem__ a KeyError is raised if a value is not found in the index.


#### Expected Output

Asserts should not fail.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
In [4]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.15.candidate.1
python-bits: 64
OS: Linux
OS-release: 4.15.0-46-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.22.0
pytest: None
pip: 18.1
setuptools: 40.6.2
Cython: 0.29.1
numpy: 1.16.1
scipy: 1.2.0
pyarrow: None
xarray: None
IPython: 5.0.0
sphinx: None
patsy: 0.5.1
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: None
tables: None
numexpr: 2.6.8
feather: None
matplotlib: 2.1.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: 4.6.0
html5lib: 0.9999999
sqlalchemy: 1.2.17
pymysql: None
psycopg2: 2.7.7 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
738329522,37687,pd.Series.loc.__getitem__ promotes to float64 instead of raising KeyError,phofl,closed,2020-11-07T22:34:01Z,2020-11-07T23:50:43Z,"- [x] closes #25927
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

cc @jreback Added a test. The test above was the only one explicitly added for the ``KeyError``
"
738109331,37678,TST/REF: misplaced Categorical tests,jbrockmendel,closed,2020-11-07T00:00:46Z,2020-11-08T00:14:17Z,
737953539,37671,TYP: Index._concat,jbrockmendel,closed,2020-11-06T18:15:31Z,2020-11-08T00:54:08Z,
738106920,37677,REF/TST: collect indexing tests by method,jbrockmendel,closed,2020-11-06T23:51:28Z,2020-11-08T00:58:43Z,
738047788,37673,CLN: only call _wrap_results one place in nanmedian,jbrockmendel,closed,2020-11-06T21:17:40Z,2020-11-08T01:03:19Z,
737873429,37667,BUG: CategoricalIndex.equals casting non-categories to np.nan,jbrockmendel,closed,2020-11-06T16:04:19Z,2020-11-08T01:05:39Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
738172036,37683,CLN: _replace_single,jbrockmendel,closed,2020-11-07T04:52:12Z,2020-11-08T02:08:02Z,"Confirmed that the `regex` kwarg is always bool, which lets us remove some gymnastics (L2570-L2586).  Then removed always-True `convert` kwarg."
735419718,37604,ENH: extra colors generated by get_standard_colors,ivanovmg,closed,2020-11-03T15:32:42Z,2020-11-08T02:11:13Z,"#### Is your feature request related to a problem?

Not necessarily an issue, but one can get some code improvement and consistency in ``pandas.plotting._matplotlib.style.get_standard_colors``, when it is related to cycling colors.
During refactoring of the module (#37203) it turned out that cycling the colors works in a strange (although tolerable way).

```python
>>> from pandas.plotting._matplotlib.style import get_standard_colors
>>> get_standard_colors(num_colors=2, color='rgb')
'rgb'
>>> get_standard_colors(num_colors=3, color='rgb')
'rgb'
>>> get_standard_colors(num_colors=4, color='rgb')
'rgbr'
>>>
```

From the snippets above one can see that in the second and third examples work reasonable.
Meanwhile, the first one returns three colors, rather than two.
Extra color(s) will be ignored anyway, but in regards to the discussion in the referenced PR, the way of cycling colors can be improved https://github.com/pandas-dev/pandas/pull/37203#discussion_r516294252

#### Describe the solution you'd like

I suggest using matplotlib's cycler (or other generator) to cycle through the colors obtained and get the exact number of colors required, without any extra.
This will simplify the code involving colors cycling in ``get_standard_colors``.

#### Expected Output

```python
>>> from pandas.plotting._matplotlib.style import get_standard_colors
>>> get_standard_colors(num_colors=2, color='rgb')
'rg'
```

#### API breaking implications

Will break some tests related to ``get_standard_colors`` only as they expect extra colors to be returned on some occasions.

#### Describe alternatives you've considered

See https://github.com/pandas-dev/pandas/pull/37203 without involving modifications to color cycling."
735549450,37610,BUG: pd.Timestamp loses fold when removing timezone which is inconsistent with datetime.datetime,AlexeyDmitriev,closed,2020-11-03T18:35:51Z,2020-11-08T02:22:48Z,"- [+] I have checked that this issue has not already been reported.

- [+] I have confirmed this bug exists on the latest version of pandas.
I have tested on {'dirty': False, 'error': None, 'full-revisionid': '67a3d4241ab84419856b84fc3ebc9abcbe66c6b3', 'version': '1.1.4'}

- [-] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Problem description

datetime.datetime saves the fold when replacing tzinfo, but pandas.Timestamp doesn't 

#### Code Sample, a copy-pastable example

```python
import dateutil.tz
import pandas
from datetime import datetime as dt
print(pandas.Timestamp(1256427000000000000, tz=dateutil.tz.gettz('Europe/Moscow'), unit='ns'))
# Timestamp('2009-10-25 02:30:00+0300', tz='dateutil//usr/share/zoneinfo/Europe/Moscow')
print(pandas.Timestamp(1256427000000000000, tz=dateutil.tz.gettz('Europe/Moscow'), unit='ns').fold)
# 1
print(pandas.Timestamp(1256427000000000000, tz=dateutil.tz.gettz('Europe/Moscow'), unit='ns').replace(tzinfo=None).fold)
# 0
print(dt(2009, 10, 25, 2,30, fold=1, tzinfo=dateutil.tz.gettz('Europe/Moscow')).replace(tzinfo=None).fold)
# 1
```


#### Expected Output
```
Timestamp('2009-10-25 02:30:00+0300', tz='dateutil//usr/share/zoneinfo/Europe/Moscow')
1
1
1
```

#### Output of ``pd.show_versions()``

<details>


commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.8.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.5.0
Version          : Darwin Kernel Version 19.5.0: Tue May 26 20:41:44 PDT 2020; root:xnu-6153.121.2~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.4
numpy            : 1.16.4
pytz             : 2020.4
dateutil         : 2.8.0
pip              : 20.1.1
setuptools       : 39.0.1
Cython           : 3.0a6
pytest           : 5.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.0.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
737433490,37660,REF: implement _wrap_reduction_result,jbrockmendel,closed,2020-11-06T03:45:04Z,2020-11-08T02:38:59Z,
738332518,37688,TYP: make some internal funcs keyword-only,jbrockmendel,closed,2020-11-07T22:54:51Z,2020-11-08T02:40:21Z,
738338920,37691,REF: make Series._replace_single a regular method,jbrockmendel,closed,2020-11-07T23:44:47Z,2020-11-08T02:41:15Z,
737877573,37668,TST: MatplotlibDeprecationWarning in test related to pie chart,ivanovmg,closed,2020-11-06T16:10:31Z,2020-11-08T03:00:20Z,"- [x] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```bash
pytest -Werror pandas/tests/plotting/tests_frame.py::TestDataFramePlots::test_pie_df_nan
```


#### Problem description

When testing ``test_frame.py`` the following warning is issued:
```
============================================================================================================= warnings summary =============================================================================================================
pandas/tests/plotting/test_frame.py::TestDataFramePlots::test_pie_df_nan
pandas\plotting\_matplotlib\core.py:1551: MatplotlibDeprecationWarning: normalize=None does not normalize if the sum is less than 1 but this behavior is deprecated since 3.3 until two minor releases later. After the deprecation period the default value will be normalize=True. To prevent normalization pass normalize=False
    results = ax.pie(y, labels=blabels, **kwds)

-- Docs: https://docs.pytest.org/en/stable/warnings.html
```

This code is the reason for the warnings:

```
matplotlib/axes/_axes.py
...
    def pie(...):
        ...
        if normalize is None:
            if sx < 1:
                cbook.warn_deprecated(
                    ""3.3"", message=""normalize=None does not normalize ""
                    ""if the sum is less than 1 but this behavior ""
                    ""is deprecated since %(since)s until %(removal)s. ""
                    ""After the deprecation ""
                    ""period the default value will be normalize=True. ""
                    ""To prevent normalization pass normalize=False "")
            else:
                normalize = True
```

#### Expected Output

No warnings.

#### Versions

The issue is present starting from matplotlib version 3.3.
Before that ``normalize`` kwarg was not applicable to ``plt.pie``.

<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : 210c06e2f29bdb968fa715756bef9dcf1dfdc818
python           : 3.8.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1251

pandas           : 1.2.0.dev0+1079.g210c06e2f
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 49.6.0.post20201009
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : 5.37.4
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.4
fastparquet      : 0.4.1
gcsfs            : 0.7.1
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 2.0.0
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.0
sqlalchemy       : 1.3.20
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.51.2
</details>"
719440617,37081,PERF: regression in DataFrame reduction ops performance,jorisvandenbossche,closed,2020-10-12T14:49:31Z,2020-11-08T03:01:28Z,"From https://pandas.pydata.org/speed/pandas/#stat_ops.FrameOps.time_op?Cython=0.29.21&Cython=0.29.16&p-op='sum'&p-dtype='int'&p-axis=0&commits=3a043f2d-4c03d07b&x-axis-scale=date

Reproducer:

```
values = np.random.randn(100000, 4)  
df = pd.DataFrame(values).astype(""int"")
%timeit df.sum()
```

increased with a  factor 2 to 3 x somewhere the last days."
729832460,37426,PERF: reverted change from commit 7d257c69 to solve issue  #37081,ukarroum,closed,2020-10-26T19:10:02Z,2020-11-08T03:01:32Z,"reverted change from commit 7d257c697 to solve issue  #37081

- [x] closes #37081
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
237576202,16748,DataFrameGroupBy.boxplot with subplots=False fails when using column param,tdpetrou,closed,2017-06-21T15:31:43Z,2020-11-08T03:03:11Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame({'cat':np.random.choice(list('abcde'), 100), 
                    'v':np.random.rand(100), 
                    'v1':np.random.rand(100)})
df.groupby('cat').boxplot(subplots=False, column='v')
```
outputs
`KeyError: ""['v'] not in index""`
#### Problem description
The boxplot works when either `subplots=False` or `column='v'` but not when they are both specified.

#### Expected Output
A single axes plot with each group having its own boxplot. The column 'cat' would label the x-axis.
#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.1.final.0
python-bits: 64
OS: Darwin
OS-release: 15.6.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.20.2
pytest: 3.0.7
pip: 9.0.1
setuptools: 35.0.2
Cython: 0.25.2
numpy: 1.13.0
scipy: 0.19.0
xarray: None
IPython: 6.0.0
sphinx: 1.5.5
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: 1.2.0
tables: 3.4.2
numexpr: 2.6.2
feather: None
matplotlib: 2.0.2
openpyxl: 2.4.7
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: 0.9.6
lxml: 3.7.3
bs4: 4.6.0
html5lib: 0.999999999
sqlalchemy: 1.1.9
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
pandas_gbq: None
pandas_datareader: 0.3.0.post

</details>
"
484204619,28102,DataFrameGroupby.boxplot fails when subplots=False,charlesdong1991,closed,2019-08-22T20:37:07Z,2020-11-08T03:03:16Z,"- [x] closes #16748
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

![Screen Shot 2020-10-13 at 8 06 48 PM](https://user-images.githubusercontent.com/9269816/95898874-037e6980-0d90-11eb-85b3-e00fe36611b9.png)
"
226226696,16228,ERR: left_index AND left_on or right_index AND right_on should raise,Satyadev592,closed,2017-05-04T09:40:56Z,2020-11-08T03:08:00Z,"There seems to be a bug in the pandas merge , particularly on the left_index. 

```
df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],
                'B': ['B0', 'B1', 'B2', 'B3'],
                'C': ['C0', 'C1', 'C2', 'C3'],
                'D': ['D0', 'D1', 'D2', 'D3']},index=[0, 1, 2, 3])

df2 = pd.DataFrame({'A': ['A0', 'A5', 'A6', 'A7'],
                'B': ['B1', 'B5', 'B6', 'B7'],
                'C': ['A1', 'C5', 'C6', 'C7'],
                'D': ['B1', 'D5', 'D6', 'D7']},index=[4, 5, 6, 7])

pd.merge(df1, df2, how='outer', left_index=True, left_on='A', right_on='A')
pd.merge(df1, df2, how='outer', right_index=True, left_on='A', right_on='A')
```

The above two merges give me different output. 

This a fork of a stack overflow post-
http://stackoverflow.com/questions/43779003/will-output-change-if-right-index-is-used-instead-of-left-index-with-both-left-o"
734295330,37577,ENH: DataFrame Constructions from Data Classes,daskol,closed,2020-11-02T08:19:05Z,2020-11-08T08:45:17Z,"#### Is your feature request related to a problem?

I wish to construct `pandas.DataFrame` from iterable of `dataclasses.dataclass` as from iterable of tuples `DataFrame.from_records`. The rationale behind is that data classes is more typed object than general tuple or dictionary. Also, data classes more memory efficient than `tuple`'s. It makes data classes attractive to use them instead of `dict`'s or `tuple`'s whenever schema is known.

#### Describe the solution you'd like

I would like class method `.from_dataclasses` which allows `DataFrame` construction and type inference from uniform (for simplicity) sequence of data classes. See example below.

```python
import pandas as pd
from dataclasses import dataclass


@dataclass
class Record:
    id: int
    name: str
    constant: float

df = pd.DataFrame.from_dataclasses([
    Record(0, 'Landau', 3.1415926),
    Record(1, 'Kapitsa', 2.718281828459045),
    Record(2, 'Bogolyubov', 6.62607015),
])

print(df.dtypes)
#  id            int64
#  name         object
#  constant    float64
#  dtype: object
````
In the example above schema of `DataFrame` is infered with `Record.__annotations__` dictionary which contains type user provided type information. API could also provide ways to validate schema in runtime by comparying type of actual type and specified type for a column.

#### API breaking implications

There is no API breaking in general but there is requirements to minimum Python version (which is 3.7).
"
733815740,37547,ENH: Improve error reporting for wrong merge cols,phofl,closed,2020-10-31T22:05:53Z,2020-11-08T12:06:18Z,"- [x] closes #16228
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Fixed error reporting when double conditions are given. Additionally I found, that something like 
```
left = DataFrame({""a"": [1, 2], ""b"": [3, 4]})
right = DataFrame({""a"": [1, 1], ""c"": [5, 6]})
pd.merge(left, right, right_on=""a"")
```

raised an uncontrolled error:

```
Traceback (most recent call last):
  File ""/home/developer/.config/JetBrains/PyCharm2020.2/scratches/scratch_4.py"", line 25, in <module>
    print(pd.merge(left, right, right_on=""a""))
  File ""/home/developer/PycharmProjects/pandas/pandas/core/reshape/merge.py"", line 72, in merge
    op = _MergeOperation(
  File ""/home/developer/PycharmProjects/pandas/pandas/core/reshape/merge.py"", line 643, in __init__
    self._validate_specification()
  File ""/home/developer/PycharmProjects/pandas/pandas/core/reshape/merge.py"", line 1247, in _validate_specification
    if len(self.right_on) != len(self.left_on):
TypeError: object of type 'NoneType' has no len()
```

"
738333443,37689,CLN: Clean indexing tests,phofl,closed,2020-11-07T23:01:09Z,2020-11-08T12:07:04Z,"- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Found a few unuesed statements"
738367217,37695,Dataframe.sum() returns an error with MultiIndex columns and skipna=False,GYHHAHA,closed,2020-11-08T04:02:54Z,2020-11-08T14:55:31Z,"- [ ] closes #37622
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
733782946,37540,TYP: PoC for honest ABC,rhshadrach,closed,2020-10-31T18:38:26Z,2020-11-08T15:11:08Z,"PoC implementation for using honest ABCs for pandas objects. Currently only implements ABCNDFrame. Compare:

```
def foo(obj: ABCNDFrame):
    reveal_type(obj.attrs)


def bar(obj: ABCDataFrame):
    reveal_type(obj.attrs)
```

In this PR, mypy will output `Revealed type is 'builtins.dict[Union[typing.Hashable, None], Any]'` for the former whereas the latter is `Revealed type is 'Any'`.

A script can be written (and I plan to, if this idea gets buy-in) to generate the ABC class off of the concrete class. Likewise, tests can be written to ensure that ABCNDFrame and NDFrame don't diverge.

docstrings can be removed from the ABC class; I just wanted to highlight that we could just have the docs in the ABC class (don't know if that would be desirable).

cc @simonjayhawkins, @jreback, @jbrockmendel "
737560630,37662,"Move inconsistent namespace check to pre-commit, fixup more files",MarcoGorelli,closed,2020-11-06T08:10:27Z,2020-11-08T21:43:01Z,xref https://github.com/pandas-dev/pandas/pull/37401#discussion_r515312381
736191724,37631,"BUG: Regression in 1.1.0. ""invalid slicing for a 1-ndim ExtensionArray""",buhrmann,closed,2020-11-04T15:09:52Z,2020-11-09T00:38:08Z,"- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of pandas.
- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
s1 = pd.Series(list(""abc"")).astype(""category"").iloc[[0]]
s2 = pickle.loads(pickle.dumps(s1))
print(s1.dropna())
print(s2.dropna())
```

```bash
0    a
dtype: category
Categories (3, object): ['a', 'b', 'c']

---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-1-c7b1204ccdb7> in <module>
      6 s2 = pickle.loads(pickle.dumps(s1))
      7 print(s1.dropna())
----> 8 print(s2.dropna())

~/anaconda/envs/grapy/lib/python3.7/site-packages/pandas/core/series.py in dropna(self, axis, inplace, how)
   4883 
   4884         if self._can_hold_na:
-> 4885             result = remove_na_arraylike(self)
   4886             if inplace:
   4887                 self._update_inplace(result)

~/anaconda/envs/grapy/lib/python3.7/site-packages/pandas/core/dtypes/missing.py in remove_na_arraylike(arr)
    564     """"""
    565     if is_extension_array_dtype(arr):
--> 566         return arr[notna(arr)]
    567     else:
    568         return arr[notna(np.asarray(arr))]

~/anaconda/envs/grapy/lib/python3.7/site-packages/pandas/core/series.py in __getitem__(self, key)
    904             key = check_bool_indexer(self.index, key)
    905             key = np.asarray(key, dtype=bool)
--> 906             return self._get_values(key)
    907 
    908         return self._get_with(key)

~/anaconda/envs/grapy/lib/python3.7/site-packages/pandas/core/series.py in _get_values(self, indexer)
    966     def _get_values(self, indexer):
    967         try:
--> 968             return self._constructor(self._mgr.get_slice(indexer)).__finalize__(self,)
    969         except ValueError:
    970             # mpl compat if we look up e.g. ser[:, np.newaxis];

~/anaconda/envs/grapy/lib/python3.7/site-packages/pandas/core/internals/managers.py in get_slice(self, slobj, axis)
   1559 
   1560         blk = self._block
-> 1561         array = blk._slice(slobj)
   1562         block = blk.make_block_same_class(array, placement=slice(0, len(array)))
   1563         return type(self)(block, self.index[slobj])

~/anaconda/envs/grapy/lib/python3.7/site-packages/pandas/core/internals/blocks.py in _slice(self, slicer)
   1756             if not isinstance(first, slice):
   1757                 raise AssertionError(
-> 1758                     ""invalid slicing for a 1-ndim ExtensionArray"", first
   1759                 )
   1760             # GH#32959 only full-slicers along fake-dim0 are valid

AssertionError: ('invalid slicing for a 1-ndim ExtensionArray', array([ True]))
```

#### Problem description

Not sure what changes in the serialization roundtrip through pickle, but it seems the copied Series cannot be indexed with a Boolean slice anymore, tripping up `dropna()` as a result. The following code more directly exposes the error:

```python
s1 = pd.Series(list(""abc"")).astype(""category"").iloc[[0]]
s2 = pickle.loads(pickle.dumps(s1))
s2[[True]]
```

The error happens e.g. when processing multiple Series in parallel (triggering serialization with pickle), and when a categorical Series has been filtered down to a single row. With another dtype, or more than one row, this error doesn't get triggered. 

The regression must been introduced in version 1.1.0, as in 1.0.5 the above code works as expected.

#### Expected Output

Behaviour of slicing, `dropna` etc. should be same before and after pickling a Series, and independent of the number of rows.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Mon Aug 31 22:12:52 PDT 2020; root:xnu-6153.141.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.1.4
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 20.0.2
setuptools       : 46.1.1.post20200322
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : 0.9.3
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.46.0

</details>
"
738523217,37704,"REF: simplify NDFrame.replace, ObjectBlock.replace",jbrockmendel,closed,2020-11-08T18:16:28Z,2020-11-09T00:39:32Z,"This makes it so that we only call BlockManager.replace from one spot, so we can rule out more cases when reasoning about the arguments in ObjectBlock.replace.

This will have a merge conflict with #37696; doesnt matter which one goes in first.  One both are in, there are further simplifications available.

I'm hopeful that we'll be able to get rid of Categorical.replace altogether (and with it, CategoricalBlock.replace, CategoricalBlock._replace_list), but that's a few steps away at best.
"
737388111,37658,REF: dont support dt64tz in nanmean,jbrockmendel,closed,2020-11-06T01:36:47Z,2020-11-09T00:44:40Z,"nanmean is annotated as taking a ndarray, no reason for it to support dt64tz"
737022189,37650,REF: implement Categorical.encode_with_my_categories,jbrockmendel,closed,2020-11-05T15:18:13Z,2020-11-09T00:54:30Z,"Categorical._validate_listlike is misleading, since it treats not-in-categories entries as NAs, whereas we usually want to raise when we see those.  This implements the more obviously-named encode_with_my_categories"
447237850,26491,Label-based indexing on a Series with an index of dtype=object raises TypeError when using slices with integer bound,plammens,closed,2019-05-22T16:39:13Z,2020-11-09T02:52:06Z,"When indexing a pandas axis that has an `index` of `dtype=object`, with label-based indexing, passing a slice that contains an integer bound results in a `TypeError`, even if the integer *is indeed a label* in the `index` of the series.

### Details
#### Code Sample

When indexing a pandas axis that has an `index` of `dtype=object`, with label-based indexing, passing a slice that contains an integer bound results in a `TypeError`, even if the integer *is indeed a label* in the `index` of the series.

```python
import pandas as pd

series = pd.Series(range(4), index=[1, 'spam', 2, 'eggs'])
series
## 1       0
## spam    1
## 2       2
## eggs    3
## dtype: int64

series.index
## Index([1, 'spam', 2, 'eggs'], dtype='object')

series.loc['spam':'eggs']
## spam    1
## 2       2
## eggs    3
## dtype: int64

series.loc[1:'eggs']
# raises TypeError
```
#### Problem description

When indexing a `pd.Series` (or an equivalent pandas object) that has an `index` of `dtype=object` (upcasted for example from a list of `int`s and `str`s), with `.loc`'s label-based indexing, passing a slice that contains an integer bound (as in `1:'eggs'`) results in a `TypeError`, even if the integer *is indeed a label* in the `index` of the series:

```text
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Paolo\Code\PycharmProjects\pandas\pandas\core\indexing.py"", line 1504, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File ""C:\Users\Paolo\Code\PycharmProjects\pandas\pandas\core\indexing.py"", line 1871, in _getitem_axis
    return self._get_slice_axis(key, axis=axis)
  File ""C:\Users\Paolo\Code\PycharmProjects\pandas\pandas\core\indexing.py"", line 1537, in _get_slice_axis
    slice_obj.step, kind=self.name)
  File ""C:\Users\Paolo\Code\PycharmProjects\pandas\pandas\core\indexes\base.py"", line 4784, in slice_indexer
    kind=kind)
  File ""C:\Users\Paolo\Code\PycharmProjects\pandas\pandas\core\indexes\base.py"", line 5002, in slice_locs
    start_slice = self.get_slice_bound(start, 'left', kind)
  File ""C:\Users\Paolo\Code\PycharmProjects\pandas\pandas\core\indexes\base.py"", line 4914, in get_slice_bound
    label = self._maybe_cast_slice_bound(label, side, kind)
  File ""C:\Users\Paolo\Code\PycharmProjects\pandas\pandas\core\indexes\base.py"", line 4861, in _maybe_cast_slice_bound
    self._invalid_indexer('slice', label)
  File ""C:\Users\Paolo\Code\PycharmProjects\pandas\pandas\core\indexes\base.py"", line 3154, in _invalid_indexer
    kind=type(key)))
TypeError: cannot do slice indexing on <class 'pandas.core.indexes.base.Index'> with these indexers [1] of <class 'int'>
```

Other non-integer (and non-float) slice bounds are accepted, as shown above.

Is this the expected behaviour? Possibly it's a documentation issue. According to the section [Selection by Label](http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#indexing-label) in the indexing doc page, the second warning says:

> .loc is strict when you present slicers that are not compatible (or convertible) with the index type. For example using integers in a DatetimeIndex. These will raise a TypeError.

What exactly is meant by ""compatible or convertible""? An `int` is definitely an `object`, so it should be compatible with the index type.

Furthermore, the first paragraph in the section reads (emphasis mine):

> pandas provides a suite of methods in order to have purely label based indexing. This is a strict inclusion based protocol. Every label asked for must be in the index, or a KeyError will be raised. When slicing, both the start bound AND the stop bound are included, if present in the index. **Integers are valid labels**, but they refer to the label and not the position.

And, finally, the subsection [Slicing with labels](http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#slicing-with-labels) says:

> When using .loc with slices, if both the start and the stop labels are present in the index, then elements located between the two (including them) are returned

By reading this, to me it would seem that using a slice such as `1:'eggs'` or `1:2` in the above example with `loc` should be perfectly valid. The indexers are present in the index, and, as stated above, integers are valid labels.

----------------------------

The exception originates at the `_maybe_cast_slice_bound` check, which rejects all integers regardless of whether the `Index` contains any integers:

https://github.com/pandas-dev/pandas/blob/6d2398a58fda68e40f116f199439504558c7774c/pandas/core/indexes/base.py#L4846-L4863


#### Expected Output

I expected `series.loc[1:'eggs']` not to raise `TypeError` because of the integer label. I expected that expression to return the following slice view of the `Series`:

```text
1       0
spam    1
2       2
eggs    3
dtype: int64
```

#### Output of ``pd.show_versions()``

<details>

```text
INSTALLED VERSIONS
------------------
commit: 6d2398a58fda68e40f116f199439504558c7774c
python: 3.7.3.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.25.0.dev0+598.g6d2398a58
pytest: 4.5.0
pip: 19.1.1
setuptools: 41.0.1
Cython: 0.29.7
numpy: 1.16.3
scipy: 1.3.0
pyarrow: 0.13.0
xarray: 0.12.1
IPython: 7.5.0
sphinx: 2.0.1
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2019.1
blosc: 1.8.1
bottleneck: 1.2.1
tables: 3.5.1
numexpr: 2.6.9
feather: None
matplotlib: 3.1.0
openpyxl: 2.6.2
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.8
lxml.etree: 4.3.3
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.3.3
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: 0.2.1
fastparquet: 0.3.1
pandas_gbq: None
pandas_datareader: None
gcsfs: None
```

</details>
"
738614722,37708,BUG: Wrong index after monthly resampling,justmert,closed,2020-11-09T01:53:36Z,2020-11-09T02:54:31Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
#### Reproduce

```python
df_sent = df_date.groupby('name').resample('M').count()
```
**df_date**
![image](https://user-images.githubusercontent.com/37740842/98491929-65bf7280-2247-11eb-8167-2a172d9f3e1e.png)


**df_sent**
![image](https://user-images.githubusercontent.com/37740842/98491951-75d75200-2247-11eb-9be4-173f11f57707.png)

#### Problem description
When i resample my dataframe to month, it gives wrong index. As you can see my dataframe (__df_date__) has dates from __2020-06-13 10:23:00__ to __2020-09-11 00:22:00__ which has month **6** to **9** so if i resample to month, i expect new dataframe's months from  __2020-06__ to __2020-09__. 
#### Expected Output
![image](https://user-images.githubusercontent.com/37740842/98492238-4aa13280-2248-11eb-88f3-bf472a1e9253.png)
After resampling, new dataframe needs to has these months just like the above


I have searched a solution to this unexpected behaviour and couldn't find anything. If i have wrong intuition about resampling, please correct me. Also i have encountered the same problem in [stackoverflow](https://stackoverflow.com/questions/52527619/wrong-index-after-monthly-resampling-of-dataframe-pandas) which hasn't got a solution

#### Output of ``pd.show_versions()``

<details>

pandas version= 1.1.3
INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.9.4-arch1-1
Version          : #1 SMP PREEMPT Wed, 04 Nov 2020 21:41:09 +0000
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.0.post20201006
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.3
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.20
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.51.2

</details>
"
738368165,37696,"REF: implement replace_regex, remove unreachable branch in ObjectBlock.replace",jbrockmendel,closed,2020-11-08T04:12:17Z,2020-11-09T03:32:20Z,
723854400,37209,DOC: add value counts as related method to count,erictleung,closed,2020-10-17T21:51:22Z,2020-11-09T05:09:36Z,"Counting values using `.value_counts()`, more generally, is a counting task, so it felt natural to link the `.count()` page to `.value_counts()`.

I copied the description for `.value_counts()` from the ""See also"" box in [`.drop_duplicates()`](https://github.com/pandas-dev/pandas/blob/9fed16cd4c302e47383480361260d63dc23cbefc/pandas/core/frame.py#L5102).

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
737366929,37657,BUG: unpickling modifies Block.ndim,jbrockmendel,closed,2020-11-06T00:38:38Z,2020-11-09T11:20:22Z,"- [x] closes #37631
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
737754185,37664,REF: simplify cycling through colors,ivanovmg,closed,2020-11-06T13:23:06Z,2020-11-09T11:33:41Z,"- [ ] closes #37604
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I tried to address #37604, but it turns out that on some occasions it is absolutely mandatory to return more colors than ``num_colors`` says.
In particular, this is the test that cannot be updated and the underlying code cannot be easily modified as well.

```
pandas/tests/plotting/test_frame.py
...
class TestDataFramePlots(TestPlotBase):
...
    def test_bar_user_colors(self):
        df = DataFrame(
            {""A"": range(4), ""B"": range(1, 5), ""color"": [""red"", ""blue"", ""blue"", ""red""]}
        )
        # This should *only* work when `y` is specified, else
        # we use one color per column
        ax = df.plot.bar(y=""A"", color=df[""color""])
        result = [p.get_facecolor() for p in ax.patches]
        expected = [
            (1.0, 0.0, 0.0, 1.0),
            (0.0, 0.0, 1.0, 1.0),
            (0.0, 0.0, 1.0, 1.0),
            (1.0, 0.0, 0.0, 1.0),
        ]
        assert result == expected
```

When ""y"" is specified, then the number of colors must be the length of the series df[""A""],
but inside the BarPlot class there is no opportunity to check for that condition.

So, let ``get_standard_colors`` return maximum of ``num_colors`` and ``len(colors)``.
This way all tests work without modifications,
just minor improvement in the cycling function."
737879754,37669,TST: fix warning for pie chart,ivanovmg,closed,2020-11-06T16:13:48Z,2020-11-09T11:33:52Z,"- [ ] closes #37668
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Fix MatplotlibDeprecationWarning for pie chart by specifically passing ``normalize=True``."
737861198,37666,TST: match matplotlib warning message,ivanovmg,closed,2020-11-06T15:47:11Z,2020-11-09T11:34:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Follow up on #36982.
Since, currently ``assert_produces_warning`` now allows matching warning message,
instead of the comment we will match the message."
738939008,37713,Backport PR #37657 on branch 1.1.x: BUG: unpickling modifies Block.ndim,simonjayhawkins,closed,2020-11-09T11:19:25Z,2020-11-09T12:49:28Z,Backport PR #37657
736783823,37644,BUG: preserve fold in Timestamp.replace,AlexKirko,closed,2020-11-05T09:54:34Z,2020-11-09T13:10:19Z,"- [X] closes #37610
- [X] 1 tests added / 1 passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

## Problem
We currently lose `fold` information (whether the Timestamp corresponds to the first or second instance of wall clock time in a DST transition) when calling `Timestamp.replace`.

## Solution
A simple addition to the code of `Timestamp.replace` should fix this.

## Test
I added the test for the use case I came up with in the issue discussion. The OP example is losing `fold` when deleting timezone information with `Timetsamp.replace`, and that's not really a bug, but replacing a valid dateutil timezone with itself and losing `fold` definitely is.

The proposed solution fixes the original example as well. I just don't think we should be tracking it in tests, as it's not clear to me why `fold` must be preserved in a `Timestamp` with no timezone information (but it is the convention recommended in [PEP 495, fold section](https://www.python.org/dev/peps/pep-0495/#the-fold-attribute) to let users keep invalid fold and to just ignore it).

## Some details
IIRC, we ignore `fold`, when it doesn't do anything, so we should be safe preserving `fold` while replacing tzinfo with None, as in the OP example. I remember this coming up when we introduced fold support, and we made sure that the fold-aware functions don't care what fold is outside of a DST transition with a dateutil timezone (this was done to satisfy the requirements of [PEP 495, fold section](https://www.python.org/dev/peps/pep-0495/#the-fold-attribute))."
733943243,37556,TYP: Check untyped defs (except vendored),simonjayhawkins,closed,2020-11-01T11:27:00Z,2020-11-09T13:21:17Z,"@jbrockmendel has recently addressed many of the modules that were not previously checked. There are now many less modules outstanding.

There has been a concern in the past that if we add ignores, the issues may not be addressed and sit in the codebase. (from previous experience with other linting tools)

There are however pros to including the ignores. The issues are visible to people working on the relevant code and when reviewing the code. **Also as mypy will be checking the relevant modules, PRs changing these modules will have additional static checking on ci and the chance of regression/errors is likely reduced.**

IMO the pros always outweighed the cons, but now that there are less ignores involved (admittedly the number is still not trivial), I think it is appropriate to reconsider this approach.

One caveat, NumPy types currently resolve to `Any` and we have not yet discussed how we will 'switch' to using NumPy types #36092. (reverting to using `check_untyped_defs=False` on some modules could make the switch more managable)

Another possible upside, is in the review of typing PRs to fix these issues. Reviewers of typing PRs cannot see explicitly see why changes are being made and a typical reviewer is normally suspicious of any changes to the codebase. This can lead to long review cycles of typing PRs. I'm not sure if having the resolved mypy errors in the diff will help, but if the hypothesis is correct could be an argument for following a similar approach for resolving the numerous errors that will be reported once NumPy types no longer resolve to `Any`"
738657951,37710,REF: remove ObjectBlock._replace_single,jbrockmendel,closed,2020-11-09T04:07:19Z,2020-11-09T15:53:13Z,
738854210,37712,BUG: rolling covariance then dropping nans doesn't drop first level index which corresponds to those nans,needvim2c,closed,2020-11-09T09:28:04Z,2020-11-09T18:00:52Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample

```python
import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.randn(10, 5))
df = df.rolling(5).cov().dropna()
df.index.levels[0]
```

#### Current Output

```python
Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')
```

#### Problem description

Since doing a rolling covariance would cause the first 4 entries to the dataframe to be nan, which I dropped. However, if I try to access the first level of the index, I would get the full original index, being from 0 to 9. One would expect to receive an index for which the corresponding nan entries was removed for the new first level index. However, 

```python
df.index.get_level_values(0)
```

gives the expected output.

#### Expected Output

```python
Int64Index([4, 5, 6, 7, 8, 9], dtype='int64')
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.9.0.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.1.4
numpy            : 1.19.3
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
571335211,32268,"`df.loc[mask, 'column'] = df['column2']` fails if mask is all False and column is a localized datetime",arnau126,closed,2020-02-26T12:54:54Z,2020-11-09T18:06:03Z,"#### Code Sample

```python
import pandas as pd

df1 = pd.DataFrame({
    'dt': pd.to_datetime(['2017-07-17 00:00:00', '2018-08-08 00:00:00']),
    'dt_tz': pd.to_datetime(['2017-07-17 00:00:00', '2018-08-08 00:00:00'], utc=True),
    'char': ['a', 'b'],
    'num':  [1, 2]
})

df2 = pd.DataFrame({
    'dt': pd.to_datetime(['2019-09-19 00:00:00', '2020-02-20 00:00:00']),
    'dt_tz': pd.to_datetime(['2019-09-19 00:00:00', '2020-02-20 00:00:00'], utc=True),
    'char': ['c', 'd'],
    'num':  [3, 4],
})

mask = pd.Series([False, False])

# the following lines do nothing (as expected) because mask is all False
df1.loc[mask, ""dt""] = df2[""dt""]
df1.loc[mask, ""char""] = df2[""char""] 
df1.loc[mask, ""num""] = df2[""num""]

```

```python
# this line should do nothing, too. But instead it raises a ValueError
df1.loc[mask, ""dt_tz""] = df2[""dt_tz""]
```

Traceback:
<details>

```python
ValueError                                Traceback (most recent call last)
<ipython-input-3-17df28957215> in <module>
----> 1 df1.loc[mask, ""dt_tz""] = df2[""dt_tz""]

~/.virtualenvs/lr/lib/python3.7/site-packages/pandas/core/indexing.py in __setitem__(self, key, value)
    668             key = com.apply_if_callable(key, self.obj)
    669         indexer = self._get_setitem_indexer(key)
--> 670         self._setitem_with_indexer(indexer, value)
    671 
    672     def _validate_key(self, key, axis: int):

~/.virtualenvs/lr/lib/python3.7/site-packages/pandas/core/indexing.py in _setitem_with_indexer(self, indexer, value)
   1009                     labels, value, plane_indexer, lplane_indexer, self.obj
   1010                 ):
-> 1011                     setter(labels[0], value)
   1012 
   1013                 # per label values

~/.virtualenvs/lr/lib/python3.7/site-packages/pandas/core/indexing.py in setter(item, v)
    959                     s._consolidate_inplace()
    960                     s = s.copy()
--> 961                     s._data = s._data.setitem(indexer=pi, value=v)
    962                     s._maybe_update_cacher(clear=True)
    963 

~/.virtualenvs/lr/lib/python3.7/site-packages/pandas/core/internals/managers.py in setitem(self, **kwargs)
    559 
    560     def setitem(self, **kwargs):
--> 561         return self.apply(""setitem"", **kwargs)
    562 
    563     def putmask(self, **kwargs):

~/.virtualenvs/lr/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, filter, **kwargs)
    440                 applied = b.apply(f, **kwargs)
    441             else:
--> 442                 applied = getattr(b, f)(**kwargs)
    443             result_blocks = _extend_blocks(applied, result_blocks)
    444 

~/.virtualenvs/lr/lib/python3.7/site-packages/pandas/core/internals/blocks.py in setitem(self, indexer, value)
   2439             isinstance(indexer, np.ndarray) and indexer.size == 0
   2440         ):
-> 2441             return super().setitem(indexer, value)
   2442 
   2443         obj_vals = self.values.astype(object)

~/.virtualenvs/lr/lib/python3.7/site-packages/pandas/core/internals/blocks.py in setitem(self, indexer, value)
   1794             indexer = indexer[0]
   1795 
-> 1796         check_setitem_lengths(indexer, value, self.values)
   1797         self.values[indexer] = value
   1798         return self

~/.virtualenvs/lr/lib/python3.7/site-packages/pandas/core/indexers.py in check_setitem_lengths(indexer, value, values)
    112             ):
    113                 raise ValueError(
--> 114                     ""cannot set using a list-like indexer ""
    115                     ""with a different length than the value""
    116                 )

ValueError: cannot set using a list-like indexer with a different length than the value
```

</details>


#### Problem description

When mask is all False and the column is a localized datetime, it raises the error: `ValueError: cannot set using a list-like indexer with a different length than the value`.
This doesn't happens when the column is another type like naive datetime, str or int.

#### Expected Output

`df1.loc[mask, ""dt_tz""] = df2[""dt_tz""]` shouldn't raise an error, just do nothing.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-40-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.0.0
Cython           : 0.29.15
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.2.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.0.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.2.17
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : None
numba            : None


</details>
"
736587247,37641,BUG: RollingGroupby duplicates columns in index even with group_keys=False,venaturum,closed,2020-11-05T03:48:46Z,2020-11-09T18:56:37Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

arrays = [['val1', 'val1', 'val2'], ['val1', 'val1', 'val2']]
index = pd.MultiIndex.from_arrays(arrays, names=('idx1', 'idx2'))

s = pd.Series([1, 2, 3], index=index)
print(
    s.groupby([""idx1"", ""idx2""], group_keys=False).rolling(1).mean()
)

```

#### Problem description

the group_keys parameter in Seried.groupby() is broken since 1.1.0 when using .rolling()

It does not seem to be an issue if another function instead of *rolling* is used, eg 
```python
    s.groupby([""idx1"", ""idx2""], group_keys=False).mean()
```
In pandas=1.1.{0,1} the output is

    idx1 idx2  
    val1 val1  (val1, val1)   1.0
         val1  (val1, val1)   2.0
    val2 val2  (val2, val1)   3.0

In pandas=1.1.{2,3,4} the output is

    idx1 idx2  idx1 idx2     
    val1 val1  val1 val1   1.0
                    val1   2.0
    val2 val2  val2 val2   3.0

#### Expected Output (as per pandas 1.0.*)

    idx1 idx2       
    val1 val1   1.0
         val1   2.0
    val2 val2   3.0

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.6.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.17134
machine          : AMD64
processor        : Intel64 Family 6 Model 63 Stepping 2, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.1
numpy            : 1.16.1
pytz             : 2018.9
dateutil         : 2.8.0
pip              : 10.0.1
setuptools       : 39.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.3.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 7.2.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 2.6.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
462173323,27104,Partial indexing with list on MultiIndex with missing value includes them despite not being in the list,toobaz,closed,2019-06-28T19:15:56Z,2020-11-09T20:27:35Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: df = pd.DataFrame({'col':[1, 2, 3, 4, 5], 'ind1':['a','b','c','d',np.nan], 'ind2':[1,2,3,4,5] }).set_index(['ind1', 'ind2'])                                                                                                          

In [3]: df.loc['a']                                                                                                                                                                                                                           
Out[3]: 
      col
ind2     
1       1

In [4]: df.loc[['a']]                                                                                                                                                                                                                         
Out[4]: 
           col
ind1 ind2     
a    1       1
NaN  5       5


```
#### Problem description

``Out[3]`` is correct, ``Out[4]`` is wrong - the ``NaN`` line should not be there.

From #15107.

#### Expected Output

Same as ``Out[3]``.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 45ea26763da832189747ac9f86630fece84b4f18
python           : 3.7.3.candidate.1
python-bits      : 64
OS               : Linux
OS-release       : 4.9.0-9-amd64
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : it_IT.UTF-8
LOCALE           : it_IT.UTF-8

pandas           : 0.25.0.dev0+824.g45ea26763
numpy            : 1.16.4
pytz             : 2016.7
dateutil         : 2.8.0
pip              : 9.0.1
setuptools       : 41.0.1
Cython           : 0.29.2
pytest           : 3.0.6
hypothesis       : 3.6.1
sphinx           : 1.4.9
blosc            : None
feather          : None
xlsxwriter       : 0.9.6
lxml.etree       : 4.3.2
html5lib         : 0.999999999
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: 0.2.1
bs4              : 4.5.3
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.2
matplotlib       : 3.0.2
numexpr          : 2.6.9
openpyxl         : 2.3.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : 1.0.15
tables           : 3.4.4
xarray           : None
xlrd             : 1.0.0
xlwt             : 1.3.0
xlsxwriter       : 0.9.6


</details>
"
738568926,37706,BUG: Fix return of missing values when applying loc to single level of MultiIndex,phofl,closed,2020-11-08T22:31:11Z,2020-11-09T20:29:14Z,"- [x] closes #27104
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This confused ``-1`` representing missing values in input with the ``-1`` signaling which components to exclude from output."
738050040,37674,CLN: no need to call _wrap_results in nanvar,jbrockmendel,closed,2020-11-06T21:21:47Z,2020-11-09T21:02:33Z,
31996841,6929,PERF: Poor numerical stability of rolling_kurt and rolling_skew,jaimefrio,closed,2014-04-22T18:16:47Z,2020-11-09T21:20:05Z,"The numerical stability of the current implementation of `rolling_kurt` and `rolling_skew` is very poor:

```
In [14]: x = np.random.rand(10)

In [15]: pd.Series(x).rolling(4).skew()
Out[15]:
0         NaN
1         NaN
2         NaN
3   -0.971467
4    0.596391
5   -1.163135
6   -0.611469
7   -1.624713
8   -0.021832
9    0.754653
dtype: float64

In [16]: pd.Series(x + 5000).rolling(4).skew()
Out[16]:
0         NaN
1         NaN
2         NaN
3   -0.968964
4    0.608522
5   -1.171751
6   -0.619354
7   -1.628942
8   -0.033949
9    0.741304
dtype: float64

In [17]: pd.Series(x).rolling(4).kurt()
Out[17]:
0         NaN
1         NaN
2         NaN
3    2.030185
4   -2.647975
5    1.183004
6   -2.178587
7    2.658058
8   -4.258505
9    0.939264
dtype: float64

In [18]: pd.Series(x + 100).rolling(4).kurt()
Out[18]:
0         NaN
1         NaN
2         NaN
3    2.030230
4   -2.648801
5    1.182382
6   -2.178574
7    2.657934
8   -4.258813
9    0.938661
dtype: float64
```

This can be solved using an updating algorithm, similar to what is done in #6817 for `rolling_var`.
"
272476776,18185,Trying to reindex category Series with invalid fill_value raises error without message,toobaz,closed,2017-11-09T08:26:37Z,2020-11-09T21:24:31Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: pd.Series([1, 2, 3, 1], dtype='category').reindex([1,2,3,4,5], fill_value=-1)
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-2-dd2f847ae90c> in <module>()
----> 1 pd.Series([1, 2, 3, 1], dtype='category').reindex([1,2,3,4,5], fill_value=-1)

~/nobackup/repo/pandas/pandas/core/series.py in reindex(self, index, **kwargs)
   2638     @Appender(generic._shared_docs['reindex'] % _shared_doc_kwargs)
   2639     def reindex(self, index=None, **kwargs):
-> 2640         return super(Series, self).reindex(index=index, **kwargs)
   2641 
   2642     @Appender(generic._shared_docs['fillna'] % _shared_doc_kwargs)

~/nobackup/repo/pandas/pandas/core/generic.py in reindex(self, *args, **kwargs)
   3005         # perform the reindex on the axes
   3006         return self._reindex_axes(axes, level, limit, tolerance, method,
-> 3007                                   fill_value, copy).__finalize__(self)
   3008 
   3009     def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,

~/nobackup/repo/pandas/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy)
   3023             obj = obj._reindex_with_indexers({axis: [new_index, indexer]},
   3024                                              fill_value=fill_value,
-> 3025                                              copy=copy, allow_dups=False)
   3026 
   3027         return obj

~/nobackup/repo/pandas/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups)
   3126                                                 fill_value=fill_value,
   3127                                                 allow_dups=allow_dups,
-> 3128                                                 copy=copy)
   3129 
   3130         if copy and new_data is self._data:

~/nobackup/repo/pandas/pandas/core/internals.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy)
   4139         if axis == 0:
   4140             new_blocks = self._slice_take_blocks_ax0(indexer,
-> 4141                                                      fill_tuple=(fill_value,))
   4142         else:
   4143             new_blocks = [blk.take_nd(indexer, axis=axis, fill_tuple=(

~/nobackup/repo/pandas/pandas/core/internals.py in _slice_take_blocks_ax0(self, slice_or_indexer, fill_tuple)
   4178                 return [blk.take_nd(slobj, axis=0,
   4179                                     new_mgr_locs=slice(0, sllen),
-> 4180                                     fill_tuple=fill_tuple)]
   4181 
   4182         if sl_type in ('slice', 'mask'):

~/nobackup/repo/pandas/pandas/core/internals.py in take_nd(self, indexer, axis, new_mgr_locs, fill_tuple)
   2402         # but are passed the axis depending on the calling routing
   2403         # if its REALLY axis 0, then this will be a reindex and not a take
-> 2404         new_values = self.values.take_nd(indexer, fill_value=fill_value)
   2405 
   2406         # if we are a 1-dim object, then always place at 0

~/nobackup/repo/pandas/pandas/core/categorical.py in take_nd(self, indexer, allow_fill, fill_value)
   1712         # filling must always be None/nan here
   1713         # but is passed thru internally
-> 1714         assert isna(fill_value)
   1715 
   1716         codes = take_1d(self._codes, indexer, allow_fill=True, fill_value=-1)

AssertionError: 


```
#### Problem description

At least, the error can be improved (after all, nothing in the docs suggests ``fill_value`` must be a ``missing_value``, so the fact that this _actually works_ with ``fill_value=np.nan`` is an implementation detail), but maybe we could even actually support this? The effort of just adding one element to the categories and set new elements to it should be minimal.

#### Expected Output

Either the new elements set to -1, or at least a more meaningful error message.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.0-3-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: it_IT.UTF-8
LOCALE: it_IT.UTF-8

pandas: 0.22.0.dev0+84.g8dac63314
pytest: 3.0.6
pip: 9.0.1
setuptools: 36.6.0
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.19.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: 1.5.6
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: 1.2.0dev
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.0.0
openpyxl: None
xlrd: 1.0.0
xlwt: 1.1.2
xlsxwriter: 0.9.6
lxml: None
bs4: 4.5.3
html5lib: 0.999999999
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.2.1


</details>
"
739315136,37720,TST: Add test for missing fillvalue in categorical dtype,phofl,closed,2020-11-09T19:48:40Z,2020-11-09T21:36:10Z,"- [x] closes #18185
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

cc @jreback Could not find a test for something similar with reindex."
739326139,37721,CLN: clean categorical indexes tests,phofl,closed,2020-11-09T20:05:19Z,2020-11-09T22:16:17Z,Found this while searching for other tests. Deleted code fragement was probably for ix previously.
738376647,37697,DOC: capitalize Gregorian and fix word order,erictleung,closed,2020-11-08T05:28:39Z,2020-11-10T02:03:22Z,"Gregorian is a proper noun
(https://en.wikipedia.org/wiki/Proleptic_Gregorian_calendar). Also,
""proleptic"" is modifying the compound noun ""Gregorian epoch"" rather than
""Gregorian"" modifying a ""proleptic epoch"".

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
408390005,25236,BUG: .loc indexing into MultiIndex with tuple with non-found keys gives wrong error,tikael1011,closed,2019-02-09T02:47:34Z,2020-11-10T03:20:06Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here

idx = ['str1', 'str2', 'str3']
df_full.set_index(idx, drop=False, inplace=True)
for i, rc in df.iterrows():
    df.at[i, 'prev_state'] = df_full.loc[tuple(rc[e] for e in idx)]['state']

## df is a dataframe with columns in idx and prev_state
```
#### Problem description

The above piece of code works in 0.23.4, yet after I upgraded to 0.24.1, it throws 

``` python
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py"", line 1494, in __getitem__
    return self._getitem_tuple(key)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py"", line 873, in _getitem_tuple
    self._has_valid_tuple(tup)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py"", line 221, in _has_valid_tuple
    raise IndexingError('Too many indexers')
pandas.core.indexing.IndexingError: Too many indexers
```

I did compare that part of source code in both 0.23.4 and 0.24.1, and they look like the same.(maybe I am wrong) Please help.

#### Output of ``pd.show_versions() for 0.23.4``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.4.final.0
python-bits: 64
OS: Darwin
OS-release: 17.5.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: None
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: None
pip: 9.0.1
setuptools: 40.6.2
Cython: None
numpy: 1.15.4
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.7.4
pytz: 2018.6
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: 1.2.16
pymysql: None
psycopg2: 2.7.6.1 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
None

</details>

#### Output of ``pd.show_versions() for 0.24.1``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.4.final.0
python-bits: 64
OS: Darwin
OS-release: 17.5.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: None
LOCALE: en_US.UTF-8

pandas: 0.24.1
pytest: None
pip: 9.0.1
setuptools: 40.6.2
Cython: None
numpy: 1.15.4
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.7.4
pytz: 2018.6
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: 1.2.16
pymysql: None
psycopg2: 2.7.6.1 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
None

</details>"
739484940,37729,TST/REF: collect indexing tests by method,jbrockmendel,closed,2020-11-10T01:08:07Z,2020-11-10T03:33:24Z,
406095603,25113,Fix validation error type `SS02` and check in CI,kynan,closed,2019-02-03T14:48:55Z,2020-11-10T03:38:04Z,"#### Problem description
In order to have a continuous check by the CI on validation error `SS02` (Summary does not start with a capital letter), fixing them in the code base enables the addition to the CI for automated testing in the future.

`SS02` has just 61 occurrences (see [overview of the errors](https://gist.github.com/198cfa552c727d71bd7360fda66fdbb4)):

method | file | file_line | github_link
-- | -- | -- | --
pandas.tseries.offsets.DateOffset.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.BusinessDay.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.BusinessHour.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.CustomBusinessDay.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.CustomBusinessHour.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.MonthOffset.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.MonthEnd.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.MonthBegin.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.BusinessMonthEnd.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.BusinessMonthBegin.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.CustomBusinessMonthEnd.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.CustomBusinessMonthBegin.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.SemiMonthOffset.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.SemiMonthEnd.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.SemiMonthBegin.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.Week.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.WeekOfMonth.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.LastWeekOfMonth.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.QuarterOffset.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.BQuarterEnd.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.BQuarterBegin.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.QuarterEnd.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.QuarterBegin.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.YearOffset.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.BYearEnd.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.BYearBegin.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.YearEnd.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.YearBegin.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.FY5253.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.FY5253Quarter.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
... | ... | ... | ...
pandas.tseries.offsets.Tick.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.Day.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.Hour.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.Minute.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.Second.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.Milli.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.Micro.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.Nano.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.BDay.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.BMonthEnd.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.BMonthBegin.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.CBMonthEnd.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.CBMonthBegin.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.tseries.offsets.CDay.normalize | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.Timestamp.ceil | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.Timestamp.combine | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.Timestamp.floor | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.Timestamp.fromordinal | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.Timestamp.fromtimestamp | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.Timestamp.replace | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.Timestamp.strftime | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.Timestamp.strptime | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.Timedelta.view | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.Timedelta.ceil | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.Timedelta.floor | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.Timedelta.to_pytimedelta | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.HDFStore.groups | pandas/io/pytables.py | 1093.0 | https://github.com/pandas-dev/pandas/blob/master/pandas/io/pytables.py#L1093
pandas.Panel.items | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.Panel.major_axis | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone
pandas.Panel.minor_axis | None | NaN | https://github.com/pandas-dev/pandas/blob/master/None#LNone

Todo:
* [ ]  get rid of the errors in the code base
* [ ]  update the `code_check.sh` script to take into account the `SS02` type of errors"
732502398,37490,BUG: File saved with to_stata and version=119 cannot be opened with STATA 15.1 SE,raffaem,closed,2020-10-29T17:13:45Z,2020-11-10T03:38:45Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
df = pd.DataFrame({""A"":[1]})
df.to_stata(""a.dta"", version=119)

```

#### Problem description

I have Stata/SE 15.1.

I run the above code, then try to open `a.dta` with my version of Stata.

But Stata doesn't open the file and instead gives the following error:

> dataset too large
>     This .dta file format was created by Stata/MP and has more variables than your Stata can handle.

Of course the file is not too large, since it contains only 1 observation and 1 variable.
It looks like pandas is saving the file in the ""MP"" version. 
I propose to save it in the SE version, or at least specify which sub-version of Stata the file is saved in in the documentation.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : Italian_Italy.1252

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.0.post20201006
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.3
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.20
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.51.2

</details>
"
739367493,37724,"REF: remove Categorical._validate_listlike, unbox_listlike",jbrockmendel,closed,2020-11-09T21:14:28Z,2020-11-10T03:43:13Z,Getting closer to having these standardized.
738443838,37700,DOC: Add caveat to to_stata with version=119,hongshaoyang,closed,2020-11-08T11:20:49Z,2020-11-10T04:08:07Z,"- [x] closes #37490
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
739069714,37718,TST: improve readability using dedent in test_to_string.py,ivanovmg,closed,2020-11-09T14:21:22Z,2020-11-10T04:43:51Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Improve readability of expected outputs using dedent in ``pandas/tests/io/formats/test_to_string.py``"
739058481,37717,TST: move and reformat latex_na_rep test,ivanovmg,closed,2020-11-09T14:07:30Z,2020-11-10T04:44:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Move test to a more suitable location (``class TestToLatexFormatters``)
and reformat using ``_dedent`` and raw string for better readability."
738982486,37714,"CLN: cleanup, deduplicate plotting/test_frame",ivanovmg,closed,2020-11-09T12:23:17Z,2020-11-10T04:44:38Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

1. Make more reasonable var names
2. Get rid of one test in favor of parametrization in another one"
725522565,37274,TST: fix warnings on multiple subplots,ivanovmg,closed,2020-10-20T12:16:02Z,2020-11-10T04:45:40Z,"- [ ] xref #37178 partially
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This PR suggests a (partial) fix for such warnings, which take place when running test suite on ``pandas/tests/plotting``.
```
UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared
```

The problem is the following.
The function ``_check_plot_works`` creates subplot(211) if axes are not provided.
This is fine, when the plot to be created is a single-axes plot.
However, if the plot itself is to be plotted on multiple axes (for instance, df.hist would plot each column on a dedicated subplot, by default), then the warnings mentioned are emitted.

Two ways to handle it:
- Find cases where plot is a multiaxes-plot and catch warnings there. This is fine, but in some parametrized tests the plots are either single-axes or multiple-axes. It makes it difficult to figure out without splitting the tests.
- Change function ``_check_plot_works``: do not create subplot(211).

I created ``_check_single_plot_works`` which does not create subplots and used it in the problematic cases.
I guess, that the name is not the best (I would prefer ``_check_plot_works``), but I encourage those concerned to have a discussion here.

Please see comments in ``_check_plot_works`` (I will remove them once we come to the conclusion on the topic)."
739304926,37719,TST: Add test for KeyError with MultiIndex,phofl,closed,2020-11-09T19:32:07Z,2020-11-10T07:33:45Z,"- [x] closes #25236
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

cc @jreback Added a test"
727979161,37358,DOC mention pre-commit as a way to run (some) code checks,MarcoGorelli,closed,2020-10-23T07:18:58Z,2020-11-10T09:42:07Z,"xref this comment https://github.com/pandas-dev/pandas/pull/37240#discussion_r510518893

> as a followup we might want to update the contributing docs to put a pointer to these types of checks here (e.g. rather than just mentioning code_checks.sh) esp as we are using these a lot more now"
738455911,37701,DOC: Highlight pre-commit section,hongshaoyang,closed,2020-11-08T12:36:07Z,2020-11-10T09:43:17Z,"- [x] closes #37358 
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
739814049,37740,[WIP] ci testing of fixes for numpy types,simonjayhawkins,closed,2020-11-10T11:08:41Z,2020-11-10T13:21:53Z,xref #36092
737528319,37661,BUG: RollingGroupby when groupby key is in the index,mroeschke,closed,2020-11-06T07:07:49Z,2020-11-10T13:40:23Z,"- [x] closes #37641
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
739917640,37741,Backport PR #37661 on branch 1.1.x: BUG: RollingGroupby when groupby key is in the index,simonjayhawkins,closed,2020-11-10T13:39:47Z,2020-11-10T14:52:34Z,Backport PR #37661
739793606,37738,TYP: fix mypy ignored error in pandas/io/formats/latex.py,ivanovmg,closed,2020-11-10T10:39:33Z,2020-11-10T15:22:54Z,"- [ ] xref #37715 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Handle mypy ignored error in ``pandas/io/formats/latex.py``."
740036745,37743,BUG: rolling groupby does not respect min_periods when center=True,wfvining,closed,2020-11-10T16:02:40Z,2020-11-10T19:18:01Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.



#### Code Sample, a copy-pastable example

```python
s = pd.Series(1, index=list(range(0, 5)) * 4)
s.groupby(s.index).rolling(window=3, center=True, min_periods=1).sum()
```

Outputs:
```
0  0    2.0
   0    3.0
   0    3.0
   0    1.0
1  1    2.0
   1    3.0
   1    3.0
   1    1.0
2  2    2.0
   2    3.0
   2    3.0
   2    1.0
3  3    2.0
   3    3.0
   3    3.0
   3    1.0
4  4    2.0
   4    3.0
   4    3.0
   4    NaN
dtype: float64
```

#### Problem description

For the given input `s`, where every element is 1, the snippet above should return the same as counting the number of values in each window:
```python
s.groupby(s.index).rolling(window=3, center=True, min_periods=1).count()
```

The output above is inconsistent with versions of pandas prior to 1.1 and is inconsistent in the semantics of `min_periods`; the first element of each group has the same number of periods as the last, but its value is computed correctly. The last element of each group is also computed incorrectly (should be 2). Finally, the last element of the final group (4) should not be NA, but should again be 2.

Some similar issues have been reported since the changes to `.groupby().rolling()` made in pandas 1.1.0.
- #36040
- #37141 might cover the same bug, but it is not immediately clear (I would also very much prefer to have this fixed in the 1.1.x line rather than wait for 1.2 and need to exclude 1.1.x in my dependencies).

#### Expected Output

```
0  0    2.0
   0    3.0
   0    3.0
   0    2.0
1  1    2.0
   1    3.0
   1    3.0
   1    2.0
2  2    2.0
   2    3.0
   2    3.0
   2    2.0
3  3    2.0
   3    3.0
   3    3.0
   3    2.0
4  4    2.0
   4    3.0
   4    3.0
   4    2.0
dtype: float64
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.1.4
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200814
Cython           : None
pytest           : 6.0.1
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
739360248,37723,REF: collect boilerplate in _datetimelike_compat,jbrockmendel,closed,2020-11-09T21:02:07Z,2020-11-10T19:30:14Z,"
"
739607195,37734,TST: use default_axes=True where multiple plots,ivanovmg,closed,2020-11-10T06:00:23Z,2020-11-10T22:56:37Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Use new kwarg ``default_axes`` in ``_check_plot_works`` (#37274), which allows one to eliminate catching warnings related to multiple subplots created.
There warnings are not related to the plotting function, but rather were caused by the ``_check_plot_works`` itself.
"
738359524,37694,TYP: @final for NDFrame methods,jbrockmendel,closed,2020-11-08T02:52:02Z,2020-11-10T23:28:45Z,attempt 2
734069777,37566,BUG: nunique not ignoring both None and np.nan,Tomer-Eliahu,closed,2020-11-01T21:47:38Z,2020-11-10T23:41:47Z,"- [x] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np
pd.Series(['yes','yes', None, np.nan]).nunique(dropna = True)

```

#### Problem description
The output was 2.
It seems that nunique can only ignore None or np.nan but not both together. This seems odd to me and I think it 
should either be changed or the docs should reflect this.

#### Expected Output
I expected to get 1 as the result but I got 2.
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.49+
Version          : #1 SMP Sun Oct 18 19:43:35 PDT 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.18.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 46.1.3.post20200325
Cython           : 0.29.21
pytest           : 5.4.1
hypothesis       : 5.10.0
sphinx           : 3.0.2
blosc            : None
feather          : 0.4.1
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: 0.9.0
bs4              : 4.9.0
bottleneck       : 1.3.2
fsspec           : 0.8.4
fastparquet      : None
gcsfs            : 0.6.1
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pyxlsb           : None
s3fs             : 0.5.1
scipy            : 1.4.1
sqlalchemy       : 1.3.16
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0

</details>
"
739528591,37731,REF: use extract_array in DataFrame.combine_first,arw2019,closed,2020-11-10T03:03:16Z,2020-11-11T00:58:38Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
728833894,37384,replace Appender decorator with doc,smartvinnetou,closed,2020-10-24T17:15:24Z,2020-11-11T01:26:46Z,"- [x] ref #31942
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] replaces Appender with doc for NDFrame.compare()
"
739405952,37725,ENH: Disallow subsetting columns in groupby with axis=1,rhshadrach,closed,2020-11-09T22:21:53Z,2020-11-11T02:36:21Z,"E.g.

```
df = pd.DataFrame({'A': [1], 'B': [2], 'C': [3]})
g = df.groupby([0, 0, 1], axis=1)
g[['A', 'B']].sum()
```

Other than using `.size()`, I think just about any op will raise. This can be explicitly raised with a more helpful error message when a user subsets the columns."
14112199,3547,to_html vertically expands multiindex cells if there are empty strings,ghost,closed,2013-05-08T17:19:50Z,2020-11-11T10:18:21Z,"Notice in the html below the second index column's 'a' field is given as `<th rowspan=""2"" valign=""top"">a</th>`, expanding over what should be an empty cell.

I actually found this using a pivot table, `margins=True`, as this creates a row keyed like `('All', '', '')`.

```
print df = pd.DataFrame({'c1': ['a', 'b'], 'c2': ['a', ''], 'data': [1, 2]}).set_index(['c1', 'c2'])
       data
c1 c2      
a  a      1
b         2

print df.to_html()
<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th></th>
      <th>data</th>
    </tr>
    <tr>
      <th>c1</th>
      <th>c2</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>a</th>
      <th rowspan=""2"" valign=""top"">a</th>
      <td> 1</td>
    </tr>
    <tr>
      <th>b</th>
      <td> 2</td>
    </tr>
  </tbody>
</table>
```
"
740606011,37756,"Revert ""DOC: test organization""",jorisvandenbossche,closed,2020-11-11T09:20:56Z,2020-11-11T11:49:41Z,"Reverts pandas-dev/pandas#37637

(CI is failing because of this)"
736498840,37637,DOC: test organization,jbrockmendel,closed,2020-11-04T23:49:27Z,2020-11-11T15:35:04Z,"Documenting the MO behind the test organization we've been doing.

1) I expect this is not valid rst ATM
2) The style does not match the rest of the docs, so probably needs a re-write.

"
735482548,37607,BUG: nunique not ignoring both None and np.nan,GYHHAHA,closed,2020-11-03T16:52:21Z,2020-11-11T17:03:30Z,"- [x] closes #37566
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
740109665,37745,TYP: follow-up to #37723,jbrockmendel,closed,2020-11-10T17:37:34Z,2020-11-11T17:12:46Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
723320331,37160,API: Index.__cmp__(Series) return NotImplemented,jbrockmendel,closed,2020-10-16T15:27:06Z,2020-11-11T19:06:09Z,"- [x] closes #36759
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Gets rid of an unnecessary special case.

Could do this as a deprecation first.
"
739636679,37736,BUG: read_html - file path cannot be pathlib.Path type,inspurwusixuan,closed,2020-11-10T06:47:19Z,2020-11-11T19:20:38Z,"Fix `read_html` TypeError when parsing `pathlib.Path` object.
"
738493471,37703,remove unnecessary use of Appender,smartvinnetou,closed,2020-11-08T15:44:52Z,2020-11-11T20:31:02Z,"- [x] ref #31942
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] removes the last use of Appender() in NDFrame
"
723571384,37178,TST: suppress matplotlib warnings (or determine if they represent a bug),jbrockmendel,closed,2020-10-16T22:44:52Z,2020-11-11T20:44:18Z,"There are a bunch of these in the test logs:

```
pandas/tests/plotting/test_hist_method.py::TestDataFramePlots::test_hist_column_order_unchanged[None-expected0]
pandas/tests/plotting/test_hist_method.py::TestDataFramePlots::test_hist_column_order_unchanged[column1-expected1]
pandas/tests/plotting/test_hist_method.py::TestDataFramePlots::test_hist_with_legend[None-None]
   pandas/tests/plotting/common.py:556: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared
    ret = f(**kwargs)
```

Do these represent problems we need to do something about?  cc @tacaswell 

If not, we should suppress them.  My attempts to do so with `pytestmark = pytest.mark.filterwarnings` have failed, could use another pair of eyeballs."
732846703,37512,BUG: Series.replace does not preserve dtype of original Series,arw2019,closed,2020-10-30T03:11:32Z,2020-11-11T21:06:53Z,"- [x] closes #33484
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Reheating #33622 (stale)"
741063111,37767,CI: remove unused import,charlesdong1991,closed,2020-11-11T20:53:26Z,2020-11-11T22:20:51Z,xref #37703
741179781,37777,BUG: Groupby head/tail with axis=1 fails,rhshadrach,closed,2020-11-12T00:45:27Z,2020-11-12T00:51:51Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This potentially closes #9772. The other issue there involves the perhaps unexpected fact that using `axis=1` with `groupby(...).apply` does _not_ transpose the groups that the function is going to be applied to. I've expressed my thoughts on why we should not do so in https://github.com/pandas-dev/pandas/issues/9772#issuecomment-723498819."
740412233,37752,TST/REF: fixturize test_drop_duplicates,jbrockmendel,closed,2020-11-11T02:47:38Z,2020-11-12T02:19:02Z,"The datetimelike de-duplication is unrelated, just didnt seem to merit its own PR."
667543537,35445,WIP: ENH: Add numba engine to groupby apply,mroeschke,closed,2020-07-29T04:42:05Z,2020-11-12T05:15:20Z,"- [x] closes #31845
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Some notes:

- The passed function must be a reduction op
- The numba engine will drop the grouping column by default
- Can only operate on numeric data and will return float64

Preliminary default timing:

```
(pandas-dev) matthewroeschke:pandas-mroeschke matthewroeschke$ ipython
Python 3.8.3 | packaged by conda-forge | (default, Jun  1 2020, 17:21:09)
Type 'copyright', 'credits' or 'license' for more information
IPython 7.16.1 -- An enhanced Interactive Python. Type '?' for help.

In [1]: df_g = pd.DataFrame({'a': range(10**4), 'b': range(10**4), 'c': range(10**4)})

In [2]: def f(x):
   ...:     return np.sum(x) + 1
   ...:

In [3]: df_g.groupby('a').apply(f)
Out[3]:
          a      b      c
a
0         1      1      1
1         2      2      2
2         3      3      3
3         4      4      4
4         5      5      5
...     ...    ...    ...
9995   9996   9996   9996
9996   9997   9997   9997
9997   9998   9998   9998
9998   9999   9999   9999
9999  10000  10000  10000

[10000 rows x 3 columns]

In [4]: %timeit df_g.groupby('a').apply(f)
3.07 s ± 57.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [5]: df_g.groupby('a').apply(f, engine='numba', engine_kwargs={'parallel': True})
Out[5]:
            0        1
0         1.0      1.0
1         2.0      2.0
2         3.0      3.0
3         4.0      4.0
4         5.0      5.0
...       ...      ...
9995   9996.0   9996.0
9996   9997.0   9997.0
9997   9998.0   9998.0
9998   9999.0   9999.0
9999  10000.0  10000.0

[10000 rows x 2 columns]

In [6]: %timeit df_g.groupby('a').apply(f, engine='numba', engine_kwargs={'parallel': True})

510 ms ± 3.46 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```"
683943579,35848,BUG: Index.get_slice_bounds does not accept datetime.date or tz naive datetime.datetimes,mroeschke,closed,2020-08-22T06:47:21Z,2020-11-12T05:15:24Z,"- [x] closes #35690
- [x] closes #34077
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This method appeared under tested so added some additional tests for numeric and object `Index`
"
695486154,36208,BUG: GroupbyRolling with an empty frame,mroeschke,closed,2020-09-08T02:01:13Z,2020-11-12T05:15:26Z,"- [x] closes #36197
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
739049252,37716,df.melt() overwriting column named 'variable',BenScarrott,open,2020-11-09T13:55:23Z,2020-11-12T10:26:53Z,"when melting df using a column named 'variable' as an id_var.
resultant frame has original 'variable' column overwritten with the newly melted 'variable' column.
would suggest checking for presence of 'variable' column and appending '_i' to newly created columns.

[example_melt_overwrite.zip](https://github.com/pandas-dev/pandas/files/5510886/example_melt_overwrite.zip)

foo = pd.read_pickle('example_melt_overwrite.pkl')
foo.melt(id_vars=['DateTime','variable'])
"
735481945,37606,BUG: set index of DataFrame.apply(f) when f returns dict (#37544),ma3da,closed,2020-11-03T16:51:28Z,2020-11-12T16:12:14Z,"- [x] closes #37544
- [x] tests (added) updated / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
648725356,35080,BUG: Strange behavior at testing test_ts_plot_with_tz,StefRe,closed,2020-07-01T06:56:19Z,2020-11-12T16:23:38Z,"I came across a rather strange and annoying behavior when running the plotting tests that leaves me completely clueless.   

`TestTSPlot.test_ts_plot_with_tz['UTC']` fails when running the whole test suite but passes when run in isolation. In particular it fails when run **after all** `TestRegistration` tests and passes when run **on its own or after any** of the six test functions in `TestRegistration`:

### FAILURE:
```
(pandas-dev) C:\Daten\Python\pandas>pytest pandas\tests\plotting\ -k ""TestRegistration | test_ts_plot_with_tz""
====================================================================== test session starts ======================================================================
platform win32 -- Python 3.8.2, pytest-5.4.3, py-1.9.0, pluggy-0.13.1
rootdir: C:\Daten\Python\pandas, inifile: setup.cfg
plugins: hypothesis-5.19.0, asyncio-0.14.0, cov-2.10.0, forked-1.2.0, xdist-1.32.0
collected 479 items / 460 deselected / 19 selected

pandas\tests\plotting\test_converter.py ......                                                                                                             [ 31%]
pandas\tests\plotting\test_datetimelike.py F............                                                                                                   [100%]

=========================================================================== FAILURES ============================================================================
____________________________________________________________ TestTSPlot.test_ts_plot_with_tz['UTC'] _____________________________________________________________

self = <pandas.tests.plotting.test_datetimelike.TestTSPlot object at 0x000002965C714E20>, tz_aware_fixture = 'UTC'

    @pytest.mark.slow
    def test_ts_plot_with_tz(self, tz_aware_fixture):
        # GH2877, GH17173, GH31205, GH31580
        tz = tz_aware_fixture
        index = date_range(""1/1/2011"", periods=2, freq=""H"", tz=tz)
        ts = Series([188.5, 328.25], index=index)
        with tm.assert_produces_warning(None):
            _check_plot_works(ts.plot)
            ax = ts.plot()
            xdata = list(ax.get_lines())[0].get_xdata()
            # Check first and last points' labels are correct
>           assert (xdata[0].hour, xdata[0].minute) == (0, 0)
E           AttributeError: 'numpy.datetime64' object has no attribute 'hour'

pandas\tests\plotting\test_datetimelike.py:57: AttributeError
==================================================================== short test summary info ====================================================================
FAILED pandas/tests/plotting/test_datetimelike.py::TestTSPlot::test_ts_plot_with_tz['UTC'] - AttributeError: 'numpy.datetime64' object has no attribute 'hour'
========================================================= 1 failed, 18 passed, 460 deselected in 2.98s ==========================================================
```
### SUCCESS:
```
(pandas-dev) C:\Daten\Python\pandas>pytest pandas\tests\plotting\ -k test_ts_plot_with_tz
====================================================================== test session starts ======================================================================
platform win32 -- Python 3.8.2, pytest-5.4.3, py-1.9.0, pluggy-0.13.1
rootdir: C:\Daten\Python\pandas, inifile: setup.cfg
plugins: hypothesis-5.19.0, asyncio-0.14.0, cov-2.10.0, forked-1.2.0, xdist-1.32.0
collected 479 items / 466 deselected / 13 selected

pandas\tests\plotting\test_datetimelike.py .............                                                                                                   [100%]

============================================================== 13 passed, 466 deselected in 2.43s ===============================================================
```
As I said it passes when run after any of the individual functions in  `TestRegistration`, i.e. with `-k ""test_register_by_default | test_ts_plot_with_tz""`, `""test_registering_no_warning | test_ts_plot_with_tz""` etc.    

Tests are run in an environment set up as described [here](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#creating-a-python-environment-pip). Pytest and plugin versions see above. Pandas fresh snapshot as of yesterday `1.1.0.dev0+1977.g1706d830d`.   

I'm pretty sure it's a question of my setup, so sorry for bothering you with it, but maybe someone has any idea or pointers of where to look for the reason of this behavior."
740980497,37762,BUG: clean the figure windows created by tests,onshek,closed,2020-11-11T18:30:43Z,2020-11-12T16:23:53Z,"- [x] closes #35080
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

fixed: FAILED pandas/tests/plotting/test_datetimelike.py::TestTSPlot::test_ts_plot_with_tz['UTC']
reference:
- https://github.com/pandas-dev/pandas/issues/35080#issuecomment-652988132 @StefRe as co-author (have already marked as  the co-author in my forked repo, however, I'm not sure whether as well as right here)
- https://stackoverflow.com/a/8228808/7069618
- https://stackoverflow.com/a/33343289/7069618"
741876833,37790,BUG: freq inference for tz-aware DatetimeIndex,jbrockmendel,closed,2020-11-12T19:41:53Z,2020-11-12T22:25:27Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
741086944,37769,CLN: removing imports from generic.py,Mikhaylov-yv,closed,2020-11-11T21:36:23Z,2020-11-12T22:57:44Z,"
"
90510737,10420,Pandas Interpolate Does not Handle Empty Values at Front of Series,cancan101,closed,2015-06-23T22:55:14Z,2020-11-12T23:44:11Z,"[This code](https://github.com/pydata/pandas/blob/7d6fb510c1dfdbe7342f32f05ca5fd69b7854081/pandas/core/common.py#L1646) means the interpolator won't handle extrapolating to the front of the Series even though the underlying implementations may have no problem with the extrapolation. 

See for example [`UnivariateSpline`](http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.interpolate.UnivariateSpline.html) whose default behavior is extrapolation.

Interpolation works fine at the end of the Series.

For example:

``` python
s = pd.Series([1, 2, 3, 4, np.nan, 6, np.nan])
s.interpolate(method='spline', order=1)
0    1
1    2
2    3
3    4
4    5
5    6
6    7
dtype: float64
```

but:

``` python
s = pd.Series([np.nan, 2, 3, 4, np.nan, 6, 7])
s.interpolate(method='spline', order=1)
0   NaN
1     2
2     3
3     4
4     5
5     6
6     7
dtype: float64
```
"
638367645,34769,TST: split up tests/plotting/test_frame.py into subdir & modules,jreback,closed,2020-06-14T14:54:47Z,2020-11-13T02:51:08Z,"we should split up this test file into a separate subdir and multiple test modules for better groking. first PR should be a straight move, then can split (or all in one is ok)"
741879630,37791,CLN: remove unreachable in interpolate_with_fill,jbrockmendel,closed,2020-11-12T19:46:33Z,2020-11-13T04:28:23Z,
741849341,37789,CLN: remove no-longer-reachable Block.replace code,jbrockmendel,closed,2020-11-12T19:00:19Z,2020-11-13T04:29:31Z,"We recently changed NDFrame.replace such that Block.replace never gets a list for `to_replace`, so this chunk of code is never reachable."
741735434,37788,REF: implement Block._putmask_simple for non-casting putmask,jbrockmendel,closed,2020-11-12T16:19:02Z,2020-11-13T04:30:26Z,Much easier to reason about than Block.putmask.
726024770,37295,"BUG: Regression adding Timedelta to DatetimeIndex, get ValueError: Inferred frequency None from passed values does not conform to passed frequency",nathanielatom,closed,2020-10-21T00:01:32Z,2020-11-13T04:40:04Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
pd.date_range(start=pd.Timestamp('2019-03-26 00:00:00-0400', tz='Canada/Eastern'), 
              end=pd.Timestamp('2020-10-17 00:00:00-0400', tz='Canada/Eastern'), 
              freq='D') + pd.Timedelta('1D')
```

#### Traceback

<details>

```python-traceback
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/.miniconda/lib/python3.7/site-packages/pandas/core/arrays/datetimelike.py in _validate_frequency(cls, index, freq, **kwargs)
   1156             if not np.array_equal(index.asi8, on_freq.asi8):
-> 1157                 raise ValueError
   1158         except ValueError as e:

ValueError: 

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
<ipython-input-2-d8f454fa3ee3> in <module>
----> 1 pd.date_range(start=pd.Timestamp('2019-03-26 00:00:00-0400', tz='Canada/Eastern'), end=pd.Timestamp('2020-10-17 00:00:00-0400', tz='Canada/Eastern'), freq='D') + pd.Timedelta('1D')

~/.miniconda/lib/python3.7/site-packages/pandas/core/indexes/extension.py in method(self, other)
    145             return NotImplemented
    146         meth = getattr(self._data, opname)
--> 147         result = meth(_maybe_unwrap_index(other))
    148         return _wrap_arithmetic_op(self, other, result)
    149 

~/.miniconda/lib/python3.7/site-packages/pandas/core/ops/common.py in new_method(self, other)
     63         other = item_from_zerodim(other)
     64 
---> 65         return method(self, other)
     66 
     67     return new_method

~/.miniconda/lib/python3.7/site-packages/pandas/core/arrays/datetimelike.py in __add__(self, other)
   1399             result = self._add_nat()
   1400         elif isinstance(other, (Tick, timedelta, np.timedelta64)):
-> 1401             result = self._add_timedeltalike_scalar(other)
   1402         elif isinstance(other, BaseOffset):
   1403             # specifically _not_ a Tick

~/.miniconda/lib/python3.7/site-packages/pandas/core/arrays/datetimelike.py in _add_timedeltalike_scalar(self, other)
   1254             new_freq = self.freq
   1255 
-> 1256         return type(self)(new_values, dtype=self.dtype, freq=new_freq)
   1257 
   1258     def _add_timedelta_arraylike(self, other):

~/.miniconda/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py in __init__(self, values, dtype, freq, copy)
    282 
    283         if inferred_freq is None and freq is not None:
--> 284             type(self)._validate_frequency(self, freq)
    285 
    286     @classmethod

~/.miniconda/lib/python3.7/site-packages/pandas/core/arrays/datetimelike.py in _validate_frequency(cls, index, freq, **kwargs)
   1169                 f""Inferred frequency {inferred} from passed values ""
   1170                 f""does not conform to passed frequency {freq.freqstr}""
-> 1171             ) from e
   1172 
   1173     # monotonicity/uniqueness properties are called via frequencies.infer_freq,

ValueError: Inferred frequency None from passed values does not conform to passed frequency D
```

</details>

#### Problem description

Unexpected ValueError when adding a Timedelta to a timezone-aware DatetimeIndex. It appears using timezone-aware Timestamp objects is necessary to reproduce - simple date strings won't.

#### Expected Output

The output from the same code sample in `v0.25.3` is:

```python
DatetimeIndex(['2019-03-27 00:00:00-04:00', '2019-03-28 00:00:00-04:00',
               '2019-03-29 00:00:00-04:00', '2019-03-30 00:00:00-04:00',
               '2019-03-31 00:00:00-04:00', '2019-04-01 00:00:00-04:00',
               '2019-04-02 00:00:00-04:00', '2019-04-03 00:00:00-04:00',
               '2019-04-04 00:00:00-04:00', '2019-04-05 00:00:00-04:00',
               ...
               '2020-10-09 00:00:00-04:00', '2020-10-10 00:00:00-04:00',
               '2020-10-11 00:00:00-04:00', '2020-10-12 00:00:00-04:00',
               '2020-10-13 00:00:00-04:00', '2020-10-14 00:00:00-04:00',
               '2020-10-15 00:00:00-04:00', '2020-10-16 00:00:00-04:00',
               '2020-10-17 00:00:00-04:00', '2020-10-18 00:00:00-04:00'],
              dtype='datetime64[ns, Canada/Eastern]', length=572, freq=None)
```

don't know about later versions, before `v1.1.3`.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 16.7.0
Version          : Darwin Kernel Version 16.7.0: Thu Dec 20 21:53:35 PST 2018; root:xnu-3789.73.31~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_CA.UTF-8
LOCALE           : en_CA.UTF-8

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.0
Cython           : 0.29.20
pytest           : 6.1.1
hypothesis       : None
sphinx           : 1.8.5
blosc            : 1.9.2
feather          : None
xlsxwriter       : 1.2.1
lxml.etree       : 4.5.2
html5lib         : 0.9999999
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: 0.9.0
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 2.6.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.11
tables           : 3.6.1
tabulate         : None
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.51.2

</details>
"
65688699,9772,Groupby built by columns : cannot use .head() or .apply(),JonasAbernot,closed,2015-04-01T13:50:32Z,2020-11-13T04:43:10Z,"``` python
import numpy as np
import pandas as pd

df = pd.DataFrame({i:pd.Series(np.random.normal(size=10),
                                index=range(10)) for i in range(11)})

df_g = df.groupby(['a']*6+['b']*5, axis=1)
```

This, if I well understood, should build a groupby object grouping columns, and so give the possibility to later aggregate them. And indeed :

``` python
df_g.sum()
```

works well. But

``` python
df_g.head()
```

Throws an error:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/jonas/Code/pandas/pandas/core/groupby.py"", line 986, in head
    in_head = self._cumcount_array() < n
  File ""/home/jonas/Code/pandas/pandas/core/groupby.py"", line 1044, in _cumcount_array
    cumcounts[indices] = values
IndexError: index 10 is out of bounds for axis 1 with size 10
```

and

``` python
df_g.apply(lambda x : x.sum())
```

from which I expected the same result as the first example, gives this table :

```
           a         b
0  -0.381070       NaN
1  -1.214075       NaN
2  -1.496252       NaN
3   3.392565       NaN
4  -0.782376       NaN
5   1.306043       NaN
6        NaN -1.772334
7        NaN  4.125280
8        NaN  1.992329
9        NaN  4.283854
10       NaN -4.791092
```

I didn't really get what's happening, I don't exclude a misunderstanding or an error from myself.

```
pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.8.final.0
python-bits: 64
OS: Linux
OS-release: 3.13.0-46-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: fr_FR.UTF-8

pandas: 0.16.0-28-gcb8c130
nose: 1.3.4
Cython: 0.20.2
numpy: 1.9.2
scipy: 0.14.0
statsmodels: None
IPython: 3.0.0-dev
sphinx: 1.2.2
patsy: None
dateutil: 2.4.1
pytz: 2015.2
bottleneck: None
tables: 3.1.1
numexpr: 2.4
matplotlib: 1.4.3
openpyxl: 1.7.0
xlrd: 0.9.2
xlwt: 0.7.5
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.999
httplib2: None
apiclient: None
sqlalchemy: 0.9.7
pymysql: None
psycopg2: 2.5.3 (dt dec mx pq3 ext)
```
"
741155131,37775,CLN: _get_daily_rule,jbrockmendel,closed,2020-11-11T23:40:21Z,2020-11-13T04:49:53Z,Broken off from branch addressing the infer_freq bug mentioned on today's dev call.
741128526,37773,"TST/REF: collect Index setop, indexing tests",jbrockmendel,closed,2020-11-11T23:00:11Z,2020-11-13T04:53:50Z,
741250377,37780,BUG: adding Timedelta to DatetimeIndex raising incorrectly,jbrockmendel,closed,2020-11-12T03:39:27Z,2020-11-13T04:56:53Z,"- [x] closes #37295
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
740197227,37747,CLN: remove unused min_count argument in libgroupby.group_nth,arw2019,closed,2020-11-10T19:51:16Z,2020-11-13T05:03:27Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
380698307,23693,DataFrame.reindex with specified fill method fails for MultiIndex,aberres,closed,2018-11-14T13:29:12Z,2020-11-13T05:11:10Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

i = pd.MultiIndex.from_tuples([('a', 'b'), ('d', 'e')])
df = pd.DataFrame([[0, 7], [3, 4]], index=i, columns=['x', 'y'])

print(df)
#      x  y
# a b  0  7
# d e  3  4

i2 = pd.MultiIndex.from_tuples([('a', 'b'), ('d', 'e'), ('h', 'i')])
# same behavior
#i2 = pd.MultiIndex.from_tuples([('a', 'b', 'c'), ('d', 'e', 'f'), ('h', 'i', 'j')])

print(df.reindex(i2, axis=0, method='ffill'))
#       x    y
# a b  3.0  4.0
# d e  NaN  NaN
# h i  0.0  7.0

```
#### Problem description

The reindexing operation above introduces a row to the `MultiIndex`. When no fill method is specified the new row is added and filled with NA as expected.

When `ffill` is specified the behavior is not explainable for me. The index is updated as expected but a NA row is added in the middle of the existing data.

#### Expected Output

Not sure if `ffill` for MultiIndexes is designed like this, but I was hoping for

```
#       x    y
# a b  3.0  4.0
# d e  0.0  7.0
# h i  0.0  7.0
```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None

pandas: 0.23.4
pytest: 3.7.1
pip: 18.1
setuptools: 39.0.1
Cython: None
numpy: 1.15.0
scipy: None
pyarrow: 0.11.1
xarray: None
IPython: 6.5.0
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: 1.6.1
bottleneck: 1.2.1
tables: None
numexpr: 2.6.8
feather: None
matplotlib: 3.0.2
openpyxl: 2.5.5
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0.1
sqlalchemy: 1.2.10
pymysql: None
psycopg2: 2.7.5 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: 0.1.6
pandas_gbq: None
pandas_datareader: None
</details>
"
739488987,37730,TST/REF: collect/parametrize tests from tests.generic,jbrockmendel,closed,2020-11-10T01:16:38Z,2020-11-13T05:14:00Z,For some of these its just breaking up big tests and giving them meaningful names.
739348377,37722,BUG: loc.__getitem__[[na_value]] with CategoricalIndex containing NAs,jbrockmendel,closed,2020-11-09T20:41:42Z,2020-11-13T05:16:23Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
728035842,37359,REGR: Notebook (html) repr of DataFrame no longer follows min_rows/max_rows settings,jorisvandenbossche,closed,2020-10-23T08:50:45Z,2020-11-13T05:45:48Z,"On master, the html repr in notebooks of a DataFrame is no longer following the min_rows/max_rows logic (see https://pandas.pydata.org/docs/dev/user_guide/options.html#frequently-used-options). 

If you have a DataFrame with many rows, it incorrectly shows 60 (max_rows) rows in the truncated repr intead of 10 (min_rows). 

cc @ivanovmg (might be related to the refactoring you have been doing, but to be clear, I didn't check if this is actually the case!) 

"
737282060,37655,TST: split up tests/plotting/test_frame.py into subdir & modules #34769,Mikhaylov-yv,closed,2020-11-05T21:29:47Z,2020-11-13T05:52:50Z,"Created instead of #37538

- [x] closes #34769
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I moved the file to a new directory.
What is the best structure for this folder?

[before_test_ploting_frame](https://gist.github.com/Mikhaylov-yv/999de1451e8ecc836215b30db34ef841)
[after_test_ploting_frame](https://gist.github.com/Mikhaylov-yv/98eca32c21f59cc9e7653cd96d5fabd2)"
728134218,37363,REGR: Notebook (html) repr of DataFrame no longer follows min_rows/max_rows settings ,ivanovmg,closed,2020-10-23T11:16:39Z,2020-11-13T05:53:19Z,"- [ ] closes #37359
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Fix display logic in number of rows,
explained here https://pandas.pydata.org/docs/dev/user_guide/options.html#frequently-used-options"
742137140,37806,Backport PR #37780 on branch 1.1.x (BUG: adding Timedelta to DatetimeIndex raising incorrectly),meeseeksmachine,closed,2020-11-13T04:40:39Z,2020-11-13T09:04:33Z,Backport PR #37780: BUG: adding Timedelta to DatetimeIndex raising incorrectly
742323033,37809,DOC add note about how to skip tests during CI,MarcoGorelli,closed,2020-11-13T09:55:50Z,2020-11-13T10:01:46Z,"Tomorrow there will be a PyDataGlobal sprint, which'll probably involve many, many documentation-related pull requests, so this might help with not using up azure pipelines resources unnecessarily

EDIT
----
doesn't seem to be working :flushed:"
285687170,19056,"DataFrame.xs(""key"", axis=1, drop_level=False) still dropping level",boulund,closed,2018-01-03T13:28:31Z,2020-11-13T13:09:29Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame([[1, 2, 3], [2, 4, 6]], columns=[""type1_subtype1_subsubtype1"", ""type1_subtype1_subsubtype2"", ""type2_subtype1_subsubtype2""], index=[""rowtype_rowsubtype1_rowlevel1"", ""row_rowlevel0_rowlevel1""])
df.columns = d.columns.str.split(""_"", expand=True).rename([""Sample_type"", ""Subtype"", ""SubSubtype""])
df.index = d.index.str.split(""_"", expand=True).rename([""Rowtype"", ""rowsubtype"", ""rowlevel""])
print(""Incorrect behavior:"")
print(df.xs(""type1"", axis=1, drop_level=False))
print(""Expected output:"")
print(df.transpose().xs(""type1"", drop_level=False).transpose())
```

#### Problem description
I checked if there was a previous issue about this in the issue tracker but my searches gave nothing similar. 

It appears that `drop_level=False` doesn't work as intended when taking a cross section along `axis=1`. Current workaround is to transpose the dataframe, then do the cross section, and then transpose it back again (as in my example above). 

#### Expected Output
```
Sample_type                         type1            
Subtype                          subtype1            
SubSubtype                    subsubtype1 subsubtype2
Rowtype rowsubtype  rowlevel                         
rowtype rowsubtype1 rowlevel1           1           2
row     rowlevel0   rowlevel1           2           4
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.1.final.0
python-bits: 64
OS: Linux
OS-release: 3.10.0-693.11.1.el7.x86_64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.20.1
pytest: 3.0.7
pip: 9.0.1
setuptools: 36.5.0.post20170921
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.19.0
xarray: None
IPython: 5.3.0
sphinx: 1.5.6
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.3.0
numexpr: 2.6.2
feather: 0.3.1
matplotlib: 2.0.2
openpyxl: 2.4.7
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: 0.9.6
lxml: 3.7.3
bs4: 4.6.0
html5lib: 0.999
sqlalchemy: 1.1.9
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
pandas_gbq: None
pandas_datareader: None
None
</details>

  "
735592066,37611,Add tests for categorical with null ea as input,phofl,closed,2020-11-03T19:49:26Z,2020-11-13T13:14:46Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

cc @jreback Is this what you had in mind for ExtensionArrays and Categoricals?"
739410045,37726,TST: Add test for filling new rows through reindexing MultiIndex,phofl,closed,2020-11-09T22:29:15Z,2020-11-13T13:16:08Z,"- [x] closes #23693
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Issue was fixed previously"
612285289,33987,BUG:Cannot write as xlsx to GCS,gfelot,closed,2020-05-05T02:37:41Z,2020-11-13T13:39:29Z,"

#### Code Sample, a copy-pastable example

```python
output_path = f""gs://my_bucket/incidents/prediction/{ds}_incidents_result""
[...]
export_app.to_parquet(f""{output_path}.parquet"")
export_app.to_excel(f""{output_path}.xlsx"")

```

#### Problem description

I want to write the DF as a `.parquet` and a `.xlsx` file to a GCloud Storage bucket.
I launch the job in a K8S pod and I finally got the error message :

>textPayload: ""[Errno 2] No such file or directory: 'gs://my_bucket/incidents/prediction/2020-04-29_incidents_result.xlsx'

Next I change the `to_excel` -> `to_csv` and everything works as expected.

Do the `to_excel` can handle a path as `gs://...` ? Because it's the only issue I can see with that.


#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
 ------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.138+
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : 0.6.1
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : 1.3.0
xlsxwriter       : None
numba            : None
"
741050426,37766,CI/TST: read_html test_banklist_url_positional_match failing with ResourceWarning on Travis,jorisvandenbossche,closed,2020-11-11T20:31:18Z,2020-11-13T15:25:13Z,"Travis builds are recently failing, eg https://travis-ci.org/github/pandas-dev/pandas/jobs/742927007

```
=================================== FAILURES ===================================

_____________ TestReadHtml.test_banklist_url_positional_match[bs4] _____________

[gw0] linux -- Python 3.7.8 /home/travis/miniconda3/envs/pandas-dev/bin/python

self = <pandas.tests.io.test_html.TestReadHtml object at 0x7f3091661050>

    @tm.network

    def test_banklist_url_positional_match(self):

        url = ""http://www.fdic.gov/bank/individual/failed/banklist.html""

        # Passing match argument as positional should cause a FutureWarning.

        with tm.assert_produces_warning(FutureWarning):

            df1 = self.read_html(

>               url, ""First Federal Bank of Florida"", attrs={""id"": ""table""}

            )

pandas/tests/io/test_html.py:130: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7f309168d4d0>

type = None, value = None, traceback = None

    def __exit__(self, type, value, traceback):

        if type is None:

            try:

>               next(self.gen)

E               AssertionError: Caused unexpected warning(s): [('ResourceWarning', ResourceWarning(""unclosed <ssl.SSLSocket fd=18, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.10', 43648), raddr=('172.217.212.95', 443)>""), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/html5lib/treebuilders/base.py', 38), ('ResourceWarning', ResourceWarning(""unclosed <ssl.SSLSocket fd=16, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.10', 43650), raddr=('172.217.212.95', 443)>""), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/html5lib/treebuilders/base.py', 38), ('ResourceWarning', ResourceWarning(""unclosed <ssl.SSLSocket fd=43, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.10', 43962), raddr=('172.217.214.95', 443)>""), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/bs4/builder/_html5lib.py', 335), ('ResourceWarning', ResourceWarning(""unclosed <ssl.SSLSocket fd=42, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.10', 46504), raddr=('74.125.124.95', 443)>""), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/bs4/builder/_html5lib.py', 335), ('ResourceWarning', ResourceWarning(""unclosed <ssl.SSLSocket fd=41, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.10', 46502), raddr=('74.125.124.95', 443)>""), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/bs4/builder/_html5lib.py', 335), ('ResourceWarning', ResourceWarning(""unclosed <ssl.SSLSocket fd=15, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.10', 43640), raddr=('172.217.212.95', 443)>""), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/bs4/builder/_html5lib.py', 335)]

../../../miniconda3/envs/pandas-dev/lib/python3.7/contextlib.py:119: AssertionError
```"
704796489,36467,CI: ResourceWarning in `pandas/tests/io/test_html.py`,fangchenli,closed,2020-09-19T04:08:12Z,2020-11-13T15:26:41Z,"It occurs occasionally on the travis-37-cov build.

```
_____________ TestReadHtml.test_banklist_url_positional_match[bs4] _____________

[gw0] linux -- Python 3.7.8 /home/travis/miniconda3/envs/pandas-dev/bin/python

self = <pandas.tests.io.test_html.TestReadHtml object at 0x7f40f8fef450>

    @tm.network

    def test_banklist_url_positional_match(self):

        url = ""http://www.fdic.gov/bank/individual/failed/banklist.html""

        # Passing match argument as positional should cause a FutureWarning.

        with tm.assert_produces_warning(FutureWarning):

            df1 = self.read_html(

>               url, ""First Federal Bank of Florida"", attrs={""id"": ""table""}

            )

pandas/tests/io/test_html.py:130: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7f40f9070190>

type = None, value = None, traceback = None

    def __exit__(self, type, value, traceback):

        if type is None:

            try:

>               next(self.gen)

E               AssertionError: Caused unexpected warning(s): [('ResourceWarning', ResourceWarning(""unclosed <ssl.SSLSocket fd=18, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.43', 54722), raddr=('172.217.204.95', 443)>""), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/html5lib/_utils.py', 85), ('ResourceWarning', ResourceWarning(""unclosed <ssl.SSLSocket fd=17, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.43', 54720), raddr=('172.217.204.95', 443)>""), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/html5lib/_utils.py', 85), ('ResourceWarning', ResourceWarning(""unclosed <ssl.SSLSocket fd=21, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.43', 52284), raddr=('142.250.97.95', 443)>""), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/html5lib/_utils.py', 85), ('ResourceWarning', ResourceWarning(""unclosed <ssl.SSLSocket fd=23, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.43', 54628), raddr=('173.194.210.95', 443)>""), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/html5lib/_utils.py', 85), ('ResourceWarning', ResourceWarning(""unclosed <ssl.SSLSocket fd=22, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.43', 37618), raddr=('173.194.215.95', 443)>""), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/html5lib/_utils.py', 85), ('ResourceWarning', ResourceWarning(""unclosed <ssl.SSLSocket fd=24, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.43', 38812), raddr=('173.194.217.95', 443)>""), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/html5lib/_utils.py', 85), ('ResourceWarning', ResourceWarning(""unclosed <ssl.SSLSocket fd=28, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.43', 38824), raddr=('173.194.217.95', 443)>""), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/html5lib/_utils.py', 85), ('ResourceWarning', ResourceWarning(""unclosed <ssl.SSLSocket fd=27, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.20.0.43', 54640), raddr=('173.194.210.95', 443)>""), '/home/travis/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/html5lib/_utils.py', 85)]

../../../miniconda3/envs/pandas-dev/lib/python3.7/contextlib.py:119: AssertionError
```

"
741254544,37781,REF: simplify Block.replace,jbrockmendel,closed,2020-11-12T03:49:20Z,2020-11-13T16:19:35Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
742015710,37802,REF: avoid try/except in Block.where,jbrockmendel,closed,2020-11-12T23:57:46Z,2020-11-13T16:20:31Z,
742418394,37812,CI: The `set-env` and `add-path` commands are deprecated and will be disabled on November 16th.,simonjayhawkins,closed,2020-11-13T12:34:36Z,2020-11-13T16:20:36Z,https://github.com/pandas-dev/pandas/actions/runs/361523610
741541506,37786,CI/TST: use https to avoid ResourceWarning in html tests,jorisvandenbossche,closed,2020-11-12T12:00:25Z,2020-11-13T16:31:05Z,"See #36467 
(no idea if this would actually fix anything, just mimicking what @alimcmaster1 did in https://github.com/pandas-dev/pandas/pull/36480)"
742578135,37814,Backport PR #37812 on branch 1.1.x (CI: The `set-env` and `add-path` commands are deprecated and will be disabled on November 16th.),meeseeksmachine,closed,2020-11-13T16:21:17Z,2020-11-13T18:43:40Z,Backport PR #37812: CI: The `set-env` and `add-path` commands are deprecated and will be disabled on November 16th.
736377188,37635,BUG: pivot() modifies arguments,Jacob-Stevens-Haas,closed,2020-11-04T19:44:14Z,2020-11-14T02:21:09Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Minimal Working Example

One of the documentation examples, slightly modified:
```python
import pandas as pd
df = pd.DataFrame({
       ""lev1"": [1, 1, 1, 2, 2, 2],
       ""lev2"": [1, 1, 2, 1, 1, 2],
       ""lev3"": [1, 2, 1, 2, 1, 2],
       ""lev4"": [1, 2, 3, 4, 5, 6],
       ""values"": [0, 1, 2, 3, 4, 5]})
a = [""lev1"", ""lev2""]
b=[""lev3""]
df.pivot(index=a, columns=b)
print(a)
```
Displays:
```
['lev1', 'lev2', 'lev3']
```

#### Problem description

I would expect that my arguments passed to `.pivot` remain unchanged.  I understand that lists, being mutable, can be modified by a function when they are used as arguments.  I also understand that I can pass a set here instead of a list to keep it from being modified.  However, I think it's reasonable to expect functions to be side-effect free in this case, especially when the documentation asks for a list and does not mention side effects.   (FWIW, this behavior does not occur if I set the `values=` parameter).

The issue cropped up when I wanted to `.pivot` multiple DataFrames using the same index.  To do so, I created a partial function of `.pivot` with an anonymous list as the `index=` paramter.  Here's an MWE using the dataframe from above:
```python
from functools import partial
alt_pivot = lambda df, index, columns: df.pivot(index=index, columns=columns)
ppivot = partial(alt_pivot, index=[""lev1"", ""lev2""])
print(ppivot)
ppivot(df, columns=b)
print(ppivot)
```
produces the output:
```
functools.partial(<function <lambda> at 0x00000159C45EAEE8>, index=['lev1', 'lev2'])
functools.partial(<function <lambda> at 0x00000159C45EAEE8>, index=['lev1', 'lev2', 'lev3'])
```
#### Expected Output
In the MWE above,
```python
print(a)
```
produces
```
['lev1', 'lev2']
```

#### Output of ``pd.show_versions()``
(Version in my project.  Confirmed same behavior in `pandas==1.1.4` and `master` branch.)
<details>
INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.16299
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.2.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.19
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
745144132,37923,REF: de-duplicate CategoricalIndex.get_indexer,jbrockmendel,closed,2020-11-17T22:47:09Z,2020-11-18T02:53:10Z,
745062473,37916,REF: de-duplicate IntervalIndex compatiblity checks,jbrockmendel,closed,2020-11-17T20:26:58Z,2020-11-18T02:54:00Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
745092700,37917,REF: ensure CategoricalIndex._shallow_copy only ever gets Categorical,jbrockmendel,closed,2020-11-17T21:14:57Z,2020-11-18T02:59:59Z,"still not as tightly restricted as id like, but a step in the right direction."
743626322,37889,"BUG:BadZipFile CRC-32 for file 'docProps/core.xml' when  I use pandas read excel accord an error """,530435745,closed,2020-11-16T08:18:47Z,2020-11-18T03:02:59Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here

```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
486714981,28210,cannot output csv with IntervalIndex,AlJohri,closed,2019-08-29T03:26:31Z,2020-11-18T08:51:39Z,"#### Code Sample, a copy-pastable example if possible

Using `pd.interval_range`:
```python
pd.DataFrame({'a': [1, 2, 3]}, index=pd.interval_range(0, 1, periods=3)).to_csv('hello.csv')
```

Using `pd.IntervalIndex.from_arrays`:
```python
pd.DataFrame({'a': [1, 2, 3]}, index=pd.IntervalIndex.from_arrays(np.array([0, 1, 2]), np.array([1, 2, 3]))).to_csv('hello.csv')
```

#### Problem description

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-6f913f6c0211> in <module>
----> 1 pd.DataFrame({'a': [1, 2, 3]}, index=pd.IntervalIndex.from_arrays(np.array([0, 1, 2]), np.array([1, 2, 3]))).to_csv('hello.csv')

~/Development/propensity-to-subscribe-modeling/.venv/lib/python3.7/site-packages/pandas/core/generic.py in to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)
   3226             decimal=decimal,
   3227         )
-> 3228         formatter.save()
   3229 
   3230         if path_or_buf is None:

~/Development/propensity-to-subscribe-modeling/.venv/lib/python3.7/site-packages/pandas/io/formats/csvs.py in save(self)
    200                 self.writer = UnicodeWriter(f, **writer_kwargs)
    201 
--> 202             self._save()
    203 
    204         finally:

~/Development/propensity-to-subscribe-modeling/.venv/lib/python3.7/site-packages/pandas/io/formats/csvs.py in _save(self)
    322                 break
    323 
--> 324             self._save_chunk(start_i, end_i)
    325 
    326     def _save_chunk(self, start_i, end_i):

~/Development/propensity-to-subscribe-modeling/.venv/lib/python3.7/site-packages/pandas/io/formats/csvs.py in _save_chunk(self, start_i, end_i)
    354         )
    355 
--> 356         libwriters.write_csv_rows(self.data, ix, self.nlevels, self.cols, self.writer)

TypeError: Argument 'data_index' has incorrect type (expected numpy.ndarray, got list)
```

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.0
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.2
setuptools       : 41.1.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.7.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.14.1
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
```

</details>
"
745035895,37914,CI: fix mypy error (quick fix),simonjayhawkins,closed,2020-11-17T19:44:46Z,2020-11-18T11:47:39Z,see https://github.com/pandas-dev/pandas/pull/37913#issuecomment-729155748
745259001,37928,BUG: SingleBlockManager' object has no attribute 'external_vaues' while using drop_duplicate,Jasonsey,closed,2020-11-18T02:14:57Z,2020-11-18T11:50:15Z,"
- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
In [1]: import pandas as pd

In [2]: pd.__version
Out[2]: '1.1.3'

In [3]: df = pd.DataFrame({'a': [1,2,3,4,5], 'b': [3,3,3,3,3]})

In [4]: df.drop_duplicates()
```

#### Problem description

The following error occurred when executing the code above

```shell

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-7-c128d71b3023> in <module>
----> 1 df.drop_duplicates()

~/.conda/envs/test/lib/python3.6/site-packages/pandas/core/frame.py in drop_duplicates(self, subset, keep, inplace, ignore_index)
   5106
   5107         inplace = validate_bool_kwarg(inplace, ""inplace"")
-> 5108         duplicated = self.duplicated(subset, keep=keep)
   5109
   5110         result = self[-duplicated]

~/.conda/envs/test/lib/python3.6/site-packages/pandas/core/frame.py in duplicated(self, subset, keep)
   5245
   5246         vals = (col.values for name, col in self.items() if name in subset)
-> 5247         labels, shape = map(list, zip(*map(f, vals)))
   5248
   5249         ids = get_group_index(labels, shape, sort=False, xnull=False)

~/.conda/envs/test/lib/python3.6/site-packages/pandas/core/frame.py in <genexpr>(.0)
   5244             raise KeyError(diff)
   5245
-> 5246         vals = (col.values for name, col in self.items() if name in subset)
   5247         labels, shape = map(list, zip(*map(f, vals)))
   5248

~/.conda/envs/test/lib/python3.6/site-packages/pandas/core/generic.py in __getattr__(self, name)
   5137             if self._info_axis._can_hold_identifiers_and_holds_name(name):
   5138                 return self[name]
-> 5139             return object.__getattribute__(self, name)
   5140
   5141     def __setattr__(self, name: str, value) -> None:

~/.conda/envs/test/lib/python3.6/site-packages/pandas/core/series.py in values(self)
    536                '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')
    537         """"""
--> 538         return self._mgr.external_vaues()
    539
    540     @property

AttributeError: 'SingleBlockManager' object has no attribute 'external_vaues'
```

#### Expected Output
No error occured
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.6.12.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 20.1.0
Version          : Darwin Kernel Version 20.1.0: Sat Oct 31 00:07:11 PDT 2020; root:xnu-7195.50.7~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : zh_CN.UTF-8
LOCALE           : zh_CN.UTF-8

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.1.post20201107
Cython           : None
pytest           : 6.1.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : 0.10.1
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.20
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
745453844,37934,BUG: json_normalize() does not parse floats in scientific notation,KaiRoesnerAtSAP,closed,2020-11-18T08:39:37Z,2020-11-18T12:13:12Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import json
import pandas as pd

body = '''[
{ ""c1"": 1, ""c2"": ""1.01"" },
{ ""c1"": 2, ""c2"": ""1E-3"" },
{ ""c1"": 3, ""c2"": ""3.03"" }
]'''

df1 = pd.read_json(body)
df2 = pd.json_normalize(json.loads(body))
print(df1, ""\n\n"", df1.dtypes)
print(""\n\n"", df2, ""\n\n"", df2.dtypes)
```

#### Problem description

When creating a dataframe from json string input via `json.loads()` and `pd.json_normalize()` float values in scientific notation (e.g. ""1E-3"") are not parsed as floats. When using `pd.read_json()` they are parsed correctly.

Output:
```
    c1     c2
0   1  1.010
1   2  0.001
2   3  3.030 

 c1      int64
c2    float64
dtype: object

    c1    c2
0   1  1.01
1   2  1E-3
2   3  3.03 

 c1     int64
c2    object
dtype: object
```

#### Expected Output

```
    c1     c2
0   1  1.010
1   2  0.001
2   3  3.030 

 c1      int64
c2    float64
dtype: object

    c1    c2
0   1  1.010
1   2  0.001
2   3  3.030

 c1      int64
c2    float64
dtype: object
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-1019-gcp
Version          : #19-Ubuntu SMP Tue Jun 23 15:46:40 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.4
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.1.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.19
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
745101534,37918,Deprecate inplace in Categorical.remove_unused_categories,OlehKSS,closed,2020-11-17T21:29:28Z,2020-11-18T13:32:02Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Related to #37643
"
708937906,36635,TST: GH 32431 check print label indexing on nan,wesleyboelrijk,closed,2020-09-25T13:32:08Z,2020-11-18T13:53:20Z,"- [x] closes #32431 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry --> not applicable

This is my first contribution to pandas. I added a simple test that was requested in one of the open issues."
745130187,37919,REF: de-duplicate pointwise get_indexer for IntervalIndex,jbrockmendel,closed,2020-11-17T22:19:56Z,2020-11-18T16:10:19Z,
718744018,37047,REF: back IntervalArray by a single ndarray,jbrockmendel,closed,2020-10-11T02:43:36Z,2020-11-18T16:12:44Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The main motivation here is for perf for methods that current cast to object dtype, including value_counts, set ops, values_for_(argsort|factorize)

Also we get an actually-simple simple_new"
743293598,37865,CLN: share some more datetimelike index methods,jbrockmendel,closed,2020-11-15T16:37:01Z,2020-11-18T16:21:25Z,
740130540,37746,CI: troubleshoot windows builds,jbrockmendel,closed,2020-11-10T18:08:19Z,2020-11-18T16:27:54Z,Windows builds are still hitting what look like OOM errors.  Trying splitting these into smaller pieces; its fragile.
745801444,37940,CI: flake8 check does not work properly,charlesdong1991,closed,2020-11-18T16:18:27Z,2020-11-18T16:30:41Z,"xref #37938

Have seen it in #37923 and in #37703 , so ci checks ran successfully in those feature PRs, but actually they should have detected this unused import.

Basically flake8 check in CI should detect cases such as `unused imports`, however, right now, observed from those two PRs, it didn't detect them in feature PRs and then resulted in CI failure on master"
745784143,37938,CI: Fix ci flake error,charlesdong1991,closed,2020-11-18T15:57:35Z,2020-11-18T16:50:36Z,"- [ ] xref #36305 
 came across it in #36305 
"
597439266,33435,Support downcasting of nullable arrays,yixinxiao7,closed,2020-04-09T17:28:41Z,2020-11-18T16:55:40Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Addressing GH #33013. Supports downcasting of nullable dtypes by transferring elements in extension array to ndarray to avoid TypeError thrown by np.allclose. Enhancement documented on v1.1.0 in the ""other enhancements"" section. Unit test written in test_to_numeric.
"
46788945,8628,ENH: decode for Categoricals,fkaufer,closed,2014-10-24T22:10:05Z,2020-11-18T18:21:51Z,"There seems no direct way to return to the original dtype and the [documentation](http://pandas-docs.github.io/pandas-docs-travis/categorical.html#object-creation) recommends: _""To get back to the original Series or numpy array, use Series.astype(original_dtype) or np.asarray(categorical)""_

That's slow and a `decode` or `decat` method would be trivial:

``` python
df=pd.DataFrame(np.random.choice(list(u'abcde'), 4e6).reshape(1e6, 4),
    columns=list(u'ABCD'))                                     
for col in df.columns: df[col] = df[col].astype('category')   

%timeit for col in df.columns: df[col].astype('unicode')      
1 loops, best of 3: 1.06 s per loop

%timeit for col in df.columns: cats=df[col].cat.categories; cats[df[col].cat.codes]    
10 loops, best of 3: 33.2 ms per loop   
```

I was working with ~10 categories (partially longer strings) on a 20 mio rows dataset where the difference was even bigger (unfortunately can't reproduce it with dummy data) and using `astype` felt rather buggy (minutes) than only a performance issue.

Given the current limitations on exporting categorical data, having a fast `decode` method would be very convenient. Since category codes are most often strings an optional parameter for direct character set encoding would also be good to have for such a method.

``` python
%timeit for col in df.columns: df[col].astype('unicode').str.encode('latin1')  
1 loops, best of 3: 3.95 s per loop
%timeit for col in df.columns: cats=pd.Series(df[col].cat.categories).str.encode('latin1'); cats[df[col].cat.codes]                                                                  
10 loops, best of 3: 74.5 ms per loop   
```
"
745215785,37926,CLN: tests/window/*,mroeschke,closed,2020-11-18T01:26:10Z,2020-11-18T18:33:29Z,"* Renamed `test_window.py` to `test_win_type.py`
* Added `'count'` to `arithmetic_win_operators`
* Reused fixtures
* Used more pytest idioms in `test_moments_consistency_expanding.py`
"
283365509,18853,DataFrame.drop(Timestamp) on MultiIndex with NaT incorrectly drops the NaT row,jeremywhelchel,closed,2017-12-19T20:51:35Z,2020-11-18T18:35:06Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame(
    index=pd.MultiIndex.from_tuples([('blah', pd.NaT)],
                                    names=['name', 'date']),
)
print df

Empty DataFrame
Columns: []
Index: [(blah, NaT)]

print df.drop(pd.Timestamp('2001'), level='date')

Empty DataFrame
Columns: []
Index: []

```
#### Problem description

Timestamp('2001') isn't actually in the index. The drop() call should raise an error. For example when just operating on a single-level datetime index it throws this error:
```python
df = pd.DataFrame(index=[pd.NaT])
df.drop(pd.Timestamp('2001'))
ValueError: labels [Timestamp('2001-01-01 00:00:00')] not contained in axis
```
I would expect the same thing. Silently dropping the NaT value was causing a hard to find bug in my code.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.6.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-97-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: en_US.UTF-8
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.21.0
pytest: None
pip: None
setuptools: None
Cython: None
numpy: 1.11.1
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 2.0.0
sphinx: None
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2017.3
blosc: None
bottleneck: None
tables: 3.1.1
numexpr: 2.5
feather: None
matplotlib: 1.5.2
openpyxl: None
xlrd: 0.9.3
xlwt: None
xlsxwriter: None
lxml: 3.4.4
bs4: 4.5.3
html5lib: 1.0b8
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.8
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.2.0
</details>
"
743066894,37848,Updated script to check inconsistent pandas namespace,mohdkashif93,closed,2020-11-14T19:43:11Z,2020-11-18T19:04:43Z,"- [ ] closes #37838 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Updates the script for inconsistent namespace usage. To be merged after all the errors are fixed with the updated script. The script is described here : https://github.com/MarcoGorelli/PyDataGlobal2020-sprint/issues/1 by @MarcoGorelli "
743055063,37839,DOC: section in indexing user guide to show use of np.where,suvayu,closed,2020-11-14T18:28:22Z,2020-11-18T19:14:12Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Add a section in the user guide to illustrate the use of `np.where` and `np.select` to enlarge a dataframe conditionally.  The example is taken from SO, as mentioned in [this issue](https://github.com/MarcoGorelli/PyDataGlobal2020-sprint/issues/5#issuecomment-727241262)."
745911272,37944,CLN: test_moments_expanding_consistency.py,mroeschke,closed,2020-11-18T18:39:04Z,2020-11-18T20:07:45Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Breaking up the predefined `moments_consistency_*` functions into more scoped tests with more pytest idioms. "
727887275,37355, PERF/ENH: add fast astyping for Categorical,arw2019,closed,2020-10-23T04:14:28Z,2020-11-18T20:36:45Z,"- [x] closes #8628
- [ ] tests added / passed
- [x] benchmarks added
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

To illustrate the speed-up, the set-up is (from OP):
```python
import numpy as np
import pandas as pd

rng = np.random.default_rng()

df = pd.DataFrame(
    rng.choice(np.array(list(""abcde"")), 4_000_000).reshape(1_000_000, 4),
    columns=list(""ABCD""),
)

for col in df.columns:
    df[col] = df[col].astype(""category"")
```
On master
``` python
In [5]: %timeit [df[col].astype('unicode') for col in df.columns] 
250 ms ± 601 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
```
versus on this branch
``` python
In [5]: %timeit [df[col].astype('unicode') for col in df.columns] 
5.38 ms ± 47 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```"
741946077,37794,[BUG]: Fix bug in MultiIndex.drop dropped nan when non existing key was given,phofl,closed,2020-11-12T21:39:08Z,2020-11-18T20:37:25Z,"- [x] closes #18853
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Additional question:
The doc says (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.drop.html) that ``drop`` raises when not all labels are found, but
```
pd.DataFrame(index=pd.MultiIndex.from_product([range(3), range(3)])).drop([1, 5], level=0)
```
does not raise, because ``1`` is found. Should we implement, that it raises every time when at least one of the keys is not found?"
743356835,37877,DEPR: Index.asi8,jbrockmendel,closed,2020-11-15T22:10:38Z,2020-11-19T00:27:24Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
563982550,31922,BUG: segmentation fault with pd.NA and np.ndarray.__contains__,jorisvandenbossche,closed,2020-02-12T12:56:45Z,2020-11-19T00:32:30Z,"Testing for membership in an ndarray segfaults:

```
In [1]: pd.NA in np.array([""a""], dtype=object) 
Segmentation fault (core dumped)
```

Now, this might be something that needs to be fixed in numpy, as for plain lists it ""works"":

```
In [3]: pd.NA in ['a']  
...
TypeError: boolean value of NA is ambiguous
```

but numpy might not expect an error instead of True/False."
698880605,36283,TST: #31922 assert no segmentation fault with numpy.array.__contains__,ylin00,closed,2020-09-11T06:03:39Z,2020-11-19T00:34:52Z,"- [ ] closes #31922
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] new test cases
"
746019251,37946,CLN: test_moments_rolling_consistency.py,mroeschke,closed,2020-11-18T21:26:53Z,2020-11-19T01:10:48Z,"Same changes as https://github.com/pandas-dev/pandas/pull/37944
"
743031939,37833,Performance regression in fillna,TomAugspurger,closed,2020-11-14T16:17:40Z,2020-11-19T02:07:37Z,"https://pandas.pydata.org/speed/pandas/index.html#replace.FillNa.time_fillna?python=3.8&Cython=0.29.21&p-inplace=False&commits=2eb353063d6bc4f2ed22e9943d26465e284b8393-68f53cd9e556698fe786aefe90466f233eb55841

https://github.com/pandas-dev/pandas/compare/2eb353063d6bc4f2ed22e9943d26465e284b8393...68f53cd9e556698fe786aefe90466f233eb55841

- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---"
638432682,34777,ENH: Support Multi-Index for columns in parquet format,yohplala,closed,2020-06-14T21:19:18Z,2020-11-19T02:09:10Z,"#### Is your feature request related to a problem?

I would like to save DataFrame with Multi-Index used for columns into parquet format.
This is currently not possible.

```python
import pandas as pd
df = pd.DataFrame([[1,2,3],[4,5,6],[7,8,9]], columns= pd.MultiIndex.from_product([['1'],['a', 'b', 'c']]))
df.to_parquet('test.parquet')
"""""" doesn't work, whatever the engine """"""
ValueError: parquet must have string column names
```

#### Describe alternatives you've considered

To do so with actual state of library, a [piece of code has been shared on SO](https://stackoverflow.com/questions/54861430/how-do-i-save-multi-indexed-pandas-dataframes-to-parquet).
I will use it for now.

```python
import pyarrow as pa
import pyarrow.parquet as pq
table = pa.Table.from_pandas(df)
pq.write_table(table, 'test.parquet')
df_test_read = pd.read_parquet('test.parquet')
```

Thanks for your help!
Bests,"
745988520,37945,PERF: use np.putmask instead of ndarray.__setitem__,jbrockmendel,closed,2020-11-18T20:41:56Z,2020-11-19T02:16:36Z,"- [x] closes #37833
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
744412030,37909,BUG: use compression=None (again) to avoid inferring compression,twoertwein,closed,2020-11-17T05:00:42Z,2020-11-19T04:12:57Z,"`read_fwf` was using `compression=None` to infer compression (in addition to `compression=""infer""`). This is undocumented but was enforced in a test.

In #37639, I thought this was the expected behavior for all functions. That as wrong, `read_csv` explicitly states that `compression=None` does not infer any compression.
"
742931707,37828,CLN: File handling for PyArrow parquet,twoertwein,closed,2020-11-14T05:35:26Z,2020-11-19T04:12:57Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

De-duplicate file handling of PyArrows' parquet."
344232897,22046,Replacing multiple columns (or just one) with iloc does not work,mitar,closed,2018-07-24T22:58:31Z,2020-11-19T08:46:12Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas

columns = pandas.DataFrame({'a2': [11, 12, 13], 'b2': [14, 15, 16]})
inputs = pandas.DataFrame({'a1': [1, 2, 3], 'b1': [4, 5, 6], 'c1': [7, 8, 9]})

inputs.iloc[:, [1]] = columns.iloc[:, [0]]

print(inputs)
```

#### Problem description

I have a code which is replacing a set of columns with another set of columns, based on column indices. To make things done without a special case, I assumes I could just use `iloc` to both select and set columns in a DataFrame. But it seems that this not work and fails in strange ways.

#### Expected Output

```
   a1  b1  c1
0   1  11   7
1   2  12   8
2   3  13   9
```

But in reality, you get:

```
    a1  b1   c1
0  1.0 NaN  7.0
1  2.0 NaN  8.0
2  3.0 NaN  9.0
```

See how values converted to float and how column is `NaN`s?

But, if I do the following I get expected results:

```
inputs.iloc[:, [1]] = [[11], [12], [13]]
```

This also works:

```
inputs.iloc[:, [1]] = columns.iloc[:, [0]].values
```

So if it works with lists and ndarrays, one would assume it would also work with DataFrames themselves. But it does not.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.13.0-46-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.3
pytest: None
pip: 18.0
setuptools: 40.0.0
Cython: None
numpy: 1.15.0
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
745514902,37936,BUG: style.background_gradient works in jupyter but not in Github,alexiska,closed,2020-11-18T10:03:55Z,2020-11-19T14:02:46Z,"The bakground color gradient works fine in jupyter but when I upload it to Github the color disappears. Is there anyway I can fix this?

From jupyter
![image](https://user-images.githubusercontent.com/38601205/99515458-47671e80-298d-11eb-9cba-1c05ed5f0862.png)

After uploading to Github
![image](https://user-images.githubusercontent.com/38601205/99515685-901ed780-298d-11eb-9287-5e6bed9693ea.png)

"
746249197,37951,CLN: test_ewm_rolling_consistency.py,mroeschke,closed,2020-11-19T04:58:41Z,2020-11-19T17:56:57Z,"Same changes as https://github.com/pandas-dev/pandas/pull/37944 and https://github.com/pandas-dev/pandas/pull/37946

Additionally allows removal of `moments/conftest.py` and `tests/window/common.py`
"
744929448,37912,DOC: add two new examples to the transform docs,marktgraham,closed,2020-11-17T17:17:43Z,2020-11-19T18:40:17Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This PR adds two examples to the transform user guide. The examples apply the transform method to a groupby object. This is from the PyData Global sprint, from [this issue](https://github.com/MarcoGorelli/PyDataGlobal2020-sprint/issues/5). The examples are taken from [here](https://stackoverflow.com/a/37189976/14583166) and [here](https://stackoverflow.com/a/54417351/14583166)."
152042577,13044,COMPAT: should an empty string match a format (or just be NaT),jreback,closed,2016-04-30T17:29:37Z,2020-11-19T18:58:15Z,"after #13033

I left this alone, but should an empty string just be a `NaT` (like we do elsewhere), even when a `format`is present? 

```
In [1]:  td = pd.Series(['May 04', 'Jun 02', ''], index=[1, 2, 3])

In [2]: td
Out[2]: 
1    May 04
2    Jun 02
3          
dtype: object

In [3]: pd.to_datetime(td, format='%b %y', errors='coerce')
Out[3]: 
1   2004-05-01
2   2002-06-01
3          NaT
dtype: datetime64[ns]

In [4]: pd.to_datetime(td, format='%b %y', errors='raise')
ValueError: time data '' does not match format '%b %y' (match)
```

related is that we don't coerce empty strings here either (and we have an odd error message).

```
In [1]: pd.to_datetime([1, ''], unit='s', errors='coerce')
Out[1]: DatetimeIndex(['1970-01-01 00:00:01', 'NaT'], dtype='datetime64[ns]', freq=None)

In [2]: pd.to_datetime([1, ''], unit='s')
ValueError: invalid literal for long() with base 10: ''
```
"
743046854,37835,COMPAT: should an empty string match a format (or just be NaT),fgebhart,closed,2020-11-14T17:39:54Z,2020-11-19T18:58:21Z,"- [x] closes #13044
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry (not applicable)
"
746110953,37948,DOC: Change warning for sort behavior in concat,phofl,closed,2020-11-18T23:08:14Z,2020-11-19T19:30:23Z,"We should either add a ``versionchanged`` attribute, for when this behavior changed or remove the warning completly. Additionally I removed the ``sort=False`` input for the function, because this is no longer necessary."
739468809,37728,Bug in iloc aligned objects,phofl,closed,2020-11-10T00:27:19Z,2020-11-19T20:00:27Z,"- [x] closes #22046
- [x] closes #37593
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

If iloc should be purely position based, this PR fixes the bug described in the issue. One test depends on the wrong behavior, so I had to adjust it."
746870144,37959,CLN: More tests/window/*,mroeschke,closed,2020-11-19T19:43:09Z,2020-11-19T20:47:37Z,"- Utilize more `frame_or_series` fixtures
- Cleaned `conftest.py`
- Moved some tests to the appropriate location
- Reorg `test_groupby` into `TestRolling` and `TestExpanding` classes
"
47034013,8659,API/ENH: master issue for pd.rolling_apply,leeong05,closed,2014-10-28T14:42:51Z,2020-11-19T21:13:15Z,"Catchall for `rolling_*` issues:
- [x] #12537 exclude nuiscance columns
- [ ] #12536 fully support timedeltas
- [ ] #4130 (return type of `rolling_apply`)
- [x] #3185 (stop using `np.apply_along_axis`)
- [x] #5071, #12950 (pass frames, operate 2-D)
- [x] #9413 `rolling_quantile`
- [ ] #9481 `rolling_idxmax`
- [ ] #10759 rolling_qcut 
- [ ] #11446, #20773  casting in `.apply`
- [x] #12595 casting in all window functions (min/max are good)
- [x] #10702 namespacing (though prob just create a `Rolling` object), #11603 
- [x] #12535 testing of datetimelike with nans
- [x] #12541 count should be integer dtype (and deal with infs)
- [ ] #15095 Table-wise

---

Hi all,

I intended to apply a function that gives on each day a ranking based on means calculated from previous n-day's data. The natural way is to use pd.rolling_apply. A toy example:

```
In [93]: df = pd.DataFrame(np.random.randint(10, size=20).reshape(4, 5))

In [94]: df
Out[94]: 
   0  1  2  3  4
0  2  0  0  2  0
1  9  5  5  6  1
2  2  3  6  8  8
3  5  1  2  9  0

In [95]: import bottleneck as bn

In [96]: bn.nanrankdata(df.mean())
Out[96]: array([ 4. ,  1.5,  3. ,  5. ,  1.5])
```

Up to now, it is cool. Then:

```
In [97]: pd.rolling_apply(df, 2, lambda x: bn.nanrankdata(bn.nanmean(x, axis=0)))
Out[97]: 
    0   1   2   3   4
0 NaN NaN NaN NaN NaN
1   1   1   1   1   1
2   1   1   1   1   1
3   1   1   1   1   1
```

This is clearly wrong. Is this a bug?
"
746276150,37953,DOC: re-use storage_options,twoertwein,closed,2020-11-19T06:03:21Z,2020-11-19T21:17:00Z,"The `{{ }}` are necessary so that `format` doesn't complain about missing keys.

TODO:

- `compression` can be re-used"
746884951,37960,CLN: Add comment and clarify if condition in indexing,phofl,closed,2020-11-19T20:06:16Z,2020-11-19T22:44:15Z,"Implemented comments

cc @jbrockmendel "
364165689,22841,Overflow when running reductions on float16 columns in pandas Series,jsleroux,closed,2018-09-26T18:58:14Z,2020-11-20T02:50:13Z,"When running reductions on dataframe columns of dtype float16, we ran into a surprising behaviour:

```python
import pandas as pd
df = pd.DataFrame(data=[1000]*100, columns=['A'], dtype='float16')

mean = df['A'].mean()
sum = df['A'].sum()
print('Mean: {}, sum: {}'.format(mean, sum))

# Both of these should be true, but they're not, returning inf in pandas 0.23.4
assert mean == 1000
assert sum == 1000 * 100
```

After investigation, we found that the accumulator used in (for example) mean() and sum() is not big enough and it eventually overflows. In both nansum() and nanmean() functions (to which sum() and mean() delegate their work), when the data type is 'float', the accumulator is downcasted to the original dtype of the data:

* nansum: https://github.com/pandas-dev/pandas/blob/af7b0ba461a5b81733afdc7fc816a869b798093d/pandas/core/nanops.py#L333
* nanmean: https://github.com/pandas-dev/pandas/blob/af7b0ba461a5b81733afdc7fc816a869b798093d/pandas/core/nanops.py#L352

In our case, because the original dtype is float16, the accumulator is downcasted to float16, for example in nansum():
```python
    dtype_sum = dtype_max
    if is_float_dtype(dtype):
        dtype_sum = dtype
```
(Similar code in `nanmean`.)

However, pandas did not always behave like that. The current behaviour was added in the following commits, to solve other bugs:

* sum(): https://github.com/pandas-dev/pandas/commit/73f25b147119039022a698280767850812209e06
* mean(): https://github.com/pandas-dev/pandas/commit/3896e5eb2f554cb7dcae6ba785c187d9f6ae3fd3

Also, for the ""int"" codepaths, the accumulator is never downcast and always set to float64. It is only for the ""float"" cases that the size of the accumulator is set to be the same as the dtype of the column.

We'd be willing to submit a pull request, but we're not sure what the best fix here would be. Should we just always have a float64 accumulator in these functions for the float cases, instead of downcasting it? If not, what would a good fix look like? 

By the way, things are a bit different in numpy, with the accumulator being set to float64 in more cases, and with the option for users to specify the dtype of the accumulator (and at the same time the output). Having the same option in pandas would have allowed us to at least work around this, by requesting a float64 accumulator. What there a decision made in pandas not to offer a `dtype` argument to sum, etc. like numpy does? Otherwise, we could implement that also in the pull request.

(Cc @chrish42)"
747070929,37969,ENH: bfloat16 when available?,mw66,closed,2020-11-20T01:46:10Z,2020-11-20T02:50:43Z,"Same numpy bfloat16 feature request for pandas:

https://github.com/numpy/numpy/issues/17809

Hi,

I saw NEP 41 Status: | Accepted

https://github.com/numpy/numpy/blob/master/doc/neps/nep-0041-improved-dtype-support.rst

I'm just wondering when will bfloat16 be available in numpy?

For some ML problems, this will reduce the memory requirement by half (or process 2x more data), can we give this high priority?

Thanks.
"
601209402,33593,BUG: TypeError: _reconstruct: First argument must be a sub-type of ndarray,JamesAllingham,closed,2020-04-16T16:07:27Z,2020-11-20T06:40:03Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample

```python
import pandas as pd
pd.read_pickle(""filtered_data.pickle"")

```

#### Problem description

I am able to import the dataframe (which can be downloaded in zipped form from [here](https://javierantoran.github.io/assets/datasets/filtered_flight_data.pickle.zip)) with Pandas version 0.24.2 but get the following error with version 1.0.3:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/homes/jua23/.virtualenvs/arch_uncert/lib/python3.7/site-packages/pandas/io/pickle.py"", line 182, in read_pickle
    return pickle.load(f)
TypeError: _reconstruct: First argument must be a sub-type of ndarray
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-42-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0
Cython           : 0.29.15
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
746888165,37961,CLN: Remove duplicate from MultiIndex.equals,phofl,closed,2020-11-19T20:11:17Z,2020-11-20T08:18:15Z,"- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

really small clean up"
745885383,37942,BUG: Area plot overwrites matplotlib's default share y axes behaviour,lrusnac,closed,2020-11-18T17:59:26Z,2020-11-20T09:38:14Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import matplotlib.pyplot as plt

df0 = pd.DataFrame([{'a': 10, 'b': 12}, {'a': 13, 'b': 9}])
df1 = pd.DataFrame([{'a': 100, 'b': 120}, {'a': 130, 'b': 90}])

fig, (axes_0, axes_1) = plt.subplots(1, 2, figsize=(12, 6), sharey='row')
df0.plot(ax=axes_0, kind='area')
df1.plot(ax=axes_1, kind='area')
```

#### Problem description

The area plot overwrites matplotlib's share y axes behaviour if `sharey` parameter is not passed to `.plot` and as a result the second plot get's cut off.
<img width=""776"" alt=""Screenshot 2020-11-18 at 18 55 02"" src=""https://user-images.githubusercontent.com/1005173/99568355-9afa5c00-29cf-11eb-8267-e3f358c4d6f8.png"">



#### Expected Output

same plot made directly with matplotlib, expected behaviour: 
```python
fig, (axes_0, axes_1) = plt.subplots(1, 2, figsize=(12, 6), sharey='row')
axes_0.stackplot(df0.index, df0['a'], df0['b'])
axes_1.stackplot(df1.index, df1['a'], df1['b'])
```
<img width=""773"" alt=""Screenshot 2020-11-18 at 18 56 54"" src=""https://user-images.githubusercontent.com/1005173/99568558-db59da00-29cf-11eb-916b-c967c33ec6be.png"">
"
495297152,28501,read_json with dtype=False infers Missing Values as None,WillAyd,closed,2019-09-18T15:24:02Z,2020-11-20T14:39:29Z,"Run against master:

```python
In [13]: pd.read_json(""[null]"", dtype=True)
Out[13]:
    0
0 NaN

In [14]: pd.read_json(""[null]"", dtype=False)
Out[14]:
      0
0  None
```

I think the second above is an issue - should probably return `np.nan` instead of `None`"
743037994,37834,BUG: Parse missing values using read_json with dtype=False to NaN instead of None (GH28501),avinashpancham,closed,2020-11-14T16:52:13Z,2020-11-20T14:49:39Z,"
- [x] closes #28501 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
747069730,37968,BUG: IntervalIndex.putmask with datetimelike dtypes,jbrockmendel,closed,2020-11-20T01:42:56Z,2020-11-20T15:40:53Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Implements `NDArrayBackedExtensionArray.putmask` which will hopefully be rolled into `__array_function__` to simplify a bunch of code."
747074345,37970,REF: share more methods in ExtensionIndex ,jbrockmendel,closed,2020-11-20T01:55:54Z,2020-11-20T16:20:05Z,
747340806,37975,TST: add messages to bare pytest raises in pandas/tests/io/pytables/test_timezones.py,marktgraham,closed,2020-11-20T09:43:41Z,2020-11-20T17:47:03Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This PR adds messages to the bare pytest raises in pandas/tests/io/pytables/test_timezones.py. This PR references https://github.com/pandas-dev/pandas/issues/30999."
746270217,37952,TST: add error message match for raise in test_datetimelike.py GH30999,liaoaoyuan97,closed,2020-11-19T05:49:50Z,2020-11-20T19:38:53Z,"Reference #30999

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
655733457,35259,ENH: Basis for a StringDtype using Arrow,xhochy,closed,2020-07-13T10:20:01Z,2020-11-20T20:50:34Z,"- [x] xref #35169
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
747837859,37982,DEPR: how keyword in PeriodIndex.astype,jbrockmendel,closed,2020-11-20T22:34:35Z,2020-11-21T00:00:49Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
705681466,36525,BUG: formatters ignored in to_html,mizuy,closed,2020-09-21T15:02:11Z,2020-11-21T00:01:07Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame(dict(c1=pd.Series(range(3),dtype='int64'),
                       c2=pd.Series(range(3),dtype='Int64'),
                       c3=pd.Series(range(3),dtype='float64'),
                       c4=pd.Series(range(3),dtype='string'),
                       c5=pd.Series([0,1,0],dtype='bool'),
                       c6=pd.Series([0,1,0],dtype='boolean'),
                      ))

formatter = {c:lambda x: f""[{x}]"" for c in df.columns}
display(HTML(df.to_html(formatters=formatter)))
```

  | c1 | c2 | c3 | c4 | c5 | c6
-- | -- | -- | -- | -- | -- | --
_| [0] | 0 | [0.0] | 0 | [False] | False
_| [1] | 1 | [1.0] | 1 | [True] | True
_| [2] | 2 | [2.0] | 2 | [False] | False



#### Problem description

DataFrame.to_html(formatters=formatters) is not working for MaskedArrays (new arrays with missing values)

#### Expected Output

formatters should also be applied to MaskedArrays.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.76-linuxkit
Version          : #1 SMP Tue May 26 11:42:35 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.2
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.6.0.post20200917
Cython           : 0.29.21
pytest           : 6.0.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.3.4
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.8.2
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : 0.2.2
scipy            : 1.5.2
sqlalchemy       : 1.3.19
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : None
numba            : 0.48.0

</details>
"
747724339,37980,DOC: Fix typo,MicaelJarniac,closed,2020-11-20T18:57:30Z,2020-11-21T00:24:45Z,
747903102,37985,CLN: make MultiIndex._shallow_copy signature match other subclasses,jbrockmendel,closed,2020-11-21T02:22:27Z,2020-11-21T21:32:12Z,"annotations

put MultiIndex.symmetric_difference adjacent to other set operations
"
747107825,37971,PERF: IntervalArray.argsort,jbrockmendel,closed,2020-11-20T03:25:43Z,2020-11-21T22:00:25Z,"IntervalIndex.argsort has a more performant implementation (at least for default kwargs).  This just moves that up to IntervalArray.

```
In [2]: idx = pd.IntervalIndex.from_breaks(range(10**4))

In [3]: arr = idx._data

In [4]: %timeit arr.argsort()
24.5 ms ± 458 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)     # <-- master
79 µs ± 1.64 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)   # <-- PR
```"
294869506,19556,Inconsistent MultiIndex behaviour when indexing selects no rows,lauhayden,closed,2018-02-06T18:42:24Z,2020-11-21T22:21:04Z,"```python
mi = pd.MultiIndex.from_arrays([
        np.array(['a', 'a', 'b', 'b']), 
        np.array(['1', '2', '2', '3']), 
        np.array(['alpha', 'beta', 'alpha', 'beta']),
    ], names=['one', 'two', 'three'])
df = pd.DataFrame(np.random.rand(4, 3), index=mi)
df2 = df.loc[('b', '1', slice(None)), :]
print(df2)
print(df2.index)
```
Output
```
Empty DataFrame
Columns: [0, 1, 2]
Index: []
MultiIndex(levels=[['a', 'b'], ['1', '2', '3'], ['alpha', 'beta']],
           labels=[[], [], []],
           names=['one', 'two', 'three'])
```
Indexing `df` results in an empty dataframe in this case. However, if the third level of the MultiIndex is left out (as below), it results in a `KeyError`.
```python
df2 = df.loc[('b', '1'), :]
```

In my opinion, this behaviour is confusing and inconsistent. Preferably, both cases would throw `KeyError`.

This particular behaviour was previously discussed in issue #19543.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.1.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-43-Microsoft
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.22.0
pytest: None
pip: 9.0.1
setuptools: 27.2.0
Cython: None
numpy: 1.14.0
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2017.3
blosc: None
bottleneck: None
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: None
openpyxl: 2.4.9
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0b10
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>"
493105019,28424,ExtensionArray being checked if is instance of collections.abc.Sequence,fpim,closed,2019-09-13T02:09:27Z,2020-11-21T22:21:46Z,"```python
from pandas.api.extensions import ExtensionDtype,ExtensionArray, ExtensionScalarOpsMixin
class FP:

    def __add__(self, other):
        return self + other
    pass
@pd.api.extensions.register_extension_dtype
class FPType(ExtensionDtype):
    name = 'fp'
    type = FP
    @classmethod
    def construct_from_string(cls, string):
        if string == cls.name:
            return cls()
        else:
            raise TypeError(""Cannot construct a '{}' from ""
                            ""'{}'"".format(cls, string))

# collections.abc.Sequence -> data is sequence, _reduce is possible
class FPArray(ExtensionArray,ExtensionScalarOpsMixin,collections.abc.Sequence):
    _dtype = FPType()
    @property
    def dtype(self):
        return self._dtype

    def __init__(self, values):
        values = np.array(values)  # TODO: avoid potential copy
        self.data = values

    def __len__(self):
        return len(self.data)
    def __getitem__(self, *args):
        result = operator.getitem(self.data, *args)
        return result


    def take(self,indexer):
        return self.data.take(indexer)

    def __contains__(self,item):
        return item in self.data

    def __iter__(self):
        for i in self.data:
            yield i

    def __reversed__(self):
        return reversed(self.data)

    def _reduce(self, name, skipna=True, **kwargs):
        return sum(self.data+100)
```

The ExtensionArray  array itself is just for illustration and is not the problem, the problematic part is

```python
arr = FPArray([FP(),FP(),FP(),FP()])
df = pd.DataFrame(arr )
```

when it reached line 444 of pandas/core/frame.py (https://github.com/pandas-dev/pandas/blob/master/pandas/core/frame.py#L444)

```python
            if not isinstance(data, abc.Sequence):
                data = list(data)
```
where `data` is referring to my FPArray, if data is not abc.Sequence, my ExtensionArray will be casted to a list.

Having my ExtensionArray casted to a list mean my ExtensionArray will eventually casted back to a normal Pandas series.

The Solution should be having `ExtensionArray` to be a subclass of `abc.Sequence`.

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.1
numpy            : 1.17.2
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
663867959,35382,BUG: Inconsistent ordering of rows when merging with how=left and how=right ,pandamatic,closed,2020-07-22T15:51:50Z,2020-11-21T22:23:27Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
df = pd.DataFrame({'a': [1, 0, 1]})

df.merge(df, on='a', how='left', sort=False)

	a
0	1
1	1
2	0
3	1
4	1


df.merge(df, on='a', how='right', sort=False)

	a
0	1
1	1
2	1
3	1
4	0

```

#### Problem description

Order of output rows should be the same for how=left and how=right in this case.


#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.181-140.257.amzn2.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : fr_FR.UTF-8
LOCALE           : fr_FR.UTF-8

pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : 0.4.2
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None


</details>
"
747121893,37972,TST: add nullable array frame constructor dtype tests,arw2019,closed,2020-11-20T04:04:52Z,2020-11-21T22:44:09Z,"- [x] closes #28424
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
745138217,37920,"PERF: Introducing HashTables for datatypes with 8,16 and 32 bits",realead,closed,2020-11-17T22:35:22Z,2020-11-21T22:59:07Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Starts work on #33287

Proof of concept for 32bit/16bit hash tables. If works it should become blue print for UInt32/Int16/UInt16/Float32-HashTables.
"
741042022,37764,Bug in loc did not raise KeyError when missing combination with slice(None) was given,phofl,closed,2020-11-11T20:17:04Z,2020-11-21T23:42:22Z,"- [x] closes #19556
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Additional question: When an non existent key is given for ``get_locs`` it raises a ``KeyError``. When just a missing combination of keys is given (``(b,1)`` in the new test), while the individual keys exist, an empty array is returned. Is this the expected behavior?"
747008397,37964,BUG: Bug in setitem raising ValueError when setting more than one column via array,phofl,closed,2020-11-19T23:35:59Z,2020-11-21T23:42:56Z,"- [x] xref #37954
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Did not add whatsnew, cause works on 1.1.4

Array to set has to have more or less rows than columns, otherwise bug won't occur. Wrong values would be set silently here.

This is only a regression on master, not related to any release

cc @jbrockmendel 
"
729127835,37406,BUG: Fix inconsistent ordering between left and right in merge,phofl,closed,2020-10-25T22:18:48Z,2020-11-21T23:50:46Z,"- [x] closes #35382
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Passed sort through for right join too. Did not see a reason why not.

On a related note: I think outer joins return a inconsistent order too.

```
df = pd.DataFrame({'a': [1, 0, 1]})

print(df.merge(df, on='a', how='outer', sort=False))
   a
0  1
1  1
2  1
3  1
4  0
```

instead of
```
   a
0  1
1  1
2  0
3  1
4  1
```

like ``left`` and ``right`` now. Would have to add a ``sort`` keyword to outer join. Should I open an issue for that?
"
747599476,37977,BUG: CategoricalIndex.where nulling out non-categories,jbrockmendel,closed,2020-11-20T15:57:12Z,2020-11-22T01:45:54Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

ATM CategoricalIndex.where has some idiosyncratic behavior:

```
ci = CategoricalIndex([""a"", ""b"", ""c"", ""d""])
mask = np.array([True, False, True, False])

>>> ci.where(mask, 2)
CategoricalIndex(['a', nan, 'c', nan], categories=['a', 'b', 'c', 'd'], ordered=False, dtype='category')
```

This makes that call raise instead.

Index.where is only used in one place in reshape.merge.  Might be worth deprecating+privatizing.
"
733572267,37528,BUG: isin incorrectly casting ints to datetimes,jbrockmendel,closed,2020-10-30T23:26:50Z,2020-11-22T02:43:22Z,"- [x] closes #36621
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

De-duplication in follow-up."
654579456,35207,ENH: raise limit for completion number of columns and warn beyond,bthyreau,closed,2020-07-10T07:41:05Z,2020-11-22T04:57:46Z,"Hi,

Currently the user-completion (ipython ""\<tab\>"") on a dataframe with more than 100 columns will silently ignore some columns, letting an unaware user confused on whether data disapeared.

This is actually documented, and due to an arbitrary limit set to workaround a performance issue ( See #18587 )

Dataframe with more than 100 columns are quite common, so this can potentially affect and suprise many users. Therefore, I suggest to increase that limit to, say, 1000. In any case, it would probably be good to warn the user hitting that limit. 

The attached quickfix raises the limit to 1000 and adds a warning beyond. 
(Note that I didn't experience any completion latency increasing with the axis size, so I'm not sure whether this limit is still relevant in the first place).

"
688482989,35973,ENH: implement timeszones support for read_json(orient='table') and astype() from 'object',attack68,closed,2020-08-29T07:45:26Z,2020-11-22T08:35:29Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Currently timezones raise a `NotImplementedError` when using the `read_json(orient='table')` method.

This PR aims to fix what I believe is a fairly common request (numerous workarounds and questions exist on StackOverflow).

The PR aims to reconstitute DataFrames via json columns with timezones, Index with timezones or MultiIndex with timezones and/or combinations."
747870816,37984,BUG: IntervalArray.astype(categorical_dtype) losing ordered,jbrockmendel,closed,2020-11-21T00:00:09Z,2020-11-22T15:32:00Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
367675209,23040,ValueError: module functions cannot set METH_CLASS or METH_STATIC,joh,closed,2018-10-08T08:01:26Z,2020-11-22T16:39:39Z,"#### Code Sample, a copy-pastable example if possible

```python
import matplotlib.pyplot as plt
import pandas as pd
```
#### Problem description

If I import pyplot before pandas, I get the following error:

```
Traceback (most recent call last):
  File ""bug_pandas.py"", line 2, in <module>
    import pandas as pd
  File ""/home/joh/.local/lib/python3.6/site-packages/pandas/__init__.py"", line 57, in <module>
    from pandas.io.api import *
  File ""/home/joh/.local/lib/python3.6/site-packages/pandas/io/api.py"", line 19, in <module>
    from pandas.io.packers import read_msgpack, to_msgpack
  File ""/home/joh/.local/lib/python3.6/site-packages/pandas/io/packers.py"", line 69, in <module>
    from pandas.util._move import (
ValueError: module functions cannot set METH_CLASS or METH_STATIC
```

If I re-order the imports so that pandas is imported first, the error disappears. Not sure if this is an issue with pyplot or pandas, but the exception seems to come from within pandas.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.6.final.0
python-bits: 64
OS: Linux
OS-release: 4.18.8-041808-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: en_US.UTF-8
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: 3.8.0
pip: 9.0.1
setuptools: 40.4.3
Cython: None
numpy: 1.15.2
scipy: 0.19.1
pyarrow: None
xarray: None
IPython: 5.5.0
sphinx: 1.6.7
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: 1.2.0
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 2.1.1
openpyxl: 2.4.9
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: 4.2.1
bs4: 4.6.0
html5lib: 0.999999999
sqlalchemy: 1.1.11
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
585576091,32896,DataFrame.iloc does not raise for setitem if empty,SaturnFromTitan,closed,2020-03-21T20:26:06Z,2020-11-23T03:18:45Z,"This is a follow-up on [#32886 (comment)](https://github.com/pandas-dev/pandas/pull/32886#discussion_r396024385)

`test_setitem_ndarray_3d` in `pandas/tests/indexing/test_indexing.py` isn't raising for empty index, iloc and DataFrame parametrization.

The mentioned PR added a new `xfail` test for this combination: `test_setitem_ndarray_3d_does_not_fail_for_iloc_empty_dataframe`. The goal of this PR is to remove the `xfail` test and remove the associated `pytest.skip` in `test_setitem_ndarray_3d`."
708584990,36621,BUG: isin - dtype conversions (Timestamp vs ints),asishm,closed,2020-09-25T02:07:51Z,2020-11-23T11:22:20Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---


related: #34125 : could be closed in favor of that if it is too much of a duplicate.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
s = pd.Series(pd.date_range(""jan-01-2013"", ""jan-05-2013""))
s.isin([1356998400000000000])
>>>
0     True
1    False
2    False
3    False
4    False
dtype: bool
```

#### Problem description

Problem arises here because `algos._ensure_dtype(s)` returns an `np.int64` array (that matches the data) and `dtype=<M8[ns]`. 

#### Expected Output

```py
0    False
1    False
2    False
3    False
4    False
dtype: bool
```

"
720374089,37101,ENH: .read_pickle(...) from zip containing hidden OS X/macOS metadata files/folders,ml-evs,closed,2020-10-13T14:53:40Z,2020-11-23T13:18:38Z,"- [x] closes #37098 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This PR allows for `.zip` files created by OS X and macOS that contain `__MACOSX` and `.DS_STORE` metadata folders to be loaded by `read_pickle/read_csv/read_table/read_json` without error. 

~It does **not** work with `pd.read_csv(...)` as in that case the compression is handled in the C code. If this enhancement is deemed desirable then I'm willing to have a go at writing it (and the test is already written).~

(No longer true as of https://github.com/pandas-dev/pandas/pull/36997)

Other similar folders may exist from other operating systems, in which case the list could be pulled out as a constant which could be used in the tests and in the module itself."
748127887,37990,CLN: avoid try/except in Index methods,jbrockmendel,closed,2020-11-22T00:02:18Z,2020-11-23T15:13:49Z,
748329771,38005,TST/REF: collect indexing tests by method,jbrockmendel,closed,2020-11-22T19:29:47Z,2020-11-23T15:14:26Z,
748331367,38006,TST/REF: collect tests from test_multilevel,jbrockmendel,closed,2020-11-22T19:36:02Z,2020-11-23T15:15:35Z,
748316653,38004,REF: ensure_arraylike in algos.isin,jbrockmendel,closed,2020-11-22T18:48:28Z,2020-11-23T15:17:43Z,"This is one of two places where we do not explicitly call ensure_arraylike before ensure_data.  Once I get the other one sorted out, we can annotate _ensure_data and others."
748138481,37993,[WIP] DOC: MultiIndex EX01 errors,xinrong-meng,closed,2020-11-22T01:28:44Z,2020-11-23T17:09:52Z,"- [x] xref #37875, #27977
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
228730732,16360,Column loses category when using .loc for a one row dataframe,nyejon,closed,2017-05-15T14:20:13Z,2020-11-23T18:59:08Z,"
If I convert a list of columns to type 'category'

```python

import pandas as pd 

d1 = {'one' : ['a'],
     'two' : ['a']}

d2 = {'one' : ['a', 'b'],
     'two' : ['a', 'b']}


df1 = pd.DataFrame(d1)
df2 = pd.DataFrame(d2)
df1.loc[: , 'one']= df1['one'].astype('category')
df2.loc[: , 'one'] = df2['one'].astype('category')

print('df1')
print(df1)
print(df1.dtypes)
print('df2')
print(df2)
print(df2.dtypes)

df1
  one two
0   a   a
one    category
two      object
dtype: object
df2
  one two
0   a   a
1   b   b
one    category
two      object
dtype: object

df1['one'] = df1['one'].cat.set_categories(df2['one'].cat.categories)

print('Assigning without loc')
print(df1)
print(df1.dtypes)

Assigning without loc
  one two
0   a   a
one    category
two      object
dtype: object

df1.loc[:, 'one'] = df1['one'].cat.set_categories(df2['one'].cat.categories)
print('Assigning with loc')
print(df1)
print(df1.dtypes)

Assigning with loc
  one two
0   a   a
one    object
two    object
dtype: object

df2.loc[:, 'one'] = df2['one'].cat.set_categories(df2['one'].cat.categories)
print('Assigning df2 with loc')
print(df2)
print(df2.dtypes)

Assigning df2 with loc
  one two
0   a   a
1   b   b
one    category
two      object
dtype: object

```
#### Problem description

I am trying to convert my defined categorical columns to the category type. It works when the dataframe is longer than one row, but if it is only one row it keeps the datatype as object.

With only one row I get the following column outputs:

`df.dtypes`

```
  one two
0   a   a
one    object
two    object
dtype: object
```

#### Expected Output

I would expect the column to be type category even for one row.

`df.dtypes`

```
  one two
0   a   a
one    category
two    object
dtype: object
```



#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.5.3.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.20.1
pytest: No

ne
pip: 9.0.1
setuptools: 35.0.2
Cython: None
numpy: 1.12.1
scipy: 0.18.1
xarray: None
IPython: 5.1.0
sphinx: None
patsy: None
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.0.0
openpyxl: 2.4.1
xlrd: 1.0.0
xlwt: None
xlsxwriter: 0.9.6
lxml: None
bs4: None
html5lib: 0.9999999
sqlalchemy: 1.1.5
pymysql: None
psycopg2: None
jinja2: 2.9.4
s3fs: None
pandas_gbq: None
pandas_datareader: None

</details>
"
748073165,37988,TST: add test to verify column does not lose categorical type when using loc,fgebhart,closed,2020-11-21T18:26:51Z,2020-11-23T20:11:24Z,"- [x] closes #16360
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry (not applicable)

Added a test to verify the behavior described in issue #16360. However, I'm not 100% sure about the location of the test."
548096756,30884,to_sql into MSSQL datetimeoffset column drops offset for DataFrame with one row,gordthompson,open,2020-01-10T14:02:55Z,2020-11-23T21:20:02Z,"#### Problem description

(Prompted by [this Stack Overflow question](https://stackoverflow.com/q/59529424/2144390).)

When using `to_sql` with `mssql+pyodbc` and `fast_executemany=True`, uploading a DataFrame with a single row containing a tz-aware datetime into a `datetimeoffset` column causes the timezone offset to be lost. Doing the same thing with a DataFrame containing more than one row produces the correct result.

```python
from pprint import pprint
import sys
import urllib

import pandas as pd
import sqlalchemy as sa

print(sys.version)
# 3.8.1 (tags/v3.8.1:1b293b6, Dec 18 2019, 22:39:24) [MSC v.1916 32 bit (Intel)]

print(f'SQLAlchemy {sa.__version__}, pandas {pd.__version__}, pyodbc {pyodbc.version}')
# SQLAlchemy 1.3.12, pandas 0.25.3, pyodbc 4.0.28

connection_string = (
    r'DRIVER=ODBC Driver 17 for SQL Server;'
    r'SERVER=localhost,49242;'
    r'DATABASE=myDb;'
    r'Trusted_Connection=Yes;'
    r'UseFMTONLY=Yes;'
)
connection_uri = 'mssql+pyodbc:///?odbc_connect=' + urllib.parse.quote_plus(connection_string)
engine = sa.create_engine(connection_uri, fast_executemany=True)

# test environment
table_name = 'DateTimeOffset_Test'
engine.execute(sa.text(f""DROP TABLE IF EXISTS [{table_name}]""))
engine.execute(sa.text(f""CREATE TABLE [{table_name}] (id int primary key, dto datetimeoffset)""))

# test data
my_tz = datetime.timezone(datetime.timedelta(hours=-7))
dto_value = datetime.datetime(2020, 1, 1, 0, 0, 0, tzinfo=my_tz)
print(dto_value)  # 2020-01-01 00:00:00-07:00
#                                        ^

# test code
num_rows = 1
row_data = [(x, dto_value) for x in range(num_rows)]
df = pd.DataFrame(row_data, columns=['id', 'dto'])
print(df)
#    id                       dto
# 0   0 2020-01-01 00:00:00-07:00
#                            ^ - good
df.to_sql(table_name, engine, if_exists='append', index=False)
result = engine.execute(sa.text(f""SELECT id, CAST(dto as varchar(50)) AS foo FROM [{table_name}]"")).fetchall()
pprint(result)
# [(1, '2020-01-01 00:00:00.0000000 +00:00')]
#                                     ^ - bad
```

Simply changing `num_rows = 1` to `num_rows = 2` produces correct results.

```python
# test code
num_rows = 2
row_data = [(x, dto_value) for x in range(num_rows)]
df = pd.DataFrame(row_data, columns=['id', 'dto'])
print(df)
#    id                       dto
# 0   0 2020-01-01 00:00:00-07:00
# 1   1 2020-01-01 00:00:00-07:00
#                            ^ - good
df.to_sql(table_name, engine, if_exists='append', index=False)
result = engine.execute(sa.text(f""SELECT id, CAST(dto as varchar(50)) AS foo FROM [{table_name}]"")).fetchall()
pprint(result)
# [(0, '2020-01-01 00:00:00.0000000 -07:00'),
#  (1, '2020-01-01 00:00:00.0000000 -07:00')]
#                                     ^ - good
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 32
OS               : Windows
OS-release       : 8.1
machine          : AMD64
processor        : AMD64 Family 18 Model 1 Stepping 0, AuthenticAMD
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 44.0.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : 0.9.3
psycopg2         : 2.8.4 (dt dec pq3 ext)
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.12
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
744536652,37910,BUG: EA inplace add (and other ops) with non-EA arg broken,marco-neumann-by,closed,2020-11-17T08:49:49Z,2020-11-24T00:24:01Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas (`1.0.4`).

- [X] (optional) I have confirmed this bug exists on the master branch of pandas (`87247370c14629c1797fcf3540c6bd93b777a17e`).

---

#### Code Sample, a copy-pastable example

```python
import numpy as np
import pandas as pd

ser1 = pd.Series([1], dtype=""Int64"")
ser2 = pd.Series([1.0], dtype=np.float64)
ser1 += ser2
```

Note that the issue is NOT specific to the integer EA, but happens with all EAs that don't use NumPy dtypes.

#### Problem description

```
Traceback (most recent call last):
  File ""bug.py"", line 6, in <module>
    ser1 += ser2
  File "".../pandas/core/generic.py"", line 11305, in __iadd__
    return self._inplace_method(other, type(self).__add__)  # type: ignore[operator]
  File "".../pandas/core/generic.py"", line 11289, in _inplace_method
    if self.ndim == 1 and result._indexed_same(self) and result.dtype == self.dtype:
TypeError: Cannot interpret 'Int64Dtype()' as a data type
```

This is likely a fallout of #37508. See here:

https://github.com/pandas-dev/pandas/blob/87247370c14629c1797fcf3540c6bd93b777a17e/pandas/core/generic.py#L11283-L11292

There, the result dtype (which is a NumPy dtype in this case) is compared with the EA dtype. This raises the TypeError, because NumPy only allows comparing its dtype objects with ""real"" NumPy dtypes (and EAs don't provide NumPy-compatible dtypes).

#### Expected Output

Pass. I'm pretty sure that worked with `1.0.3` (the mentioned PR in question was backported to the `1.0.x` branch between the `1.0.3` and `1.0.4` release).
"
748428093,38012,REF: Implement isin on DTA instead of DTI,jbrockmendel,closed,2020-11-23T03:00:47Z,2020-11-24T03:07:29Z,
748167884,37995,Removed  period to make it more Consistent,vineethraj510,closed,2020-11-22T04:38:01Z,2020-11-24T03:15:10Z,Removed period as above lines don't have it to make it more consistent
748128296,37991,CLN: always pass ndim to make_block,jbrockmendel,closed,2020-11-22T00:05:41Z,2020-11-24T04:31:52Z,"Found along the way that we can incorrectly put an ndarray[object] into an ExtensionBlock if we're not careful.  This will fix some of those, will make another branch that fixes the rest (with tests)"
747909253,37986,REGR: fix inplace operations for EAs with non-EA arg,arw2019,closed,2020-11-21T03:01:47Z,2020-11-24T09:37:47Z,"- [x] closes #37910 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
492622799,28401,BUG: Constructing a Series from a scalar generally doesn't work for extension types,jschendel,closed,2019-09-12T07:00:36Z,2020-11-24T13:33:06Z,"#### Code Sample, a copy-pastable example if possible
Construction without specifying a dtype results in `object` dtype for `Interval` and `Period`:
```python-traceback
In [2]: pd.Series(pd.Interval(0, 1), index=range(3))
Out[2]: 
0    (0, 1]
1    (0, 1]
2    (0, 1]
dtype: object

In [3]: pd.Series(pd.Period(""2019Q1"", freq=""Q""), index=range(3))
Out[3]: 
0    2019Q1
1    2019Q1
2    2019Q1
dtype: object
```
This looks okay for a tz-aware `Timestamp`:
```python-traceback
In [4]: pd.Series(pd.Timestamp(""2019"", tz=""US/Eastern""), index=range(3))
Out[4]: 
0   2019-01-01 00:00:00-05:00
1   2019-01-01 00:00:00-05:00
2   2019-01-01 00:00:00-05:00
dtype: datetime64[ns, US/Eastern]
```

Specifying a dtype raises for `Interval`, `Period` and tz-aware `Timestamp`:
```python-traceback
In [5]: pd.Series(pd.Interval(0, 1), index=range(3), dtype=pd.IntervalDtype(""int64""))
---------------------------------------------------------------------------
TypeError: IntervalArray(...) must be called with a collection of some kind, (0, 1] was passed

In [6]: pd.Series(pd.Period(""2019Q1"", freq=""Q""), index=range(3), dtype=pd.PeriodDtype(""Q""))
---------------------------------------------------------------------------
ValueError: Buffer has wrong number of dimensions (expected 1, got 0)

In [7]: pd.Series(pd.Timestamp(""2019"", tz=""US/Eastern""), index=range(3), dtype=pd.DatetimeTZDtype(tz=""US/Eastern""))
---------------------------------------------------------------------------
TypeError: 'Timestamp' object is not iterable
```

Both of the above patterns appear to be working fine when using scalars that correspond to non-extensions dtypes (e.g. numeric, tz-naive, timedelta).

#### Problem description
The `Series` constructor is not correctly inferring the dtype from scalar `Interval`/`Period` objects when a dtype isn't specified, and raises for `Interval`/`Period`/tz-aware `Timestamp` when a dtype is specified.

#### Expected Output
I'd expect the `Series` to be constructed with the proper dtype.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 261c3a6673304f2f59f7cb2c1b285c3bdd2ff92e
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.14-041914-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.0+332.g261c3a667
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : 0.29.10
pytest           : 4.6.2
hypothesis       : 4.23.6
sphinx           : 1.8.5
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.3.3
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : 0.3.0
gcsfs            : None
lxml.etree       : 4.3.3
matplotlib       : 3.1.0
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : 0.11.1
pytables         : None
s3fs             : 0.2.1
scipy            : 1.2.1
sqlalchemy       : 1.3.4
tables           : 3.5.2
xarray           : 0.12.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8
</details>
"
674994328,35605,ENH: Styler for specific column styling,attack68,closed,2020-08-07T12:50:24Z,2020-11-24T14:01:40Z,"Pandas Styler already contains the functionality needed for specific `row` and `column` styling via CSS class referencing.

Firstly it can add `table_styles` within the preliminary `<style>` tag, based on given `selectors`.
Secondly it adds specific CSS classes to each element, be it, `heading0` `col1` `row0` etc.

The current solution to add styling to a specific column can be done in two ways:

1) **Using Apply**: this is kind of hinted at in the docs and how most people attack the problem initially.

```
df = pd.DataFrame(data=[[0,1],[1,2],[3,4]], columns=['A', 'B'])
s = df.style.set_uuid('_')
s.apply(lambda x: ['color: red' for v in x], subset=['A'])
s
```

The problem with this approach is that it generates a lot of HTML code, each cell needs an id, and each id is added in the
initial <style> tag as the means of identifying elements. Without manually specifying the UUID (which are typically about 45bytes long) this creates a lot of useless data transmission and slows down websites for large tables:

`'<style  type=""text/css"" >\n#T__row0_col0,#T__row1_col0,#T__row2_col0{\n            color:  red;\n            color:  red;\n            color:  red;\n        }</style><table id=""T__"" ><thead>    <tr>        <th class=""blank level0"" ></th>        <th class=""col_heading level0 col0"" >A</th>        <th class=""col_heading level0 col1"" >B</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=""T__level0_row0"" class=""row_heading level0 row0"" >0</th>\n                        <td id=""T__row0_col0"" class=""data row0 col0"" >0</td>\n                        <td id=""T__row0_col1"" class=""data row0 col1"" >1</td>\n            </tr>\n            <tr>\n                        <th id=""T__level0_row1"" class=""row_heading level0 row1"" >1</th>\n                        <td id=""T__row1_col0"" class=""data row1 col0"" >1</td>\n                        <td id=""T__row1_col1"" class=""data row1 col1"" >2</td>\n            </tr>\n            <tr>\n                        <th id=""T__level0_row2"" class=""row_heading level0 row2"" >2</th>\n                        <td id=""T__row2_col0"" class=""data row2 col0"" >3</td>\n                        <td id=""T__row2_col1"" class=""data row2 col1"" >4</td>\n            </tr>\n    </tbody></table>'`

2) **Using Table Styles**: since `col0` is a class identifier added to each cell relevant to column 'A' one can use table styles to apply directly:

```
df = pd.DataFrame(data=[[0,1],[1,2],[3,4]], columns=['A', 'B'])
s = Styler(df, uuid='_', cell_ids=False)
s.set_table_styles([{'selector': '.col0', 'props': [('color', 'red')]}])
s
```

This might be preferred in some instances since it can reduce the amount of HTML, particularly when combined with `cell_ids` argument. It only outputs the class type and can ignore individual ids, including their UUIDs.

`'<style  type=""text/css"" >
    #T__ .col0 {
          color: red;
    }</style><table id=""T__"" ><thead>    <tr>        <th class=""blank level0"" ></th>        <th class=""col_heading level0 col0"" >A</th>        <th class=""col_heading level0 col1"" >B</th>    </tr></thead><tbody>
                <tr>
                        <th id=""T__level0_row0"" class=""row_heading level0 row0"" >0</th>
                        <td  class=""data row0 col0"" >0</td>
                        <td  class=""data row0 col1"" >1</td>
            </tr>
            <tr>
                        <th id=""T__level0_row1"" class=""row_heading level0 row1"" >1</th>
                        <td  class=""data row1 col0"" >1</td>
                        <td  class=""data row1 col1"" >2</td>
            </tr>
            <tr>
                        <th id=""T__level0_row2"" class=""row_heading level0 row2"" >2</th>
                        <td  class=""data row2 col0"" >3</td>
                        <td  class=""data row2 col1"" >4</td>
            </tr>
    </tbody></table>'`

Therefore the **only missing code** is to put this together in a function which automatically maps the column name to the added CSS class and appends it to the existing table styles. 

#### API breaking implications

This only provides new functionality and will not impact previous versions.
"
748149302,37994,CLN: Use new hashtables in libindex to avoid casting,jbrockmendel,closed,2020-11-22T02:37:53Z,2020-11-24T15:20:33Z,"Small, hard-to-measure perf improvement, since it only makes a difference on the first lookup:

```
In [2]: rng = pd.Index(range(50)).repeat(20).astype(""category"")
In [14]: %timeit rng._engine.clear_mapping(); rng.get_loc(40)
24.8 µs ± 624 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- master
21.3 µs ± 296 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- PR

```"
748380203,38011,TST/REF: collect Index.equals tests,jbrockmendel,closed,2020-11-22T23:47:34Z,2020-11-24T15:24:33Z,"
"
748934956,38019,TST/REF: collect Index setops tests,jbrockmendel,closed,2020-11-23T16:13:30Z,2020-11-24T15:26:27Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
748344395,38008,REF: define DTA._infer_matches,jbrockmendel,closed,2020-11-22T20:36:39Z,2020-11-24T15:27:17Z,
742013812,37801,REGR: SeriesGroupBy where index has a tuple name fails,rhshadrach,closed,2020-11-12T23:52:40Z,2020-11-24T16:55:15Z,"- [x] closes #37755
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The groupby code that calls `Series.__getitem__` catches `KeyError` and `TypeError`. If a tuple is fed into an `Index` that is not a `MultiIndex`, we were raising a `ValueError` but `KeyError` seems more appropriate here and also fixes the regression.

While working on this, I found that we raise a `KeyError` inappropriately if an index of tuples contains duplicates, however this exists back in 1.0.x and so is not part of the regression. I've opened #37800 for this."

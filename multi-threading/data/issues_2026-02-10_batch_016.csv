id,number,title,user,state,created_at,updated_at,body
777370246,38890,CLN: add typing for dtype arg in directories core/indexes and core/strings (GH38808),avinashpancham,closed,2021-01-01T23:22:05Z,2021-01-04T01:11:41Z,"Follow on PR for #38808

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
777541963,38911,DOC: how to revert MultiIndex.to_flat_index,adamjstewart,closed,2021-01-02T23:28:30Z,2021-01-04T01:12:59Z,Took me way too long to figure this out. Hopefully this benefits someone else!
776889896,38847,"CI,STYLE: narrow down ignore-words-list of codespell",chinggg,closed,2020-12-31T09:25:12Z,2021-01-04T01:13:47Z,"- [x] xref #38820 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry  
see Issue #38802, task4"
777576044,38915,REF: de-duplicate code in libparsing/libperiod,jbrockmendel,closed,2021-01-03T05:19:22Z,2021-01-04T01:18:49Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
777698315,38928,BUG: UnknownTimezoneWarning from df.to_datetime(),nabelekt,closed,2021-01-03T19:08:29Z,2021-01-04T01:22:05Z,"I have `test.csv`:
```
datetime
Oct 19, 2020 06:58:20 PM EDT
Oct 19, 2020 04:45:33 PM EDT
```

and `test.py`:
```
import pandas as pd
from datetime import datetime

input_file_path = 'test.csv'

df = pd.read_csv(input_file_path, delimiter=';')

print(df)

df['datetime'] = pd.to_datetime(df['datetime'])

print(df)
```

Running `python test.py` results in:
```
                       datetime
0  Oct 19, 2020 06:58:20 PM EDT
1  Oct 19, 2020 04:45:33 PM EDT
/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/dateutil/parser/_parser.py:1218: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.
  category=UnknownTimezoneWarning)
             datetime
0 2020-10-19 18:58:20
1 2020-10-19 16:45:33
```

The `UnknownTimezoneWarning` from `dateutil` is what I am concerned about. I don't believe there is currently a way to pass an argument to `panda`'s `to_datetime()` to avoid this. Am I missing something or does this need to be addressed?

Running with `pandas` v1.2.0, `python` v3.7.3."
777691156,38925,ENH: Add support to import optional submodule and specify different min_version than default,lithomas1,closed,2021-01-03T18:23:27Z,2021-01-04T03:38:53Z,"- [ ] closes #38888
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @jreback, @arw2019 "
777381652,38894,TST/CLN: deduplicate troublesome rank values,mzeitlin11,closed,2021-01-02T01:11:31Z,2021-01-04T03:55:08Z,"xref https://github.com/pandas-dev/pandas/pull/38681#discussion_r548554225
"
774736717,38696,Updated PULL_REQUEST_TEMPLATE.md,BobinMathew,closed,2020-12-25T13:59:23Z,2021-01-04T04:45:57Z,"- [ ] closes #38696
- [ ] tests added / passed
- [ ] passes `pre-commit run --from-ref=upstream/master --to-ref=HEAD --all-files`
- [ ] whatsnew entry

"
777737194,38933,DOC: improve shared content between comparison pages,afeld,closed,2021-01-03T23:31:09Z,2021-01-04T04:53:19Z,"This pull request does a few things between the SAS and Stata pages, in separate commits:

- Makes the headings match, where it makes sense for them to
- Create more shared includes, as a follow-up to https://github.com/pandas-dev/pandas/pull/38887
- Improves some wording and ensures more methods are linked in the comparison includes

The motivation here is that I'm working on adding the other sections to the Comparison to Spreadsheets page, and want to ensure they're consistent.

---

- [ ] ~~closes #xxxx~~
- [x] tests added / passed
- [ ] ~~passes `black pandas`~~
- [ ] ~~passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`~~
- [ ] ~~whatsnew entry~~
"
729155149,37409,"TST: rename generic/, base/ test directories",jbrockmendel,closed,2020-10-26T00:34:11Z,2021-01-04T06:19:33Z,"base -> index_or_series
generic -> frame_or_series"
727002182,37330,BUG: IntervalIndex.take without fill_value,jbrockmendel,closed,2020-10-22T02:56:01Z,2021-01-04T06:19:44Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

`IntervalIndex.take([0, 1, -1])` without explicitly passing fill_value should not fill the -1 as NA.

Fixing this allows us to use the base class `take` for `IntervalIndex.take` (the original motivation)"
596986419,33415,WIP: Timestamp/DTA match stdlib tzawareness-compat behavior,jbrockmendel,closed,2020-04-09T03:01:58Z,2021-01-04T06:20:18Z,"- [x] closes #28507
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I'm not super-happy with the implementation adding `op` kwarg to check_compatible_with and assert_tzawareness_compat, looking for something cleaner."
567830828,32118,"REF: make pd._testing directory, split out asserters",jbrockmendel,closed,2020-02-19T20:37:12Z,2021-01-04T06:20:41Z,"Scattered throughout the tests we have other helpers like assert_block_equal that probably belong in `tm`.  Before trying to collect all of these, it makes sense to split this already-cumbersome module."
777364834,38887,"DOC: create shared includes for comparison docs, take III",afeld,closed,2021-01-01T22:35:15Z,2021-01-04T08:43:07Z,"From original pull request (https://github.com/pandas-dev/pandas/pull/38771):

> This will help ensure consistency between the examples.

- [ ] ~~closes #xxxx~~
- [x] tests added / passed
- [ ] ~~passes `black pandas`~~
- [ ] ~~passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`~~
- [ ] ~~whatsnew entry~~
"
758869091,38347,Extend check namespace usage,kevinetienne,closed,2020-12-07T21:36:39Z,2021-01-04T09:32:19Z,"- [ ] xref #38093
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

TLDR: This code changes might be not in a mergeable state as false positives remain when updating the pattern.

The change is a bit bigger to what is described in #38093 as more files needed to be fixed, which is fine I think.

The stack displaying the files to fix might be limited to a few items (9 items in my case) when running the command `pre-commit run inconsistent-namespace-usage --all-files`. Every time a set of files was fixed a few others appeared which is why I ended fixing up more files and why my list of false positive is not exhaustive.

I have committed the change to the PATTERN in a separate commit as we have false positives. Such as the following files which contain:
 1. a similar function in the numpy package (ex: np.array see https://github.com/pandas-dev/pandas/blob/862cd05df4452592a99dd1a4fa10ce8cfb3766f7/pandas/tests/test_strings.py#L188)
 2. a similar function in the pyarrow package (ex: pyarrow.array see https://github.com/pandas-dev/pandas/blob/862cd05df4452592a99dd1a4fa10ce8cfb3766f7/pandas/tests/io/test_parquet.py#L839)
 3. a function can be called on dataframe or a serie (ex: `df.isna` and `aggr.isna` see https://github.com/pandas-dev/pandas/blob/862cd05df4452592a99dd1a4fa10ce8cfb3766f7/pandas/tests/groupby/test_categorical.py#L528 and https://github.com/pandas-dev/pandas/blob/862cd05df4452592a99dd1a4fa10ce8cfb3766f7/pandas/tests/groupby/test_categorical.py#L1570
 4. or a function is called with any prefixes (see the few calls to `factorize` in https://github.com/pandas-dev/pandas/blob/862cd05df4452592a99dd1a4fa10ce8cfb3766f7/pandas/tests/test_algos.py#L52)
 5. `eval` vs `pd.eval` (see https://github.com/pandas-dev/pandas/blob/862cd05df4452592a99dd1a4fa10ce8cfb3766f7/pandas/tests/computation/test_eval.py)

I'm probably missing a few other cases but 1) 2) and 3) can be added in the negative lookahead (ignore function/methods which start with np/pyarrow/df). However the regex might grow bigger and bigger. 5) can be ignored entirely either within the regex or with an exclusion list.

Let me know you're thinking as I see three solutions:
 - update the regex and maybe add an exclusion list (for case like eval) and see where that leads
 - remove last commit for PATTERN and create another ticket to address the other cases
 - close the ticket and find/investigate another solution (ex: separate class vs function check/check import instead...)
"
777793198,38935,DOC: remove use of head() in the comparison docs,afeld,closed,2021-01-04T03:43:50Z,2021-01-04T13:21:20Z,"This helps to clarify the examples by removing code that isn't relevant. Added a dedicated section to the SAS, SQL, and Stata pages.

This builds on https://github.com/pandas-dev/pandas/pull/38933; ~~will rebase and mark as ready for review once that's merged. In the meantime, the last commit is the one that can be reviewed.~~ Thanks!

- [ ] ~~closes #xxxx~~
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] ~~whatsnew entry~~
"
777790720,38934,ENH: Improve numerical stability for groupby.mean and groupby.cumsum,phofl,closed,2021-01-04T03:34:55Z,2021-01-04T13:26:44Z,"- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
772264135,38614,BUG: disallow scalar in CategoricalIndex construtor,jbrockmendel,closed,2020-12-21T15:46:02Z,2021-01-04T13:29:49Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
777928124,38941,DOC: minor tweaks to formatting on SQL comparison page,afeld,closed,2021-01-04T08:41:33Z,2021-01-04T13:33:16Z,"Adding code formatting, missing punctuation, etc. No changes of substance.

- [ ] ~~closes #xxxx~~
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] ~~whatsnew entry~~"
777519516,38908,BUG: pandas 1.2.0 df.rolling().aggregate('skew') modified original data,qinxuye,closed,2021-01-02T20:22:25Z,2021-01-04T13:37:14Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
In [1]: import pandas as pd

In [2]: pd.__version__
Out[2]: '1.2.0'

In [3]: import numpy as np

In [4]: df = pd.DataFrame({'a': [1., np.nan, 5.], 'b': [.1, .5, 1.0]})

In [5]: df
Out[5]: 
     a    b
0  1.0  0.1
1  NaN  0.5
2  5.0  1.0

In [6]: df.rolling(4).aggregate('skew')
Out[6]: 
    a   b
0 NaN NaN
1 NaN NaN
2 NaN NaN

In [7]: df
Out[7]: 
     a    b
0 -2.0 -0.9
1  NaN -0.5
2  2.0  0.0

```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

For pandas 1.2.0, `df.rolling().aggregate('skew')` modified original data. And I verified that `kurt` has the same problem.

#### Expected Output

Original data should not be modified, the behavior works well in pandas 1.1.5.

```
In [1]: import pandas as pd                                                     

In [3]: import numpy as np                                                      

In [4]: df = pd.DataFrame({'a': [1., np.nan, 5.], 'b': [.1, .5, 1.0]})          

In [5]: pd.__version__                                                          
Out[5]: '1.1.5'

In [6]: df.rolling(4).aggregate('skew')                                         
Out[6]: 
    a   b
0 NaN NaN
1 NaN NaN
2 NaN NaN

In [7]: df                                                                      
Out[7]: 
     a    b
0  1.0  0.1
1  NaN  0.5
2  5.0  1.0
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

In [8]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.2.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Oct 29 22:56:45 PDT 2020; root:xnu-6153.141.2.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : zh_CN.UTF-8
LOCALE           : zh_CN.UTF-8

pandas           : 1.2.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : 0.29.16
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : 2.7.2
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
778038878,38942,Backport PR #38893: doc fix for testing.assert_series_equal check_freq arg,simonjayhawkins,closed,2021-01-04T11:47:48Z,2021-01-04T13:47:10Z,Backport PR #38893
777539772,38909,BUG: Fixed regression in rolling.skew and rolling.kurt modifying object,phofl,closed,2021-01-02T23:07:19Z,2021-01-04T14:04:50Z,"- [x] closes #38908
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Alternative would be np.copy before releasing gil. In this case we would have to touch values_copy twice in case of ``min_val - mean_val > -1e5``"
778101678,38945,Backport PR #38909 on branch 1.2.x (BUG: Fixed regression in rolling.skew and rolling.kurt modifying object),meeseeksmachine,closed,2021-01-04T13:39:05Z,2021-01-04T15:23:01Z,Backport PR #38909: BUG: Fixed regression in rolling.skew and rolling.kurt modifying object
777695520,38927,TST: stricten xfails,jbrockmendel,closed,2021-01-03T18:51:51Z,2021-01-04T16:22:59Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
774004603,38664,CLN: Rolling helper attributes,mroeschke,closed,2020-12-23T19:36:27Z,2021-01-04T16:32:07Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

`is_datetimelike` was just an internal check and can be in-lined 

`_on` can just be calculated once in the init"
777371042,38891,Backport PR #38471 on branch 1.2.x (DOC: fixes for assert_frame_equal check_freq argument),kylekeppler,closed,2021-01-01T23:29:45Z,2021-01-04T16:49:22Z,"Backport PR #38471
"
777379295,38893,doc fix for testing.assert_series_equal check_freq arg,kylekeppler,closed,2021-01-02T00:46:40Z,2021-01-04T16:50:08Z,"This fixes `check_freq` arg docs for `assert_series_equal` like #38471 did for `assert_frame_equal`.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
773262580,38641,ENH/ERR: More consistent error reporting with invalid win_type in rolling,mroeschke,closed,2020-12-22T21:24:52Z,2021-01-04T17:50:11Z,"- [x] closes #15969
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Also in-lining methods that were only used once and moving a BaseIndexer test that was using `win_type`"
306942661,20421,API: Allow MultiIndex.rename() to accept a dict as an argument,Dr-Irv,closed,2018-03-20T16:30:42Z,2021-01-04T18:55:42Z,"#### Code Sample, a copy-pastable example if possible

```python
In [5]: mi=pd.MultiIndex.from_product([list('abc'), list('xy'), [1,2]],names=['abc','xy','num'])

In [6]: mi.names
Out[6]: FrozenList(['abc', 'xy', 'num'])

In [7]: mi.rename(['def','v'], level=[0,2]).names
Out[7]: FrozenList(['def', 'xy', 'v'])


In [8]: mi.rename({'xy' : 'foo'})
---------------------------------------------------------------------------
# Traceback omitted

ValueError: Length of names must match number of levels in MultiIndex.
```
#### Problem description

In coordination with how `DataFrame.rename()` supports using a dictionary to rename columns, I think it would be worthwhile to allow `MultiIndex.rename()` to also support a dictionary argument.  The value of this is that you then don't need to know the level numbers of each of the index names.

If implemented, this would also help with issue #17334, since we would then implement `MultiIndex.rename` with its own docstring.


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: 01882ba5b4c21b0caf2e6b9279fb01967aa5d650
python: 3.6.4.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.23.0.dev0+657.g01882ba5b
pytest: 3.4.0
pip: 9.0.1
setuptools: 38.5.1
Cython: 0.25.1
numpy: 1.14.1
scipy: 1.0.0
pyarrow: 0.8.0
xarray: None
IPython: 6.2.1
sphinx: 1.7.1
patsy: 0.5.0
dateutil: 2.6.1
pytz: 2018.3
blosc: 1.5.1
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 2.2.0
openpyxl: 2.5.0
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.2
lxml: 4.1.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.5
pymysql: 0.8.0
psycopg2: None
jinja2: 2.10
s3fs: 0.1.3
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
752525272,38126,ENH: Implement dict-like support for rename and set_names in MultiIndex,phofl,closed,2020-11-28T01:23:22Z,2021-01-04T18:56:29Z,"- [x] closes #20421
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I tried my hand in adding support for dict-like renamings for MultiIndexes. This seemed as the most straight forward way to me, because we do not have to add complex logic as long as we can cast the dict-like objects to regular configurations.

We probably must add a docstring to MultiIndex.rename now?"
778272340,38950,REF: de-duplicate tslibs.fields,jbrockmendel,closed,2021-01-04T18:19:52Z,2021-01-04T19:30:14Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
777717069,38929,TST: Replace pytest.xfail,rhshadrach,closed,2021-01-03T21:10:21Z,2021-01-04T21:05:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Part of #38902."
776042643,38785,BUG: Series.apply with DatetimeTZDtype passes DatetimeIndex to UDF,mikeronayne,closed,2020-12-29T19:03:54Z,2021-01-04T21:16:22Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Problem description

Calling `.apply` on a `Series` of `DatetimeTZDtype` passes the data as a `DatetimeIndex` to the user-supplied function, instead of each value of the `Series` individually.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

def f(val):
    print(f""Function received: {val}\n"")
    return val

s = pd.Series([pd.Timestamp.utcnow()])

new_s = s.apply(f)

print(new_s)
```

#### Actual Output

```
Function received: DatetimeIndex(['2020-12-29 18:48:48.688326+00:00'], dtype='datetime64[ns, UTC]', freq=None)

0   2020-12-29 18:48:48.688326+00:00
dtype: datetime64[ns, UTC]
```

Interestingly, if `f` returns `None`, then `apply` will first pass the `Series` as a `DatetimeIndex`, then it passes the actual `Timestamp`. The return value of that `apply` is still a length-1 `Series` (whose one value is `None`), not a length-2 `Series` with two `None` values.

#### Expected Output

```
Function received: 2020-12-29 18:48:48.688326+00:00

0   2020-12-29 18:48:48.688326+00:00
dtype: datetime64[ns, UTC]
```

**NOTE: If the `Series` is created with `pd.Timestamp.now()` (not UTC), then we get the expected behavior.**

#### Why this is a problem

Users write their `apply` functions with the expectation that it will receive the values of the `Series` one by one. This leads to errors when operations are applied to the received value that do not work on a `DatetimeIndex`, but would work on a `Timestamp`.

Additionally, users do not expect different behavior from the `apply` function based on whether their data has time zone information or not.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.2.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200814
Cython           : None
pytest           : 6.0.1
hypothesis       : None
sphinx           : 3.3.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
778296460,38954,CLN: Unify number recognition tests for all parsers,phofl,closed,2021-01-04T19:03:38Z,2021-01-04T23:23:01Z,"- [x] closes #38926
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

Minor regex improvements.
Is a fixture the right thing to do here? 
"
777694150,38926,CLN: Unify number recognition tests in read_csv for all parsers,phofl,closed,2021-01-03T18:42:54Z,2021-01-04T23:24:44Z,"Follow up for #38420 
We should parametrize over all parsers for the test sets to ensure a consistent behavior
"
775086717,38727,BUG: pd.read_json May Not Maintain Numeric String Index,theoniko,closed,2020-12-27T18:29:22Z,2021-01-04T23:29:00Z,"- [x] closes #28556
- [x] tests added / passed
- [ ] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
496589845,28556,pd.read_json May Not Maintain Numeric String Index,WillAyd,closed,2019-09-21T00:14:06Z,2021-01-04T23:29:00Z,"```python
>>> df = pd.DataFrame(range(3), index=list(""123""))
>>> df.to_json(orient=""split"")
'{""columns"":[0],""index"":[""1"",""2"",""3""],""data"":[[0],[1],[2]]}'
>>> pd.read_json(df.to_json(orient=""split""), orient=""split"").index
Int64Index([1, 2, 3], dtype='int64')
```

Note that the string nature of the values should be preserved via roundtrip here, but ends up being lossy anyway. Noted during refactor of #28510"
778425379,38958,"Backport PR #38957 on branch 1.2.x (DOC: move API breaking ""check_freq"" section from v1.2.1rst to v1.1.0.rst)",meeseeksmachine,closed,2021-01-04T22:45:26Z,2021-01-05T00:11:03Z,"Backport PR #38957: DOC: move API breaking ""check_freq"" section from v1.2.1rst to v1.1.0.rst"
724229390,37236,CLN: core/dtypes/cast.py::maybe_infer_to_datetimelike,arw2019,closed,2020-10-19T03:29:23Z,2021-01-05T01:17:46Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
778425594,38959,TYP/CLN: assorted cleanups,jbrockmendel,closed,2021-01-04T22:45:54Z,2021-01-05T01:18:03Z,
777727674,38932,BUG: rank_2d raising with mixed dtypes,mzeitlin11,closed,2021-01-03T22:21:58Z,2021-01-05T01:30:14Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
777363615,38886,CLN: add typing for dtype arg in core/arrays (GH38808),avinashpancham,closed,2021-01-01T22:25:32Z,2021-01-05T02:02:14Z,"Follow on PR for #38808

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
621720948,34274,BUG: small error in pandas/io/pytables.py,SilasK,closed,2020-05-20T12:26:47Z,2021-01-05T02:22:33Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here


df.to_hdf('file.h5','name', format='table')

~/miniconda3/lib/python3.6/site-packages/pandas/io/pytables.py in _maybe_convert_for_string_atom(name, block, existing_col, min_itemsize, nan_rep, encoding, errors)
   4798         # we cannot serialize this data, so report an exception on a column
   4799         # by column basis
-> 4800         for i in range(len(block.shape[0])):
   4801 
   4802             col = block.iget(i)

TypeError: object of type 'int' has no len()

```




#### Problem description

I don't know why exactly pandas cannot serialize my data frame with a multi-index.
But the above error prevents the real error message to be printed.



#### Expected Output

Error message. 

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 16.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 46.1.3.post20200325
Cython           : 0.29.18
pytest           : 5.4.1
hypothesis       : 5.15.0
sphinx           : 3.0.3
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 3.8.0
html5lib         : 1.0.1
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.14.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 3.8.0
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.8
numba            : 0.48.0


</details>
"
777624174,38919,BUG: fix the bad error raised by HDFStore.put(),1MLightyears,closed,2021-01-03T12:00:19Z,2021-01-05T02:22:38Z,"- [x] closes #34274
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
*P.S. something was wrong with `git diff upstream/master` so I directly ran `flake8 ./pandas/io/pytables.py` as it's the only file changed*

------
I was running into the same problem as #34274 and found where the error is. But (as I've just started using pandas for several days:sweet_smile:) I have few knowledge about pandas, so maybe I didn't make the full use of pandas' components.

As seen in #34274, if a `DataFrame` contains non-string elements and is about to be written into an HDF5 file by `HDFStore.put()`, a `TypeError: object of type 'int' has no len()` error is raised.

But it's not the right ""error"" expected. `HDFStore.put()` can't serialize some types of element, `so report an exception on a column by column basis` is actually needed.

This commit fixes this, now it raises `TypeError: Cannot serialize the column [{column_No}] because its data contents are not string but [{non_string_type}] object dtype` as expected."
778456574,38961,REF: move functions out of _testing/__init__,jbrockmendel,closed,2021-01-04T23:52:11Z,2021-01-05T02:26:32Z,
590418279,33146,"Error in slicing by datetime January 1, 2020",ivan7707,closed,2020-03-30T16:18:10Z,2021-01-05T02:27:21Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

dt = {'No': {pd.Timestamp('2020-01-01 00:00:00'): 123,
  pd.Timestamp('1900-01-01 00:00:00'): 345,
  pd.Timestamp('2017-04-18 00:00:00'): 946,
  pd.Timestamp('2020-01-02 00:00:00'): 940 },
 'CB': {pd.Timestamp('2020-01-01 00:00:00'): 'Sc',
  pd.Timestamp('1900-01-01 00:00:00'): 'Dr',
  pd.Timestamp('2017-04-18 00:00:00'): 'St',
  pd.Timestamp('2020-01-02 00:00:00'): 'Sc'}}

df = pd.DataFrame(dt)


# **only January 1 does not work properly**
print(df['2019-12-31':])
print(df['2020-01-01':])
print(df['2020-01-02':])


# **All work properly**
print(df[df.index >='2019-12-31'])
print(df[df.index >='2020-01-01'])
print(df[df.index >='2020-01-02'])

```
#### Problem description
Slicing by datetime index on January 1, 2020 returns the whole DataFrame

**print(df['2020-01-01':])**

Shows the whole DataFrame

#### Expected Output
# only January 1 does not work properly


**print(df['2020-01-01':])**

incorrectly shows the whole DataFrame

                   No  CB
2020-01-01  123  Sc
1900-01-01  345  Dr
2017-04-18  946  St
2020-01-02  946  Sc


**print(df['2020-01-02':])**

No  CB
2020-01-02  946  Sc


# All work properly
**print(df[df.index >='2019-12-31'])**

No  CB
2020-01-01  123  Sc
2020-01-02  946  Sc


**print(df[df.index >='2020-01-01'])**


#works properly
                     No  CB
2020-01-01  123  Sc
2020-01-02  946  Sc

**print(df[df.index >='2020-01-02'])**

                     No  CB
2020-01-01  123  Sc
2020-01-02  946  Sc


#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.2.final.0
python-bits      : 32
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2018.9
dateutil         : 2.8.0
pip              : 20.0.2
setuptools       : 40.6.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 7.4.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.0.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
732308993,37482,ENH: Allow tz-aware origin parameter in pandas.to_datetime,venaturum,closed,2020-10-29T13:21:15Z,2021-01-05T04:57:34Z,"#### Is your feature request related to a problem?

I wish I could use pandas to map back and forth between tz-aware timestamps and floats.

This has been raised before
[#16842](https://github.com/pandas-dev/pandas/issues/16842)

however it was argued that the incumbent behaviour did not need altering, but I'd like to plead my case below.

#### Describe the solution you'd like

Currently if *origin* parameter in pd.to_datetime is tz-aware an exception is produced;
*ValueError: origin offset ..... must be tz-naive*

I believe if a tz-aware origin is used then the result of pd.to_datetime should be a tz-aware timestamp.

#### API breaking implications

I believe an implementation is possible which does not break the api, where the timezone is extracted from the origin parameter (and it could be None) and used to localize other timestamps in the code where necessary.

#### Describe alternatives you've considered

I have considered writing my own to_datetime to accept timezone aware origins, or perhaps using tz_convert as a mediator between the mapping, however I would still argue the default behaviour of to_datetime is not ideal.

#### Additional context

Consider the following code, and for context in the Australia/Sydney timezone clocks were wound forward an hour at 2am on the 4th of October 2020.  

```python
import pandas as pd

tz = pytz.timezone('Australia/Sydney')
origin = pd.Timestamp('2020-10-04', tz=tz)
test_date_1 = pd.Timestamp('2020-10-04 1:00', tz=tz)
test_date_2 = pd.Timestamp('2020-10-04 3:00', tz=tz)

print(f'There is {(test_date_1 - origin).total_seconds()/3600} hours from origin to {test_date_1}')
print(f'There is {(test_date_2 - origin).total_seconds()/3600} hours from origin to {test_date_2}')

```
The code will correctly calculate the time delta, with an origin set to the start of the day:
*There is 1.0 hours from origin to 2020-10-04 01:00:00+10:00*
*There is 2.0 hours from origin to 2020-10-04 03:00:00+11:00*

How can I map the number 2 back to test_date_2?

This code
```python
pd.to_datetime(2, unit='h', origin=origin)
```
produces
*ValueError: origin offset 2020-10-04 00:00:00+10:00 must be tz-naive*

This code
```python
pd.to_datetime(2, unit='h', origin=origin.tz_localize(None))
```
produces a tz-naive Timestamp: *Timestamp('2020-10-04 02:00:00')* 

Trying to localize it of course does not work, and raises an exception
```python
pd.to_datetime(2, unit='h', origin=origin.tz_localize(None)).tz_localize(tz)
```
*NonExistentTimeError: 2020-10-04 02:00:00*

The answer to ""What time is 2hrs past midnight on the 4th of October 2020 in Sydney. Australia"" is not ambiguous.  It has an answer and I believe pandas should be able to accomodate this."
777579674,38916,Remove Python2 numeric relics,eumiro,closed,2021-01-03T05:52:11Z,2021-01-05T07:10:41Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Modernize the code to Python 3 by removing Python 2 numeric hacks, such as:

- `a / b` of two integers now always returns a float, so no need to convert `a` and/or `b` to float (or to use float literals)
- `a // b` of two integers returns an integer, use it instead of `int(a / b)`
- `math.ceil`, `math.floor`, and `round` return integer, so no need to convert the result to integer
- `1e6` is a float, converting it to `int(1e6)` is slower than `10 ** 6` (or `1_000_000`)
"
748372952,38010,BUG: loc returning wrong elements for non-monotonic DatetimeIndex,phofl,closed,2020-11-22T23:07:12Z,2021-01-05T08:49:35Z,"- [x] closes #33146
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

That one was tricky. When only one existing key was given or both string parts exist, ``Index.slice_indexer`` is not raising KeyError but selecting a lof of erroneous values"
702598333,36395,BUG: indexing with DataFrame with nullable boolean dtype,lgelmi,closed,2020-09-16T09:16:22Z,2021-01-05T09:09:31Z,"Premise: I tried to look for similar issues (closed or not), but (to my surprise) I couldn't find any.

#### Problem

I often filter my data through comparison with given values, this breaks using pd.NA.

```python
import pandas as pd
from numpy import nan
from pandas import NA

NA == 1, nan == 1
>> (<NA>, False)

NA != 1, nan != 1
>> (<NA>, True)

NA > 1, nan > 1
>> (<NA>, False)

NA < 1, nan < 1
>> (<NA>, False)
```
Which implies:

```python
import pandas as pd

df = pd.DataFrame([1,2,NA], dtype=""Int8"")
df == 2
>>
       0
0     1
1     2
2  <NA>
```
and, even worse: 

```python
df[df == 2]
>>
Traceback (most recent call last):
  [ ... ]
  File ""../lib/python3.7/site-packages/pandas/core/internals/blocks.py"", line 2861, in _extract_bool_array
    assert mask.dtype == bool, mask.dtype
AssertionError: object
```

As you surely know, this would have worked flawlessly with numpy.nan.

#### The solution I'd like

pd.NA and numpy.nan should behave the same, especially in regards of comparisons.

#### API breaking implications

As far as I know pd.NA has been declared experimental, so this should not break much, but may greatly simplify the transition to it for performance and type consistency purposes (which are, from my point of view, the main advantages).

#### Describe alternatives you've considered

I'm not entirely sure pd.NA should be the same as nan in all regards. The documentation itself does not imply it at all. 
That being said, I'd still like to be able to filter my data without so much pain. :)  "
694560851,36175,BUG: read_excel for ods files raising UnboundLocalError in certain cases,asishm,closed,2020-09-06T22:58:37Z,2021-01-05T09:37:02Z,"- [x] closes #36122 
- [x] closes #35802
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Need some guidance on tests here. Do we need tests for other file formats here as well? Also unsure about naming conventions.

The primary bug in this case (apart from the indentation problem in the original code) is that the code was ignoring cases where the XML of the cell had multiple child nodes that were not spaces

reverted implementation from #33233 incorporating part of its modifications."
777891463,38940,⬆️ UPGRADE: Autoupdate pre-commit config,github-actions[bot],closed,2021-01-04T07:40:09Z,2021-01-05T09:41:33Z,"<!-- START pr-commits -->
<!-- END pr-commits -->

## Base PullRequest

default branch (https://github.com/pandas-dev/pandas/tree/master)

## Command results
<details>
<summary>Details: </summary>

<details>
<summary><em>add path</em></summary>

```Shell
/home/runner/work/_actions/technote-space/create-pr-action/v2/node_modules/npm-check-updates/bin
```



</details>
<details>
<summary><em>pip install pre-commit</em></summary>

```Shell
Collecting pre-commit
  Downloading pre_commit-2.9.3-py2.py3-none-any.whl (184 kB)
Collecting cfgv>=2.0.0
  Using cached cfgv-3.2.0-py2.py3-none-any.whl (7.3 kB)
Collecting identify>=1.0.0
  Downloading identify-1.5.11-py2.py3-none-any.whl (97 kB)
Collecting nodeenv>=0.11.1
  Using cached nodeenv-1.5.0-py2.py3-none-any.whl (21 kB)
Collecting pyyaml>=5.1
  Using cached PyYAML-5.3.1-cp39-cp39-linux_x86_64.whl
Collecting toml
  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)
Collecting virtualenv>=20.0.8
  Downloading virtualenv-20.2.2-py2.py3-none-any.whl (5.7 MB)
Collecting appdirs<2,>=1.4.3
  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)
Collecting distlib<1,>=0.3.1
  Using cached distlib-0.3.1-py2.py3-none-any.whl (335 kB)
Collecting filelock<4,>=3.0.0
  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)
Collecting six<2,>=1.9.0
  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)
Installing collected packages: six, filelock, distlib, appdirs, virtualenv, toml, pyyaml, nodeenv, identify, cfgv, pre-commit
Successfully installed appdirs-1.4.4 cfgv-3.2.0 distlib-0.3.1 filelock-3.0.12 identify-1.5.11 nodeenv-1.5.0 pre-commit-2.9.3 pyyaml-5.3.1 six-1.15.0 toml-0.10.2 virtualenv-20.2.2
```

### stderr:

```Shell
WARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.
You should consider upgrading via the '/opt/hostedtoolcache/Python/3.9.1/x64/bin/python -m pip install --upgrade pip' command.
```

</details>
<details>
<summary><em>pre-commit autoupdate || (exit 0);</em></summary>

```Shell
Updating https://github.com/python/black ... already up to date.
Updating https://gitlab.com/pycqa/flake8 ... already up to date.
Updating https://github.com/PyCQA/isort ... [INFO] Initializing environment for https://github.com/PyCQA/isort.
updating 5.6.4 -> 5.7.0.
Updating https://github.com/asottile/pyupgrade ... [INFO] Initializing environment for https://github.com/asottile/pyupgrade.
already up to date.
Updating https://github.com/pre-commit/pygrep-hooks ... already up to date.
Updating https://github.com/asottile/yesqa ... already up to date.
Updating https://github.com/pre-commit/pre-commit-hooks ... [INFO] Initializing environment for https://github.com/pre-commit/pre-commit-hooks.
already up to date.
Updating https://github.com/codespell-project/codespell ... [INFO] Initializing environment for https://github.com/codespell-project/codespell.
already up to date.
```



</details>
<details>
<summary><em>pre-commit run -a || (exit 0);</em></summary>

```Shell
[INFO] Installing environment for https://github.com/python/black.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://gitlab.com/pycqa/flake8.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://gitlab.com/pycqa/flake8.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/PyCQA/isort.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/asottile/pyupgrade.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for local.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for local.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for local.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/asottile/yesqa.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/pre-commit/pre-commit-hooks.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/codespell-project/codespell.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
black..................................................................................................Passed
flake8.................................................................................................Passed
flake8 (cython)........................................................................................Passed
flake8 (cython template)...............................................................................Passed
isort..................................................................................................Passed
pyupgrade..............................................................................................Passed
rst ``code`` is two backticks..........................................................................Passed
rst directives end with two colons.....................................................................Passed
rst ``inline code`` next to normal text................................................................Passed
Generate pip dependency from conda.....................................................................Passed
flake8-rst.............................................................................................Passed
Check for non-standard imports.........................................................................Passed
Check for non-standard numpy.random-related imports excluding pandas/_testing.py.......................Passed
Check for non-standard imports in test suite...........................................................Passed
Check for incorrect code block or IPython directives...................................................Passed
Check for use of not concatenated strings..............................................................Passed
Check for strings with wrong placed spaces.............................................................Passed
Check for import of private attributes across modules..................................................Passed
Check for use of private functions across modules......................................................Passed
Check for use of bare pytest raises....................................................................Passed
Check for inconsistent use of pandas namespace in tests................................................Passed
Check for use of Union[Series, DataFrame] instead of FrameOrSeriesUnion alias..........................Passed
Check for use of foo.__class__ instead of type(foo)....................................................Passed
Check for use of comment-based annotation syntax and missing error codes...............................Passed
Check code for instances of os.remove..................................................................Passed
Strip unnecessary `# noqa`s............................................................................Passed
Fix End of Files.......................................................................................Passed
Trim Trailing Whitespace...............................................................................Passed
codespell..............................................................................................Passed
```



</details>

</details>

## Changed files
<details>
<summary>Changed file: </summary>

- .pre-commit-config.yaml

</details>

<hr>

[:octocat: Repo](https://github.com/technote-space/create-pr-action) | [:memo: Issues](https://github.com/technote-space/create-pr-action/issues) | [:department_store: Marketplace](https://github.com/marketplace/actions/create-pr-action)"
776386360,38801,"BUG: pandas 1.2.0 and Pyarrow [0.16.0, 1.0.0) are incompatible for some column types",ADraginda,closed,2020-12-30T10:32:34Z,2021-01-05T12:23:29Z,"#35259 added optional importing from pyarrow. The currently listed minimum version of pyarrow is 0.15.1, and the logic of said PR guards against importing attributes from `pyarrow.compute` because it is not available in 0.15.1

Problem: pyarrow added the compute module in [0.16.0](https://github.com/apache/arrow/commit/27dded680e84f1a628de1bddff1f4eb62fbc5887#diff-3d08757408024228c4443730cc3536ab39c9436cd2e4cb63e5da34c69c18962f) but attributes imported by pandas are not available in that module until [1.0.0](https://github.com/apache/arrow/commit/dcd17bf36e0f7b18e8b8f466ed2cb3eb396955d8#diff-3d08757408024228c4443730cc3536ab39c9436cd2e4cb63e5da34c69c18962f)

Therefore: with pandas 1.2.0 and merging a DataFrame (with an Array String column?) :

| Pyarrow Version       | Behavior |
| -------------: | ----- |
| not installed              | works |
| < 0.15.1                     | not supported |
| [0.15.1, 0.16.0).         | works (pyarrow.compute.equal not used?) |
| [0.16.0, 1.0.0)           | ArgumentError |
| [1.0.0, 2.0.0 (latest)] | works (using pyarrow.compute.equal) |

Adding another try/except around the comparison functions in pandas [string_arrow.py](https://github.com/pandas-dev/pandas/blob/master/pandas/core/arrays/string_arrow.py#L39) will change the table of functionality to:

| Pyarrow Version       | Behavior |
| -------------: | ----- |
| not installed              | works |
| < 0.15.1                     | not supported |
| [0.15.1, 1.0.0).         | works (pyarrow.compute.equal not used?) |
| [1.0.0, 2.0.0 (latest)] | works (using pyarrow.compute.equal) |"
765462140,38451,"DOC: name not defined in section ""2.14.1 Basic plotting: plot""",gepcel,closed,2020-12-13T15:17:43Z,2021-01-05T12:57:09Z,"In the [pdf version of document](https://pandas.pydata.org/docs/pandas.pdf), section ""User Guide -> Visualization -> 2.14.1 Basic plotting: plot"", page 588,  there are `name 'pd' (or 'ts') is not defined` problems.

![image](https://user-images.githubusercontent.com/4179106/102015941-57f69300-3d99-11eb-86c7-28ae8c6a8196.png)
"
776392540,38803,BUG: avoid attribute error with pyarrow >=0.16.0 and <1.0.0,ADraginda,closed,2020-12-30T10:47:20Z,2021-01-05T13:23:45Z,"Problem: The minimum pyarrow [listed](https://pandas.pydata.org/docs/dev/whatsnew/v1.2.0.html#increased-minimum-versions-for-dependencies) as an optional dependency is 0.15.1.  Pyarrow added the compute module that is imported in the change here with pyarrow [0.16.0](https://github.com/apache/arrow/commit/27dded680e84f1a628de1bddff1f4eb62fbc5887#diff-3d08757408024228c4443730cc3536ab39c9436cd2e4cb63e5da34c69c18962f) but attributes imported by pandas are not available in that module until [1.0.0](https://github.com/apache/arrow/commit/dcd17bf36e0f7b18e8b8f466ed2cb3eb396955d8#diff-3d08757408024228c4443730cc3536ab39c9436cd2e4cb63e5da34c69c18962f) thus anyone using pyarrow [0.16.0, 1.0.0) will get an Attribute error.

This PR adds as try/expect around accessing the attributes such that they are only accessed if available (i.e. pyarrow >=1.0.0)

- [X] closes #38801
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
778993459,38970,Backport PR #38841 on branch 1.2.x (Update conf.py to execute imports during pdf building),meeseeksmachine,closed,2021-01-05T12:58:45Z,2021-01-05T14:12:30Z,Backport PR #38841: Update conf.py to execute imports during pdf building
779001502,38971,Backport PR #38803 on branch 1.2.x (BUG: avoid attribute error with pyarrow >=0.16.0 and <1.0.0),meeseeksmachine,closed,2021-01-05T13:05:41Z,2021-01-05T14:12:45Z,Backport PR #38803: BUG: avoid attribute error with pyarrow >=0.16.0 and <1.0.0
778352222,38957,"DOC: move API breaking ""check_freq"" section from v1.2.1rst to v1.1.0.rst",kylekeppler,closed,2021-01-04T20:42:11Z,2021-01-05T15:37:43Z,"and add reference to 1.1.0 whats new update in v1.2.1.rst

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

This moves the doc notes for `assert_series_equal` and `assert_frame_equal` from the 1.2.1 whats new to the 1.1.0 whats new and adds a small reference to the section in the 1.1.0 page in the 1.2.1 page. This came up in #38471 and makes more sense to me than the current approach, but happy to close if current state is preferred.
"
776343445,38799,TST: Add hook for Disallow bare pytest.raises,MarcoGorelli,closed,2020-12-30T08:51:07Z,2021-01-05T16:56:39Z,"xref #30999

Adding a hook with lots of excluded folders (for now), this can be made stricter as we go along, there's not that much left"
777627524,38920,TST: GH30999 Add placeholder messages to pandas/tests/io/test_sql.py and remove test for numexpr < 2.6.8,moink,closed,2021-01-03T12:22:31Z,2021-01-05T17:01:47Z,"This is my attempt to finally finish off #30999

In pandas/tests/io/test_sql.py, there is a whole test class skipped. It looks like xref #20536 is supposed to address that, but no one has commented there since March 2018, so I don't think that's going to be fixed any time soon. I noticed that there were other tests in the same module with `match=""<insert message here>""` so I decided to put it in the two tests that I can't figure out the correct error message for.

In pandas/tests/computation/test_compat.py there was an if statement that the numexpr library is at least 2.6.8. I tried to set up an environment with a lower version but conda couldn't resolve the dependencies. That test isn't running in the CI (xref #38876) and that test was last touched in a substantive way in 2016. I think that portion of the test is no longer required.

Not sure that I actually addressed these correctly, but I made an attempt so we could have a conversation about it in a more concrete way.

- [x] xref #30999
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
777686823,38924,TST: 26807 split pandas/tests/tseries/offsets/test_offsets.py into multiple smaller test modules,moink,closed,2021-01-03T17:59:12Z,2021-01-05T17:03:38Z,"This is to address xref #26807 specifically for pandas/tests/tseries/offsets/test_offsets.py . I tried to get all the new modules below about 1000 lines and break it up in logical ways.

Other than moving code, I have changed some list construction for pytest parameterizing to be a tiny bit more compact by changing a bunch of sequential `append`s to a single list literal. Other than that no changes.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
778284927,38951,DOC: clarify and spellcheck indexing documentation,cjbraun,closed,2021-01-04T18:42:54Z,2021-01-05T17:09:34Z,
779289204,38973,TST: update pre-commit config to only exclude extension from bare pytest.raises check,moink,closed,2021-01-05T16:56:16Z,2021-01-05T19:10:57Z,"With #38920 I eliminated all instances of `pytest.raise` without `match=msg` in pandas/tests/computation and pandas/tests/io. #38799 was happening around the same time and missed that they were fixed. So this closes the loop and now only pandas/tests/extension needs to be excluded from the linting check.

I don't think the bare `pytest.raise`s in pandas/tests/extensions will be removed. They are in a pretty complex inheritance hierarchy and reused for many different types of errors and error messages. So I propose that this PR closes #30999.

- [x] closes #30999 
- [ ] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
710382825,36704,Github action for release,simonjayhawkins,closed,2020-09-28T15:47:38Z,2021-01-05T19:18:59Z,"for initial feedback. maybe way off base with this.

can be seen in action at https://github.com/simonjayhawkins/pandas/actions

the alternative of retaining a separate repo for release can be seen at https://github.com/simonjayhawkins/pandas-release/actions"
776738033,38843,BUG: inconsistent concat casting EA vs non-EA,jbrockmendel,closed,2020-12-31T03:04:11Z,2021-01-05T22:14:19Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Because we dont have 2D EAs, we end up passing axis=0 to concat_compat from internals.concat.  As a result we drop empty arrays only when EAs are present.  This leads to dtypes being preserved in EA cases while being cast to object in non-EA cases.

One solution is just to support 2D EAs.  But since I like the dropping-empties behavior better anyway, this is good too."
779330745,38975,DOC: remove is_lexsorted from MultiIndex docstring,kylekeppler,closed,2021-01-05T17:31:37Z,2021-01-06T00:15:31Z,"`is_lexsorted` has been deprecated (https://github.com/pandas-dev/pandas/pull/38701)

- [x] closes https://github.com/pandas-dev/pandas/issues/38953
- [ ] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
776619746,38831,BUG: frame_length_0[key] = nonempty_frame raises ValueError,jbrockmendel,closed,2020-12-30T20:15:05Z,2021-01-06T00:35:38Z,"```
df = pd.DataFrame(columns=[""A"", ""B""])

other = pd.DataFrame({""B"": [1, 2]})

>>> df[""B""] = other
ValueError: Cannot set a frame with no defined index and a value that cannot be converted to a Series

>>> df[[""B""]] = other   # <-- works as expected
```"
777726452,38931, BUG: DataFrame.__setitem__ raising ValueError with string indexer and empty df and df to set,phofl,closed,2021-01-03T22:14:19Z,2021-01-06T01:13:51Z,"- [x] closes #38831
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Dont have to convert df to series, df has an index.
cc @jbrockmendel "
779871854,38987,Fix bug on master,phofl,closed,2021-01-06T02:02:54Z,2021-01-06T02:53:30Z,"- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

The banklist file was removed and replaced with banklist.csv. This causes failures on master.

This is just a temporary fix for the user guide.

cc @jreback
"
778289152,38953,DOC: MultiIndex.is_lexsorted not linked,kylekeppler,closed,2021-01-04T18:50:32Z,2021-01-06T04:27:04Z,"#### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.MultiIndex.html?highlight=is_lexsorted

#### Documentation problem

On the API page for `MultiIndex`, `is_lexsorted` does not link to an API page for that method. That method has been deprecated and does not contain a docstring, perhaps that is why there is no link?

#### Suggested fix for documentation

Since this method is deprecated, suggest removing `is_lexsorted` from the list of methods on the `MultiIndex` API page. Alternatively could add a docstring and API page.

What is preferred?"
776729776,38841,Update conf.py to execute imports during pdf building,gepcel,closed,2020-12-31T02:32:07Z,2021-01-06T04:38:01Z,"closes #38451

According [https://ipython.readthedocs.io/en/stable/sphinxext.html](), it should be `ipython_execlines` not `ipython_exec_lines`. Should close issue #38451

"
779917011,38991,Backport PR #38987 on branch 1.2.x (Fix bug on master),meeseeksmachine,closed,2021-01-06T02:53:25Z,2021-01-06T12:53:03Z,Backport PR #38987: Fix bug on master
779948819,38993,DOC: add more sections to spreadsheet comparison,afeld,closed,2021-01-06T03:27:45Z,2021-01-06T14:53:19Z,"[Preview (link to PDF on Google Drive)](https://drive.google.com/file/d/1uUyTQyEAX3F6h4EJqKz7Mz4KehGBnAig/view?usp=sharing)

This pull request gets closer to full parity with [SAS](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sas.html)/[STATA](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_stata.html) comparison pages by adding the Data Input/Output through Merging sections. It still needs Missing Data and GroupBy, but wanted to get this in while I was at a good stopping place. Each section was done in its own commit, if it's easier to review that way.

---

- [x] ~~closes~~ part of https://github.com/pandas-dev/pandas/issues/38990
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] ~~whatsnew entry~~"
777808790,38938,DOC: Update contributing.rst,BobinMathew,closed,2021-01-04T04:42:01Z,2021-01-06T15:02:41Z,"#### Location of the documentation

doc/source/development/contributing.rst

#### Documentation problem

Need additional information about the command that verifies the linting of code files.

#### Suggested fix for documentation

`pre-commit run --from-ref=upstream/master --to-ref=HEAD --all-files`

The command verifies the linting of code files, it looks for common mistake patterns
*NOTE: An additional paragraph is preferred for this*"
779790891,38984,BUG: MultiIndex.intersection duplicating nans in result,phofl,closed,2021-01-06T00:34:12Z,2021-01-06T15:24:33Z,"- [x] xref #38623
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

This also aligns the multiindex implementation with the base implementation a bit more."
779499422,38978,CLN: Multiindex tests,phofl,closed,2021-01-05T19:59:44Z,2021-01-06T15:24:58Z,"- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

Found a few wrong tests and parametrized the similar ones
"
779755094,38983,BUG: IntegerDtype.__eq__(np.dtype(object)) returns True,jbrockmendel,closed,2021-01-06T00:01:06Z,2021-01-06T16:31:16Z,Also looks like `IntegerArray.astype(other_nullable_integer_dtype)` is returning an ndarray
780500336,39000,Remove Scatter and Hexbin from Series plot documentation,1nF0rmed,closed,2021-01-06T11:56:45Z,2021-01-06T18:36:21Z,"- [x] closes #38976
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] Remove scatter and hexbin entry from https://github.com/pandas-dev/pandas/blob/v1.2.0/pandas/plotting/_core.py#L603-L1708
"
779386076,38976,DOC: Scatter and Hexbin from Series plot documentation,ghost,closed,2021-01-05T18:18:35Z,2021-01-06T18:36:30Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.plot.html

#### Documentation problem

As the scatter and hexbin kinds are not supported for Series, it shouldn't appear in the docs for Series, just for DataFrames.

#### Suggested fix for documentation

Simply removing them. 
"
778611306,38965,BUG: Timedelta(td64_out_of_bounds) silently overflowing,jbrockmendel,closed,2021-01-05T05:58:46Z,2021-01-06T18:42:34Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry
"
779817082,38985,REF: de-duplication in libperiod,jbrockmendel,closed,2021-01-06T01:01:31Z,2021-01-06T18:44:48Z,
775627284,38765,"TYP: DataFrame.(dot, __matmul__)",arw2019,closed,2020-12-28T23:32:46Z,2021-01-06T19:29:38Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref https://github.com/pandas-dev/pandas/pull/38416/files#r549438292"
776613924,38830,BUG: Misaligned columns from read_csv on Ecobee CVS,inducer,closed,2020-12-30T19:57:34Z,2021-01-06T22:56:26Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
df = pd.read_csv(""pdbug.csv"", comment=""#"")
df.info()
print(df)
```
with `pdbug.csv` being this:
```csv
#,Thermostat,identifier,(REDACTED)
#,Thermostat,name,Downstairs
#,Start,date,2020-12-22
#,End,date,2020-12-30

Date,Time,System Setting,System Mode,Calendar Event,Program Mode,Cool Set Temp (F),Heat Set Temp (F),Current Temp (F),Current Humidity (%RH),Outdoor Temp (F),Wind Speed (km/h),Cool Stage 1 (sec),Heat Stage 1 (sec),Fan (sec),DM Offset,Thermostat Temperature (F),Thermostat Humidity (%RH),Study (F),Study2,Breakfast Nook (F),Breakfast Nook2
2020-12-22,02:55:00,heat,heatOff,,Sleep,85,61,67.4,40,30.8,0,0,0,0,,67.4,40,,,,,
2020-12-22,03:00:00,heat,heatOff,,Sleep,85,61,67.3,40,29.6,0,0,0,0,,67.3,40,,,,,
2020-12-22,03:05:00,heat,heatOff,,Sleep,85,61,67.2,40,29.6,0,0,0,180,,67.2,40,,,,,
2020-12-22,03:10:00,heat,heatOff,,Sleep,85,61,67.1,42,29.6,0,0,0,120,,67.1,42,,,,,
2020-12-22,03:15:00,heat,heatOff,,Sleep,85,61,67.1,40,29.6,0,0,0,0,,67.1,40,,,,,
2020-12-22,03:20:00,heat,heatOff,,Sleep,85,61,67,40,29.6,0,0,0,0,,67,40,,,,,
2020-12-22,03:25:00,heat,heatOff,,Sleep,85,61,67,40,29.6,0,0,0,0,,67,40,,,,,
2020-12-22,03:30:00,heat,heatOff,,Sleep,85,61,66.9,40,29,0,0,0,0,,66.9,40,,,,,
```
(a slight edit of a CSV file of HVAC system performance data downloaded from https://ecobee.com)
gives
```
<class 'pandas.core.frame.DataFrame'>
Index: 8 entries, 2020-12-22 to 2020-12-22
Data columns (total 22 columns):
 #   Column                      Non-Null Count  Dtype  
---  ------                      --------------  -----  
 0   Date                        8 non-null      object 
 1   Time                        8 non-null      object 
 2   System Setting              8 non-null      object 
 3   System Mode                 0 non-null      float64
 4   Calendar Event              8 non-null      object 
 5   Program Mode                8 non-null      int64  
 6   Cool Set Temp (F)           8 non-null      int64  
 7   Heat Set Temp (F)           8 non-null      float64
 8   Current Temp (F)            8 non-null      int64  
 9   Current Humidity (%RH)      8 non-null      float64
 10  Outdoor Temp (F)            8 non-null      int64  
 11  Wind Speed (km/h)           8 non-null      int64  
 12  Cool Stage 1 (sec)          8 non-null      int64  
 13  Heat Stage 1 (sec)          8 non-null      int64  
 14  Fan (sec)                   0 non-null      float64
 15  DM Offset                   8 non-null      float64
 16  Thermostat Temperature (F)  8 non-null      int64  
 17  Thermostat Humidity (%RH)   0 non-null      float64
 18  Study (F)                   0 non-null      float64
 19  Study2                      0 non-null      float64
 20  Breakfast Nook (F)          0 non-null      float64
 21  Breakfast Nook2             0 non-null      float64
dtypes: float64(10), int64(8), object(4)
memory usage: 1.4+ KB
                Date  Time System Setting  System Mode Calendar Event  Program Mode  Cool Set Temp (F)  Heat Set Temp (F)  Current Temp (F)  Current Humidity (%RH)  Outdoor Temp (F)  Wind Speed (km/h)  Cool Stage 1 (sec)  Heat Stage 1 (sec)  Fan (sec)  DM Offset  Thermostat Temperature (F)  Thermostat Humidity (%RH)  Study (F)  Study2  Breakfast Nook (F)  Breakfast Nook2
2020-12-22  02:55:00  heat        heatOff          NaN          Sleep            85                 61               67.4                40                    30.8                 0                  0                   0                   0        NaN       67.4                          40                        NaN        NaN     NaN                 NaN              NaN
2020-12-22  03:00:00  heat        heatOff          NaN          Sleep            85                 61               67.3                40                    29.6                 0                  0                   0                   0        NaN       67.3                          40                        NaN        NaN     NaN                 NaN              NaN
2020-12-22  03:05:00  heat        heatOff          NaN          Sleep            85                 61               67.2                40                    29.6                 0                  0                   0                 180        NaN       67.2                          40                        NaN        NaN     NaN                 NaN              NaN
2020-12-22  03:10:00  heat        heatOff          NaN          Sleep            85                 61               67.1                42                    29.6                 0                  0                   0                 120        NaN       67.1                          42                        NaN        NaN     NaN                 NaN              NaN
2020-12-22  03:15:00  heat        heatOff          NaN          Sleep            85                 61               67.1                40                    29.6                 0                  0                   0                   0        NaN       67.1                          40                        NaN        NaN     NaN                 NaN              NaN
2020-12-22  03:20:00  heat        heatOff          NaN          Sleep            85                 61               67.0                40                    29.6                 0                  0                   0                   0        NaN       67.0                          40                        NaN        NaN     NaN                 NaN              NaN
2020-12-22  03:25:00  heat        heatOff          NaN          Sleep            85                 61               67.0                40                    29.6                 0                  0                   0                   0        NaN       67.0                          40                        NaN        NaN     NaN                 NaN              NaN
2020-12-22  03:30:00  heat        heatOff          NaN          Sleep            85                 61               66.9                40                    29.0                 0                  0                   0                   0        NaN       66.9                          40                        NaN        NaN     NaN                 NaN              NaN
```

#### Problem description

Observe that the ""Date"" column absorbs both the date and time values provided, and that the ""Time"" column returend by read_csv contains data intended for the next column over (""System Setting""). The source CSV seems sane, but the `read_csv` output does not.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.9.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.9.0-5-amd64
Version          : #1 SMP Debian 5.9.15-1 (2020-12-17)
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : de_DE.UTF-8
LOCALE           : de_DE.UTF-8

pandas           : 1.2.0
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.3.1
setuptools       : 44.0.0
Cython           : 0.29.21
pytest           : 6.2.0
hypothesis       : None
sphinx           : 3.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
780807763,39006,DOC: Clarify index_col behavior for read_csv,phofl,closed,2021-01-06T19:53:22Z,2021-01-06T23:01:11Z,"- [x] closes #38830
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them

"
743678831,37892,BUG: pandas not connecting to private S3 endpoint,MarkiesFredje,closed,2020-11-16T09:36:53Z,2021-01-07T12:16:18Z,"
#### Problem description

Connecting to  https://bucket.s3.amazonaws.com rather then private endpoint

```python
df = pd.read_parquet('s3://bucket/file.parquet', storage_options={'client_kwargs':{'endpoint_url': S3_ENDPOINT}})
```
as workaround I do:

```python
# ecs is a S3FileSystem object with client_kwargs

with ecs.open('bucket/file.parquet', 'rb') as f:
    df = pd.read_parquet(f)
```

#### Expected Output

Connecting to private S3 bucket

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1127.19.1.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.1.0.post20200704
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.4.0
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : None
sqlalchemy       : 1.3.18
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.50.1

</details>
"
525997261,29751,Inconsistent .loc slicing behaviour with NaNs in MultiIndex dataframe,pepicello,closed,2019-11-20T18:31:52Z,2021-01-07T13:58:08Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np

a = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).set_index([0, 1])
b = pd.DataFrame([[1, np.nan, 3], [4, np.nan, 6], [7, np.nan, 9]]).set_index([0, 1])
a_idx = a.index[1]
b_idx = b.index[1]
```

```
>>> a.loc[a_idx:]
     2
0 1
4 5  6
7 8  9
>>> a.loc[:a_idx]
     2
0 1
1 2  3
4 5  6
>>> b.loc[b_idx:]
       2
0 1
7 NaN  9
>>> b.loc[:b_idx]
       2
0 1
1 NaN  3
4 NaN  6
```

#### Problem description

As above, slicing on MultiIndex has different behaviours with nans: it excludes the current index of selection from the slice. This is possibly because - rightly so - there is no match on NaNs when comparing the indices, but if that is the case, this should also not work:

```
>>> b.loc[b_idx]
2    6
Name: (4, nan), dtype: int64
```

Note: might be a duplicate of #25154, but OP does not slice directly on the MultiIndex with NaNs, so I am not sure if fixing that would also fix this issue.

#### Expected Output

```
>>> b.loc[b_idx:]
       2
0 1
4 NaN  6
7 NaN  9
```

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.1
numpy            : 1.16.4
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.2
setuptools       : 41.0.1
Cython           : 0.29.13
pytest           : 5.1.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.7
tables           : None
xarray           : 0.12.3
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.1.8

</details>
"
775699827,38772,BUG: Added test cases to check loc on multiindex with NaNs #29751,kasim95,closed,2020-12-29T05:05:16Z,2021-01-07T13:58:12Z,"- [x] closes #29751 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Added test cases to check loc on multiindex containing NaN values using `np.nan`, `pd.NA`, and `None`"
781281220,39018,CLN: add typing to dtype arg in core/common.py (GH38808),tushushu,closed,2021-01-07T12:33:52Z,2021-01-07T14:07:44Z,"Follow the issue - https://github.com/pandas-dev/pandas/issues/38808

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
779891101,38989,BUG: read_csv raising when null bytes are in skipped rows,phofl,closed,2021-01-06T02:24:24Z,2021-01-07T18:48:20Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example
I have a few weird csv files with null bytes in the first row. Since 1.2.0 they are raising even if I skip the first row,

```python
pd.read_csv(""test.csv"", engine=""python"", skiprows=[0])
pd.read_csv(""test.csv"", engine=""c"", skiprows=[0])
```
Could not get this to fail with an example created in code.
#### Problem description


Both engines raising 
```
Traceback (most recent call last):
  File ""/home/developer/.config/JetBrains/PyCharm2020.3/scratches/scratch_4.py"", line 377, in <module>
    pd.read_csv(""/media/sf_Austausch/test.csv"", engine=""c"", skiprows=[0], nrows=2)
  File ""/home/developer/PycharmProjects/pandas/pandas/io/parsers.py"", line 605, in read_csv
    return _read(filepath_or_buffer, kwds)
  File ""/home/developer/PycharmProjects/pandas/pandas/io/parsers.py"", line 457, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File ""/home/developer/PycharmProjects/pandas/pandas/io/parsers.py"", line 814, in __init__
    self._engine = self._make_engine(self.engine)
  File ""/home/developer/PycharmProjects/pandas/pandas/io/parsers.py"", line 1045, in _make_engine
    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]
  File ""/home/developer/PycharmProjects/pandas/pandas/io/parsers.py"", line 1894, in __init__
    self._reader = parsers.TextReader(self.handles.handle, **kwds)
  File ""pandas/_libs/parsers.pyx"", line 517, in pandas._libs.parsers.TextReader.__cinit__
  File ""pandas/_libs/parsers.pyx"", line 619, in pandas._libs.parsers.TextReader._get_header
  File ""pandas/_libs/parsers.pyx"", line 813, in pandas._libs.parsers.TextReader._tokenize_rows
  File ""pandas/_libs/parsers.pyx"", line 1942, in pandas._libs.parsers.raise_parser_error
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 8: invalid continuation byte

Process finished with exit code 1
```

This worked on 1.1.5.

Relevant changes are:

https://github.com/pandas-dev/pandas/blob/9b16b1e1ee049b042a7d59cddd8fbd913137223f/pandas/io/common.py#L556:L557
infers encoding now, which leads to

https://github.com/pandas-dev/pandas/blob/9b16b1e1ee049b042a7d59cddd8fbd913137223f/pandas/io/common.py#L643:L651
 where errors is always strict for read_csv. On 1.1.5 in case of no encoding given, ``errors`` was set to ``replace``, which caused this to work. Was this an intended change? Labeling as Regression for now.


#### Expected Output
```
   !   xyz  0  1
0  *  2000  1  2
1  *  2001  0  0
```

cc @gfyoung @twoertwein
This was caused by #36997

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : cd0224d26c6050f0e638861b9e557086673c14c1
python           : 3.8.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-58-generic
Version          : #64-Ubuntu SMP Wed Dec 9 08:16:25 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.3.0.dev0+356.ge0cb09e917
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.6.0.post20201009
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : 5.37.1
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.3
fastparquet      : 0.4.1
gcsfs            : 0.7.1
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 2.0.0
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.2
sqlalchemy       : 1.3.20
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.51.2

</details>
"
781413797,39019,DOC: np.bool -> np.bool_,simonjayhawkins,closed,2021-01-07T15:52:25Z,2021-01-07T18:59:32Z,"xref #34848, #34835

a couple missed that are causing doc build failures with numpy-1.20.0rc2, https://github.com/pandas-dev/pandas/pull/36092/checks?check_run_id=1662818738

probably want to backport this to prevent possible future ci failures"
780087487,38997,REGR: errors='replace' when encoding/errors are not specified,twoertwein,closed,2021-01-06T06:02:23Z,2021-01-07T19:07:02Z,"- [x] closes #38989
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

Should 1.3 use `errors='replace'` when no `encoding/errors` are specified or use `errors=None` (strict)?"
781529725,39021,Backport PR #38997 on branch 1.2.x (REGR: errors='replace' when encoding/errors are not specified),meeseeksmachine,closed,2021-01-07T18:49:07Z,2021-01-07T21:23:21Z,Backport PR #38997: REGR: errors='replace' when encoding/errors are not specified
781529502,39020,Backport PR #39019 on branch 1.2.x (DOC: np.bool -> np.bool_),meeseeksmachine,closed,2021-01-07T18:48:43Z,2021-01-07T21:23:37Z,Backport PR #39019: DOC: np.bool -> np.bool_
780868204,39010,BUG: 1.2. version fails in adding column if number on column name beginning.,Malachov,closed,2021-01-06T21:51:33Z,2021-01-08T00:19:44Z,"- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of pandas.
- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

Code that raise traceback

```python
import pandas as pd

df = pd.DataFrame([[1, 2], [3, 4]])
df['0 - Name'] = [1, 2]
```
Result:

ValueError: format number 1 of ""0 - Name"" is not recognized

If other name witho no mumber on column name, it will pass.
In older version, worked correctly

Tested on new virtualenv - just `pip install pandas`
Failed also on Travis  (Python 3.6 passed, 3.7 and 3.8 failed), so probably not my IDE setup or something similar

Full traceback here...

```python-traceback
Traceback (most recent call last):
  File ""c:\Users\truton\ownCloud\Github\test\del\lib\site-packages\pandas\core\generic.py"", line 3823, in _set_item
    loc = self._info_axis.get_loc(key)
  File ""c:\Users\truton\ownCloud\Github\test\del\lib\site-packages\pandas\core\indexes\range.py"", line 354, in get_loc
    raise KeyError(key)
KeyError: '0 - Name'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\Users\truton\ownCloud\Github\test\del.py"", line 4, in <module>
    df['0 - Name'] = [1, 2]
  File ""c:\Users\truton\ownCloud\Github\test\del\lib\site-packages\pandas\core\frame.py"", line 3163, in __setitem__
    self._set_item(key, value)
  File ""c:\Users\truton\ownCloud\Github\test\del\lib\site-packages\pandas\core\frame.py"", line 3240, in _set_item
    NDFrame._set_item(self, key, value)
  File ""c:\Users\truton\ownCloud\Github\test\del\lib\site-packages\pandas\core\generic.py"", line 3826, in _set_item
    self._mgr.insert(len(self._info_axis), key, value)
  File ""c:\Users\truton\ownCloud\Github\test\del\lib\site-packages\pandas\core\internals\managers.py"", line 1197,
in insert
    new_axis = self.items.insert(loc, item)
  File ""c:\Users\truton\ownCloud\Github\test\del\lib\site-packages\pandas\core\indexes\numeric.py"", line 174, in insert
    item = self._validate_fill_value(item)
  File ""c:\Users\truton\ownCloud\Github\test\del\lib\site-packages\pandas\core\indexes\numeric.py"", line 119, in _validate_fill_value
    if is_bool(value) or is_bool_dtype(value):
  File ""c:\Users\truton\ownCloud\Github\test\del\lib\site-packages\pandas\core\dtypes\common.py"", line 1384, in is_bool_dtype
    dtype = get_dtype(arr_or_dtype)
  File ""c:\Users\truton\ownCloud\Github\test\del\lib\site-packages\pandas\core\dtypes\common.py"", line 1607, in get_dtype
    return pandas_dtype(arr_or_dtype)
  File ""c:\Users\truton\ownCloud\Github\test\del\lib\site-packages\pandas\core\dtypes\common.py"", line 1799, in pandas_dtype
    npdtype = np.dtype(dtype)
  File ""c:\Users\truton\ownCloud\Github\test\del\lib\site-packages\numpy\core\_internal.py"", line 177, in _commastring
    (len(result)+1, astr))
ValueError: format number 1 of ""0 - Name"" is not recognized
```

#### Problem description

Very strange is than it happens only if it's first assignment, so this will work correctly with no error

```python
import pandas as pd

df = pd.DataFrame([[1, 2], [3, 4]])
df['a - Name'] = [1, 2]
df['0 - Name'] = [1, 2]
```

#### Expected Output

I do not create dataframe with numeric names, but thing is, that if source data are numpy, than created dataframe has numeric range index. If user in my library insert data - i create some derivations - want to keep names + name of derivation - thus if somebody insert numpy, it will fail.

Expected output is creating new column with number in column name start - or generating columns like ['1', '2', '3'] from arrays which works...

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.17763
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : cs_CZ.UTF-8
LOCALE           : None.None

pandas           : 1.2.0
numpy            : 1.19.5
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.1
setuptools       : 46.1.3
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
781741977,39032,Backport PR #39023 on branch 1.2.x (Fix regression in setitem when expanding DataFrame with specific column name format),meeseeksmachine,closed,2021-01-08T00:20:10Z,2021-01-08T01:14:21Z,Backport PR #39023: Fix regression in setitem when expanding DataFrame with specific column name format
781554182,39023,Fix regression in setitem when expanding DataFrame with specific column name format,phofl,closed,2021-01-07T19:30:03Z,2021-01-08T08:41:05Z,"- [x] closes #39010
- [x] tests added / passed
- [x] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [x] whatsnew entry

We obviously want cast to object if value is string."
545336099,30699,BUG: Datetimelike equality comparisons with Categorical,jschendel,closed,2020-01-04T21:43:39Z,2021-01-08T14:10:02Z,"#### Code Sample, a copy-pastable example if possible
Consider the following setup on master:
```python
In [1]: import pandas as pd; pd.__version__
Out[1]: '0.26.0.dev0+1576.gdd0d353fb'

In [2]: dti = pd.date_range(""2020"", periods=3) 
   ...: dti_tz = pd.date_range(""2020"", periods=3, tz=""UTC"") 
   ...: tdi = pd.timedelta_range(""0 days"", periods=3) 
   ...: pi = pd.period_range(""2020Q1"", periods=3, freq=""Q"")
```

Equality comparisons with an equivalent `Categorical` are incorrect for `DatetimeIndex`:
```python
In [3]: dti == pd.Categorical(dti)
Out[3]: array([False, False, False])

In [4]: dti_tz == pd.Categorical(dti_tz)
Out[4]: array([False, False, False])
```

Equality comparisons raise for `PeriodIndex`:
```python
In [5]: pi == pd.Categorical(pi)
---------------------------------------------------------------------------
ValueError: Value must be Period, string, integer, or datetime
```
Looks good for `TimedeltaIndex`:
```python
In [6]: tdi == pd.Categorical(tdi)
Out[6]: array([ True,  True,  True])
```

The incorrect behavior above is generally consistent when replacing the index with its extension array/Series equivalent, `Categorical` with `CategoricalIndex`/`Series[Categorical]`, and `==` with `!=`.

The only exception appears to be that a couple cases work when when you have a  `Categorical`/ `CategoricalIndex` on the RHS and an extension array on the LHS:
```python
In [7]: pd.Categorical(dti) == dti.array
Out[7]: array([ True,  True,  True])

In [8]: pd.CategoricalIndex(pi) == pi.array
Out[8]: array([ True,  True,  True])
```
Though note that the above does not work for `dti_tz.array`

#### Problem description

Equality comparisons for datetimelike arrays/indexes are largely incorrect when comparing to equivalent categoricals.  There is some tie in to #19513 but I think this specific component is pretty clear cut.

#### Expected Output
I'd expect all variants of `==` to in the examples above to return `array([ True,  True,  True])`, and all variants of `!=` to return  `array([False, False, False])`.

cc @jbrockmendel 
"
779849331,38986,BUG: Datetimelike equality comparisons with Categorical ,ftrihardjo,closed,2021-01-06T01:37:23Z,2021-01-08T14:10:07Z,"- [ ] closes #30699 
- [ ] tests added / passed
- [ ] Ensure all linting tests pass, see [here](https://pandas.pydata.org/pandas-docs/dev/development/contributing.html#code-standards) for how to run them
- [ ] whatsnew entry
"
777277366,38878,BUG: Unexpected typecast on datetime.date in groupby.key(),ghost,closed,2021-01-01T12:27:19Z,2021-01-08T14:17:27Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
>>> import datetime as dt
>>> import pandas as pd
>>> df = pd.DataFrame([[dt.date(2021,1,1), 1, 'a'], [dt.date(2021,1,2),  2, 'b']])
>>> g = df.groupby([0, 1])
>>> g.groups.key()
... dict_keys([(Timestamp('2021-01-01 00:00:00'), 1), (Timestamp('2021-01-02 00:00:00'), 2)])
```

#### Problem description
When multiple columns pass to groupby, the `datetime.date` type columns cast o the `Timestamp` type.

Due to the unexpected type casting, following code raises a `KeyError`.

```python
>>> list(g.get_group(key) for key in g.groups.keys())
... KeyError: (Timestamp('2021-01-01 00:00:00'), 1)
```
[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

This typecast will only be performed if multiple indexes are specified.

```python
>>> g = df.groupby([0])
>>> g.groups.key()
... dict_keys([datetime.date(2021, 1, 1), datetime.date(2021, 1, 2)])
```

#### Expected Output
```python
>>> g.groups.key()
... dict_keys([(datetime.date(2021, 1, 1), 1), (datetime.date(2021, 1, 2), 2)])
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 20.2.0
Version          : Darwin Kernel Version 20.2.0: Wed Dec  2 20:39:59 PST 2020; root:xnu-7195.60.75~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : ja_JP.UTF-8
pandas           : 1.2.0
numpy            : 1.19.4
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.1.1
Cython           : 0.29.21
pytest           : 6.2.1
hypothesis       : None
sphinx           : 3.4.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.6.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : 2.7.2
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.6.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.52.0

</details>"
781839589,39034,TST: flesh out EA setitem tests,jbrockmendel,closed,2021-01-08T04:44:42Z,2021-01-08T15:51:42Z,Preliminary to fixing some behavior that these hit
776672304,38835,BUG: astype_nansafe with copy=False,jbrockmendel,closed,2020-12-30T23:13:03Z,2020-12-31T01:57:55Z,"- [x] closes #34456
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Opens up possibility of addressing #23092"
776660592,38833,REF: move putmask internals in array_algos.putmask,jbrockmendel,closed,2020-12-30T22:25:53Z,2020-12-31T01:59:44Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
776678754,38838,Backport PR #38816 on branch 1.2.x (REGR: groupby.sem with nuisance columns),meeseeksmachine,closed,2020-12-30T23:39:23Z,2020-12-31T03:02:41Z,Backport PR #38816: REGR: groupby.sem with nuisance columns
776582108,38826,CLN: add typing to dtype arg in selection of files in core/arrays (GH38808),avinashpancham,closed,2020-12-30T18:27:55Z,2020-12-31T03:08:33Z,"Follow on PR for #38808
- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
753318178,38179,PERF: Introducing hash tables for complex64 and complex128,realead,closed,2020-11-30T09:39:23Z,2020-12-31T08:01:48Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Next step for #33287

The complex case is somewhat less straight forward, because Cython [defines](https://github.com/cython/cython/blob/master/Cython/Includes/numpy/__init__.pxd#L755) `complex128`/`complex64` as:

```
ctypedef float complex  complex64_t
ctypedef double complex complex128_t
```

where `double complex` actually means a Cython-datatype defined here https://github.com/cython/cython/blob/master/Cython/Utility/Complex.c#L56

Thus a conversion between numpy's complex and Cython's complex must happens somewhere.



"
776532209,38820,"CI,STYLE: GH38802 narrow down list of ignored words",chinggg,closed,2020-12-30T16:18:54Z,2020-12-31T09:07:34Z,"- [x] closes #38811 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
776426647,38806,DOC: fix sphinx directive error in 1.2.1 release notes,simonjayhawkins,closed,2020-12-30T12:12:59Z,2020-12-31T10:37:20Z,
776912041,38848,Backport PR #38806: DOC: fix sphinx directive error in 1.2.1 release notes,simonjayhawkins,closed,2020-12-31T10:35:30Z,2020-12-31T12:00:42Z,Backport PR #38806
776515407,38816,REGR: groupby.sem with nuisance columns,rhshadrach,closed,2020-12-30T15:40:27Z,2020-12-31T13:33:07Z,"- [x] closes #38774
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

We were using the same integer locs in the result of `std` and `count` when computing the quotient for `sem`. However, `std` will drop nuisance columns and `count` will not.

I could only find two tests involving nuisance columns, and these only test sum/mean. Created #38815 for a followup."
776040215,38784,Backport PR #38737 on branch 1.2.x (BUG/REG: RollingGroupby MultiIndex levels dropped),meeseeksmachine,closed,2020-12-29T18:56:53Z,2020-12-31T14:38:28Z,Backport PR #38737: BUG/REG: RollingGroupby MultiIndex levels dropped
776736327,38842,API/BUG: silently-ignored dtype in DataFrame constructor,jbrockmendel,closed,2020-12-31T02:57:48Z,2020-12-31T15:36:58Z,"I would expect this to raise rather than silently ignore:
```
df = pd.DataFrame({""A"": [""foo"", ""bar""]}, dtype=""i8"")

>>> df
     A
0  foo
1  bar

>>> df.dtypes
A    object
dtype: object
```"
776675115,38836,"DEPR: try_cast kwarg in mask, where",jbrockmendel,closed,2020-12-30T23:23:04Z,2020-12-31T15:56:06Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

It is silently ignored ATM."
776942971,38850,DOC: sphinx error in 1.2.1 release notes,simonjayhawkins,closed,2020-12-31T12:17:45Z,2020-12-31T15:59:40Z,
776528924,38819,REGR: read_excel does not work for most file handles,twoertwein,closed,2020-12-30T16:11:15Z,2020-12-31T16:24:45Z,"- [x] closes #38788
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
774744086,38697,TST: GH30999 Add match=msg to all pytest.raises in tests/indexes,moink,closed,2020-12-25T14:51:32Z,2020-12-31T16:25:27Z,"This pull request partially addresses xref #30999 to remove bare pytest.raises by adding match=msg. It doesn't close that issue as I have only addressed modules in pandas/tests/indexes

I made a few of the raised errors more specific types. There are two other changes that are a bit more complex but I will put specific comments on the code lines to explain those.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
776578703,38825,TST: GH30999 Change all pytest.raises in pandas/tests/indexing to tm.external_error_raised,moink,closed,2020-12-30T18:18:50Z,2020-12-31T16:40:17Z,"Addresses xref #30999 for pandas/tests/indexing by changing three bare `pytest.raise` instances to `tm.external_error_raised`

This is the last of the simpler PRs for #30999 - after that it gets more complex.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
774138308,38672,BUG: SeriesGroupBy.value_counts is inconsistent with Series.value_counts when the Series is categorical,venaturum,closed,2020-12-24T02:27:43Z,2020-12-31T16:41:17Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame()
df[""col1""] = [""A"", ""A"", ""B"", ""B""]
df[""col2""] = pd.Categorical([""1"", ""2"", ""1"", ""1""], categories=[""1"", ""2"", ""3""])

print(df.groupby(""col1"").col2.value_counts())

```



#### Problem description

A *value_counts* applied to the col2 Series:
```python
df.col2.value_counts()
```

 yields:
```
1      3
2      1
3      0
Name: col2, dtype: int64
```
Because col2 is categorical we get the 0 count for value 3, as desired.

If we groupby col1 first 
```python
df.groupby(""col1"").col2.value_counts()
```
we get the following output:
```
col1  col2
A     1       1
      2       1
B     1       2
Name: col2, dtype: int64
```

#### Expected Output

The expected output can be achieved with a workaround:
```python
df.groupby(""col1"").col2.apply(pd.Series.value_counts)
```

which yields:
```
col1   
A     2    1
      1    1
      3    0
B     1    2
      3    0
      2    0
Name: col2, dtype: int64
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.4
numpy            : 1.19.1
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 20.2.3
setuptools       : 41.2.0
Cython           : 0.29.21
pytest           : 5.3.5
hypothesis       : None
sphinx           : 2.4.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fsspec           : 0.6.0
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.10
tables           : None
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
777069262,38853,Backport PR #38850 on branch 1.2.x (DOC: sphinx error in 1.2.1 release notes),meeseeksmachine,closed,2020-12-31T15:51:36Z,2020-12-31T17:19:33Z,Backport PR #38850: DOC: sphinx error in 1.2.1 release notes
777072644,38855,TST: GH30999 address all bare pytest.raises in pandas/tests/series and add EMPTY_STRING_PATTERN to tm,moink,closed,2020-12-31T16:04:09Z,2020-12-31T18:47:41Z,"This PR is to address xref #30999 in pandas/tests/series

Three of the four errors raised within `pytest.raises` were for `NotImplementedError` with the empty string as the error message. Rather than individually making a regular expression for the empty string, I added one as a constant to `pandas/_testing/__init__.py` . There are two more similar cases in other modules so I wanted to put it somewhere that all the testing modules import. But if you don't like it I can revert it and instead use an individual empty string pattern in each case.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
776729661,38840,TYP: follow-ups to recent PRs,jbrockmendel,closed,2020-12-31T02:31:36Z,2020-12-31T19:10:02Z,
732835325,37510,BUG: index label messed up when reset_level then unstack then sort_index,hliatrussellinvestments,closed,2020-10-30T02:38:29Z,2020-12-31T19:11:28Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd

df1 = pd.DataFrame({'id1': [1, 2, 3, 4],
                    'id2': [3,4,1,2],
                    'id3': [1,1,1,1],
                    'x': [1,2,3,4]})
df1.set_index(['id1', 'id2', 'id3'], inplace=True)

new_levels = ['n1', 'n2', 'n3', None]
df1.index.set_levels(levels=new_levels, level='id1', inplace=True)
df1.index.set_levels(levels=new_levels, level='id2', inplace=True)

df1.unstack('id3')[('x',1)].sort_index()
```

#### Problem description

I would expect the index after the reset_level operations should be just normal index, but when doing sort_index, it seems index level ‘id1’ of row 1 is incorrectly changed to ‘n1’ from NaN and the same for level ‘id2’ of row 2. 
The result is:
id1  id2
n1   n2     4
     n3     1
n2   n1     2
n3   n1     3
Name: (x, 1), dtype: int64

#### Expected Output
id1  id2
n1   n3     1
n2   NaN    2
n3   n1     3
NaN  n2     4
Name: (x, 1), dtype: int64

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252
pandas           : 1.1.1
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 47.1.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
776175638,38796,BUG: GH38672 SeriesGroupBy.value_counts for categorical,venaturum,closed,2020-12-30T03:06:51Z,2020-12-31T22:09:33Z,"Unobserved categories in Series were being dropped in value_counts, which was inconsistent with Series.value_counts

- [x] closes #38672
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
771333684,38578,"TST/REF: io/parser/(test_dtypes.py, test_usecols.py)",arw2019,closed,2020-12-19T08:08:08Z,2020-12-31T22:13:44Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

#38370 adds a pyarrow engine to the csv reader. Only a fraction of the io/parser tests pass when pyarrow is used and the rest has to be xfailed/skipped, resulting in a large diff on the PR.

xref https://github.com/pandas-dev/pandas/pull/38370#discussion_r539579072 suggests reorganizing the tests into classes so groups of tests can be xfailed with a single mark.

I'm grouping the tests logically (not based on whether or not they pass with pyarrow) but merging this this _will_ reduce the diff in #38370 substantively.

Likely I will submit a follow-on with some further reorg. I'm happy to push that to this PR if that's preferred, though.

Verifying that total number of tests is unchanged:
```
(pandas-dev) andrewwieteska@Andrews-MacBook-Pro pandas % pytest  pandas/tests/io/parser/test_dtypes.py  pandas/tests/io/parser/test_usecols.py       
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.6, pytest-6.1.2, py-1.9.0, pluggy-0.13.1
rootdir: /Users/andrewwieteska/repos/pandas, configfile: setup.cfg
plugins: forked-1.2.0, xdist-2.1.0, cov-2.10.1, asyncio-0.14.0, hypothesis-5.41.2, instafail-0.4.1
collected 372 items                                                                                                                                                   

pandas/tests/io/parser/test_dtypes.py ......................................................................................................................... [ 32%]
............................................................................................                                                                    [ 57%]
pandas/tests/io/parser/test_usecols.py ........................................................................................................................ [ 89%]
.......................................                                                                                                                         [100%]

======================================================================== 372 passed in 32.49s =========================================================================
```
versus on master:
```
(pandas-dev) andrewwieteska@Andrews-MacBook-Pro pandas % pytest  pandas/tests/io/parser/test_dtypes.py  pandas/tests/io/parser/test_usecols.py
========================================================================= test session starts =========================================================================
platform darwin -- Python 3.8.6, pytest-6.1.2, py-1.9.0, pluggy-0.13.1
rootdir: /Users/andrewwieteska/repos/pandas, configfile: setup.cfg
plugins: forked-1.2.0, xdist-2.1.0, cov-2.10.1, asyncio-0.14.0, hypothesis-5.41.2, instafail-0.4.1
collected 372 items                                                                                                                                                   

pandas/tests/io/parser/test_dtypes.py ......................................................................................................................... [ 32%]
............................................................................................                                                                    [ 57%]
pandas/tests/io/parser/test_usecols.py ........................................................................................................................ [ 89%]
.......................................                                                                                                                         [100%]

======================================================================== 372 passed in 32.90s =========================================================================
```"
776941947,38849,TST: GH30999 address all bare pytest.raises in pandas/tests/arrays/boolean/test_arithmetic.py,moink,closed,2020-12-31T12:14:36Z,2021-01-01T00:03:54Z,"xref #30999 I thought I was done with the simple bare `pytest.raise` instances but I somehow missed this test module. Four instances, three fixed by adding `match` and one by using `tm.external_error_raised` because I believe the error comes from numpy.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
775207719,38744,REF/POC: Share groupby/series algos (rank),mzeitlin11,closed,2020-12-28T05:40:50Z,2021-01-01T00:21:34Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Right now there are 3 rank algorithms which are extremely similar, but have slight differences. This seems to cause some maintenance pain.  First, inconsistencies between implementations allows more bug potential (like #32593, only a bug for `rank_2d`). For issues like #32859 which affect all 3, a fix would have to be applied in 3 places and testing added separately for `Series`, `DataFrame`, and `GroupBy`. This pr attempts to mitigate these issues by combining the implementations `group_rank` and `rank_1d` (which as an additional bonus gives the enhancement of object support for `GroupBy.rank()` (#38278)).

Is this kind of refactor/deduplication helpful? If so, similar logic can probably be applied elsewhere.

The diff here makes the changes look more complicated than they are because `rank_1d` is essentially replaced by `group_rank`. The original `group_rank` implementation is only slightly changed to allow for optional labels and object support.


Benchmarks look ok:
<details>

```
      before           after         ratio
     [9f1a41de]       [5cd81d25]
     <master>         <ref/rank>
       7.87±0.1ms       7.77±0.1ms     0.99  frame_methods.Rank.time_rank('float')
       2.61±0.1ms       2.55±0.1ms     0.97  frame_methods.Rank.time_rank('int')
         57.9±3ms         59.0±6ms     1.02  frame_methods.Rank.time_rank('object')
      2.67±0.04ms      2.58±0.08ms     0.97  frame_methods.Rank.time_rank('uint')
      
       10.8±0.4ms       9.45±0.3ms    ~0.87  series_methods.Rank.time_rank('float')
       7.39±0.2ms         6.79±1ms     0.92  series_methods.Rank.time_rank('int')
         52.9±4ms         47.7±2ms    ~0.90  series_methods.Rank.time_rank('object')
         7.32±1ms       6.58±0.3ms    ~0.90  series_methods.Rank.time_rank('uint')
         
         
          314±3μs         352±40μs    ~1.12  groupby.GroupByMethods.time_dtype_as_field('datetime', 'rank', 'direct')
          316±8μs         322±10μs     1.02  groupby.GroupByMethods.time_dtype_as_field('datetime', 'rank', 'transformation')
         421±10μs         475±60μs    ~1.13  groupby.GroupByMethods.time_dtype_as_field('float', 'rank', 'direct')
         409±10μs         482±60μs    ~1.18  groupby.GroupByMethods.time_dtype_as_field('float', 'rank', 'transformation')
          505±3μs         420±10μs    ~0.83  groupby.GroupByMethods.time_dtype_as_field('int', 'rank', 'direct')
-        510±20μs          410±3μs     0.80  groupby.GroupByMethods.time_dtype_as_field('int', 'rank', 'transformation')
         411±20μs         441±60μs     1.07  groupby.GroupByMethods.time_dtype_as_group('datetime', 'rank', 'direct')
         486±30μs         509±10μs     1.05  groupby.GroupByMethods.time_dtype_as_group('datetime', 'rank', 'transformation')
         470±50μs          424±9μs    ~0.90  groupby.GroupByMethods.time_dtype_as_group('float', 'rank', 'direct')
         409±10μs         505±60μs    ~1.23  groupby.GroupByMethods.time_dtype_as_group('float', 'rank', 'transformation')
          404±7μs         470±60μs    ~1.16  groupby.GroupByMethods.time_dtype_as_group('int', 'rank', 'direct')
          407±7μs          412±9μs     1.01  groupby.GroupByMethods.time_dtype_as_group('int', 'rank', 'transformation')
 ```
 
 </details>
         
         "
749301165,38029,BUG: unstack with missing levels results in incorrect index names,GYHHAHA,closed,2020-11-24T03:07:32Z,2021-01-01T01:35:55Z,"- [x] closes #37510
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
777155027,38869,TST/REF: implement tests.frame.constructors,jbrockmendel,closed,2020-12-31T23:07:40Z,2021-01-01T02:02:11Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
776168032,38792,"BUG: DataFrame(dt64data, dtype=td64) corner cases",jbrockmendel,closed,2020-12-30T02:29:39Z,2021-01-01T02:04:12Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
683098647,35832,Jsonlines append mode,nullhack,closed,2020-08-20T21:10:35Z,2021-01-01T08:21:55Z,"**Description**: Adding support to append mode for `to_json` when 'orient' is `records` and 'lines' is `True`

**Motivation**: [jsonlines](http://jsonlines.org/) is a format that is gaining some space (e.g. [Cloud Storage](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json)). In pandas It's achieved using `to_json` with `orient='records'` and `lines=True`. But `to_json` doesn't have the option to use `mode='a'` to append to a file, this PR aims to provide a simple append mode for jsonlines using pandas.

---

- [X] closes #35849
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
777248416,38875,QST: function similarity,ZhangLockerberg,closed,2021-01-01T09:09:19Z,2021-01-01T09:15:40Z,Does Pandas also support a function like what np.vstack or np.hstack does?
777071117,38854,Fix broken link in docs of DataFrame.to_hdf,felixdivo,closed,2020-12-31T15:58:31Z,2021-01-01T12:14:28Z,"The link in https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_hdf.html in ""See also"" seems to be broken. This should fix it.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
776577369,38824,TST:  add missing loc label indexing test,vangorade,closed,2020-12-30T18:15:28Z,2021-01-01T18:18:05Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
635654526,34673,BUG: read_excel: incorrect multi-index values - 2,kuraga,closed,2020-06-09T18:11:00Z,2021-01-01T19:45:27Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
df = pd.read_excel('test.xlsx', header=[0, 1], index_col=[0, 1])
print(df.index)
```
(`engine` could be `xlrd` or `openpyxl`, doesn't matter.)

[test.xlsx](https://github.com/pandas-dev/pandas/files/4753954/test.xlsx)
![image](https://user-images.githubusercontent.com/1063219/84183752-0d682700-aa95-11ea-932b-4d35ffbb6a9c.png)

#### Problem description

```
MultiIndex([('x', 'B')],
           names=['A', 'B'])
```

#### Expected Output

```
MultiIndex([('x', None)],
           names=['A', 'B'])
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.42-calculate
machine          : x86_64
processor        : Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz
byteorder        : little
LC_ALL           : None
LANG             : ru_RU.utf8
LOCALE           : ru_RU.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.4.0.post20200518
Cython           : 0.29.17
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.8
numba            : None
</details>
"
768402779,38517,BUG: read_excel forward-filling MI names,mzeitlin11,closed,2020-12-16T02:58:53Z,2021-01-01T20:02:32Z,"- [x] closes #34673
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Added test with OP's issue as a sheet in the style of existing sheets in `tests/io/data/excel/testmultiindex.*`

(the same as the ""both_name"" sheet, but with the cell below the MI name empty)

New sheet looks like:
<img width=""393"" alt=""Screen Shot 2020-12-15 at 9 50 02 PM"" src=""https://user-images.githubusercontent.com/37011898/102298700-961dbf00-3f1f-11eb-9c98-034268ad9d68.png"">

In master without this change, the empty cell was being filled with the index name `ilvl2`"
54794700,9304,additional keys in groupby indices when NAs are present,josepm,closed,2015-01-19T18:03:13Z,2021-01-01T20:41:44Z,"```
In [386]: h = pd.DataFrame({'a':[1,2,1,np.nan,1], 'b':[1,2,3,3,2], 'c':[2,3,1,4,2]})

In [387]: gh=h.groupby(['a', 'b'])

In [388]: gh.groups.keys()
Out[388]: [(1.0, 2), (nan, 3), (1.0, 3), (1.0, 1), (2.0, 2)]

In [389]: gh.indices.keys()
Out[389]: [(1.0, 2), (1.0, 3), (2.0, 3), (1.0, 1), (2.0, 2)]  # Incorrect
```

The tuple (2.0, 3) should not be here.
The problem goes away when there are no NAs
"
777127046,38864,TST: GH30999 add match=msg to 2 pytest.raises in pandas/tests/io/json/test_normalize.py,moink,closed,2020-12-31T20:06:22Z,2021-01-01T20:43:23Z,"xref #30999 for pandas/tests/io/json/test_normalize.py

And with this my work on #30999 is done. Will head over to the issue to explain why I am not fixing the remaining 15 instances of `pytest.raise`

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
693718240,36131,BUG: Timestamp == date match stdlib,jbrockmendel,closed,2020-09-04T22:08:55Z,2021-01-01T21:09:12Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

ATM we have one reasonable behavior `Timestamp(""2020-09-04"") == date(2020, 9, 4)` and two un-reasonable behaviors: ``Timestamp(""2020-09-04"").tz_localize(""US/Pacific"") == date(2020, 9, 4)`, `Timestamp.now() == Timestamp.now().date()`.  Since the stdlib datetime doesnt consider `datetime(2020, 9, 4) == date(2020, 9, 4)`, this follows the stdlib and considers them never equal.

<s>I'm still getting one test failure locally.</s>

cc @mroeschke "
777183051,38871,TST: move generic/methods/ files to frame/methods/,jbrockmendel,closed,2021-01-01T03:04:28Z,2021-01-01T21:11:38Z,
776687136,38839,DOC: fix includes,afeld,closed,2020-12-31T00:10:27Z,2021-01-01T21:14:25Z,"- Added missing `.` to the `.. include`s
- Put the includes in a folder to exclude from Sphinx, to avoid `:orphan:` being written to the HTML

Before:

<img width=""1156"" alt=""Screen_Shot_2020-12-30_at_7_08_52_PM"" src=""https://user-images.githubusercontent.com/86842/103387595-99de4380-4ad2-11eb-85bf-8ac8da85676a.png"">

After:

<img width=""1186"" alt=""Screen Shot 2020-12-30 at 7 07 41 PM"" src=""https://user-images.githubusercontent.com/86842/103387540-5f74a680-4ad2-11eb-992c-44de074462ff.png"">

---

- [ ] ~~closes #xxxx~~
- [ ] tests added / passed
- [ ] ~~passes `black pandas`~~
- [ ] ~~passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`~~
- [ ] ~~whatsnew entry~~
"
776665611,38834,BUG: IntervalIndex.intersection returning duplicates,phofl,closed,2020-12-30T22:46:08Z,2021-01-01T21:33:37Z,"- [x] closes #38743
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Moved the wrong test code into a new test with adjusted behavior
"
604651749,33719,BUG: idxmin() fails for nullable integer data type (Int64),cklb,closed,2020-04-22T10:41:20Z,2021-01-01T21:38:39Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample

```python
import pandas as pd

s = pd.Series([1, 2, 3, 4], dtype=""Int64"")
s.idxmax()
```

#### Problem description

Currently, the `idxmax` call fails with
```python-traceback
  File ""~lib/python3.8/site-packages/pandas/core/series.py"", line 2110, in idxmax
    i = nanops.nanargmax(com.values_from_object(self), skipna=skipna)
  File ""~lib/python3.8/site-packages/pandas/core/nanops.py"", line 64, in _f
    raise TypeError(
TypeError: reduction operation 'argmax' not allowed for this dtype
```
being raised. This may be a duplicate of #32749, although the focus there seems to be generating the exception I already encountered. 

#### Expected Output
Should be `3`, I guess.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.5.13-arch2-1
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : de_DE.UTF-8
LOCALE           : de_DE.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.0
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : None
numba            : None
None

</details>
"
582289958,32749,BUG: regression in error raised by idxmin/idxmax for extension dtypes,jorisvandenbossche,closed,2020-03-16T13:21:04Z,2021-01-01T21:38:39Z,"
With pandas 1.0.1:

```
In [7]: from pandas.tests.extension.decimal import DecimalArray, make_data 

In [8]: s = pd.Series(DecimalArray(make_data()[:5]))  

In [9]: s 
Out[9]: 
0    Decimal: 0.25866656130631138221787068687262944...
1    Decimal: 0.11511905452780368808163302674074657...
2    Decimal: 0.15301679241167220890673661415348760...
3    Decimal: 0.41125672759464626526693109553889371...
4    Decimal: 0.46391316685725048074573351186700165...
dtype: decimal

In [10]: s.idxmin()   
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-10-5060aab05d12> in <module>
----> 1 s.idxmin()

~/miniconda3/envs/pandas10/lib/python3.8/site-packages/pandas/core/series.py in idxmin(self, axis, skipna, *args, **kwargs)
   2037         """"""
   2038         skipna = nv.validate_argmin_with_skipna(skipna, args, kwargs)
-> 2039         i = nanops.nanargmin(com.values_from_object(self), skipna=skipna)
   2040         if i == -1:
   2041             return np.nan

~/miniconda3/envs/pandas10/lib/python3.8/site-packages/pandas/core/nanops.py in _f(*args, **kwargs)
     62             if any(self.check(obj) for obj in obj_iter):
     63                 f_name = f.__name__.replace(""nan"", """")
---> 64                 raise TypeError(
     65                     f""reduction operation '{f_name}' not allowed for this dtype""
     66                 )

TypeError: reduction operation 'argmin' not allowed for this dtype
```

Now with master you get:

```
In [4]: s.idxmin() 
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-4-7e62303a9d43> in <module>
----> 1 s.idxmin()

~/scipy/pandas/pandas/core/series.py in idxmin(self, axis, skipna, *args, **kwargs)
   1988         """"""
   1989         skipna = nv.validate_argmin_with_skipna(skipna, args, kwargs)
-> 1990         i = nanops.nanargmin(self._values, skipna=skipna)
   1991         if i == -1:
   1992             return np.nan

~/scipy/pandas/pandas/core/nanops.py in _f(*args, **kwargs)
     69             try:
     70                 with np.errstate(invalid=""ignore""):
---> 71                     return f(*args, **kwargs)
     72             except ValueError as e:
     73                 # we want to transform an object array

~/scipy/pandas/pandas/core/nanops.py in nanargmin(values, axis, skipna, mask)
    943     """"""
    944     values, mask, dtype, _, _ = _get_values(
--> 945         values, True, fill_value_typ=""+inf"", mask=mask
    946     )
    947     result = values.argmin(axis)

~/scipy/pandas/pandas/core/nanops.py in _get_values(values, skipna, fill_value, fill_value_typ, mask)
    311         # promote if needed
    312         else:
--> 313             values, _ = maybe_upcast_putmask(values, mask, fill_value)
    314 
    315     # return a platform independent precision dtype

~/scipy/pandas/pandas/core/dtypes/cast.py in maybe_upcast_putmask(result, mask, other)
    278     """"""
    279     if not isinstance(result, np.ndarray):
--> 280         raise ValueError(""The result input must be a ndarray."")
    281     if not is_scalar(other):
    282         # We _could_ support non-scalar other, but until we have a compelling

ValueError: The result input must be a ndarray.
```

@jbrockmendel this is from no longer using `values_from_object`, but passing `_values` to the nanops function, while this expects to always receive an ndarray"
707054442,36566,BUG: TypeError on Series(dtype='string').value_counts().idxmax(),wkschwartz,closed,2020-09-23T05:16:43Z,2021-01-01T21:38:40Z,"# Expected output
Obtain the most frequent string in a series:
```pycon
>>> pd.Series('a').value_counts().idxmax()                
'a'
```

# Problem
Changing the dtype on the second example from `'object'` to `'string'` breaks this idiom:

```python-traceback
>>> pd.Series('a', dtype='string').value_counts().idxmax()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pandas/core/series.py"", line 2168, in idxmax
    i = nanops.nanargmax(self._values, skipna=skipna)
  File ""pandas/core/nanops.py"", line 71, in _f
    return f(*args, **kwargs)
  File ""pandas/core/nanops.py"", line 924, in nanargmax
    result = values.argmax(axis)
TypeError: argmax() takes 1 positional argument but 2 were given
```
The problem is evidently not due to a `'string'` index:
```pycon
>>> pd.Series([1], index=pd.Series(['a'], dtype='string')).idxmax()
'a'
```
# Output of ``pd.show_versions()``

<details>

```
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.7.9.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.2
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.1.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
```

</details>
"
638212831,34754,ENH: add masked algorithm for mean(),jorisvandenbossche,closed,2020-06-13T18:55:35Z,2021-01-01T21:46:48Z,"Similarly as we now have masked implementations for sum, prod, min and max for the nullable integer array (first PR https://github.com/pandas-dev/pandas/pull/30982, now lives at https://github.com/pandas-dev/pandas/blob/master/pandas/core/array_algos/masked_reductions.py), we can add one for the `mean` reduction as well.

Very rough check gives a nice speed-up:

```
In [27]: arr = pd.array(np.random.randint(0, 1000, 1_000_000), dtype=""Int64"") 

In [28]: arr[np.random.randint(0, 1_000_000, 1000)] = pd.NA 

In [30]: arr._reduce(""mean"") 
Out[30]: 499.27095868772903

In [31]: %timeit arr._reduce(""mean"") 
7.26 ms ± 335 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [32]: arr._data.sum(where=~arr._mask, dtype=""float64"") / (~arr._mask).sum() 
Out[32]: 499.27095868772903

In [33]: %timeit arr._data.sum(where=~arr._mask, dtype=""float64"") / (~arr._mask).sum()  
2.08 ms ± 6.89 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

The `nanmean` version lives here: https://github.com/pandas-dev/pandas/blob/master/pandas/core/nanops.py#L517 
And as reference, numpy is also adding a version that accepts a mask: https://github.com/numpy/numpy/pull/15852 (which could be used in the future, and as inspiration for the implementation now).
"
639331094,34814,ENH: add masked algorithm for mean() function,Akshatt,closed,2020-06-16T02:57:34Z,2021-01-01T21:46:52Z,"- [x] closes #34754 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
606666685,33782,BUG: Add warning if rows have more columns than expected,mproszewska,closed,2020-04-25T02:08:16Z,2021-01-01T21:54:02Z,"- [x] closes #33037
- [x] tests added and passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] warning when read rows have more columns than expected"
767152375,38488,BUG: wrong result in rolling mean\sum (possibly related with cython implementation),auderson,closed,2020-12-15T04:06:49Z,2021-01-01T21:54:54Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
df = pd.Series(np.random.random(20), dtype='float64')

df[:10] *= 1e6
df[-10:] = 0
df
Out[124]: 
0     235266.408832
1     979278.480747
2     242108.917259
3     513980.266425
4      44316.957433
5     408655.731842
6     490360.015982
7     855399.526047
8     321975.011810
9     831082.388210
10         0.000000
11         0.000000
12         0.000000
13         0.000000
14         0.000000
15         0.000000
16         0.000000
17         0.000000
18         0.000000
19         0.000000
dtype: float64

df.rolling(6).mean()
Out[126]: 
0              NaN
1              NaN
2              NaN
3              NaN
4              NaN
5     2.682618e+05
6     2.723245e+05
7     2.937347e+05
8     3.589816e+05
9     2.737199e+05
10    2.429527e+05
11    2.411146e+05
12    1.258042e+05
13    9.726441e+04
14    1.032524e+04
15    2.546585e-11
16    2.546585e-11
17    2.546585e-11
18    2.546585e-11
19    2.546585e-11
dtype: float64

df.rolling(6).sum()
Out[127]: 
0              NaN
1              NaN
2              NaN
3              NaN
4              NaN
5     1.609571e+06
6     1.633947e+06
7     1.762408e+06
8     2.153889e+06
9     1.642319e+06
10    1.457716e+06
11    1.446688e+06
12    7.548251e+05
13    5.835865e+05
14    6.195146e+04
15    1.527951e-10
16    1.527951e-10
17    1.527951e-10
18    1.527951e-10
19    1.527951e-10
dtype: float64
```

#### Problem description

if the head is substantially larger than the 0s in the tail, then rolling mean\sum will not generate correct results. I roughly looked into the source code and guess maybe it's related to cython ""rolling_mean""

#### Expected Output

0 in row 15 to 20, not 2.546585e-11

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-112-generic
Version          : #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.3.1
setuptools       : 51.0.0.post20201207
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.6.1
html5lib         : None
pymysql          : 0.9.2
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: 0.9.0
bs4              : None
bottleneck       : None
fsspec           : 0.8.4
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.4
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.51.2

</details>
"
687768820,35947,paste_windows() wrong arg for c_wchar_p,folger,closed,2020-08-28T06:16:24Z,2021-01-01T22:07:33Z,"paste_windows() now directly used handle return from safeGetClipboardData(CF_UNICODETEXT) as argument for c_wchar_p, which should used safeGlobalLock(handle) instead, while copy_windows(text) doing the right thing.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
734074478,37569,PERF: faster numeric indexes comparisons when self is identical to other ,topper-123,closed,2020-11-01T22:11:40Z,2021-01-01T22:13:54Z,"Further performance improvement over #37130, this time improving all numeric indexes. The improvement is larger for `Int64Index`, because that doesn't need to check for `na` values.

Examples:

```python
>>> n = 100_000
>>> idx1 = pd.Int64Index(range(n))
>>> idx2 = idx1.view()
>>> %timeit idx1 == idx2
145 µs ± 2.43 µs per loop  # master
11.4 µs ± 290 ns per loop  # this PR
>>> idx3 = pd.Float64Index(range(n))
>>> idx4 = idx3.view()
>>> %timeit idx3 == idx4
104 µs ± 2.12 µs per loop  # master
49.8 µs ± 886 ns per loop  # this PR
```
"
720895984,37109,PERF: Faster comparisons of indexes when compared to self,topper-123,closed,2020-10-13T21:59:33Z,2021-01-01T22:19:25Z,"Examples:

```python
>>> rng = pd.RangeIndex(100_000)
>>> %timeit rng == rng
429 µs ± 96.9 µs per loop  # master
14.1 µs ± 93.8 ns per loop  # this PR
>>> idx = rng.astype(int)
>>> %timeit idx == idx
347 µs ± 1.42 µs per loop  #master
14.1 µs ± 91.4 ns per loop  # this PR
>>> idx = rng.astype(object)
>>> %timeit idx == idx
5.97 ms ± 12.7 µs per loop  # master
3.72 ms ± 28.4 µs per loop  # this PR
```
"
752958890,38153,BUG: loc raising KeyError for string slices in list-like indexer and DatetimeIndex,phofl,closed,2020-11-29T16:56:26Z,2021-01-01T22:34:51Z,"- [x] closes #27180
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I looked into this and added an implementation for list-like indexers containing slices and DatetimeIndexes. If we want to support this we probably have to do something like this."
741047127,37765,CI: migrate from travis-ci.org to travis-ci.com,jorisvandenbossche,closed,2020-11-11T20:25:23Z,2021-01-01T22:53:51Z,"See https://docs.travis-ci.com/user/migrate/open-source-repository-migration, if we want to keep the Travis CI builds, we need to migrate to the .com version before the end of the year."
745170327,37924,BUG: idxmax/min (and argmax/min) for Series with underlying ExtensionArray,tonyyyyip,closed,2020-11-17T23:44:09Z,2021-01-02T00:26:01Z,"- [x] closes #32749, closes #33719, closes #36566
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Currently `pd.Series([1, 2, 3]).astype('Int8').idxmax()` raises an error.
This request proposes a fix.
My first time using Github and opening a pull request. Apologise if it's no good."
701591230,36375,BUG: MultiIndex.from_tuples() fails to infer dtype from nan-only values from an index,ssche,open,2020-09-15T04:09:50Z,2021-01-02T01:31:45Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
In [23]: df2 = pd.DataFrame({'a': [np.nan, 1, 2, 1], 'b': [1, 1, 2, 1], 'd': [np.nan, np.nan, np.nan, np.nan], 'c': [1, 1, 1, 1]}) 

In [24]: df2.dtypes

Out[24]: 
a    float64
b      int64
d    float64
c      int64
dtype: object

In [29]: s2 = df2.groupby(['a', 'b', 'd'], dropna=False)['c'].sum()                                                                                                                                
In [30]: s2.index                                                                                                                                                                                  
Out[30]: 
MultiIndex([(1.0, 1, nan),
            (2.0, 2, nan),
            (nan, 1, nan)],
           names=['a', 'b', 'd'])

In [31]: s2.index.get_level_values(2).dtype                                                                                                                                                        
Out[31]: dtype('float64')

In [32]: pd.MultiIndex.from_tuples(s2.index)                                                                                                                                                       
Out[32]: 
MultiIndex([(1.0, 1, nan),
            (2.0, 2, nan),
            (nan, 1, nan)],
           )

In [34]: pd.MultiIndex.from_tuples(s2.index).get_level_values(2).dtype                                                                                                                             
Out[34]: dtype('O')
```

#### Problem description

When a `MultiIndex` is re-created from another `MultiIndex` which contains a nan-only column by using `.from_tuples()`, the dtype of the nan-only level is `object` when it should be `float64`.

As a workaround, the dtype can be inferred correctly when the original index is wrapped in a `tuple`:
```python
In [35]: pd.MultiIndex.from_tuples(tuple(s2.index)).get_level_values(2).dtype                                                                                                                      
Out[35]: dtype('float64')
```

This problem also doesn't occur when there is at least one non-nan value in the index column:
```python
In [36]: df3 = pd.DataFrame({'a': [np.nan, 1, 2, 1], 'b': [1, 1, 2, 1], 'd': [np.nan, np.nan, np.nan, 1], 'c': [1, 1, 1, 1]})
In [37]: pd.MultiIndex.from_tuples(df3.groupby(['a', 'b', 'd'], dropna=False)['c'].sum().index).get_level_values(2).dtype
Out[37]: dtype('float64')
```


#### Expected Output
The dtype is inferred correctly even without wrapping the source index into a tuple.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_AU.UTF-8
LOCALE           : en_AU.UTF-8

pandas           : 1.1.2
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 46.4.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
777331926,38879,BUG: silently ignoring dtype kwarg in Index.__new__,jbrockmendel,closed,2021-01-01T18:37:36Z,2021-01-02T01:37:47Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

ATM we handle object but not others, xref #21311"
777086682,38857,"BUG: DataFrame(ndarray, dtype=categoricaldtype)",jbrockmendel,closed,2020-12-31T17:01:44Z,2021-01-02T02:17:50Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
777254292,38877,BUG: rolling does not accept MultiIndex name,metazoic,closed,2021-01-01T09:50:55Z,2021-01-02T02:21:36Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

If
```python
df = pandas.DataFrame()
df['date'] = pandas.date_range('2021-1-1', '2021-1-5', freq='h')
df.loc[df.date <= '2021-1-3', 'category'] = 'A'
df.loc[df.date > '2021-1-3', 'category'] = 'B'
df.set_index(['category','date'], inplace=True)
df['value'] = 1
```
then
```python
df.groupby('category').rolling('D', on='date')
```
breaks with
```python
ValueError: invalid on specified as date, must be a column (of DataFrame), an Index or None
```

#### Problem description

The [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html?highlight=rolling#pandas.DataFrame.rolling) states that the `on` argument can be a `MultiIndex` level (and then confusingly adds ""rather than the DataFrame's index"").

#### Expected Output

`rolling` should accept a `MultiIndex` level. Interestingly,
```python
df.groupby('category').resample('D', level='date')
```
works, but it would be nice to unify the interfaces: `resample` accepts `level` and `on` for index and column respectively, while `rolling` accepts `on` for both."
532611142,30041,arm64 CI takes too much time to complete .,ossdev07,closed,2019-12-04T11:03:00Z,2021-01-02T04:38:39Z,"Hi, I am trying to add support for arm64 in travis CI for Pandas. but unfortunately, all tests are taking more than 50min to complete and hence, travis shows up :
`The job exceeded the maximum time limit for jobs, and has been terminated.`

I have attached the logs here:
[travis-ci-test.txt](https://github.com/pandas-dev/pandas/files/3921219/travis-ci-test.txt)

Can anyone help me to identify the root cause for this ? "
637327297,34720,BUG #34621 added nanosecond support to class Period,OlivierLuG,closed,2020-06-11T21:23:01Z,2021-01-02T08:31:58Z,"Closes #34621
Closes #17053
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

The class Period do not support nanosecond. I've made a quick and dirty code to support nanoseconds. I have struggled to find an alternative to dateutil.parser, but I didn't found an alternative.

The PR may be a performance issue as I've add a Timestamp constructor to the dateutil.parser. There is for sure a better solution. Please let me know !"
638214188,34755,TST: Period with Timestamp overflow,OlivierLuG,closed,2020-06-13T19:04:44Z,2021-01-02T08:31:59Z,"- [x] closes #13346
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

The first part of the test checks that the period inside boundaries does not raise exception when calling `start_time` and `end_time` methods. The second part checks that methods raise exceptions when outside of boundaries.
The parameters are `Timestamp.min` and `Timestamp.max`, so the tests will follow any change of the boundaries.
"
632648773,34622,TST: boolean indexing using .iloc #20627,OlivierLuG,closed,2020-06-06T18:24:00Z,2021-01-02T08:31:59Z,"- [x] closes #20627
- [1] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

A simple test was added to close this issue.
"
629513991,34537,CI/TST #34131 fixed test_floordiv_axis0_numexpr_path,OlivierLuG,closed,2020-06-02T21:04:50Z,2021-01-02T08:31:59Z,"xref #34131 

- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Test `/pandas/tests/frame/test_floordiv_axis0_numexpr_path` is slow. It builds a df with 10^6 elements to tests functions `floordiv` and `pow`. The number of elements can be reduced with keeping the same min/max values inside the dataframe.

Before/after timeit comparison:
```
26.6 s ± 1.04 s per loop (mean ± std. dev. of 7 runs, 1 loop each)
37.6 ms ± 3.45 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```"
757724441,38317,DEPR: xlwt for writing excel files,rhshadrach,closed,2020-12-05T17:04:46Z,2021-01-02T14:15:03Z,"- [x] closes #26552
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Deprecates both specifying `engine=""xlwt""` and the option `io.excel.xls.writer`. Note that a warning is emitted when a user sets the `io.excel.xls.writer` option, but not via the context manager. Whatsnew now reads:

![image](https://user-images.githubusercontent.com/45562402/101409318-bd371980-38ab-11eb-80f4-f64afc259438.png)

"
733606008,37530,CLN: Use ABCNDFrame instead of ABCDataFrame and ABCSeries,rhshadrach,closed,2020-10-31T02:00:13Z,2021-01-02T14:15:04Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
764092921,38427,REGR: Assigning label with registered EA dtype raises,rhshadrach,closed,2020-12-12T17:35:29Z,2021-01-02T14:16:01Z,"- [x] closes #38386
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Previous behavior in `io.parsers` depended on `is_bool_dtype(""boolean"")` erroneously raising `AttributeError`.

Test currently iterates over the names:

> ['category', 'interval', 'boolean', 'Float32', 'Float64', 'Int8', 'Int16', 'Int32', 'Int64', 'UInt8', 'UInt16', 'UInt32', 'UInt64', 'string']

It skips any EA where the name is a property; I don't see a good way to iterate over these in the test.

I only added a note about `is_bool_dtype` in the whatsnew as the `DataFrame.__setitem__` issue does not occur on 1.1.x. It didn't seem appropriate for any section besides other."
777336146,38881,TST: strictly xfail mid-test,rhshadrach,closed,2021-01-01T19:07:34Z,2021-01-02T15:41:53Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Using `pytest.xfail` when a test is running will immediately stop the test and will not xpass if test would otherwise succeed. It doesn't seem to be documented anywhere, but an alternative would be to use the request fixture. In this demo, I've added an xfail that runs successfully, making the test fail (xpass).

If this looks like a good idea, I can make a tracking issue to replace `pytest.xfail` with the method used here.
"
777101658,38861,BUG: additional keys in groupby indices when NAs are present,rhshadrach,closed,2020-12-31T18:06:26Z,2021-01-02T20:52:46Z,"- [x] closes #9304
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

`lib.indices_fast` included logic to skip over null values (-1's), but did not include the case where the first index is null. Currently this function is only used in `sorting.get_indexer_dict` where the case of all null group_index is handled separately."
766932478,38471,DOC: fixes for assert_frame_equal check_freq argument,kylekeppler,closed,2020-12-14T21:12:10Z,2021-01-03T01:38:30Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This is just a minor documentation change to fix the `versionadded` tag on the `check_freq` argument to `assert_frame_equal` and also to add a description of this backwards incompatible chage to the 1.1.0 Whats New (as recommended in https://github.com/pandas-dev/pandas/issues/35570#issuecomment-674064533)."
777253854,38876,TST: GH30999 TESTING PR do not merge or review,moink,closed,2021-01-01T09:47:17Z,2021-01-03T10:31:13Z,"There are 3 tests that don't run on my dev machine even with tweaks. They should fail in the CI and I can get the messages from there. So for now this is just a PR designed to run (and fail) the CI pipelines.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
563937349,31920,pd.read_csv does not recognize scientific notation if 'decimal' attribute is set with engine=python,onnoeberhard,closed,2020-02-12T11:30:37Z,2021-01-03T16:36:21Z,"Contents of 'abc.tsv':
```
a	b
1,2	3,5E-1
1,2e2	3,5
```
Code:
```python
>>> import pandas as pd
>>> pd.read_csv('abc.tsv', '\t', decimal=',')
       a     b
0    1.2  0.35
1  120.0  3.50
>>> pd.read_csv('abc.tsv', '\t', decimal=',', engine='python')
       a       b
0    1.2  3,5E-1
1  1,2e2     3.5
```
#### Problem description
The outputs of the two read_csv commands above should be the same. In this case an obvious ""solution"" is to just use the C-engine, however I have to process a file where I need to use a regular expression as the delimiter.

#### Expected Output
```python
>>> pd.read_csv('abc.tsv', '\t', decimal=',', engine='python')
       a     b
0    1.2  0.35
1  120.0  3.50
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.2.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : None
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : 0.29.15
pytest           : 5.3.5
hypothesis       : 5.4.1
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.48.0

</details>
"
665863196,35416,ENH: Auto Detect engine in read_excel,fzumstein,closed,2020-07-26T21:26:31Z,2021-01-03T16:48:19Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

This works:

```python
import pandas as pd
df = pd.read_excel('Book2.xlsb', engine='pyxlsb')
```

This doesn't:

```python
import pandas as pd
df = pd.read_excel('Book2.xlsb')
```

#### Problem description

All supported Excel formats (`xls`, `xlsx`, `xlsm`) have a default engine based on the extension, so you can simply do:

```
df = pd.read_excel('Book2.xlsx')
```

For `xlsb`,  when you do `df = pd.read_excel('Book2.xlsb')`, you get this error:

```
XLRDError: Excel 2007 xlsb file; not supported
```

#### Expected Output

No error, i.e. it should figure out that for `xlsb` extensions, `pyxlsb` is the default engine.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None

pandas           : 1.0.5
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 49.2.0.post20200714
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pytest           : 5.4.3
pyxlsb           : 1.0.6
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : None

</details>
"
620860005,34252,"pd.set_option(""io.excel.xlsx.reader"", ""openpyxl"") does not work",simonm3,closed,2020-05-19T10:24:06Z,2021-01-03T16:48:19Z,"This works fine for an xlsx file:

`pd.read_excel(path, engine=""openpyxl"")`

However if i try using set_option then read_excel fails:
```
pd.set_option(""io.excel.xlsx.reader"", ""openpyxl"")
pd.read_excel(path)
```
ImportError: Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd."
777351534,38882,CLN: Use signed integers in khash maps for signed integer keys,realead,closed,2021-01-01T20:56:10Z,2021-01-03T16:56:28Z,"
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

The goal of this PR is to make this comment obsolete: https://github.com/pandas-dev/pandas/blob/1fc5efd76ea1f85979fa2364a292d85482e338aa/pandas/_libs/src/klib/khash.h#L619-L624

see also [this discussion]( https://github.com/pandas-dev/pandas/pull/37920#discussion_r527191378) for even more details.

The issue is:

- the current way (inherited from khash-original) is too clever (and suprising)
- it is wrong as the (unsigned) keys are converted back to signed values (which is implementation defined behavior) for example here: https://github.com/pandas-dev/pandas/blob/1fc5efd76ea1f85979fa2364a292d85482e338aa/pandas/_libs/hashtable_func_helper.pxi.in#L110

Using signed integer, one must take into account that hash-function with signed integers might have undefined/implementation defined behavior, thus we cast signed integers to unsigned counterpart in the (64bit-) hash function now."
466931928,27338,CategoricalDtype.update_dtype fails with Categorical,WillAyd,closed,2019-07-11T14:35:26Z,2021-01-03T17:10:41Z,ref https://github.com/pandas-dev/pandas/pull/27318#discussion_r302265250 it appears that CategoricalDtype might want to accept a Categorical as an argument but fails to handle that properly. Not sure intent here but we either want to fix to allow Categorical or alternately simplify and only allow `str/CategoricalDtype` input
421764489,25744,Series.update fails with categorical types,AlexRiina,closed,2019-03-16T03:26:02Z,2021-01-03T17:10:41Z,"```python
cats = pandas.api.types.CategoricalDtype(['a', 'b', 'c', 'd'])
# cats = None
s1 = pandas.Series(['a', 'b', 'c'], index=[1, 2, 3], dtype=cats)
s2 = pandas.Series(['b', 'a'], index=[1, 2], dtype=cats)

s1.update(s2)
```

With the dtype=None, the `s1` series is updated to be `['b', 'a', 'c']` but with the `dtype=cats`, the update fails with error ""ValueError: NumPy boolean array indexing assignment cannot assign 3 input values to the 2 output values where the mask is true""

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.7.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-36-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 3.3.2
pip: 18.0
setuptools: 40.5.0
Cython: 0.27.3
numpy: 1.15.4
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 7.2.0
sphinx: 1.6.6
patsy: 0.5.1
dateutil: 2.7.2
pytz: 2018.4
blosc: None
bottleneck: 1.2.1
tables: None
numexpr: 2.6.4
feather: None
matplotlib: 3.0.1
openpyxl: 2.4.10
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.2
lxml.etree: 4.1.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.1
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
777242654,38873,TST: Series.update with categorical,ftrihardjo,closed,2021-01-01T08:35:16Z,2021-01-03T17:10:45Z,"- [ ] closes #25744, closes #27338
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
777459707,38899,Validate axis args,tonyyyyip,closed,2021-01-02T13:21:07Z,2021-01-03T17:14:44Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This is a follow up PR on #37924 @jreback cheers"
774954489,38710,ENH: Add xlsb auto detection to read_excel and respect default options,lithomas1,closed,2020-12-27T00:39:30Z,2021-01-03T17:19:47Z,"- [x] closes #35416 , closes #34252
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Note: Even though most of the reader options for read_excel only have 1 option, I think it is still useful to respect pandas.set_option. 


Also, if some people have older xlrd(e.g. 1.2.0) and don't have openpyxl, they can use options to force pandas to read xlsx files with xlrd. (this is possible now, but xlrd's maintainer has made it clear he doesn't support for things like this)"
777540082,38910,REF: simplify Index.__new__,jbrockmendel,closed,2021-01-02T23:10:19Z,2021-01-03T18:04:46Z,Avoid recursing where possible.
777551680,38912,CLN: re-use sanitize_index,jbrockmendel,closed,2021-01-03T01:02:09Z,2021-01-03T18:05:17Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
734437027,37584,CLN refactor core dtypes,MarcoGorelli,closed,2020-11-02T11:42:28Z,2021-01-03T18:06:10Z,"Some refactorings found by Sourcery https://sourcery.ai/

I've removed the ones of the kind
```diff
- if param:
-     var = a
- else:
-     var = b
+ var = a if param else b
```"
728102253,37361,CLN Apply style to Jupyter notebook,MarcoGorelli,closed,2020-10-23T10:26:29Z,2021-01-03T18:06:48Z,"We apply many style checks to our source code files / Python snippets in documentation, so I've applied them to the (lone) Jupyter notebook.

Not adding anything to CI as it's arguably not worth it for just a single file"
777513688,38907,BUG: casting on concat with empties,jbrockmendel,closed,2021-01-02T19:38:03Z,2021-01-03T18:08:41Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Covers the case missed by #38843"
767343330,38495,Major Performance regression of df.groupby(..).indices,bordingj,closed,2020-12-15T08:57:00Z,2021-01-03T18:44:14Z,"I'm experiencing major performance regressions with pandas=1.1.5 versus 1.1.3

Version 1.1.3:
```
Python 3.7.9 | packaged by conda-forge | (default, Dec  9 2020, 20:36:16) [MSC v.1916 64 bit (AMD64)]
Type 'copyright', 'credits' or 'license' for more information
IPython 7.19.0 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.19.0
Python 3.7.9 | packaged by conda-forge | (default, Dec  9 2020, 20:36:16) [MSC v.1916 64 bit (AMD64)] on win32
In[2]: import time
 ... : import numpy as np
 ... : import pandas as pd
 ... : pd.__version__
Out[2]: '1.1.3'
In[3]: numel = 10000000
 ... : df = pd.DataFrame(dict(a=np.random.rand(numel), b=np.random.randint(0,4000, numel)))
 ... : start = time.time()
 ... : groupby_indices = df.groupby('b').indices
 ... : time.time() - start
Out[3]: 0.46085023880004883
```

Version 1.1.5:
```
Python 3.7.9 | packaged by conda-forge | (default, Dec  9 2020, 20:36:16) [MSC v.1916 64 bit (AMD64)]
Type 'copyright', 'credits' or 'license' for more information
IPython 7.19.0 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.19.0
Python 3.7.9 | packaged by conda-forge | (default, Dec  9 2020, 20:36:16) [MSC v.1916 64 bit (AMD64)] on win32
In[2]: import time
 ... : import numpy as np
 ... : import pandas as pd
 ... : pd.__version__
Out[2]: '1.1.5'
In[3]: numel = 10000000
 ... : df = pd.DataFrame(dict(a=np.random.rand(numel), b=np.random.randint(0,4000, numel)))
 ... : start = time.time()
 ... : groupby_indices = df.groupby('b').indices
 ... : time.time() - start
Out[3]: 57.36550998687744
```"
777374153,38892,ASV: Add asv for groupby.indices,phofl,closed,2021-01-01T23:56:57Z,2021-01-03T18:44:59Z,"- [x] closes #38495


Asv for the indices performance regression
"
763122879,38420,BUG: read_csv not recognizing numbers appropriately when decimal is set,phofl,closed,2020-12-12T00:54:19Z,2021-01-03T18:47:35Z,"- [x] closes #31920
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The old regex was shorter but pretty buggy.

Let's assume ``decimal="",""`` and ``thousands="".""``

For example something like ``1a.2,3`` was interpreted as numeric by the regex and converted to ``1a2.3``, because the ``.`` was not escaped in the regex.
Also something like ``1,2,3`` was interpreted as numeric by the regex and converted to ``1.2.3``. 

This one is not quite finished. We have to define a few thousand separator related things:

How strict would we want to be? Should ``1.2,3`` be interpreted as numeric and be converted to ``12.3`` or is only something like ``1.234,5`` relevant as thousands separator? C Engine validation is not strict
Additionally should ``,2`` be the number ``0.2``? -> Currently it is, because the C engine behaves the same"
775696920,38771,DOC: create shared includes for comparison docs,afeld,closed,2020-12-29T04:53:05Z,2021-01-03T19:39:08Z,"This will help ensure consistency between the examples.

Similar to https://github.com/pandas-dev/pandas/pull/38735, but focusing on shared content in a different folder. Particularly interested to see if the build fails in the same way.

- [ ] ~~closes #xxxx~~
- [ ] tests added / passed
- [ ] ~~passes `black pandas`~~
- [ ] ~~passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`~~
- [ ] ~~whatsnew entry~~
"
776677841,38837,"DOC: create shared includes for comparison docs, take II",afeld,closed,2020-12-30T23:35:20Z,2021-01-03T19:39:32Z,"This is a duplicate of https://github.com/pandas-dev/pandas/pull/38771, but instead of defining `:tips:` in a `:suppress:` block, it attempts to tell flake8-rst to ignore those warnings. In other words, only one of these pull requests should be merged; the other can be closed.

- [ ] ~~closes #xxxx~~
- [ ] tests added / passed
- [ ] ~~passes `black pandas`~~
- [ ] ~~passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`~~
- [ ] ~~whatsnew entry~~
"
777479224,38901,TST: strictly xfail,rhshadrach,closed,2021-01-02T15:40:36Z,2021-01-03T20:35:30Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Continuation of #38881. This handles cases where replacing pytest.xfail is clear, opened #38902 for the handful of remaining cases.

In the case where `pytest.xfail` is used for a test which has been yet to be implemented, I've added ~~`assert False` to maintain the current behavior of counting it as an xfail. Maybe there is a better alternative here.~~ `raise NotImplementedError`."
715347900,36909,BUG: Left join is broken with a MultiIndex with only one level,theemathas,closed,2020-10-06T05:20:33Z,2021-01-03T22:22:53Z,"- [X] I have checked that this issue has not already been reported.


- [X] I have confirmed this bug exists on the latest version of pandas.
- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

foo = pd.DataFrame(data={'c': 3}, index=pd.MultiIndex.from_tuples([(1, 2)], names=('a', 'b')))
bar = pd.DataFrame(data={'d': 4}, index=pd.MultiIndex.from_tuples([(2,)], names=('b',)))
joined = foo.join(bar, how='left')
print(joined)
```

#### Expected Output

```
     c d
b a       
2 1  3 4
```

(Throwing an exception on the line where `bar` is created is also possibly acceptable, although confusing.)

#### Produced Output

```
     c   d
b a       
2 1  3 NaN
```

#### Problem description

When joining with a data frame with a MultiIndex with a single level in it, left joins seem to not work properly. This behavior is confusing (why would a MultiIndex with a single index not work?), and is inconsistent with similar code which produces expected output.

Possibly related issues: #34292, #29252, #34357

#### Examples of similar code with expected behavior

Example 1:

<details>

```python
import pandas as pd

foo = pd.DataFrame(data={'c': 3}, index=pd.MultiIndex.from_tuples([(1, 2)], names=('a', 'b')))
bar = pd.DataFrame(data={'d': 4}, index=pd.Index([2], name='b'))
joined = foo.join(bar, how='left')
print(joined)
```
produces
```
     c  d
a b      
1 2  3  4
```

</details>

Example 2:

<details>

```python
import pandas as pd

foo = pd.DataFrame(data={'c': 3}, index=pd.MultiIndex.from_tuples([(1, 2, 99)], names=('a', 'b', 'x')))
bar = pd.DataFrame(data={'d': 4}, index=pd.MultiIndex.from_tuples([(2, 99)], names=('b', 'x')))
joined = foo.join(bar, how='left')
print(joined)
```
produces
```
        c  d
b x  a      
2 99 1  3  4
```

</details>

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : English_United States.1252

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 50.3.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
```

</details>
"
723854390,37208,BUG: Join did not work correctly when one MultiIndex had only one level,phofl,closed,2020-10-17T21:51:17Z,2021-01-03T22:26:06Z,"- [x] closes #36909
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

``droplevel`` for ``MultiIndex`` returns an ``Index`` when the resulting ``MultiIndex`` had only one level. But if the input had only one level and no level should be dropped, the return was a ``MultiIndex``. This did not seem consistent, so I changed it, that in this case an Index would be returned too. If this is not the desired behavior, we could fix the ``join`` problems after calling this functions.

If this is not desired, we should add a note to the docstring, that the return for ``droplevel`` is a MultiIndex, if no level is dropped.
"
642058136,34872,API: Honor copy for dict-input in DataFrame,TomAugspurger,closed,2020-06-19T15:39:16Z,2021-01-03T22:36:54Z,"Closes https://github.com/pandas-dev/pandas/issues/32960.

~Still need a whatsnew. The tl/dr is~

1. We can't honor no-copy for a dict with multiple values of the same dtype: `DataFrame({""A"": np.array([1, 2]), ""B"": np.array([1, 2])})` as long as we have consolidation.
~2. Currently, this is an API breaking change for something like `pd.DataFrame({""A"": np.array([1, 2])})`. To resolve that, I think we'll need something like a default of `copy=None`, and then require `copy=False` to enable no-copy for dict inputs (but keep no copy by default for 2d ndarrays and data frames).~

Unsure how we're going to reconcile that with the behavior of EAs.

EDIT: I've changed the default `copy` to `None`, which means

1. no copy for ndarray / dataframe input (`copy=True` to copy)
2. copy for dict input (`copy=False` to not copy)."
607142128,33814,BUG: Merge between partial index and index fails if result is empty,Rufflewind,closed,2020-04-26T23:32:41Z,2021-01-03T23:21:42Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas
pandas.merge(
    pandas.DataFrame({'a': [1], 'i': [2]}).set_index(['a', 'i']),
    pandas.DataFrame({'i': [1]}).set_index(['i']),
    left_on=['i'],
    right_index=True,
)
```

#### Problem description

This merge fails with the following error:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python3.8/site-packages/pandas/core/reshape/merge.py"", line 88, in merge
    return op.get_result()
  File ""/usr/lib/python3.8/site-packages/pandas/core/reshape/merge.py"", line 668, in get_result
    self._maybe_add_join_keys(result, left_indexer, right_indexer)
  File ""/usr/lib/python3.8/site-packages/pandas/core/reshape/merge.py"", line 824, in _maybe_add_join_keys
    key_col.name = name
AttributeError: 'numpy.ndarray' object has no attribute 'name'
```

Thing is, it works fine if I change `i` in the left DataFrame from `[2]` to `[1]`. It only fails if there's no overlap between the join keys.

This behavior is problematic because the failure depends on the outcome of the merge, so it's difficult to avoid.

#### Expected Output

I expect this to not fail and just return an empty result.

#### Output of ``pd.show_versions()``

<details>

```
>>> pandas.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.6.2-arch1-2
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_FYL.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```

</details>
"
625934316,34414,BUG: merge between partial index and index fails when result is empty,phofl,closed,2020-05-27T18:35:11Z,2021-01-03T23:23:12Z,"- [x] closes #33814
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The Issue was caused when setting the name of an numpy array."
684760278,35873,BUG: PythonParser does not use decimal separator when usecols and parse_date are specified,cmulders,closed,2020-08-24T15:25:06Z,2021-01-03T23:26:00Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example
Repl: https://repl.it/repls/TiredLightcyanNetworking#main.py

```python
import pandas as pd
import io

data = io.StringIO('""dump"",""-9,1"",20101010')

read_options = {
  'names': ['col', 'col1', 'col2'],
  'usecols': ['col1', 'col2'],
  'parse_dates': [""col2""],
  'decimal': "","",
}

data.seek(0)
c_engine = pd.read_csv(
  data,
  engine=""c"",
  **read_options,
)

print(c_engine['col1'].dtype) # float64

data.seek(0)
python_engine = pd.read_csv(
  data,
  engine=""python"",
  **read_options,
)

print(python_engine['col1'].dtype) # object instead of float64

pd.testing.assert_frame_equal(c_engine, python_engine)
# Raises
# Attribute ""dtype"" are different
# [left]:  float64
# [right]: object
```

#### Problem description

The PythonParser produces an invalid result when using a combination of options. It does not convert the comma correctly when the options 'usecols' and 'decimal' are used. The C engine does produce a correct result.

It seems that this check is wrong, the columns are shifted:
https://github.com/pandas-dev/pandas/blob/v1.1.1/pandas/io/parsers.py#L3089

The columns specified in that list are adjusted for the 'usecols' option (so shifted by 1 in the example), while the read line still has _all_ columns.

#### Expected Output

Expect that decimals are correctly converted for all columns. However, the python engine does not when there are columns specified for date parsing.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-1019-gcp
Version          : #19-Ubuntu SMP Tue Jun 23 15:46:40 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.17
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
758020058,38334,"BUG: read_csv not converting to float for python engine with decimal sep, usecols and parse_dates",phofl,closed,2020-12-06T20:52:47Z,2021-01-03T23:28:15Z,"- [x] closes #35873
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

self.columns is alread resticted to usecols, while data is not restricted yet, so we have use the initial column indices"
777481922,38903,ENH: Use Kahan summation to calculate groupby.sum(),phofl,closed,2021-01-02T15:58:45Z,2021-01-03T23:29:09Z,"- [x] closes #38778 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This simplifies the op example.

I think 
```
series = Series([1e16, 99, -5e15, -5e15])
series.sum()
```
is dispatched to numpy? The result is wrong too. ``100.0``

Mean doesn't work either. I will look through the functions to determine which need Kahan summation too and open an issue to track these."
777365250,38888,BUG: import_optional_dependency doesn't check version for submodules,arw2019,closed,2021-01-01T22:38:39Z,2021-01-04T00:14:16Z,"xref https://github.com/pandas-dev/pandas/pull/38370#discussion_r550723991

We'd like `import_optional_dependency` to correctly handle submodule imports AND to be able to import a specified minimum version of the dependency (for when a particular feature requires higher than global minimum version).

cc @lithomas1

#### Expected Output
```
In [14]: import pandas as pd
    ...: from pandas.compat._optional import import_optional_dependency
    ...: import_optional_dependency(""pyarrow.csv"", min_version=""3.0"")
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-14-7cfc85634756> in <module>
      1 import pandas as pd
      2 from pandas.compat._optional import import_optional_dependency
----> 3 import_optional_dependency(""pyarrow.csv"", min_version=""3.0"")

~/repos/pandas/pandas/compat/_optional.py in import_optional_dependency(name, extra, raise_on_missing, on_version, min_version)

ImportError: Pandas requires version '3.0' or newer of 'pyarrow.csv' (version '2.0.0' currently installed).
```

```
In [16]: import_optional_dependency(""pyarrow.csv"", min_version=""3.0"", on_version=""warn"")
/Users/andrewwieteska/repos/pandas/pandas/compat/_optional.py:137: UserWarning: Pandas requires version '3.0' or newer of 'pyarrow.csv' (version '2.0.0' currently installed).
```"
571034786,32259,Bug: groupby with sort=False creates buggy MultiIndex,xin-jin,closed,2020-02-26T02:58:14Z,2021-01-04T00:15:57Z,"```python
d = pd.to_datetime(['2020-11-02', '2019-01-02', '2020-01-02', '2020-02-04', '2020-11-03', '2019-11-03', '2019-11-13', '2019-11-13'])
a = np.arange(len(d))
b = np.random.rand(len(d))
df = pd.DataFrame({'d': d, 'a': a, 'b': b})
t = df.groupby(['d', 'a'], sort=False).mean()
```

The index of `t` is certainly not sorted, but `t.index.is_lexsorted()` returns True.


Another more subtle example is 
```python
d = [3,4,10,0,1,2,5,3]
a = np.arange(len(d))
b = np.random.rand(len(d))
df = pd.DataFrame({'d': d, 'a': a, 'b': b})
t = df.groupby(['d', 'a'], sort=False).mean()
```

This time the lexsort flag is correct. However, calling sortlevel will not sort the new MultiIndex correctly, that is, `t.index.sortlevel(['d', 'a'])[0]` returns

```
MultiIndex([( 3, 0),
            ( 3, 7),
            ( 4, 1),
            (10, 2),
            ( 0, 3),
            ( 1, 4),
            ( 2, 5),
            ( 5, 6)],
           names=['d', 'a'])
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-72-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : 0.29.15
pytest           : 5.3.5
hypothesis       : 5.5.4
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: 0.8.1
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.45.1

</details>
"
777426807,38897,TST/REF: split io/parsers/test_common into multiple files,arw2019,closed,2021-01-02T08:35:39Z,2021-01-04T00:50:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Another precursor to #38370"
765093653,38442,BENCH/REF: parametrize CSV benchmarks on engine,arw2019,closed,2020-12-13T06:57:01Z,2020-12-17T01:43:28Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The diff in basic PR implementing the `pyarrow`-based CSV engine (#38370) is quite big. A part of that PR is a small refactor of the CSV I/O benchmarks such that they take `engine` as a parameter. Most of that refactor is not dependent on having the pyarrow engine so I'm spinning it off here to de-bloat #38370."
766970513,38475,DOC: Add doc-string examples for pd.read_sql using custom parse_dates arg values,avinashpancham,closed,2020-12-14T22:08:33Z,2020-12-17T02:18:57Z,"follow on PR  for  #37823

- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
764547103,38434,REGR: DataFrame.shift(axis=1) raises TypeError when columns is CategoricalIndex,topper-123,closed,2020-12-12T23:08:16Z,2020-12-17T13:46:45Z,"A regression since 1.1:

```python
>>> ci = pd.CategoricalIndex([""a"", ""b""])
>>> df = pd.DataFrame([[1, 2], [3, 4]], index=ci, columns=ci)
>>> df.shift(axis=0)  # ok
     a    b
a  NaN  NaN
b  1.0  2.0
>>> df.shift(axis=1)  # not ok, worked in v1.1
TypeError: 'fill_value=0' is not present in this Categorical's categories
```
"
743291862,37863,API/BUG: DatetimeIndex.argsort does not match DatetimeArray.argsort,jbrockmendel,closed,2020-11-15T16:27:42Z,2020-12-17T13:50:58Z,"The DatetimeIndex returns `self.asi8.argsort(*args, **kwargs)`, while the DatetimeArray uses the M8[ns] values.  This puts NaTs at the front for DTI and the end for DTA.

Changing the DTI method (actually the DTI/TDI/PI methods) to match their array counterparts breaks 12 tests, 11 of them resample, 1 test_grouping."
324022259,21103,"""too many SQL variables"" Error with pandas 0.23 - enable multivalues insert #19664 issue",tripkane,closed,2018-05-17T13:26:21Z,2020-12-17T14:20:22Z,"
```python

#let's import packages to use
import numpy as np
import pandas as pd
from sqlalchemy import create_engine


# #use pandas to import data
df = pd.DataFrame(np.arange(0,20000,1))
#create the engine to connect pandas with sqlite3
engine = create_engine('sqlite://')
#create connection
conn = engine.connect()
#convert df to sql table
df.to_sql('test',engine, if_exists='replace',chunksize=1000)
#print results
result = conn.execute(""select * from test"")
for row in result:
    print(row['index'])
conn.close()


```
#### Problem description

In pandas 0.22 I could write a dataframe to sql of reasonable size without error. Now I receive this error ""OperationalError: (sqlite3.OperationalError) too many SQL variables"". I am converting a dataframe with ~20k+ rows to sql. After looking around I suspect the problem lies in the limit set by sqlite3: SQLITE_MAX_VARIABLE_NUMBER which is set to 999 by default (based on their docs). This can apparently be changed by recompiling sqlite and adjusting this variable accordingly. I can confirm that for a df of length (rows) 499 this works. I can also confirm that this test version works with a row length of 20k and a chunksize of 499 inputted with df_to_sql works. In my real case the limit is 76. These numbers are clearly dependent on data size of each row so a method is required to estimate this based on data type and number of columns. #19664 

#### Expected  #Output
 
runfile('H:/Tests/Pandas_0.23_test.py', wdir='H:/Tests')
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

####Trace###########

runfile('H:/Tests/Pandas_0.23_test.py', wdir='H:/Tests')
Traceback (most recent call last):

  File ""<ipython-input-2-7d10a48edaae>"", line 1, in <module>
    runfile('H:/Tests/Pandas_0.23_test.py', wdir='H:/Tests')

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""H:/Tests/Pandas_0.23_test.py"", line 19, in <module>
    df.to_sql('test',engine, if_exists='fail',chunksize=500)

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\generic.py"", line 2127, in to_sql
    dtype=dtype)

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\io\sql.py"", line 450, in to_sql
    chunksize=chunksize, dtype=dtype)

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\io\sql.py"", line 1149, in to_sql
    table.insert(chunksize)

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\io\sql.py"", line 663, in insert
    self._execute_insert(conn, keys, chunk_iter)

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\io\sql.py"", line 638, in _execute_insert
    conn.execute(*self.insert_statement(data, conn))

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\sqlalchemy\engine\base.py"", line 948, in execute
    return meth(self, multiparams, params)

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\sqlalchemy\sql\elements.py"", line 269, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\sqlalchemy\engine\base.py"", line 1060, in _execute_clauseelement
    compiled_sql, distilled_params

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\sqlalchemy\engine\base.py"", line 1200, in _execute_context
    context)

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\sqlalchemy\engine\base.py"", line 1413, in _handle_dbapi_exception
    exc_info

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\sqlalchemy\util\compat.py"", line 203, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\sqlalchemy\util\compat.py"", line 186, in reraise
    raise value.with_traceback(tb)

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\sqlalchemy\engine\base.py"", line 1193, in _execute_context
    context)

  File ""C:\Users\kane.hill\AppData\Local\Continuum\anaconda3\lib\site-packages\sqlalchemy\engine\default.py"", line 507, in do_execute
    cursor.execute(statement, parameters)

OperationalError: (sqlite3.OperationalError) too many SQL variables [SQL: 'INSERT INTO test (""index"", ""0"") VALUES (?, ?), (?, ?), (?, ?), (?, ?), (?, ?), (?, ?), (?, ?), (?, ?), (?, ?), (?, ?), (?, ?), (?, ?), (?,


#### Output of ``pd.show_versions()``

<details>

pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.5.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en
LOCALE: None.None

pandas: 0.23.0
pytest: 3.5.1
pip: 10.0.1
setuptools: 39.1.0
Cython: 0.28.2
numpy: 1.14.3
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.4.0
sphinx: 1.7.4
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: 1.2.1
tables: 3.4.3
numexpr: 2.6.5
feather: None
matplotlib: 2.2.2
openpyxl: 2.5.3
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.4
lxml: 4.2.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.7
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
769318841,38531,CLN: share .values,jbrockmendel,closed,2020-12-16T22:32:47Z,2020-12-17T15:42:21Z,
756793083,38285,REF: avoid catching all exceptions in libreduction,jbrockmendel,closed,2020-12-04T04:33:41Z,2020-12-17T15:43:21Z,Most of the libreduction edits are from #35417
747012130,37965,BUG: Make DTI/TDI/PI argsort match their underlying arrays,jbrockmendel,closed,2020-11-19T23:43:03Z,2020-12-17T15:43:39Z,"- [x] closes #37863
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The fix to keep resample working is a kludge I'd like to make unnecessary before merging; cc @mroeschke"
749013241,38021,ENH: support 2D in DatetimeArray._from_sequence,jbrockmendel,closed,2020-11-23T18:01:58Z,2020-12-17T15:44:46Z,"Broken off from a branch that fixes #37682, the constraint on which is that _validate_setitem_value needs to handle 2D."
768260354,38511,BUG: MultiIndex.equals returning incorrectly True when Indexes contains NaN,phofl,closed,2020-12-15T22:29:30Z,2020-12-17T16:11:00Z,"- [x] xref #38439
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Depending on the expected behavior of align, this may have hidden an additional bug in align in the referenced issue."
748217374,38000,DOC: DataFrame.last() needs the index to be sorted to deliver the expected results,alejsm,closed,2020-11-22T10:56:39Z,2020-12-17T16:54:23Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.last.html

#### Documentation problem

For the method to work it is needed that the **_DatetimeIndex_ of the dataframe is sorted**, otherwise it will notwork.
Example:
```
import pandas as pd

df = pd.DataFrame({
    'Date': ['2020-01-03', '2020-01-02', '2020-01-05', '2020-01-01', '2020-01-04'],
    'Value': [1, 2, 3, 4, 5]
    })

df['Date'] = pd.to_datetime(df.Date)
df.set_index('Date', inplace=True)
#df.sort_index(inplace=True) This line is needed to make it work
df.last('3D')
```
returns all the rows in the DataFrame
```
                   Value
Date	
2020-01-03	1
2020-01-02	2
2020-01-05	3
2020-01-01	4
2020-01-04	5
```


#### Suggested fix for documentation

It should be mentioned that for the _last()_ method to work, the **_DatetimeIndex_ must be sorted**, or the user may think that there is a bug in the function.
"
746175164,37950,"ENH: 2D compat for DTA tz_localize, to_period",jbrockmendel,closed,2020-11-19T01:43:19Z,2020-12-17T17:47:48Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
635866975,34683,CLN: dont consolidate in reshape.concat,jbrockmendel,closed,2020-06-10T01:52:28Z,2020-12-17T21:48:07Z,"Looks like this consolidation was added here https://github.com/pandas-dev/pandas/commit/3b1c5b74e0ca5a782d4c070aac002b3e6d7e5290 in 2012, no clear reason why it is needed.  About to start an asv run."
770235422,38545,BENCH: Increase sample of CategoricalIndexIndexing.time_get_indexer_list benchmark,mroeschke,closed,2020-12-17T17:30:47Z,2020-12-17T23:17:29Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref https://github.com/pandas-dev/pandas/pull/38476#discussion_r543329205"
500381318,28690,`reindex` of empty CategoricalIndex sometimes fails if target is not a `CategoricalIndex`,batterseapower,closed,2019-09-30T16:19:30Z,2020-12-17T23:40:05Z,"#### Code Sample, a copy-pastable example if possible

This fails with `AttributeError: 'Index' object has no attribute 'codes'`:

```python
pd.DataFrame(columns=pd.CategoricalIndex([], categories=['A'])).reindex(columns=pd.Index(['A']))
```

The backtrace is:

```
c:\python36\lib\site-packages\pandas\util\_decorators.py in wrapper(*args, **kwargs)
    195         @wraps(func)
    196         def wrapper(*args, **kwargs):
--> 197             return func(*args, **kwargs)
    198 
    199         if not PY2:

c:\python36\lib\site-packages\pandas\core\frame.py in reindex(self, *args, **kwargs)
   3807         kwargs.pop('axis', None)
   3808         kwargs.pop('labels', None)
-> 3809         return super(DataFrame, self).reindex(**kwargs)
   3810 
   3811     @Appender(_shared_docs['reindex_axis'] % _shared_doc_kwargs)

c:\python36\lib\site-packages\pandas\core\generic.py in reindex(self, *args, **kwargs)
   4354         # perform the reindex on the axes
   4355         return self._reindex_axes(axes, level, limit, tolerance, method,
-> 4356                                   fill_value, copy).__finalize__(self)
   4357 
   4358     def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,

c:\python36\lib\site-packages\pandas\core\frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy)
   3734         if columns is not None:
   3735             frame = frame._reindex_columns(columns, method, copy, level,
-> 3736                                            fill_value, limit, tolerance)
   3737 
   3738         index = axes['index']

c:\python36\lib\site-packages\pandas\core\frame.py in _reindex_columns(self, new_columns, method, copy, level, fill_value, limit, tolerance)
   3756         new_columns, indexer = self.columns.reindex(new_columns, method=method,
   3757                                                     level=level, limit=limit,
-> 3758                                                     tolerance=tolerance)
   3759         return self._reindex_with_indexers({1: [new_columns, indexer]},
   3760                                            copy=copy, fill_value=fill_value,

c:\python36\lib\site-packages\pandas\core\indexes\category.py in reindex(self, target, method, level, limit, tolerance)
    521             else:
    522 
--> 523                 codes = new_target.codes.copy()
    524                 codes[indexer == -1] = cats[missing]
    525                 new_target = self._create_from_codes(codes)
```

#### Problem description

This should not fail, but rather return an `0x1` DataFrame with the new columns. Other, closely-related, expressions **do** work correctly in this scenario e.g.:

```
pd.DataFrame(columns=pd.CategoricalIndex([], categories=['A'])).reindex(columns=pd.Index(['B']))
pd.DataFrame(columns=pd.CategoricalIndex([], categories=['A'])).reindex(columns=pd.CategoricalIndex(['A']))
pd.DataFrame(columns=pd.CategoricalIndex([], categories=['A'])).reindex(columns=pd.CategoricalIndex(['B']))
```

#### Expected Output

```
pd.DataFrame(columns=pd.Index(['A']))
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.1.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 63 Stepping 2, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.24.1
pytest: 3.1.2
pip: 19.2.1
setuptools: 39.0.1
Cython: 0.29.6
numpy: 1.17.0
scipy: 1.2.1
pyarrow: 0.13.0
xarray: 0.12.1
IPython: 6.1.0
sphinx: 2.1.1
patsy: 0.4.1
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: 1.3.0.dev0
tables: None
numexpr: 2.6.9
feather: None
matplotlib: 2.2.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: 4.3.4
bs4: 4.7.1
html5lib: 0.9999999
sqlalchemy: 1.1.11
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
fastparquet: 0.3.1
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
735415500,37603,TST: Remove stacklevel checks for deprecated resample kwargs,plammens,closed,2020-11-03T15:27:15Z,2020-12-18T09:09:39Z,"- [N/A] closes #xxxx (see https://github.com/pandas-dev/pandas/pull/36629#issuecomment-720757082)
- [x] tests added / passed (tests are failing locally but on unrelated things, think it's a problem somewhere in master)
- [x] passes `black pandas` 
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [N/A] whatsnew entry

This removes `stacklevel` requirements from the deprecation warnings for `resample` and `Grouper` kwargs in an attempt to simplify the warning-handling code (see #36629 for ""why""). This only changes `tests.resample.test_deprecated`.

The second commit is optional (it just extracts the deprecated argument checking code out from `Grouper.__new__` to declutter the latter); I can revert it if needed."
767781785,38504,REG: DataFrame.shift with axis=1 and CategoricalIndex columns,jbrockmendel,closed,2020-12-15T16:33:25Z,2020-12-18T09:35:17Z,"- [x] closes #38434
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
708756689,36629,CLN: Improve setting of resample deprecation warnings' stacklevel,plammens,closed,2020-09-25T08:37:02Z,2020-12-18T11:11:51Z,"- [N/A] closes #xxxx (see https://github.com/pandas-dev/pandas/pull/36369#issuecomment-698763910)
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [N/A] whatsnew entry

The essence of the problem is that the tests for deprecated resample kwargs require that the warning is raised exactly where the user called the resample functionality—i.e. with the appropriate `stacklevel`— (I guess so that the warning message is more user-friendly?), but there are multiple entry points that use these deprecated parameters, so this can't be easily solved by just adjusting the stacklevel by a constant amount.

The old solution was to centralize the management of these warnings in the greatest-common-denominator call site (which was `Grouper.__new__`) and ""guess"" the correct value of `stacklevel` by relying on the current call graph (and hoping it never changes). This of course is poorly maintainable. (For a more detailed explanation of what is wrong with this, see https://github.com/pandas-dev/pandas/pull/36369#issuecomment-694808777.)

The essence of my proposed solution (this PR) is to add a call to the deprecated-parameter-checking code to each of these entry points—this allows instead to use a constant `stacklevel` value (2) in each of these. Now since some of these public call sites appear in the call graph of others, we have to ensure this code is only called once within any call path starting at any of these entry points. This is what the `call_once` context manager takes care of."
767210333,38492,BUG: CategoricalIndex.reindex fails when Index passed with labels all in category,GYHHAHA,closed,2020-12-15T05:59:55Z,2020-12-18T11:23:46Z,"- [x] closes #28690
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
770763260,38557,Backport PR #38526 on branch 1.2.x (CI: pin xlrd<2.0),meeseeksmachine,closed,2020-12-18T10:17:00Z,2020-12-18T12:49:05Z,Backport PR #38526: CI: pin xlrd<2.0
770835946,38559,Backport PR #38514 on branch 1.2.x (CI: un-xfail),meeseeksmachine,closed,2020-12-18T12:07:24Z,2020-12-18T14:14:33Z,Backport PR #38514: CI: un-xfail
770734577,38555,Backport PR #38504: REG: DataFrame.shift with axis=1 and CategoricalIndex columns,simonjayhawkins,closed,2020-12-18T09:33:46Z,2020-12-18T15:05:08Z,Backport PR #38504
681827358,35804,Performance regression in reindex.LevelAlign.time_reindex_level,TomAugspurger,closed,2020-08-19T13:09:49Z,2020-12-18T16:44:50Z,"https://pandas.pydata.org/speed/pandas/index.html#reindex.LevelAlign.time_reindex_level?commits=a0c8425a5f2b74e8a716defd799c4a3716f66eff-2a319238e47d3d7362188f7da4eb911199c423b8

Somewhere in https://github.com/pandas-dev/pandas/compare/a0c8425a5f2b74e8a716defd799c4a3716f66eff...2a319238e47d3d7362188f7da4eb911199c423b8"
681827843,35805,Performance regression in index_object.IndexEquals.time_non_object_equals_multiindex,TomAugspurger,closed,2020-08-19T13:10:33Z,2020-12-18T16:44:51Z,"https://pandas.pydata.org/speed/pandas/index.html#index_object.IndexEquals.time_non_object_equals_multiindex?commits=a0c8425a5f2b74e8a716defd799c4a3716f66eff-2a319238e47d3d7362188f7da4eb911199c423b8

Somewhere in https://github.com/pandas-dev/pandas/compare/a0c8425a5f2b74e8a716defd799c4a3716f66eff...2a319238e47d3d7362188f7da4eb911199c423b8"
770543700,38553,REF: helpers for sanitize_array,jbrockmendel,closed,2020-12-18T03:47:02Z,2020-12-18T18:32:15Z,
770874162,38560,"Revert ""REF: remove special casing from Index.equals (always dispatchto subclass) (#35330)""",simonjayhawkins,closed,2020-12-18T13:08:58Z,2020-12-18T19:05:32Z,"This reverts commit 0b90685f4df2024754748b992d5eaaa352e7caa5.

- [ ] closes #35804
- [ ] closes #35805
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

```
       before           after         ratio
     [54682234]       [05c97adb]
     <master>         <revert-35330>
-         293±5ms       3.75±0.2μs     0.00  index_object.IndexEquals.time_non_object_equals_multiindex

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE INCREASED.
```

```
       before           after         ratio
     [54682234]       [05c97adb]
     <master>         <revert-35330>
-      1.44±0.1ms         800±20μs     0.56  reindex.LevelAlign.time_reindex_level

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE INCREASED.
```"
771107022,38565,"Backport PR #38560 on branch 1.2.x (Revert ""REF: remove special casing from Index.equals (always dispatchto subclass) (#35330)"")",meeseeksmachine,closed,2020-12-18T19:05:46Z,2020-12-18T20:54:56Z,"Backport PR #38560: Revert ""REF: remove special casing from Index.equals (always dispatchto subclass) (#35330)"""
749971877,38046,BUG: BDay() offsetting not behaving as expected,mhorsley30896,closed,2020-11-24T19:18:09Z,2020-12-18T23:05:35Z,"```python
import pandas as pd
from pandas.tseries.offsets import CDay, BDay

idx = pd.bdate_range(""2010/02/01"",""2010/02/15"", freq='6H')

t1 = idx + BDay(offset=pd.Timedelta(3, unit='H'))

t2 = idx + CDay(offset=pd.Timedelta(3, unit='H'))

t1.equals(t2)

False
```

#### Problem description

BDay does not seem to be offsetting when a timedelta is supplied.

#### Expected Output

True

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.6.12.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-514.6.1.el7.x86_64
Version          : #1 SMP Wed Jan 18 13:06:36 UTC 2017
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C
LOCALE           : None.None

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.1.post20201107
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: 0.9.0
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.3
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.20
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.51.2

</details>
"
757793659,38324,BUG: BDay() offsetting not behaving as expected,theoniko,closed,2020-12-05T22:16:35Z,2020-12-18T23:05:39Z,"- [x] closes  #38046
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
767244560,38493,"TYP: func argument to DataFrame.apply, DataFrame.applymap, core.apply.frame_apply",arw2019,closed,2020-12-15T06:53:44Z,2020-12-18T23:06:51Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Spinning off from #38416, xref https://github.com/pandas-dev/pandas/pull/38416#discussion_r542911035"
771088877,38563,REF: handle non-list_like cases upfront in sanitize_array,jbrockmendel,closed,2020-12-18T18:34:33Z,2020-12-18T23:31:55Z,so we can rule them out later
771193488,38568,TST/REF: misplaced DataFrame.append test,jbrockmendel,closed,2020-12-18T22:00:13Z,2020-12-19T02:13:08Z,
771204265,38569,TST/REF: indexes/datetimes/methods/,jbrockmendel,closed,2020-12-18T22:24:42Z,2020-12-19T02:13:30Z,
523136235,29623,BUG: incorrect output of first('1M') in case if first index is the last day of the month,vfilimonov,closed,2019-11-14T22:17:40Z,2020-12-19T02:14:08Z,"#### Code Sample

```python
x = pd.Series(1, index=pd.bdate_range('2010-03-31', periods=100))
print(x.first('1M'))  # Returns March and April
print(x.first('2M'))  # Returns March, April and May
# etc...
```
#### Problem description

In case if the first index of the series falls on the last day of the month, `Series.first('1M')` returns two first months (this one day + the next month).

If the first value is not on the last day of the month - result is correct:

```python
x = pd.Series(1, index=pd.bdate_range('2010-03-30', periods=100))
print(x.first('1M'))  # Returns only March
print(x.first('2M'))  # Returns March and April
# etc...
```

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.4.0
Cython           : None
pytest           : 5.2.1
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : None
fastparquet      : 0.1.6
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.0.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.12.1
pytables         : None
s3fs             : 0.3.4
scipy            : 1.3.1
sqlalchemy       : 1.3.8
tables           : 3.4.4
xarray           : None
xlrd             : 1.1.0
xlwt             : None
xlsxwriter       : None


</details>
"
605634527,33748,BUG: to_hdf and HDFStore raise KeyError for DataFrame subclasses,sytham,closed,2020-04-23T15:30:32Z,2020-12-19T02:19:00Z,"- [x ] I have checked that this issue has not already been reported.

- [x ] I have confirmed this bug exists on the latest version of pandas.
---
#### Code Sample, a copy-pastable example

```python
import pandas as pd
class SubDataFrame(pd.DataFrame):
    @property
    def _constructor(self):
        return SubDataFrame

# fails with KeyError
sdf = SubDataFrame({'a':[1,2], 'b':[3,4]})
sdf.to_hdf('test.h5', 'test')

with pd.HDFStore('test.h5') as store:
    store.put('test', sdf)
```

#### Problem description

``to_hdf()`` and ``HDFStore.put()`` fail for DataFrame subclasses.

This happens because in ``pandas/io/pytables.py`` in ``_create_storer`` line 1578 (or thereabouts), the ``_TYPE_MAP`` is accessed by ``type()``, whereas upon ``_create_storer`` entry, the check is done using ``isinstance()``. 

#### Expected Output

The check upon entry of ``_create_storer`` should at least be consistent with the way ``_TYPE_MAP`` is accessed. So if the choice is not to support writing DataFrame subclasses to HDF, instead of a ``KeyError``, a ``TypeError(""value must be None, Series, or DataFrame"")`` should be raised.

But ideally, storing subclasses of DataFrame to HDF should be supported.

#### Output of ``pd.show_versions()``
<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.10.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.111-1.el7.centos.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200325
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.10.2
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.2.1
sqlalchemy       : 1.3.11
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.46.0

</details>
"
771206001,38570,TST/REF: misplaced Series.reindex test,jbrockmendel,closed,2020-12-18T22:29:35Z,2020-12-19T02:19:05Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
756249900,38262,BUG: to_hdf and HDFStore for subclasses,ivanovmg,closed,2020-12-03T14:34:39Z,2020-12-19T02:19:07Z,"- [ ] closes #33748
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Fix the bug that the subclasses of a dataframe and a series cannot be saved ``to_hdf``.
The tests were added.
When testing I found that ``assert_frame_equal`` does not work on subclasses as well - probably need to fix it in a separate PR.
For now in the tests I compare values only."
755359298,38236,TYP: handle mypy ignore in pandas/_testing,ivanovmg,closed,2020-12-02T15:12:33Z,2020-12-19T02:22:44Z,"- [ ] xref #37715
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Handle mypy ignore comments in ``pandas/_testing.py``, function ``write_to_compressed``."
675224104,35612,BUG: DataFrameGroupBy.__getitem__ fails to propagate dropna=True,arw2019,closed,2020-08-07T19:18:05Z,2020-12-19T02:34:44Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

xref #35014 

Creating a separate issue as the `dropna=True` requires a different fix to `dropna=False`  (resolved by #35078)

#### Problem description

The setup is:
``` python
In [1]: import pandas as pd                                                                                                                    
In [2]: df = pd.DataFrame({""A"": [0, 0, 1, None], ""B"": [1, 2, 3, None]})                                                                        
In [3]: gb = df.groupby(""A"", dropna=True)                                                                                                      
```
All three of these commands:
``` python
In [4]: gb['B'].transform(len)                                                                                                                 
In [5]: gb[['B']].transform(len)                                                                                                               
In [6]: gb.transform(len)                                                                                                                      
```
generate a variant of this error
``` python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-4-3bae7d67a46f> in <module>
----> 1 gb['B'].transform(len)

/workspaces/pandas-arw2019/pandas/core/groupby/generic.py in transform(self, func, engine, engine_kwargs, *args, **kwargs)
    487 
    488         if not isinstance(func, str):
--> 489             return self._transform_general(
    490                 func, *args, engine=engine, engine_kwargs=engine_kwargs, **kwargs
    491             )

/workspaces/pandas-arw2019/pandas/core/groupby/generic.py in _transform_general(self, func, engine, engine_kwargs, *args, **kwargs)
    556 
    557         result.name = self._selected_obj.name
--> 558         result.index = self._selected_obj.index
    559         return result
    560 

/workspaces/pandas-arw2019/pandas/core/generic.py in __setattr__(self, name, value)
   5167         try:
   5168             object.__getattribute__(self, name)
-> 5169             return object.__setattr__(self, name, value)
   5170         except AttributeError:
   5171             pass

/workspaces/pandas-arw2019/pandas/_libs/properties.pyx in pandas._libs.properties.AxisProperty.__set__()
     64 
     65     def __set__(self, obj, value):
---> 66         obj._set_axis(self.axis, value)

/workspaces/pandas-arw2019/pandas/core/series.py in _set_axis(self, axis, labels, fastpath)
    422         if not fastpath:
    423             # The ensure_index call above ensures we have an Index object
--> 424             self._mgr.set_axis(axis, labels)
    425 
    426     # ndarray compatibility

/workspaces/pandas-arw2019/pandas/core/internals/managers.py in set_axis(self, axis, new_labels)
    214 
    215         if new_len != old_len:
--> 216             raise ValueError(
    217                 f""Length mismatch: Expected axis has {old_len} elements, new ""
    218                 f""values have {new_len} elements""

ValueError: Length mismatch: Expected axis has 3 elements, new values have 4 elements
```
#### Expected Output

All three should return:
``` python                                                                                                                 
Out[9]: 
   B
0  2
1  2
2  1
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 9843926e39dee88f689afc51cbf0f2fdc232dcd3
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-42-generic
Version          : #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+54.g9843926e3
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.1.0.post20200704
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : 5.19.0
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : 0.4.0
gcsfs            : 0.6.2
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.1
</details>
"
679689966,35751,"BUG: DataFrame.groupby(., dropna=True, axis=0) incorrectly throws ShapeError",arw2019,closed,2020-08-16T05:04:18Z,2020-12-19T02:34:50Z,"- [x] closes #35612
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This PR makes a few more changes to propagate `dropna` correctly for sliced groupby objects. 

It depends on #35444 for the changes to `pandas/core/groupby/generic.py`. I put them in by hand for now so that the tests pass but there should be no diff once #35444 is merged."
769455181,38535,CLN: use .view(i8) instead of .astype(i8) for datetimelike values,jbrockmendel,closed,2020-12-17T03:07:24Z,2020-12-19T02:39:37Z,prelude to deprecating the astype behavior
771341321,38579,DOC:fix of DOCUMENTATION,Navron4500,closed,2020-12-19T09:01:08Z,2020-12-19T11:01:16Z,"- [ ] closes #38311
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ Timedelta.isoformat DOC had duplicates i have deleted one of them. And their was two lines with wrong output that i have also fixed.] whatsnew entry
"
757539992,38311,DOC: Wrong output in the example of Timedelta.isoformat,gurukiran07,closed,2020-12-05T04:46:56Z,2020-12-19T11:06:37Z,"#### Location of the documentation

[`pandas.Timedelta.isoformat`](https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.isoformat.html#pandas.Timedelta.isoformat)

#### Documentation problem

The output on an example in the docs has the wrong output(`Timedelta.isoformat` works fine)

![Screenshot from 2020-12-04 16-40-36](https://user-images.githubusercontent.com/47276342/101234113-ce80fa00-36e2-11eb-8e2c-9623f41a69ae.png)

While it should've been `'P0DT1H0M10S'`
"
769719624,38538,QST: error while using pickle to load the files,sagar-m,closed,2020-12-17T08:56:06Z,2020-12-19T13:48:07Z,"
Hi, I am getting the below error while using pickle to load the files on kaggle. It has worked for everyone, but it is not working for me. The file path is correct. Thank you for your help.

```

---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
<timed exec> in <module>

/opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression)
    167     if not isinstance(fp_or_buf, str) and compression == ""infer"":
    168         compression = None
--> 169     f, fh = get_handle(fp_or_buf, ""rb"", compression=compression, is_text=False)
    170 
    171     # 1) try standard library Pickle

/opt/conda/lib/python3.7/site-packages/pandas/io/common.py in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)
    497         else:
    498             # Binary mode
--> 499             f = open(path_or_buf, mode)
    500         handles.append(f)
    501 

FileNotFoundError: [Errno 2] No such file or directory: '../input/riiid-cross-validation-files/cv2_train.pickle'
```"
765396634,38446,"BUG: first(""2M"") returning incorrect results",phofl,closed,2020-12-13T13:37:15Z,2020-12-19T17:21:21Z,"- [x] closes #29623
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
 
This would fix #29623 for offsets with n > 1
cc @simonjayhawkins "
771451140,38585,CLN: simplify soft_convert_objects,jbrockmendel,closed,2020-12-19T20:43:00Z,2020-12-20T04:45:42Z,
331860696,21456,API/BUG: Raise when int-dtype coercions fail,gfyoung,closed,2018-06-13T06:39:59Z,2020-12-20T17:34:52Z,"Related to the Index and Series constructors.

Closes #15832.

cc @ucals (since this is mostly based off what you did in #15859)"
771431192,38584,REF: simplify sanitize_array/_try_cast,jbrockmendel,closed,2020-12-19T18:33:12Z,2020-12-20T19:19:03Z,
771754555,38598,BUG: Temporarily pin the version of Sphinx to 3.3.1 in requirements.,shawnbrown,closed,2020-12-21T00:31:14Z,2020-12-21T02:59:21Z,"A recent change in Sphinx v3.4.0 is breaking pandas' CI *Build documentation* step.

Until this is resolved, we can temporarily pin Sphinx to the previous version (3.3.1).

_Submitted in response to @jreback in https://github.com/pandas-dev/pandas/issues/38593#issuecomment-748690637_

------

Here's some info that might help for getting this sorted, long term:

Sphinx Changelog: Release 3.4.0 (released Dec 20, 2020)
https://www.sphinx-doc.org/en/master/changes.html#release-3-4-0-released-dec-20-2020

CI build logs from some passing and failing builds:

* A successful build using Sphinx 3.3.1 - https://pastebin.com/raw/jLwVAJsP
* A failing build using Sphinx 3.4.0 - https://pastebin.com/raw/bpHTm2DD
* A here's a diff between these two logs - https://pastebin.com/psJ5hqP8

While passing and failing builds both emit warnings, the following warnings are exclusive to the failing builds under Sphinx 3.4.0:
```
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.BusinessDay.rst:18: WARNING: autosummary: failed to import base
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.BusinessDay.rst:18: WARNING: autosummary: failed to import offset
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.BusinessDay.rst:40: WARNING: autosummary: failed to import __call__
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.BusinessDay.rst:40: WARNING: autosummary: failed to import rollback
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.BusinessDay.rst:40: WARNING: autosummary: failed to import rollforward
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.BusinessMonthBegin.rst:18: WARNING: autosummary: failed to import base
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.BusinessMonthBegin.rst:36: WARNING: autosummary: failed to import __call__
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.BusinessMonthBegin.rst:36: WARNING: autosummary: failed to import rollback
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.BusinessMonthBegin.rst:36: WARNING: autosummary: failed to import rollforward
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.BusinessMonthEnd.rst:18: WARNING: autosummary: failed to import base
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.BusinessMonthEnd.rst:36: WARNING: autosummary: failed to import __call__
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.BusinessMonthEnd.rst:36: WARNING: autosummary: failed to import rollback
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.BusinessMonthEnd.rst:36: WARNING: autosummary: failed to import rollforward
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessMonthBegin.rst:18: WARNING: autosummary: failed to import base
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessMonthBegin.rst:18: WARNING: autosummary: failed to import cbday_roll
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessMonthBegin.rst:18: WARNING: autosummary: failed to import month_roll
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessMonthBegin.rst:18: WARNING: autosummary: failed to import offset
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessMonthBegin.rst:43: WARNING: autosummary: failed to import __call__
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessMonthBegin.rst:43: WARNING: autosummary: failed to import rollback
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessMonthBegin.rst:43: WARNING: autosummary: failed to import rollforward
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessMonthEnd.rst:18: WARNING: autosummary: failed to import base
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessMonthEnd.rst:18: WARNING: autosummary: failed to import cbday_roll
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessMonthEnd.rst:18: WARNING: autosummary: failed to import month_roll
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessMonthEnd.rst:18: WARNING: autosummary: failed to import offset
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessMonthEnd.rst:43: WARNING: autosummary: failed to import __call__
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessMonthEnd.rst:43: WARNING: autosummary: failed to import rollback
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessMonthEnd.rst:43: WARNING: autosummary: failed to import rollforward
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessDay.rst:18: WARNING: autosummary: failed to import base
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessDay.rst:18: WARNING: autosummary: failed to import offset
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessDay.rst:40: WARNING: autosummary: failed to import __call__
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessDay.rst:40: WARNING: autosummary: failed to import rollback
/home/runner/work/pandas/pandas/doc/source/docstring of pandas._libs.tslibs.offsets.CustomBusinessDay.rst:40: WARNING: autosummary: failed to import rollforward
```"
527235093,29791,Have `pd.array` infer new extension types,TomAugspurger,closed,2019-11-22T14:40:07Z,2020-12-21T10:08:06Z,"Currently `pd.array` sometimes requires an explicit `dtype=...` to get one of our extension arrays (we'll infer for Period, Datetime, and Interval).

This proposal is to have it infer the extension type for

* strings -> StringArray
* boolean -> BooleanArray
* integer -> IntegerArray

All of these currently return PandasArray.

Concretely, we'll need to teach `infer_dtype` how not to infer `mixed` for a mix of strings / booleans and NA values, similar to how it handles integer-na

```python
In [27]: lib.infer_dtype([True, None], skipna=False)
Out[27]: 'mixed'

In [28]: lib.infer_dtype(['a', None], skipna=False)
Out[28]: 'mixed'

In [29]: lib.infer_dtype([0, np.nan], skipna=False)
Out[29]: 'integer-na'
```

and then handle those in `array`."
771796472,38603,Backport PR #38598 on branch 1.2.x (BUG: Temporarily pin the version of Sphinx to 3.3.1 in requirements.),meeseeksmachine,closed,2020-12-21T02:59:51Z,2020-12-21T12:17:19Z,Backport PR #38598: BUG: Temporarily pin the version of Sphinx to 3.3.1 in requirements.
771103592,38564,TST: GH30999 Add match=msg to test_nat_comparisons_invalid,moink,closed,2020-12-18T18:59:20Z,2020-12-21T12:37:04Z,"This pull request xref #30999 to remove bare pytest.raises. It doesn't close that issue as I have only addressed one file: pandas/tests/scalar/test_nat.py . In that file there was only one test that had a bare pytest.raises and I added a message to the two instances of pytest.raises in that test. It required modifications to the test parameters in order to populate the message.

I did not add a whatsnew entry since it's only a tiny change to one test. Let me know if I should add one (and I am a bit unclear on how, i.e. what version this would end up in).

This is my first ever pull request to an open source project. I am expecting that I will have to make changes before it's accepted; thanks for your patience.

- [ ] xref #30999
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
769101428,38525,BUG: pyarrow_array_to_numpy_and_mask is not taking offset into account when creating mask,westonpace,closed,2020-12-16T16:59:20Z,2020-12-21T13:54:38Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
from pyarrow import Table

df = pd.DataFrame({'int_na': [0, None, 2, 3, None, 5, 6, None, 8]}, dtype=pd.Int64Dtype())
print(df)
```
```python
    int_na

0 0 
1 <NA>
 2 2 
3 3 
4 <NA>
 5 5 
6 6 
7 <NA> 
8 8
```
```python
Table.from_pandas(df).slice(2, None).to_pandas()
```
```python
  int_na
0 2
1 <NA>
2 1
3 5
4 <NA>
5 1
6 8
```

#### Problem description

The pandas array does not accurately reflect the pyarrow array.

#### Expected Output

```python
Table.from_pandas(df).slice(2, None).to_pandas()
```
```python
  int_na
0 2
1 3
2 <NA>
3 5
4 6
5 <NA>
6 8
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.9.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-58-generic
Version          : #64-Ubuntu SMP Wed Dec 9 08:16:25 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.5
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 49.6.0.post20201009
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 3.0.0.dev391+g26aef88b5.d20201216
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
759574179,38367,BUG: Pandas 1.1.5 location-based indexing error with quantized pivot table,tgaddair,closed,2020-12-08T16:08:38Z,2020-12-21T13:55:18Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import numpy as np
import pandas as pd

input_df = pd.DataFrame(**{
    'index': [0, 1], 
    'columns': ['loss', 'category_64973.fc_size', 'category_64973.num_fc_layers', 'training.learning_rate'], 
    'data': [[1.0549572706222534, 240, 2, 0.0014908184659929895], [1.225046157836914, 160, 2, 0.0013734204727201226]]
})

input_df['training.learning_rate'] = pd.qcut(
    input_df['training.learning_rate'],
    q=10,
    precision=3,
    duplicates='drop',
)

data = input_df.pivot_table(
    index='category_64973.fc_size',
    columns='training.learning_rate',
    values='loss',
    aggfunc='mean'
)

# Seaborn code starts here
mask = np.zeros(data.shape, bool)
mask = pd.DataFrame(mask,
                    index=data.index,
                    columns=data.columns,
                    dtype=bool)

mask | pd.isnull(data)

```

#### Problem description

An error occurs when attempting to plot a quantized pivot table using Seaborn with the latest version of Pandas (`1.1.5`).

The code above is a self-contained example showing what Seaborn is doing when `heatmap()` is called on the input pivot table (`data`).  See this usage in the Ludwig framework: https://github.com/uber/ludwig/blob/master/ludwig/utils/visualization_utils.py#L1392.  Prior to v1.1.5, this code was working fine and used to generate plots in Ludwig.

The stack trace is as follows:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/repos/ludwig/env/lib/python3.7/site-packages/pandas/core/indexing.py in _has_valid_tuple(self, key)
    701             try:
--> 702                 self._validate_key(k, i)
    703             except ValueError as err:

~/repos/ludwig/env/lib/python3.7/site-packages/pandas/core/indexing.py in _validate_key(self, key, axis)
   1368         else:
-> 1369             raise ValueError(f""Can only index by location with a [{self._valid_types}]"")
   1370 

ValueError: Can only index by location with a [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array]

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
<ipython-input-1-e654830c5b85> in <module>
     32                     dtype=bool)
     33 
---> 34 mask | pd.isnull(data)

~/repos/ludwig/env/lib/python3.7/site-packages/pandas/core/ops/__init__.py in f(self, other, axis, level, fill_value)
    638             self, other, op, axis, default_axis, fill_value, level
    639         ):
--> 640             return _frame_arith_method_with_reindex(self, other, op)
    641 
    642         if isinstance(other, ABCSeries) and fill_value is not None:

~/repos/ludwig/env/lib/python3.7/site-packages/pandas/core/ops/__init__.py in _frame_arith_method_with_reindex(left, right, op)
    572     )
    573 
--> 574     new_left = left.iloc[:, lcols]
    575     new_right = right.iloc[:, rcols]
    576     result = op(new_left, new_right)

~/repos/ludwig/env/lib/python3.7/site-packages/pandas/core/indexing.py in __getitem__(self, key)
    871                     # AttributeError for IntervalTree get_value
    872                     pass
--> 873             return self._getitem_tuple(key)
    874         else:
    875             # we by definition only have the 0th axis

~/repos/ludwig/env/lib/python3.7/site-packages/pandas/core/indexing.py in _getitem_tuple(self, tup)
   1441     def _getitem_tuple(self, tup: Tuple):
   1442 
-> 1443         self._has_valid_tuple(tup)
   1444         try:
   1445             return self._getitem_lowerdim(tup)

~/repos/ludwig/env/lib/python3.7/site-packages/pandas/core/indexing.py in _has_valid_tuple(self, key)
    705                     ""Location based indexing can only have ""
    706                     f""[{self._valid_types}] types""
--> 707                 ) from err
    708 
    709     def _is_nested_tuple_indexer(self, tup: Tuple) -> bool:

ValueError: Location based indexing can only have [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array] types
```

Note that this last `mask | pd.isnull(data)` operations succeeds with Pandas 1.1.4 and all other dependencies being left the same.

#### Expected Output

The `mask | pd.isnull(data)` call should succeed.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.7.8.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.1.0
Cython           : 0.29.21
pytest           : 6.1.0
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : 0.10.1
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.2
bottleneck       : None
fsspec           : 0.8.4
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 2.0.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.20
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.52.0

</details>
"
769898804,38539,BUG: fix array conversion from Arrow for slided array,jorisvandenbossche,closed,2020-12-17T11:26:21Z,2020-12-21T13:58:38Z,Closes #38525
771367776,38580,BUG: In Multilevel.dtypes when level name is not specified it gives wrong output,gurukiran07,closed,2020-12-19T11:59:11Z,2020-12-21T14:49:13Z,"- [X] I have checked that this issue has not already been reported.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

There was a recent PR to add `MultiIndex.dtypes` #37073  but this only works when each level is named.
https://github.com/pandas-dev/pandas/blob/03e1c899077d335a057a3f36c09645499638d417/pandas/core/indexes/multi.py#L703-L710

If `level` has no name then, `level.name` would be `None`.

Example where it  fails:

```python3
idx_multitype = pd.MultiIndex.from_product(
     [[1, 2, 3], [""a"", ""b"", ""c""], pd.date_range(""20200101"", periods=2, tz=""UTC"")],
 )
{level.name: level.dtype for level in idx_multitype.levels}
# {None: datetime64[ns, UTC]}
```
When level is unamed i.e `None` may be we can add `level_0`, `level_1` etc. this would be consistent with API too(`reset_index` would add `level_x`). 

@jreback 
@arw2019 "
772189503,38611,Backport PR #38539 on branch 1.2.x (BUG: fix array conversion from Arrow for slided array),meeseeksmachine,closed,2020-12-21T13:57:25Z,2020-12-21T15:28:26Z,Backport PR #38539: BUG: fix array conversion from Arrow for slided array
772189626,38612,Backport PR #38532 on branch 1.2.x (BUG: Regression in logical ops raising ValueError with Categorical columns with unused categories),meeseeksmachine,closed,2020-12-21T13:57:37Z,2020-12-21T15:28:46Z,Backport PR #38532: BUG: Regression in logical ops raising ValueError with Categorical columns with unused categories
771501837,38589,TST/REF: collect tests by method,jbrockmendel,closed,2020-12-20T03:15:04Z,2020-12-21T15:33:29Z,
770121405,38541,"ERROR : ""('HY000', 'The driver did not supply an error!')"" or  ""The incoming request has too many parameters"" when trying to push a df to a MS SQL database using the to_sql() method with argument method='multi'",AntoineWeber,closed,2020-12-17T15:02:46Z,2020-12-21T15:39:50Z,"- I have checked that this issue has not already been reported.
- [Yes, it was already opened [here](https://github.com/pandas-dev/pandas/issues/21103). However the issue was closed even if the problem is still here]

- I have confirmed this bug exists on the latest version of pandas.
- [Yes, currently version 1.1.5]

- (optional) I have confirmed this bug exists on the master branch of pandas.
- [No, did not check this.]

---
#### Problem description

I am currently trying to push a dataframe to a MS SQL database using pandas' `to_sql()`  (using sqlalchemy and pyodbc) method. My dataframe is rather large with a 14k x 72 shape and various types (strings, floats, integers.)

When trying to push the dataframe without any argument set to the `to_sql()` method (except the table name and DB connection), the data will push extremely slow (> 30min for this 5mb df).
To try to fasten this, I came accross the `method=` argument, that I set to `'multi'`. Without specifying a chunksize, I get the error 
```('HY000', 'The driver did not supply an error!')```

So then I tried to set a chunksize, but whenever this latter one is bigger than ~20, I get the error ```The incoming request has too many parameters. The server supports a maximum of 2100 parameters. Reduce the number of parameters and resend the request.```
Setting the chunksize to 10 works, but the upload is ~3min, which is still too slow..

From my understanding, such an upload should be doable normally in a few seconds.

Am I the only one having this issue ?

## Sample code
```
# connection to the db
engine = sqlalchemy.create_engine(""mssql+pyodbc://{},{}/{}?driver=SQL Server?Trusted_Connection=yes""
                                  .format(server_ip, port_number, db_name))
connection = engine.connect()

# read the df
df_raw = pd.read_csv(files_path, index_col=0, low_memory=False)

# push to database
with connection.begin():
    df_raw.to_sql('RawTestData', con=connection, if_exists='replace', index=False, chunksize=chunk_size, method='multi')
```

#### Expected Output

Using the `method = 'multi'` argument should speed the upload of the data. I guess in a few seconds maximum.

#### Output of ``pd.show_versions()``
<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.8.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : French_Switzerland.1252

pandas           : 1.1.5
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.1.post20201107
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.3
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.20
tables           : 3.6.1

</details>
"
769328448,38532,BUG: Regression in logical ops raising ValueError with Categorical columns with unused categories,phofl,closed,2020-12-16T22:51:44Z,2020-12-21T15:50:24Z,"- [x] closes #38367
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

@simonjayhawkins 

This would fix the issue, but is pretty dirty. If we decide to merge this, we should remove this again with #38140 Also test would have to be adjusted (expected remove unused category)"
771285782,38573,TST/REF: collect datetimelike factorize tests,jbrockmendel,closed,2020-12-19T02:42:21Z,2020-12-21T16:12:43Z,
771687453,38593,DOC: Add datatest package to list of third-party extension accessors.,shawnbrown,closed,2020-12-20T20:03:46Z,2020-12-21T16:33:20Z,"This change adds the `datatest` package to the list of third-party extension accessors. Since `datatest` supports more than `pandas`, I wasn't sure if it was appropriate to add it to the **Data cleaning and validation** section, so I only added it to the accessors list for now.

Unfortunately, the ""Classes"" column was not wide enough--so this patch affects all rows in the table. This shouldn't happen again as the column is now wide enough to accommodate all of the classes that support extension accessors."
771313259,38577,"TST/REF: Remove duplicate .plot.hist() tests, consolidate others",mzeitlin11,closed,2020-12-19T05:43:26Z,2020-12-21T17:03:59Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

When looking into `matplotlib` warnings issued from `test_hist_method.py::TestSeriesPlots::test_hist_legacy` found a bunch of duplicate tests for `hist`. Also moved remaining `hist` tests from `test_series.py` to `test_hist_method.py`."
771986993,38607,REGR: astype(str) of object array with byte objects,jorisvandenbossche,closed,2020-12-21T08:55:23Z,2020-12-21T17:27:18Z,"On released version, we see this behaviour:

```
In [1]: idx = pd.Index(['あ', b'a'], dtype='object')

In [2]: idx
Out[2]: Index(['あ', b'a'], dtype='object')

In [4]: idx.astype(str)
Out[4]: Index(['あ', 'a'], dtype='object')
```

So where the `bytes` object `b""a""` gets converted to the string `""a""`.

On master (since a few days), however, we now get:

```
In [7]: idx.astype(str)
Out[7]: Index(['あ', 'b'a''], dtype='object')
```

so where the `bytes` object gets converted to the string `""b'a'""`

Possibly due to https://github.com/pandas-dev/pandas/pull/38518 cc @jbrockmendel 
"
772173224,38610,"Revert ""REF: use astype_nansafe in Index.astype""",simonjayhawkins,closed,2020-12-21T13:33:19Z,2020-12-21T17:34:05Z,Reverts pandas-dev/pandas#38518
771742001,38596,CLN: Remove _get_roll_func,mroeschke,closed,2020-12-20T23:30:06Z,2020-12-21T18:20:56Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
771738591,38595,CLN: Window _constructors definitions,mroeschke,closed,2020-12-20T23:13:19Z,2020-12-21T18:21:03Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Redefined in more appropriate locations. "
701387901,36369,DOC: Enhance asfreq docs,plammens,closed,2020-09-14T20:02:42Z,2020-12-21T20:15:23Z,"- [n/a] closes #xxxxx
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] tests added

Enhanced some docstrings for time-series related methods; mainly `asfreq`. Namely:
 - Converted `NDFrame.asfreq` into a template to use the specific `klass` in `Series.asfreq` and `DataFrame.asfreq`
 - Fully explain the behaviour of `NDFrame.asfreq` in the extended summary.
 - Add an extended summary to `asfreq` in `PeriodArray`/`PeriodIndex`
 - Use `klass` kwarg in docstring template for `PeriodAray.asfreq`\`PeriodIndex.asfreq`
 - Add an extended summary to `date_range`
 - Other very minor fixes/enhancements


The original motivator for this PR was [this question](https://stackoverflow.com/q/63860589/6117426), in particular the confusion of the asker."
772427864,38624,Backport PR #38613 on branch 1.2.x (CI: move py38 slow to azure #38429),meeseeksmachine,closed,2020-12-21T20:02:56Z,2020-12-21T22:00:48Z,Backport PR #38613: CI: move py38 slow to azure #38429
772323349,38618,TST/REF: consolidate hist tests,mzeitlin11,closed,2020-12-21T17:02:52Z,2020-12-21T23:07:37Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Splitting up #38577, following @ivanovmg's comment. First step moves some `test_hist...` tests from `test_series.py` to `test_hist_method.py`, after this the remainder of `hist` tests in `test_series.py` will be duplicates to be deleted in a followup pr."
769633305,38537,TYP: pandas/io/sql.py (easy: bool/str) ,arw2019,closed,2020-12-17T07:26:33Z,2020-12-21T23:45:28Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
767166458,38490,REF: share astype code in MaskedArray,jbrockmendel,closed,2020-12-15T04:36:05Z,2020-12-22T00:11:44Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @jorisvandenbossche i dont know if this is the optimal way to do this, but it seems like there is a lot of share-ability here."
771262183,38572,CLN: DatetimeArray.astype,jbrockmendel,closed,2020-12-19T00:50:08Z,2020-12-22T00:12:19Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
771404446,38583,REF: implement construct_2d_arraylike_from_scalar,jbrockmendel,closed,2020-12-19T15:51:13Z,2020-12-22T00:13:10Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
767917050,38505,REF: require listlike in maybe_cast_to_datetime,jbrockmendel,closed,2020-12-15T17:51:59Z,2020-12-22T01:02:40Z,
766953707,38473,BUG: require arraylike in infer_dtype_from_array,jbrockmendel,closed,2020-12-14T21:41:34Z,2020-12-22T01:03:45Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
772549621,38628,TST/REF: Delete duplicate hist tests,mzeitlin11,closed,2020-12-22T00:24:13Z,2020-12-22T01:21:35Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

The tests deleted in this pr are also found starting here: https://github.com/pandas-dev/pandas/blob/e4f425dddda6b299fe5f5e05f18f128302f8604c/pandas/tests/plotting/test_hist_method.py#L26-L27
"
772377433,38620,BUG: tm.assert_produces_warning() doesn't work with expected_warning=None or False,mzeitlin11,closed,2020-12-21T18:29:49Z,2020-12-22T13:40:41Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import warnings
import pandas._testing as tm

with tm.assert_produces_warning(None):
    warnings.warn(UserWarning)

with tm.assert_produces_warning(False):
    warnings.warn(RuntimeWarning)
```

#### Problem description

Docs for this function say ""To check that no warning is returned, specify ``False`` or ``None``"". It also includes the second example as something that should raise an `AssertionError`. However, both these examples do not raise.

#### Expected Output

Raise `AssertionError` for both.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 9a46a4b700d1b794d9a944928ac95f4cb742b259
python           : 3.8.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Oct 29 22:56:45 PDT 2020; root:xnu-6153.141.2.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.3.0.dev0+125.g9a46a4b70
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.3.1
setuptools       : 49.6.0.post20201009
Cython           : 0.29.21
pytest           : 6.2.0
hypothesis       : 5.43.3
sphinx           : 3.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.4
fastparquet      : 0.4.1
gcsfs            : 0.7.1
matplotlib       : 3.3.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 2.0.0
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.3
sqlalchemy       : 1.3.20
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.2
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.52.0

</details>
"
723017409,37157,BUG: pd.read_sql_table() raises unknown column error when column name of a table contains `%`,TrigunaBN,closed,2020-10-16T08:27:57Z,2020-12-22T13:57:17Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

###### Mysql Create Table
```
DROP TABLE IF EXISTS test.my_table;
CREATE TABLE test.my_table 
(
    `id` VARCHAR(3),
    PRIMARY KEY(id),
    `price` DOUBLE NULL,
    `%_variation` DOUBLE NULL
);

INSERT INTO test.my_table VALUES('101', 25.2, 0.1);
INSERT INTO test.my_table VALUES('102', 40.2, -0.5);
INSERT INTO test.my_table VALUES('103', 55.2, 0.9);
```

```python
import pandas as pd
from sqlalchemy import create_engine

DB_PW = 'My_password'
DB_PORT = '3306'
DB_NAME = 'Test'

db_uri = f'mysql+mysqldb://root:{DB_PW}@localhost:{DB_PORT}'
engine = create_engine(f'{db_uri}/{DB_NAME}')

df = pd.read_sql_table(table_name='my_table', con=engine, schema=DB_NAME)
```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]
![image](https://user-images.githubusercontent.com/41903335/96234571-e7e6af00-0f99-11eb-9715-ddf167cde9b0.png)

![image](https://user-images.githubusercontent.com/41903335/96234256-7b6bb000-0f99-11eb-88da-b82c0b361df7.png)

As you can see above, the `%_variation` column is read as `%%_variation` in the SELECT statement. <br> 
But the `%%_variation` column is not present in the database and thus reading this table from the database provides an error.

#### Expected Output

I expect pandas to read the table from the database. <br>

I tried reading with pandas version 1.0.5 and I was able to read the table from the database without any problem. <br>
![image](https://user-images.githubusercontent.com/41903335/96232062-2bd8b480-0f98-11eb-9228-56a6ba23c3e8.png)


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.3
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.19
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
742119386,37804,ENH: Add 'end' option in resample's origin,GYHHAHA,closed,2020-11-13T04:10:10Z,2020-12-22T14:02:49Z,"In some cases I need to calculate the time group from the end of time series instead of the beginning.

For example, if user uses `s.resample('7min', origin='end', closed='right')` , the following expected groups are (09:23:45, 09:30:45], (09:16:45, 09:23:45], (09:09:45, 09:16:45], ... , (08:20:45, 08:27:45]. And under normal cases, `closed` should be `right` when `origin` is `end` since last value should serve as the end value of the last group.

```python
>>>idx = pd.date_range('20200101 8:26:35', '20200101 9:31:58', freq='77s')
>>>idx
DatetimeIndex(['2020-01-01 08:26:35', '2020-01-01 08:27:52',
               '2020-01-01 08:29:09', '2020-01-01 08:30:26',
               '2020-01-01 08:31:43', '2020-01-01 08:33:00',
               ... ...
               '2020-01-01 09:25:37', '2020-01-01 09:26:54',
               '2020-01-01 09:28:11', '2020-01-01 09:29:28',
               '2020-01-01 09:30:45'],
              dtype='datetime64[ns]', freq='77S')
>>>data = np.ones(len(idx))
>>>s = pd.Series(data,index=idx)
>>>s
2020-01-01 08:26:35    1.0
2020-01-01 08:27:52    1.0
2020-01-01 08:29:09    1.0
.... ...
2020-01-01 09:23:03    1.0
2020-01-01 09:24:20    1.0
2020-01-01 09:25:37    1.0
2020-01-01 09:26:54    1.0
2020-01-01 09:28:11    1.0
2020-01-01 09:29:28    1.0
2020-01-01 09:30:45    1.0
Freq: 77S, dtype: float64
```

Thus the sum operation on this resampler shows:

```python
>>>s.resample('7min', origin='end', closed='right').sum()
2020-01-01 08:27:45    1.0
2020-01-01 08:34:45    6.0
2020-01-01 08:41:45    5.0
2020-01-01 08:48:45    6.0
2020-01-01 08:55:45    5.0
2020-01-01 09:02:45    6.0
2020-01-01 09:09:45    5.0
2020-01-01 09:16:45    6.0
2020-01-01 09:23:45    5.0
2020-01-01 09:30:45    6.0
Freq: 7T, dtype: float64
```"
599804448,33548,BUG: groupby resample different results with .agg() vs .mean(),pblankley,closed,2020-04-14T19:24:39Z,2020-12-22T14:07:19Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

data = pd.DataFrame({
    'cat': ['cat_1', 'cat_1', 'cat_2', 'cat_1', 'cat_2', 'cat_1', 'cat_2', 'cat_1'],
    'num': [5,20,22,3,4,30,10,50],
    'date': ['2019-2-1', '2018-02-03','2020-3-11','2019-2-2', '2019-2-2', '2018-12-4','2020-3-11', '2020-12-12']
})
data['date'] = pd.to_datetime(data['date'])
using_agg = data.groupby('cat').resample('Y', on='date').agg({'num': 'mean'})
using_mean = data.groupby('cat').resample('Y', on='date').mean()

print(using_agg)
print(using_mean)

assert(np.allclose(using_agg['num'], using_mean['num']))

```

#### Problem description

I expect to get the same result from using .agg({col_name: 'mean'}) and I expect to get from .mean()

It's very surprising the results are different here, and really worrying for me, considering historic code for us might be producing incorrect results. Can anyone shed some light on why this may be the case?

#### Expected Output

The output from `using_mean` is correct and should be equal to the output from `using_agg`

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.3.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 18.1
setuptools       : 40.6.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
761948593,38408,ENH: add end and end_day origin for resample,GYHHAHA,closed,2020-12-11T06:22:06Z,2020-12-22T14:07:34Z,"- [x] closes #37804
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
744324134,37905,BUG: groupby resample different results with .agg() vs .mean() ,jalmaguer,closed,2020-11-17T01:09:39Z,2020-12-22T14:07:38Z,"- [x] closes #33548
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
275795079,18414,KeyError when calling to_coo() on SparseDataFrame,ediphy-azorab,closed,2017-11-21T17:20:31Z,2020-12-22T14:10:54Z,"#### Code Sample, a copy-pastable example if possible

```python
In [45]: t_df = idx = pd.Int64Index([2,3,4])
    ...: t_df = pd.DataFrame(data=0, columns=idx, index=idx)
    ...: t_df.apply(pd.SparseArray).sparse.to_coo() # This line blows up in find_common_types

```
#### Problem description

Currently, a `KeyError` is raised while trying to get the first column type (cast.py#1070).

#### Expected Output

A (very) sparse matrix

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.10.0-38-generic
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: C.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.21.0
pytest: None
pip: 10.0.0.subpip_fix
setuptools: 36.5.0
Cython: None
numpy: 1.13.3
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2017.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.1.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: 4.1.0
bs4: 4.6.0
html5lib: 1.0b10
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.5.0
</details>
"
772434089,38626,Bug: assert_produces_warning(None) not raising AssertionError with warning,mzeitlin11,closed,2020-12-21T20:14:12Z,2020-12-22T14:31:26Z,"- [x] closes #38620
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
771143449,38567,BUG: .sparse.to_coo() with numeric col index without a 0,mzeitlin11,closed,2020-12-18T20:13:14Z,2020-12-22T14:31:58Z,"- [x] closes #18414
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Added tests for some other column name types which raised before this change, if that's overkill I can just test the int case from the OP."
772967365,38632,Backport PR #37534 on branch 1.2.x (BUG: error raise when column contains percentage),meeseeksmachine,closed,2020-12-22T13:59:10Z,2020-12-22T15:16:43Z,Backport PR #37534: BUG: error raise when column contains percentage
772580830,38629,REF: collected dtypes.cast and construction simplifications,jbrockmendel,closed,2020-12-22T02:03:44Z,2020-12-22T15:31:17Z,Also catch a warning in an io test that has been bugging me.
772973726,38633,Backport PR #37905 on branch 1.2.x (BUG: groupby resample different results with .agg() vs .mean() ),meeseeksmachine,closed,2020-12-22T14:07:50Z,2020-12-22T15:41:58Z,Backport PR #37905: BUG: groupby resample different results with .agg() vs .mean() 
733723650,37534,BUG: error raise when column contains percentage,erfannariman,closed,2020-10-31T13:44:42Z,2020-12-22T16:50:06Z,"- [x] closes #37157 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
768393717,38516,API: CategoricalDtype.__eq__ with categories=None stricter,jbrockmendel,closed,2020-12-16T02:35:21Z,2020-12-22T17:45:47Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

xref #37929

This will allow us to remove special-casing for Categorical in Block._astype, Index.astype, and Categorical.astype."
702979701,36403,BLD/CI fix arm64 build #36397,fangchenli,closed,2020-09-16T18:22:07Z,2020-12-22T19:02:35Z,"- [x] closes #36397 and 
closes #34149 and
closes #30041

"
679855713,35757,CI: Unpin Pytest + Pytest Asyncio Min Version,alimcmaster1,closed,2020-08-17T00:09:59Z,2020-12-22T19:03:19Z,"- [x] closes #35620

Pytest 6.0.0+ will require pytest-asyncio>=0.12.0 for these async tests to run.

Also worth noting `pytest-asyncio 0.12.0 requires pytest>=5.4.0`. (So maybe we should think about bumping min pytest version)

```
____________________________________________________________________________________ ERROR collecting pandas/tests/indexes/test_base.py _____________________________________________________________________________________
../../.conda/envs/pandas-dev/lib/python3.8/site-packages/pluggy/hooks.py:286: in __call__
    return self._hookexec(self, self.get_hookimpls(), kwargs)
../../.conda/envs/pandas-dev/lib/python3.8/site-packages/pluggy/manager.py:93: in _hookexec
    return self._inner_hookexec(hook, methods, kwargs)
../../.conda/envs/pandas-dev/lib/python3.8/site-packages/pluggy/manager.py:84: in <lambda>
    self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
../../.conda/envs/pandas-dev/lib/python3.8/site-packages/pytest_asyncio/plugin.py:39: in pytest_pycollect_makeitem
    item = pytest.Function(name, parent=collector)
../../.conda/envs/pandas-dev/lib/python3.8/site-packages/_pytest/nodes.py:95: in __call__
    warnings.warn(NODE_USE_FROM_PARENT.format(name=self.__name__), stacklevel=2)
E   pytest.PytestDeprecationWarning: Direct construction of Function has been deprecated, please use Function.from_parent.
E   See https://docs.pytest.org/en/stable/deprecations.html#node-construction-changed-to-node-from-parent for more details.
```

cc @simonjayhawkins "
772265493,38615,BUG: remove unused tz keyword from PeriodIndex constructor,jbrockmendel,closed,2020-12-21T15:48:10Z,2020-12-22T19:05:58Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
772276229,38616,REGR: pandas 1.2rc fails merge with AttributeError: 'bool' object has no attribute 'all',jorisvandenbossche,closed,2020-12-21T16:04:58Z,2020-12-22T19:22:19Z,"From the dask test suite:

```
left = pd.DataFrame({""x"": [1, 1], ""z"": [""foo"", ""foo""]})
right = pd.DataFrame({""x"": [1, 1], ""z"": [""foo"", ""foo""]})

pd.merge(left, right, how=""right"", left_index=True, right_on=""x"")
```

This fails on master / 1.2.0rc:

```
In [18]: pd.merge(left, right, how=""right"", left_index=True, right_on=""x"")
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-18-94bf9fb575b1> in <module>
      2 right = pd.DataFrame({""x"": [1, 1], ""z"": [""foo"", ""foo""]})
      3 
----> 4 pd.merge(left, right, how=""right"", left_index=True, right_on=""x"")

~/scipy/pandas/pandas/core/reshape/merge.py in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)
     93         validate=validate,
     94     )
---> 95     return op.get_result()
     96 
     97 

~/scipy/pandas/pandas/core/reshape/merge.py in get_result(self)
    710             result = self._indicator_post_merge(result)
    711 
--> 712         self._maybe_add_join_keys(result, left_indexer, right_indexer)
    713 
    714         self._maybe_restore_index_levels(result)

~/scipy/pandas/pandas/core/reshape/merge.py in _maybe_add_join_keys(self, result, left_indexer, right_indexer)
    866                 if mask_left.all():
    867                     key_col = rvals
--> 868                 elif mask_right.all():
    869                     key_col = lvals
    870                 else:

AttributeError: 'bool' object has no attribute 'all'

```"
773133346,38638,BUG: Fix regression in pandas merge,phofl,closed,2020-12-22T17:50:03Z,2020-12-22T20:02:03Z,"- [x] closes #38616
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Since this is a regression on master I think a whatsnew is not necessary?
"
759389162,38357,DOC: Clarify stability of sorting in documentation of DataFrame.sort_values for multiple columns / labels,jotasi,closed,2020-12-08T12:02:54Z,2020-12-22T20:32:57Z,"#### Location of the documentation

Description of the argument `kind` in the documentation of [pandas.DataFrame.sort_values](https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.sort_values.html).

#### Documentation problem

The current documentation states (emphasize mine):

>Choice of sorting algorithm. See also ndarray.np.sort for more information. **mergesort is the only stable algorithm. For DataFrames, this option is only applied when sorting on a single column or label**.

To me that means, that the `kind` argument is ignored if you sort by multiple columns / labels. But I think one could even interpret it as `mergesort` explicitely is not done in that case. Running some tests, it does look like the sorting is stable when sorting multiple columns / labels. But it is not written anywhere. If it really is guaranteed that the sorting is stable for sorting by multiple columns / labels, it would be nice to have this written down explicitely in the documentation.

#### Suggested fix for documentation

IF that is actually the case, I would suggest to add a sentence to the effect that the sorting is stable if sorting is done by multiple columns / labels. E.g. change the `kind` argument description to:

>Choice of sorting algorithm. See also ndarray.np.sort for more information. mergesort is the only stable algorithm. ~~For DataFrames, this option is only applied when sorting on a single column or label.~~ **For DataFrames, if sorting by multiple columns or labels, this argument is ignored, defaulting to a stable sorting algorithm.**"
769374168,38533,BUG&TST: HTML formatting error in Styler.render() in rowspan attribute,JoseNavy,closed,2020-12-17T00:27:41Z,2020-12-22T20:34:02Z,"- [x] closes [#38234](https://github.com/pandas-dev/pandas/issues/38234)
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
764087916,38426,DOC: Clarify that DataFrame.sort_values is stable for sorting by multiple columns or labels,jotasi,closed,2020-12-12T17:31:36Z,2020-12-22T20:34:34Z,"- [x] closes #38357
- ~~tests added / passed~~
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- ~~whatsnew entry~~

From what I understand from the code, for `by` containing more than one entry, `pandas.DataFrame.sort_values` uses `pandas.core.sorting.lexsort_indexer`, which converts the columns to ordered `Categorical`s, gets grouped lexicographic group indices via `get_group_index`, compressed these via `compress_group_index` and then sorts using counting sort or mergesort via `get_group_index_sorter`. Both of these are stable, as explained in `get_group_index_sorter`'s docstring. Therefore, the sorting should be stable if `sort_values` is called with `len(by)>1`."
772596879,38631,DOC: capitalize NumPy as proper noun,erictleung,closed,2020-12-22T02:54:32Z,2020-12-22T20:51:52Z,"In a similar vein to https://github.com/pandas-dev/pandas/pull/37808

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
769316863,38530,REF: avoid special-casing Categorical astype,jbrockmendel,closed,2020-12-16T22:28:54Z,2020-12-22T20:53:45Z,Sits on top of #38516
773187821,38639,Backport PR #38638 on branch 1.2.x (BUG: Fix regression in pandas merge),meeseeksmachine,closed,2020-12-22T19:22:50Z,2020-12-22T21:02:15Z,Backport PR #38638: BUG: Fix regression in pandas merge
773128987,38637,REF: simplify logic in assert_produces_warning,ivanovmg,closed,2020-12-22T17:43:02Z,2020-12-22T21:04:09Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Simplified logic in ``assert_produces_warning`` by extracting functions ``_assert_caught_expected_warning``, ``_assert_caught_no_extra_warnings`` and ``_is_unexpected_warning``."
773232321,38640,Backport PR #38533 on branch 1.2.x (BUG&TST: HTML formatting error in Styler.render() in rowspan attribute),meeseeksmachine,closed,2020-12-22T20:34:22Z,2020-12-22T22:21:32Z,Backport PR #38533: BUG&TST: HTML formatting error in Styler.render() in rowspan attribute
458913767,26974,Index constructor doesn't validate kwargs,TomAugspurger,closed,2019-06-20T22:26:36Z,2020-12-22T23:45:29Z,"```python
In [4]: pd.Index([1, 2], abc=1)                                                                                                                                                                                                                                                                                                                                           
Out[4]: Int64Index([1, 2], dtype='int64')
```

This should raise, since `abc` isn't a valid keyword."
350074506,22315,Index() constructor is inconsistent in detecting subclass-specific keyword arguments,toobaz,closed,2018-08-13T15:16:54Z,2020-12-22T23:45:29Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: pd.Index(['20000101', '20000102'])
Out[2]: Index(['20000101', '20000102'], dtype='object')

In [3]: pd.Index(['20000101', '20000102'], freq='D')
Out[3]: Index(['20000101', '20000102'], dtype='object')

In [4]: pd.Index(['20000101', '20000102'], tz='utc')
Out[4]: DatetimeIndex(['2000-01-01 00:00:00+00:00', '2000-01-02 00:00:00+00:00'], dtype='datetime64[ns, UTC]', freq=None)
```

#### Problem description

I see no obvious reason why ``tz`` should trigger the interpretation of data as datetime, and ``freq`` shouldn't. And I think the only viable option is to refrain entirely from using kwargs to do inference - unless we are committed to support them all, which could be a pain.

From the point of view of users, the only change would be to replace
```
pd.Index(['20000101', '20000102'], tz='utc')
```
with
```
pd.Index(['20000101', '20000102'], tz='utc', dtype='datetime64')
```

#### Expected Output

``Out[3]`` and ``Out[4]`` should be instances of the same class.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: eb0ac5437f9fee2939b3c3c4dab3ed745b5a9c38
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.0-6-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: it_IT.UTF-8
LOCALE: it_IT.UTF-8

pandas: 0.24.0.dev0+460.geb0ac5437
pytest: 3.5.0
pip: 9.0.1
setuptools: 39.2.0
Cython: 0.28.4
numpy: 1.14.3
scipy: 0.19.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: 1.5.6
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: 1.2.0dev
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.2.2.post1634.dev0+ge8120cf6d
openpyxl: 2.3.0
xlrd: 1.0.0
xlwt: 1.3.0
xlsxwriter: 0.9.6
lxml: 4.1.1
bs4: 4.5.3
html5lib: 0.999999999
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.2.1
gcsfs: None


</details>
"
328967099,21311,BUG/API: Index constructor does not enforce specified dtype ,jorisvandenbossche,closed,2018-06-04T09:01:23Z,2020-12-22T23:45:29Z,"#### Code Sample, a copy-pastable example if possible

Manually specifying a dtype does not garantuee the output is in that dtype. Eg with Series if incompatible data is passed, an error is raised, while for Index it just silently outputs another dtype:

```python
In [11]: pd.Series(['a', 'b', 'c'], dtype='int64')
...
ValueError: invalid literal for int() with base 10: 'a'

In [12]: pd.Index(['a', 'b', 'c'], dtype='int64')
Out[12]: Index(['a', 'b', 'c'], dtype='object')

```
"
173525866,14093,ERR: stricter checks on Index construction when tz is passed,jreback,closed,2016-08-26T19:28:18Z,2020-12-22T23:45:29Z,"from [comment](https://github.com/pydata/pandas/pull/13981#issuecomment-239846027) on #13981 

```
In [1]: pd.Index([datetime.datetime(2012,1,1), datetime.datetime(2012,1, 2)], tz=""Europe/Brussels"")
Out[1]: DatetimeIndex(['2012-01-01 00:00:00+01:00', '2012-01-02 00:00:00+01:00'], dtype='datetime64[ns, Europe/Brussels]', freq=None)

In [2]: pd.Index([1, 2], tz=""Europe/Brussels"")
Out[2]: DatetimeIndex(['1970-01-01 00:00:00+01:00', '1970-01-01 00:00:00+01:00'], dtype='datetime64[ns, Europe/Brussels]', freq=None)

In [3]: pd.Index(['20120102', '20120103'], tz=""Europe/Brussels"")
Out[3]: DatetimeIndex(['2012-01-02 00:00:00+01:00', '2012-01-03 00:00:00+01:00'], dtype='datetime64[ns, Europe/Brussels]', freq=None)
```

[1] is fine/expected
[2] should be disallowed (e.g. integer construction)
[3] is ok, maybe should disallow to be consistent.

I am not sure what this would break. But we don't necessarily want to force datetime construction on a generic `Index` (as opposed to using `.to_datetime` or a constructor, e.g. `pd.date_range`). This is a bit too much inference.

So the rule would be that it has to be actual datetimes/Timestamps, if tz is provided (which is a form of dtype).
"
771753399,38597,DEPR: allowing subclass-specific keywords in pd.Index.__new__,jbrockmendel,closed,2020-12-21T00:26:41Z,2020-12-23T00:08:31Z,"- [x] closes #21311, closes #14093, closes #22315, closes #26974
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
773479119,38652,DOC: example for doing a reversed rolling window,willb0246,closed,2020-12-23T04:36:59Z,2020-12-23T04:38:11Z,"Trying to fix #38627 
"
770992461,38561,Move docstring of NDFrame.replace in preparation of #32542,oguzhanogreden,closed,2020-12-18T15:55:35Z,2020-12-23T06:47:41Z,"This is a pre-cursor PR for #32542 as requested by @jreback [here](https://github.com/pandas-dev/pandas/pull/32542#issuecomment-744043100).

It moves the docstring of `replace()` method(s) to `pandas.core.shared_docs` so that we can reuse most of it for index classes."
734434355,37582,CLN refactor core indexes,MarcoGorelli,closed,2020-11-02T11:38:39Z,2020-12-23T08:06:32Z,"Some refactorings found by Sourcery https://sourcery.ai/

I've removed the ones of the kind
```diff
- if param:
-     var = a
- else:
-     var = b
+ var = a if param else b
```"
765292454,38444,CI: add slash dispatch workflow to trigger pre-commit checks with comment,MarcoGorelli,closed,2020-12-13T11:16:20Z,2020-12-23T08:06:55Z,"This would be an on-demand bot to run pre-commit checks on a PR, which can be triggered by commenting

```
@github-actions pre-commit
```

on a pull request (see [here](https://github.com/nbQA-dev/nbQA/pull/518) for a demo).

Use case: if a PR is submitted and is good-to-go except for some linting error, we can just comment `/pre-commit-run` and have the bot fixup the errors."
724422347,37239,DOC: documentation rendering issue for functions using typing overloads,shaido987,closed,2020-10-19T09:04:18Z,2020-12-23T11:58:26Z,"#### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.concat.html


#### Documentation problem
This is how the method header looks for concat:

```python
pandas.concat(objs: Union[Iterable[‘DataFrame’], Mapping[Label, ‘DataFrame’]], axis='0', join: str = ""'outer'"", 
ignore_index: bool = 'False', keys='None', levels='None', names='None', verify_integrity: bool = 'False', 
sort: bool = 'False', copy: bool = 'True') → ’DataFrame’
pandas.concat(objs: Union[Iterable[FrameOrSeries], Mapping[Label, FrameOrSeries]], axis='0', join: str = ""'outer'"", 
ignore_index: bool = 'False', keys='None', levels='None', names='None', verify_integrity: bool = 'False', 
sort: bool = 'False', copy: bool = 'True') → FrameOrSeriesUnion
```

For comparison, this is merge:
```python
pandas.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, 
right_index=False, sort=False, suffixes='_x', '_y', copy=True, indicator=False, validate=None)
```

As can be seen above, concat keeps the type information and there are two entries (likely due to the different output types).

I'm not sure why this occurs, but there are several overloaded methods in the code which could be related: https://github.com/pandas-dev/pandas/blob/54fa3da2e0b65b733d12de6cbecbf2d8afef4e9f/pandas/core/reshape/concat.py#L40-L83"
773557976,38655,BUG: multiple index dataframe with index names groupy rolling computing reture wrong index,foolcage,closed,2020-12-23T07:43:07Z,2020-12-23T14:06:46Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample

```python
import pandas as pd
arrays = [np.array(['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux']), np.array(['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two'])]
df = pd.DataFrame(np.random.randn(8, 4), index=arrays)
df.groupby(level=0).rolling(window=2).mean()
```
the result:
```
                    0         1         2         3
bar bar one       NaN       NaN       NaN       NaN
        two -0.361421 -0.696686 -0.746137 -0.053621
baz baz one       NaN       NaN       NaN       NaN
        two  1.014213  0.575803  0.199990 -0.596995
foo foo one       NaN       NaN       NaN       NaN
        two -0.337821  0.921825  0.256034 -0.572588
qux qux one       NaN       NaN       NaN       NaN
        two -0.404824 -1.209763  0.581025 -0.741480
```

```
#now set the name to the index
df.index.names=['a','b']
df.groupby(level=0).rolling(window=2).mean()
```
the result:
```
            0         1         2         3
a
bar       NaN       NaN       NaN       NaN
bar -0.361421 -0.696686 -0.746137 -0.053621
baz       NaN       NaN       NaN       NaN
baz  1.014213  0.575803  0.199990 -0.596995
foo       NaN       NaN       NaN       NaN
foo -0.337821  0.921825  0.256034 -0.572588
qux       NaN       NaN       NaN       NaN
qux -0.404824 -1.209763  0.581025 -0.741480
```


#### Problem description

multiple index dataframe with index names groupy rolling computing reture wrong index

#### Expected Output

the return result should keep the original index

#### Output of ``pd.show_versions()``

<details>

In [36]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.8.2.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Mon Aug 31 22:12:52 PDT 2020; root:xnu-6153.141.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : zh_CN.UTF-8
LOCALE           : zh_CN.UTF-8

pandas           : 1.1.5
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.2
Cython           : None
pytest           : 6.2.0
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.2.14
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.1.0
xlwt             : None
numba            : None
</details>
"
769186846,38527,BUG: can't use to_clipboard/read_clipboard on WSL 2.0,mbkupfer,closed,2020-12-16T18:50:27Z,2020-12-23T14:08:08Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
>>> pd.read_clipboard()
PyperclipException:
    Pyperclip could not find a copy/paste mechanism for your system.
    For more information, please visit
    https://pyperclip.readthedocs.io/en/latest/introduction.html#not-implemented-error

>>> pd.DataFrame().to_clipboard()
PyperclipException:
    Pyperclip could not find a copy/paste mechanism for your system.
    For more information, please visit
    https://pyperclip.readthedocs.io/en/latest/introduction.html#not-implemented-error
```

#### Problem description

`pd.read_clipboard` and `pd.DataFrame.to_clipboard` should work. For instance, I can run the following with no issues:
```python
>>> import pyperclip
>>> pyperclip.copy('foo')
>>> pyperclip.paste()
'foo'
```

#### Expected Output
N/A
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.128-microsoft-standard
Version          : #1 SMP Tue Jun 23 12:58:10 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.5
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
770250151,38546,patch wsl compatibility,mbkupfer,closed,2020-12-17T17:52:57Z,2020-12-23T14:08:11Z,"Change wsl check to something more universal and consistent

- [x] closes #38527
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
768848658,38522,Better inference of spreadsheet formats.,cjw296,closed,2020-12-16T13:09:26Z,2020-12-23T14:11:06Z,"See:

https://github.com/pandas-dev/pandas/issues/38424#issuecomment-744062773
https://github.com/pandas-dev/pandas/pull/38456

Discussion happening on #38424, code review happening here ;-)"
220757339,15969,ERR: Most consistent error handling when passing win_type='freq' in rolling,TomAugspurger,closed,2017-04-10T20:14:53Z,2020-12-23T14:27:49Z,"Working on https://github.com/dask/dask/issues/2190 (df.rolling('5s') for dask), and I think these should all be equivalent.

```python
In [26]: import pandas as pd
In [27]: import numpy as np
In [31]: from pandas.tseries.frequencies import to_offset
In [28]: s = pd.Series(range(10), index=pd.date_range('2017', freq='s', periods=10))
```

```python
>>> s.rolling('2s')  # Case 1: correct
>>> s.rolling(window=2000000000, min_periods=1, win_type='freq')  # Case 2
>>> s.rolling(window=to_offset('2s'), min_periods=1, win_type='freq')  # Case 3
>>> s.rolling(window=pd.Timedelta('2s'), min_periods=1, win_type='freq')  # Same as 3
```

I don't *think* there are any parsing ambiguities.

Currently we have

```python
# Case 1
In [33]: s.rolling('2s')
Out[33]: Rolling [window=2000000000,min_periods=1,center=False,win_type=freq,axis=0]
```

```python
# Case 2
In [35]: s.rolling(window=2000000000, min_periods=1, win_type='freq')
```
```pytb
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-2-a24a29dff0ab> in <module>()
----> 1 s.rolling(window=2000000000, min_periods=1, win_type='freq')

/Users/taugspurger/.virtualenvs/dask-dev/lib/python3.6/site-packages/pandas/core/generic.py in rolling(self, window, min_periods, freq, center, win_type, on, axis)
   5502                                    min_periods=min_periods, freq=freq,
   5503                                    center=center, win_type=win_type,
-> 5504                                    on=on, axis=axis)
   5505
   5506         cls.rolling = rolling

/Users/taugspurger/.virtualenvs/dask-dev/lib/python3.6/site-packages/pandas/core/window.py in rolling(obj, win_type, **kwds)
   1795
   1796     if win_type is not None:
-> 1797         return Window(obj, win_type=win_type, **kwds)
   1798
   1799     return Rolling(obj, **kwds)

/Users/taugspurger/.virtualenvs/dask-dev/lib/python3.6/site-packages/pandas/core/window.py in __init__(self, obj, window, min_periods, freq, center, win_type, axis, on, **kwargs)
     76         self.win_type = win_type
     77         self.axis = obj._get_axis_number(axis) if axis is not None else None
---> 78         self.validate()
     79
     80     @property

/Users/taugspurger/.virtualenvs/dask-dev/lib/python3.6/site-packages/pandas/core/window.py in validate(self)
    505                 raise ValueError('Invalid win_type {0}'.format(self.win_type))
    506             if getattr(sig, self.win_type, None) is None:
--> 507                 raise ValueError('Invalid win_type {0}'.format(self.win_type))
    508         else:
    509             raise ValueError('Invalid window {0}'.format(window))

ValueError: Invalid win_type freq
```

```python
# Case 3
In [36]: s.rolling(window=to_offset('2s'), min_periods=1, win_type='freq')
```
```pytb
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-36-93e14a8d05f6> in <module>()
----> 1 s.rolling(window=to_offset('2s'), min_periods=1, win_type='freq')

/Users/taugspurger/Envs/dask-dev/lib/python3.6/site-packages/pandas/core/generic.py in rolling(self, window, min_periods, freq, center, win_type, on, axis)
   5502                                    min_periods=min_periods, freq=freq,
   5503                                    center=center, win_type=win_type,
-> 5504                                    on=on, axis=axis)
   5505
   5506         cls.rolling = rolling

/Users/taugspurger/Envs/dask-dev/lib/python3.6/site-packages/pandas/core/window.py in rolling(obj, win_type, **kwds)
   1795
   1796     if win_type is not None:
-> 1797         return Window(obj, win_type=win_type, **kwds)
   1798
   1799     return Rolling(obj, **kwds)

/Users/taugspurger/Envs/dask-dev/lib/python3.6/site-packages/pandas/core/window.py in __init__(self, obj, window, min_periods, freq, center, win_type, axis, on, **kwargs)
     76         self.win_type = win_type
     77         self.axis = obj._get_axis_number(axis) if axis is not None else None
---> 78         self.validate()
     79
     80     @property

/Users/taugspurger/Envs/dask-dev/lib/python3.6/site-packages/pandas/core/window.py in validate(self)
    507                 raise ValueError('Invalid win_type {0}'.format(self.win_type))
    508         else:
--> 509             raise ValueError('Invalid window {0}'.format(window))
    510
    511     def _prep_window(self, **kwargs):

ValueError: Invalid window <2 * Seconds>
```"
755332283,38234,BUG: Minor HTML formatting error in Styler.render(): rowspan attribute should use quotes,fwt,closed,2020-12-02T14:41:16Z,2020-12-23T14:38:03Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame(data=[[1, 2]], index=[[""l0"", ""l0""], [""l1a"", ""l1b""]])
s = Styler(df, uuid=""_"", cell_ids=False)
assert '<th id=""T__level0_row0"" class=""row_heading level0 row0"" rowspan=""2"">l0</th>' in s.render()
```

#### Problem description

This issue is very similar to #35706. The ``colspan`` attribute has been fixed in #36223 but the ``rowspan`` attribute hasn't been fixed yet. Fixing this would improve HTML correctness and make the ``Styler.render()`` behavior more consistent.

Compare [line 330](https://github.com/attack68/pandas/blob/42f600e2acc0cd905c078ab77b473f0537699cc5/pandas/io/formats/style.py#L330) (colspan attribute) and [line 376](https://github.com/attack68/pandas/blob/42f600e2acc0cd905c078ab77b473f0537699cc5/pandas/io/formats/style.py#L376) (rowspan attribute).

#### Expected Output

The ``rowspan`` attribute uses quotes and the assert statement doesn't fail.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.76-linuxkit
Version          : #1 SMP Tue May 26 11:42:35 UTC 2020
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 50.3.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
773306334,38646,TST/BUG: _gen_two_subplots always adding subplot even with passed axis,mzeitlin11,closed,2020-12-22T23:07:24Z,2020-12-23T14:49:02Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

`kwargs.get(""ax"", fig.add_subplot(211))` was always adding a subplot (since the argument to `.get()` will be evaluated even if it doesn't need to be used). Assuming the intended behavior was for `fig.add_subplot(211)` to be called only if `ax` is not passed. This change also gets rid of half of remaining `matplotlib` warnings from 
`pandas/tests/plotting/test_hist_method.py::TestSeriesPlots::test_hist_legacy`"
773273612,38643,REF: use _should_compare to short-circuit set operations,jbrockmendel,closed,2020-12-22T21:49:32Z,2020-12-23T15:21:49Z,
773312147,38648,REF: standardize get_indexer/get_indexer_non_unique code,jbrockmendel,closed,2020-12-22T23:22:11Z,2020-12-23T15:22:24Z,
770482317,38552,"BUG: Index([date]).astype(""category"").astype(object) roundtrip",jbrockmendel,closed,2020-12-18T00:53:45Z,2020-12-23T15:52:12Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This also makes some MultiIndex constructor behavior consistent with Index behavior, with the side-effect of fixing the thorny cases in #36131"
713209209,36791,How to use nrows along with chunksize in read_json(),madolmo,closed,2020-10-01T21:53:46Z,2020-12-23T16:32:22Z,"- [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [x] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

Why do I need to use _nrows_ when reading large json line files with _chunksize_ option?
Since version 1.1 I'm having troubles with the function _read_json()_ because even if I specify the option _chunksize_ with the correct value (the value that used to work with pandas v.1.0.5), the file seems to be read at once, with a memroy error in my case. If I add the _nrows_ option this doesn't happen but why? And what is the value you have to specify for the _nrows_ parameter in order to load the entire file? Do you have to know in advance the maximum number of rows? Is there any special value for ""all rows"" like -1 o 0 ?

Thanks

```python

#this raises a Memory Error (with a 4GB file) - this worked on version 1.0.5
reader = pd.read_json(f""{path}map_records.json"",orient='records' ,lines=True, chunksize=100000)
chunks=[chunk[(chunk.bidbasket==""BSKGEOALL00000000001"")&(chunk.tipomappa == ""AULTIPMPS_GIT"")][['bidsubzona','idoriginale','bidciv','bidbasket','tipomappa']]  for chunk in reader]

#this works, but it loads up to <nrorws> rows and I have to know the maximum number of rows in advance
reader = pd.read_json(f""{path}map_records.json"",orient='records' ,lines=True, chunksize=100000, nrows=20000000)
chunks=[chunk[(chunk.bidbasket==""BSKGEOALL00000000001"")&(chunk.tipomappa == ""AULTIPMPS_GIT"")][['bidsubzona','idoriginale','bidciv','bidbasket','tipomappa']]  for chunk in reader]



```
"
629929701,34548,Chunksize from json memory consumption as high as without chunksize,hasnain2808,closed,2020-06-03T11:47:53Z,2020-12-23T16:32:22Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

While working on the PR (#33962) for adding nrows to read_json it was seen that maybe the chunksize parameter for read_json is not doing its task. ASV benchmarks were carried out but as this issue is not related to the PR we decided to create a new issue for it

Here is the link to the paste that has all the results
[paste](https://pastebin.com/E22PAxKe)
The results are nicely added this to this spreadsheet
[spreadsheet](https://docs.google.com/spreadsheets/d/1MjE2iMrlYnWbnfSyizfYI3IbwSK1z760VKeoYfCBeJs/edit?usp=sharing)
"
757316524,38293,BUG: read_json does not respect chunksize,robertwb,closed,2020-12-04T18:37:19Z,2020-12-23T16:32:40Z,"- [X] closes #36791
- [X] closes #34548
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
773389801,38651,REF: use DatetimeArray._validate_setitem_value for DatetimeBlock._can_hold_element,jbrockmendel,closed,2020-12-23T02:18:44Z,2020-12-23T17:02:34Z,
765579766,38456,DOC: update wording about when xlrd engine can be used,cjw296,closed,2020-12-13T18:18:03Z,2020-12-23T17:04:25Z,"See https://github.com/pandas-dev/pandas/issues/38424#issuecomment-744043899.
"
726452321,37310,BUG: stabilize sort_values algorithms for Series and time-like Indices,AlexKirko,closed,2020-10-21T12:52:49Z,2020-12-23T17:48:20Z,"- [X] closes #35922
- [X] 18 tests changed / 18 passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

## Problem

Our `sort_values` functions currently behave differently for different objects: for most `Index` subclasses they are stable when sorting in descending order (this was introduced by #35604), but for DateTime-like `Index` subclasses and `Series` they are unstable. This isn't good as sorting should be stable across the board.

## Details

Came across this one while introducing missing-value support to `Index.sort_values` in #35604, so I had to limit that PR to non-DateTime-like `Index` subclasses. The problem was that we had different expectations for sorting stability baked into our test suite, so unifying sorting algorithms and missing-value support needed a bunch of careful test changes and altering both `sort_values` and algorithms in `sorting.py`.

Since this PR necessarily includes changes in several places, I have commented on all the changes made in the code and the unusual changes in the tests to make reviewing the code easier (see ""On test changes"" below).

## On test changes

Most changes I made in the tests are for cases where we were expecting an unstable sort or expected NaNs to be sorted to the beginning of a list of duplicates for ascending sort and to the end for descending (we forced this by inserting NaN-likes at 0 position and reversing when sorting in descending order in `Series.sort_values`).

## Default behavior changes

Since DateTime-like `Index` subclasses now support `na_position` using the same implementation as the other `Index` subclasses, they now sort missing values to the end of the `Index` by default.

## Performance

Ran the full benchmark suite, and there are no performance regressions.

## Out-of-scope

The only type I didn't touch so far is MultiIndex. It can't be sorted the same way through nargsort, and I don't think we should be doing it in this PR, if it all (stabilizing descending order MultiIndex.sort_values will definitely be a PITA, and it's a very narrow use case, in my opinion)."
773920047,38658,Backport PR #38293 on branch 1.2.x (BUG: read_json does not respect chunksize),meeseeksmachine,closed,2020-12-23T16:33:30Z,2020-12-23T18:12:58Z,Backport PR #38293: BUG: read_json does not respect chunksize
764453449,38433,BUG/API: allowing scalars in Categorical constructor,jbrockmendel,closed,2020-12-12T21:54:07Z,2020-12-23T18:44:29Z,"I expected this to raise

```
>> pd.Categorical(""A"")
['A']
Categories (1, object): ['A']
```

This gets listified inside the call to `maybe_infer_to_datetimelike` in `Categorical.__init__`"
274177147,18306,Fix matplotlib warnings,TomAugspurger,closed,2017-11-15T14:33:36Z,2020-12-23T18:50:05Z,"This is with matplotlib 2.1.0

```
===================================================================== warnings summary ======================================================================
pandas/tests/plotting/test_frame.py::TestDataFramePlots::()::test_subplots_multiple_axes
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/plotting/_tools.py:204: UserWarning: When passing multiple axes, layout keyword is ignored
    ""ignored"", UserWarning)

pandas/tests/plotting/test_frame.py::TestDataFramePlots::()::test_line_colors
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/plotting/_core.py:188: UserWarning: 'colors' is being deprecated. Please use 'color'instead of 'colors'
    warnings.warn((""'colors' is being deprecated. Please use 'color'""

pandas/tests/plotting/test_frame.py::TestDataFramePlots::()::test_errorbar_plot
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/plotting/_core.py:1727: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared
    plot_obj.generate()
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/plotting/_core.py:1727: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared
    plot_obj.generate()
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/plotting/_core.py:1727: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared
    plot_obj.generate()

pandas/tests/plotting/test_frame.py::TestDataFramePlots::()::test_errorbar_timeseries
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/plotting/_core.py:1727: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared
    plot_obj.generate()
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/plotting/_core.py:1727: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared
    plot_obj.generate()
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/plotting/_core.py:1727: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared
    plot_obj.generate()

pandas/tests/plotting/test_hist_method.py::TestSeriesPlots::()::test_hist_legacy
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
    warnings.warn(message, mplDeprecation, stacklevel=1)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
    warnings.warn(message, mplDeprecation, stacklevel=1)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
    warnings.warn(message, mplDeprecation, stacklevel=1)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
    warnings.warn(message, mplDeprecation, stacklevel=1)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
    warnings.warn(message, mplDeprecation, stacklevel=1)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
    warnings.warn(message, mplDeprecation, stacklevel=1)

pandas/tests/plotting/test_hist_method.py::TestDataFramePlots::()::test_tight_layout
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/pandas/pandas/tests/plotting/test_hist_method.py:248: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared
    _check_plot_works(df.hist)

pandas/tests/plotting/test_misc.py::TestSeriesPlots::()::test_autocorrelation_plot
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/pyplot.py:959: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection.
    return gcf().gca(**kwargs)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/pyplot.py:959: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection.
    return gcf().gca(**kwargs)

pandas/tests/plotting/test_misc.py::TestDataFramePlots::()::test_andrews_curves
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/pyplot.py:959: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection.
    return gcf().gca(**kwargs)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/pyplot.py:959: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection.
    return gcf().gca(**kwargs)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/pyplot.py:959: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection.
    return gcf().gca(**kwargs)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/pyplot.py:959: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection.
    return gcf().gca(**kwargs)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/pyplot.py:959: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection.
    return gcf().gca(**kwargs)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/pyplot.py:959: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection.
    return gcf().gca(**kwargs)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/pyplot.py:959: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection.
    return gcf().gca(**kwargs)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/pyplot.py:959: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection.
    return gcf().gca(**kwargs)

pandas/tests/plotting/test_misc.py::TestDataFramePlots::()::test_parallel_coordinates_with_sorted_labels
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/axes/_base.py:2918: UserWarning: Attempting to set identical left==right results
  in singular transformations; automatically expanding.
  left=0, right=0
    'left=%s, right=%s') % (left, right))

pandas/tests/plotting/test_misc.py::TestDataFramePlots::()::test_radviz
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/pyplot.py:959: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection.
    return gcf().gca(**kwargs)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/pyplot.py:959: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection.
    return gcf().gca(**kwargs)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/pyplot.py:959: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection.
    return gcf().gca(**kwargs)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/pyplot.py:959: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection.
    return gcf().gca(**kwargs)

pandas/tests/plotting/test_series.py::TestSeriesPlots::()::test_hist_legacy
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
    warnings.warn(message, mplDeprecation, stacklevel=1)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
    warnings.warn(message, mplDeprecation, stacklevel=1)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
    warnings.warn(message, mplDeprecation, stacklevel=1)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
    warnings.warn(message, mplDeprecation, stacklevel=1)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
    warnings.warn(message, mplDeprecation, stacklevel=1)
  /Users/taugspurger/Envs/pandas-dev/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
    warnings.warn(message, mplDeprecation, stacklevel=1)
```"
766949871,38472,BUG: disallow scalar in Categorical constructor,jbrockmendel,closed,2020-12-14T21:35:02Z,2020-12-23T19:18:59Z,"- [x] closes #38433
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
393459294,24381,Datetimelike index / arrays ignore integer sign and size in astype,TomAugspurger,closed,2018-12-21T13:29:03Z,2020-12-23T19:24:36Z,"For datetimelike indexes, `.astype` ignore the sign and size of an integer dtype, and always uses int64

```python
In [1]: import pandas as pd

In [2]: idx = pd.date_range('2000', periods=4)

In [3]: idx.astype('uint16')
Out[3]:
Int64Index([946684800000000000, 946771200000000000, 946857600000000000,
            946944000000000000],
           dtype='int64')
```

We don't have Index subclasses for each of these types (though we do have uint64 now).

In #24024 we agreed(?) that following the Index was the best behavior for the array class.

```python
In [7]: idx._data.astype('uint16')
Out[7]:
array([946684800000000000, 946771200000000000, 946857600000000000,
       946944000000000000])
```

(that's also int64).

---

Some questions:

1. Do we want to change this? Note that for ""normal"" usage of datetimes, this is *going* to truncate / overflow. This is particularly unfriendly to 32-bit systems, where ""int"" is ""int32"".

```python
In [9]: idx._data.astype('int32').astype(""int32"")
Out[9]: array([1380122624, -476971008, 1960902656,  103809024], dtype=int32)
```

So maybe we would want safe casting?

```pytb
In [10]: idx._data.astype('int32').astype(""int32"", casting=""safe"")
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-10-2c2a4a677a5c> in <module>
----> 1 idx._data.astype('int32').astype(""int32"", casting=""safe"")

TypeError: Cannot cast array from dtype('int64') to dtype('int32') according to the rule 'safe'
```

2. ExtensionIndex classes are in the not too distant future, so we may be able to relatively easily get Index classes that work with these different integer sizes / signs. So we might be able to properly support this in the future (if we want to)."
773924993,38659,TST: fix rest of mpl warnings,mzeitlin11,closed,2020-12-23T16:42:43Z,2020-12-23T19:26:05Z,"- [x] closes #18306
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
773936426,38660,Backport PR #38456 on branch 1.2.x (DOC: update wording about when xlrd engine can be used),meeseeksmachine,closed,2020-12-23T17:04:38Z,2020-12-23T19:29:34Z,Backport PR #38456: DOC: update wording about when xlrd engine can be used
773960734,38661,CLN: numba.prange usage,mroeschke,closed,2020-12-23T17:54:43Z,2020-12-23T20:05:58Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref https://github.com/pandas-dev/pandas/pull/38417#discussion_r547925255"
773500815,38653,"BUG: MultiIndex, IntervalIndex intersection with Categorical",jbrockmendel,closed,2020-12-23T05:38:11Z,2020-12-23T20:16:29Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
771306423,38575,BUG: construction from dt64/td64 values with td64/dt64 dtype,jbrockmendel,closed,2020-12-19T04:54:05Z,2020-12-23T20:38:53Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
774023354,38666,CLN: Rolling _prep_values,mroeschke,closed,2020-12-23T20:20:46Z,2020-12-23T22:36:25Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
771774439,38600,DEPS: unpin sphinx from doc build,jreback,closed,2020-12-21T01:43:20Z,2020-12-23T22:37:05Z,once warnings are fixed in updated sphinx can unpin #38598
763875560,38424,shift default excel read engine from xlrd to openpyxl,leo4183,closed,2020-12-12T14:14:14Z,2020-12-23T23:01:22Z,"since v2.0.0, xlrd no longer supports excel files other than "".xls"".  manually specify pd.read_excel engine (to openpyxl and etc) is a bit annoying (otherwise pandas would complain about the missing xlrd module).  is it possible to shift the default engine to sth more commonly used (eg. openpyxl )"
226562511,16253,[pandas.tools.plotting._subplots] squeeze option is not respected if `ax` is provided,Bolayniuss,closed,2017-05-05T12:48:04Z,2020-12-23T23:29:48Z,"`pandas.tools.plotting._subplots` returns a flatten array for the axes if `ax` parameter is provided regardless the value of the parameter `squeeze`. If `ax` is a 2d list, `ax` is not flattened but then the test `len(ax) == naxes` fails. The test should be `ax.size == naxes`.

This issue forbid the use of the `ax` parameter in plotting function like `scatter_matrix` which expect `ax` to be a NxN array. 

```python
import pandas as pd
from pandas.tools.plotting import scatter_matrix
import matplotlib.pyplot as plt

df = pd.DataFrame(dict(a=[0, 1, 2, 3, 4, 5], b=5, 6, 7, 8, 9])

f, axes = plt.subplots(2, 2)
scatter_matrix(df, ax=axes)
```
 
```error
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-10-8860714df14f> in <module>()
----> 1 scatter_matrix(df, ax=axes)

/Users/bolay/.envs/scantrust_tools/lib/python2.7/site-packages/pandas/tools/plotting.pyc in scatter_matrix(frame, alpha, figsize, ax, grid, diagonal, marker, density_kwds, hist_kwds, range_padding, **kwds)
    371     for i, a in zip(lrange(n), df.columns):
    372         for j, b in zip(lrange(n), df.columns):
--> 373             ax = axes[i, j]
    374 
    375             if i == j:

IndexError: too many indices for array
```

and if axes is a 2d list

```python
scatter_matrix(df, ax=axes.tolist())
```

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-11-aeac24929f47> in <module>()
----> 1 scatter_matrix(df, ax=axes.tolist())

/Users/bolay/.envs/scantrust_tools/lib/python2.7/site-packages/pandas/tools/plotting.pyc in scatter_matrix(frame, alpha, figsize, ax, grid, diagonal, marker, density_kwds, hist_kwds, range_padding, **kwds)
    347     naxes = n * n
    348     fig, axes = _subplots(naxes=naxes, figsize=figsize, ax=ax,
--> 349                           squeeze=False)
    350 
    351     # no gaps between subplots

/Users/bolay/.envs/scantrust_tools/lib/python2.7/site-packages/pandas/tools/plotting.pyc in _subplots(naxes, sharex, sharey, squeeze, subplot_kw, ax, layout, layout_type, **fig_kw)
   3389             else:
   3390                 raise ValueError(""The number of passed axes must be {0}, the ""
-> 3391                                  ""same as the output plot"".format(naxes))
   3392 
   3393         fig = ax.get_figure()

ValueError: The number of passed axes must be 4, the same as the output plot
```"
773992705,38662,REF: collect dt64<->dt64tz astype in dtypes.cast,jbrockmendel,closed,2020-12-23T19:07:44Z,2020-12-23T23:49:52Z,From here its straightforward to put the rest of Block._astype directly in astype_nansafe
733812865,37546,ENH: Add dtype argument to read_sql_query (GH10285),avinashpancham,closed,2020-10-31T21:44:46Z,2020-12-23T23:50:52Z,"- [x] closes #10285 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
774045646,38668,BUG: `scatter_matrix` raising with 2d axis passed,mzeitlin11,closed,2020-12-23T21:12:48Z,2020-12-23T23:59:26Z,"- [x] closes #16253
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
774076146,38669,Backport PR #38657 on branch 1.2.x (DOC/CI: fix class alias docstrings for sphinx 3.4 + unpin sphinx),meeseeksmachine,closed,2020-12-23T22:37:19Z,2020-12-24T01:11:25Z,Backport PR #38657: DOC/CI: fix class alias docstrings for sphinx 3.4 + unpin sphinx
774084551,38670,Backport PR #38571 on branch 1.2.x (DEPR: Adjust read excel behavior for xlrd >= 2.0),meeseeksmachine,closed,2020-12-23T23:02:09Z,2020-12-24T01:11:40Z,Backport PR #38571: DEPR: Adjust read excel behavior for xlrd >= 2.0
771240161,38571,DEPR: Adjust read excel behavior for xlrd >= 2.0,rhshadrach,closed,2020-12-18T23:42:20Z,2020-12-24T09:04:59Z,"- [x] closes #38424
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Alternative to #38522. I've been testing this locally using both xlrd 1.2.0 and 2.0.1.

One test fails because we used to default to xlrd but now default to openpyxl, it's not clear to me if this test should be passing with openpyxl.

cc @cjw296, @jreback, @jorisvandenbossche 

"
773697944,38657,DOC/CI: fix class alias docstrings for sphinx 3.4 + unpin sphinx,jorisvandenbossche,closed,2020-12-23T11:49:38Z,2020-12-24T09:07:29Z,"Closes #38600
"
32224271,6963,BUG: concat on axis with both different and duplicate labels raising error,jorisvandenbossche,closed,2014-04-25T11:30:38Z,2020-12-24T15:08:03Z,"When concatting two dataframes where there are a) there are duplicate columns in one of the dataframes, and b) there are non-overlapping column names in both, then you get a IndexError:

```
In [9]: df1 = pd.DataFrame(np.random.randn(3,3), columns=['A', 'A', 'B1'])
   ...: df2 = pd.DataFrame(np.random.randn(3,3), columns=['A', 'A', 'B2'])

In [10]: pd.concat([df1, df2])

Traceback (most recent call last):
  File ""<ipython-input-10-f61a1ab4009e>"", line 1, in <module>
    pd.concat([df1, df2])
...
  File ""c:\users\vdbosscj\scipy\pandas-joris\pandas\core\index.py"", line 765, in take
    taken = self.view(np.ndarray).take(indexer)
IndexError: index 3 is out of bounds for axis 0 with size 3
```

I don't know if it should work (although I suppose it should, as with only the duplicate columns it does work), but at least the error message is not really helpfull. 
"
699450353,36290,[BUG]: Fix ValueError in concat() when at least one Index has duplicates,phofl,closed,2020-09-11T15:27:36Z,2020-12-24T15:09:44Z,"- [x] closes #36263
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

If obj_labes Index has duplicates and they are not removed from new_labels before redindexing, they are multiplied. So we would get a way too big index."
774482379,38682,Backport PR #38654 on branch 1.2.x ([BUG] Concat duplicates errors (or lack there of)),meeseeksmachine,closed,2020-12-24T15:08:49Z,2020-12-24T16:24:26Z,Backport PR #38654: [BUG] Concat duplicates errors (or lack there of)
774149078,38673,TST: arithmetic test parametrization/cleanup,jbrockmendel,closed,2020-12-24T03:06:05Z,2020-12-24T17:08:06Z,
772146288,38609,"TST: GH30999 Add match=msg to all ""with pytest.raises"" in pandas/tests/io/pytables/test_store.py",moink,closed,2020-12-21T12:52:25Z,2020-12-24T17:14:23Z,"This pull request xref #30999 to remove bare `pytest.raises`. It doesn't close that issue as I have only addressed one file: `pandas/tests/io/pytables/test_store.py`. In that file there were 80 instances of bare `pytest.raises`.

`test_append_to_multiple_dropna_false` is marked as failing because it isn't raising the `ValueError` that it should. Because of this, I didn't know what the message should be, and reading through the code didn't help me figure it out. I wrote a draft error message and added a `TODO` (line 3790) and I hope that's the best way to handle it.

In `test_multiple_open_close` there were several assertions that `ClosedFileError` is raised with the same error message, with one in the middle that asserts that an `AttributeError` is raised with a different error message. I moved the one for the `AttributeError` to the end of the list (line 4229) to organize the assertions a little better and make clear that the `msg` parameter is the same for all the `ClosedFileError`s.

I did not add a whatsnew entry since it only changes the tests. Let me know if I should add one (and I am a bit unclear on how, i.e. what version this would end up in).

- [ ] xref #30999
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
771554327,38590,test case for issue #38267,ftrihardjo,closed,2020-12-20T10:01:09Z,2020-12-24T17:42:44Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
774169432,38676,REF: share Index.difference,jbrockmendel,closed,2020-12-24T04:16:44Z,2020-12-24T17:50:15Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

"
774182553,38677,CLN: simplify Block.putmask,jbrockmendel,closed,2020-12-24T05:01:01Z,2020-12-24T17:50:39Z,
774149388,38674,REF: dispatch TDBlock._can_hold_element to TimedeltaArray._validate_setitem_value,jbrockmendel,closed,2020-12-24T03:07:09Z,2020-12-24T17:59:07Z,
771386086,38582,BUG: MultiIndex.dtypes to handle when no level names,gurukiran07,closed,2020-12-19T14:00:12Z,2020-12-24T18:37:22Z,"- [X] closes #38580 
- [X] tests added / passed
- [X] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
648594446,35076,BUG: Integer Capacity Higher Than Necessary in Mappings from Pandas to SQLAlchemy Types ,wolfc86,closed,2020-07-01T00:59:55Z,2020-12-24T18:51:42Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

#### Problem description

Currently, to map the Pandas data type to a SQLAlchemy type: https://github.com/pandas-dev/pandas/blob/master/pandas/io/sql.py#L1066-L1069, the code reads:

```python
elif col_type == ""integer"":
  if col.dtype == ""int32"":
    return Integer
  else:
    return BigInteger
```

This means integers of a capacity less than 32-bit are written to the database as if they were 64-bit. An example where one might get smaller integers is through the use of [pd.to_numeric](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html) and `downcast=True`.

Since there is already a check for the `col_type == ""integer""`, I think switching the `col.dtype` check to this could be a possible fix:

```python
elif col_type == ""integer"":
  if col.dtype == ""int64"":
    return BigInteger
  else:
    return Integer
```

But I'm not sure how to get started with an official PR or if this is a sane thing to do.

For context (this part is not an issue for Pandas, just explaining my interest in this issue), I discovered this when using the [df.to_sql](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html) method to persist datasets to a Postgres database. From there, I use [Postgraphile](https://www.graphile.org/postgraphile/) to auto-generate a GraphQL endpoint. I found that the `BigInteger` type ends up resolving as a string, because Javascript can't represent such large numbers [safely](https://github.com/brianc/node-pg-types/issues/28). This would be fine if the source data warranted the high-capacity, but for me it often doesn't.

#### Expected Output

That the column types that get written to a new table in the database more tightly match their data types in Pandas.

#### Workarounds

If using [df.to_sql](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html), one could set the `dtype` argument but this can be unruly when there are many columns in the DataFrame.

For now, I have this helper method to downcast integer columns to their lowest capacity, but then back to 32-bit to persist them to the database with the desired column type:

```python
def cast_to_int32(df):
    # Downcast to the lowest possible representation.
    for col in df.select_dtypes(include=['int']).columns.values:
        df[col] = pd.to_numeric(df[col], downcast='integer')
    # Upcast back to 32-bit (since that's what gets persisted correctly)
    for col in df.select_dtypes(include=['int8', 'int16']).columns.values:
        df[col] = df[col].astype('int32')
    return df
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-39-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 44.0.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.17
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
770351625,38548,ENH: Map pandas integer to optimal SQLAlchemy integer type (GH35076),avinashpancham,closed,2020-12-17T20:38:54Z,2020-12-24T18:51:51Z,"- [x] closes #35076
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
774526512,38685,post-merge fixup,jbrockmendel,closed,2020-12-24T17:57:07Z,2020-12-24T19:05:46Z,#38674 combined with #38677 lead to a NameError this fixes
774500037,38684,Add match=msg to pytest.raises or convert to external_error_raised for modules in tests/plotting,moink,closed,2020-12-24T16:07:43Z,2020-12-24T19:10:37Z,"This pull request xref #30999 to remove bare `pytest.raises`. It doesn't close that issue as I have only addressed files within `pandas/tests/plotting/`.

Things I did that were not simple adding `match = msg` to `pytest.raises` or converting them to `tm.external_error_raised`:

- In `pandas/plotting/_matplotlib/_core.py` I found two error messages somewhat unclear. I changed the error message in the `ValueError` raised by `_get_stacked_values` to be, imo, a little bit clearer. In addition, in the constructor for `PiePlot`, the previous error message for the `ValueError` was ""None doesn't allow negative values"" which I changed to ""pie plot doesn't allow negative values"" because I believe that was the original intent.
- In `test_frame.py`, in the `test_errorbar_plot` method, the `pytest.raises` was allowing matplotlib to raise either a `TypeError` or a `ValueError`. I changed it to an `external_error_raised` but also specified it to be a `TypeError` because that is what it is, and I didn't see the reason it was allowed to be either error.
- Also in `test_frame.py`, the test `test_invalid_xy_args` was pytest-parameterized to test two different kinds of errors: x not being a label or position, or the label being bad. I split it into two tests, calling the new test `test_bad_label`
- Also in `test_frame.py`, the test `test_partially_invalid_plot_data` looped over a list with one element in it. I guessed that this was a remnant of an older test strategy that is no longer employed. I removed the list and made the kind of plot being tested more explicit.
- In a couple of test modules I moved a few lines out of the context of the `pytest.raises` because they were setup lines that didn't raise the error: I wanted to make it clear exactly which pandas method was raising the error.

I will note that another developer, [DylanBrdt](https://github.com/DylanBrdt) commented on the issue that they would address these files but they haven't been active on github since February.

- [ ] xref #30999
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
562119995,31817,ENH: add arrow engine to read_csv,lithomas1,closed,2020-02-09T04:27:30Z,2020-12-24T19:55:35Z,"- [x] closes #23697 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

ASV's, 100,000 rows, 5 columns, for reading from BytesIO & StringIO buffers (running on 2 core machine).
```
[ 75.00%] ··· io.csv.ReadCSVEngine.time_read_bytescsv                                                                                                                                                                               ok
[ 75.00%] ··· ========= ============
                engine
              --------- ------------
                  c       51.7±6ms
                python    598±40ms
               pyarrow   27.8±0.8ms
              ========= ============

[100.00%] ··· io.csv.ReadCSVEngine.time_read_stringcsv                                                                                                                                                                              ok
[100.00%] ··· ========= ===========
                engine
              --------- -----------
                  c      53.8±20ms
                python    584±30ms
               pyarrow    29.9±1ms
              ========= ===========
```
"
774487284,38683,REF: simplify coerce_to_target_dtype,jbrockmendel,closed,2020-12-24T15:24:26Z,2020-12-24T20:35:28Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
774537889,38686,CLN: algos.searchsorted,jbrockmendel,closed,2020-12-24T18:56:58Z,2020-12-24T20:36:10Z,"
"
753335834,38183,BUG: assert_frame_equal can be very slow,ivirshup,closed,2020-11-30T10:03:11Z,2020-12-24T20:40:41Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

Comparisons which can take less than a second, take 50 seconds when they are combined:

```python
import pandas as pd
import numpy as np
from string import ascii_letters

idx = np.random.choice(np.array(list(ascii_letters)), size=10000).astype(object)
a = np.random.random_integers(0, 100, (10000, 1000))

# This is fine
%time pd.testing.assert_frame_equal(pd.DataFrame(a), pd.DataFrame(a.copy()))
# CPU times: user 409 ms, sys: 20.8 ms, total: 429 ms
# Wall time: 429 ms

# As is this
%time pd.testing.assert_index_equal(pd.Index(idx), pd.Index(idx.copy()))
# CPU times: user 1.06 ms, sys: 26 µs, total: 1.08 ms
# Wall time: 1.08 ms

# This is weird:
%time pd.testing.assert_frame_equal(pd.DataFrame(a, index=idx), pd.DataFrame(b, index=idx))
# CPU times: user 50.5 s, sys: 62.6 ms, total: 50.6 s
# Wall time: 50.6 s
```

#### Problem description

It seems weird that the whole process slows down when the index has strings in it. It's especially weird since comparing the index itself is fairly fast. This may be a regression, since I've never noticed this before, and this seem very noticeable.

This might have something to do with  #38091

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Oct 29 22:56:45 PDT 2020; root:xnu-6153.141.2.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.4
numpy            : 1.19.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.2
Cython           : 0.29.21
pytest           : 6.1.2
hypothesis       : None
sphinx           : 3.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.4
fastparquet      : 0.4.1
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 2.0.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.4
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : None
numba            : 0.51.2
```


</details>
"
753995081,38202,PERF: fix assert_frame_equal can be very slow,ivanovmg,closed,2020-12-01T04:23:13Z,2020-12-24T20:40:45Z,"- [ ] closes #38183
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Add kwarg ``check_index`` into ``assert_series_equal`` to allow elimination of multiple index checking (for each column) in ``assert_frame_equal``.

Performance wise.

```
import pandas as pd
import numpy as np
from string import ascii_letters

idx = np.random.choice(np.array(list(ascii_letters)), size=10000).astype(object)
a = np.random.random_integers(0, 100, (10000, 1000))

%time pd.testing.assert_frame_equal(pd.DataFrame(a), pd.DataFrame(a.copy()))
Wall time: 319 ms

%time pd.testing.assert_index_equal(pd.Index(idx), pd.Index(idx.copy()))
Wall time: 993 µs

%time pd.testing.assert_frame_equal(pd.DataFrame(a, index=idx), pd.DataFrame(a, index=idx))
Wall time: 281 ms
```"
774100806,38671,REF: share Index.union,jbrockmendel,closed,2020-12-24T00:01:43Z,2020-12-24T22:27:58Z,
774542026,38687,Split up pandas/tests/io/pytables/test_store.py into smaller modules,moink,closed,2020-12-24T19:20:20Z,2020-12-25T04:56:22Z,"pandas/tests/io/pytables/test_store.py is more than 5000 lines long, which is pretty long for a test module. It would be good to break it up into smaller, logical pieces, each in its own test module.

Came out of PR 38609.

OT: @moink love to split this file up: pandas/tests/io/pytables/test_store.py in a separate PR :->

_Originally posted by @jreback in https://github.com/pandas-dev/pandas/issues/38609#issuecomment-749256512_"
217761927,15832,API/BUG: Handling Dtype Coercions in Series/Index,gfyoung,closed,2017-03-29T04:30:25Z,2020-12-25T14:40:44Z,"Off `master` (<a href=""https://github.com/pandas-dev/pandas/commit/bd169dc0a91f50031f6c2240075ff84d6b296576"">bd169d</a>):
~~~python
# Case I: Overflow on int64
>>> Index([np.iinfo(np.uint64).max-1],dtype='int64')
...
OverflowError: Python int too large to convert to C long

# Case II: Coercion to uint64
>>> Index([-1], dtype='uint64')
UInt64Index([18446744073709551615], dtype='uint64')

# Case III: Ignoring coercion to int
>>> Index([1, 2, 3.5], dtype=int)
Float64Index([1.0, 2.0, 3.5], dtype='float64')
~~~

So we got some coercions that fail but others that work.  Although all of these issues involve `Index`, the first two are also applicable to `Series` (in the last issue, it does coerce all elements to integer).

How should we handle failed coercions?  Is the second case even a failure?  Should iron out this.

xref #12758
xref #15187"
753330794,38182,BUG/DEPR: Fix unwanted warnings from the Series(Index[datetime objects]) deprecation,jorisvandenbossche,closed,2020-11-30T09:56:54Z,2020-12-25T23:16:52Z,"See https://github.com/pandas-dev/pandas/pull/36697#discussion_r497786546 and https://github.com/pandas-dev/pandas/pull/37193#issuecomment-715228272

Due to the `Index.is_all_dates` + `Series(Index[datetime object])` deprecation warnings, there are several code examples that are not using this attribute or constructor method directly, that still cause this warning (from internal pandas code, without direct control over this by the user). 

Users should not see unrelated deprecation warnings from the internals of pandas that they can't do anything about themselves, so IMO we should either fix those for 1.2, or delay the deprecationg for 1.3"
774800773,38700,CLN: remove unnecessary casting from Block methods,jbrockmendel,closed,2020-12-26T00:16:36Z,2020-12-26T03:13:29Z,
774363009,38679,"Revert ""Deprecated casting an object-dtype index of datetime objects to DatetimeIndex in the Series constructor (23598)""",simonjayhawkins,closed,2020-12-24T12:16:19Z,2020-12-26T10:03:57Z,"This **partially** reverts commit 90a61350b1be7a200238212bd8d118a25a4d3d95.

- [ ] closes #38182
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
774795812,38699,"Backport PR #38679 on branch 1.2.x (Revert ""Deprecated casting an object-dtype index of datetime objects to DatetimeIndex in the Series constructor (23598)"")",meeseeksmachine,closed,2020-12-25T23:18:02Z,2020-12-26T10:53:42Z,"Backport PR #38679: Revert ""Deprecated casting an object-dtype index of datetime objects to DatetimeIndex in the Series constructor (23598)"""
774343725,38678,DOC: 1.2.0 release date,simonjayhawkins,closed,2020-12-24T11:31:03Z,2020-12-26T11:25:56Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
774860766,38702,Backport PR #38678 on branch 1.2.x (DOC: 1.2.0 release date),meeseeksmachine,closed,2020-12-26T11:26:21Z,2020-12-26T12:27:25Z,Backport PR #38678: DOC: 1.2.0 release date
774892260,38704,DOC: Start v1.2.1 release notes,simonjayhawkins,closed,2020-12-26T15:27:24Z,2020-12-26T17:23:10Z,
774906864,38706,Backport PR #38704 on branch 1.2.x (DOC: Start v1.2.1 release notes),meeseeksmachine,closed,2020-12-26T17:23:22Z,2020-12-26T18:24:25Z,Backport PR #38704: DOC: Start v1.2.1 release notes
775016381,38716,Backport PR #38715 on branch 1.2.x (CI: pin jedi version<0.18.0),meeseeksmachine,closed,2020-12-27T10:53:32Z,2020-12-27T12:55:06Z,Backport PR #38715: CI: pin jedi version<0.18.0
774974910,38713,DOC: fix link redirect to NumFOCUS,partev,closed,2020-12-27T04:37:16Z,2020-12-27T15:37:08Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
772319827,38617,CI: move arm64 build off travis,fangchenli,closed,2020-12-21T16:59:36Z,2020-12-27T15:54:31Z,
774995947,38715,CI: pin jedi version<0.18.0,arw2019,closed,2020-12-27T08:05:17Z,2020-12-27T17:55:01Z,"... so CI jobs pass.

xref #38703. OP should be kept open until upstream fix comes through."
775082897,38725,pandas.DataFrame.plot arguments 'xlabel' & 'ylabel' not working.,dave-espinosa,closed,2020-12-27T18:03:23Z,2020-12-27T18:29:01Z,"Hello everyone.

I was rehearsing some exercises with pandas.DataFrame.plot. I stumbled upon one (apparent) issue regarding '_xlabel_' and '_ylabel_' arguments. I followed instructions provided at [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html](this site), as that is my current working Pandas version. If my understanding is correct, those arguments are to display some _str_ inserted by user, at _xlabel_ & _ylabel_ respectively, right? What the documentation says is the following:

- xlabel: Name to use for the xlabel on x-axis --> ERROR SHOWN: 'Line2D' object has no property 'xlabel'
- ylabel: Name to use for the ylabel on Y-axis --> ERROR SHOWN: 'Line2D' object has no property 'yabel'

Currently, I am using the following lines of code AFTER df.plot() has been used, to obtain the desired result; however I'd love to know if the previously mentioned behaviour is an actual issue, or some missunderstanding on the current documentation:

`plt.ylabel(ylabel=my_ylabel, fontsize=tz) 
plt.xlabel(xlabel=my_xlabel, fontsize=tz)`

Thank you in advance."
775096744,38730,BUG: casting column in place doesn't change data type,tlatorre-uchicago,closed,2020-12-27T19:44:03Z,2020-12-27T19:50:29Z,"- [x] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
>>> a = pd.DataFrame({'a': [b'blah']})
>>> a.dtypes
a    object
dtype: object
>>>  a.a = a.a.astype(np.dtype(""S8""))
>>> a.dtypes
a    object
dtype: object
>>> a['b'] = a.a.astype(np.dtype(""S8""))
>>> a.dtypes
a    object
b       |S8
dtype: object
```

#### Problem description

Naively it seems like this should behave similarly to other data types. Trying the same example and casting int64 -> int32 works.

#### Expected Output

```python
>>> a = pd.DataFrame({'a': [b'blah']})
>>> a.dtypes
a    object
dtype: object
>>>  a.a = a.a.astype(np.dtype(""S8""))
>>> a.dtypes
a       |S8
dtype: object
```

#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.6.6-300.fc32.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.18.4
pytz             : 2020.4
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.6.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.12.0
pandas_datareader: 0.8.0
bs4              : None
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.5.2
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.1.2
xlsxwriter       : None
"
774718835,38695,TST: GH30999 Add match=msg to all pytest.raises in tests/generic/methods/test_sample.py,moink,closed,2020-12-25T11:56:33Z,2020-12-27T22:11:51Z,"This pull request partially addresses xref #30999  to remove bare `pytest.raises` by adding `match=msg`. It doesn't close that issue as I have only addressed one test module: pandas/tests/generic/methods/test_sample.py

The only other thing I did was move a few lines out of the `pytest.raises` context as they were setup lines that did not raise the error, in order to make it clear which pandas method was raising the error.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
320849048,20975,"Slicing columns with mixed types <str>,<int> fails with ValueError",achampion,closed,2018-05-07T15:28:50Z,2020-12-27T22:55:00Z,"#### Code Sample, a copy-pastable example if possible

```python
In []:
df = pd.DataFrame({'test':1, 1:2, 2:3}, index=[0])
df.loc[:, 'test':]
Out[]:
   test  1  2
0     1  2  3

In []:
df.loc[:, 1:]
Out[]:
TypeError                                 Traceback (most recent call last)
<ipython-input-655-3b97c047dcab> in <module>()
      1 df = pd.DataFrame({'test':1, 1:2, 2:3}, index=[0])
----> 2 df.loc[:, 1:]
[snip]
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexes/base.py in _invalid_indexer(self, form, key)
   1574                         ""indexers [{key}] of {kind}"".format(
   1575                             form=form, klass=type(self), key=key,
-> 1576                             kind=type(key)))
   1577 
   1578     def get_duplicates(self):

TypeError: cannot do slice indexing on <class 'pandas.core.indexes.base.Index'> with these indexers [1] of <class 'int'>

In []:
df = pd.DataFrame({0:1, 1:2, 2:3}, index=[0])
df.loc[:, 1:]
Out[]:
   1  2
0  2  3
```
#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]
When having a columns with mixed types, e.g. `<str>, <int>` any slicing using the `int` columns causes a `ValueError`.

When you have just `int` columns slicing by `int` does not cause an issue.

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.1.final.0
python-bits: 64
OS: Darwin
OS-release: 17.5.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.22.0
pytest: 3.2.2
pip: 10.0.1
setuptools: 39.0.1
Cython: 0.28.1
numpy: 1.14.2
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: 1.7.2
patsy: None
dateutil: 2.6.1
pytz: 2018.4
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.2.2
openpyxl: None
xlrd: None
xlwt: 1.3.0
xlsxwriter: None
lxml: None
bs4: 4.6.0
html5lib: 0.9999999
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.6.0
</details>
"
775018980,38717,"Slicing columns with mixed types <str>,<int> fails with ValueError #20975",hungyiwu,closed,2020-12-27T11:12:59Z,2020-12-27T22:55:04Z,"- [ ] closes #20975 
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
774895624,38705,TST: Add match=msg to all pytest.raises in pandas/tests/io/json,moink,closed,2020-12-26T15:53:44Z,2020-12-27T22:57:19Z,"This pull request partially addresses xref #30999 to remove bare `pytest.raises` by adding `match=msg`. It doesn't close that issue as I have only addressed test modules in tje pandas/tests/io/json directory.

I also moved a couple of lines out of the `pytest.raises` context to make clear which pandas method was raising the error. And I deleted a couple of unreachable asserts in tests.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
199709649,15095,API: Table-wise rolling / expanding / EWM function application,TomAugspurger,closed,2017-01-10T01:29:46Z,2020-12-27T23:02:31Z,"In https://github.com/pandas-dev/pandas/pull/11603#issuecomment-162113949 (the main PR implementing the deferred API for rolling / expanding / ewm), we discussed how to specify table-wise `apply`s. `Groupby.apply(f)` feeds the entire group (all columns) to `f`. For backwards-compatibility, `.rolling(n).apply(f)` needed to be column-wise.

https://github.com/pandas-dev/pandas/pull/11603#issuecomment-162116556 mentions a possible API like what I added for `.style`

- `axis=0`: apply to each column independently
- `axis=1`: apply to each row independently
- `axis=None`: apply the supplied function to the entire table

So it'd be `df.rolling(n).apply(f, axis=None)`.
Do people like the axis=0 / 1 / None idiom? Is it obvious enough?

This is prompted by @josef-pkt's [post on the mailinglist](https://groups.google.com/forum/#!topic/pydata/FcAT8LBPmlg). Needing a rolling OLS.

An example:

```python
In [2]: import numpy as np
   ...: import pandas as pd
   ...:
   ...: np.random.seed(0)
   ...: df = pd.DataFrame(np.random.randint(0, 10, size=(10, 2)), columns=[""A"", ""B""])
   ...: df
   ...:
Out[2]:
   A  B
0  5  0
1  3  3
2  7  9
3  3  5
4  2  4
5  7  6
6  8  8
7  1  6
8  7  7
9  8  1
```

For a concrete example, get the table-wise max (this is equivalent to `df.rolling(4).max().max(1)`)

```python
In [10]: df.rolling(4).apply(np.max, axis=None)
Out[10]:
0    NaN
1    NaN
2    NaN
3    9.0
4    9.0
5    9.0
6    8.0
7    8.0
8    8.0
9    8.0
dtype: float64
```

A real example is something like a rolling OLS:

```python
import statsmodels.api as sm
f = lambda x: sm.OLS.from_formula('A ~ B', data=x).fit()  # wrong, but w/e

df.rolling(5).apply(f, axis=None)
```"
762881767,38417,ENH: Add method argument to rolling constructor to allow table-wise rolling,mroeschke,closed,2020-12-11T20:44:59Z,2020-12-27T23:38:35Z,"- [x] closes #15095
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
724603123,37244,QST: Rolling mean triggers exception : Cannot cast array data from dtype('float64') to dtype('int32') [Works on macOS (64 bits?) not on Linux 32 bits]),aamalikcmu,closed,2020-10-19T13:19:31Z,2020-12-27T23:54:12Z,"- [ Check] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ Check] I have asked my usage related question on [StackOverflow](https://stackoverflow.com/questions/64373351/pandas-rolling-window-cannot-cast-array-data-from-dtypeint64-to-dtypeint).

---

#### Question about pandas

I am trying to calculate the average of a rolling window [of x number of seconds, in this case 300] using Panda's rolling window function. This works perfectly fine on my mac 
![image](https://user-images.githubusercontent.com/61522212/96454822-66d22680-11ea-11eb-936a-8fcd5db56ff6.png)

however the same code fails to work on a armv71 [32 bit system Linux 4.9.82-ti-r102  armv7l GNU/Linux]. here is a minimal reproducible example : 

```python

import pandas as pd
import times

df = pd.read_csv('example.csv', index_col=2)
df.index = pd.to_datetime(df.index)
df.index
try:
    # Interval is the step size
    interval = 300

    # input to rolling window
    window_val = str(interval) + 's'

    # Variable
    start = time.time()

    df['Variable_Average_' + str(interval)] = df \
        .groupby('Group_By_Number') \
        .rolling(window=window_val)['Variable'] \
        .mean() \
        .reset_index(drop=True) \
        .values

except:
    print(traceback.format_exc())
```

This produces the following error for me : 



```
Traceback (most recent call last):
  File ""Test2.py"", line 19, in <module>
    .rolling(window=window_val)['Variable'] \
  File ""/home/fiq/venv/lib/python3.7/site-packages/pandas/core/window/rolling.py"", line 2091, in mean
    return super().mean(*args, **kwargs)
  File ""/home/fiq/venv/lib/python3.7/site-packages/pandas/core/window/rolling.py"", line 1488, in mean
    return self._apply(window_func, center=self.center, name=""mean"", **kwargs)
  File ""/home/fiq/venv/lib/python3.7/site-packages/pandas/core/window/rolling.py"", line 2194, in _apply
    **kwargs,
  File ""/home/fiq/venv/lib/python3.7/site-packages/pandas/core/window/rolling.py"", line 588, in _apply
    result = calc(values)
  File ""/home/fiq/venv/lib/python3.7/site-packages/pandas/core/window/rolling.py"", line 574, in calc
    closed=self.closed,
  File ""/home/fiq/venv/lib/python3.7/site-packages/pandas/core/window/indexers.py"", line 301, in get_window_bounds
    index_array = self.index_array.take(indicies)
TypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'

```

On the line where I call the rolling mean function.  I have tried typecasting all the data to int32 before reaching the rolling stage and i still get the same error.

The data used for this example is as follows :

![image](https://user-images.githubusercontent.com/61522212/96456056-fdebae00-11eb-11eb-9076-0543aa173558.png)

with tmstamp being its index and all other columns of type str and int32 respectively. "
771455830,38586,TYP: Added cast to ABC EA types,rhshadrach,closed,2020-12-19T21:12:24Z,2020-12-27T23:59:27Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This is the last set of ABCs to cast. Going out with a whimper, mypy didn't identify any new issues."
771467588,38588,TST: Added tests for ABC classes,rhshadrach,closed,2020-12-19T22:28:46Z,2020-12-27T23:59:35Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

`test_abc_types` is a subset of the tests added and can be removed in a followup."
775113497,38734,Backport PR #38344 on branch 1.2.x (CI: Move sql builds from Travis to Github actions),meeseeksmachine,closed,2020-12-27T21:44:48Z,2020-12-28T00:09:58Z,Backport PR #38344: CI: Move sql builds from Travis to Github actions
698461537,36275,Fix #35484 - re-allow sqlalchemy declarative system to use `%` in read_sql.,machow,closed,2020-09-10T20:33:13Z,2020-12-28T00:38:39Z,"- [x] closes #35484
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

* Adds tests to cover sql cases combining parameters and `%`.
* Notably, these tests highlight two regressions in v1.1
  - `pd.read_sql(""SELECT 1 %% 2"", engine)` fails. See #35871.
  - `pd.read_sql(sqlalchemy.sql.text(""SELECT 1 % 2""), engine)` fails. See #35484.
* Fixes second issue by modifying SqlDatabase to not use `no_parameter`, and wrapping string queries in `sql.text()` when no parameters are passed.

I was going nuts trying to keep track of all the cases, so tossed them into a table! I didn't have time to explicitly check v1.05, so would be helpful if someone could verify that column (or I may be able to loop back to it)!

| query |   v1.05 | v.1.1 | PR | issue |
| --- | --- | --- | --- | --- |
| no params, `%` | ❌ | ✅ | ✅ | #34211 (merged) |
| no params, `%%` | ✅ | ❌ | ❌ | #35871 |
| params, `%` | ❌ | ❌ | ❌ |  https://github.com/psycopg/psycopg2/issues/827 |
| params, `%%` | ✅ | ✅ | ✅ | | 
| sqla declarative, `%` | ✅ | ❌ | ✅ | #35484 |"
774569964,38689,REF: make pd._testing a directory,jbrockmendel,closed,2020-12-24T22:02:10Z,2020-12-28T01:19:04Z,We've accumulated some other helpers outside of pd._testing that id like to move there (xref #38688).  This is a good time to split the large file.
773998009,38663,Develop pre check,cjlynch278,closed,2020-12-23T19:20:30Z,2020-12-28T03:49:56Z,"- [ x] closes #37957
- [ ] tests added / passed
- [ x passes `black pandas`
- [ x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
774927267,38707,TST: Remove broken test for deprecated functionality,theOehrly,closed,2020-12-26T20:10:28Z,2020-12-28T14:26:14Z,"Registering converters by default on import is deprecated since #28722.

The test did not fail as the subprocess call contains errors in the python code. Specifically, the code is in double and single quotes. This means that the python code passed to the subprocess call is never actually excecuted. Python in the subprocess is called to ""excecute"" a string. This is will simply finish with exit code 0 as there is nothing to do basically.
Additonally, the test code contains a stray closing bracket at the end which would make the code fail if it was excecuted. 

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
756404256,38267,BUG: Inconsistent behavior of `.replace()` in Int64 series with `<NA>`.,tongr,closed,2020-12-03T17:35:06Z,2020-12-28T14:55:46Z,"### Code Sample

This code does work as expected (0 value is replaced by `<NA>` in an Int64 series).
```python
>>> import pandas as pd
>>> s = pd.Series([0, 1]).astype(""Int64"")
>>> s.replace(0, pd.NA)
0    <NA>
1       1
dtype: Int64
```

In contrast, when having `<NA>` values in the series `.replace()` does not work as expected (0 is **not** replaced by `<NA>`).
```python
>>> import pandas as pd
>>> s = pd.Series([0, None]).astype(""Int64"")
>>> s.replace(0, pd.NA)
0       0
1    <NA>
dtype: object
```

However, the code does work if we add `.fillna()` before replacing:
```python
>>> import pandas as pd
>>> s = pd.Series([0, None]).astype(""Int64"")
>>> s.fillna(0).replace(0, pd.NA)
0    <NA>
1    <NA>
dtype: Int64
```

### Problem description
When applying the replace on a Int64 series with or without `<NA>` I expect consistent behavior.
Also the result type of `.replace()` seems to be inconsistent based on the usage of `.fillna()` on the series (without `fillna()` the type changes to `object`).

### Expected behavior
I would expect that `s.replace(0, pd.NA)` and `s.fillna(0).replace(0, pd.NA)` on an Int64 series will **always** lead to the same result.

### Output of pd.show_versions()
<details>
  <summary>Details</summary>

INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.8.0.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-53-generic
Version          : #59~18.04.1-Ubuntu SMP Wed Oct 21 12:14:56 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.4
numpy            : 1.19.2
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.3
setuptools       : 50.3.2.post20201201
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
</details>"
774682017,38693,TST: Inconsistent behavior of .replace() in Int64 series with NA ,ftrihardjo,closed,2020-12-25T08:47:35Z,2020-12-28T14:55:49Z,"- [ ] closes #38267
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
775258768,38748,"Revert ""TST: Bare pytest raises"" in base extension tests",jorisvandenbossche,closed,2020-12-28T07:46:04Z,2020-12-28T15:35:00Z,"Reverts pandas-dev/pandas#38576, see discussion there. We should not add detailed error message asserts, as those tests are also used by external packages, that might have different error messages."
774566949,38688,TST: implement tm.check_setitem_equivalents,jbrockmendel,closed,2020-12-24T21:49:55Z,2020-12-28T15:40:13Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
771821345,38605,REF: simplify CategoricalIndex.__new__,jbrockmendel,closed,2020-12-21T04:15:14Z,2020-12-28T16:04:23Z,Part of a larger goal of making `Index.__new__` share code with `Series.__init__` and `pd.array`
716241380,36936,BUG: loc with empty multiindex raises exception,egorvavilov,closed,2020-10-07T06:32:33Z,2020-12-28T16:41:47Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
Hi, last line of snippet throws an exception ``ValueError: operands could not be broadcast together with shapes (0,) (2,) (0,)``

```python
import pandas as pd

arrays = [['val1', 'val1', 'val2'], ['val1', 'val1', 'val2']]
index = pd.MultiIndex.from_arrays(arrays, names=('idx1', 'idx2'))

df = pd.DataFrame([1, 2, 3], index=index, columns=['value'])

df.loc[df[df['value'] == 0].index, :] = 4

```

#### Problem description

``.loc`` raises error in case dataframe has duplicates in index. ``df[df['value'] == 0].index`` produces ``MultiIndex([], names=['idx1', 'idx2'])`` and it seems to be absolutely valid to put it into ``.loc`` method.

#### Expected Output

I expect ``.loc`` operation to make no changes to original ``df`` cause multiindex in ``.loc`` is empty.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.17763
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.3
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 46.0.0
Cython           : None
pytest           : 6.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.3.0
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 7.14.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : 1.0.6
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : None

</details>
"
762732111,38415,BUG: MultiIndex.__repr__ is broken when `display.max_seq_items` = 1,galipremsagar,closed,2020-12-11T18:30:16Z,2020-12-28T18:38:47Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
In[11]: pd.set_option(""display.max_seq_items"", 2)
In[12]: pd.MultiIndex.from_tuples(tuples)
Out[12]: 
MultiIndex([(1,  'red'),
            ...
            (2, 'blue')],
           length=40)
In[13]: tuples = [(1, 'red'), (1, 'blue'), (2, 'red'), (2, 'blue')]
In[14]: pd.MultiIndex.from_tuples(tuples)
Out[14]: 
MultiIndex([(1,  'red'),
            ...
            (2, 'blue')],
           length=4)
In[15]: pd.set_option(""display.max_seq_items"", 1)
In[16]: pd.MultiIndex.from_tuples(tuples)
Out[16]: 
MultiIndex([...
            (1,  'red'),
            (1, 'blue'),
            (2,  'red'),
            (2, 'blue')],
           length=4)
```

#### Problem description

The output of `MultiIndex.__repr__` is dependent on `display.max_seq_items`, the output is correct for different values of `max_seq_items` except when it is set to 1. When it is set to 1, the entire records in MultiIndex are being printed and also `""...""` are being prepended to the data output.



#### Expected Output
```python
In[12]: pd.MultiIndex.from_tuples(tuples)
Out[12]: 
MultiIndex([(1,  'red'),
            ...],
           length=40)
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.7.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-53-generic
Version          : #59-Ubuntu SMP Wed Oct 21 09:38:44 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.1.5
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.3.1
setuptools       : 49.6.0.post20201009
Cython           : 0.29.21
pytest           : 6.1.2
hypothesis       : 5.43.3
sphinx           : 3.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.4
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.52.0

</details>
"
765204247,38443,BUG: Fix Index.__repr__ when `display.max_seq_items` = 1,skvrahul,closed,2020-12-13T09:10:20Z,2020-12-28T18:38:55Z,"- [x] closes #38415 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
775065664,38722,FutureWarning: _AXIS_NAMES has been deprecated.,JSunRae,closed,2020-12-27T16:17:35Z,2020-12-28T19:51:44Z,"At random intervals I keep getting the following error.

> \.vscode\extensions\ms-python.python-2020.12.424452561\pythonFiles\lib\python\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_resolver.py:193: FutureWarning: _AXIS_NUMBERS has been deprecated.
>   attr = getattr(var, name)
> \.vscode\extensions\ms-python.python-2020.12.424452561\pythonFiles\lib\python\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_resolver.py:193: FutureWarning: _AXIS_NAMES has been deprecated.
>   attr = getattr(var, name)

This seems to be refering to how I am using a dataframe in pandas. Though, due to the delays, I am not sure what specifically. I have found only one other refernce to this issue on the internet [https://intellij-support.jetbrains.com/hc/en-us/community/posts/360009512560-PyDev-FutureWarning-Django-console](url)
Saying they believe it occurs on `df.columns`
I'm running my project though a conda environment with:           

>        Version                   Build  Channel
> absl-py                   0.9.0                    pypi_0    pypi
> astor                     0.8.1                    pypi_0    pypi
> astunparse                1.6.3                    pypi_0    pypi
> beautifulsoup4            4.9.1                    py37_0
> brotlipy                  0.7.0           py37he774522_1000
> bzip2                     1.0.8                he774522_0
> ca-certificates           2020.7.22                     0
> cachetools                4.1.1                    pypi_0    pypi
> certifi                   2020.6.20                py37_0
> cffi                      1.14.3           py37h7a1dbc1_0
> chardet                   3.0.4                 py37_1003
> conda                     4.8.5                    py37_0
> conda-build               3.18.11                  py37_0
> conda-package-handling    1.6.1            py37h62dcd97_0
> console_shortcut          0.1.1                         4
> cryptography              3.1.1            py37h7a1dbc1_0
> filelock                  3.0.12                     py_0
> gast                      0.2.2                    pypi_0    pypi
> glob2                     0.7                        py_0
> google-auth               1.18.0                   pypi_0    pypi
> google-auth-oauthlib      0.4.1                    pypi_0    pypi
> google-pasta              0.2.0                    pypi_0    pypi
> grpcio                    1.30.0                   pypi_0    pypi
> h5py                      2.10.0                   pypi_0    pypi
> idna                      2.10                       py_0
> importlib-metadata        1.7.0                    pypi_0    pypi
> jinja2                    2.11.2                     py_0
> keras-applications        1.0.8                    pypi_0    pypi
> keras-preprocessing       1.1.2                    pypi_0    pypi
> libarchive                3.4.2                h5e25573_0
> libiconv                  1.15                 h1df5818_7
> liblief                   0.10.1               ha925a31_0
> libxml2                   2.9.10               h464c3ec_1
> lz4-c                     1.9.2                h62dcd97_1
> markdown                  3.2.2                    pypi_0    pypi
> markupsafe                1.1.1            py37hfa6e2cd_1
> menuinst                  1.4.16           py37he774522_0
> numpy                     1.19.0                   pypi_0    pypi
> oauthlib                  3.1.0                    pypi_0    pypi
> openssl                   1.1.1h               he774522_0
> opt-einsum                3.2.1                    pypi_0    pypi
> pip                       20.2.3                   py37_0
> pkginfo                   1.5.0.1                  py37_0
> powershell_shortcut       0.0.1                         3
> protobuf                  3.12.2                   pypi_0    pypi
> psutil                    5.7.2            py37he774522_0
> py-lief                   0.10.1           py37ha925a31_0
> pyasn1                    0.4.8                    pypi_0    pypi
> pyasn1-modules            0.2.8                    pypi_0    pypi
> pycosat                   0.6.3            py37he774522_0
> pycparser                 2.20                       py_2
> pyopenssl                 19.1.0                     py_1
> pysocks                   1.7.1                    py37_1
> python                    3.7.7                h81c818b_4
> python-libarchive-c       2.9                        py_0
> pytz                      2020.1                     py_0
> pywin32                   227              py37he774522_1
> pyyaml                    5.3.1            py37he774522_1
> requests                  2.24.0                     py_0
> requests-oauthlib         1.3.0                    pypi_0    pypi
> rsa                       4.6                      pypi_0    pypi
> ruamel_yaml               0.15.87          py37he774522_1
> scipy                     1.4.1                    pypi_0    pypi
> setuptools                49.6.0                   py37_1
> six                       1.15.0                     py_0
> soupsieve                 2.0.1                      py_0
> sqlite                    3.33.0               h2a8f88b_0
> style                     1.1.0                    pypi_0    pypi
> tensorboard               1.15.0                   pypi_0    pypi
> tensorboard-plugin-wit    1.7.0                    pypi_0    pypi
> tensorflow                2.2.0                    pypi_0    pypi
> tensorflow-estimator      1.15.1                   pypi_0    pypi
> tensorflow-gpu            1.15.0                   pypi_0    pypi
> termcolor                 1.1.0                    pypi_0    pypi
> tqdm                      4.49.0                     py_0
> update                    0.0.1                    pypi_0    pypi
> urllib3                   1.25.10                    py_0
> vc                        14.1                 h0510ff6_4
> vs2015_runtime            14.16.27012          hf0eaf9b_3
> werkzeug                  1.0.1                    pypi_0    pypi
> wheel                     0.35.1                     py_0
> win_inet_pton             1.1.0                    py37_0
> wincertstore              0.2                      py37_0
> wrapt                     1.12.1                   pypi_0    pypi
> xz                        5.2.5                h62dcd97_0
> yaml                      0.2.5                he774522_0
> zipp                      3.1.0                    pypi_0    pypi
> zlib                      1.2.11               h62dcd97_4
> zstd                      1.4.5                h04227a9_0

Anyone got any ideas?"
775162435,38740,DEPR: Hide deprecated attrs _AXIS_NAMES & _AXIS_NUMBERS,topper-123,closed,2020-12-28T02:57:52Z,2020-12-28T20:12:52Z,"- [x] closes #38722
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Users that do e.g. `[getattr(pd.DataFrame, x) for x in dir(pd.DataFrame())]` or `inspect.getmembers(pd.DataFrame())` currently get an unfriendly deprecation warning. This fixes that by adding `_AXIS_NAMES` & `_AXIS_NUMBERS` to `_hidden_attrs`.

@JSunRae, is it possible for you to add this to your pandas source code and see if this fixes your problem?

xref: #33637"
772259172,38613,CI: move py38 slow to azure #38429,fangchenli,closed,2020-12-21T15:38:23Z,2020-12-28T21:19:40Z,"xref #38429

We're not testing databases on py38 slow currently. Keep the issue open until we figure out what to do with its SQL tests.

"
775562031,38755,Backport PR #38740 on branch 1.2.x (DEPR: Hide deprecated attrs _AXIS_NAMES & _AXIS_NUMBERS),meeseeksmachine,closed,2020-12-28T19:53:07Z,2020-12-28T21:20:04Z,Backport PR #38740: DEPR: Hide deprecated attrs _AXIS_NAMES & _AXIS_NUMBERS
775583838,38758,CLN: simplify tm.decompress_file,twoertwein,closed,2020-12-28T20:56:22Z,2020-12-28T22:23:12Z,Could re-use `get_handle` but I'm not sure whether the testing code should depend on the to-be tested code?
775171214,38741,"BUG: IntervalIndex, PeriodIndex, DatetimeIndex symmetric_difference with Categorical",jbrockmendel,closed,2020-12-28T03:36:04Z,2020-12-28T23:19:58Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
775563881,38756,TST: use frame_or_series for some tz_localize/tz_convert tests,jbrockmendel,closed,2020-12-28T19:58:36Z,2020-12-28T23:27:16Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
775622346,38763,Backport PR #38761 on branch 1.2.x (CI: print build version),meeseeksmachine,closed,2020-12-28T23:12:44Z,2020-12-28T23:59:57Z,Backport PR #38761: CI: print build version
774988378,38714,REGR: to_csv problems with zip compression and large dataframes,chmielcode,closed,2020-12-27T06:53:59Z,2020-12-29T00:05:07Z,"#### Code Sample, a copy-pastable example

```python
import pandas as pd
import io
f = io.BytesIO()
d = pd.DataFrame({'a':[1]*5000})
d.to_csv(f, compression='zip')
f.seek(0)
pd.read_csv(f, compression='zip')
```

#### Problem description
Writing large (over 1163 rows) dataframes to csv with zip compression (inferred or explicit; to file or io.BytesIO) creates a corrupted zip file.
ValueError: Multiple files found in ZIP file. Only one file per ZIP: ['zip', 'zip', 'zip', 'zip', 'zip']

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : Polish_Poland.1250

pandas           : 1.2.0
numpy            : 1.19.3
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.1.0.post20201221
Cython           : None
pytest           : 6.2.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : 1.3.2
fsspec           : 0.8.5
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.4
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : 0.16.2
xlrd             : None
xlwt             : None
numba            : 0.52.0

</details>
"
775577582,38757,REF: implement nested_data_to_arrays,jbrockmendel,closed,2020-12-28T20:37:16Z,2020-12-29T00:05:42Z,"In the next step, I plan to use this in the relevant case for ndarray, which we currently treat differently from EA.

Categorical edits are unrelated, just sneaking in."
775098913,38731,DOC: reference the visualization page from the ecosystem one,afeld,closed,2020-12-27T20:00:44Z,2020-12-29T00:08:09Z,"The link already exists in the other direction, so want to make sure readers who come across the latter understand the built-in option.

- [ ] ~~closes #xxxx~~
- [ ] ~~tests added / passed~~
- [ ] ~~passes `black pandas`~~
- [ ] ~~passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`~~
- [ ] ~~whatsnew entry~~
"
775154832,38739,CLN: remove duplicate banklist.html file,afeld,closed,2020-12-28T02:20:14Z,2020-12-29T00:08:24Z,"Wasn't quite a duplicate — had a missing `<tr>`. Also updated the `.gitignore` to more accurately only exclude HTML files from `doc/source/_static/`, since other files are checked in.

- [ ] ~~closes #xxxx~~
- [ ] tests added / passed
- [ ] ~~passes `black pandas`~~
- [ ] ~~passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`~~
- [ ] ~~whatsnew entry~~
"
775593914,38761,CI: print build version,fangchenli,closed,2020-12-28T21:27:36Z,2020-12-29T01:57:33Z,"xref #38344 

followup
"
774959552,38711,BUG: loc with empty multiindex raises exception,kasim95,closed,2020-12-27T01:36:38Z,2020-12-29T02:16:54Z,"- [x] closes #36936 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
767031445,38480,CI: Supress moto server logs in tests,mroeschke,closed,2020-12-14T23:54:45Z,2020-12-29T02:56:46Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

https://github.com/pandas-dev/pandas/pull/38096 originally did not do the trick. This change worked for me locally."
775634447,38767,Backport PR #38728 on branch 1.2.x (REGR: to_csv created corrupt ZIP files when chunksize<rows),meeseeksmachine,closed,2020-12-29T00:05:21Z,2020-12-29T03:16:45Z,Backport PR #38728: REGR: to_csv created corrupt ZIP files when chunksize<rows
774935269,38708,"REGR: repr of stringified floats (e.g. ""3.50"") drop ending ""0""",topper-123,closed,2020-12-26T21:24:11Z,2020-12-29T03:18:49Z,"Example:

```python
>>> ser = pd.Series([""3.50""])
>>> ser
0    3.5  # wrong repr, should show 3.50
dtype: object
>>> ser[0]
'3.50'  # this is ok
```
"
775585226,38759,"BUG: float-like string, trailing 0 truncation",mzeitlin11,closed,2020-12-28T21:01:04Z,2020-12-29T03:20:56Z,"- [x] closes #38708
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
775675596,38769,"Backport PR #38759 on branch 1.2.x (BUG: float-like string, trailing 0 truncation)",meeseeksmachine,closed,2020-12-29T03:19:31Z,2020-12-29T10:22:15Z,"Backport PR #38759: BUG: float-like string, trailing 0 truncation"
775807701,38775,Jinja2 error when trying to render a Pandas DataFrame to html from within a PyInstaller distribution,boazdori,closed,2020-12-29T10:02:56Z,2020-12-29T12:48:09Z,"- [x] I have checked that this issue has not already been reported.
Yes
- [x] I have confirmed this bug exists on the latest version of pandas.
Yes
- [x] (optional) I have confirmed this bug exists on the master branch of pandas.
Yes
---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example
pandas-io-formats-style.py line 133
loader = jinja2.PackageLoader(""pandas"", ""io/formats/templates"")
env = jinja2.Environment(loader=loader, trim_blocks=True)
template = env.get_template(""html.tpl"")

```python
# Your code here
replaced the above lines with the following
import sys

if getattr(sys, 'frozen', False):
        # we are running in a bundle
        bundle_dir = sys._MEIPASS
        loader = jinja2.FileSystemLoader(bundle_dir)
else:
        loader = jinja2.PackageLoader(""pandas"", ""io/formats/templates"")
        
    env = jinja2.Environment(loader=loader, trim_blocks=True)
    template = env.get_template(""html.tpl"")


#### Problem description
When trying to package projects which include pandas and jinja2 with pyinstaller and running the produced exe we are getting the following error:
File ""lib\site-packages\pandas\io\formats\style.py"", line 62, in <module>
File ""lib\site-packages\pandas\io\formats\style.py"", line 135, in Styler
File ""lib\site-packages\jinja2\environment.py"", line 883, in get_template
File ""lib\site-packages\jinja2\environment.py"", line 857, in _load_template
File ""lib\site-packages\jinja2\loaders.py"", line 115, in load
File ""lib\site-packages\jinja2\loaders.py"", line 248, in get_source
jinja2.exceptions.TemplateNotFound: html.tpl

#### Expected Output
To run after compilation
#### Notes
If the code is replaced in pandas-io-formats-style.py line 133 as suggested above the problem is resolved. 
See StackOverflow link below
[https://stackoverflow.com/questions/31259673/unable-to-include-jinja2-template-to-pyinstaller-distribution](URL)"
775093235,38729,BUG: DataFrame.where with EA other,jbrockmendel,closed,2020-12-27T19:17:30Z,2020-12-29T14:30:43Z,"```
df = pd.DataFrame({""A"": [1, 2, 3], ""B"": [4, 5, 6]})
arr = pd.array([7, pd.NA, 9])
mask = np.ones(df.shape, dtype=bool)

>>> result = df.where(mask, arr)
AttributeError: 'IntegerArray' object has no attribute 'reshape'
```

we can kludge around this by using `np.reshape(arr, shape)` instead of `arr.reshape(shape)`, but that'll cast to object, which is not great.

Non-kludge solution: 2D EAs"
692136954,36097,Center window,adamamiller,closed,2020-09-03T17:06:40Z,2020-12-29T14:56:12Z,"- [x] closes #20012
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Added center functionality for `VariableWindowIndexer`. Note - I am unsure if the NotImplementedError in lines 1966-1969 in rolling.py still correctly raises an error for offset based windows."
774477813,38680,CLN: Add typing for dtype argument in io/sql.py,avinashpancham,closed,2020-12-24T14:55:29Z,2020-12-29T14:56:38Z,"Follow up PR for #37546:

- Added typing `Optional[DtypeArg]` for dtype arg in pandas/io, since those functions accept single values and dicts as `dtype` args
- Added typing `Optional[Dtype]` for dtype arg in pandas/core, since those functions only accept a single value as `dtype` args
- Added typing `Optional[NpDtype]` for dtype arg in pandas/core for functions that only accept numpy dtypes as `dtype` args


- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry


"
775909668,38777,Backport PR #38766 on branch 1.2.x (BLD: fix build failure py3.9.1 on OSX),meeseeksmachine,closed,2020-12-29T14:06:02Z,2020-12-29T15:35:02Z,Backport PR #38766: BLD: fix build failure py3.9.1 on OSX
775632321,38766,BLD: fix build failure py3.9.1 on OSX,jbrockmendel,closed,2020-12-28T23:55:12Z,2020-12-29T15:57:32Z,"Ran `brew upgrade` yesterday and ended up bumping from 3.9.0 to 3.9.1, after which I got build failures 

```
    LooseVersion(python_target) < ""10.9""
  File ""/usr/local/Cellar/python@3.9/3.9.1_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/distutils/version.py"", line 306, in __init__
    self.parse(vstring)
  File ""/usr/local/Cellar/python@3.9/3.9.1_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/distutils/version.py"", line 314, in parse
    components = [x for x in self.component_re.split(vstring)
TypeError: expected string or bytes-like object
```

No idea why this just started breaking now.

"
775179064,38742,CLN: NDFrame._where,jbrockmendel,closed,2020-12-28T04:10:29Z,2020-12-29T15:58:08Z,"- [ ] closes #38729 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
775840897,38776,"CI,STYLE: add spell check?",MarcoGorelli,closed,2020-12-29T11:20:53Z,2020-12-29T17:03:13Z,"IIRC it was suggested in a comment some time ago to use a spell checker, so I thought I'd make a little PR showing how `codespell` would work (for now just applied to `pandas/core`).

Is this something we'd want? False positives can be ignored in `setup.cfg` in the `[codespell]` section"
728921788,37393,DOC: make RST files conform to pandas token usage,LeviMatus,closed,2020-10-25T02:19:23Z,2020-12-29T17:17:34Z,"See also: #32316

- [x] closes #32316
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry


In #36845 the RST files were updated to normalize usage of the word ""pandas"". The following tokens were replaced:
+ \`pandas\`
+ \`\`pandas\`\`
+ \*pandas\*
+ \*\*pandas\*\*
+ Pandas

In that PR, I wrongly said that I caught all patterns. I was wrong, and this PR address that (my apologies). 

#### Misc Note

I'm also working on a rule for `code_checks.sh` to catch this in the future. I have the logic of the rule, but I'm struggling to get it to output nicely in such a way that it conforms with the rest of the rules in `code_checks.sh`. I want to leave this information here in case someone has any ideas I can bounce of off, or in case anyone else wants to try their hand at this. I could be spinning my wheels there for awhile and don't want it to hold up getting these changes in.

I used the following rule to find the remaining cases:
```shell
    MSG='Check doc/source RST files which format pandas incorrectly'; echo $MSG
    invgrep -R --include=*.rst -Poz '((\*|`)+)pandas\2|((((\.\. code-block:: ipython\s+)(\s +[^\n]*)+)|.\.\ [^\n]+https?[^\n]+|(`+).*\s+<https?[^\n]+>\8(__)?|`.*`_+\s+~+|.*:ref:`.*`)(*SKIP)(?!)|\bPandas\b(?!-))' doc/source/ | sed 's/\x0/\n/g'
    RET=$(($RET + $?)) ; echo $MSG ""DONE""
```

This finds easily identifiable formatting errors (\`\`pandas\`\`, for example), but it also respects article titles and code examples in an RST `code-block`s. It uses `grep -v` to get all text on a single line, but that causes error messages to lose the line number of the file that the error occurred on (everything in file `foo.rst` is reported to have happened on line 1). Because of this I'm not including the regex in this PR. If anyone is interested in giving me pointers on getting that bit fixed, or even taking my idea and running with it themselves, please do so.

Edit:

Because its desired to retain \`\`pandas\`\` in some places where it contextually make sense, regex won't be able to capture such uses. The above regex can instead be:
```shell
    MSG='Check doc/source RST files which format pandas incorrectly'; echo $MSG
    invgrep -R --include=*.rst -Poz '(\*+|`)pandas\1|((((\.\. code-block:: ipython\s+)(\s +[^\n]*)+)|.\.\ [^\n]+https?[^\n]+|(`+).*\s+<https?[^\n]+>\7(__)?|`.*`_+\s+~+|.*:ref:`.*`)(*SKIP)(?!)|\bPandas\b(?!-))' doc/source/ | sed 's/\x0/\n/g'
    RET=$(($RET + $?)) ; echo $MSG ""DONE""
```

which will ignore \`\`pandas\`\` but catch \`pandas\`
"
738155152,37682,BUG: Index.where vs Series.where mismatch,jbrockmendel,closed,2020-11-07T03:48:34Z,2020-12-29T18:09:25Z,"If in merge._maybe_add_join_keys we replace a call to Index.where with a call to Series.where, we get a test failure because of the following mismatch:

```
dr = pd.date_range(""2001-01-01"", periods=3)
lvals = pd.DatetimeIndex([dr[0], dr[1], pd.NaT])._data
rvals = pd.Categorical([dr[0], pd.NaT, dr[2]])

mask = np.array([False, False, True])

>>> pd.Series(lvals).where(~mask, rvals)._values
array([Timestamp('2001-01-01 00:00:00'), Timestamp('2001-01-02 00:00:00'),
       978480000000000000], dtype=object)

>>> pd.Index(lvals).where(~mask, rvals)._values
<DatetimeArray>
['2001-01-01 00:00:00', '2001-01-02 00:00:00', '2001-01-03 00:00:00']
Length: 3, dtype: datetime64[ns]
```

The Index version looks better behaved.  These should share an implementation."
751159040,38073,BUG: Series.where casting dt64 to int64,jbrockmendel,closed,2020-11-25T22:43:24Z,2020-12-29T18:13:45Z,"- [x] closes #37682
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This sits on top of #38021"
774023033,38665,REF: Index.__new__,jbrockmendel,closed,2020-12-23T20:20:08Z,2020-12-29T18:25:51Z,Aiming towards sharing more of this with Series and pd.array
775665060,38768,CLN: rolling.py and window/aggregations.pyx,mroeschke,closed,2020-12-29T02:31:39Z,2020-12-29T18:43:13Z,"* Inlined a helper method
* Added `const`s
* Removed unneeded comments/code
"
775625044,38764,BUG: Series construction with mismatched dt64 data vs td64 dtype,jbrockmendel,closed,2020-12-28T23:23:35Z,2020-12-29T19:07:22Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Fixing this for DataFrame is much harder, so punting for now."
775129329,38737,BUG/REG: RollingGroupby MultiIndex levels dropped,mroeschke,closed,2020-12-27T23:44:21Z,2020-12-29T19:35:36Z,"- [x] closes #38523
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This was accidentally caused by https://github.com/pandas-dev/pandas/pull/37661 where I thought https://github.com/pandas-dev/pandas/issues/37641 was a bug when it was actually the correct behavior.

Therefore, I had to change the behavior of some prior tests. "
595133188,33321,BUG: DataFrame.mode index dtype is not type stable,TomAugspurger,closed,2020-04-06T13:49:14Z,2020-12-29T19:51:23Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---



#### Code Sample, a copy-pastable example

The `.index.dtype` is not stable for DataFrame.mode. It depends on whether the DataFrame is empty and possible the dtypes

```python
In [60]: pd.DataFrame([], columns=['a', 'b']).mode().index.dtype
Out[60]: dtype('O')
```

#### Problem description

The index dtype should always be Int64, to match the non-empty case

#### Expected Output

```python
In [61]: pd.DataFrame({""A"": ['a']}).mode().index.dtype
Out[61]: dtype('int64')
```"
775103231,38732,BUG: Ensure series/frame mode() keeps int index,mzeitlin11,closed,2020-12-27T20:31:03Z,2020-12-29T20:00:09Z,"- [x] closes #33321
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
776013692,38783,TYP: ExtensionIndex,jbrockmendel,closed,2020-12-29T17:44:25Z,2020-12-29T20:33:09Z,"
"
626808722,34441,Add date dtype,zbrookle,closed,2020-05-28T20:55:54Z,2020-12-29T20:41:26Z,"- [ ] closes #32473

This is the beginning of the date data type, and it definitely works properly on a high level. For some of the places where strings might need to be converted and such I used the cython code that was implemented in tslib. The time complexity is still linear, but some of those methods may need to be rewritten for dates in cython,  which I'm happy to do.
"
623545862,34330,BUG: Fix using dtype with parse_dates in read_csv,mproszewska,closed,2020-05-23T01:45:15Z,2020-12-29T20:42:50Z,"- [x] closes #34066
- [x] tests added/passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
698327511,36273,Fix issue #36271 to disambiguate json string,tbachlechner,closed,2020-09-10T18:19:56Z,2020-12-29T20:44:17Z,"pd.read_json() fails currently for strings that look like fsspec_url and contain ""://"". adding another condition to fix this at least in most cases

- [X] closes #36271
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
683781554,35845,CI: Revert 31323 for deprecation warning from Jedi,charlesdong1991,closed,2020-08-21T19:24:07Z,2020-12-29T20:45:23Z,"- [x] closes #31407

ipython/ipython#12102 has been solved, and the new release came out a couple weeks ago, so revert should be fine according to https://github.com/ipython/ipython/issues/12102#issuecomment-670987484
"
775681949,38770,DOC: suppress debug messages when displaying plots,partev,closed,2020-12-29T03:46:04Z,2020-12-29T22:00:30Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
752445950,38119,REF: dtypes.cast,jbrockmendel,closed,2020-11-27T19:46:07Z,2020-12-29T22:59:27Z,"There are too many functions in `dtypes.cast`, making it difficult to know which one to use in a given situation.  e.g. when working with `Index.insert` in #38102 I considered `maybe_promote`, `maybe_upcast`, `maybe_casted_values`, `find_common_type`.

Best case scenario we can de-duplicate some of these and just remove some of them, but failing that, I think we could find new homes for e.g. `construct_1d_arraylike_from_scalar`, `construct_1d_object_array_from_listlike`, `construct_1d_ndarray_preserving_na`."
773266142,38642,REGR: GroupBy.indices no longer includes unobserved categories,jorisvandenbossche,closed,2020-12-22T21:32:49Z,2020-12-29T23:16:30Z,"Does anybody know if this was an intentional change? (I don't directly find something about it in the whatsnew)

```
In [9]: pd.__version__ 
Out[9]: '1.0.5'

In [10]: df = pd.DataFrame({""key"": pd.Categorical([""b""]*5, categories=[""a"", ""b"", ""c"", ""d""]), ""col"": range(5)}) 

In [11]: gb = df.groupby(""key"")   

In [12]: list(gb.indices)  
Out[12]: ['a', 'b', 'c', 'd']
```

vs

```
In [1]: pd.__version__
Out[1]: '1.3.0.dev0+92.ga2d10ba88a'

In [2]: df = pd.DataFrame({""key"": pd.Categorical([""b""]*5, categories=[""a"", ""b"", ""c"", ""d""]), ""col"": range(5)})

In [3]: gb = df.groupby(""key"")

In [4]: list(gb.indices)
Out[4]: ['b']
```

This already changed in pandas 1.1, so not a recent change.

The consequence of this is that iterating over `gb` vs iterating over `gb.indices` is not consistent anymore.

cc @mroeschke @rhshadrach "
290687528,19356,.isin implicitly converts data types,caerner,closed,2018-01-23T02:30:36Z,2020-12-29T23:44:27Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
pd.__version__
Out[1]: '0.22.0'

data = pd.Series([-9.0, 0.0])
data.isin([-9, -0.5])

Out[2]: 
0     True
1    False
dtype: bool

data = pd.Series([-9, 0])
data.isin([-9, -0.5])

Out[3]: 
0    True
1    True
dtype: bool


```
#### Problem description

.isin seems to implicitly convert the data type of what is passed in values depending on the data type of the Series. In the above example, I would expect the output to be (True, False) in both cases. There seems to have been a change in behavior after pandas version 0.19.2. With version 0.19.2 the output is in both cases (True, False).


#### Expected Output

```python
import pandas as pd
pd.__version__
Out[1]: '0.19.2'

data = pd.Series([-9.0, 0.0])
data.isin([-9, -0.5])

Out[2]: 
0     True
1    False
dtype: bool

data = pd.Series([-9, 0])
data.isin([-9, -0.5])

Out[3]: 
0    True
1    False
dtype: bool


```
"
339180898,21804,BUG: unwanted casting in .isin,jreback,closed,2018-07-07T22:51:49Z,2020-12-29T23:44:28Z,"xref #19508  for a partial PR

```
In [3]: values = [1, 0.5]
   ...: pd.Series([1, 0]).isin(values)
   ...: 
Out[3]: 
0    True
1    True
dtype: bool
```
should be [True, False]"
775037706,38718,Change tests of casting an interval range index from float to int to reflect current behaviour,moink,closed,2020-12-27T13:23:52Z,2020-12-30T03:27:23Z,"This is to address an issue I came across in PR #38697 where there is a test, `test_subtype_integer_errors`, in pandas/tests/indexes/interval/test_astype.py marked as failing because it is not raising `ValueErrors` when casting an interval range index from float to int. My reading of #15832 is that it is unnecessary to raise a `ValueError` in this case.

As I wrote in the comments to #38697:

The question is, as a user of pandas, if you had an interval range such as [(0.0, 0.25], (0.25, 0.5], (0.5, 0.75], (0.75, 1.0], (1.0, 1.25], (1.25, 1.5], (1.5, 1.75], (1.75, 2.0], (2.0, 2.25], (2.25, 2.5]], and you cast it to int, what would you want or expect? The old (failing) version of the test said you expect a ValueError. I am saying you expect [(0, 0], (0, 0], (0, 0], (0, 1], (1, 1], (1, 1], (1, 1], (1, 2], (2, 2], (2, 2]] which, as I said, looks not that useful as an IntervalIndex, but is not wrong either.

So what I plan to do is delete two `pytest.raises` calls in `test_subtype_integer_errors` but also add a test to ensure that casting an interval range from float to int works as expected.


> 
> Your logic makes sense to me, but I'd defer to someone with more experience on how we handle `IntervalIndex`. 
> 
> If being removed from this test, we definitely want to make sure this specific casting behavior is being tested somewhere.  To keep this pr simple and easier to merge, I'd suggest reverting modification of this test, then making a followup pr to clean up this test and add tests for any untested casting behavior.
> 
> _Originally posted by @mzeitlin11 in https://github.com/pandas-dev/pandas/pull/38697#discussion_r549023944_
"
775042437,38719,TST: GH38718 Tests for casting an interval range from float to int,moink,closed,2020-12-27T13:55:43Z,2020-12-30T03:27:32Z,"xrefs #38697 , #15832

- [x] closes #38718
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
712134288,36745,DOC: TimeStamp.strftime() missing argument description,Dr-Irv,closed,2020-09-30T17:25:03Z,2020-12-30T08:09:57Z,"#### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.Timestamp.strftime.html?highlight=strftime#pandas.Timestamp.strftime

#### Documentation problem

The argument for `TimeStamp.strftime()` is missing in the docs

#### Suggested fix for documentation

Argument is a format string.  Should reference python docs https://docs.python.org/3/library/datetime.html
"
734002333,37559,Add ss02,abhishekmangla,closed,2020-11-01T16:10:09Z,2020-12-30T08:10:29Z,"- [x] closes #25113, closes https://github.com/pandas-dev/pandas/issues/36745
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
771307490,38576,TST: Bare pytest raises,MJafarMashhadi,closed,2020-12-19T05:01:24Z,2020-12-30T08:23:49Z,"- xref #30999

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry [N/A]
"
741101491,37770,TST: Isin converted floats unnecessarily to int causing rounding issues,phofl,closed,2020-11-11T22:03:28Z,2020-12-30T09:10:11Z,"- [x] closes #19356
- [x] closes #21804
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry


"
773323379,38649,BUG: Fix regression for groupby.indices in case of unused categories,phofl,closed,2020-12-22T23:58:34Z,2020-12-30T09:10:38Z,"- [x] closes #38642
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

 To show the unused we categories, we can dispatch back as we did before #36842. Since Categorical does not support nan, the reason for the switch does not exist in this case. I think handling unused categories in ``get_indexer_dict`` is not desirable, because it would introduce a lot of convolution for one corner case we could handle here pretty easily.
Since we are only using ``indices`` in grouper only for categoricals, we do not have to worry about sorting as the test shows.

cc @jorisvandenbossche @mroeschke "
775075816,38724,TST: GH30999 Add match=msg to all but two pytest.raises in tests/io,moink,closed,2020-12-27T17:20:11Z,2020-12-30T11:08:25Z,"This pull request partially addresses xref #30999 to remove bare `pytest.raises` by adding `match=msg`. It doesn't close that issue as I have only addressed test modules in the pandas/tests/io/ directory.

I thought this was going to be relatively small because there were only 25 such instances, but it turned out some of them were in methods that were reused several times, so this ended up larger than I expected.

There were a couple of cases where I found the structure of the tests quite confusing, with an internal function definition and two nested asserts. This structure also made it difficult to add the assertion about the error message. I restructured the tests to be, in my opinion, more straightforward.

There's one file, pandas/tests/io/test_sql.py, with two instances of `pytest.raises` in tests which were skipped on my machine and which I couldn't figure out how to run, so those are left as is.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
776127505,38790,Backport PR #38649 on branch 1.2.x (BUG: Fix regression for groupby.indices in case of unused categories),meeseeksmachine,closed,2020-12-29T23:16:53Z,2020-12-30T11:11:20Z,Backport PR #38649: BUG: Fix regression for groupby.indices in case of unused categories
775253569,38747,⬆️ UPGRADE: Autoupdate pre-commit config,github-actions[bot],closed,2020-12-28T07:33:34Z,2020-12-30T12:14:18Z,"<!-- START pr-commits -->
<!-- END pr-commits -->

## Base PullRequest

default branch (https://github.com/pandas-dev/pandas/tree/master)

## Command results
<details>
<summary>Details: </summary>

<details>
<summary><em>add path</em></summary>

```Shell
/home/runner/work/_actions/technote-space/create-pr-action/v2/node_modules/npm-check-updates/bin
```



</details>
<details>
<summary><em>pip install pre-commit</em></summary>

```Shell
Collecting pre-commit
  Downloading pre_commit-2.9.3-py2.py3-none-any.whl (184 kB)
Collecting cfgv>=2.0.0
  Using cached cfgv-3.2.0-py2.py3-none-any.whl (7.3 kB)
Collecting identify>=1.0.0
  Downloading identify-1.5.10-py2.py3-none-any.whl (97 kB)
Collecting nodeenv>=0.11.1
  Using cached nodeenv-1.5.0-py2.py3-none-any.whl (21 kB)
Collecting pyyaml>=5.1
  Using cached PyYAML-5.3.1-cp39-cp39-linux_x86_64.whl
Collecting toml
  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)
Collecting virtualenv>=20.0.8
  Downloading virtualenv-20.2.2-py2.py3-none-any.whl (5.7 MB)
Collecting appdirs<2,>=1.4.3
  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)
Collecting distlib<1,>=0.3.1
  Using cached distlib-0.3.1-py2.py3-none-any.whl (335 kB)
Collecting filelock<4,>=3.0.0
  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)
Collecting six<2,>=1.9.0
  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)
Installing collected packages: six, filelock, distlib, appdirs, virtualenv, toml, pyyaml, nodeenv, identify, cfgv, pre-commit
Successfully installed appdirs-1.4.4 cfgv-3.2.0 distlib-0.3.1 filelock-3.0.12 identify-1.5.10 nodeenv-1.5.0 pre-commit-2.9.3 pyyaml-5.3.1 six-1.15.0 toml-0.10.2 virtualenv-20.2.2
```

### stderr:

```Shell
WARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.
You should consider upgrading via the '/opt/hostedtoolcache/Python/3.9.1/x64/bin/python -m pip install --upgrade pip' command.
```

</details>
<details>
<summary><em>pre-commit autoupdate || (exit 0);</em></summary>

```Shell
Updating https://github.com/python/black ... already up to date.
Updating https://gitlab.com/pycqa/flake8 ... already up to date.
Updating https://github.com/PyCQA/isort ... already up to date.
Updating https://github.com/asottile/pyupgrade ... [INFO] Initializing environment for https://github.com/asottile/pyupgrade.
already up to date.
Updating https://github.com/pre-commit/pygrep-hooks ... already up to date.
Updating https://github.com/asottile/yesqa ... already up to date.
Updating https://github.com/pre-commit/pre-commit-hooks ... [INFO] Initializing environment for https://github.com/pre-commit/pre-commit-hooks.
updating v3.3.0 -> v3.4.0.
```



</details>
<details>
<summary><em>pre-commit run -a || (exit 0);</em></summary>

```Shell
[INFO] Installing environment for https://github.com/python/black.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://gitlab.com/pycqa/flake8.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://gitlab.com/pycqa/flake8.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/PyCQA/isort.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/asottile/pyupgrade.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for local.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for local.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for local.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/asottile/yesqa.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
[INFO] Installing environment for https://github.com/pre-commit/pre-commit-hooks.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
black..................................................................................................Passed
flake8.................................................................................................Passed
flake8 (cython)........................................................................................Passed
flake8 (cython template)...............................................................................Passed
isort..................................................................................................Passed
pyupgrade..............................................................................................Passed
rst ``code`` is two backticks..........................................................................Passed
rst directives end with two colons.....................................................................Passed
rst ``inline code`` next to normal text................................................................Passed
Generate pip dependency from conda.....................................................................Passed
flake8-rst.............................................................................................Passed
Check for non-standard imports.........................................................................Passed
Check for non-standard numpy.random-related imports excluding pandas/_testing.py.......................Passed
Check for non-standard imports in test suite...........................................................Passed
Check for incorrect code block or IPython directives...................................................Passed
Check for use of not concatenated strings..............................................................Passed
Check for strings with wrong placed spaces.............................................................Passed
Check for import of private attributes across modules..................................................Passed
Check for use of private functions across modules......................................................Passed
Check for inconsistent use of pandas namespace in tests................................................Passed
Check for use of Union[Series, DataFrame] instead of FrameOrSeriesUnion alias..........................Passed
Check for use of foo.__class__ instead of type(foo)....................................................Passed
Check for use of comment-based annotation syntax and missing error codes...............................Passed
Check code for instances of os.remove..................................................................Passed
Strip unnecessary `# noqa`s............................................................................Passed
Fix End of Files.......................................................................................Passed
Trim Trailing Whitespace...............................................................................Passed
```



</details>

</details>

## Changed files
<details>
<summary>Changed file: </summary>

- .pre-commit-config.yaml

</details>

<hr>

[:octocat: Repo](https://github.com/technote-space/create-pr-action) | [:memo: Issues](https://github.com/technote-space/create-pr-action/issues) | [:department_store: Marketplace](https://github.com/marketplace/actions/create-pr-action)"
776417094,38805,TST: GH30999 add match=msg to pytest.raises in modules with one simple instance each,moink,closed,2020-12-30T11:46:47Z,2020-12-30T13:30:18Z,"This pull request partially addresses xref #30999 to remove bare pytest.raises by adding match=msg. It doesn't close that issue as I have only addressed the following three modules:

pandas/tests/dtypes/test_inference.py
pandas/tests/libs/test_hashtable.py
pandas/tests/resample/test_resampler_grouper.py

They are scattered around the test directory but each of them only had one instance of a bare pytest.raises which could be fixed with a simple addition of match=msg or change to tm.external_error_raised

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
776381657,38800,TST: GH30999 Add match=msg to all pytest.raises in pandas/tests/reshape,moink,closed,2020-12-30T10:21:53Z,2020-12-30T13:33:42Z,"This pull request partially addresses xref #30999 to remove bare pytest.raises by adding match=msg. It doesn't close that issue as I have only addressed test modules in the pandas/tests/reshape/ directory.

This one was super simple. Fixed 5 bare pytest.raises and didn't do anything complicated or controversial.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
776409206,38804,TST: GH30999 add match=msg to all pytest.raises in pandas/tests/window,moink,closed,2020-12-30T11:27:00Z,2020-12-30T13:47:34Z,"This pull request partially addresses xref #30999 to remove bare `pytest.raises` by adding `match=msg`. It doesn't close that issue as I have only addressed test modules in the pandas/tests/window/ directory.

I have added `match=msg` to 3 instances of `pytest.raises` and converted another to a `tm.external_error_raised`. Nothing complicated or controversial, imo.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
757009837,38287,DOC: Out-of-date list of sort algorithms,jamesmyatt,closed,2020-12-04T10:58:24Z,2020-12-30T13:49:06Z,"#### Location of the documentation

e.g. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.sort_values.html, https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_index.html

#### Documentation problem

List of values for `kind` should be updated to reflect changes in NumPy: https://numpy.org/doc/stable/reference/generated/numpy.sort.html

Specifically, since NumPy 1.15.0, the list has been `{‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’}`

#### Suggested fix for documentation

Full list of options is provided. Also, the statement that ""‘mergesort’ is the only stable algorithm"" is now incorrect.

I can do this, when I get time. But I'm more than happy for someone else to do it if they want.
"
767751912,38503,DOC: Corrected Out-of-date list of sort algorithms,FullBeardDev,closed,2020-12-15T16:05:37Z,2020-12-30T13:49:09Z,"Reference Issues/PRs
Fixes: #38287

Implemented changes:
Changed the docstring of all the methods using np.sort as it now includes one additional stable algorithm
"
93396762,10521,BUG: inconsisten multi-level indexing when levels are dropped,feldman4,closed,2015-07-06T22:58:51Z,2020-12-30T13:50:13Z,"`DF.loc['A', :, 1]` returns a DataFrame with the full MultiIndex if the second level has more than one entry, and a truncated index if the second level has only one entry (same as, e.g., `DF.loc['A',0,1]`). Is this the intended behavior?
"
587673356,33013,ENH: Support downcasting of nullable dtypes in to_numeric,jorisvandenbossche,closed,2020-03-25T12:50:15Z,2020-12-30T13:59:01Z,"This currently does not yet work:

```
In [5]: pd.to_numeric(pd.Series([1, 2, 3], dtype=""Int64""), downcast='integer') 
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-5-64a04efaaac0> in <module>
----> 1 pd.to_numeric(pd.Series([1, 2, 3], dtype=""Int64""), downcast='integer')

~/scipy/pandas/pandas/core/tools/numeric.py in to_numeric(arg, errors, downcast)
    179             for dtype in typecodes:
    180                 if np.dtype(dtype).itemsize <= values.dtype.itemsize:
--> 181                     values = maybe_downcast_to_dtype(values, dtype)
    182 
    183                     # successful conversion

~/scipy/pandas/pandas/core/dtypes/cast.py in maybe_downcast_to_dtype(result, dtype)
    141         dtype = np.dtype(dtype)
    142 
--> 143     converted = maybe_downcast_numeric(result, dtype, do_round)
    144     if converted is not result:
    145         return converted

~/scipy/pandas/pandas/core/dtypes/cast.py in maybe_downcast_numeric(result, dtype, do_round)
    234                     return new_result
    235             else:
--> 236                 if np.allclose(new_result, result, rtol=0):
    237                     return new_result
    238 

<__array_function__ internals> in allclose(*args, **kwargs)

~/miniconda3/envs/dev/lib/python3.7/site-packages/numpy/core/numeric.py in allclose(a, b, rtol, atol, equal_nan)
   2169 
   2170     """"""
-> 2171     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
   2172     return bool(res)
   2173 

<__array_function__ internals> in isclose(*args, **kwargs)

~/miniconda3/envs/dev/lib/python3.7/site-packages/numpy/core/numeric.py in isclose(a, b, rtol, atol, equal_nan)
   2268 
   2269     xfin = isfinite(x)
-> 2270     yfin = isfinite(y)
   2271     if all(xfin) and all(yfin):
   2272         return within_tol(x, y, atol, rtol)

TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```

I suppose because under the hood a conversion to object dtype array is happening somewhere."
775248906,38746,ENH: support downcasting of nullable EAs in pd.to_numeric,arw2019,closed,2020-12-28T07:22:31Z,2020-12-30T14:06:49Z,"- [x] closes #33013
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Picking up #33435.

In `pd.to_numeric` when encountering an `IntegerArray` and `FloatingArray` (or `Series` built from them) we downcast `_data` then reconstruct the array using the downcast `_data` and the `_mask` from the original array."
687299171,35923,BUG: Inconsistent results using pd.json_normalize() on a generator object versus list (off by one),ldacey,closed,2020-08-27T14:41:34Z,2020-12-30T14:08:34Z,"- [ x] I have checked that this issue has not already been reported.

- [ x] I have confirmed this bug exists on the latest version of pandas.

---

#### Code Sample, a copy-pastable example
Only one value is returned with this:

```python
def gen():
    test = [{'created_at': '2020-08-24T09:30:05Z',
             '_id': '5f43889de6a98fd57afce7be'},
            {'created_at': '2020-08-23T11:16:09Z',
             '_id': '5f44b03799944352493d9317'},
           ]
    for val in test:
        yield val
        
results = gen()
pd.json_normalize(results)

```
![image](https://user-images.githubusercontent.com/12517855/91455973-528e3f00-e8b5-11ea-9aa4-84d1aba7fc3c.png)

This returns all values though:
```
results = gen()
list_ = [x for x in results]
pd.json_normalize(list_)
```
And so does this:

```python
def list_():
    final = []
    test = [{'created_at': '2020-08-24T09:30:05Z',
             '_id': '5f43889de6a98fd57afce7be'},
            {'created_at': '2020-08-23T11:16:09Z',
             '_id': '5f44b03799944352493d9317'},
           ]
    for val in test:
        final.append(val)
    return final

results = list_()
pd.json_normalize(results)
```

![image](https://user-images.githubusercontent.com/12517855/91456094-72bdfe00-e8b5-11ea-9d78-e7415b3ef46c.png)


#### Problem description

Using pd.json_normalize() on a generator always seems to reduce the expected results by 1. I first noticed this on a REST API where a column informed me that I should expect 901 results but I kept getting 900 results each time. When I tried to append the results to a list and normalize that, I got the expected 901 results.

#### Expected Output
Perhaps this is an expected output. It just caused me some headaches earlier and it was not immediately obvious that I was missing one record. I would expect that my example above would result in the same 2 row DataFrame.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-1028-azure
Version          : #29~18.04.1-Ubuntu SMP Fri Jun 5 14:32:34 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200814
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.17.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 1.0.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.19
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.48.0

</details>
"
774744904,38698,BUG: Fix pd.json_normalize to not skip the first element of a generator input,avinashpancham,closed,2020-12-25T14:57:06Z,2020-12-30T14:08:38Z,"

- [x] closes #35923
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
776454306,38807,Backport PR #38723 on branch 1.2.x (BUG: inconsistency between frame.any/all with dt64 vs dt64tz),meeseeksmachine,closed,2020-12-30T13:24:07Z,2020-12-30T14:45:19Z,Backport PR #38723: BUG: inconsistency between frame.any/all with dt64 vs dt64tz
775083362,38726,DOC: chat-ops,MarcoGorelli,closed,2020-12-27T18:06:20Z,2020-12-30T15:07:16Z,"xref https://github.com/pandas-dev/pandas/pull/38444#issuecomment-749711369

There are some GitHub Actions workflows which can be triggered via comments:

1. If you comment `@github-actions pre-commit` on a pull request, it will trigger a workflow which will autofix formatting errors (e.g. from `black` or `isort`)
2. `@meeseeksdev backport ` will trigger a workflow which will backport a given change to a branch (e.g. `@meeseeksdev backport 1.2.x`)
3. If you comment `take` on an issue, it will trigger a workflow which will assign the issue to you

These should be documented somewhere in the docs - I think `doc/source/development/contributing.rst` might be a sensible location. `take` is already documented in there, but the other two aren't"
775546862,38754,DOC: chat-ops,1nF0rmed,closed,2020-12-28T19:11:48Z,2020-12-30T15:07:21Z,"- [x] closes #38726
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] Added documentation for `@githubactions pre-commit`
- [x] Added documentation for `@meeseeksdev backport`
"
776159709,38791,BUG: Categorical with non-nano dt64,jbrockmendel,closed,2020-12-30T01:49:07Z,2020-12-30T15:42:28Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Fixes the flaky xfails in test_drop_duplicates"
775074653,38723,BUG: inconsistency between frame.any/all with dt64 vs dt64tz,jbrockmendel,closed,2020-12-27T17:12:34Z,2020-12-30T15:45:14Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
776484253,38811,"CI,STYLE: narrow down list of ignored words",chinggg,closed,2020-12-30T14:32:14Z,2020-12-30T16:12:03Z,"I have used `grep` to check if the words in the list exist in the code.  
Only `ba` can be deleted from the list.
"
775534507,38753,REGR: pd.read_csv segfaults with 1.2 (has worked since before pandas 1.0),snowman2,closed,2020-12-28T18:37:15Z,2020-12-30T18:41:00Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

Wish I could give more information, but this is all I have (not sure I can share the input data):
```
 Fatal Python error: Segmentation fault
Current thread 0x00007f658531a740 (most recent call first):
  File ""/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 2056 in read
  File ""/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 1052 in read
  File ""/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 463 in _read
  File ""/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 605 in read_csv
```
Once I added the pin `pandas<1.2` everything works again.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.7.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-1020-aws
Version          : #29-Ubuntu SMP Wed Jun 14 15:54:52 UTC 2017
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.2.0
numpy            : 1.19.1
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0
Cython           : None
pytest           : 6.2.1
hypothesis       : None
sphinx           : 3.4.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.6.2
html5lib         : None
pymysql          : None
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.4
sqlalchemy       : 1.3.22
tables           : None
tabulate         : 0.8.7
xarray           : 0.16.2
xlrd             : None
xlwt             : None
numba            : None
None

</details>
"
776517501,38817,MAINT: regex char class improve,tylerjereddy,closed,2020-12-30T15:44:33Z,2020-12-30T18:42:07Z,"* remove superfluous regex character classes
from the codebase (those that contain a single
character incur the overhead of a class with
none of the advantages of a class)

* for more details, see similar change in NumPy:
https://github.com/numpy/numpy/pull/18083

* check performed with some simple [scraping code](https://github.com/tylerjereddy/regex-improve)"
776522411,38818,TST/CLN: Remove duplicate abc tests,rhshadrach,closed,2020-12-30T15:55:30Z,2020-12-30T18:43:34Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

From #38588, this test is a proper subset of the tests immediately below them."
776084361,38789,BUG: Fix precise_xstrtod segfault on long exponent,mzeitlin11,closed,2020-12-29T21:00:46Z,2020-12-30T18:54:26Z,"- [x] closes #38753
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

@snowman2 can you check if this branch returns behavior to what you had before 1.2?"
776558738,38821,ENH: concat & merge: return True if left DataFrame is modified,yohplala,closed,2020-12-30T17:27:06Z,2020-12-30T19:36:53Z,"Hi,
This ticket is to propose 2 changes:

1st change: make available `concat` as method of DataFrame (similar to `merge`)
Concat could accept a list of DataFrame, similar to `pd.concat`.

2nd change: whichever `pandas.DataFrame.merge` or (newly implemented) `pandas.DataFrame.concat`
- modify left DataFrame in place
- return a boolean:
   - `True`if left DataFrame has actually been modified
   - `False`if left DataFrame has actually not been modified

Such a feature would prevent the user to make a check `pandas.DataFrame.equals()` to know if the left DataFrame has been modified or not.
Personnally, in my case, the left DataFrame can be a DataFrame I am just reading from a data file, and the right DataFrame, some 'new data' I am thinking it is new and needs to be merged/concatenated to the existing one.

If my merge/concat processing do not result in any change, I will not overwrite the same DataFrame on disk (useless: it has not been modified)

Could this be considered?
Thanks for your help.
Bests"
775049036,38720,TST: GH30999 Add match=msg to all pytest.raises in tests/reductions and add an error message to nanmedian,moink,closed,2020-12-27T14:36:43Z,2020-12-30T19:58:59Z,"This pull request partially addresses xref #30999 to remove bare `pytest.raises` by adding `match=msg`. It doesn't close that issue as I have only addressed test modules in the pandas/tests/reductions directory.

When going through the tests I found one case where the error message was the empty string. I decided that wasn't correct so I modified the `TypeError` to inherit the error message from the `ValueError` it was raising from. This made the error message similar to other messages in the same test.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
776169542,38793,REF: implement array_algos.putmask,jbrockmendel,closed,2020-12-30T02:37:19Z,2020-12-30T20:03:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
776586835,38828,Backport PR #38789 on branch 1.2.x (BUG: Fix precise_xstrtod segfault on long exponent),meeseeksmachine,closed,2020-12-30T18:41:31Z,2020-12-30T20:28:25Z,Backport PR #38789: BUG: Fix precise_xstrtod segfault on long exponent
578807706,32593,[BUG] Dataframe.rank() produces wrong results for float columns,karthikeyann,closed,2020-03-10T19:23:15Z,2020-12-30T21:11:28Z,"#### Code to reproduce the issue

```python
import numpy as np
import pandas as pd

indx = np.array([5, 4, 3, 2, 1, 6, 7, 8, 9, 10])
col1 = np.array([5, 4, 3, 5, 8, 5, 2, 1, 6, 6])
col2 = np.array([5, 4, np.nan, 5, 8, 5, np.inf, np.nan, 6, -np.inf])
pdf = pd.DataFrame(index=indx)
pdf[""col1""] = col1.astype('f8')
pdf[""col2""] = col2.astype('f8')

ranked_df = pdf.rank()

ranked_series = pd.DataFrame(index=indx)
ranked_series[""col1""] = pdf[""col1""].rank()
ranked_series[""col2""] = pdf[""col2""].rank()

print(ranked_df.equals(ranked_series))
#False
```
#### Problem description

Dataframe rank produces individual column ranks. But when float column is present, individual ranks are not equal to Dataframe rank. 

#### Expected Output
`True`

#### Output of ``pd.show_versions()``

<details>

pd.show_versions()                                                                                                  

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-72-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0.post20200309
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
776081804,38788,BUG: read_excel throws FileNotFoundError with s3fs objects,likealostcause,closed,2020-12-29T20:53:12Z,2020-12-30T21:12:30Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample
I get a FileNotFoundError when running the following code:
```python
import pandas as pd
import s3fs
fs = s3fs.S3FileSystem()
with fs.open('s3://bucket_name/filename.xlsx') as f:
    pd.read_excel(f)
    # NOTE: pd.ExcelFile(f) throws same error
```
```python-traceback
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/home/airflow/GVIP/venv/lib/python3.8/site-packages/pandas/util/_decorators.py"", line 299, in wrapper
    return func(*args, **kwargs)
  File ""/home/airflow/GVIP/venv/lib/python3.8/site-packages/pandas/io/excel/_base.py"", line 336, in read_excel
    io = ExcelFile(io, storage_options=storage_options, engine=engine)
  File ""/home/airflow/GVIP/venv/lib/python3.8/site-packages/pandas/io/excel/_base.py"", line 1062, in __init__
    ext = inspect_excel_format(
  File ""/home/airflow/GVIP/venv/lib/python3.8/site-packages/pandas/io/excel/_base.py"", line 938, in inspect_excel_format
    with get_handle(
  File ""/home/airflow/GVIP/venv/lib/python3.8/site-packages/pandas/io/common.py"", line 648, in get_handle
    handle = open(handle, ioargs.mode)
FileNotFoundError: [Errno 2] No such file or directory: '<File-like object S3FileSystem, bucket_name/filename.xlsx>'
```

#### Problem description
I should be able to read in the File-like object from s3fs when using `pd.read_excel` or `pd.ExcelFile`. Pandas 1.1.x allows for this, but it looks like changes to `pd.io.common.get_handle` in 1.2 have made this impossible. The simple workaround for this is to just use the s3 URI instead of using `s3fs` to open it first, but to my knowledge, the ability to use `read_excel` with an s3fs object was not intended to be deprecated in 1.2. 

#### My noob guess on what's going wrong
I'm new to contributing to open source projects, so I don't know exactly how to fix this, but it looks like the issue is that the `pd.io.common.get_handle` method in 1.2 thinks the s3fs object is a file handle rather than a file-like buffer. To solve this, I would think something similar to the [`need_text_wrapping` boolean option](https://github.com/pandas-dev/pandas/blob/1.1.x/pandas/io/common.py#L503) from the `get_handle` method in 1.1.x needs to be added to 1.2's `get_handle` in order to tell pandas that the s3fs object needs a TextIOWrapper rather than treating it like a local file handle.

If someone could give me a little guidance on how to fix this, I'd be happy to give my first open-source contribution a go, but if that's not really how this works, I understand.

#### Expected Output
<class 'pandas.core.frame.DataFrame'>

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.0.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-197-generic
Version          : #229-Ubuntu SMP Wed Nov 25 11:05:42 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.5
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : 0.5.2
scipy            : 1.5.4
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
776508446,38814,CLN: Add typing for dtype argument in io directory (GH38808),avinashpancham,closed,2020-12-30T15:25:05Z,2020-12-30T21:24:26Z,"incremental PR for issue #38808
- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
770577129,38554,DOC: add Comparison with Excel,afeld,closed,2020-12-18T05:22:13Z,2020-12-30T21:40:47Z,"- [x] closes https://github.com/pandas-dev/pandas/issues/22993
- [x] tests added / passed
- [ ] ~~passes `black pandas`~~
- [ ] ~~passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`~~
- [ ] ~~whatsnew entry~~

## Background

I teach [a class on pandas for public policy students](https://github.com/afeld/python-public-policy/blob/master/syllabus.md#readme), and for many of them, spreadsheets are the only point of reference they have for working with tabular data. It would be very helpful to have (official) document comparing the two to point them to.

This is my first contribution to pandas and first time using reStructuredText, so feedback welcome. Thanks in advance!

## TODOs

Making a running checklist to show what I've done already, and what else I plan to do. Hoping for some preliminary feedback (like is there still interest in having this page) before spending too much more time on it.

Happy to continue in this pull request until complete with all of them, or get this merged sooner than later and take care of the others in follow-up pull requests. Slight preference for the latter (some documentation being better than none, less to review at once, etc.), but open to whatever.

- [x] start with what @rotuna did in https://github.com/pandas-dev/pandas/pull/23042
- [x] add links to canonical Excel documentation, as [previously requested](https://github.com/pandas-dev/pandas/pull/23042#discussion_r224289676)
- [x] fix for CI
- [x] link from Getting Started pages
- [ ] incorporate structure from [SAS](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sas.html)/[STATA](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_stata.html) comparison pages
   - [x] Data structures
   - [ ] Data input / output
      - [x] Mention `read_excel()`
   - [ ] Data operations
   - [ ] String processing
   - [ ] Merging
   - [ ] Missing data
   - [ ] GroupBy
   - [ ] Other considerations
- [ ] convert examples to use [`tips` dataset](https://raw.github.com/pandas-dev/pandas/master/pandas/tests/io/data/csv/tips.csv)
- [ ] add info about charting

## Questions

- [x] Which whatsnew file should I add to?
- [x] I noticed that [`doc/source/_static` is in the `.gitignore`](https://github.com/pandas-dev/pandas/blob/54682234e3a3e89e246313bf8f9a53f98b199e7b/.gitignore#L113), but [there are files checked into that folder](https://github.com/pandas-dev/pandas/tree/master/doc/source/_static). Is that intentional? - https://github.com/pandas-dev/pandas/pull/38739
- [ ] Some of the comparison documentation refers to ""columns"", while other refer to ""Series"". Is there a preference, or can they be used interchangeably?
- [x] Since spreadsheet software is largely interchangeable/compatible, would it make sense to make the page more general as ""Comparison to spreadsheets""?
- [ ] Thoughts about including [slightly more subjective content](https://colab.research.google.com/github/afeld/python-public-policy/blob/main/pandas_crash_course.ipynb#scrollTo=efp6dYYtL-Je), such as _why_ one might want to use spreadsheets vs. pandas?"
774480686,38681,BUG: DataFrame.rank with np.inf and np.nan,mzeitlin11,closed,2020-12-24T15:03:41Z,2020-12-30T21:57:24Z,"- [x] closes #32593
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Seemed like two distinct issues here - first `np.inf` wasn't distinguished from `np.nan` (https://github.com/pandas-dev/pandas/issues/32593#issuecomment-597376199).  The changes to `skip_condition` logic handles that, but doesn't fix issue from OP. Rest of diff is to handle when `np.inf` and `np.nan` are both included."
776638722,38832,Backport PR #38819 on branch 1.2.x (REGR: read_excel does not work for most file handles),meeseeksmachine,closed,2020-12-30T21:12:57Z,2020-12-30T23:18:37Z,Backport PR #38819: REGR: read_excel does not work for most file handles
775784301,38774,BUG: sem() with level raised ValueError in pandas 1.2,wjsi,closed,2020-12-29T09:06:29Z,2020-12-30T23:38:49Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

idx = pd.MultiIndex.from_arrays([
    [str(i) for i in range(100)], np.random.choice(['A', 'B'], size=(100,))
], names=['a', 'b'])

data_dict = dict((str(i), np.random.rand(100)) for i in range(10))
data_dict['string'] = [str(i) for i in range(100)]
data_dict['bool'] = np.random.choice([True, False], (100,))
data = pd.DataFrame(data_dict, index=idx)

data.sem(level=1)
```

#### Problem description

Unexpected exception raised:

```
Traceback (most recent call last):
  File ""/Users/wenjun/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 3417, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-8-cd9abe134148>"", line 1, in <module>
    data.sem(level=1)
  File ""/Users/wenjun/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py"", line 10925, in sem
    return NDFrame.sem(self, axis, skipna, level, ddof, numeric_only, **kwargs)
  File ""/Users/wenjun/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py"", line 10665, in sem
    return self._stat_function_ddof(
  File ""/Users/wenjun/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py"", line 10655, in _stat_function_ddof
    return self._agg_by_level(
  File ""/Users/wenjun/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py"", line 10552, in _agg_by_level
    return getattr(grouped, name)(**kwargs)
  File ""/Users/wenjun/miniconda3/lib/python3.8/site-packages/pandas/core/groupby/groupby.py"", line 1612, in sem
    result.iloc[:, cols] = result.iloc[:, cols] / np.sqrt(
  File ""/Users/wenjun/miniconda3/lib/python3.8/site-packages/pandas/core/indexing.py"", line 691, in __setitem__
    iloc._setitem_with_indexer(indexer, value, self.name)
  File ""/Users/wenjun/miniconda3/lib/python3.8/site-packages/pandas/core/indexing.py"", line 1636, in _setitem_with_indexer
    self._setitem_single_block(indexer, value, name)
  File ""/Users/wenjun/miniconda3/lib/python3.8/site-packages/pandas/core/indexing.py"", line 1860, in _setitem_single_block
    self.obj._mgr = self.obj._mgr.setitem(indexer=indexer, value=value)
  File ""/Users/wenjun/miniconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py"", line 568, in setitem
    return self.apply(""setitem"", indexer=indexer, value=value)
  File ""/Users/wenjun/miniconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py"", line 427, in apply
    applied = getattr(b, f)(**kwargs)
  File ""/Users/wenjun/miniconda3/lib/python3.8/site-packages/pandas/core/internals/blocks.py"", line 1025, in setitem
    values[indexer] = value
ValueError: shape mismatch: value array of shape (2,12) could not be broadcast to indexing result of shape (11,2)
```

#### Expected Output

Correct aggregation results shall be returned.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 3e89b4c4b1580aa890023fc550774e63d499da25
python           : 3.8.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Tue Nov 10 00:10:30 PST 2020; root:xnu-6153.141.10~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : zh_CN.UTF-8
pandas           : 1.2.0
numpy            : 1.19.1
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200814
Cython           : 0.29.21
pytest           : 6.0.1
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : 0.4.2
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.1
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.51.2

</details>
"
775114371,38735,DOC: make shared includes for describing the datasets in the Getting Started tutorials,afeld,closed,2020-12-27T21:50:53Z,2020-12-31T00:11:06Z,"Just a refactor; the net result should be the same.

- [ ] ~~closes #xxxx~~
- [x] tests added / passed
- [ ] ~~passes `black pandas`~~
- [ ] ~~passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`~~
- [ ] ~~whatsnew entry~~"
627226268,34456,"BUG: behaviour of astype_nansafe(.., copy=False) / SparseArray.astype bug",jorisvandenbossche,closed,2020-05-29T12:16:00Z,2020-12-31T01:49:34Z,"So our internal `astype_nansafe` has the following behaviour:

```
In [53]: pd.core.dtypes.cast.astype_nansafe(np.array([1, 2, 3]), np.dtype(""float64"")) 
Out[53]: array([1., 2., 3.])

In [54]: pd.core.dtypes.cast.astype_nansafe(np.array([1, 2, 3]), np.dtype(""float64""), copy=False) 
Out[54]: array([4.9e-324, 9.9e-324, 1.5e-323])
```

where it seems it assumes it can take a view if `copy=False`, which of course depends on the dtype it is casting to:

https://github.com/pandas-dev/pandas/blob/043b60920ede5a2589ed29685ba7247fbbc4ea5f/pandas/core/dtypes/cast.py#L990

I am not fully sure about the intent of `astype_nansafe` or whether this was intentional (I would say it is a bug, as I would expect that function to take care of whether `copy=False` is possible or not).

This also causes a bug in SparseArray's `astype` with `copy=False`:

```
In [59]: pd.arrays.SparseArray([1, 2, 3]).astype(float, copy=False)  
Out[59]: 
[5e-324, 1e-323, 1.5e-323]
Fill: 0.0
IntIndex
Indices: array([0, 1, 2], dtype=int32)
```"
775202200,38743,BUG: IntervalIndex intersections are not intersections in many cases,ivirshup,closed,2020-12-28T05:26:06Z,2020-12-31T01:50:22Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas._testing as tm

idx_unique = tm.makeIntervalIndex(k=5)
idx_non_unique = idx_unique[[0, 0, 1, 2, 3, 4]]

assert idx_unique.intersection(idx_non_unique).equals(idx_non_unique)
assert idx_non_unique.intersection(idx_unique).equals(idx_non_unique)
# As an implementation note:
assert idx_non_unique._intersection_non_unique(idx_unique).equals(idx_non_unique)
```

All of these should probably fail

#### Problem description

Intersections on `IntervalIndexes` should not repeat duplicated elements, similar to all other `Index` types.

I came across this while writing tests for #38654

For example, if we defined a test:

```python
def check_intersection_commutative(left, right):
    assert left.intersection(right).equals(right.intersection(left))

@pytest.mark.parametrize(""index_maker"", tm.index_subclass_makers_generator())
def test_intersection_uniqueness(index_maker):
    if index_make is tm.makeMultiIndex:
        pytest.pass()  # This index maker is bugged
    ind_unique = index_maker(k=5)
    ind_non_unique = idx_unique[[0, 0, 1, 2, 3, 4]]

    check_intersection_commutative(ind_unique, ind_non_unique)
    assert idx_unique.intersection(idx_non_unique).is_unique

```

Only `IntervalIndex` would fail (not including the bugged `makeMultiIndex`).

#### Expected Output

All of the assertions in the original example should fail.

#### Output of ``pd.show_versions()``

<details>
<summary> dev environment </summary>

```
INSTALLED VERSIONS
------------------
commit           : 9f1a41dee0e9167361d9f435b4c3d499d01e415a
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Tue Nov 10 00:10:30 PST 2020; root:xnu-6153.141.10~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.3.0.dev0+210.g9f1a41dee0.dirty
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200712
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : 5.20.3
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : 0.4.1
gcsfs            : 0.6.2
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 0.17.1
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.1
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.0
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.1
```

</details>

<details>
<summary> 1.1.5 </summary>

```
INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.8.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Tue Nov 10 00:10:30 PST 2020; root:xnu-6153.141.10~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.5
numpy            : 1.19.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.3.3
setuptools       : 51.0.0
Cython           : 0.29.21
pytest           : 6.2.1
hypothesis       : None
sphinx           : 3.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.5
fastparquet      : 0.4.1
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 2.0.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.4
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.2
xlrd             : 1.2.0
xlwt             : None
numba            : 0.52.0
```

</details>"

id,number,title,user,state,created_at,updated_at,body
549886890,31024,CI: Travis Remove Unrequired Exclude,alimcmaster1,closed,2020-01-14T23:57:25Z,2020-05-01T22:03:39Z,"Follow up from #30540 this exclusion is no longer required.

I checked the build config is the same using
https://config.travis-ci.com/explore"
111974120,11357,BUG? Consistency of column parsing,chris-b1,open,2015-10-17T17:13:24Z,2020-05-02T03:53:38Z,"xref #11328

``` python
In [100]: df = pd.DataFrame({0: range(5), 1:range(5)})

In [101]: df.to_excel('tmp.xlsx')

In [102]: df.to_csv('tmp.csv')

In [103]: pd.read_excel('tmp.xlsx', index_col=0).columns
Out[103]: Int64Index([0, 1], dtype='int64')

In [106]: pd.read_csv('tmp.csv', index_col=0).columns
Out[106]: Index([u'0', u'1'], dtype='object')

```
"
611002950,33934,BUG: pytest with cProfile of test_numba segfault,jbrockmendel,closed,2020-05-01T22:35:07Z,2020-05-02T04:11:43Z,"```
python3 -m cProfile -o stats2.dat -m pytest pandas/tests/groupby/aggregate/test_numba.py
[...] 
collected 66 items                                                                                                                                                                                                                                                                  
pandas/tests/groupby/aggregate/test_numba.py ...Segmentation fault: 11
```

cc @mroeschke "
332491713,21479,handling compression: filename vs magic numbers,smsaladi,open,2018-06-14T17:17:27Z,2020-05-02T04:23:45Z,"biopython/biopython#1686 is discussing reading in compressed files. pandas handles this pretty nicely (never run into issues myself), but it looks like the code infers the compression from the file extension. 

https://github.com/pandas-dev/pandas/blob/v0.23.1/pandas/io/common.py#L238

Why do this over sniffing for the magic number and inferring from there? If there was discussion about this in the past, could you point me to the relevant issue/email (I tried searching the issue tracker without luck)?"
335814253,21640,API: Unify compression-kwarg for IO-methods,h-vetinari,closed,2018-06-26T13:16:08Z,2020-05-02T04:25:51Z,"Currently, there's:
* `'compression'` in `to_csv`, `to_json`, `to_parquet`, `to_pickle`
* `'complib'` in `to_hdf` (also `'complevel'`)
* `'compress'` in `to_msgpack`

Those are all the writer-methods from https://pandas.pydata.org/pandas-docs/version/0.23.1/io.html which have compression.

Seems to me that the kwargs of `to_hdf` and `to_msgpack` should be unified with the other methods."
395198138,24546,(row) Index Name is not displayed with header=False master tracker,simonjayhawkins,open,2019-01-02T10:03:18Z,2020-05-02T04:28:20Z,"xref https://github.com/pandas-dev/pandas/issues/23788#issuecomment-440588818

- [x] to_html #23788
- [ ] to_csv #24840
- [ ] to_string
- [ ] to_latex
- [ ] other output formatting methods to be checked."
491897631,28377,File options for IO methods,TomAugspurger,open,2019-09-10T21:04:36Z,2020-05-02T04:33:59Z,"Split from https://github.com/pandas-dev/pandas/pull/27899. How should we handle options to be passed to the `open` call (like `encoding`, `errors`).

A few options

* pass through `**kwargs`
* a ""storage_options"" kwarg that's a dict with some specification

Want to thing about things like S3, etc. too."
509753317,29125,Unify error messages when opening inexistent excel/csv file,nottatdat,open,2019-10-21T06:17:06Z,2020-05-02T04:35:21Z,"Similar to https://github.com/pandas-dev/pandas/issues/29102, excel and csv file extensions now have inconsistent errors for nonexistent files.

```
import pandas as pd
pd.read_csv('file_1.csv')
IOError: [Errno 2] File file_1.csv does not exist: 'file_1.csv'

pd.read_excel('file_2.xlsx')
IOError: [Errno 2] No such file or directory: 'file_2.xlsx'
```

Would be better to have a uniform error message across multiple file extensions, like the one with json in https://github.com/pandas-dev/pandas/pull/29104:

```
FileNotFoundError: File 'no_file.json' does not exist
```"
587978506,33020,read_csv ParserError,ekomens1,closed,2020-03-25T20:42:21Z,2020-05-02T09:31:39Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
#import pandas as pd
import pandas as pd 

#Indicate file path
url = 'http://localhost:8888/edit/Dropbox/COURSES/Data%20Analysis%20Using%20Python/BookNEW.csv'

#Read data
df= pd.read_csv(url)
df.header()
```
#### Problem description
I am reading CSV file but getting ParserError
[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
398632579,24748,ENH: Only apply first group once in fast GroupBy.apply,fjetter,closed,2019-01-13T08:53:12Z,2020-05-02T09:38:12Z,"The issue of applying a function to the first row twice has been reported quite a few times (issues are usually referenced to #2936) and is also a documented shortcoming/implementation detail (see Notes in [docs](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html))

The argumentation is usually that this needs to be done to determine whether or not we can take a fast path for the calculation. Even if we're using the fast path for the calculation, however, the first row is still evaluated twice. I believe with this refactoring it would be possible to only eval the first row once iff the fast path is taken.
For functions with side effects or very expensive calculations in general this may be a big deal as discussed in various other issues.

I'm wondering if this small change may already help out folks. Truth be told, I'm not 100% certain about what's actually happening in this loop regarding the slider and item cache. For the use case where I encountered this, it seems to do the trick, though.

closes #2936
closes #2656
closes #7739
closes #10519
closes #12155
closes #20084
closes #21417

- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
537761094,30262,Builtin loading of files from multi-file zip archives,abmyii,closed,2019-12-13T20:20:39Z,2020-05-02T10:49:38Z,"Could a solution be implemented to simplify loading a dataset from a multi-file zip archive? I know it is a bit insignificant compared to most other issues (especially since there are workarounds), but I'd be happy to implement it myself if I'm given the go-ahead so as to not waste your time.
Thanks for this amazing library!"
611092041,33937,PERF: use fastpaths for is_period_dtype checks,jbrockmendel,closed,2020-05-02T02:45:12Z,2020-05-02T14:57:19Z,"This gets most of the is_period_dtype and needs_i8_conversion checks.  There are a few left in reshape.merge where i need to track down what type of objects we're working with.

"
610881012,33927,TST/REF: Move static methods out of consistency classes,charlesdong1991,closed,2020-05-01T17:53:19Z,2020-05-02T15:33:19Z,"In order to keep such tests refactor PR more readable, in this PR, I only move those methods which are actually staticmethods out of huge consistency classes to be independent tests, so no code changes! And after this PR, the big consistency classes are broken down to much smaller.

This one will pave the way for replacing some other final fixtures & staticmethods out of classes 

cc @jreback "
610972757,33932,TST: tighten xfails in test_strings,jbrockmendel,closed,2020-05-01T21:16:35Z,2020-05-02T16:21:19Z,cc @TomAugspurger 
609428116,33882,"TST: stricter xfails, catch/suppress warnings",jbrockmendel,closed,2020-04-29T23:18:14Z,2020-05-02T16:34:09Z,Remove box_df_fail which is no longer used
610931838,33929,BUG: Fix small memory access errors in ujson objToJSON.c,seberg,closed,2020-05-01T19:42:04Z,2020-05-02T16:34:48Z,"These were found (and validated fixed) using valgrind. It is likely
that none of these changes should create any issues (except maybe
on a debug build of python?). But silencing valgrind warnings is
good on its own, to ease possible future debugging using valgrind.

----

NOTE: I removed skipping of `test_astype_generic_timestamp_no_frequency`, this is just out of curiosity, I would be very surprised if it made a difference. So this should probably be only merged after undoing the test change.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
601856081,33607,ENH: general concat with ExtensionArrays through find_common_type,jorisvandenbossche,closed,2020-04-17T10:05:51Z,2020-05-02T17:26:21Z,"Exploring what is being discussed in https://github.com/pandas-dev/pandas/issues/22994. @TomAugspurger your idea seems to be working nicely! (it almost removes as much code than it adds (apart from tests/docs), ànd fixes the EA concat bugs ;))

Few notes compared to proposal in #22994:

- Since we already have a `find_common_type` function, decided to use this as the ""get_concat_dtype"", since it seems this does what is needed for concat
- Extending the EA interface with a `ExensionDtype._get_common_type` method that is used in pandas' `find_common_type` function to dispatch the logic to the extension type

What I already handled:

- general protocol, using this for concat with axis=0 (eg concatting series) and using it as example for IntegerDtype
- handling categoricals this way (which removes the `concat_categorical` helper). This turned up a few cases where we have value-dependent behaviour right now, which we can't easily preserve (mainly regarding NaNs and int dtype, see below)

Still need to handle sparse (those has failing tests now) and maybe datetime, and check other failures."
600085240,33559,BUG: Series.apply() throws an error on empty series when dtype = pd.StringDtype,IDDT,closed,2020-04-15T07:34:30Z,2020-05-02T17:49:04Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
pd.Series(['a', 'b', 'c']).iloc[0:0].apply(lambda x: x) #works as expected
pd.Series(['a', 'b', 'c']).astype('string').iloc[0:0].apply(lambda x: x) #throws an exception
```

#### Problem description
Applying a function to an empty Series with dtype `pd.StringDtype` throws `ValueError: PandasArray must be 1-dimensional.`.

#### Expected Output
Apply does nothing.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.6.3-arch1-1
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
446047785,26469,BUG: Series construction with EA dtype and index but no data fails,jorisvandenbossche,closed,2019-05-20T10:46:44Z,2020-05-02T17:49:04Z,"Using master (0.25.0dev):

```
In [8]: pd.Series(None, index=[1, 2, 3], dtype='Int64')                                                                                                                                                             
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/scipy/pandas/pandas/core/internals/construction.py in _try_cast(arr, take_fast_path, dtype, copy, raise_cast_failure)
    691         if is_integer_dtype(dtype):
--> 692             subarr = maybe_cast_to_integer_array(arr, dtype)
    693 

~/scipy/pandas/pandas/core/dtypes/cast.py in maybe_cast_to_integer_array(arr, dtype, copy)
   1311         if not hasattr(arr, ""astype""):
-> 1312             casted = np.array(arr, dtype=dtype, copy=copy)
   1313         else:

TypeError: data type not understood

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-8-9447295feee6> in <module>
----> 1 pd.Series(None, index=[1, 2, 3], dtype='Int64')

~/scipy/pandas/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath)
    202                 data = data._data
    203             elif isinstance(data, dict):
--> 204                 data, index = self._init_dict(data, index, dtype)
    205                 dtype = None
    206                 copy = False

~/scipy/pandas/pandas/core/series.py in _init_dict(self, data, index, dtype)
    295 
    296         # Input is now list-like, so rely on ""standard"" construction:
--> 297         s = Series(values, index=keys, dtype=dtype)
    298 
    299         # Now we just make sure the order is respected, if any

~/scipy/pandas/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath)
    253             else:
    254                 data = sanitize_array(data, index, dtype, copy,
--> 255                                       raise_cast_failure=True)
    256 
    257                 data = SingleBlockManager(data, index, fastpath=True)

~/scipy/pandas/pandas/core/internals/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure)
    620         subarr = _try_cast(arr, False, dtype, copy, raise_cast_failure)
    621     else:
--> 622         subarr = _try_cast(data, False, dtype, copy, raise_cast_failure)
    623 
    624     # scalar like, GH

~/scipy/pandas/pandas/core/internals/construction.py in _try_cast(arr, take_fast_path, dtype, copy, raise_cast_failure)
    711             # create an extension array from its dtype
    712             array_type = dtype.construct_array_type()._from_sequence
--> 713             subarr = array_type(arr, dtype=dtype, copy=copy)
    714         elif dtype is not None and raise_cast_failure:
    715             raise

~/scipy/pandas/pandas/core/arrays/integer.py in _from_sequence(cls, scalars, dtype, copy)
    305     @classmethod
    306     def _from_sequence(cls, scalars, dtype=None, copy=False):
--> 307         return integer_array(scalars, dtype=dtype, copy=copy)
    308 
    309     @classmethod

~/scipy/pandas/pandas/core/arrays/integer.py in integer_array(values, dtype, copy)
    110     TypeError if incompatible types
    111     """"""
--> 112     values, mask = coerce_to_array(values, dtype=dtype, copy=copy)
    113     return IntegerArray(values, mask)
    114 

~/scipy/pandas/pandas/core/arrays/integer.py in coerce_to_array(values, dtype, mask, copy)
    202 
    203     if not values.ndim == 1:
--> 204         raise TypeError(""values must be a 1D list-like"")
    205     if not mask.ndim == 1:
    206         raise TypeError(""mask must be a 1D list-like"")

TypeError: values must be a 1D list-like
```

while this works fine for non-EA dtypes. 

The problem is that the None / np.nan is eventually passed to the `_from_sequence` method, which expects a list-like. I don't think we should let people fix their `_from_sequence`, but we should rather make sure we never pass that to it."
611088430,33936,"REF: move bits of offsets to liboffsets, de-privatize",jbrockmendel,closed,2020-05-02T02:18:07Z,2020-05-02T17:58:49Z,"xref #32942, #33394"
598832866,33521,TYP: disallow decorator preserves function signature,simonjayhawkins,closed,2020-04-13T11:19:21Z,2020-05-02T18:00:27Z,xref #33455
608316872,33846,BUG: Series construction with EA dtype and index but no data fails,simonjayhawkins,closed,2020-04-28T13:25:19Z,2020-05-02T18:01:13Z,"- [ ] closes #26469
- [ ] closes #33559
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
597484723,33439,BUG: Mysterious Series.get() with Int64Index bug,sam-cohan,closed,2020-04-09T18:53:17Z,2020-05-02T18:09:55Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here

def get_t_minus_n_val(n):

    def f(x):
        # assume index is days_from_origin
        if n == 3 and x.index[-1] == 27:
            import pdb; pdb.set_trace()
            print(x.index[-1], n, x.get(x.index[-1] - n)) # for debugging
        return x.get(x.index[-1] - n, np.NaN)

    f.__name__ = f""t_minus_{n}_days""

    return f

res = data_df.set_index(""days_from_origin_"").groupby(""device"").agg({""metric1"": get_t_minus_n_val(3)})

> <ipython-input-452-9b903578cb56>(7)f()
-> print(x.index[-1], n, x.get(x.index[-1] - n)) # for debugging
(Pdb) x.get(24)
(Pdb) x.iloc[-5:]
23     60221064
24    232131096
25     46413584
26    133181464
27    229400712
Name: metric1, dtype: int64
(Pdb) 24 in x.index
False
(Pdb) 24 in x.index.values
True
Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27],
           dtype='int64')
(Pdb) x.to_dict().get(24)
232131096
```

#### Problem description
I am not able to get a minimum repro of this as it is seems to be data dependent. Instead, I am capturing the bug by showing you my pdb debugging statements in hopes that someone that knows the code can figure out where the problem is.
Basically, I am doing a custom agg function which needs to grab an element from a Series object, and even though the value clearly exists in the index, it returns None. If I first convert to dict, then it does get the value. 
I was not able to repro this by simply creating a new series and calling .get on it... that works just fine. And in fact, if I filter the dataframe for just that device, then it works just fine. It is definitely some sort of internal state issue which happens as a result of groupby having more records...

#### Expected Output

obviously I expect x.get(24) to return the correct value instead of None.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 17.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 41.2.0
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
607105920,33813,TST: Clean moments consistency,charlesdong1991,closed,2020-04-26T20:28:20Z,2020-05-03T00:38:59Z,"As discussed in https://github.com/pandas-dev/pandas/pull/30577#issuecomment-570014613

This PR is the first step to achieve it, it basically does one thing: break up the giant consistency test in `common` into four tests based on test contents: `var`, `std`, `cov`, and `series data`.

cc @jreback "
110751293,11278,List indexer on PeriodIndex doesn't coerce strings,max-sixty,closed,2015-10-09T23:25:59Z,2020-05-03T16:09:14Z,"``` python
In [2]: index = pd.period_range(start='2000', periods=20, freq='B')
In [5]: series = pd.Series(range(20), index=index)

In [6]: series
Out[6]: 
2000-01-03     0
2000-01-04     1
2000-01-05     2
...
2000-01-26    17
2000-01-27    18
2000-01-28    19
Freq: B, dtype: int64

In [7]: series.loc['2000-01-14']
Out[7]: 9
```

Supplying a list of Periods as the indexer works as expected:

``` python
In [15]: series[[pd.Period(d, freq='B') for d in ['2000-01-14', '2000-01-18']]]
Out[15]: 
2000-01-14     9
2000-01-18    11
Freq: B, dtype: int64
```

But not with strings:

``` python
In [10]: series.loc[['2000-01-14', '2000-01-15']]
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-10-e922e8a9ed74> in <module>()
----> 1 series.loc[['2000-01-14', '2000-01-15']]

/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/core/indexing.py in __getitem__(self, key)
   1187             return self._getitem_tuple(key)
   1188         else:
-> 1189             return self._getitem_axis(key, axis=0)
   1190 
   1191     def _getitem_axis(self, key, axis=0):

/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/core/indexing.py in _getitem_axis(self, key, axis)
   1321                     raise ValueError('Cannot index with multidimensional key')
   1322 
-> 1323                 return self._getitem_iterable(key, axis=axis)
   1324 
   1325             # nested tuple slicing

/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/core/indexing.py in _getitem_iterable(self, key, axis)
    931     def _getitem_iterable(self, key, axis=0):
    932         if self._should_validate_iterable(axis):
--> 933             self._has_valid_type(key, axis)
    934 
    935         labels = self.obj._get_axis(axis)

/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/core/indexing.py in _has_valid_type(self, key, axis)
   1269 
   1270                 raise KeyError(""None of [%s] are in the [%s]"" %
-> 1271                                (key, self.obj._get_axis_name(axis)))
   1272 
   1273             return True

KeyError: ""None of [['2000-01-14', '2000-01-15']] are in the [index]""
```

It also gives `NaN` without the `loc` indexer:

``` python
In [16]: series[['2000-01-14', '2000-01-18']]
Out[16]: 
2000-01-14   NaN
2000-01-18   NaN
dtype: float64
```

When supplied with the `int`s behind the `PeriodIndex`, it doesn't resolve a single int, but will resolve a list:

``` python

In [4]: index.values
Out[4]: 
array([7827, 7828, 7829, 7830, 7831, 7832, 7833, 7834, 7835, 7836, 7837,
       7838, 7839, 7840, 7841, 7842, 7843, 7844, 7845, 7846])

In [9]: series.loc[[7829, 7830]]
Out[9]: 
7829    2
7830    3
dtype: int64


In [8]: series.loc[7829]
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-8-b6f5fe23a769> in <module>()
----> 1 series.loc[7829]

/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/core/indexing.py in __getitem__(self, key)
   1187             return self._getitem_tuple(key)
   1188         else:
-> 1189             return self._getitem_axis(key, axis=0)
   1190 
   1191     def _getitem_axis(self, key, axis=0):

/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/core/indexing.py in _getitem_axis(self, key, axis)
   1331 
   1332         # fall thru to straight lookup
-> 1333         self._has_valid_type(key, axis)
   1334         return self._get_label(key, axis=axis)
   1335 

/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/core/indexing.py in _has_valid_type(self, key, axis)
   1283 
   1284             try:
-> 1285                 key = self._convert_scalar_indexer(key, axis)
   1286                 if not key in ax:
   1287                     error()

/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/core/indexing.py in _convert_scalar_indexer(self, key, axis)
    161         ax = self.obj._get_axis(min(axis, self.ndim - 1))
    162         # a scalar
--> 163         return ax._convert_scalar_indexer(key, kind=self.name)
    164 
    165     def _convert_slice_indexer(self, key, axis):

/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/tseries/base.py in _convert_scalar_indexer(self, key, kind)
    333 
    334         if kind in ['loc'] and lib.isscalar(key) and (is_integer(key) or is_float(key)):
--> 335             self._invalid_indexer('index',key)
    336 
    337         return super(DatetimeIndexOpsMixin, self)._convert_scalar_indexer(key, kind=kind)

/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/core/index.py in _invalid_indexer(self, form, key)
    942                                                            klass=type(self),
    943                                                            key=key,
--> 944                                                            kind=type(key)))
    945 
    946     def get_duplicates(self):
```
"
542956628,30515,BUG: List indexer on PeriodIndex doesn't coerce strings,proost,closed,2019-12-27T18:38:13Z,2020-05-03T16:49:49Z,"- [x] closes #11278
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Description:
In single object indexer case,
datetime string converted to datetime-like object. Because indexing use "".xs"" which is "".get_loc"" that has type casting labels code. 

But in list indexer, 
there is no type check and casting code. So i add them.

this bug happen not only series but also dataframe And ""datetimeindex"" as well as ""periodIndex""
"
611322458,33951,CLN: remove unnecessary ABC classes,jbrockmendel,closed,2020-05-03T03:19:47Z,2020-05-03T17:36:09Z,
100438790,10802,Should empty strings be null for notnull() and isnull()?,nickeubank,closed,2015-08-12T00:10:03Z,2020-05-03T20:09:24Z,"Is there a reason that `notnull()` and `isnull()` consider an empty string to not be a missing value? 

```
pd.isnull('')
False
```

Seems like in string data, people usually think of the empty string as ""missing"". 
"
223188763,16077,Series.str.cat not handling NA when using other,leoruiz,open,2017-04-20T20:25:40Z,2020-05-03T20:17:14Z,"
```python
Series(['a', 'b', 'c']).str.cat(['A', np.NaN, 'C'], sep=',')

0    a,A
1    NaN
2    c,C
dtype: object

```
#### Problem description

I was expecting NaN to be skipped and just get ""b"" in the second line. 

When using na_rep it does something like this.

```python
Series(['a', 'b', 'c']).str.cat(['A', np.NaN, 'C'], sep=',', na_rep='X')

0    a,A
1    b,X
2    c,C
dtype: object
```

I feel this behavior is inconsistent. 

I can see that if you are expecting to get output that could be used like a record/csv current behavior would help. Perhaps this could work as a parameter.

#### Expected Output

```python
0    a,A
1    b
2    c,C
dtype: object
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.4.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-59-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.19.2
nose: None
pip: 8.1.2
setuptools: 28.2.0
Cython: None
numpy: 1.11.1
scipy: None
statsmodels: None
xarray: None
IPython: 4.2.0
sphinx: 1.3.1
patsy: None
dateutil: 2.4.2
pytz: 2016.4
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: None
openpyxl: None
xlrd: 1.0.0
xlwt: 1.1.1
xlsxwriter: 0.9.6
lxml: None
bs4: None
html5lib: 0.999999999
httplib2: None
apiclient: None
sqlalchemy: 1.0.13
pymysql: None
psycopg2: 2.6.2 (dt dec pq3 ext lo64)
jinja2: 2.9.6
boto: None
pandas_datareader: None

</details>
"
318442186,20845,str.cat should return categorical data for categorical caller,h-vetinari,open,2018-04-27T14:58:37Z,2020-05-03T20:19:45Z,"The `str.cat`-accessor works for Series and Index, and returns an object of the corresponding type:

```
s = pd.Series(['a', 'b', 'a'])
t = pd.Index(['a', 'b', 'a'])
## all of the following return the same Series
s.str.cat(s)
s.str.cat(t)
s.str.cat(s.values)
s.str.cat(list(s))
# 0    aa
# 1    bb
# 2    aa
# dtype: object

## all of the following return the same Index
t.str.cat(s)
t.str.cat(t)
t.str.cat(s.values)
t.str.cat(list(s))
# Index(['aa', 'bb', 'aa'], dtype='object')
```

But the data loses its property of being a category after `str.cat`, which is inconsistent, IMO

```
sc = s.astype('category')
tc = pd.Index(['a', 'b', 'a'], dtype='category') # conversion does not work, see #20843
sc.str.cat(s)
# 0    aa
# 1    bb
# 2    aa
# dtype: object
## as opposed to:
sc.str.cat(s).astype('category')
# 0    aa
# 1    bb
# 2    aa
# dtype: category
# Categories (2, object): [aa, bb]
tc.str.cat(s) # crashes, see # 20842
```

xref #20842 #20843 "
523420546,29633,Series.str.rsplit not working with regex patterns (v 0.24.2 and 0.25.2),jamespreed,open,2019-11-15T11:36:55Z,2020-05-03T20:34:31Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

S = pd.Series(['1+1=2'])
S.str.rsplit(r'\+|=')
# returns:
0    [1+1=2]

S.str.rsplit(r'\+|=', expand=True)
# returns:
       0
0  1+1=2
```
#### Problem description
The `str.rsplit` method is not recognizing regex patterns.  This example above is from the documentation.

#### Expected Output
```python
S.str.rsplit(r'\+|=')
0    [1, 2, 3]

S.str.rsplit(r'\+|=', expand=True)
     0    1    2
0    1    1    2
```
#### Output of ``pd.show_versions()``

Version 0.24.2:
<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.2
pytest: 5.0.1
pip: 19.1.1
setuptools: 41.0.1
Cython: 0.29.12
numpy: 1.16.4
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 7.6.1
sphinx: 2.1.2
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: 1.2.1
tables: 3.5.2
numexpr: 2.6.9
feather: None
matplotlib: 3.1.0
openpyxl: 2.6.2
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.8
lxml.etree: 4.3.4
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.3.5
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>

Version 0.25.2:
<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.2
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
553449876,31199,Writing HDF5 with StringDtype columns fails with AttributeError: 'StringArray' object has no attribute 'size',gerritholl,open,2020-01-22T10:33:25Z,2020-05-03T20:34:58Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas
df = pandas.read_csv(""s3://noaa-global-hourly-pds/2019/72047299999.csv"",
        parse_dates=[""DATE""],
        dtype=dict.fromkeys([""STATION"", ""NAME"", ""REPORT_TYPE"",
            ""QUALITY_CONTROL"", ""WND"", ""CIG"", ""VIS"", ""TMP"", ""DEW"", ""SLP"", ""AA1"",
            ""AA2"", ""AY1"", ""AY2"", ""GF1"", ""MW1"", ""REM""], pandas.StringDtype()))
df.to_hdf(""/tmp/fubar.h5"", ""root"")
pandas.read_hdf(""/tmp/fubar.h5"", ""root"")
```
#### Problem description

The code fails to write the HDF5.  It fails with an exception `AttributeError: 'StringArray' object has no attribute 'size'`:

```
Traceback (most recent call last):
  File ""mwe15.py"", line 7, in <module>
    df.to_hdf(""/tmp/fubar.h5"", ""root"")
  File ""/media/nas/x21324/miniconda3/envs/py37e/lib/python3.7/site-packages/pandas/core/generic.py"", line 2504, in to_hdf
    encoding=encoding,
  File ""/media/nas/x21324/miniconda3/envs/py37e/lib/python3.7/site-packages/pandas/io/pytables.py"", line 282, in to_hdf
    f(store)
  File ""/media/nas/x21324/miniconda3/envs/py37e/lib/python3.7/site-packages/pandas/io/pytables.py"", line 274, in <lambda>
    encoding=encoding,
  File ""/media/nas/x21324/miniconda3/envs/py37e/lib/python3.7/site-packages/pandas/io/pytables.py"", line 1042, in put
    errors=errors,
  File ""/media/nas/x21324/miniconda3/envs/py37e/lib/python3.7/site-packages/pandas/io/pytables.py"", line 1709, in _write_to_group
    data_columns=data_columns,
  File ""/media/nas/x21324/miniconda3/envs/py37e/lib/python3.7/site-packages/pandas/io/pytables.py"", line 3100, in write
    self.write_array(f""block{i}_values"", blk.values, items=blk_items)
  File ""/media/nas/x21324/miniconda3/envs/py37e/lib/python3.7/site-packages/pandas/io/pytables.py"", line 2906, in write_array
    empty_array = value.size == 0
AttributeError: 'StringArray' object has no attribute 'size'
```

It does write a file `/tmp/fubar.h5`, but pandas cannot read it.  Commenting out the writing bit and leaving only the reading bit results in:

```
Traceback (most recent call last):
  File ""mwe15.py"", line 8, in <module>
    pandas.read_hdf(""/tmp/fubar.h5"", ""root"")
  File ""/media/nas/x21324/miniconda3/envs/py37e/lib/python3.7/site-packages/pandas/io/pytables.py"", line 428, in read_hdf
    auto_close=auto_close,
  File ""/media/nas/x21324/miniconda3/envs/py37e/lib/python3.7/site-packages/pandas/io/pytables.py"", line 814, in select
    return it.get_result()
  File ""/media/nas/x21324/miniconda3/envs/py37e/lib/python3.7/site-packages/pandas/io/pytables.py"", line 1829, in get_result
    results = self.func(self.start, self.stop, where)
  File ""/media/nas/x21324/miniconda3/envs/py37e/lib/python3.7/site-packages/pandas/io/pytables.py"", line 798, in func
    return s.read(start=_start, stop=_stop, where=_where, columns=columns)
  File ""/media/nas/x21324/miniconda3/envs/py37e/lib/python3.7/site-packages/pandas/io/pytables.py"", line 3068, in read
    blk_items = self.read_index(f""block{i}_items"")
  File ""/media/nas/x21324/miniconda3/envs/py37e/lib/python3.7/site-packages/pandas/io/pytables.py"", line 2752, in read_index
    variety = _ensure_decoded(getattr(self.attrs, f""{key}_variety""))
  File ""/media/nas/x21324/miniconda3/envs/py37e/lib/python3.7/site-packages/tables/attributeset.py"", line 301, in __getattr__
    ""'%s'"" % (name, self._v__nodepath))
AttributeError: Attribute 'block0_items_variety' does not exist in node: '/root'
Closing remaining open files:/tmp/fubar.h5...done
```

#### Expected Output

I expect a well-formed HDF5 file to be written, and the `pandas.read_hdf` command to return a `DataFrame` identical to the one written to disk just before.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.12.14-lp150.12.82-default
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.0rc0
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.6.0.post20191101
Cython           : 0.29.14
pytest           : 5.3.0
hypothesis       : None
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.0
s3fs             : 0.4.0
scipy            : 1.3.2
sqlalchemy       : 1.3.11
tables           : 3.6.1
tabulate         : None
xarray           : 0.14.1
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.46.0
</details>
"
325932717,21186,DataFrame slicing TimedeltaIndex with str,Tux1,closed,2018-05-24T01:32:54Z,2020-05-03T21:36:09Z,"#### Code Sample, a copy-pastable example if possible

```python
cols = ['15:00:00', '15:01:00', '15:02:00', '15:02:30']
cols_td = pd.to_timedelta(cols)

df = pd.DataFrame(0, index=range(4), columns=cols_td)

vw1 = df.loc[:, :cols[-2]]
vw2 = df.loc[:, :cols_td[-2]]
print(vw1)
print(vw2)

print('Statement below must be equal')
print(vw1.equals(vw2))
```
#### Problem description

vw1 is wrong. vw2 is correct
Slicing TimdeltaIndex DataFrame with string doesn't seem to work. It doesn't take into account seconds 

#### Expected Output
Both views must be equal

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.3.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 79 Stepping 1, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.20.3
pytest: 3.2.1
pip: 9.0.1
setuptools: 36.5.0.post20170921
Cython: 0.26.1
numpy: 1.13.3
scipy: 0.19.1
xarray: None
IPython: 6.1.0
sphinx: 1.6.3
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.2
feather: None
matplotlib: 2.1.0
openpyxl: 2.4.8
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.2
lxml: 4.1.0
bs4: 4.6.0
html5lib: 0.999999999
sqlalchemy: 1.1.13
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
pandas_gbq: None
pandas_datareader: None

</details>
"
368225350,23059,misrepresented fractional seconds in timestamps and timedeltas,cbertinato,open,2018-10-09T14:00:09Z,2020-05-03T21:40:00Z,"#### Code Sample

```python
>>> timestamp = 1490193630.8
>>> pd.to_timedelta(timestamp, unit='s')
Timedelta('17247 days 14:40:30.799999')
```

However, this works:
```python
>>> pd.to_timedelta(timestamp*1e9, unit='ns')
Timedelta('17247 days 14:40:30.800000')
```
#### Problem description

Depending upon the value of the float to be converted to a timestamp or timedelta and on the unit, the resulting timestamp and timedelta will occasionally misrepresent the fractional part of the input float.

#### Expected Output
`Timedelta('17247 days 14:40:30.8')`
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: a4482db46535b018053b4351689ccc3c79257d8c
python: 3.6.5.final.0
python-bits: 64
OS: Darwin
OS-release: 17.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.0.dev0+716.ga4482db46
pytest: 3.5.1
pip: 9.0.3
setuptools: 39.0.1
Cython: 0.28.2
numpy: 1.14.3
scipy: 1.1.0
pyarrow: 0.9.0.post1
xarray: 0.10.3
IPython: 6.4.0
sphinx: 1.7.4
patsy: None
dateutil: 2.7.3
pytz: 2018.4
blosc: 1.5.1
bottleneck: 1.2.1
tables: 3.4.3
numexpr: 2.6.5
feather: 0.4.0
matplotlib: 2.2.2
openpyxl: 2.5.3
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.4
lxml: 4.2.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.7
pymysql: 0.8.1
psycopg2: None
jinja2: 2.10
s3fs: 0.1.5
fastparquet: 0.1.5
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
479833710,27881,timedelta is considered as np.number?,charlesdong1991,open,2019-08-12T20:24:27Z,2020-05-03T21:47:27Z,"When querying numbers from dataframe with `df.select_dtypes`, if i choose `include=np.number`, then timedelta will be also selected. But based on documentation, this should not happen. I checked the tests, it seems allow this to happen on purpose, then how should I only select numeric columns? should DOC be changed at least?

```python
df = pd.DataFrame({'a': pd.date_range(""2012-01-01"", periods=2), 
                   'b': np.random.randn(2), 
                   'd': pd.date_range(""2012-01-01"", periods=2).astype(str),
                   'e': pd.date_range(""2012-01-01"", periods=2, tz='UTC'),
                   'f': pd.timedelta_range(""1 days"", periods=2)
})
df.select_dtypes(include=['number'])
```
If diving to code, it seems because `timedelta` also is represented by `np.timedelta64` and is subclass of `np.number`.

Output:
the column b, f are selected

Expected Output:
only column b will be selected as a dataframe. Or a doc change instead?
"
358391972,22650,BUG: Dont include deleted rows from sas7bdat files (#15963),troels,closed,2018-09-09T16:35:50Z,2020-05-04T05:30:40Z,"Sas7bdat files may contain rows which are actually deleted.
 
After reverse engineering the file format a bit, I found that   
if the page_type has bit 7 set, there is a bitmap following
the normal row data with a bit set for a given row if it has been
deleted. Use that information to not include deleted rows in
the resulting dataframe.

This PR is built on top of #22628

I've added two test-cases one from the issue  and another constructed
example, which tests parsing on different page types and across multiple sas7bdat-pages. 

The constructed example is rather large, mainly to have a test case where the data 
flows across several pages.

- [X] closes #15963
- [X] tests added / passed
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

"
611812414,33967,pandas.read_csv outputs two data frames in Jupiter Notebook ,BastianZim,closed,2020-05-04T11:46:38Z,2020-05-04T12:30:35Z,"### My code:

First cell:
```python
import pandas as pd
test = pd.read_csv(""dump.csv"")
```

Second cell:
```python
test
```

Output:
<img width=""1114"" alt=""Screenshot 2020-05-04 at 13 42 52"" src=""https://user-images.githubusercontent.com/10774221/80962334-284bd980-8e0d-11ea-9d3e-456a112770d4.png"">

and 

```python
type(test)
```

Output: 
<img width=""1113"" alt=""Screenshot 2020-05-04 at 13 43 45"" src=""https://user-images.githubusercontent.com/10774221/80962402-47e30200-8e0d-11ea-9a56-4bf387b37257.png"">

It seems like the command imports two dataframes but only in Jupyter Notebook.

I tested it in Spyder and there everything works fine.

### Setup:

The file I am reading: [sp500.csv.zip](https://github.com/pandas-dev/pandas/files/4574412/sp500.csv.zip)


```python
pd.show_versions()
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.4.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.0.3
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.4.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.0.3
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```"
600558337,33571,BUG: Debug grouped quantile with NA values,dsaxton,closed,2020-04-15T20:10:13Z,2020-05-04T12:48:47Z,"- [x] closes #33569
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
610976952,33933,PERF: Datetimelike lookups,jbrockmendel,closed,2020-05-01T21:26:37Z,2020-05-04T13:26:01Z,cc @TomAugspurger this should address at least some of the regressions you posted about this morning.
611490265,33958,CLN: remove ABCDateOffset,jbrockmendel,closed,2020-05-03T19:39:46Z,2020-05-04T15:11:51Z,
611482587,33957,CLN: remove ABCSparseArray,jbrockmendel,closed,2020-05-03T19:01:16Z,2020-05-04T15:15:08Z,
611538832,33961,PERF: 30% faster is_period_object checks,jbrockmendel,closed,2020-05-04T00:03:10Z,2020-05-04T15:33:44Z,"Doing an appropriate isinstance check occurs all in C-space.

```
In [1]: import pandas as pd                                                                                                                                                   
In [2]: from pandas._libs.lib import *                                                                                                                                        
In [3]: obj = pd.Period(2004, ""A"")                                                                                                                                            

In [4]: %timeit is_scalar(obj)                                                                                                                                                
78.4 ns ± 0.761 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR
101 ns ± 1.74 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- master

In [5]: %timeit is_period(obj)                                                                                                                                                
54.1 ns ± 0.548 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR
80.7 ns ± 1.14 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- master

In [6]: %timeit is_period([])                                                                                                                                                 
50.1 ns ± 1.02 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR
340 ns ± 26.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- master

In [7]: %timeit is_scalar([])                                                                                                                                                 
65.6 ns ± 0.421 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR
67.1 ns ± 0.293 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- master

In [8]: obj2 = object()                                                                                                                                                       

In [9]: %timeit is_scalar(obj2)                                                                                                                                               
544 ns ± 6.97 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- PR
782 ns ± 6.3 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- master

```"
600891257,33586,API: Specify the dtype of new columns added in reindex,burk,closed,2020-04-16T09:33:23Z,2020-05-04T15:56:10Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```
df = pd.DataFrame({'x': [np.nan, 1., 2.]}).astype(pd.SparseDtype(""float"", np.nan))
df = df.reindex(['x', 'y'], axis='columns')
df.info()
```
```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3 entries, 0 to 2
Data columns (total 2 columns):
x    -4 non-null Sparse[float64, nan]
y    -6 non-null float64
dtypes: Sparse[float64, nan](1), float64(1)
memory usage: 176.0 bytes
```

#### Problem description

When re-indexing the columns of a sparse dataframe, new columns are not sparse. This is problematic especially since the new columns would be completely sparse.

#### Expected Output

I'd expect that the new column was also of type `Sparse[float64, 0.0]`.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-45-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.2
Cython           : 0.29.15
pytest           : 5.3.5
hypothesis       : None
sphinx           : 2.4.1
blosc            : None
feather          : 0.4.0
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : 0.9.3
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : 1.3.13
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
353428997,22485,DOC: ENH: Add a table of contents to the PDF documentation,dave-kirby,closed,2018-08-23T15:13:41Z,2020-05-04T15:58:27Z,"For python packages that I use a lot I like to have a PDF version of the documentation on my tablet for reference.  The PDF version of the Pandas doc is far less useful than most because it does not have a Table of Contents in the sidebar.  This makes it extremely difficult to quickly look up anything, especially as the first 450 pages are the change log.   

Compare the [Pandas PDF](https://pandas.pydata.org/pandas-docs/stable/pandas.pdf) with [Numpy](https://docs.scipy.org/doc/numpy-1.11.0/numpy-ref-1.11.0.pdf) or [Scipy](https://docs.scipy.org/doc/scipy-0.16.0/scipy-ref-0.16.0.pdf) PDFs to see what I mean.
"
611976966,33972,Backport PR #33566 on branch 1.0.x (CI: Fix jedi deprecation warning for 0.17.0 on IPython),meeseeksmachine,closed,2020-05-04T15:46:22Z,2020-05-04T16:49:24Z,Backport PR #33566: CI: Fix jedi deprecation warning for 0.17.0 on IPython
611980307,33973,Backport PR #33080 on branch 1.0.x (CI troubleshoot azure),meeseeksmachine,closed,2020-05-04T15:50:43Z,2020-05-04T16:55:22Z,Backport PR #33080: CI troubleshoot azure
611251277,33945,PERF: use fastpath for is_categorical_dtype calls,jbrockmendel,closed,2020-05-02T18:48:49Z,2020-05-04T17:01:58Z,This doesn't get all of them
608319194,33847,CI: Skip permissions test when running as sudo,TomAugspurger,closed,2020-04-28T13:28:18Z,2020-05-04T17:40:39Z,Closes https://github.com/pandas-dev/pandas/issues/33210
531683153,29987," "".fillna(method='bfill', limit=7) hangs with ""specifying a limit for fillna has not been implemented yet""",hkn0509,closed,2019-12-03T03:09:13Z,2020-05-04T18:31:00Z,"Hi! Please help me with this problem. Thank you in advance for your help.

Source code and data are attached.

Executing this statement: 
	  **labeled_features = labeled_features.fillna(method='bfill', limit=7)  ## where ""labeled_features"" is a dataframe**
it hangs with this error:	  
          **""NotImplementedError: specifying a limit for fillna has not been implemented yet""**

The code belongs to ""Modeling Guide for Predictive Maintenance Python 3 Notebook"",  downloaded from
	""https://notebooks.azure.com/Microsoft/projects/PredictiveMaintenance"".  

Operating System: Windows 10
Google Chrome

I tried with Anaconda3-5.2.0-Windows-x86_64 (Python 3.6), and afterwards I tried with Python 3.8, Pandas 0.25.3, and iPython. 
Actually with Anaconda3-2019.10-Windows-x86_64, Jupyter 6.0.1, Pandas 0.25.1, Python 3.7.4, Numpy 1.16.5

---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-27-259154593492> in <module>
[PredictiveMaintenance_SourceAndData.zip](https://github.com/pandas-dev/pandas/files/3914325/PredictiveMaintenance_SourceAndData.zip)

      1 labeled_features = final_feat.merge(failures, on=['datetime', 'machineID'], how='left')
----> 2 labeled_features = labeled_features.fillna(method='bfill', limit=7) # fill backward up to 24h
      3 labeled_features = labeled_features.fillna('none')
      4 labeled_features.head()

~\Anaconda3\lib\site-packages\pandas\core\frame.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs)
   4242             limit=limit,
   4243             downcast=downcast,
-> 4244             **kwargs
   4245         )
   4246 

~\Anaconda3\lib\site-packages\pandas\core\generic.py in fillna(self, value, method, axis, inplace, limit, downcast)
   6235                 inplace=inplace,
   6236                 coerce=True,
-> 6237                 downcast=downcast,
   6238             )
   6239         else:

~\Anaconda3\lib\site-packages\pandas\core\internals\managers.py in interpolate(self, **kwargs)
    567 
    568     def interpolate(self, **kwargs):
--> 569         return self.apply(""interpolate"", **kwargs)
    570 
    571     def shift(self, **kwargs):

~\Anaconda3\lib\site-packages\pandas\core\internals\managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)
    436                     kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy)
    437 
--> 438             applied = getattr(b, f)(**kwargs)
    439             result_blocks = _extend_blocks(applied, result_blocks)
    440 

~\Anaconda3\lib\site-packages\pandas\core\internals\blocks.py in interpolate(self, method, axis, inplace, limit, fill_value, **kwargs)
   1947         values = self.values if inplace else self.values.copy()
   1948         return self.make_block_same_class(
-> 1949             values=values.fillna(value=fill_value, method=method, limit=limit),
   1950             placement=self.mgr_locs,
   1951         )

~\Anaconda3\lib\site-packages\pandas\util\_decorators.py in wrapper(*args, **kwargs)
    206                 else:
    207                     kwargs[new_arg_name] = new_arg_value
--> 208             return func(*args, **kwargs)
    209 
    210         return wrapper

~\Anaconda3\lib\site-packages\pandas\core\arrays\categorical.py in fillna(self, value, method, limit)
   1842         if limit is not None:
   1843             raise NotImplementedError(
-> 1844                 ""specifying a limit for fillna has not "" ""been implemented yet""
   1845             )
   1846 

NotImplementedError: specifying a limit for fillna has not been implemented yet

[PredictiveMaintenance_SourceAndData.zip](https://github.com/pandas-dev/pandas/files/3914330/PredictiveMaintenance_SourceAndData.zip)

"
110865220,11288,ENH: Add window to exponentially weighted moment functions,matthewgilbert,closed,2015-10-11T16:12:52Z,2020-05-04T18:34:26Z,"Currently there is no way to specify a rolling window for the exponentially weighted functions. From the [docs](http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-moment-functions) the weighting function is giving by

![eq1](https://cloud.githubusercontent.com/assets/2401026/10417648/45d29b66-7011-11e5-97b6-2f0a137f86eb.png)

However I am wondering if it is straightforward to add a _window_ parameter, similar to the rolling window statistics. This would change the formula to look like

![eq2](https://cloud.githubusercontent.com/assets/2401026/10417649/4c34e6d0-7011-11e5-97bb-b99b8c7677f5.png)
"
612092445,33975,Backport PR #33309 on branch 1.0.x (DOC: include Offset.__call__ to autosummary to fix sphinx warning),meeseeksmachine,closed,2020-05-04T18:53:10Z,2020-05-04T19:56:48Z,Backport PR #33309: DOC: include Offset.__call__ to autosummary to fix sphinx warning
604346807,33717,BUG: DTA/TDA/PA setitem incorrectly allowing i8,jbrockmendel,closed,2020-04-21T23:26:39Z,2020-05-04T20:03:22Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
596245451,33386,"BUG: ""nan"" as a string instead of missing value.",vasili111,closed,2020-04-08T01:23:20Z,2020-05-04T20:17:03Z,"*Sorry I cannot share the data. I tried to make test data but it does not gives same error or different missing values as described below.*

*Added more info at bottom about `pd.NA`*

I am loading data with code:

    df = pd.read_csv(""C:/data.csv"")

When loading data I am getting this warning:

    C:\Users\User1\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py:3063: DtypeWarning: Columns (162,247,274,292,304,316,321,335,345,347,357,379,389,390,393,395,400,401,420,424,447,462,465,467,478,481,534,536,538,570,616,632,653,666,675,691,707,754,758,762,766,770,774,778,782,784,785,786,788,789,790,792,793,794,796,797,798,800,801,802,804,805,806,808,809,810,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,867,868,871,872,875,876,880,1367,1368,1370,1371,1373,1374,1376,1377,1379,1380,1382,1383,1385,1386,1388,1389,1391,1392,1394,1395,1397,1398,1400,1401,1403,1404,1406,1407,1409,1410,1412,1413,1415,1416,1418,1419,1421,1422,1424,1425,2681) have mixed types.Specify dtype option on import or set low_memory=False.
      interactivity=interactivity, compiler=compiler, result=result)

As I understood this warning is not a problem and I can ignore it.

*For example code with expected output look at the bottom of question (error wich is described below is not replicated with that example because of different data!)*

After I am running this code:


    
    import pandas as pd
    import numpy as np
    
    
    
    col1 = [""var1"", ""var3"", ""var5""]
    col2 = [""var2"", ""var4"", ""var6""]
    colR = [""Result1"", ""Result2"", ""Result3""]
    
    s1 = df[col1].isnull().to_numpy()
    s2 = df[col2].isnull().to_numpy()
    
    conditions = [~s1 & ~s2, s1 & s2, ~s1 & s2, s1 & ~s2]
    choices = [""Both values"", np.nan, df[col1], df[col2]]
    
    df = pd.concat([df, pd.DataFrame(np.select(conditions, choices), columns=colR, index=df.index)], axis=1)
    

Newly created columns apon running code above contain `nan` but colums that are loaded from `csv` file contain `NaN`.

After running `df['var1'].value_counts(dropna=False)`, I am getting output:

    NaN    3453
    0.0    3002
    1.0     314
    Name: var1, dtype: int64

After running `df['Result1'].value_counts(dropna=False)`, I am getting output:

    0.0            3655
    nan            2665
    1.0             407
    Both values      42
    Name: Result1, dtype: int64

Notice that `var1` contains `NaN` values but `Result1` contains `nan` values.

When I run `df['var1'].value_counts(dropna=False).loc[[np.nan]]` I am getting output:

    NaN    3453
    Name: weeklyivr_q1, dtype: int64

When I run `df['Result1'].value_counts(dropna=False).loc[[np.nan]]` I am getting error (variable names in error are different but key idea is that there are no missing values):

    ---------------------------------------------------------------------------
    KeyError                                  Traceback (most recent call last)
    <ipython-input-52-0daeac75fdb4> in <module>
         27 #combined_IVR[""weeklyivr_q1""].value_counts(dropna=False)
         28 #combined_IVR[""my_weekly_ivr_1""].value_counts(dropna=False).loc[[""Both values""]]
    ---> 29 combined_IVR[""my_weekly_ivr_1""].value_counts(dropna=False).loc[[np.nan]]
         30 #combined_IVR[""weeklyivr_q1""].value_counts(dropna=False).loc[[np.nan]]
    
    ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\indexing.py in __getitem__(self, key)
       1764 
       1765             maybe_callable = com.apply_if_callable(key, self.obj)
    -> 1766             return self._getitem_axis(maybe_callable, axis=axis)
       1767 
       1768     def _is_scalar_access(self, key: Tuple):
    
    ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\indexing.py in _getitem_axis(self, key, axis)
       1950                     raise ValueError(""Cannot index with multidimensional key"")
       1951 
    -> 1952                 return self._getitem_iterable(key, axis=axis)
       1953 
       1954             # nested tuple slicing
    
    ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\indexing.py in _getitem_iterable(self, key, axis)
       1591         else:
       1592             # A collection of keys
    -> 1593             keyarr, indexer = self._get_listlike_indexer(key, axis, raise_missing=False)
       1594             return self.obj._reindex_with_indexers(
       1595                 {axis: [keyarr, indexer]}, copy=True, allow_dups=True
    
    ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\indexing.py in _get_listlike_indexer(self, key, axis, raise_missing)
       1549 
       1550         self._validate_read_indexer(
    -> 1551             keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing
       1552         )
       1553         return keyarr, indexer
    
    ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing)
       1636             if missing == len(indexer):
       1637                 axis_name = self.obj._get_axis_name(axis)
    -> 1638                 raise KeyError(f""None of [{key}] are in the [{axis_name}]"")
       1639 
       1640             # We (temporarily) allow for some missing keys with .loc, except in
    
    KeyError: ""None of [Float64Index([nan], dtype='float64')] are in the [index]""

When I am running `df['Result1'].value_counts(dropna=False).loc[['nan']]` I am getting:

    nan    2665
    Name: my_weekly_ivr_1, dtype: int64

So `nan` in 'Result1' column is string. 


If i replace `choices = [""Both values"", np.nan, df[col1], df[col2]]` with `choices = [""Both values"", pd.NA, df[col1], df[col2]]` and after run:

    df['Result1'].value_counts(dropna=False).loc[[np.nan]]

I am getting output: 

    NaN    2665
    Name: Result1, dtype: int64

So in this case `np.nan` produces string and `pd.NA` missing value.

<br>

**Question:**

Why am getting `nan` in 'Result1' column when using np.nan? What can be a reason and how to fix this?

--------

Example code with expected output:

    import pandas as pd
    import numpy as np
         
    df = pd.DataFrame({ 'var1': ['a', 'b', 'c',np.nan, np.nan],
                       'var2': [1, 2, np.nan , 4, np.nan], 
                       'var3': [np.nan , ""x"", np.nan, ""y"", ""z""],
                       'var4': [np.nan , 4, np.nan, 5, 6],
                       'var5': [""a"", np.nan , ""b"", np.nan, ""c""],
                       'var6': [1, np.nan , 2, np.nan, 3]
                     })

    
    
    col1 = [""var1"", ""var3"", ""var5""]
    col2 = [""var2"", ""var4"", ""var6""]
    colR = [""Result1"", ""Result2"", ""Result3""]
    
    s1 = df[col1].isnull().to_numpy()
    s2 = df[col2].isnull().to_numpy()
    
    conditions = [~s1 & ~s2, s1 & s2, ~s1 & s2, s1 & ~s2]
    choices = [""Both values"", np.nan, df[col1], df[col2]]
    
    df = pd.concat([df, pd.DataFrame(np.select(conditions, choices), columns=colR, index=df.index)], axis=1)


The result ( `df` ) looks like:

      var1  var2 var3  var4 var5  var6      Result1      Result2      Result3
    0    a   1.0  NaN   NaN    a   1.0  Both values          NaN  Both values
    1    b   2.0    x   4.0  NaN   NaN  Both values  Both values          NaN
    2    c   NaN  NaN   NaN    b   2.0            c          NaN  Both values
    3  NaN   4.0    y   5.0  NaN   NaN            4  Both values          NaN
    4  NaN   NaN    z   6.0    c   3.0          NaN  Both values  Both values
"
612129325,33976,Backport PR #31146 on branch 1.0.x (Remove possibly illegal test data),meeseeksmachine,closed,2020-05-04T19:57:28Z,2020-05-04T20:59:49Z,Backport PR #31146: Remove possibly illegal test data
612077438,33974,REF: use is_foo_object checks instead of _typ checks,jbrockmendel,closed,2020-05-04T18:28:21Z,2020-05-04T21:12:20Z,
612132276,33977,DOC: whatsnew for 33717,jbrockmendel,closed,2020-05-04T20:02:42Z,2020-05-04T22:30:50Z,xref #33717 
512820850,29235,Categories reordered to alphabetical when plotting histograms,dmey,closed,2019-10-26T10:44:24Z,2020-05-05T00:50:16Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

import matplotlib.pyplot as plt

df = pd.DataFrame({
    'width': [0.7, 0.2, 0.15, 0.2, 1.1],
    'length': [1.5, 0.5, 1.2, 0.9, 3]
    }, index= ['pig', 'rabbit', 'duck', 'chicken', 'horse'])

# Whether or not `column` is passed the order is still reordered to alphabetical.
df.hist(column=['width','length'])
plt.show()
```
#### Problem description

When plotting histograms from a DataFrame (see code-snippet above) the order of subplots is not maintained. Passing `column` does not appear to have any effect on the ordering either. 


#### Expected Output

Plots/subplots order equals column order in DataFrame.

#### Output of ``pd.show_versions()``

```
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.2
pytest: 5.2.1
pip: 19.1.1
setuptools: 41.0.1
Cython: None
numpy: 1.16.4
scipy: 1.2.2
pyarrow: None
xarray: 0.12.1
IPython: 7.5.0
sphinx: None
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.1.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: 4.7.1
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
```
"
612285049,33986,QST: Should list-likes with pd.NA have an inferred nullable dtype if possible?,dsaxton,closed,2020-05-05T02:36:39Z,2020-05-05T08:17:26Z,"It looks like in each of these cases the dtype is inferred as object, but does it make sense to try to use a nullable extension type when possible?
```python
[ins] In [1]: pd.Series([1, 2, pd.NA]).dtype                                                                                                                                                                 
Out[1]: dtype('O')

[ins] In [2]: pd.Series([True, False, pd.NA]).dtype                                                                                                                                                          
Out[2]: dtype('O')

[ins] In [3]: pd.Series([""a"", ""b"", pd.NA]).dtype                                                                                                                                                             
Out[3]: dtype('O')
```
"
598651905,33513,BUG: Fix Categorical.min / max bug,dsaxton,closed,2020-04-13T03:40:56Z,2020-05-05T09:07:23Z,"- [ ] closes #33450
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
592805125,33241,tostring->tobytes,jbrockmendel,closed,2020-04-02T17:28:22Z,2020-05-05T09:10:25Z,"- [x] closes #33238
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
612454103,33993,Backport PR #33241 on branch 1.0.x (tostring->tobytes),meeseeksmachine,closed,2020-05-05T09:10:10Z,2020-05-05T09:42:56Z,Backport PR #33241: tostring->tobytes
612431834,33990,CI: test_unsupported_other fails on pyarrow 0.17,simonjayhawkins,closed,2020-05-05T08:31:25Z,2020-05-05T09:43:56Z,"xref https://github.com/pandas-dev/pandas/issues/33300#issuecomment-623690451, https://github.com/pandas-dev/pandas/pull/33422#discussion_r406240082"
612487645,33994,CI/DEP: Use numba.extending.is_jitted for numba version > 0.49.0,simonjayhawkins,closed,2020-05-05T10:10:31Z,2020-05-05T10:39:05Z,xref #33687
612509861,33995,BLD: recursive inclusion of DLLs in package data (#33246),simonjayhawkins,closed,2020-05-05T10:49:44Z,2020-05-05T11:27:41Z,xref #33246
611418590,33954,DOC: update conf.py to use new numpy doc url,codeloop,closed,2020-05-03T13:50:10Z,2020-05-05T11:30:45Z,"- [X] closes #33953
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
610339589,33902,DEPR: Deprecate try_sort,charlesdong1991,closed,2020-04-30T19:45:14Z,2020-05-05T12:23:21Z,"xref #33336 

While working on another PR, and @WillAyd brought up `try_sort`, and after a quick look, I don't think `try_sort` is needed in that check method in `indexes/api.py`. (I also dont think it is right to have try_sort there), so deprecate it.

"
612531895,33996,Backport PR #33954 on branch 1.0.x (DOC: update conf.py to use new numpy doc url),meeseeksmachine,closed,2020-05-05T11:30:04Z,2020-05-05T12:27:15Z,Backport PR #33954: DOC: update conf.py to use new numpy doc url
611958770,33970,DOC: start 1.0.4,simonjayhawkins,closed,2020-05-04T15:21:33Z,2020-05-05T12:37:49Z,"NOTE: opened against 1.0.x branch as to not commit to release at this stage.

I think could backport a few PRs moving the whatsnews and only after a release remove from 1.1 whatsnew on master. This gives us the chance to easily abandon the release at any time."
612557657,33998,Backport PR #33462 on branch 1.0.x (BUG: None converted to NaN after …,simonjayhawkins,closed,2020-05-05T12:16:39Z,2020-05-05T12:55:24Z,"…groupby first and last)

xref #33462

fixes regression in 1.0.2 (#32800)"
608307651,33845,TestDatetimeIndex.test_reindex_with_same_tz 32-bit CI failing in MacPython,TomAugspurger,closed,2020-04-28T13:13:30Z,2020-05-05T14:18:01Z,"https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=34375&view=logs&j=517fe804-fa30-5dc2-1413-330699242c05&t=2e128ad5-2f7f-5333-3f34-c85b8fbc7250&l=1267

```
=================================== FAILURES ===================================
_________________ TestDatetimeIndex.test_reindex_with_same_tz __________________
[gw1] linux -- Python 3.7.0 /venv/bin/python

        rng_a = date_range(""2010-01-01"", ""2010-01-02"", periods=24, tz=""utc"")
        rng_b = date_range(""2010-01-01"", ""2010-01-02"", periods=23, tz=""utc"")
        result1, result2 = rng_a.reindex(
            rng_b, method=""nearest"", tolerance=timedelta(seconds=20)
        )
        expected_list1 = [
            ""2010-01-01 00:00:00"",
            ""2010-01-01 01:05:27.272727272"",
            ""2010-01-01 02:10:54.545454545"",
            ""2010-01-01 03:16:21.818181818"",
            ""2010-01-01 04:21:49.090909090"",
            ""2010-01-01 05:27:16.363636363"",
            ""2010-01-01 06:32:43.636363636"",
            ""2010-01-01 07:38:10.909090909"",
            ""2010-01-01 08:43:38.181818181"",
            ""2010-01-01 09:49:05.454545454"",
            ""2010-01-01 10:54:32.727272727"",
            ""2010-01-01 12:00:00"",
            ""2010-01-01 13:05:27.272727272"",
            ""2010-01-01 14:10:54.545454545"",
            ""2010-01-01 15:16:21.818181818"",
            ""2010-01-01 16:21:49.090909090"",
            ""2010-01-01 17:27:16.363636363"",
            ""2010-01-01 18:32:43.636363636"",
            ""2010-01-01 19:38:10.909090909"",
            ""2010-01-01 20:43:38.181818181"",
            ""2010-01-01 21:49:05.454545454"",
            ""2010-01-01 22:54:32.727272727"",
            ""2010-01-02 00:00:00"",
        ]
        expected1 = DatetimeIndex(
            expected_list1, dtype=""datetime64[ns, UTC]"", freq=None,
        )
        expected2 = np.array([0] + [-1] * 21 + [23], dtype=np.int64,)
        tm.assert_index_equal(result1, expected1)
>       tm.assert_numpy_array_equal(result2, expected2)
E       AssertionError: numpy array are different
E       
E       Attribute ""dtype"" are different
E       [left]:  int32
E       [right]: int64

```"
611645076,33963,BUG: memory_usage(deep=True) of dtype='string' is wrong,mizuy,closed,2020-05-04T06:57:54Z,2020-05-05T14:18:58Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
In [4]: s = pd.Series(['a','b','c'],dtype='object')
   ...: print(s.memory_usage())
   ...: print(s.memory_usage(deep=True))
152
326

In [5]: s = pd.Series(['a','b','c'],dtype='string')
   ...: print(s.memory_usage())
   ...: print(s.memory_usage(deep=True))
152
152
```

#### Problem description

memory_usage(deep=True) for new 'string' dtype is not 'deep' size.

#### Expected Output
In [3]: s = pd.Series(['a','b','c'],dtype='string')
   ...: print(s.memory_usage())
   ...: print(s.memory_usage(deep=True))
152
326

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 20.1
setuptools       : 40.8.0
Cython           : 0.29.16
pytest           : 5.2.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.2
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : 0.15.0
pytables         : None
pytest           : 5.2.1
pyxlsb           : None
s3fs             : 0.2.2
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
tabulate         : 0.8.5
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.2
numba            : None
</details>
"
611899161,33969,Fix expected dtype in test_reindex_with_same_tz,TomAugspurger,closed,2020-05-04T14:02:12Z,2020-05-05T14:26:16Z,Closes https://github.com/pandas-dev/pandas/issues/33845
612279605,33985,BUG: fix memory_usage method with deep of StringArray,kotamatsuoka,closed,2020-05-05T02:16:20Z,2020-05-05T14:33:28Z,"- [ ] closes #33963
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
612592974,34000,Backport PR #33761 on branch 1.0.x (REGR: fix DataFrame reduction wit…,simonjayhawkins,closed,2020-05-05T13:14:40Z,2020-05-05T14:39:31Z,"…h EA columns and numeric_only=True)

xref #33761

fixes regression in 1.0.0 (#33256)"
585198601,32870,DOC: Remove latest whatsnew from header,TomAugspurger,closed,2020-03-20T16:48:07Z,2020-05-05T15:28:10Z,"In favor of going via the ""Release notes"" which stays in the header.

Closes https://github.com/pandas-dev/pandas/issues/32748"
590685722,33157,Backport PR #33102 on branch 1.0.x (PERF: fix performance regression in memory_usage(deep=True) for object dtype),meeseeksmachine,closed,2020-03-31T00:12:22Z,2020-05-05T15:55:25Z,Backport PR #33102: PERF: fix performance regression in memory_usage(deep=True) for object dtype
587665252,33012,PERF: Performance regression with memory_usage(deep=True) on `object` columns,Lawrr,closed,2020-03-25T12:35:52Z,2020-05-05T16:04:39Z,"#### Code Sample, a copy-pastable example if possible

```python
import time

import pandas as pd
import numpy as np

df = pd.DataFrame({
    ""a"": np.empty(10000000),
    ""b"": np.empty(10000000),
})

df[""a""] = df[""a""].astype(""object"")

s = time.time()
mem = df.memory_usage(deep=True)
print(""memory_usage(deep=True) took %.4fsecs"" % (time.time() - s))
```

#### Problem description

Performance of `memory_usage(deep=True)` on `object` columns seems to have regressed significantly since v0.23.4. Once in v0.24.0, and another regression in v1.0.0 that remains in v1.0.3.

**Output**
v1.0.3
```
memory_usage(deep=True) took 26.4566secs
```
v0.24.0
```
memory_usage(deep=True) took 6.0479secs
```
v0.23.4
```
memory_usage(deep=True) took 0.4633secs
```

Removing `df[""a""] = df[""a""].astype(""object"")` reverts it back to the expected magnitude of speed in v1.0.3:
```
memory_usage(deep=True) took 0.0024secs
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.16.0-77-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_AU.UTF-8
LOCALE           : en_AU.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
612202590,33980,"BUG: Series.update() raises ValueError if dtype=""string""",RagBlufThim,closed,2020-05-04T22:12:54Z,2020-05-05T16:27:25Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
a = pd.Series([""a"", None, ""c""], dtype=""string"")
b = pd.Series([None, ""b"", None], dtype=""string"")
a.update(b)

```
results in:
```python-traceback
Traceback (most recent call last):

  File ""<ipython-input-15-b9da8f25067a>"", line 1, in <module>
    a.update(b)

  File ""C:\tools\anaconda3\envs\Simple\lib\site-packages\pandas\core\series.py"", line 2810, in update
    self._data = self._data.putmask(mask=mask, new=other, inplace=True)

  File ""C:\tools\anaconda3\envs\Simple\lib\site-packages\pandas\core\internals\managers.py"", line 564, in putmask
    return self.apply(""putmask"", **kwargs)

  File ""C:\tools\anaconda3\envs\Simple\lib\site-packages\pandas\core\internals\managers.py"", line 442, in apply
    applied = getattr(b, f)(**kwargs)

  File ""C:\tools\anaconda3\envs\Simple\lib\site-packages\pandas\core\internals\blocks.py"", line 1676, in putmask
    new_values[mask] = new

  File ""C:\tools\anaconda3\envs\Simple\lib\site-packages\pandas\core\arrays\string_.py"", line 248, in __setitem__
    super().__setitem__(key, value)

  File ""C:\tools\anaconda3\envs\Simple\lib\site-packages\pandas\core\arrays\numpy_.py"", line 252, in __setitem__
    self._ndarray[key] = value

ValueError: NumPy boolean array indexing assignment cannot assign 3 input values to the 1 output values where the mask is true

```


#### Problem description

The example works if I leave off the `dtype=""string""` (resulting in the implicit dtype `object`). 
IMO update should work for all dtypes, not only the ""old"" ones.

`a = pd.Series([1, None, 3], dtype=""Int16"")` etc. also raises ValueError, while the same with `dtype=""float64""`works. 

It looks as if update doesn't work with the new nullable dtypes (the ones with `pd.NA`).

#### Expected Output

The expected result is that `a.update(b)` updates `a` without raising an exception, not only for `object` and `float64`, but also for `string` and `Int16` etc..

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 ..., GenuineIntel
...

pandas           : 1.0.3
numpy            : 1.18.1
...

</details>
"
612171032,33979,PERF: make _Tick into a cdef class,jbrockmendel,closed,2020-05-04T21:11:26Z,2020-05-05T16:57:58Z,"The actual ""PERF"" part is that in the follow-up we can cimport `is_tick_object` which will be appreciably faster than the `getattr(other, ""_typ"", None) == ""dateoffset"" and hasattr(other, ""delta"")` checks we now do"
611268944,33947,BUG: CategoricalIndex.__contains__ incorrect NaTs,jbrockmendel,closed,2020-05-02T20:35:34Z,2020-05-05T17:19:15Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
612701751,34004,Backport PR #33292 on branch 1.0.x (REGR: Fix bug when replacing cate…,simonjayhawkins,closed,2020-05-05T15:41:40Z,2020-05-05T19:30:18Z,"…gorical value with self)

xref #33292 

fixes regression in 1.0.0 #33288"
612690651,34003,Backport PR #32870 on branch 1.0.x (DOC: Remove latest whatsnew from …,simonjayhawkins,closed,2020-05-05T15:26:06Z,2020-05-05T19:30:54Z,"…header)

xref #32870"
612641920,34001,Backport PR #33629 on branch 1.0.x (BUG: Fix Categorical use_inf_as_n…,simonjayhawkins,closed,2020-05-05T14:21:33Z,2020-05-05T19:31:54Z,"…a bug)

xref #33629

fixed regression in 1.0.0 (#33594)

# NOTE: code changes in #33629 not really suitable for backport. "
612732414,34006,CLN: remove unused out-of-bounds handling,jbrockmendel,closed,2020-05-05T16:26:29Z,2020-05-05T22:40:51Z,"We only have one test that goes through the OOB handling code, and that re-raises within the OOB handling.  We're better off being strict here like everywhere else rather than having spotty support."
612427609,33989,DOC: Add plotting examples and fix broken examples,deppen8,closed,2020-05-05T08:24:00Z,2020-05-06T07:49:55Z,"This PR improves the `pandas.plotting._misc.py` module docs by:
1. Adding example plots where they were missing from docstrings.
2. Fixing examples that were not formatted to render with
```
.. plot::
    :context: close-figs
```
3. Simplifying the syntax of some oddly-written examples.

In some cases, I replicated the examples found in the [User Guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#plotting-tools)."
612715438,34005,release note for #33102,simonjayhawkins,closed,2020-05-05T16:01:51Z,2020-05-06T09:14:14Z,xref #33157
232118876,16536,AttributeError: module 'pandas' has no attribute 'plotting',stepseazy,closed,2017-05-30T00:58:05Z,2020-05-06T10:53:25Z,"I have no idea why I'm getting this error, as I looked in the pandas folder and there is clearly a subfolder called plotting. please help.

RIk


> import os
> import math
> import numpy as np
> import h5py
> import tqdm as tqdm
> import keras
> from keras.models import Sequential, Model
> from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape
> from keras.layers import Embedding, Input, merge
> from keras.layers.recurrent import SimpleRNN, LSTM
> from keras.layers.convolutional import Convolution2D, MaxPooling2D
> from keras.optimizers import SGD, Adam, RMSprop
> import sklearn.metrics as metrics
> 





> /home/rik/anaconda3/bin/python /home/rik/PycharmProjects/self_driving/self_driving.py
> Using TensorFlow backend.
> Traceback (most recent call last):
>   File ""/home/rik/PycharmProjects/self_driving/self_driving.py"", line 6, in <module>
>     import keras
>   File ""/home/rik/anaconda3/lib/python3.5/site-packages/keras/__init__.py"", line 3, in <module>
>     from . import activations
>   File ""/home/rik/anaconda3/lib/python3.5/site-packages/keras/activations.py"", line 3, in <module>
>     from . import backend as K
>   File ""/home/rik/anaconda3/lib/python3.5/site-packages/keras/backend/__init__.py"", line 64, in <module>
>     from .tensorflow_backend import *
>   File ""/home/rik/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 1, in <module>
>     import tensorflow as tf
>   File ""/home/rik/anaconda3/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
>     from tensorflow.python import *
>   File ""/home/rik/anaconda3/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 77, in <module>
>     from tensorflow.python.estimator import estimator_lib as estimator
>   File ""/home/rik/anaconda3/lib/python3.5/site-packages/tensorflow/python/estimator/estimator_lib.py"", line 24, in <module>
>     from tensorflow.python.estimator.inputs import inputs
>   File ""/home/rik/anaconda3/lib/python3.5/site-packages/tensorflow/python/estimator/inputs/inputs.py"", line 23, in <module>
>     from tensorflow.python.estimator.inputs.pandas_io import pandas_input_fn
>   File ""/home/rik/anaconda3/lib/python3.5/site-packages/tensorflow/python/estimator/inputs/pandas_io.py"", line 28, in <module>
>     import pandas as pd
>   File ""/home/rik/anaconda3/lib/python3.5/site-packages/pandas/__init__.py"", line 51, in <module>
>     plot_params = pandas.plotting._style._Options(deprecated=True)
> AttributeError: module 'pandas' has no attribute 'plotting'
> 
> Process finished with exit code 1"
613266683,34022,Backport PR #33513 on branch 1.0.x  (BUG: Fix Categorical.min / max bug),simonjayhawkins,closed,2020-05-06T12:03:08Z,2020-05-06T12:44:33Z,"xref #33513

regression in #27929 (i.e. 1.0.0) https://github.com/pandas-dev/pandas/issues/33450#issuecomment-624625326"
569441193,32194,Extension dtypes are modified when performing groupby aggregate with agg dict,griffindore,closed,2020-02-23T05:30:11Z,2020-05-06T12:49:41Z,"#### Code Sample, a copy-pastable example if possible

```python
x = np.array([0,1,2,3,4,5], dtype=np.uint8)    # some data 
y = np.array([0,1,2,3,4,5], dtype=np.int32)    # some other data 
g = ['foo', 'bar', 'baz', 'foo', 'bar', 'baz'] # grouping column 
df = pd.DataFrame({'x': x, 'y': y, 'g': g})    #  construct dataframe from np arrays preserving dtypes

# dtypes maintained through a groupby agg as expected
df.groupby('g').agg('first').dtypes
# Out[7]: 
# x    uint8
# y    int32
# dtype: object

# dtypes maintained through a groupby agg  as expected when agg_funcs is dict
df.groupby('g').agg({'x':'first', 'y':'first'}).dtypes
# Out[8]: 
# x    uint8
# y    int32
# dtype: object

# Use new extension dtypes
df = pd.DataFrame({'x': pd.array(x, copy=False), 'y': pd.array(y, copy=False), 'g': g})

# dtypes maintained through a groupby agg as expected when agg_funcs is a scalar
df.groupby('g').agg('first').dtypes
# Out[10]: 
# x    UInt8
# y    Int32
# dtype: object

# dtypes are not maintained through a groupby agg when agg_funcs is a dict and column dtypes are extension dtypes
df.groupby('g').agg({'x':'first', 'y':'first'}).dtypes
# Out[11]: 
# x    float64
# y    float64
# dtype: object
```
#### Problem description

When using Extension dtypes, StringDtype, BooleanDtype, Int64Dtype, Int32Dtype, etc., in a DataFrame, the dtypes are modified to float64 when performing a `groupby.agg` with a dict defining the reducing function per column. This change in dtypes doesn't occur with numpy dtypes, and it doesn't occur when using a single agg func as scalar instead of the dict.

#### Expected Output

```python
df.groupby('g').agg({'x':'first', 'y':'first'}).dtypes
# Out[11]: 
# x    UInt8
# y    Int32
# dtype: object
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.14.0
pytz             : 2018.7
dateutil         : 2.7.5
pip              : 18.1
setuptools       : 40.6.2
Cython           : 0.29.1
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 7.3.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.3.3
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.0.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : 0.1.3
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.36.2
</details>
"
612134482,33978,REF: simplify Timedelta arithmetic methods,jbrockmendel,closed,2020-05-04T20:06:45Z,2020-05-06T14:11:03Z,"In particular this avoids the ""_typ"" checks which go through python-space"
611795407,33965,Feature Request: Add Direct Support to Plotly Express as a submodule in pandas.DataFrame.iplot,jtelleriar,closed,2020-05-04T11:18:15Z,2020-05-06T14:17:02Z,"Could support to Plotly Express be directly added within pandas as a iplot submodule?:

https://plotly.com/python/plotly-express/

Thanks! "
585737685,32905,Fix to _get_nearest_indexer for pydata/xarray#3751,spencerkclark,closed,2020-03-22T14:57:14Z,2020-05-06T14:32:23Z,"@jbrockmendel thanks for encouraging me to submit a PR.  I'm totally open to any suggested changes; this is just a sketch of something that would work for our needs and hopefully not break anything in pandas.

To those not familiar, some changes introduced in https://github.com/pandas-dev/pandas/pull/31511 broke some functionality of an Index subclass we define in xarray.  More details/discussion can be found in https://github.com/pydata/xarray/issues/3751 and https://github.com/pydata/xarray/pull/3764, but the gist of the issue is that for better or for worse, we have been relying on left and right distances in `_get_nearest_indexer` being determined using NumPy arrays rather than Index objects (as they were prior to https://github.com/pandas-dev/pandas/pull/31511).  

One way to fix this is to override `_get_nearest_indexer` within our Index subclass, which we've temporarily done in https://github.com/pydata/xarray/pull/3764, but this potentially fragile.  It would be great if we could find a solution here.

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
589492963,33089,BUG: Don't cast nullable Boolean to float in groupby,dsaxton,closed,2020-03-28T02:34:30Z,2020-05-06T14:52:16Z,"- [x] closes #33071
- [x] closes #32194
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
613367089,34025,Backport PR #32905 on branch 1.0.x (Fix to _get_nearest_indexer for p…,simonjayhawkins,closed,2020-05-06T14:31:30Z,2020-05-06T15:20:53Z,"…ydata/xarray#3751)

xref #32905 

regression in 1.0.2 (backport of #31511) "
613374802,34027,Backport PR #33693 on branch 1.0.x (BUG: Fix memory issues in rolling…,simonjayhawkins,closed,2020-05-06T14:41:33Z,2020-05-06T16:08:51Z,"….min/max)

xref https://github.com/pandas-dev/pandas/pull/33693#issuecomment-619536995"
613358927,34024,CLN: remove _typ checks in Timestamp methods,jbrockmendel,closed,2020-05-06T14:20:48Z,2020-05-06T17:23:44Z,
609815667,33895,BUG: resample().nearest() throws an exception if tz is provided for the DatetimeIndex,ghost,closed,2020-04-30T10:48:56Z,2020-05-06T17:58:20Z,"Here a simple create of a pandas DataFrame:
```python
import pandas as pd
df = pd.DataFrame(range(5), index=pd.date_range('1/1/2000', periods=5, freq='H', tz='UTC'))
print(df) gives:
2000-01-01 00:00:00+00:00  0
2000-01-01 01:00:00+00:00  1
2000-01-01 02:00:00+00:00  2
2000-01-01 03:00:00+00:00  3
2000-01-01 04:00:00+00:00  4
```
When trying to resample like this
```python
df.resample('15Min').nearest()
```
this throws an exception:
```python
File ""C:\Users\user\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3331, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-41-9b9242799057>"", line 1, in <module>
    df.resample('15Min').nearest()
  File ""C:\Users\user\anaconda3\lib\site-packages\pandas\core\resample.py"", line 517, in nearest
    return self._upsample(""nearest"", limit=limit)
  File ""C:\Users\user\anaconda3\lib\site-packages\pandas\core\resample.py"", line 1095, in _upsample
    res_index, method=method, limit=limit, fill_value=fill_value
  File ""C:\Users\user\anaconda3\lib\site-packages\pandas\util\_decorators.py"", line 227, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\user\anaconda3\lib\site-packages\pandas\core\frame.py"", line 3856, in reindex
    return self._ensure_type(super().reindex(**kwargs))
  File ""C:\Users\user\anaconda3\lib\site-packages\pandas\core\generic.py"", line 4544, in reindex
    axes, level, limit, tolerance, method, fill_value, copy
  File ""C:\Users\user\anaconda3\lib\site-packages\pandas\core\frame.py"", line 3744, in _reindex_axes
    index, method, copy, level, fill_value, limit, tolerance
  File ""C:\Users\user\anaconda3\lib\site-packages\pandas\core\frame.py"", line 3760, in _reindex_index
    new_index, method=method, level=level, limit=limit, tolerance=tolerance
  File ""C:\Users\user\anaconda3\lib\site-packages\pandas\core\indexes\base.py"", line 3145, in reindex
    target, method=method, limit=limit, tolerance=tolerance
  File ""C:\Users\user\anaconda3\lib\site-packages\pandas\core\indexes\base.py"", line 2740, in get_indexer
    indexer = self._get_nearest_indexer(target, limit, tolerance)
  File ""C:\Users\user\anaconda3\lib\site-packages\pandas\core\indexes\base.py"", line 2821, in _get_nearest_indexer
    left_distances = abs(self.values[left_indexer] - target)
numpy.core._exceptions.UFuncTypeError: ufunc 'subtract' cannot use operands with types dtype('<M8[ns]') and dtype('O')
```

However, if we remove the **tz** filed it works:
```python
import pandas as pd
df = pd.DataFrame(range(5), index=pd.date_range('1/1/2000', periods=5, freq='H'))
df.resample('15Min').nearest()

OutPut: 
                     0
2000-01-01 00:00:00  0
2000-01-01 00:15:00  0
2000-01-01 00:30:00  1
2000-01-01 00:45:00  1
2000-01-01 01:00:00  1
2000-01-01 01:15:00  1
2000-01-01 01:30:00  2
2000-01-01 01:45:00  2
2000-01-01 02:00:00  2
2000-01-01 02:15:00  2
2000-01-01 02:30:00  3
2000-01-01 02:45:00  3
2000-01-01 03:00:00  3
2000-01-01 03:15:00  3
2000-01-01 03:30:00  4
2000-01-01 03:45:00  4
2000-01-01 04:00:00  4
```

The used pandas version is:
```
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
```


</details>
"
611134838,33939,TST: add pattern for nearest,kotamatsuoka,closed,2020-05-02T08:26:10Z,2020-05-06T17:58:24Z,"- [ ] closes #33895
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
611229749,33944,TYP: annotate core.algorithms,jbrockmendel,closed,2020-05-02T16:51:44Z,2020-05-06T19:01:06Z,
607179784,33815,TST: check freq on series.index in assert_series_equal,jbrockmendel,closed,2020-04-27T02:14:35Z,2020-05-06T20:08:13Z,Before long we'll roll that check into assert_index_equal
623489298,34324,REF: make MonthOffset a cdef class,jbrockmendel,closed,2020-05-22T21:41:33Z,2020-05-23T01:10:41Z,
623488413,34323,REF: make SingleConstructorOffset a cdef class,jbrockmendel,closed,2020-05-22T21:39:36Z,2020-05-23T01:11:15Z,Bite the bullet on importing cache_readonly into liboffsets
622222285,34286,CLN: always dispatch-to-series,jbrockmendel,closed,2020-05-21T03:34:04Z,2020-05-23T02:07:42Z,A couple more cleanups follow this.
623487703,34322,REF: make FY523Mixin a cdef class,jbrockmendel,closed,2020-05-22T21:37:31Z,2020-05-23T02:19:35Z,
622219266,34285,REF: make BusinessMixin a cdef class,jbrockmendel,closed,2020-05-21T03:24:30Z,2020-05-23T02:20:55Z,"Getting the pickle working on this one was a PITA, hopefully the worst is behind us."
622362884,34288,DOC: improve pie chart example,MarcoGorelli,closed,2020-05-21T09:10:02Z,2020-05-23T04:22:05Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.DataFrame.plot.pie.html

**Note**: You can check the latest versions of the docs on `master` [here](https://pandas.pydata.org/docs/dev/).

#### Documentation problem

This example doesn't look great:
![image](https://user-images.githubusercontent.com/33491632/82543164-06f82680-9b4b-11ea-98bf-c7245506410a.png)

The legend covers the labels, and the two pies are so close that ""mercury"" is hard to read.

#### Suggested fix for documentation

Make a more readable plot"
622584953,34296,"DOC:updated pie plot legend overlap issue, closes #34288",wicky1234444,closed,2020-05-21T15:30:44Z,2020-05-23T04:22:16Z,"- [X] closes #34288
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
580798183,32688,Accept dateutil frequency constants in date_range,mason-98,closed,2020-03-13T19:29:48Z,2020-05-23T04:31:19Z,"- [x] closes #9314
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
623395066,34320,ENH: `name` attribute for pd.Interval,jolespin,closed,2020-05-22T18:10:56Z,2020-05-23T06:29:33Z,"#### Is your feature request related to a problem?

No
#### Describe the solution you'd like

`pd.Interval(x, y, name=""Feature"")`

#### API breaking implications

Not that I know of. 

#### Describe alternatives you've considered

Tried `setattr`

"
623175774,34308,CLN: consolidate arrow roundtrip tests for nullable dtypes in base masked tests,jorisvandenbossche,closed,2020-05-22T12:33:00Z,2020-05-23T06:48:21Z,"We recently splitted the test_boolean and test_integer in multiple files in each a directory, but I think a next step can be to deduplicate some of the common tests for the different nullable/masked dtypes. 
Certainly given that we are going to add float tests as well (cfr https://github.com/pandas-dev/pandas/pull/34307)

Starting here with the arrow-compat/roundtrip tests for both integer/boolean.

I didn't yet write a full blown conftest.py and just defined a `data` fixture inline, since here it's just a single file. But if adding more later, we probably want to move this to a `conftest.py` (or a base test class), although this duplicates a bit the `tests/arrays/integer/conftest.py` and `tests/arrays/boolean/conftest.py`

cc @dsaxton @jbrockmendel "
623625634,34333,DOC: fix deprecated groupby squeeze example in v0.12.0 whatsnew,jorisvandenbossche,closed,2020-05-23T10:06:22Z,2020-05-23T11:52:01Z,"This should fix failing CI, due to https://github.com/pandas-dev/pandas/pull/33218"
619065807,34191,DEPR: deprecate Index.__getitem__ with float key,jorisvandenbossche,closed,2020-05-15T15:38:03Z,2020-05-23T13:17:29Z,"I would propose to deprecate interpreting float numbers as integers in Index `__getitem__`.

Numpy does (no longer) allow this:

```
In [1]: a = np.array([1, 2, 3])   

In [3]: a[0.0]      
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-3-2ec1d2c6d8d4> in <module>
----> 1 a[0.0]

IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
```

With a Series with `iloc`, we also don't allow this:

```
In [7]: pd.Series(a).iloc[0.0] 
...
TypeError: Cannot index by location index with a non-integer key
```

But for Index we do check if the float is integer-like and in that case convert to integer:

```
In [5]: pd.Index(a)[0.0]  
Out[5]: 1
```

However, this is not for all Index subclasses the case, eg:

```
In [9]: pd.DatetimeIndex(a)[0.0]                                                                                                                                                                                   
...
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
```

So I think we can simply deprecate this to align Index with numpy/Series/some Index subclasses."
619075271,34193,DEPR: deprecate Index.__getitem__ with float key,jorisvandenbossche,closed,2020-05-15T15:53:17Z,2020-05-23T13:17:33Z,"See #34191 for motivation

Closes #34191"
315096613,20721,"DEPR: Series ndarray properties (strides, data, base, itemsize, flags)",jorisvandenbossche,closed,2018-04-17T14:50:25Z,2020-05-23T14:54:48Z,"- [x] closes #20419
- [ ] tests added / passed
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
613472867,34031,CLN: Use to_numpy directly in cov / corr,dsaxton,closed,2020-05-06T17:02:40Z,2020-05-23T15:12:15Z,
592170246,33218,32380 deprecate squeeze in groupby,phofl,closed,2020-04-01T20:02:44Z,2020-05-23T19:59:10Z,"- [x] closes #32380
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I deprecated the squeeze keyword. I think I got every points this keyword was used. 
I hope that I considered every point necessary to deprecate a keyword. I tried to use other points were a keyword was deprecated as example.
"
623378523,34319,BUG: Fillna with timedelta min/max overflow,asundt,closed,2020-05-22T17:35:32Z,2020-05-23T21:31:20Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np
print(pd.to_timedelta(np.full(10, pd.NaT),unit='seconds').fillna(pd.Timedelta.max))

```

#### Problem description

Output of the code:
```python
TimedeltaIndex(['-106752 days +00:12:43.145224',
                '-106752 days +00:12:43.145224',
                '-106752 days +00:12:43.145224',
                '-106752 days +00:12:43.145224',
                '-106752 days +00:12:43.145224',
                '-106752 days +00:12:43.145224',
                '-106752 days +00:12:43.145224',
                '-106752 days +00:12:43.145224',
                '-106752 days +00:12:43.145224',
                '-106752 days +00:12:43.145224'],
               dtype='timedelta64[ns]', freq=None)
```

The returned Series is filled with the value for pd.Timedelta.min and not the max value as expected. The opposite happens when trying to fillna with pd.Timedelta.min, which returns a series of the max values instead.

This doesn't seem to occur when filling a series that isn't converted to timedelta:

```python
 pd.Series(pd.NaT, index=np.arange(10)).fillna(pd.Timedelta.max)
```

And it seems to be an overflow problem because the following produces a number close to pd.Timedelta.max, as expected:

```python
 pd.to_timedelta(np.full(10, pd.NaT),unit='seconds').fillna(pd.Timedelta.max - pd.Timedelta('1 s'))
```

#### Expected Output

```python
0   106751 days 23:47:16.854775
1   106751 days 23:47:16.854775
2   106751 days 23:47:16.854775
3   106751 days 23:47:16.854775
4   106751 days 23:47:16.854775
5   106751 days 23:47:16.854775
6   106751 days 23:47:16.854775
7   106751 days 23:47:16.854775
8   106751 days 23:47:16.854775
9   106751 days 23:47:16.854775
dtype: timedelta64[ns]
```

#### Output of ``pd.show_versions()``

<details>

commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.4.0.post20200518
Cython           : 0.29.17
pytest           : 5.4.2
hypothesis       : 5.11.0
sphinx           : 3.0.3
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : 3.5.2
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.8
numba            : 0.49.1

</details>
"
618451877,34178,PERF: avoid creating numpy array in groupby.first|last,topper-123,closed,2020-05-14T18:36:35Z,2020-05-24T16:56:36Z,"A unneeded numpy array is created for each group when calling ``groupby.first`` and ``groupby.last`` on ExtensionArrays. This avoids that.

```python
>>> cat = pd.Categorical([""a""] * 1_000_000 + [""b""] * 1_000_000)
>>> ser = pd.Series(cat)
>>> %timeit ser.groupby(cat).first()
210 ms ± 3.03 ms per loop  # master
78.4 ms ± 766 µs per loop  # this PR
```

The same speedup is archieved for ``groupby.last``. The above is 3x faster than in master because there are two groups == we save creating two arrays. If there were more groups/larger arrays, we'd get even more improvements.

Also adds some type hints to help understand what parameters these funtions accept."
624003701,34359,BUG: inplace parameter will not perform the operation in place on multi column-sliced DataFrame,fluid-gun,closed,2020-05-25T02:21:31Z,2020-05-25T07:43:01Z,"Environment: Python 3.7.7 / NumPy 1.18.4 / Pandas 1.0.3

I have been testing the `inplace=True` parameters for a couple of functions, including `DataFrame.fillna()` and `DataFrame.clip()`.
When I use the inplace parameter on a multi-column slicing of a DataFrame, it will not alter the original DataFrame.

For instance, using the simple DataFrame below,
```python
df = pd.DataFrame({'A':[1,None,5],'B':[3,4,-1]})
print(df)
Out: 
     A  B
0  1.0  3
1  NaN  4
2  5.0 -1
```
the code
```python
df.loc[:, ['A','B']].fillna(0, inplace=True)
# or df.loc[:, 'A':'B'].fillna(0, inplace=True)
# or df.iloc[:,[0,1]].fillna(0, inplace=True)
# or df.iloc[:,0:2].fillna(0, inplace=True)
print(df)
```
will produce
```python
Out: 
     A  B
0  1.0  3
1  NaN  4
2  5.0 -1
```
as opposed to
```python
Out: 
     A  B
0  1.0  3
1  0.0  4
2  5.0 -1
```
Strangely enough, the parameter works perfectly when I'm slicing out just a single column
```python
df[:, 'A'].fillna(0, inplace=True)
print(df)
Out:
     A  B
0  1.0  3
1  0.0  4
2  5.0 -1
```
or when I'm using the position-based slicing method `.iloc` without specifying the first column
```python
df.iloc[:,:2].fillna(0, inplace=True)
print(df)
Out:
     A  B
0  1.0  3
1  0.0  4
2  5.0 -1
```

Let me know if this is the accepted/expected behavior or if there is a workaround.
Thanks!"
566475563,32065,"DOC: Use container directives instead of raw html for the doc ""cards""",stijnvanhoey,closed,2020-02-17T19:12:24Z,2020-05-25T08:54:51Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The PR is a follow up of #31156 and #31148 translating the raw html to docutils/sphinx directives, making it more _readable_. 

Furthermore, it fixes the redundant css in `pandas.css` versus `getting_started.css` by removing the `install-card` class and merging it with the existing `intro-card` class. 

__Note:__ The setup required an [adjustment to the theme](https://github.com/pandas-dev/pydata-bootstrap-sphinx-theme/pull/92) to overcome the usage of the `container` class by both docutils and bootstrap. "
597267787,33421,API: More permissive conversion to StringDtype,topper-123,closed,2020-04-09T13:05:15Z,2020-05-25T11:50:11Z,"- [x] closes #31204
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This is a proposal to make using ``StringDtype`` more permissive and be usable inplace of ``dtype=str``.

ATM converting to StringDtype will only accept arrays that are ``str`` already, meaning you will often have to use ``astype(str).astype(""string"")`` to be sure not to get errors, which can be tedious. For example these fail in master and work in this PR:

```python
>>> pd.Series([1,2, np.nan], dtype=""string"")
0       1
1       2
2    <NA>
dtype: string
>>> pd.array([1,2, np.nan], dtype=""string"")
<StringArray>
['1', '2', <NA>]
Length: 3, dtype: string
>>> pd.Series([1,2, np.nan]).astype(""string"")
0     1.0
1     2.0
2    <NA>
dtype: string
>>> pd.Series([1,2, np.nan], dtype=""Int64"").astype(""string"")
0       1
1       2
2    <NA>
dtype: string
```

etc. now work. Previously the above all gave errors.

Obviously tests and doc updates are still missing, but I would appreciate feedback if this solution looks ok."
623901480,34355,TST: sparse and dense concat test GH30667,ricalanis,closed,2020-05-24T16:49:09Z,2020-05-25T12:41:33Z,"- [x] closes #30668
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] Adds testing to ensure concat between dense and sparse
"
600133757,33561,PERF: operate on arrays instead of Series in DataFrame/DataFrame ops,jorisvandenbossche,closed,2020-04-15T08:53:22Z,2020-05-25T14:57:16Z,xref https://github.com/pandas-dev/pandas/pull/32779
623823549,34349,DOC: Fix error in pandas.Index.to_series,farhanreynaldo,closed,2020-05-24T08:56:13Z,2020-05-25T16:25:46Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. 

output of `python scripts/validate_docstrings.py pandas.Index.to_series`:
```
################################################################################
################################## Validation ##################################
################################################################################
```

Notes:
While working with the documentation, I noticed that index parameter in `pandas.Index.to_series` has different behaviour with `pandas.Index.to_frame`. You can't specify index parameter to False in `pandas.Index.to_series` to achieve the same index result with `pandas.Index.to_frame`. Is this intentional?

"
564854650,31957,to_sql defaults to cp1252 on Windows 10 instead of using UTF-8 as per dataframe and Oracle,Code4SAFrankie,closed,2020-02-13T17:57:00Z,2020-05-25T16:47:05Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
import pandas as pd
import dask.dataframe as dd
from dask.diagnostics import ProgressBar
from dask.distributed import Client
client = Client('192.168.1.33:8786', processes=True)
path = ""D:\\Downloads\\Wikipedia\\Yago\\""
dfFacts = dd.read_parquet(path + ""yagoFacts"", engine='pyarrow', index=False, encoding=""utf-8"")
dfFacts = dfFacts.compute()
import cx_Oracle
from sqlalchemy import types, create_engine
conn = create_engine('oracle+cx_oracle://hr:password@localhost:1521/?service_name=XEPDB1', max_identifier_length=128)
dtyp = {c:types.VARCHAR(dfFacts[c].str.len().max()) for c in dfFacts.columns.tolist()}
dfFacts.to_sql('yago_facts', conn, if_exists='replace', dtype=dtyp, index=False)
```
#### Problem description
Pandas to_sql seems to default to cp1252 on Windows 10 instead of using UTF-8 even though the dataframe and Oracle database (AL32UTF8) uses UTF-8. It fails with the following error:

```
---------------------------------------------------------------------------
UnicodeEncodeError                        Traceback (most recent call last)
<ipython-input-27-17445fb6d38f> in <module>
----> 1 dfFacts.to_sql('yago_facts', conn, if_exists='replace', dtype=dtyp, index=False)

~\AppData\Roaming\Python\Python37\site-packages\pandas\core\generic.py in to_sql(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)
   2710             chunksize=chunksize,
   2711             dtype=dtype,
-> 2712             method=method,
   2713         )
   2714 

~\AppData\Roaming\Python\Python37\site-packages\pandas\io\sql.py in to_sql(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)
    516         chunksize=chunksize,
    517         dtype=dtype,
--> 518         method=method,
    519     )
    520 

~\AppData\Roaming\Python\Python37\site-packages\pandas\io\sql.py in to_sql(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method)
   1318         )
   1319         table.create()
-> 1320         table.insert(chunksize, method=method)
   1321         if not name.isdigit() and not name.islower():
   1322             # check for potentially case sensitivity issues (GH7815)

~\AppData\Roaming\Python\Python37\site-packages\pandas\io\sql.py in insert(self, chunksize, method)
    754 
    755                 chunk_iter = zip(*[arr[start_i:end_i] for arr in data_list])
--> 756                 exec_insert(conn, keys, chunk_iter)
    757 
    758     def _query_iterator(

~\AppData\Roaming\Python\Python37\site-packages\pandas\io\sql.py in _execute_insert(self, conn, keys, data_iter)
    668         """"""
    669         data = [dict(zip(keys, row)) for row in data_iter]
--> 670         conn.execute(self.table.insert(), data)
    671 
    672     def _execute_insert_multi(self, conn, keys, data_iter):

e:\wpy-3710\python-3.7.1.amd64\lib\site-packages\sqlalchemy\engine\base.py in execute(self, object_, *multiparams, **params)
    980             raise exc.ObjectNotExecutableError(object_)
    981         else:
--> 982             return meth(self, multiparams, params)
    983 
    984     def _execute_function(self, func, multiparams, params):

e:\wpy-3710\python-3.7.1.amd64\lib\site-packages\sqlalchemy\sql\elements.py in _execute_on_connection(self, connection, multiparams, params)
    291     def _execute_on_connection(self, connection, multiparams, params):
    292         if self.supports_execution:
--> 293             return connection._execute_clauseelement(self, multiparams, params)
    294         else:
    295             raise exc.ObjectNotExecutableError(self)

e:\wpy-3710\python-3.7.1.amd64\lib\site-packages\sqlalchemy\engine\base.py in _execute_clauseelement(self, elem, multiparams, params)
   1099             distilled_params,
   1100             compiled_sql,
-> 1101             distilled_params,
   1102         )
   1103         if self._has_events or self.engine._has_events:

e:\wpy-3710\python-3.7.1.amd64\lib\site-packages\sqlalchemy\engine\base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)
   1248         except BaseException as e:
   1249             self._handle_dbapi_exception(
-> 1250                 e, statement, parameters, cursor, context
   1251             )
   1252 

e:\wpy-3710\python-3.7.1.amd64\lib\site-packages\sqlalchemy\engine\base.py in _handle_dbapi_exception(self, e, statement, parameters, cursor, context)
   1476                 util.raise_from_cause(sqlalchemy_exception, exc_info)
   1477             else:
-> 1478                 util.reraise(*exc_info)
   1479 
   1480         finally:

e:\wpy-3710\python-3.7.1.amd64\lib\site-packages\sqlalchemy\util\compat.py in reraise(tp, value, tb, cause)
    151         if value.__traceback__ is not tb:
    152             raise value.with_traceback(tb)
--> 153         raise value
    154 
    155     def u(s):

e:\wpy-3710\python-3.7.1.amd64\lib\site-packages\sqlalchemy\engine\base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)
   1224                 if not evt_handled:
   1225                     self.dialect.do_executemany(
-> 1226                         cursor, statement, parameters, context
   1227                     )
   1228             elif not parameters and context.no_parameters:

e:\wpy-3710\python-3.7.1.amd64\lib\site-packages\sqlalchemy\dialects\oracle\cx_oracle.py in do_executemany(self, cursor, statement, parameters, context)
   1126         if isinstance(parameters, tuple):
   1127             parameters = list(parameters)
-> 1128         cursor.executemany(statement, parameters)
   1129 
   1130     def do_begin_twophase(self, connection, xid):

e:\wpy-3710\python-3.7.1.amd64\lib\encodings\cp1252.py in encode(self, input, errors)
     10 
     11     def encode(self,input,errors='strict'):
---> 12         return codecs.charmap_encode(input,errors,encoding_table)
     13 
     14     def decode(self,input,errors='strict'):

UnicodeEncodeError: 'charmap' codec can't encode character '\u010d' in position 11: character maps to <undefined>
```

#### Expected Output
No errors with dataframe loaded in Oracle table.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
```
E:\WPy-3710\python-3.7.1.amd64\lib\site-packages\xarray\core\merge.py:10: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2018.6
dateutil         : 2.7.5
pip              : 20.0.2
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.3.1
hypothesis       : 3.82.1
sphinx           : 2.2.1
blosc            : 1.6.1
feather          : 0.4.0
xlsxwriter       : 1.1.2
lxml.etree       : 4.3.4
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.6.1
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : 1.2.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.0.3
numexpr          : 2.6.8
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.13
tables           : 3.4.4
xarray           : 0.11.3
xlrd             : 1.1.0
xlwt             : None
xlsxwriter       : 1.1.2
```
</details>
"
621167406,34265,REF: Make WeekOfMonthMixin a cdef class,jbrockmendel,closed,2020-05-19T17:44:19Z,2020-05-25T17:35:57Z,
622184542,34284,REF: move get_rule_month to libparsing to simplify dep structure,jbrockmendel,closed,2020-05-21T01:34:17Z,2020-05-25T17:36:17Z,
623686100,34342,REF: cdef BusinessHourMixin,jbrockmendel,closed,2020-05-23T15:44:15Z,2020-05-25T17:38:34Z,
623733365,34346,REF: move Year/Month/Quarter offsets to liboffsets,jbrockmendel,closed,2020-05-23T20:09:57Z,2020-05-25T17:39:07Z,
574077507,32395,"assignment with datetime64[ns, UTC] raises TypeError",Lmmejia11,closed,2020-03-02T16:06:07Z,2020-05-25T17:39:42Z,"#### Code Sample

```python
# Your code here

import datetime, pandas as pd

data = [[datetime.datetime(2020, 2, 28, 13, 51, 27, tzinfo=datetime.timezone.utc)],
 [datetime.datetime(2020, 2, 28, 13, 51, 27, tzinfo=datetime.timezone.utc)],
 [datetime.datetime(2020, 2, 28, 13, 51, 27, tzinfo=datetime.timezone.utc)],
 [datetime.datetime(2020, 2, 28, 13, 51, 27, tzinfo=datetime.timezone.utc)]]
df = pd.DataFrame(data, columns=['cd'])     # dtype: datetime64[ns, UTC]
df2 = pd.DataFrame(index=df.index)

df2.loc[:,'cd2'] = df['cd']                  # no error
df2.loc[df.index,'cd3'] = df['cd']     # error
df2.loc[df.index,'cd2'] = df['cd']     # no error

```
####  Problem description 

If you try to assign datetime values (with zone and indexes) to a column, it will raise TypeError: data type not understood. 
No errors raise with index ':', or when the column already has the correct type. Note that this only happens if the datetime has zone information. With tzinfo=None, no errors occur.

#### Output of ``pd.show_versions()``

I noticed this bug in version 1.0.1
No errors occur in 0.25.3

INSTALLED VERSIONS
------------------
commit           : None
pandas           : 1.0.1
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 20.0.1
setuptools       : 45.1.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 2.4.2
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.2 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : 0.6.0
lxml.etree       : None
matplotlib       : 3.1.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.2.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None"
623733746,34347,CLN: always pass op to combine_series_frame,jbrockmendel,closed,2020-05-23T20:12:25Z,2020-05-25T17:40:59Z,"
"
621882176,34277,"BUG: flex op with DataFrame, Series and ea vs ndarray",jbrockmendel,closed,2020-05-20T16:04:18Z,2020-05-25T17:45:16Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

<s>I have a couple ideas in mind that might make the edit in blocks here unnecessary, but want to profile them first, so will do that cleanup in a refactor following the bugfix.</s> Updated with a much cleaner implementation, it also de-special-cases one of our timedelta64 tests."
623221272,34312,"REGR: fix op(frame, series) with extension dtypes",jorisvandenbossche,closed,2020-05-22T13:51:01Z,2020-05-25T17:58:47Z,"Closes #34311 

cc @jbrockmendel "
623205893,34311,"BUG: op(frame, series) raising NotImplementedError with extension dtypes",jorisvandenbossche,closed,2020-05-22T13:28:58Z,2020-05-25T17:59:20Z,"```
In [1]: df = pd.DataFrame(np.random.randint(0, 100, (10, 3)), dtype=""Int64"", columns=['a', 'b', 'c']) 

In [2]: s = pd.Series([1, 2, 3], dtype=""int64"", index=[""a"", ""b"", ""c""])

In [3]: df + s 
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-3-c26e56e5136b> in <module>
----> 1 df + s

~/scipy/pandas/pandas/core/ops/__init__.py in f(self, other, axis, level, fill_value)
    715             axis = self._get_axis_number(axis) if axis is not None else 1
    716             new_data = _combine_series_frame(
--> 717                 self, other, pass_op, axis=axis, str_rep=str_rep
    718             )
    719         else:

~/scipy/pandas/pandas/core/ops/__init__.py in _combine_series_frame(left, right, func, axis, str_rep)
    516 
    517         array_op = get_array_op(func, str_rep=str_rep)
--> 518         bm = left._mgr.apply(array_op, right=rvalues.T, align_keys=[""right""])
    519         return type(left)(bm)
    520 

~/scipy/pandas/pandas/core/internals/managers.py in apply(self, f, align_keys, **kwargs)
    393 
    394             if callable(f):
--> 395                 applied = b.apply(f, **kwargs)
    396             else:
    397                 applied = getattr(b, f)(**kwargs)

~/scipy/pandas/pandas/core/internals/blocks.py in apply(self, func, **kwargs)
    334         """"""
    335         with np.errstate(all=""ignore""):
--> 336             result = func(self.values, **kwargs)
    337 
    338         return self._split_op_result(result)

~/scipy/pandas/pandas/core/ops/array_ops.py in arithmetic_op(left, right, op, str_rep)
    198     if should_extension_dispatch(lvalues, rvalues) or isinstance(rvalues, Timedelta):
    199         # Timedelta is included because numexpr will fail on it, see GH#31457
--> 200         res_values = op(lvalues, rvalues)
    201 
    202     else:

~/scipy/pandas/pandas/core/ops/common.py in new_method(self, other)
     63         other = item_from_zerodim(other)
     64 
---> 65         return method(self, other)
     66 
     67     return new_method

~/scipy/pandas/pandas/core/arrays/integer.py in integer_arithmetic_method(self, other)
    578 
    579             if getattr(other, ""ndim"", 0) > 1:
--> 580                 raise NotImplementedError(""can only perform ops with 1-d structures"")
    581 
    582             if isinstance(other, IntegerArray):

NotImplementedError: can only perform ops with 1-d structures

In [4]: df.astype(int) + s  
Out[4]: 
    a   b   c
0  86  53  66
1  77  26  56
2  11   5  45
3  97  21  84
4  20  72  38
5  62  45   7
6  91  34  20
7  26   5  98
8  18  55  66
9  86  33  80
```"
620953186,34253,Make option_context a ContextDecorator.,anntzer,closed,2020-05-19T12:54:41Z,2020-05-25T18:47:44Z,"This makes it possible to use option_context as a decorator over an
entire function, saving an indent level and making it easy to comment
out (or in) the context while keeping correct indentation.

- [ ] closes #xxxx (N/A)
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
576666809,32479,"BUG: Fix issue with datetime[ns, tz] input in Block.setitem GH32395",h-vishal,closed,2020-03-06T03:08:40Z,2020-05-25T19:29:50Z,"- [x] closes #32395 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
319597276,20927,read_sas OverflowError: int too big to convert with '31DEC9999'd formatted as DATE9. ,paul-lilley,closed,2018-05-02T15:17:33Z,2020-05-25T21:47:53Z,"```python
import os
import pandas as pd

sasDS_works = 'highDate_works.sas7bdat'
sasDS_fails = 'highDate_fails.sas7bdat'
path = r'C:\temp'

print(sasDS_works)
df = pd.read_sas(os.path.join(path, sasDS_works))
print(df)

print(sasDS_fails)
df = pd.read_sas(os.path.join(path, sasDS_fails))
print(df)
```

```sas
data temp.highDate_works ;
	highDate_num = '31DEC9999'd;
	highDate_date = '31DEC9999'd; 
run;
data temp.highDate_fails ;
	highDate_num = '31DEC9999'd;
	highDate_date = '31DEC9999'd; format highDate_date date9.;
run;
```
The sas7bdat files are zipped and uploaded.
[highdate_works.zip](https://github.com/pandas-dev/pandas/files/1967842/highdate_works.zip)


#### Problem description
Traceback (most recent call last):
  File ""pandas/_libs/tslib.pyx"", line 2075, in pandas._libs.tslib.array_with_unit_to_datetime
  File ""pandas/_libs/tslibs/timedeltas.pyx"", line 96, in pandas._libs.tslibs.timedeltas.cast_from_unit
OverflowError: int too big to convert

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/xxxx/aargh_sasData_31DEC9999.py"", line 27, in <module>
    df = pd.read_sas(os.path.join(path, sasDS_fails))
  File ""C:\xxxx\AppData\Local\Continuum\Anaconda3\envs\ratabasePython\lib\site-packages\pandas\io\sas\sasreader.py"", line 68, in read_sas
    data = reader.read()
  File ""C:\xxx\AppData\Local\Continuum\Anaconda3\envs\ratabasePython\lib\site-packages\pandas\io\sas\sas7bdat.py"", line 614, in read
    rslt = self._chunk_to_dataframe()
  File ""C:\xxxx\AppData\Local\Continuum\Anaconda3\envs\ratabasePython\lib\site-packages\pandas\io\sas\sas7bdat.py"", line 666, in _chunk_to_dataframe
    origin=""1960-01-01"")
  File ""C:\xxxx\AppData\Local\Continuum\Anaconda3\envs\ratabasePython\lib\site-packages\pandas\core\tools\datetimes.py"", line 373, in to_datetime
    values = _convert_listlike(arg._values, True, format)
  File ""C:\xxxx\AppData\Local\Continuum\Anaconda3\envs\ratabasePython\lib\site-packages\pandas\core\tools\datetimes.py"", line 229, in _convert_listlike
    errors=errors)
  File ""pandas/_libs/tslib.pyx"", line 2001, in pandas._libs.tslib.array_with_unit_to_datetime
  File ""pandas/_libs/tslib.pyx"", line 2078, in pandas._libs.tslib.array_with_unit_to_datetime
pandas._libs.tslib.OutOfBoundsDatetime: cannot convert input 2932894.0 with the unit 'd'

#### Expected Output




#### Output of ``pd.show_versions()``
<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.4.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None
pandas: 0.22.0
pytest: 3.3.2
pip: 9.0.1
setuptools: 38.4.0
Cython: 0.27.3
numpy: 1.14.2
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: 1.6.6
patsy: 0.5.0
dateutil: 2.6.1
pytz: 2018.4
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 2.1.2
openpyxl: 2.4.10
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.2
lxml: 4.2.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.1
pymysql: None
psycopg2: 2.7.3.2 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
600411667,33569,BUG: strange behaviour in quantile with group by,abdullahodibat,closed,2020-04-15T16:02:22Z,2020-05-25T21:58:07Z,"is this a bug in pandas? why null values in the grouped by field break the quantile?

``` 
df = pd.DataFrame({
    'category': ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B'],
    'value': [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]
})
quantiles = df.groupby('category')['value'].quantile(0.75)
print(quantiles)

df2 = pd.DataFrame({
    'category': ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', np.nan],
    'value': [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6,1]
})
quantiles2 = df2.groupby('category')['value'].quantile(0.75)
print(quantiles2) 
```

produces this output:
```
category
A    4.75
B    4.75
Name: value, dtype: float64
category
A    3.75
B    3.75
Name: value, dtype: float64
```

im using pandas 1.0.3"
591667302,33200,BUG: 'groupby()' results in shifted results for 'quantile()',1313e,closed,2020-04-01T06:41:12Z,2020-05-25T21:58:07Z,"#### Code Sample, a copy-pastable example if possible

```python
# Imports
import pandas as pd
import numpy as np

# Define x and bins
x = np.linspace(0, 1, 100)
bins = np.arange(0, 1, 0.1)

# Create Series object with the x-data
series = pd.Series(x, name='X')

# Group series by provided bins
data_cut = pd.cut(series, bins, include_lowest=True)
grp = series.groupby(by=data_cut)

# Calculate the 0.5 quantile of this group
perc = grp.quantile()

# For every group, print the 0.5 quantile as determined by that group and its values
for group, indices in grp.indices.items():
    print()
    print(""Bin:"", group)
    print(""Group quantile:"", perc.loc[group])
    grp_series = grp.get_group(group)
    print(""Group values quantile:"", grp_series.quantile())

```
#### Problem description

NOTE: I see that there are several other issues already open that discuss problems with `groupby` and `quantile` in v1.0.3, but I figured that I post this anyway, as it is a very easy and simple reproducible example.

Executing the code above in pandas `v1.0.3` (or any `v1.0.x` version) results in the quantiles not agreeing with each other for every group, even though they should.
Instead, it seems that the quantile as calculated by a group is shifted by 1 group.
This will become 2 groups when using ``bins = np.arange(0, 0.9, 0.1)`` and goes away when using ``bins = np.arange(0, 1.1, 0.1)`` (or the ``bins = np.linspace(0, 1, 11)`` equivalent).
The problem above does not occur for pandas `v0.24.x`, but instead gives the proper output.

Additionally, replacing `x` and `bins` with
```python
# Define x and bins
x = np.linspace(1, 10, 100)
bins = 10**np.arange(0, 1, 0.1)

```

in order to use logarithmic values (that are not equally binned), will not result in a simple shift, but simply values that make no sense at all at first.

After some more testing, it seems that the `quantile` method of a `GroupBy` object, first goes through all values that are not in a group (their indices are NaN) and then goes through the remaining data normally.
Not sure if that is helpful.


#### Expected Output
The expected output is that the two different values that are printed are always the same for each group, regardless of the bins that are used.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-91-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : nl_NL.UTF-8
LOCALE           : nl_NL.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0.post20200309
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
459299804,26989,"Series with MultiIndex ""at"" function raises ""TypeError""",ndamclean,closed,2019-06-21T17:30:57Z,2020-05-25T22:07:51Z,"#### Code Sample

```python
# Your code here
import pandas as pd
s = pd.Series(index=pd.MultiIndex.from_tuples([('a', 0)]))
s.loc['a', 0]  # loc function works fine, no complaints here
s.at['a', 0]  # raises TypeError
```

#### Stack Trace:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/pandas/core/indexing.py"", line 2270, in __getitem__
    return self.obj._get_value(*key, takeable=self._takeable)
TypeError: _get_value() got multiple values for argument 'takeable'
```
#### Problem description

I would expect the `at` function to either return the values given by the indexers or raise a `KeyError`. In the above example, the index value exists, so `at` should return the value from the series at that index (`nan`). 

The `at` function works fine for series with normal indexes. There is nothing in the documentation indicating it should not work for multi-indexes.

#### Expected Output

```
nan
nan
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-50-generic
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: C.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 3.4.2
pip: 19.1.1
setuptools: 41.0.1
Cython: 0.29.10
numpy: 1.16.4
scipy: 1.3.0
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2019.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.2.3
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
602085742,33611,BUG: set_levels set wrong order levels for MutiIndex,yixinxiao7,closed,2020-04-17T16:13:30Z,2020-05-25T22:43:58Z,"- [ ] closes #33420
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Added parameter that allows user to reset codes"
624324004,34365,ENH: Make DataFrame.insert more flexible,topper-123,open,2020-05-25T14:09:51Z,2020-05-25T23:34:25Z,"``DataFrame.insert`` is very inflexible today and can not really be used in pipes. I'd like to do several changes to the method:

* Add a inplace parameter, so the result from the op can be returned.
* allow a callable for ``value``, so the new values can be computed from existing column values.
* allow a dict for ``value``, so the several columns can be inserted simultaneously,
* add ``insert_after`` and ``insert_before`` parameters to allow label-based insertion location.
* move ``loc``  to the after ``value`` and let it have a default value of None, i.e. insert the new column at the end of the frame if location is not specified.
* deprecate ``DataFrame.assign``. It's functionality would be covered by the changed ``DataFrame.insert`` method.

The above change would allow us to be quite flexible when creating new columns in pipes. For example we could do
```python
df.insert(""formal_name"", lambda: ""Mr. "" + x[""last_name""], insert_after=""last_name"")
.pipe(...)
```

The above is obviously quite a large change. It could be discussed if it would be better make it a new method instead of changing an existing one..."
621130719,34263,REF: Implement RelativeDeltaOffset,jbrockmendel,closed,2020-05-19T16:44:12Z,2020-05-25T23:45:36Z,"RelativeDeltaOffset (basically the same as the existing DateOffset class) is a bit of an odd duck _attributes-wise, this is just a move without yet cdef-ing it."
623958246,34358,BUG: reset_index() and set_index() expands uint32 to uint64,bergkvist,closed,2020-05-24T22:23:19Z,2020-05-26T00:26:14Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
xy = pd.Series(
    index=pd.Index([1,2,3], dtype='uint32', name='x'), 
    data=[4,5,6], dtype='uint32', name='y'
)


>>> xy.reset_index().dtypes
x    uint64
y    uint32
dtype: object


>>> xy.reset_index().set_index('y').index.dtype
dtype('uint64')
```

#### Problem description
When using reset_index() or set_index(), the datatype of the affected array is expanded from uint32 to uint64. The dtype should remain unaffected.

#### Expected Output
```py
>>> xy.reset_index().dtypes
x    uint32
y    uint32
dtype: object


>>> xy.reset_index().set_index('y').index.dtype
dtype('uint32')
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.9.125-linuxkit
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.0
numpy            : 1.17.0
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.1
setuptools       : 41.0.1
Cython           : 0.29.13
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.7.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : None
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.1
numexpr          : 2.6.9
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : 1.3.6
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

</details>
"
624511532,34374,CLN: remove _combine_series_frame,jbrockmendel,closed,2020-05-25T22:50:33Z,2020-05-26T01:21:43Z,"Following this we can consolidate some dispatch_to_series calls, saving that for a separate step for clarity of exposition."
622793089,34302,TST: GH24798 df.replace() with duplicate columns,matteosantama,closed,2020-05-21T21:14:44Z,2020-05-26T02:04:34Z,"- [x] xref #24798 
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
598578153,33503,DEPR: weekofyear,mroeschke,closed,2020-04-12T21:45:31Z,2020-05-26T03:22:04Z,"Now that `isocalendar` is available on `Timestamp`, `DatetimeIndex` and the `dt` accessor, `weekofyear` already exists in `isocalendar` and can be deprecated."
489922188,28302,groupby(pd.Grouper) ignores loffset,ghost,closed,2019-09-05T18:15:35Z,2020-05-26T05:19:03Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
df = pd.date_range(start=""1/1/2018"", end=""1/2/2018"", periods=1000).to_frame()
print(df.resample(""1h"", loffset=""15min"").last().index)
print(df.groupby(pd.Grouper(freq=""1h"", loffset=""15min"")).last().index)
```
#### Problem description

I thought the two calls should be equivalent. However, the output is:

```
DatetimeIndex(['2018-01-01 00:15:00', '2018-01-01 01:15:00',
               '2018-01-01 02:15:00', '2018-01-01 03:15:00',
               '2018-01-01 04:15:00', '2018-01-01 05:15:00',
               '2018-01-01 06:15:00', '2018-01-01 07:15:00',
               '2018-01-01 08:15:00', '2018-01-01 09:15:00',
               '2018-01-01 10:15:00', '2018-01-01 11:15:00',
               '2018-01-01 12:15:00', '2018-01-01 13:15:00',
               '2018-01-01 14:15:00', '2018-01-01 15:15:00',
               '2018-01-01 16:15:00', '2018-01-01 17:15:00',
               '2018-01-01 18:15:00', '2018-01-01 19:15:00',
               '2018-01-01 20:15:00', '2018-01-01 21:15:00',
               '2018-01-01 22:15:00', '2018-01-01 23:15:00',
               '2018-01-02 00:15:00'],
              dtype='datetime64[ns]', freq='H')
DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 01:00:00',
               '2018-01-01 02:00:00', '2018-01-01 03:00:00',
               '2018-01-01 04:00:00', '2018-01-01 05:00:00',
               '2018-01-01 06:00:00', '2018-01-01 07:00:00',
               '2018-01-01 08:00:00', '2018-01-01 09:00:00',
               '2018-01-01 10:00:00', '2018-01-01 11:00:00',
               '2018-01-01 12:00:00', '2018-01-01 13:00:00',
               '2018-01-01 14:00:00', '2018-01-01 15:00:00',
               '2018-01-01 16:00:00', '2018-01-01 17:00:00',
               '2018-01-01 18:00:00', '2018-01-01 19:00:00',
               '2018-01-01 20:00:00', '2018-01-01 21:00:00',
               '2018-01-01 22:00:00', '2018-01-01 23:00:00',
               '2018-01-02 00:00:00'],
              dtype='datetime64[ns]', freq='H')
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-17134-Microsoft
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.1
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: 0.7.4
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.14.1
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
623840147,34352,Use repr of fill_value in SparseDtype repr,topper-123,closed,2020-05-24T10:51:24Z,2020-05-26T06:36:09Z,"This proposes changing the StringDtype repr to show the fill_value's repr rather than its string representation:

```python
>>> arr = pd.arrays.SparseArray([0, 1])
>>> arr.dtype
Sparse[int64, 0]  # master & this PR, unchanged
>>> arr.astype(str).dtype
Sparse[object, 0]  # master
Sparse[object, '0']  # this PR, notice quotes
```
"
601410335,33595,"DEPR: Deprecate week, weekofyear in Series.dt,DatetimeIndex",mgmarino,closed,2020-04-16T20:02:13Z,2020-05-26T06:36:11Z,"Closes https://github.com/pandas-dev/pandas/issues/33503

This PR is a followup to #33220 and implements what was discussed there.

A few comments:

- `DatetimeIndex.isocalendar` does not set the index on the returned dataframe,
  leading to some cumbersome syntax.  This may be worth changing, please let me
  know what you think.
- Should we also deprecate `Timestamp.week/weekofyear`?

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
624454283,34369,"Backport PR #32479 on branch 1.0.x (BUG: Fix issue with datetime[ns, tz] input in Block.setitem)",simonjayhawkins,closed,2020-05-25T19:29:07Z,2020-05-26T07:41:02Z,"xref #32479, #32395

regression in 1.0.0"
602585014,33644,BUG: Groupby quantiles incorrect bins #33200,mabelvj,closed,2020-04-19T00:34:00Z,2020-05-26T08:31:01Z,"Maintain the order of the bins in group_quantile. Updated tests #33200

- [x] closes #33200, closes #33569
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
624695579,34382,Backport PR #33644 on branch 1.0.x  (BUG: Groupby quantiles incorrect bins),simonjayhawkins,closed,2020-05-26T08:26:15Z,2020-05-26T09:14:57Z,"xref #33644, #33200, #33569

regression in #20405 (0.25.0)"
614266450,34053,more informative error message with np.min or np.max on unordered Categorical,simonjayhawkins,closed,2020-05-07T18:44:23Z,2020-05-26T09:27:21Z,"- [ ] closes #33115
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
614113031,34049,Bug in Series.groupby would raise ValueError when grouping by PeriodIndex level,simonjayhawkins,closed,2020-05-07T14:42:41Z,2020-05-26T09:27:53Z,"- [ ] closes #34010
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
612865524,34010,BUG: ValueError: Given date string not likely a datetime when grouping by PeriodIndex level,TomAugspurger,closed,2020-05-05T20:12:28Z,2020-05-26T09:28:10Z,"- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of pandas.
- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```pytb
In [10]: t = pd.Series([1], index=pd.PeriodIndex(['2000'], name=""A"", freq=""D""))

In [11]: t.groupby(level=""A"").size()
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/indexes/period.py in get_loc(self, key, method, tolerance)
    593         try:
--> 594             return self._engine.get_loc(key)
    595         except KeyError:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index_class_helper.pxi in pandas._libs.index.Int64Engine._check_type()

KeyError: 'A'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-11-ca2eac2f77ff> in <module>
----> 1 t.groupby(""A"").size()

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/series.py in groupby(self, by, axis, level, as_index, sort, group_keys, squeeze, observed)
   1685             group_keys=group_keys,
   1686             squeeze=squeeze,
-> 1687             observed=observed,
   1688         )
   1689

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in __init__(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated)
    407                 sort=sort,
    408                 observed=observed,
--> 409                 mutated=self.mutated,
    410             )
    411

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/groupby/grouper.py in get_grouper(obj, key, axis, level, sort, observed, mutated, validate)
    588
    589         elif is_in_axis(gpr):  # df.groupby('name')
--> 590             if gpr in obj:
    591                 if validate:
    592                     obj._check_label_or_level_ambiguity(gpr, axis=axis)

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/generic.py in __contains__(self, key)
   1848     def __contains__(self, key) -> bool_t:
   1849         """"""True if the key is in the info axis""""""
-> 1850         return key in self._info_axis
   1851
   1852     @property

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/indexes/period.py in __contains__(self, key)
    384         else:
    385             try:
--> 386                 self.get_loc(key)
    387                 return True
    388             except (TypeError, KeyError):

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/indexes/period.py in get_loc(self, key, method, tolerance)
    598
    599             try:
--> 600                 asdt, parsed, reso = parse_time_string(key, self.freq)
    601                 key = asdt
    602             except TypeError:

pandas/_libs/tslibs/parsing.pyx in pandas._libs.tslibs.parsing.parse_time_string()

pandas/_libs/tslibs/parsing.pyx in pandas._libs.tslibs.parsing.parse_datetime_string_with_reso()

ValueError: Given date string not likely a datetime.
```

#### Problem description

With a regular Index, we see the following

```python

In [13]: s = pd.Series([1], index=pd.Index(['a'], name='A'))

In [14]: s.groupby(level=""A"").size()
Out[14]:
A
a    1
dtype: int64
```

This seems to only affect PeriodIndex. DatetimeIndex works fine.

#### Expected Output

```python
A
2000-01-01    1
Freq: D, dtype: int64
```"
597427737,33433,BUG: Using `kwargs` in `Rolling.apply()`,echasnovski,closed,2020-04-09T17:07:56Z,2020-05-26T09:29:05Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
from functools import partial

import numpy as np
import pandas as pd


def foo(x, par):
    return np.sum(x + par)


df = pd.DataFrame({""gr"": [1, 1], ""a"": [1, 2]})

# With `kwargs`
try:
    print(df.groupby(""gr"")[""a""].rolling(1).apply(foo, kwargs={""par"": 10}))
except Exception as e:
    print(e)

# Using `partial`
bar = partial(foo, par=10)
print(df.groupby(""gr"")[""a""].rolling(1).apply(bar))
```

#### Problem description

**Expected**: using appropriate `kwargs` argument for `pandas.core.window.rolling.Rolling.apply()` should be equivalent to premodifying supplied function with `functools.partial()` (because it is stated in [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.window.rolling.Rolling.apply.html)).
**Current**: `kwargs` doesn't seem to be used at all, because it results into ""missing 1 required positional argument"" exception.
**Note**: This works as expected in pandas 0.25.3, but gives error as early as 1.0.0.

#### Expected Output

```
gr   
1   0    11.0
    1    12.0
Name: a, dtype: float64
gr   
1   0    11.0
    1    12.0
Name: a, dtype: float64
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-29-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : ru_UA.UTF-8
LOCALE           : ru_UA.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200325
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
614078121,34048,Bug in DataFrame.replace casts columns to ``object`` dtype if items in ``to_replace`` not in values,simonjayhawkins,closed,2020-05-07T13:55:54Z,2020-05-26T09:29:43Z,"- [ ] closes #32988
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
602586084,33645,IO: Fix S3 Error Handling,alimcmaster1,closed,2020-04-19T00:40:51Z,2020-05-26T09:30:12Z,"closes #27679 
closes #32486 

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
475220488,27679,to_parquet swallows NoCredentialsError,languitar,closed,2019-07-31T15:43:21Z,2020-05-26T09:30:28Z,"#### Code Sample, a copy-pastable example if possible

```python
try: 
   pd.DataFrame({'foo': [None, ['foo', 'bar']]}).to_parquet('s3://foo/bar') 
except Exception as e: 
   print('Here')                                                                                                                                                                                                            
```
#### Problem description

Without credentials configured, the above code does not write any output, but also doesn't end up in the exception handling code. Instead, on stdout or stderr, an exception is printed:

```
Exception ignored in: <function AbstractBufferedFile.__del__ at 0x7fe0ae8db440>
Traceback (most recent call last):
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/fsspec/spec.py"", line 1137, in __del__
    self.close()
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/fsspec/spec.py"", line 1114, in close
    self.flush(force=True)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/fsspec/spec.py"", line 986, in flush
    self._initiate_upload()
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/s3fs/core.py"", line 951, in _initiate_upload
    Bucket=self.bucket, Key=self.key, ACL=self.acl)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/s3fs/core.py"", line 939, in _call_s3
    **kwargs)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/s3fs/core.py"", line 182, in _call_s3
    return method(**additional_kwargs)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/botocore/client.py"", line 357, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/botocore/client.py"", line 648, in _make_api_call
    operation_model, request_dict, request_context)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/botocore/client.py"", line 667, in _make_request
    return self._endpoint.make_request(operation_model, request_dict)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/botocore/endpoint.py"", line 102, in make_request
    return self._send_request(request_dict, operation_model)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/botocore/endpoint.py"", line 132, in _send_request
    request = self.create_request(request_dict, operation_model)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/botocore/endpoint.py"", line 116, in create_request
    operation_name=operation_model.name)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/botocore/hooks.py"", line 356, in emit
    return self._emitter.emit(aliased_event_name, **kwargs)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/botocore/hooks.py"", line 228, in emit
    return self._emit(event_name, kwargs)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/botocore/hooks.py"", line 211, in _emit
    response = handler(**kwargs)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/botocore/signers.py"", line 90, in handler
    return self.sign(operation_name, request)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/botocore/signers.py"", line 157, in sign
    auth.add_auth(request)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/botocore/auth.py"", line 425, in add_auth
    super(S3SigV4Auth, self).add_auth(request)
  File ""/home/languitar/.pyenv/versions/analytics-3.7/lib/python3.7/site-packages/botocore/auth.py"", line 357, in add_auth
    raise NoCredentialsError
botocore.exceptions.NoCredentialsError: Unable to locate credentials
```

#### Expected Output

`Here`

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.61-1-lts
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.0
numpy            : 1.17.0
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.2.1
setuptools       : 41.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.7.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.14.1
pytables         : None
s3fs             : 0.3.1
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : 1.1.8

</details>
"
473104495,27596,Cannot write partitioned parquet file to S3,jkleint,closed,2019-07-25T22:58:55Z,2020-05-26T09:31:14Z,"Apologies if this is a pyarrow issue.

#### Code Sample, a copy-pastable example if possible

```python
pd.DataFrame({'a': range(5), 'b': range(5)}).to_parquet('s3://mybucket', partition_cols=['b'])
```
#### Problem description
Fails with `AttributeError: 'NoneType' object has no attribute '_isfilestore'`

```
Traceback (most recent call last):
  File ""/python/partparqs3.py"", line 8, in <module>
    pd.DataFrame({'a': range(5), 'b': range(5)}).to_parquet('s3://mybucket', partition_cols=['b'])
  File ""/python/lib/python3.7/site-packages/pandas/core/frame.py"", line 2203, in to_parquet
    partition_cols=partition_cols, **kwargs)
  File ""/python/lib/python3.7/site-packages/pandas/io/parquet.py"", line 252, in to_parquet
    partition_cols=partition_cols, **kwargs)
  File ""/python/lib/python3.7/site-packages/pandas/io/parquet.py"", line 118, in write
    partition_cols=partition_cols, **kwargs)
  File ""/python/lib/python3.7/site-packages/pyarrow/parquet.py"", line 1342, in write_to_dataset
    _mkdir_if_not_exists(fs, root_path)
  File ""/python/lib/python3.7/site-packages/pyarrow/parquet.py"", line 1292, in _mkdir_if_not_exists
    if fs._isfilestore() and not fs.exists(path):
AttributeError: 'NoneType' object has no attribute '_isfilestore'
Exception ignored in: <function AbstractBufferedFile.__del__ at 0x7f529985ca60>
Traceback (most recent call last):
  File ""/python/lib/python3.7/site-packages/fsspec/spec.py"", line 1146, in __del__
    self.close()
  File ""/python/lib/python3.7/site-packages/fsspec/spec.py"", line 1124, in close
    self.flush(force=True)
  File ""/python/lib/python3.7/site-packages/fsspec/spec.py"", line 996, in flush
    self._initiate_upload()
  File ""/python/lib/python3.7/site-packages/s3fs/core.py"", line 941, in _initiate_upload
    Bucket=bucket, Key=key, ACL=self.acl)
  File ""/python/lib/python3.7/site-packages/s3fs/core.py"", line 928, in _call_s3
    **kwargs)
  File ""/python/lib/python3.7/site-packages/s3fs/core.py"", line 182, in _call_s3
    return method(**additional_kwargs)
  File ""/python/lib/python3.7/site-packages/botocore/client.py"", line 357, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/python/lib/python3.7/site-packages/botocore/client.py"", line 648, in _make_api_call
    operation_model, request_dict, request_context)
  File ""/python/lib/python3.7/site-packages/botocore/client.py"", line 667, in _make_request
    return self._endpoint.make_request(operation_model, request_dict)
  File ""/python/lib/python3.7/site-packages/botocore/endpoint.py"", line 102, in make_request
    return self._send_request(request_dict, operation_model)
  File ""/python/lib/python3.7/site-packages/botocore/endpoint.py"", line 137, in _send_request
    success_response, exception):
  File ""/python/lib/python3.7/site-packages/botocore/endpoint.py"", line 231, in _needs_retry
    caught_exception=caught_exception, request_dict=request_dict)
  File ""/python/lib/python3.7/site-packages/botocore/hooks.py"", line 356, in emit
    return self._emitter.emit(aliased_event_name, **kwargs)
  File ""/python/lib/python3.7/site-packages/botocore/hooks.py"", line 228, in emit
    return self._emit(event_name, kwargs)
  File ""/python/lib/python3.7/site-packages/botocore/hooks.py"", line 211, in _emit
    response = handler(**kwargs)
  File ""/python/lib/python3.7/site-packages/botocore/retryhandler.py"", line 183, in __call__
    if self._checker(attempts, response, caught_exception):
  File ""/python/lib/python3.7/site-packages/botocore/retryhandler.py"", line 251, in __call__
    caught_exception)
  File ""/python/lib/python3.7/site-packages/botocore/retryhandler.py"", line 269, in _should_retry
    return self._checker(attempt_number, response, caught_exception)
  File ""/python/lib/python3.7/site-packages/botocore/retryhandler.py"", line 317, in __call__
    caught_exception)
  File ""/python/lib/python3.7/site-packages/botocore/retryhandler.py"", line 223, in __call__
    attempt_number, caught_exception)
  File ""/python/lib/python3.7/site-packages/botocore/retryhandler.py"", line 359, in _check_caught_exception
    raise caught_exception
  File ""/python/lib/python3.7/site-packages/botocore/endpoint.py"", line 200, in _do_get_response
    http_response = self._send(request)
  File ""/python/lib/python3.7/site-packages/botocore/endpoint.py"", line 244, in _send
    return self.http_session.send(request)
  File ""/python/lib/python3.7/site-packages/botocore/httpsession.py"", line 294, in send
    raise HTTPClientError(error=e)
botocore.exceptions.HTTPClientError: An HTTP Client raised and unhandled exception: 'NoneType' object is not iterable
```
#### Expected Output

Expected to see partitioned data show up in S3.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.2.final.0
python-bits: 64
OS: Linux
OS-release: 3.10.0-957.21.3.el7.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: None
pip: 19.0.3
setuptools: 41.0.0
Cython: 0.29.7
numpy: 1.16.2
scipy: 1.3.0
pyarrow: 0.14.0
xarray: None
IPython: 7.5.0
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.1.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: 4.3.3
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: 0.3.0
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
615053867,34086,CI: Failures Botocore Error,alimcmaster1,closed,2020-05-09T00:42:04Z,2020-05-26T09:31:52Z,"Botocore release today changed some Errors

https://pypi.org/project/botocore/#history

```
______________________ TestS3.test_write_s3_parquet_fails ______________________
[gw1] linux -- Python 3.6.10 /home/travis/miniconda3/envs/pandas-dev/bin/python
self = <pandas.tests.io.parser.test_network.TestS3 object at 0x7f5e201c9080>
tips_df =      total_bill   tip     sex smoker   day    time  size
0         16.99  1.01  Female     No   Sun  Dinner     2
1   ....75    Male     No   Sat  Dinner     2
243       18.78  3.00  Female     No  Thur  Dinner     2
[244 rows x 7 columns]
    @td.skip_if_no(""pyarrow"")
    def test_write_s3_parquet_fails(self, tips_df):
        # GH 27679
        with pytest.raises(
            FileNotFoundError, match=""The specified bucket does not exist""
        ):
>           tips_df.to_parquet(""s3://an_s3_bucket_data_doesnt_exit/not_real.parquet"")
```

PR to follow shortly."
615077453,34087,CI: Fix botocore Error,alimcmaster1,closed,2020-05-09T03:06:07Z,2020-05-26T09:31:59Z,"- [x] closes #34086

AWS Service Exceptions are not statically defined in boto3. Easiest to just catch a ClientError here.

Ref Docs:
https://boto3.amazonaws.com/v1/documentation/api/latest/guide/error-handling.html#parsing-error-responses-and-catching-exceptions-from-aws-services"
589248180,33071,Why pd.BooleanDtype() is casted to Float64 by groupby/last?,ghuname,closed,2020-03-27T16:21:42Z,2020-05-26T09:32:20Z,"#### Code Sample, a copy-pastable example if possible

```
>>> import pandas as pd
>>>
>>> df = pd.DataFrame({'a': ['x', 'x', 'y', 'y'], 'b': ['x', 'x', 'y', 'y'], 'c': [False, False, True, False]})
>>> df['d'] = df.c.astype(pd.BooleanDtype())
>>>
>>> df.dtypes
a     object
b     object
c       bool
d    boolean
dtype: object
>>>
>>> df.groupby(['a', 'b']).c.last()
a  b
x  x    False
y  y    False
Name: c, dtype: bool
>>>
>>> df.groupby(['a', 'b']).d.last()
a  b
x  x    0.0
y  y    0.0
Name: d, dtype: float64
>
```

#### Problem description

df.groupby(['a', 'b']).c.last() returns False, but df.groupby(['a', 'b']).d.last() returns Float64.
Why the difference?

#### Expected Output

I expect that both values should be False

#### Output of ``pd.show_versions()``

python           : 3.7.4.final.0
pandas           : 1.0.3
"
603786630,33693,BUG: Fix memory issues in rolling.min/max,s-scherrer,closed,2020-04-21T07:52:23Z,2020-05-26T09:32:45Z,"This fixes at least the reproducible part of #30726. I am not
totally sure what is going on here (is this a true memory leak?), and whether this fixes all issues, but it does strongly reduce memory usage as measured by `psutil`.

My tests have shown that there are two solutions that avoid growing memory
usage:

- pass memoryviews (`float64_t[:]`) instead of `ndarray[float64_t]`
- remove `starti` and `endi` as arguments to `_roll_min_max_fixed`

This commit implements both, since `_roll_min_max_fixed` doesn't use `starti` and `endi` anyways.

- [x] fixes at least part of 
closes #30726 
closes #32466
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry


I am unsure about testing: Does this need a test? If so, what would be a good way to go about testing? The tests I performed (example code in the linked issue) are probably very system specific."
545633737,30726,Rolling min/max gives malloc error,Arsilla,closed,2020-01-06T09:43:32Z,2020-05-26T09:32:54Z,"#### Code Sample

```python
import pandas as pd
import numpy as np
import skimage
from scipy import signal

for orient in [0, 1]:
    th = int(input_img.shape[orient] / 100)

    peaks, info = signal.find_peaks(1 - bw_img.mean(orient), prominence=.35, width=2)
    for pk, w in zip(peaks, info['widths']):
        w *= 2
        if orient == 0:
            sign = bw_img[:, pk]
        else:
            sign = bw_img[pk, :]
        sign = pd.Series(sign).rolling(th).max()

```
#### Problem description
The above snippet is part of a function called in my main script. Running this results in either a `malloc: Incorrect checksum for freed object 0x7fbf626f1f30: probably modified after being freed.` error or a segmentation fault. 
The culprit appears to be the `rolling().max()` line, since commenting out the line fixes the issue, as does replacing `.max()` with `.mean()`.

I can't seem to recreate the error running the above snippet alone, and I cannot figure out why. The input (`bw_img`) is just a 2D array (black and white image).

It might be related to this issue https://github.com/pandas-dev/pandas/issues/25893 expect my memory doesn't seem to be leaking. The two variants I keep seeing seem to be a checksum failed after changing deallocated memory, or that an attempted change of deallocated memory is caught.

python version: 3.6.5 (also tested on 3.7.0)
pandas version 0.25.3 (also tested 0.24 and 0.23)

Below the stacktrace:

```
Process:               python3.6 [61410]
Path:                  /Users/USER/*/python3.6
Identifier:            python3.6
Version:               ???
Code Type:             X86-64 (Native)
Parent Process:        zsh [41537]
Responsible:           python3.6 [61410]
User ID:               305159407

Date/Time:             2020-01-06 09:43:30.365 +0100
OS Version:            Mac OS X 10.14.3 (18D109)
Report Version:        12
Bridge OS Version:     3.0 (14Y674)
Anonymous UUID:        842CB73B-82E5-7A43-1D47-0BCD9BFB56A9


Time Awake Since Boot: 5500 seconds

System Integrity Protection: enabled

Crashed Thread:        0  Dispatch queue: com.apple.main-thread

Exception Type:        EXC_CRASH (SIGABRT)
Exception Codes:       0x0000000000000000, 0x0000000000000000
Exception Note:        EXC_CORPSE_NOTIFY

Application Specific Information:
abort() called
python(61410,0x1134fe5c0) malloc: Incorrect checksum for freed object 0x7f8c83801610: probably modified after being freed.
Corrupt value: 0x28
 

Thread 0 Crashed:: Dispatch queue: com.apple.main-thread
0   libsystem_kernel.dylib        	0x00007fff5b5fb23e __pthread_kill + 10
1   libsystem_pthread.dylib       	0x00007fff5b6b1c1c pthread_kill + 285
2   libsystem_c.dylib             	0x00007fff5b5641c9 abort + 127
3   libsystem_malloc.dylib        	0x00007fff5b6736e2 malloc_vreport + 545
4   libsystem_malloc.dylib        	0x00007fff5b68786c malloc_zone_error + 184
5   libsystem_malloc.dylib        	0x00007fff5b670103 tiny_free_list_remove_ptr + 544
6   libsystem_malloc.dylib        	0x00007fff5b66daee tiny_free_no_lock + 933
7   libsystem_malloc.dylib        	0x00007fff5b66d631 free_tiny + 483
8   _multiarray_umath.cpython-36m-darwin.so	0x0000000109b3357d _buffer_clear_info + 109
9   _multiarray_umath.cpython-36m-darwin.so	0x0000000109b334ff _dealloc_cached_buffer_info + 79
10  _multiarray_umath.cpython-36m-darwin.so	0x0000000109ae5152 array_dealloc + 18
11  window.cpython-36m-darwin.so  	0x000000012c56a8a6 __pyx_fuse_9__pyx_f_6pandas_5_libs_6window__roll_min_max(tagPyArrayObject_fields*, long, long, _object*, _object*, int) + 3206
12  window.cpython-36m-darwin.so  	0x000000012c569619 __pyx_fuse_9__pyx_pw_6pandas_5_libs_6window_59roll_max(_object*, _object*, _object*) + 425
13  algos.cpython-36m-darwin.so   	0x000000012aa1a42c __pyx_FusedFunction_call + 812
14  python                        	0x0000000109410ae5 PyObject_Call + 101
15  python                        	0x00000001094eea1b _PyEval_EvalFrameDefault + 25787
16  python                        	0x00000001094f2356 _PyEval_EvalCodeWithName + 2902
17  python                        	0x00000001094f2aeb fast_function + 411
18  python                        	0x00000001094f1729 call_function + 553
19  python                        	0x00000001094ee694 _PyEval_EvalFrameDefault + 24884
20  python                        	0x00000001094f2356 _PyEval_EvalCodeWithName + 2902
21  python                        	0x00000001094f2aeb fast_function + 411
22  python                        	0x00000001094f1729 call_function + 553
23  python                        	0x00000001094ee608 _PyEval_EvalFrameDefault + 24744
24  python                        	0x00000001094f2356 _PyEval_EvalCodeWithName + 2902
25  python                        	0x00000001094f2ede _PyFunction_FastCallDict + 606
26  python                        	0x0000000109410cba _PyObject_FastCallDict + 202
27  python                        	0x0000000109410e6c _PyObject_Call_Prepend + 156
28  python                        	0x0000000109410ae5 PyObject_Call + 101
29  python                        	0x00000001094eea1b _PyEval_EvalFrameDefault + 25787
30  python                        	0x00000001094f2356 _PyEval_EvalCodeWithName + 2902
31  python                        	0x00000001094f2ede _PyFunction_FastCallDict + 606
32  python                        	0x0000000109410cba _PyObject_FastCallDict + 202
33  python                        	0x0000000109410e6c _PyObject_Call_Prepend + 156
34  python                        	0x0000000109410ae5 PyObject_Call + 101
35  python                        	0x00000001094eea1b _PyEval_EvalFrameDefault + 25787
36  python                        	0x00000001094f2356 _PyEval_EvalCodeWithName + 2902
37  python                        	0x00000001094f2aeb fast_function + 411
38  python                        	0x00000001094f1729 call_function + 553
39  python                        	0x00000001094ee608 _PyEval_EvalFrameDefault + 24744
40  python                        	0x00000001094f2356 _PyEval_EvalCodeWithName + 2902
41  python                        	0x00000001094f2aeb fast_function + 411
42  python                        	0x00000001094f1729 call_function + 553
43  python                        	0x00000001094ee694 _PyEval_EvalFrameDefault + 24884
44  python                        	0x00000001094f2356 _PyEval_EvalCodeWithName + 2902
45  python                        	0x00000001094f2aeb fast_function + 411
46  python                        	0x00000001094f1729 call_function + 553
47  python                        	0x00000001094ee608 _PyEval_EvalFrameDefault + 24744
48  python                        	0x00000001094f2356 _PyEval_EvalCodeWithName + 2902
49  python                        	0x00000001094e84d0 PyEval_EvalCode + 48
50  python                        	0x000000010952156e PyRun_FileExFlags + 174
51  python                        	0x0000000109520b1a PyRun_SimpleFileExFlags + 266
52  python                        	0x000000010953d8b6 Py_Main + 3542
53  python                        	0x0000000109405c78 main + 248
54  libdyld.dylib                 	0x00007fff5b4bbed9 start + 1

Thread 1:
0   libsystem_kernel.dylib        	0x00007fff5b5f87de __psynch_cvwait + 10
1   libsystem_pthread.dylib       	0x00007fff5b6b2593 _pthread_cond_wait + 724
2   python                        	0x0000000109539f1f PyThread_acquire_lock_timed + 351
3   python                        	0x000000010954099f acquire_timed + 111
4   python                        	0x000000010954071c lock_PyThread_acquire_lock + 44
5   python                        	0x000000010945fbfb _PyCFunction_FastCallDict + 475
6   python                        	0x00000001094f175a call_function + 602
7   python                        	0x00000001094ee608 _PyEval_EvalFrameDefault + 24744
8   python                        	0x00000001094f2356 _PyEval_EvalCodeWithName + 2902
9   python                        	0x00000001094f2aeb fast_function + 411
10  python                        	0x00000001094f1729 call_function + 553
11  python                        	0x00000001094ee608 _PyEval_EvalFrameDefault + 24744
12  python                        	0x00000001094f2356 _PyEval_EvalCodeWithName + 2902
13  python                        	0x00000001094f2aeb fast_function + 411
14  python                        	0x00000001094f1729 call_function + 553
15  python                        	0x00000001094ee608 _PyEval_EvalFrameDefault + 24744
16  python                        	0x00000001094f2b89 fast_function + 569
17  python                        	0x00000001094f1729 call_function + 553
18  python                        	0x00000001094ee608 _PyEval_EvalFrameDefault + 24744
19  python                        	0x00000001094f2b89 fast_function + 569
20  python                        	0x00000001094f1729 call_function + 553
21  python                        	0x00000001094ee608 _PyEval_EvalFrameDefault + 24744
22  python                        	0x00000001094f3069 _PyFunction_FastCallDict + 1001
23  python                        	0x0000000109410cba _PyObject_FastCallDict + 202
24  python                        	0x0000000109410e6c _PyObject_Call_Prepend + 156
25  python                        	0x0000000109410ae5 PyObject_Call + 101
26  python                        	0x0000000109541216 t_bootstrap + 70
27  libsystem_pthread.dylib       	0x00007fff5b6af305 _pthread_body + 126
28  libsystem_pthread.dylib       	0x00007fff5b6b226f _pthread_start + 70
29  libsystem_pthread.dylib       	0x00007fff5b6ae415 thread_start + 13

Thread 2:
0   libsystem_pthread.dylib       	0x00007fff5b6ae3f8 start_wqthread + 0
1   ???                           	0x0000000054485244 0 + 1414025796

Thread 3:
0   libsystem_pthread.dylib       	0x00007fff5b6ae3f8 start_wqthread + 0
1   ???                           	0x0000000054485244 0 + 1414025796

Thread 4:
0   libsystem_pthread.dylib       	0x00007fff5b6ae3f8 start_wqthread + 0
1   ???                           	0x0000000054485244 0 + 1414025796

Thread 5:
0   libsystem_pthread.dylib       	0x00007fff5b6ae3f8 start_wqthread + 0
1   ???                           	0x0000000054485244 0 + 1414025796

Thread 6:
0   libsystem_pthread.dylib       	0x00007fff5b6ae3f8 start_wqthread + 0
1   ???                           	0x0000000054485244 0 + 1414025796

Thread 7:
0   libsystem_pthread.dylib       	0x00007fff5b6ae3f8 start_wqthread + 0
1   ???                           	0x0000000054485244 0 + 1414025796

Thread 8:
0   libsystem_pthread.dylib       	0x00007fff5b6ae3f8 start_wqthread + 0
1   ???                           	0x0000000054485244 0 + 1414025796

Thread 0 crashed with X86 Thread State (64-bit):
  rax: 0x0000000000000000  rbx: 0x00000001134fe5c0  rcx: 0x00007ffee67f8168  rdx: 0x0000000000000000
  rdi: 0x0000000000000307  rsi: 0x0000000000000006  rbp: 0x00007ffee67f81a0  rsp: 0x00007ffee67f8168
   r8: 0x0000000000000000   r9: 0x00007ffee67f80c0  r10: 0x0000000000000000  r11: 0x0000000000000206
  r12: 0x0000000000000307  r13: 0x0000000111e4d000  r14: 0x0000000000000006  r15: 0x000000000000002d
  rip: 0x00007fff5b5fb23e  rfl: 0x0000000000000206  cr2: 0x00007fff8e27a188
  
Logical CPU:     0
Error Code:      0x02000148
Trap Number:     133


VM Region Summary:
ReadOnly portion of Libraries: Total=708.7M resident=0K(0%) swapped_out_or_unallocated=708.7M(100%)
Writable regions: Total=483.8M written=0K(0%) resident=0K(0%) swapped_out=0K(0%) unallocated=483.8M(100%)
 
                                VIRTUAL   REGION 
REGION TYPE                        SIZE    COUNT (non-coalesced) 
===========                     =======  ======= 
Activity Tracing                   256K        2 
Dispatch continuations            16.0M        2 
Kernel Alloc Once                    8K        2 
MALLOC                           170.5M       33 
MALLOC guard page                   16K        5 
MALLOC_LARGE (reserved)            256K        3         reserved VM address space (unallocated)
STACK GUARD                         36K       10 
Stack                             24.6M       10 
VM_ALLOCATE                      102.3M      174 
VM_ALLOCATE (reserved)           160.0M        4         reserved VM address space (unallocated)
__DATA                            42.7M      669 
__FONT_DATA                          4K        2 
__LINKEDIT                       253.2M      312 
__TEXT                           455.5M      557 
__UNICODE                          564K        2 
shared memory                       12K        4 
===========                     =======  ======= 
TOTAL                              1.2G     1775 
TOTAL, minus reserved VM space     1.0G     1775 


```
"
597774679,33450,"BUG: maximum of pd.Series([np.nan],dtype=ordered_category) raise",mizuy,closed,2020-04-10T08:42:40Z,2020-05-26T09:33:58Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
In [1]: pd.__version__
Out[1]: '1.0.3'

In [2]: pd.Series([np.nan],dtype=pd.CategoricalDtype([0,1],ordered=True)).max()
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-2-5a47d189b696> in <module>
----> 1 pd.Series([np.nan],dtype=pd.CategoricalDtype([0,1],ordered=True)).max()

~/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs)
  11213             return self._agg_by_level(name, axis=axis, level=level, skipna=skipna)
  11214         return self._reduce(
> 11215             f, name, axis=axis, skipna=skipna, numeric_only=numeric_only
  11216         )
  11217

~/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)
   3870
   3871         if isinstance(delegate, Categorical):
-> 3872             return delegate._reduce(name, skipna=skipna, **kwds)
   3873         elif isinstance(delegate, ExtensionArray):
   3874             # dispatch to ExtensionArray interface

~/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, **kwargs)
   2123         if func is None:
   2124             raise TypeError(f""Categorical cannot perform the operation {name}"")
-> 2125         return func(**kwargs)
   2126
   2127     @deprecate_kwarg(old_arg_name=""numeric_only"", new_arg_name=""skipna"")

~/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs)
    212                 else:
    213                     kwargs[new_arg_name] = new_arg_value
--> 214             return func(*args, **kwargs)
    215
    216         return cast(F, wrapper)

~/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in max(self, skipna)
   2188         if not good.all():
   2189             if skipna:
-> 2190                 pointer = self._codes[good].max()
   2191             else:
   2192                 return np.nan

~/.pyenv/versions/3.7.4/lib/python3.7/site-packages/numpy/core/_methods.py in _amax(a, axis, out, keepdims, initial, where)
     28 def _amax(a, axis=None, out=None, keepdims=False,
     29           initial=_NoValue, where=True):
---> 30     return umr_maximum(a, axis, None, out, keepdims, initial, where)
     31
     32 def _amin(a, axis=None, out=None, keepdims=False,

ValueError: zero-size array to reduction operation maximum which has no identity
```

In the older version, the same code didn't raise an error.

```python
In [10]: pd.__version__
Out[10]: '0.25.3'

In [11]: pd.Series([np.nan],dtype=pd.CategoricalDtype([0,1],ordered=True)).max()
    ...:
Out[11]: nan
```

#### Problem description

Because of this behavior, I failed to df.groupby().max() for ordered categories.

#### Expected Output

Expected output should be np.nan

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 20.0.2
setuptools       : 40.8.0
Cython           : None
pytest           : 5.2.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.2
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : 0.15.0
pytables         : None
pytest           : 5.2.1
pyxlsb           : None
s3fs             : 0.2.2
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
tabulate         : 0.8.5
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.2
numba            : None

</details>
"
602472051,33629,BUG: Fix Categorical use_inf_as_na bug,dsaxton,closed,2020-04-18T14:25:58Z,2020-05-26T09:34:25Z,"- [x] closes #33594
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
601296626,33594,BUG: pandas 1.0 dropna error with categorical data if pd.options.mode.use_inf_as_na = True,thebucc,closed,2020-04-16T18:06:38Z,2020-05-26T09:34:35Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np
from pandas.api.types import CategoricalDtype

# with categorical column and use_inf_as_na = True -> ERROR
pd.options.mode.use_inf_as_na = True
df1 = pd.DataFrame([['a1', 'good'], ['b1', 'good'], ['c1', 'good'], ['d1', 'bad']], columns=['C1', 'C2'])
df2 = pd.DataFrame([['a1', 'good'], ['b1', np.inf], ['c1', np.NaN], ['d1', 'bad']], columns=['C1', 'C2'])
categories = CategoricalDtype(categories=['good', 'bad'], ordered=True)
df1.loc[:, 'C2'] = df1['C2'].astype(categories)
df2.loc[:, 'C2'] = df2['C2'].astype(categories)
df1.dropna(axis=0)  # ERROR
df2.dropna(axis=0)  # ERROR
```

#### Problem description

With the latest version of pandas (1.0.3, installed via pip on python 3.6.8), DataFrame.dropna returns an error if a column is of type **CategoricalDtype** AND **pd.options.mode.use_inf_as_na = True**. 

Exception with traceback:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/sbucc/miniconda3/envs/tf114/lib/python3.6/site-packages/pandas/core/frame.py"", line 4751, in dropna
    count = agg_obj.count(axis=agg_axis)
  File ""/home/sbucc/miniconda3/envs/tf114/lib/python3.6/site-packages/pandas/core/frame.py"", line 7800, in count
    result = notna(frame).sum(axis=axis)
  File ""/home/sbucc/miniconda3/envs/tf114/lib/python3.6/site-packages/pandas/core/dtypes/missing.py"", line 376, in notna
    res = isna(obj)
  File ""/home/sbucc/miniconda3/envs/tf114/lib/python3.6/site-packages/pandas/core/dtypes/missing.py"", line 126, in isna
    return _isna(obj)
  File ""/home/sbucc/miniconda3/envs/tf114/lib/python3.6/site-packages/pandas/core/dtypes/missing.py"", line 185, in _isna_old
    return obj._constructor(obj._data.isna(func=_isna_old))
  File ""/home/sbucc/miniconda3/envs/tf114/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 555, in isna
    return self.apply(""apply"", func=func)
  File ""/home/sbucc/miniconda3/envs/tf114/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 442, in apply
    applied = getattr(b, f)(**kwargs)
  File ""/home/sbucc/miniconda3/envs/tf114/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 390, in apply
    result = func(self.values, **kwargs)
  File ""/home/sbucc/miniconda3/envs/tf114/lib/python3.6/site-packages/pandas/core/dtypes/missing.py"", line 183, in _isna_old
    return _isna_ndarraylike_old(obj)
  File ""/home/sbucc/miniconda3/envs/tf114/lib/python3.6/site-packages/pandas/core/dtypes/missing.py"", line 283, in _isna_ndarraylike_old
    vec = libmissing.isnaobj_old(values.ravel())
TypeError: Argument 'arr' has incorrect type (expected numpy.ndarray, got Categorical)

This doesn't happen with pandas 0.24.0 or if pd.options.mode.use_inf_as_na = False (default).

#### Expected Output

no error

#### Output of ``pd.show_versions()``

<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-96-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
582289483,32748,DOC: What release version do we want in the header?,TomAugspurger,closed,2020-03-16T13:20:24Z,2020-05-26T09:34:56Z,"I think this has come up before, but which whatsnew document, if any, do we want to link from the header? The ""What's New in 1.0.1"" in the screenshot below.

<img width=""454"" alt=""Screen Shot 2020-03-16 at 8 16 42 AM"" src=""https://user-images.githubusercontent.com/1312546/76762092-a15c7800-675e-11ea-864e-11978f436b86.png"">

Choices

1. Latest release (e.g. 1.0.2)
2. Last major release (e.g. 1.0.0)
3. None

cc @jorisvandenbossche @datapythonista.

(I think my preference is for option 3, since it's not clear whether 1 or 2 is better, so put it behind an extra click via the ""Release Notes"" page)."
594051471,33292,REGR: Fix bug when replacing categorical value with self,dsaxton,closed,2020-04-04T20:04:59Z,2020-05-26T09:35:15Z,"- [ ] closes #33288
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
593988366,33288,REGR: Replacing a category with itself replaces it with np.nan,jtilly,closed,2020-04-04T18:28:55Z,2020-05-26T09:35:26Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] I have confirmed this bug exists on the master branch of pandas.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
pd.Series([""a"", ""b""]).astype(""category"").replace(""a"", ""a"")
# 0    NaN
# 1      b
# dtype: category
# Categories (1, object): [b]
```

Operating on the categorical array directly, i.e. `pd.Categorical([""a"", ""b""]).replace(""a"", ""a"")` yields the same result.

#### Problem description

Replacing a category with itself replaces it with `np.nan`. This problem was introduced with 1.0.0.

#### Expected Output

I would have expected the behavior from 0.25.3:
```python
pd.Series([""a"", ""b""]).astype(""category"").replace(""a"", ""a"")
# 0    a
# 1    b
# dtype: category
# Categories (2, object): [a, b]
```

Note that if we work with lists, we get
```python
pd.Series([""a"", ""b""]).astype(""category"").replace([""a""], [""a""])
# dtype: category
# 0    a
# 1    b
# type: object
```
which is also not what I would expect, because we're now losing the dtype. This behavior has been described elsewhere (e.g. https://github.com/pandas-dev/pandas/pull/31734#issuecomment-583130043) and it's consistent with 0.25.3.

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-176-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200325
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```

</details>
"
606181490,33761,REGR: fix DataFrame reduction with EA columns and numeric_only=True,jorisvandenbossche,closed,2020-04-24T09:39:35Z,2020-05-26T09:36:02Z,"Closes #33256

For the block-wise path in `DataFrame._reduce`, we ensure to use the EA reduction for blocks holding EAs, instead of passing the EA to the `op` function (which is typically the `nanops` nan function).

This needs https://github.com/pandas-dev/pandas/pull/33758 to be merged, since that PR fixes the test that is failing here."
593299194,33256,REGR: DataFrame.mean(numeric_only=True) raises AttributeError on v1.0.3,simonjayhawkins,closed,2020-04-03T10:59:48Z,2020-05-26T09:36:15Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import numpy as np
>>>
>>> import pandas as pd
>>>
>>> pd.__version__
'1.0.3'
>>>
>>> df_wide = pd.DataFrame(np.random.randint(1000, size=(1000, 100))).astype(""Int64"").copy()
>>>
>>> df_wide.mean(numeric_only=True)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\simon\pandas\pandas\core\generic.py"", line 11215, in stat_func
    f, name, axis=axis, skipna=skipna, numeric_only=numeric_only
  File ""C:\Users\simon\pandas\pandas\core\frame.py"", line 7896, in _reduce
    res = df._data.reduce(op, axis=1, skipna=skipna, **kwds)
  File ""C:\Users\simon\pandas\pandas\core\internals\managers.py"", line 351, in reduce
    bres = func(blk.values, *args, **kwargs)
  File ""C:\Users\simon\pandas\pandas\core\nanops.py"", line 69, in _f
    return f(*args, **kwargs)
  File ""C:\Users\simon\pandas\pandas\core\nanops.py"", line 102, in f
    if values.size == 0 and kwds.get(""min_count"") is None:
AttributeError: 'IntegerArray' object has no attribute 'size'
>>>

```
#### Problem description

This is a regression from 0.25.3

0aa48f7d9269206dea492ed14d5dfc6f46468de3 is the first bad commit
commit 0aa48f7d9269206dea492ed14d5dfc6f46468de3
Author: jbrockmendel <jbrockmendel@gmail.com>
Date:   Wed Jan 1 09:18:20 2020 -0800

    PERF: perform reductions block-wise (#29847)

on master raises `AttributeError: 'int' object has no attribute 'dtype'`

```
>>> df_wide.mean(numeric_only=True)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\simon\pandas\pandas\core\generic.py"", line 11114, in stat_func
    func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only
  File ""C:\Users\simon\pandas\pandas\core\frame.py"", line 7990, in _reduce
    res = df._data.reduce(blk_func)
  File ""C:\Users\simon\pandas\pandas\core\internals\managers.py"", line 362, in reduce
    bres = func(blk.values, *args, **kwargs)
  File ""C:\Users\simon\pandas\pandas\core\frame.py"", line 7985, in blk_func
    return op(values, axis=0, skipna=skipna, **kwds)
  File ""C:\Users\simon\pandas\pandas\core\nanops.py"", line 120, in f
    result = bn_func(values, axis=axis, **kwds)
  File ""<__array_function__ internals>"", line 6, in nanmean
  File ""C:\Users\simon\Anaconda3\envs\pandas-dev\lib\site-packages\numpy\lib\nanfunctions.py"", line 952, in nanmean
    avg = _divide_by_count(tot, cnt, out=out)
  File ""C:\Users\simon\Anaconda3\envs\pandas-dev\lib\site-packages\numpy\lib\nanfunctions.py"", line 219, in _divide_by_count
    return a.dtype.type(a / b)
AttributeError: 'int' object has no attribute 'dtype'
```

#### Expected Output

```
>>> import numpy as np
>>>
>>> import pandas as pd
>>>
>>> pd.__version__
'0.25.3'
>>>
>>> df_wide = pd.DataFrame(np.random.randint(1000, size=(1000, 100))).astype(""Int64"").copy()
>>>
>>> df_wide.mean(numeric_only=True)
0     520.057
1     507.735
2     501.618
3     506.590
4     501.500
       ...
95    507.594
96    483.273
97    506.330
98    497.068
99    508.118
Length: 100, dtype: float64
>>>
```
"
598010336,33462,BUG: None converted to NaN after groupby first and last,JDkuba,closed,2020-04-10T17:48:02Z,2020-05-26T09:36:35Z,"- [x] closes #32800
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This PR solves #32800. In #32124 `not checknull()` was introduced to `groupfirst()` and `grouplast()`. `None` is not caught in this conditional and it's being converted to `nan` because of that.

I'm a newbie and not sure whether I should add whatsnew entry or not. Should I?"
583526266,32800,Groupby converts None to NaN,BayerSe,closed,2020-03-18T07:33:14Z,2020-05-26T09:36:47Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

df = pd.DataFrame.from_dict({
    'id': ['a'],
    'value': [None]
})

df_grouped = df.groupby('id', as_index=False).first()

print(df)
print(df_grouped)
```

Output:
```python
  id value
0  a  None
  id  value
0  a  NaN
```

#### Problem description

Since at least version 1.0.2,  the type of `df_grouped`  is `NaN`.  In Version 0.25.3, the type was `None`.
This breaks my code since I later check for this value using `if var is None`, which is False when `var` is `NaN` instead of `None`.

#### Expected Output

```python
  id value
0  a  None
  id  value
0  a  None
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-88-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.0.2
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None


</details>
"
611415104,33953,DOC: Update numpy website url,datapythonista,closed,2020-05-03T13:31:39Z,2020-05-26T09:37:20Z,"When clicking on the `See Also` item of https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.argsort.html

We go to the old numpy url (docs.scipy.org), and a warning is shown to the user. We should update the target url of our numpy links.

Sphinx builds automatically these links, I guess the url is in `doc/source/conf.py`.

"
592889837,33246,BLD: recursive inclusion of DLLs in package data,chris-b1,closed,2020-04-02T19:58:38Z,2020-05-26T09:37:43Z,"- [x] step 1 towards #32857

Will also make a PR in https://github.com/MacPython/pandas-wheels to add the DLLs to the wheel packaging script"
603551902,33687,CI/DEP: Use numba.extending.is_jitted for numba version > 0.49.0,mroeschke,closed,2020-04-20T21:37:24Z,2020-05-26T09:38:20Z,"- [x] xref #33686
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
592668540,33238,CI/COMPAT: Linux py37_np_dev pipeline timeouts,AlexKirko,closed,2020-04-02T14:17:55Z,2020-05-26T09:39:13Z,"#### Problem description

Linux py37_np_dev pipeline appears to timeout for everyone after 60 minutes.
There are a couple hundred thousand errors like this:
```
Exception ignored in: 'pandas.io.sas._sas.Parser.process_byte_array_with_data'
DeprecationWarning: tostring() is deprecated. Use tobytes() instead.
DeprecationWarning: tostring() is deprecated. Use tobytes() instead.
```
Here is a [link](https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=32212&view=logs&j=3a03f79d-0b41-5610-1aa4-b4a014d0bc70&t=4d05ed0e-1ed3-5bff-dd63-1e957f2766a9&l=792078) to it failing for me."
552133021,31146,Remove possibly illegal test data,rebecca-palmer,closed,2020-01-20T08:05:33Z,2020-05-26T09:39:30Z,"Much of the test_html data looks like it was saved from real web pages, and not all of them look like the kind that are under a free software license.  (I couldn't actually check because only one of the three identifies the source site, and that site raises a security warning.)

This removes the questionable data, and adds tests that try to do something equivalent on known-free data.

If I am mistaken and these pages are under a free license, please instead document that.

This data was added in:
https://github.com/pandas-dev/pandas/commit/1d573b4421d5ff16fd68ea0cc6c63f5f1c7ebcac computer_sales_page (HP), for MultiIndex header and empty string handling in headers (should give empty not NaN)
https://github.com/pandas-dev/pandas/commit/e22fe1b25d1ff9d5c880f6c35bc91dfab7aa0b84 nyse_wsj, for thousands separator
https://github.com/pandas-dev/pandas/commit/92aa2774ca9aa89d0a2cca011707b80db995df92 macau, for MultiIndex header and thousands separator"
594744831,33309,DOC: include Offset.__call__ to autosummary to fix sphinx warning,jbrockmendel,closed,2020-04-06T01:23:39Z,2020-05-26T09:39:48Z,
589421703,33080,CI troubleshoot azure,jbrockmendel,closed,2020-03-27T21:27:01Z,2020-05-26T09:40:07Z,"- [ ] closes #33077
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
589378317,33077,CI: Test failing on master,TomAugspurger,closed,2020-03-27T19:49:03Z,2020-05-26T09:40:21Z,"An s3 / pyarrow test is failing on master. Does anyone have time to debug it? 
https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=31733&view=logs&j=a3a13ea8-7cf0-5bdb-71bb-6ac8830ae35c&t=add65f64-6c25-5783-8fd6-d9aa1b63d9d4&l=115.

There was an s3fs release earlier today, if that's related.

```pytb
2020-03-27T18:16:37.6354466Z =================================== FAILURES ===================================
2020-03-27T18:16:37.6354971Z _____________________ TestParquetPyArrow.test_s3_roundtrip _____________________
2020-03-27T18:16:37.6356687Z [gw1] linux -- Python 3.7.6 /home/vsts/miniconda3/envs/pandas-dev/bin/python
2020-03-27T18:16:37.6357366Z 
2020-03-27T18:16:37.6358186Z value = 'äº”, 27 3æœˆ 2020 18:11:53 GMT', tzinfo = <class 'dateutil.tz.tz.tzlocal'>
2020-03-27T18:16:37.6358700Z 
2020-03-27T18:16:37.6359050Z     def _parse_timestamp_with_tzinfo(value, tzinfo):
2020-03-27T18:16:37.6359491Z         """"""Parse timestamp with pluggable tzinfo options.""""""
2020-03-27T18:16:37.6359936Z         if isinstance(value, (int, float)):
2020-03-27T18:16:37.6360574Z             # Possibly an epoch time.
2020-03-27T18:16:37.6361048Z             return datetime.datetime.fromtimestamp(value, tzinfo())
2020-03-27T18:16:37.6361681Z         else:
2020-03-27T18:16:37.6362073Z             try:
2020-03-27T18:16:37.6362431Z                 return datetime.datetime.fromtimestamp(float(value), tzinfo())
2020-03-27T18:16:37.6362828Z             except (TypeError, ValueError):
2020-03-27T18:16:37.6363153Z                 pass
2020-03-27T18:16:37.6363414Z         try:
2020-03-27T18:16:37.6363776Z             # In certain cases, a timestamp marked with GMT can be parsed into a
2020-03-27T18:16:37.6364222Z             # different time zone, so here we provide a context which will
2020-03-27T18:16:37.6364618Z             # enforce that GMT == UTC.
2020-03-27T18:16:37.6365341Z >           return dateutil.parser.parse(value, tzinfos={'GMT': tzutc()})
2020-03-27T18:16:37.6365750Z 
2020-03-27T18:16:37.6366507Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/botocore/utils.py:607: 
2020-03-27T18:16:37.6367110Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-03-27T18:16:37.6367487Z 
2020-03-27T18:16:37.6368063Z timestr = 'äº”, 27 3æœˆ 2020 18:11:53 GMT', parserinfo = None
2020-03-27T18:16:37.6368765Z kwargs = {'tzinfos': {'GMT': tzutc()}}
2020-03-27T18:16:37.6369118Z 
2020-03-27T18:16:37.6369430Z     def parse(timestr, parserinfo=None, **kwargs):
2020-03-27T18:16:37.6369874Z         """"""
2020-03-27T18:16:37.6370113Z     
2020-03-27T18:16:37.6370427Z         Parse a string in one of the supported formats, using the
2020-03-27T18:16:37.6370810Z         ``parserinfo`` parameters.
2020-03-27T18:16:37.6371081Z     
2020-03-27T18:16:37.6371354Z         :param timestr:
2020-03-27T18:16:37.6371687Z             A string containing a date/time stamp.
2020-03-27T18:16:37.6371991Z     
2020-03-27T18:16:37.6372246Z         :param parserinfo:
2020-03-27T18:16:37.6372622Z             A :class:`parserinfo` object containing parameters for the parser.
2020-03-27T18:16:37.6373134Z             If ``None``, the default arguments to the :class:`parserinfo`
2020-03-27T18:16:37.6373533Z             constructor are used.
2020-03-27T18:16:37.6373811Z     
2020-03-27T18:16:37.6374133Z         The ``**kwargs`` parameter takes the following keyword arguments:
2020-03-27T18:16:37.6374471Z     
2020-03-27T18:16:37.6374723Z         :param default:
2020-03-27T18:16:37.6375085Z             The default datetime object, if this is a datetime object and not
2020-03-27T18:16:37.6375578Z             ``None``, elements specified in ``timestr`` replace elements in the
2020-03-27T18:16:37.6375955Z             default object.
2020-03-27T18:16:37.6376228Z     
2020-03-27T18:16:37.6376479Z         :param ignoretz:
2020-03-27T18:16:37.6376863Z             If set ``True``, time zones in parsed strings are ignored and a naive
2020-03-27T18:16:37.6377278Z             :class:`datetime` object is returned.
2020-03-27T18:16:37.6377589Z     
2020-03-27T18:16:37.6378571Z         :param tzinfos:
2020-03-27T18:16:37.6379161Z             Additional time zone names / aliases which may be present in the
2020-03-27T18:16:37.6379643Z             string. This argument maps time zone names (and optionally offsets
2020-03-27T18:16:37.6380091Z             from those time zones) to time zones. This parameter can be a
2020-03-27T18:16:37.6380546Z             dictionary with timezone aliases mapping time zone names to time
2020-03-27T18:16:37.6380981Z             zones or a function taking two parameters (``tzname`` and
2020-03-27T18:16:37.6381394Z             ``tzoffset``) and returning a time zone.
2020-03-27T18:16:37.6381699Z     
2020-03-27T18:16:37.6382025Z             The timezones to which the names are mapped can be an integer
2020-03-27T18:16:37.6382494Z             offset from UTC in seconds or a :class:`tzinfo` object.
2020-03-27T18:16:37.6382838Z     
2020-03-27T18:16:37.6383237Z             .. doctest::
2020-03-27T18:16:37.6383606Z                :options: +NORMALIZE_WHITESPACE
2020-03-27T18:16:37.6384047Z     
2020-03-27T18:16:37.6384378Z                 >>> from dateutil.parser import parse
2020-03-27T18:16:37.6384771Z                 >>> from dateutil.tz import gettz
2020-03-27T18:16:37.6385594Z                 >>> tzinfos = {""BRST"": -7200, ""CST"": gettz(""America/Chicago"")}
2020-03-27T18:16:37.6386479Z                 >>> parse(""2012-01-19 17:21:00 BRST"", tzinfos=tzinfos)
2020-03-27T18:16:37.6387401Z                 datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))
2020-03-27T18:16:37.6388159Z                 >>> parse(""2012-01-19 17:21:00 CST"", tzinfos=tzinfos)
2020-03-27T18:16:37.6388663Z                 datetime.datetime(2012, 1, 19, 17, 21,
2020-03-27T18:16:37.6389349Z                                   tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))
2020-03-27T18:16:37.6390349Z     
2020-03-27T18:16:37.6390823Z             This parameter is ignored if ``ignoretz`` is set.
2020-03-27T18:16:37.6391283Z     
2020-03-27T18:16:37.6391595Z         :param dayfirst:
2020-03-27T18:16:37.6392353Z             Whether to interpret the first value in an ambiguous 3-integer date
2020-03-27T18:16:37.6393105Z             (e.g. 01/05/09) as the day (``True``) or month (``False``). If
2020-03-27T18:16:37.6393685Z             ``yearfirst`` is set to ``True``, this distinguishes between YDM and
2020-03-27T18:16:37.6394156Z             YMD. If set to ``None``, this value is retrieved from the current
2020-03-27T18:16:37.6394610Z             :class:`parserinfo` object (which itself defaults to ``False``).
2020-03-27T18:16:37.6394968Z     
2020-03-27T18:16:37.6395226Z         :param yearfirst:
2020-03-27T18:16:37.6395841Z             Whether to interpret the first value in an ambiguous 3-integer date
2020-03-27T18:16:37.6396511Z             (e.g. 01/05/09) as the year. If ``True``, the first number is taken to
2020-03-27T18:16:37.6397349Z             be the year, otherwise the last number is taken to be the year. If
2020-03-27T18:16:37.6398069Z             this is set to ``None``, the value is retrieved from the current
2020-03-27T18:16:37.6398617Z             :class:`parserinfo` object (which itself defaults to ``False``).
2020-03-27T18:16:37.6399037Z     
2020-03-27T18:16:37.6399333Z         :param fuzzy:
2020-03-27T18:16:37.6399809Z             Whether to allow fuzzy parsing, allowing for string like ""Today is
2020-03-27T18:16:37.6400291Z             January 1, 2047 at 8:21:00AM"".
2020-03-27T18:16:37.6400628Z     
2020-03-27T18:16:37.6400954Z         :param fuzzy_with_tokens:
2020-03-27T18:16:37.6401391Z             If ``True``, ``fuzzy`` is automatically set to True, and the parser
2020-03-27T18:16:37.6401903Z             will return a tuple where the first element is the parsed
2020-03-27T18:16:37.6402536Z             :class:`datetime.datetime` datetimestamp and the second element is
2020-03-27T18:16:37.6403633Z             a tuple containing the portions of the string which were ignored:
2020-03-27T18:16:37.6404036Z     
2020-03-27T18:16:37.6404341Z             .. doctest::
2020-03-27T18:16:37.6404638Z     
2020-03-27T18:16:37.6405107Z                 >>> from dateutil.parser import parse
2020-03-27T18:16:37.6405634Z                 >>> parse(""Today is January 1, 2047 at 8:21:00AM"", fuzzy_with_tokens=True)
2020-03-27T18:16:37.6406513Z                 (datetime.datetime(2047, 1, 1, 8, 21), (u'Today is ', u' ', u'at '))
2020-03-27T18:16:37.6407199Z     
2020-03-27T18:16:37.6407464Z         :return:
2020-03-27T18:16:37.6407834Z             Returns a :class:`datetime.datetime` object or, if the
2020-03-27T18:16:37.6408278Z             ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the
2020-03-27T18:16:37.6408981Z             first element being a :class:`datetime.datetime` object, the second
2020-03-27T18:16:37.6409472Z             a tuple containing the fuzzy tokens.
2020-03-27T18:16:37.6409793Z     
2020-03-27T18:16:37.6416049Z         :raises ValueError:
2020-03-27T18:16:37.6416719Z             Raised for invalid or unknown string format, if the provided
2020-03-27T18:16:37.6417444Z             :class:`tzinfo` is not in a valid format, or if an invalid date
2020-03-27T18:16:37.6417891Z             would be created.
2020-03-27T18:16:37.6418311Z     
2020-03-27T18:16:37.6418726Z         :raises OverflowError:
2020-03-27T18:16:37.6419273Z             Raised if the parsed date exceeds the largest valid C integer on
2020-03-27T18:16:37.6419711Z             your system.
2020-03-27T18:16:37.6420018Z         """"""
2020-03-27T18:16:37.6420330Z         if parserinfo:
2020-03-27T18:16:37.6420877Z             return parser(parserinfo).parse(timestr, **kwargs)
2020-03-27T18:16:37.6421255Z         else:
2020-03-27T18:16:37.6421644Z >           return DEFAULTPARSER.parse(timestr, **kwargs)
2020-03-27T18:16:37.6422091Z 
2020-03-27T18:16:37.6423228Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/dateutil/parser/_parser.py:1374: 
2020-03-27T18:16:37.6423959Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-03-27T18:16:37.6424342Z 
2020-03-27T18:16:37.6424658Z self = <dateutil.parser._parser.parser object at 0x7f1de46b9350>
2020-03-27T18:16:37.6425310Z timestr = 'äº”, 27 3æœˆ 2020 18:11:53 GMT'
2020-03-27T18:16:37.6425800Z default = datetime.datetime(2020, 3, 27, 0, 0), ignoretz = False
2020-03-27T18:16:37.6426475Z tzinfos = {'GMT': tzutc()}, kwargs = {}, res = None, skipped_tokens = None
2020-03-27T18:16:37.6426876Z 
2020-03-27T18:16:37.6427164Z     def parse(self, timestr, default=None,
2020-03-27T18:16:37.6427538Z               ignoretz=False, tzinfos=None, **kwargs):
2020-03-27T18:16:37.6427849Z         """"""
2020-03-27T18:16:37.6428204Z         Parse the date/time string into a :class:`datetime.datetime` object.
2020-03-27T18:16:37.6428565Z     
2020-03-27T18:16:37.6428819Z         :param timestr:
2020-03-27T18:16:37.6429210Z             Any date/time string using the supported formats.
2020-03-27T18:16:37.6429526Z     
2020-03-27T18:16:37.6429779Z         :param default:
2020-03-27T18:16:37.6430167Z             The default datetime object, if this is a datetime object and not
2020-03-27T18:16:37.6430776Z             ``None``, elements specified in ``timestr`` replace elements in the
2020-03-27T18:16:37.6431441Z             default object.
2020-03-27T18:16:37.6431742Z     
2020-03-27T18:16:37.6432049Z         :param ignoretz:
2020-03-27T18:16:37.6432467Z             If set ``True``, time zones in parsed strings are ignored and a
2020-03-27T18:16:37.6432975Z             naive :class:`datetime.datetime` object is returned.
2020-03-27T18:16:37.6433343Z     
2020-03-27T18:16:37.6433628Z         :param tzinfos:
2020-03-27T18:16:37.6434055Z             Additional time zone names / aliases which may be present in the
2020-03-27T18:16:37.6434682Z             string. This argument maps time zone names (and optionally offsets
2020-03-27T18:16:37.6435260Z             from those time zones) to time zones. This parameter can be a
2020-03-27T18:16:37.6435702Z             dictionary with timezone aliases mapping time zone names to time
2020-03-27T18:16:37.6436155Z             zones or a function taking two parameters (``tzname`` and
2020-03-27T18:16:37.6436696Z             ``tzoffset``) and returning a time zone.
2020-03-27T18:16:37.6437208Z     
2020-03-27T18:16:37.6437620Z             The timezones to which the names are mapped can be an integer
2020-03-27T18:16:37.6438077Z             offset from UTC in seconds or a :class:`tzinfo` object.
2020-03-27T18:16:37.6438437Z     
2020-03-27T18:16:37.6438699Z             .. doctest::
2020-03-27T18:16:37.6439054Z                :options: +NORMALIZE_WHITESPACE
2020-03-27T18:16:37.6439354Z     
2020-03-27T18:16:37.6439678Z                 >>> from dateutil.parser import parse
2020-03-27T18:16:37.6440070Z                 >>> from dateutil.tz import gettz
2020-03-27T18:16:37.6440793Z                 >>> tzinfos = {""BRST"": -7200, ""CST"": gettz(""America/Chicago"")}
2020-03-27T18:16:37.6441604Z                 >>> parse(""2012-01-19 17:21:00 BRST"", tzinfos=tzinfos)
2020-03-27T18:16:37.6442377Z                 datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))
2020-03-27T18:16:37.6443465Z                 >>> parse(""2012-01-19 17:21:00 CST"", tzinfos=tzinfos)
2020-03-27T18:16:37.6443989Z                 datetime.datetime(2012, 1, 19, 17, 21,
2020-03-27T18:16:37.6444715Z                                   tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))
2020-03-27T18:16:37.6445160Z     
2020-03-27T18:16:37.6445519Z             This parameter is ignored if ``ignoretz`` is set.
2020-03-27T18:16:37.6445856Z     
2020-03-27T18:16:37.6446238Z         :param \\*\\*kwargs:
2020-03-27T18:16:37.6447528Z             Keyword arguments as passed to ``_parse()``.
2020-03-27T18:16:37.6448032Z     
2020-03-27T18:16:37.6448353Z         :return:
2020-03-27T18:16:37.6448768Z             Returns a :class:`datetime.datetime` object or, if the
2020-03-27T18:16:37.6449286Z             ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the
2020-03-27T18:16:37.6449817Z             first element being a :class:`datetime.datetime` object, the second
2020-03-27T18:16:37.6450310Z             a tuple containing the fuzzy tokens.
2020-03-27T18:16:37.6450667Z     
2020-03-27T18:16:37.6450965Z         :raises ParserError:
2020-03-27T18:16:37.6451396Z             Raised for invalid or unknown string format, if the provided
2020-03-27T18:16:37.6451904Z             :class:`tzinfo` is not in a valid format, or if an invalid date
2020-03-27T18:16:37.6452350Z             would be created.
2020-03-27T18:16:37.6452800Z     
2020-03-27T18:16:37.6453091Z         :raises TypeError:
2020-03-27T18:16:37.6453885Z             Raised for non-string or character stream input.
2020-03-27T18:16:37.6454326Z     
2020-03-27T18:16:37.6454656Z         :raises OverflowError:
2020-03-27T18:16:37.6455085Z             Raised if the parsed date exceeds the largest valid C integer on
2020-03-27T18:16:37.6455507Z             your system.
2020-03-27T18:16:37.6455922Z         """"""
2020-03-27T18:16:37.6456431Z     
2020-03-27T18:16:37.6456744Z         if default is None:
2020-03-27T18:16:37.6457160Z             default = datetime.datetime.now().replace(hour=0, minute=0,
2020-03-27T18:16:37.6457686Z                                                       second=0, microsecond=0)
2020-03-27T18:16:37.6458066Z     
2020-03-27T18:16:37.6458428Z         res, skipped_tokens = self._parse(timestr, **kwargs)
2020-03-27T18:16:37.6458778Z     
2020-03-27T18:16:37.6459191Z         if res is None:
2020-03-27T18:16:37.6459743Z >           raise ParserError(""Unknown string format: %s"", timestr)
2020-03-27T18:16:37.6460612Z E           dateutil.parser._parser.ParserError: Unknown string format: äº”, 27 3æœˆ 2020 18:11:53 GMT
2020-03-27T18:16:37.6461135Z 
2020-03-27T18:16:37.6461831Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/dateutil/parser/_parser.py:649: ParserError
2020-03-27T18:16:37.6462294Z 
2020-03-27T18:16:37.6462654Z During handling of the above exception, another exception occurred:
2020-03-27T18:16:37.6463101Z 
2020-03-27T18:16:37.6463488Z self = <pandas.tests.io.test_parquet.TestParquetPyArrow object at 0x7f1d76d75210>
2020-03-27T18:16:37.6464218Z df_compat =    A    B
2020-03-27T18:16:37.6464554Z 0  1  foo
2020-03-27T18:16:37.6464841Z 1  2  foo
2020-03-27T18:16:37.6465108Z 2  3  foo
2020-03-27T18:16:37.6465977Z s3_resource = s3.ServiceResource(), pa = 'pyarrow'
2020-03-27T18:16:37.6466419Z 
2020-03-27T18:16:37.6466772Z     def test_s3_roundtrip(self, df_compat, s3_resource, pa):
2020-03-27T18:16:37.6467176Z         # GH #19134
2020-03-27T18:16:37.6467888Z >       check_round_trip(df_compat, pa, path=""s3://pandas-test/pyarrow.parquet"")
2020-03-27T18:16:37.6468598Z 
2020-03-27T18:16:37.6468923Z pandas/tests/io/test_parquet.py:488: 
2020-03-27T18:16:37.6469382Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-03-27T18:16:37.6469883Z pandas/tests/io/test_parquet.py:176: in check_round_trip
2020-03-27T18:16:37.6470262Z     compare(repeat)
2020-03-27T18:16:37.6470636Z pandas/tests/io/test_parquet.py:168: in compare
2020-03-27T18:16:37.6471190Z     actual = read_parquet(path, **read_kwargs)
2020-03-27T18:16:37.6471616Z pandas/io/parquet.py:300: in read_parquet
2020-03-27T18:16:37.6472030Z     return impl.read(path, columns=columns, **kwargs)
2020-03-27T18:16:37.6472668Z pandas/io/parquet.py:113: in read
2020-03-27T18:16:37.6473217Z     path, _, _, should_close = get_filepath_or_buffer(path)
2020-03-27T18:16:37.6473645Z pandas/io/common.py:201: in get_filepath_or_buffer
2020-03-27T18:16:37.6474112Z     filepath_or_buffer, encoding=encoding, compression=compression, mode=mode
2020-03-27T18:16:37.6474560Z pandas/io/s3.py:48: in get_filepath_or_buffer
2020-03-27T18:16:37.6475000Z     file, _fs = get_file_and_filesystem(filepath_or_buffer, mode=mode)
2020-03-27T18:16:37.6475436Z pandas/io/s3.py:29: in get_file_and_filesystem
2020-03-27T18:16:37.6475860Z     file = fs.open(_strip_schema(filepath_or_buffer), mode)
2020-03-27T18:16:37.6476799Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/fsspec/spec.py:774: in open
2020-03-27T18:16:37.6477280Z     **kwargs
2020-03-27T18:16:37.6478236Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/s3fs/core.py:377: in _open
2020-03-27T18:16:37.6478798Z     autocommit=autocommit, requester_pays=requester_pays)
2020-03-27T18:16:37.6479601Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/s3fs/core.py:1095: in __init__
2020-03-27T18:16:37.6480120Z     cache_type=cache_type)
2020-03-27T18:16:37.6480867Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/fsspec/spec.py:1056: in __init__
2020-03-27T18:16:37.6481404Z     self.details = fs.info(path)
2020-03-27T18:16:37.6482347Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/s3fs/core.py:528: in info
2020-03-27T18:16:37.6483012Z     Key=key, **version_id_kw(version_id), **self.req_kw)
2020-03-27T18:16:37.6484814Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/s3fs/core.py:199: in _call_s3
2020-03-27T18:16:37.6485427Z     return method(**additional_kwargs)
2020-03-27T18:16:37.6486423Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/botocore/client.py:316: in _api_call
2020-03-27T18:16:37.6487194Z     return self._make_api_call(operation_name, kwargs)
2020-03-27T18:16:37.6488049Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/botocore/client.py:613: in _make_api_call
2020-03-27T18:16:37.6488645Z     operation_model, request_dict, request_context)
2020-03-27T18:16:37.6489404Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/botocore/client.py:632: in _make_request
2020-03-27T18:16:37.6489999Z     return self._endpoint.make_request(operation_model, request_dict)
2020-03-27T18:16:37.6491059Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/botocore/endpoint.py:102: in make_request
2020-03-27T18:16:37.6491692Z     return self._send_request(request_dict, operation_model)
2020-03-27T18:16:37.6492529Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/botocore/endpoint.py:135: in _send_request
2020-03-27T18:16:37.6493171Z     request, operation_model, context)
2020-03-27T18:16:37.6494139Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/botocore/endpoint.py:167: in _get_response
2020-03-27T18:16:37.6494730Z     request, operation_model)
2020-03-27T18:16:37.6495541Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/botocore/endpoint.py:218: in _do_get_response
2020-03-27T18:16:37.6496167Z     response_dict, operation_model.output_shape)
2020-03-27T18:16:37.6497203Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/botocore/parsers.py:242: in parse
2020-03-27T18:16:37.6497800Z     parsed = self._do_parse(response, shape)
2020-03-27T18:16:37.6498590Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/botocore/parsers.py:774: in _do_parse
2020-03-27T18:16:37.6499160Z     member_shapes, final_parsed)
2020-03-27T18:16:37.6499969Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/botocore/parsers.py:833: in _parse_non_payload_attrs
2020-03-27T18:16:37.6500723Z     member_shape, headers[header_name])
2020-03-27T18:16:37.6501547Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/botocore/parsers.py:302: in _parse_shape
2020-03-27T18:16:37.6502128Z     return handler(shape, node)
2020-03-27T18:16:37.6502916Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/botocore/parsers.py:175: in _get_text_content
2020-03-27T18:16:37.6503488Z     return func(self, shape, text)
2020-03-27T18:16:37.6504298Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/botocore/parsers.py:462: in _handle_timestamp
2020-03-27T18:16:37.6504870Z     return self._timestamp_parser(text)
2020-03-27T18:16:37.6505668Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/site-packages/botocore/utils.py:626: in parse_timestamp
2020-03-27T18:16:37.6506266Z     return _parse_timestamp_with_tzinfo(value, tzinfo)
2020-03-27T18:16:37.6506771Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-03-27T18:16:37.6507148Z 
2020-03-27T18:16:37.6507825Z value = 'äº”, 27 3æœˆ 2020 18:11:53 GMT', tzinfo = <class 'dateutil.tz.tz.tzlocal'>
2020-03-27T18:16:37.6508344Z 
2020-03-27T18:16:37.6508684Z     def _parse_timestamp_with_tzinfo(value, tzinfo):
2020-03-27T18:16:37.6509139Z         """"""Parse timestamp with pluggable tzinfo options.""""""
2020-03-27T18:16:37.6509563Z         if isinstance(value, (int, float)):
2020-03-27T18:16:37.6509962Z             # Possibly an epoch time.
2020-03-27T18:16:37.6510383Z             return datetime.datetime.fromtimestamp(value, tzinfo())
2020-03-27T18:16:37.6510764Z         else:
2020-03-27T18:16:37.6511070Z             try:
2020-03-27T18:16:37.6511478Z                 return datetime.datetime.fromtimestamp(float(value), tzinfo())
2020-03-27T18:16:37.6511946Z             except (TypeError, ValueError):
2020-03-27T18:16:37.6512297Z                 pass
2020-03-27T18:16:37.6512606Z         try:
2020-03-27T18:16:37.6513006Z             # In certain cases, a timestamp marked with GMT can be parsed into a
2020-03-27T18:16:37.6513528Z             # different time zone, so here we provide a context which will
2020-03-27T18:16:37.6513992Z             # enforce that GMT == UTC.
2020-03-27T18:16:37.6514846Z             return dateutil.parser.parse(value, tzinfos={'GMT': tzutc()})
2020-03-27T18:16:37.6515508Z         except (TypeError, ValueError) as e:
2020-03-27T18:16:37.6516237Z >           raise ValueError('Invalid timestamp ""%s"": %s' % (value, e))
2020-03-27T18:16:37.6517538Z E           ValueError: Invalid timestamp ""äº”, 27 3æœˆ 2020 18:11:53 GMT"": Unknown string format: äº”, 27 3æœˆ 2020 18:11:53 GMT
```"
600371803,33566,CI: Fix jedi deprecation warning for 0.17.0 on IPython,charlesdong1991,closed,2020-04-15T15:09:09Z,2020-05-26T09:40:37Z,"- [ ] closes #33567 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
600383935,33567,CI : jedi deprecation warnings fails CI,charlesdong1991,closed,2020-04-15T15:25:13Z,2020-05-26T09:40:47Z,"```
ip = <IPython.core.interactiveshell.InteractiveShell object at 0x7f10269bd910>

    @async_mark()
    async def test_tab_complete_warning(self, ip):
        # GH 16409
        pytest.importorskip(""IPython"", minversion=""6.0.0"")
        from IPython.core.completer import provisionalcompleter
    
        code = ""import pandas as pd; df = pd.DataFrame()""
        await ip.run_code(code)
        with tm.assert_produces_warning(None):
            with provisionalcompleter(""ignore""):
>               list(ip.Completer.completions(""df."", 1))

pandas/tests/frame/test_api.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7f10264bc190>
type = None, value = None, traceback = None

    def __exit__(self, type, value, traceback):
        if type is None:
            try:
>               next(self.gen)
E               AssertionError: Caused unexpected warning(s): [('DeprecationWarning', DeprecationWarning('Providing the line is now done in the functions themselves like `Script(...).complete(line, column)`'), '/home/vsts/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/jedi/api/__init__.py', 843), ('DeprecationWarning', DeprecationWarning('Providing the column is now done in the functions themselves like `Script(...).complete(line, column)`'), '/home/vsts/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/jedi/api/__init__.py', 843), ('DeprecationWarning', DeprecationWarning('Deprecated since version 0.16.0. Use Script(...).complete instead.'), '/home/vsts/miniconda3/envs/pandas-dev/lib/python3.7/site-packages/IPython/core/completer.py', 1401)]
```"
574828726,32415,RLS: 1.0.2,jbrockmendel,closed,2020-03-03T17:34:08Z,2020-05-26T12:39:17Z,@pandas-dev/pandas-core think end of this week?
618947291,34186,BUG: Accessing year 2263 datetime in a dataframe using iloc raises OutOfBoundsDatetime,RauliRuohonen,closed,2020-05-15T12:55:19Z,2020-05-26T13:44:35Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import datetime
import pandas as pd

df = pd.DataFrame({'x': [datetime.datetime(2_263, 1, 1)]})
print(df)
print('loc:', repr(df.loc[0, 'x']))
print('iloc:', end=' ')
try:
    print(repr(df.iloc[0, 0]))
except Exception as exc:
    print(repr(exc))
print('iloc after extracting column:', repr(df['x'].iloc[0]))
```

#### Problem description

Output:

```
                     x
0  2263-01-01 00:00:00
loc: datetime.datetime(2263, 1, 1, 0, 0)
iloc: OutOfBoundsDatetime('Out of bounds nanosecond timestamp: 2263-01-01 00:00:00')
iloc after extracting column: datetime.datetime(2263, 1, 1, 0, 0)
```

When a timestamp that fits into a `datetime.datetime` but does not fit into a `pd.Timestamp` is accessed through `df.iloc[0, 0]`, it results in OutOfBoundsDatetime being raised. In contrast, accesses through `df.loc[0, 'x']` or `df['x'].iloc[0]` successfully return the datetime.

It'd be more predictable and consistent to have the same behavior for all three access methods. As it is, unless you've tried all these access methods with suitable datetimes, you'll never guess which of them will work and which won't.

#### Expected Output

```
                     x
0  2263-01-01 00:00:00
loc: datetime.datetime(2263, 1, 1, 0, 0)
iloc: datetime.datetime(2263, 1, 1, 0, 0)
iloc after extracting column: datetime.datetime(2263, 1, 1, 0, 0)
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : 0.29.14
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.10.2
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.17.0
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : None
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.46.0

</details>
"
624936029,34387,"REF: is_subperiod, is_superperiod out of libfreqs",jbrockmendel,closed,2020-05-26T14:39:51Z,2020-05-26T22:10:25Z,"When we first moved these functions from tseries.frequencies to libfrequencies a couple years ago there was a question of it was really necessary, and the answer is now an unambiguous ""no"""
624575153,34378,"REF: Week, Semi-Month, FY5253 offsets to liboffsets",jbrockmendel,closed,2020-05-26T03:22:19Z,2020-05-26T22:20:00Z,Bigger chunk than usual.
625080082,34391,"REF: BusinessDay, BusinessHour to liboffsets",jbrockmendel,closed,2020-05-26T18:04:15Z,2020-05-26T22:20:16Z,
553526305,31204,convert numeric column to dedicated `pd.StringDtype()`,vadella,closed,2020-01-22T13:01:30Z,2020-05-26T22:20:20Z,"#### Code Sample, a copy-pastable example if possible

```python
pd.Series(range(5, 10), dtype=""Int64"").astype(""string"")
```
raises `TypeError: data type not understood`

while 

```python
pd.Series(range(5, 10)).astype(""string"")
```
raises `ValueError: StringArray requires a sequence of strings or missing values.`


If you first do `astype(str)`: 
```python
pd.Series(range(5, 10)).astype(str).astype(""string"")
```
and 
```python
pd.Series(range(5, 10), dtype=""Int64"").astype(str).astype(""string"")
```
work as expected:


```
0    5
1    6
2    7
3    8
4    9
dtype: string
```

While `astype(object)` raises in both cases `ValueError: StringArray requires a sequence of strings or missing values.`

#### Problem description

I can understand the `ValueError`, since you don't feed strings to the `StringArray`. Best for me would be if the `astype(""string"")` converts it to strings, or if the `astype(str)` would return a `StringArray`, but in any case, I would expect both `pd.Series(range(5, 10), dtype=""Int64"").astype(""string"")` and `pd.Series(range(5, 10)).astype(""string"")` to raise the same error.

#### Expected Output

```
0    5
1    6
2    7
3    8
4    9
dtype: string
```
 
or 

 `ValueError: StringArray requires a sequence of strings or missing values.`

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
624918223,34386,CLN: de-duplicate dispatch_to_series calls,jbrockmendel,closed,2020-05-26T14:16:03Z,2020-05-26T22:20:34Z,Unwrap td64/dt64 so those operate blockwise.
623590210,34331,"BUG: writing dataframe to excel if one of the rows has name ""render"" fails",dwintergruen,closed,2020-05-23T06:02:33Z,2020-05-26T22:21:44Z,"I try to write an dataframe to excel with excel. It fails if a row is called ""render"". 

Pandas version 1.0.3

Reproduce with:
s1 = pandas.Series((""1"",""2""),index=[""a"",""b""])
s2 = pandas.Series((""3"",""4""),index=[""b"",""b""])

df = pandas.DataFrame({""render"":s1,""huhu"":s2})
df.to_excel(""/tmp/out.xlsx"")

gives:

AttributeError                            Traceback (most recent call last)
<ipython-input-44-02936207a82a> in <module>
----> 1 df.to_excel(""/tmp/out.xlsx"")

/usr/local/anaconda3/envs/exoplanets/lib/python3.7/site-packages/pandas/core/generic.py in to_excel(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes)
   2171             index_label=index_label,
   2172             merge_cells=merge_cells,
-> 2173             inf_rep=inf_rep,
   2174         )
   2175         formatter.write(

/usr/local/anaconda3/envs/exoplanets/lib/python3.7/site-packages/pandas/io/formats/excel.py in __init__(self, df, na_rep, float_format, cols, header, index, index_label, merge_cells, inf_rep, style_converter)
    384         if hasattr(df, ""render""):
    385             self.styler = df
--> 386             df = df.data
    387             if style_converter is None:
    388                 style_converter = CSSToExcelConverter()

/usr/local/anaconda3/envs/exoplanets/lib/python3.7/site-packages/pandas/core/generic.py in __getattr__(self, name)
   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):
   5273                 return self[name]
-> 5274             return object.__getattribute__(self, name)
   5275 
   5276     def __setattr__(self, name: str, value) -> None:

AttributeError: 'DataFrame' object has no attribute 'data'"
623541847,34329,Fix MultiIndex melt when col_level is used,ashtou,closed,2020-05-23T01:20:51Z,2020-05-26T23:17:26Z,"- [x] closes #34129
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
624483331,34371,DOC: GH4607 added clarification to business DateOffset,dhuettenmoser,closed,2020-05-25T21:01:34Z,2020-05-27T01:00:57Z,"- [x] closes #4607

Added clarifying details to Business Year, Quarter, and Month, End and Begin docs
"
625233276,34397,CLN: GH29547 Replace old string formatting with f-strings,matteosantama,closed,2020-05-26T22:13:31Z,2020-05-27T02:11:48Z,Is it normal for all my old commits on other branches to appear here? Am I doing something wrong?
625338024,34401,CI: Remove unused import in offsets.py,dsaxton,closed,2020-05-27T03:00:13Z,2020-05-27T03:29:18Z,
624122619,34362,DOC: Fix EX02 and SA01 in DataFrame.select_dtypes,farhanreynaldo,closed,2020-05-25T08:04:59Z,2020-05-27T04:37:08Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. 

output of `python scripts/validate_docstrings.py pandas.DataFrame.select_dtypes`:
```
################################################################################
################################## Validation ##################################
################################################################################

1 Errors found:
        No extended summary found
```

Notes:
It seems like passing `int` to the parameter do not include/exclude all integer dtypes. But, passing `integer` would do otherwise. I think it is related to this issue https://github.com/pandas-dev/pandas/issues/29394. It would be more convenient if passing `int` as an input would match the result with passing `integer`.

"
619488747,34209,Eliminated the skipna / dropna inconsistency in the docs.,ashwinpn,closed,2020-05-16T14:34:48Z,2020-05-27T04:42:27Z,"

As @bf mentioned in the comments under the  issue #34063 ,
There are errors in the docs for Dataframe.mode, https://pandas.pydata.org/pandasdocs/dev/reference/api/pandas.DataFrame.mode.html (See source : https://github.com/pandas-dev/pandas/blob/master/pandas/core/frame.py#L8722-L8807) , where the dropna parameter (which occurs more number of times than skipna) has been named skipna (first occurrence on line 8370).  "
585810787,32914,BUG: Fix min_count issue for groupby.sum,dsaxton,closed,2020-03-22T20:58:06Z,2020-05-11T13:09:02Z,"- [x] closes #32861
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
585023124,32861,Calling `sum` with `min_count` on `SeriesGroupBy` with dtype `Int64` gives large negative value rather than pd.NA,mojones,closed,2020-03-20T12:00:27Z,2020-05-11T13:09:37Z,"```python

test_df = pd.DataFrame({'foo' : ['a'], 'bar': [1]})
test_df['bar'] = test_df['bar'].astype('Int64')
test_df.groupby('foo')['bar'].sum(min_count=2)

# output:
foo
a    -9223372036854775808
Name: bar, dtype: Int64

```
#### Problem description

Per the documentation, `sum` should return `NA` if there are fewer than `min_count` values. This works fine on the dataframe itself:

```python
test_df['bar'].sum(min_count=2)

# output
nan
```

but gives what looks like an overflow error when called after `groupby`.

I ran into this with real data when calling with `min_count=1` on a dataframe where some of the values were missing, but I thought the minimal example above was clearer.


#### Expected Output

foo
a    NaN
Name: bar, dtype: Int64

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-74-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : None
pytest           : 5.3.4
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.3.3
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.3
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.4
pyxlsb           : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
554925588,31294,"PERF: optimize is_scalar, is_iterator",jbrockmendel,closed,2020-01-24T19:58:26Z,2020-05-11T13:56:13Z,"While working on #30349 I noticed that `is_scalar` is pretty slow for non-scalar args, turns out we can get a 9-19x improvement for listlike cases, 5-10x improvement for Decimal/Period/DateOffset/Interval, with small improvements in nearly every other case while we're at it (the fractions.Fraction object is the only one where i found a small decrease in perf, within the margin of error)

xref #31291, assuming this is the desired behavior for is_iterator, this closes that.
 
Setup:
```
import pandas as pd                                                     
from pandas.core.dtypes.common import *                                 
import decimal, fractions

dec = decimal.Decimal(19.45678)
frac = fractions.Fraction(1)
i64 = np.int64(-1)
arr = np.arange(5)
ser = pd.Series(arr)
idx = pd.Index(arr)
interval = pd.Interval(0, 1)
per = pd.Period(""2017"")
offset = per - per
iterator = (x for x in [])
```

Non-Scalar Cases
```
In [6]: %timeit is_scalar([])
1.53 µs ± 36.4 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- master
79.7 ns ± 5.26 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR

In [7]: %timeit is_scalar(arr)
1.58 µs ± 43.8 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- master
77.6 ns ± 5.02 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR

In [8]: %timeit is_scalar(ser)
1.01 µs ± 51.7 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- master
92.3 ns ± 0.505 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR

In [9]: %timeit is_scalar(idx)
876 ns ± 4.86 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- master
92.7 ns ± 1.47 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR

In [10]: %timeit is_scalar(iterator)
2.08 µs ± 22.9 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)  # <-- master
869 ns ± 20.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- PR
```

Non-Built-In Scalar Cases
```
In [10]: %timeit is_scalar(dec)
1.05 µs ± 9.75 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- master
79.5 ns ± 2.16 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR

In [12]: %timeit is_scalar(interval)
748 ns ± 22.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- master
139 ns ± 5.38 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR

In [13]: %timeit is_scalar(per)
706 ns ± 24.7 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- master
127 ns ± 14.2 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR

In [14]: %timeit is_scalar(offset)
930 ns ± 40.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- master
211 ns ± 2.45 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- PR
```

Built-In Scalar Cases
```
In [3]: %timeit is_scalar(4)
60.1 ns ± 2.39 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- master
54.9 ns ± 1.15 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR

In [4]: %timeit is_scalar(4.0)
53 ns ± 4.94 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- master
48.2 ns ± 0.613 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR

In [5]: %timeit is_scalar(i64)
58.5 ns ± 1.39 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- master
58.1 ns ± 5.88 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR

In [11]: %timeit is_scalar(frac)
93.5 ns ± 2.05 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- master
94.1 ns ± 6.4 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR
```

is_iterator check
```
In [16] %timeit is_iterator(iterator)
283 ns ± 5.35 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- master
58.9 ns ± 2.31 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR

In [17] %timeit is_iterator(arr)
228 ns ± 3.98 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- master
55 ns ± 3.53 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR

In [17] %timeit is_iterator(""foo"")
231 ns ± 15.7 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)  # <-- master
49.4 ns ± 5.08 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR
```"
615490778,34111,PERF: use is_foo_dtype fastpaths,jbrockmendel,closed,2020-05-10T22:31:03Z,2020-05-11T18:02:44Z,
616029973,34118,PERF: use fast-paths for dtype checks,jbrockmendel,closed,2020-05-11T16:51:55Z,2020-05-11T18:53:16Z,
614216777,34051,BUG: DataFrameGroupBy.sum ignores min_count for boolean data type,dsaxton,closed,2020-05-07T17:16:15Z,2020-05-11T18:53:16Z,"- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of pandas.
- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

Behavior is from master:

```python
import pandas as pd

df = pd.DataFrame({""a"": [1, 2], ""b"": pd.array([True, True])})
df.groupby(""a"").sum(min_count=2)
```
gives
```python
      b
a      
1  True
2  True
```
but expected output is
```python
      b
a      
1  <NA>
2  <NA>
```

It looks to me like there's an attempt to compute a Cythonized result which fails, after which point the min_count argument is forgotten."
615465530,34107,CLN: avoid ndim checks in _libs,jbrockmendel,closed,2020-05-10T20:02:32Z,2020-05-11T18:54:03Z,These checks go through python-space
615471296,34108,CLN: make sure we do no-copy ravel(),jbrockmendel,closed,2020-05-10T20:36:40Z,2020-05-11T18:55:40Z,
614866774,34074,"BUG: Multijoining indexes always returns indexers, ignores function parameters",CuylenE,closed,2020-05-08T17:06:16Z,2020-05-11T18:57:16Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample

```python
>>>frame = pd.DataFrame([(1,2,3,4),(4,5,6,7)],columns=[""i1"", ""i2"", ""i3"",""val""]).set_index([""i1"",""i2"",""i3""])
>>>frame2 = pd.DataFrame([(1,2,3),(4,5,6)],columns=[""i1"", ""i2"", ""val""]).set_index([""i1"",""i2""])
>>>frame.index.join(frame2.index,return_indexers=False)
(MultiIndex([(1, 2, 3),
            (4, 5, 6)],
           names=['i1', 'i2', 'i3']), None, None)
```

#### Problem description
Parameter return_indexers is ignored in some cases of __join_multi

#### Expected Output
```python
>>>frame.index.join(frame2.index,return_indexers=False)
MultiIndex([(1, 2, 3),
            (4, 5, 6)],
           names=['i1', 'i2', 'i3'])
```
"
614876102,34075,BUG: Fixed __join_multi always returning indexers (#34074),CuylenE,closed,2020-05-08T17:25:19Z,2020-05-11T18:57:21Z,"- [x] closes #34074
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
614337539,34056,BUG: Make nullable booleans numeric,dsaxton,closed,2020-05-07T20:54:09Z,2020-05-11T18:59:57Z,"- [x] closes #34051
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Also a decent speedup since this is now going through Cython space:
```python
df = pd.DataFrame({""a"": [1] * 50_000 + [2] * 50_000, ""b"": pd.array([True] * 100_000)})

# master
%timeit df.groupby(""a"").sum()                                                                                                                                                                  
# 5.15 ms ± 18 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

# branch
%timeit df.groupby(""a"").sum()                                                                                                                                                                  
# 1.94 ms ± 10.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```"
272538899,18189,BUG: (linear) interpolation after resampling,Make42,closed,2017-11-09T11:58:54Z,2020-05-11T19:08:05Z,"When running

```python
    import pandas as pd
    index = pd.date_range('1/1/2000', periods=9, freq='0.9S')
    series = pd.Series(range(9), index=index)
    
    >>> series
    2000-01-01 00:00:00.000    0
    2000-01-01 00:00:00.900    1
    2000-01-01 00:00:01.800    2
    2000-01-01 00:00:02.700    3
    2000-01-01 00:00:03.600    4
    2000-01-01 00:00:04.500    5
    2000-01-01 00:00:05.400    6
    2000-01-01 00:00:06.300    7
    2000-01-01 00:00:07.200    8
    Freq: 900L, dtype: int64
```

I get

```python
    >>> series.resample(rule='0.5S').head(100)
    2000-01-01 00:00:00.000    0.0
    2000-01-01 00:00:00.500    1.0
    2000-01-01 00:00:01.000    NaN
    2000-01-01 00:00:01.500    2.0
    2000-01-01 00:00:02.000    NaN
    2000-01-01 00:00:02.500    3.0
    2000-01-01 00:00:03.000    NaN
    2000-01-01 00:00:03.500    4.0
    2000-01-01 00:00:04.000    NaN
    2000-01-01 00:00:04.500    5.0
    2000-01-01 00:00:05.000    6.0
    2000-01-01 00:00:05.500    NaN
    2000-01-01 00:00:06.000    7.0
    2000-01-01 00:00:06.500    NaN
    2000-01-01 00:00:07.000    8.0
    Freq: 500L, dtype: float64
```

However I do not expect to get

```python
    >>> series.resample(rule='0.5S').interpolate(method='linear')
    2000-01-01 00:00:00.000    0.000000
    2000-01-01 00:00:00.500    0.555556
    2000-01-01 00:00:01.000    1.111111
    2000-01-01 00:00:01.500    1.666667
    2000-01-01 00:00:02.000    2.222222
    2000-01-01 00:00:02.500    2.777778
    2000-01-01 00:00:03.000    3.333333
    2000-01-01 00:00:03.500    3.888889
    2000-01-01 00:00:04.000    4.444444
    2000-01-01 00:00:04.500    5.000000
    2000-01-01 00:00:05.000    5.000000
    2000-01-01 00:00:05.500    5.000000
    2000-01-01 00:00:06.000    5.000000
    2000-01-01 00:00:06.500    5.000000
    2000-01-01 00:00:07.000    5.000000
    Freq: 500L, dtype: float64
```

instead I expect the last value to be still 8.0 and still 7.0 for the timestamp with 6.5 seconds.

I posted this on https://stackoverflow.com/q/46728152/4533188 and I was told this might be a bug - I got the same impression.


<details>


```python
>>> pd.show_versions()
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.2.final.0
python-bits: 64
OS: Linux
OS-release: 3.13.0-129-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: de_DE.UTF-8
LOCALE: de_DE.UTF-8
pandas: 0.20.3
pytest: 3.2.1
pip: 9.0.1
setuptools: 36.2.2.post20170724
Cython: 0.26
numpy: 1.13.3
scipy: 0.19.1
xarray: None
IPython: None
sphinx: 1.6.3
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: 0.9.8
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
pandas_gbq: None
pandas_datareader: None

```

</details>
"
281971997,18774,Resample for discrete values,mullenkamp,open,2017-12-14T03:40:12Z,2020-05-11T19:10:26Z,"#### Example from https://stackoverflow.com/questions/39809703/python-pandas-resampling-instantaneous-hourly-data-to-daily-timestep-including-0

```python
index = pd.date_range('1/1/2000', periods=9, freq='6H')
series = pd.Series([2, 3, 6, 9, 5, 1, 3, 9, 4], index=index)

series
2000-01-01 00:00:00    2
2000-01-01 06:00:00    3
2000-01-01 12:00:00    6
2000-01-01 18:00:00    9
2000-01-02 00:00:00    5
2000-01-02 06:00:00    1
2000-01-02 12:00:00    3
2000-01-02 18:00:00    9
2000-01-03 00:00:00    4
Freq: 6H, dtype: int64
```
#### Problem description
This has been asked about in a stackoverflow question, but the answer is inefficient and only works for regular time series.
If we have a time series where each value is a discrete measurement, resampling/aggregating would require some kind of interpolation assumption across the resampling period. In it's simplest form, a linear interpolation would just require the time series to be shifted back one step (using the shift(-1)) and take the pandas resampled mean of the original and shifted time series.

#### Expected Output
```python
index = pd.date_range('1/1/2000', periods=9, freq='6H')
series = pd.Series([2, 3, 6, 9, 5, 1, 3, 9, 4], index=index)
series_new = (series + series.shift(-1))/2
res_good = series_new.resample('D').mean()

res_good
2000-01-01    5.375
2000-01-02    4.375
2000-01-03      NaN
```
Compared to the below which only utilizes the values within the resampling period as if the values represented some kind of aggregate measurement since the last time stamp...
```python
res_bad1 = series.resample('D').mean()
2000-01-01    5.0
2000-01-02    4.5
2000-01-03    4.0
```
If there was a closed=both option...
```python
2000-01-01    5.0
2000-01-02    4.0
```

Maybe I'm missing something in the parameters for resample or interpolate with resample, but I would imagine that this is not such an uncommon resampling situation.

Thanks!

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.13.final.0
python-bits: 32
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en
LOCALE: None.None

pandas: 0.21.0
pytest: 3.0.5
pip: 9.0.1
setuptools: 27.2.0
Cython: 0.25.2
numpy: 1.11.3
scipy: 1.0.0
pyarrow: None
xarray: 0.10.0
IPython: 5.3.0
sphinx: 1.5.1
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: 1.2.0
tables: 3.3.0
numexpr: 2.6.4
feather: None
matplotlib: 2.0.0
openpyxl: 2.4.1
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: 0.9.6
lxml: 3.7.2
bs4: 4.5.3
html5lib: None
sqlalchemy: 1.1.5
pymysql: None
psycopg2: 2.6.2 (dt dec pq3 ext)
jinja2: 2.9.4
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
616069400,34121,Fix typo in core/accessor.py,MaxHalford,closed,2020-05-11T17:54:48Z,2020-05-11T19:18:22Z,"This is just a typo fix.
"
615464139,34106,REF: make ccalendar objects cimportable,jbrockmendel,closed,2020-05-10T19:54:37Z,2020-05-11T19:39:44Z,xref #34105
121583397,11814,BUG?: `_is_view == False` for DataFrame from DataFrame,nickeubank,closed,2015-12-10T21:40:35Z,2020-05-11T19:44:44Z,"Constructing off a dataframe creates a _de facto_ view, but `._is_view` reports false. Seems like a bug. 

```
In [1]:
    df = pd.DataFrame({'col1':[1,2], 'col2':[3,4]})
    df2 = pd.DataFrame(df)
    df2._is_view     
Out[1]:
    False

In [2]:
    df.loc[0,'col1'] = -88
    df2
Out[2]:
    col1    col2
    0   -88 3
    1   2   4
```

Suggestions on how best to fix?
"
405435343,25063,API: change Index set ops sort=True -> sort=None,jorisvandenbossche,closed,2019-01-31T20:51:39Z,2020-05-11T21:06:27Z,"See discussion in https://github.com/pandas-dev/pandas/pull/25007, and particularly https://github.com/pandas-dev/pandas/pull/25007#issuecomment-459384037

This changes the parameter *value* of True to None (without any change in behaviour), so we keep the possibility open to have `sort=True` to mean ""always sorting"" in the future.

I started from the branch of Tom in https://github.com/pandas-dev/pandas/pull/25007 (as there were already useful doc changes + new tests), and changed the value + updated the tests from there.

cc @TomAugspurger @jreback 

closes https://github.com/pandas-dev/pandas/issues/24959"
615455191,34105,REF: make nat_strings cimport-able,jbrockmendel,closed,2020-05-10T19:05:40Z,2020-05-11T21:34:25Z,"Shouldn't make a real impact, mostly makes the imports easier to read"
616052042,34119,TST/REF: Deprivate check_pairwise_moment and remove ConsistencyBase,charlesdong1991,closed,2020-05-11T17:27:59Z,2020-05-11T21:43:43Z,"the `check_pairwise_moment` function will be the last function moving out of big `ConsistencyBase` class, and after this, `ConsistencyBase` class only has the `_create_data` function which creates some mock data, so we could entirely remove `ConsistencyBase` and use `Base` instead. 

cc @jreback "
606828170,33793,DOC: Fix typos and improve parts of docs,eu42,closed,2020-04-25T18:28:13Z,2020-05-12T07:59:38Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
614354179,34057,DOC: Missing examples of from_records.,bdice,closed,2020-05-07T21:25:53Z,2020-05-12T11:08:25Z,"#### Location of the documentation

`pandas.DataFrame.from_records`: https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.from_records.html

#### Documentation problem

Examples are provided for constructors like `DataFrame.from_dict` but no examples are present for `DataFrame.from_records`, which makes it slightly harder to tell whether it's appropriate for a desired use case.

#### Suggested fix for documentation

Add examples."
558671996,31566,TST: DataFrame.interpolate(axis='columns') throws exception while DataFrame.interpolate(axis=1) not (#25190),YagoGG,closed,2020-02-02T09:53:06Z,2020-05-12T12:20:46Z,"Make sure that DataFrame.interpolate allows setting having ""columns"" or ""index"" as the `axis` argument.

I included the `whatsnew` entry as well. However, since the bug was already fixed in v1.0.0, it might be reasonable to just drop it. I leave it to the reviewer's discretion, that's why I've put it in a separate commit.

- [x] closes #25190
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
616213259,34126,DOC: Add clarifications and link to tutorials in the getting started page,datapythonista,closed,2020-05-11T21:53:56Z,2020-05-12T13:25:39Z,"Some people are having problems following our getting started instructions. The notes added here should help.

Also, adding missing link to the tutorials."
407308861,25190,DataFrame.interpolate(axis='columns') throws exception while DataFrame.interpolate(axis=1) not,heavelock,closed,2019-02-06T16:06:32Z,2020-05-12T13:30:48Z,"#### Code Sample, a copy-pastable example if possible

```python
import numpy as np
import pandas as pd

x = np.linspace(0,100,1000)
y = np.sin(x)
df = pd.DataFrame(data=np.tile(y, (10,1)), index=np.arange(10), columns=x)
df.reindex(columns=x*1.005).interpolate(method='linear', axis=1)

df.reindex(columns=x*1.005).interpolate(method='linear', axis='columns')
```
#### Problem description

The interpolation with argument `axis` set to 'columns' throws an exception from 
https://github.com/pandas-dev/pandas/blob/c460a929e19535335085c652e7e35eee1d9c7c2e/pandas/core/generic.py#L6800

The exception is:
```
UnboundLocalError: local variable 'ax' referenced before assignment
```

The same method for parameter `axis=1' works fine.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.0.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-44-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.1
pytest: None
pip: 18.1
setuptools: 40.6.3
Cython: None
numpy: 1.15.4
scipy: 1.1.0
pyarrow: None
xarray: 0.11.2
IPython: 7.2.0
sphinx: None
patsy: None
dateutil: 2.7.5
pytz: 2018.7
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: 4.2.5
bs4: None
html5lib: None
sqlalchemy: 1.2.15
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
496446021,28552,"DataFrame[list[Series[datetime64ns, tz]]] drops timezone information",Koojav,closed,2019-09-20T16:36:45Z,2020-05-12T13:32:09Z,"#### Code Sample, a copy-pastable example if possible

```
s = pd.Series([pd.to_datetime('2018-10-08 13:36:45+00:00')])


Output:
0   2018-10-08 13:36:45+00:00
dtype: datetime64[ns, UTC]
```

```
pd.DataFrame([s]).min()


Output:
0   2018-10-08 13:36:45
dtype: datetime64[ns]
```

#### Problem description

When using `DataFrame().min()` method timezone information gets removed but it should remain untouched just like when using `Series().combine(...,min)`. Example:

```
s.combine(s, min)


Output:
0   2018-10-08 13:36:45+00:00
dtype: datetime64[ns, UTC]
```

#### Expected Output
```
0   2018-10-08 13:36:45+00:00
dtype: datetime64[ns, UTC]
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-64-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.0
numpy            : 1.17.0
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.1
setuptools       : 40.6.3
Cython           : 0.27.3
pytest           : 3.0.7
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.2.6
html5lib         : None
pymysql          : None
psycopg2         : 2.7.3.2 (dt dec pq3 ext lo64)
jinja2           : 2.10
IPython          : 7.2.0
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.6
matplotlib       : 2.0.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : 0.11.0
pyarrow          : 0.13.0
pytables         : None
s3fs             : None
scipy            : 1.0.0
sqlalchemy       : 1.3.7
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
616636214,34134,TST:Make test faster,Nishikoh,closed,2020-05-12T12:59:12Z,2020-05-12T13:58:52Z,"- [x] Part of #34131 
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
615080333,34088,ENH: make Tick comparisons match Timedelta behavior,jbrockmendel,closed,2020-05-09T03:25:25Z,2020-05-12T14:24:55Z,
616589136,34130,CI: test_object_factorize_dropna failing in MacPython/pandas-wheels,TomAugspurger,closed,2020-05-12T11:41:55Z,2020-05-12T14:52:21Z,"https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=35317&view=logs&j=79f3a53a-4a6d-509c-98b5-ff6e9111f67c&t=25a63239-9c30-5cf0-cb4c-3d01da7b47c5

```python
    def test_object_factorize_dropna(
        self, data, dropna, expected_codes, expected_uniques
    ):
        codes, uniques = algos.factorize(data, dropna=dropna)
    
        tm.assert_numpy_array_equal(uniques, expected_uniques)
>       tm.assert_numpy_array_equal(codes, expected_codes)
E       AssertionError: numpy array are different
E       
E       Attribute ""dtype"" are different
E       [left]:  int32
E       [right]: int64
```

I'm not sure what the correct behavior is here, but the implementation is returning int32 while we're asserting int64. The test should probably check for `np.dtype(""intp"")`."
452559309,26667,pandas cannot actually read Stata file format 104,miker985,closed,2019-06-05T15:13:17Z,2020-05-12T14:54:10Z,"#### Code Sample, a copy-pastable example if possible

Data usage agreements prevent me from attaching a Stata file of format version 104, so I cannot easily provide a copy/paste example. This is unfortunate as I have several of these files...

```
. dtaversion CIV_LSMS_1987_ANTHROPOMETRICS_A.DTA
  (file ""CIV_LSMS_1987_ANTHROPOMETRICS_A.DTA"" is .dta-format 104 from Stata 4)
```

#### Problem description

The `stata.py` module will provide error messages that it can only read certain versions and explicitly enumerates version 104 both in the [`_version_error` string](https://github.com/pandas-dev/pandas/blob/c07d71d13b21e0b6e22146f0f546f1f8e24a64b3/pandas/io/stata.py#L40-L42) and in [`StataReader._read_old_header`](https://github.com/pandas-dev/pandas/blob/c07d71d13b21e0b6e22146f0f546f1f8e24a64b3/pandas/io/stata.py#L1224)

However, `StataReader` cannot read files with version 104. This is because

1. `StataReader.__init__` calls `_read_header` [code](https://github.com/pandas-dev/pandas/blob/c07d71d13b21e0b6e22146f0f546f1f8e24a64b3/pandas/io/stata.py#L993)
1. `_read_header` calls `_read_old_header` [code](https://github.com/pandas-dev/pandas/blob/c07d71d13b21e0b6e22146f0f546f1f8e24a64b3/pandas/io/stata.py#L1025)
1. `_read_old_header` calls `_get_time_stamp` [code](https://github.com/pandas-dev/pandas/blob/c07d71d13b21e0b6e22146f0f546f1f8e24a64b3/pandas/io/stata.py#L1238)
1. `_get_time_stamp` raises a `ValueError` for format versions > 104 [code](https://github.com/pandas-dev/pandas/blob/c07d71d13b21e0b6e22146f0f546f1f8e24a64b3/pandas/io/stata.py#L1197-L1207)

None of this behavior can be overridden or otherwise configured as part of instantiating `pandas.io.stata.StataReader`

#### Expected Output

Remove 104 as a supported format and error: `ValueError: Version of given Stata file is not 105, 108, 111 (Stata 7SE), 113 (Stata 8/9), 114 (Stata 10/11), 115 (Stata 12), 117 (Stata 13), or 118 (Stata 14)`

Alternatively `_get_time_stamp` could be changed to return `""""` or a similar value to reflect that Stata did not include a file timestamp in that format version. My experiments subclassing `StataReader` to suppress the error, read the data file in as a df, and export to CSV yield a file equivalent to an equivalent `export delimited` call from Stata.

#### Output of ``pd.show_versions()``

<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.7.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-50-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: None
pip: 19.1.1
setuptools: 41.0.1
Cython: None
numpy: 1.16.4
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
616674262,34135,CI: Fix expected for 32-bit platform,TomAugspurger,closed,2020-05-12T13:51:32Z,2020-05-12T15:27:04Z,Closes https://github.com/pandas-dev/pandas/issues/34130
544950797,30641,CI: Adding build for ARM64,ossdev07,closed,2020-01-03T11:10:46Z,2020-05-12T16:02:40Z,"Added arm64 test support in travis-ci .
Modified environment creation by using archiconda instead of miniconda as miniconda is not supported in arm64 currently .

- [X] closes https://github.com/pandas-dev/pandas/issues/28986
- [X] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
615447587,34103,REF: put PeriodArray-specific method in PeriodArray,jbrockmendel,closed,2020-05-10T18:22:33Z,2020-05-12T16:10:42Z,
615427715,34099,REF: implement general case DateOffset methods on BaseOffset,jbrockmendel,closed,2020-05-10T16:31:45Z,2020-05-12T16:12:23Z,
614909367,34076,REF: share code for scalar validation in datetimelike array methods,jbrockmendel,closed,2020-05-08T18:32:29Z,2020-05-12T16:16:38Z,"The user-facing effect here is in the exception messages we show to users, where we may want to be more verbose than this.  Thoughts @gfyoung?"
215512897,15747,Proposal to change behaviour with .loc and missing keys,toobaz,closed,2017-03-20T18:25:07Z,2020-05-12T16:39:37Z,"```python
In [2]: pd.Series([1, 2, 3]).loc[[2,3]]
Out[2]: 
2    3.0
3    NaN
dtype: float64

In [3]: pd.Series([1, 2, 3]).loc[[3]]
[...]
KeyError: 'None of [[3]] are in the [index]'
```
#### Problem description

Although coherent (except for some unfortunate side-effects - some of them below) with the [docs](http://pandas.pydata.org/pandas-docs/stable/indexing.html#selection-by-label) where they say ""At least 1 of the labels for which you ask, must be in the index or a ``KeyError`` will be raised!"", the current behavior is - I claim - a terrible choice for both developers and users.

There are (at least) three ways to behave with missing labels:
1. you raise an error if requested at least one missing label
2. you raise an error if requested _only_ missing labels
2a ... while  if at least one label is present, missing labels become ``NaN`` __(current)__
2b. ... while if at least one label is present, missing labels are silently dropped
3. you never raise an error for missing labels
3a ... and they become ``NaN``
3b. ... and they are silently dropped

#### For developers

Options 1. and 3. are both much easier to implement, because in both cases you can reduce the question ""am I going to get an error?"" in smaller pieces - e.g. when indexing a ``MultiIndex``, you will get an error if you get an error on any of its levels. Option 2. is instead more complicated (and computationally expensive), because you need to first aggregate in some way across levels/axes, and only then can you decide whether to raise an error or not. Several incoherences came out as a consequence of this choice, some of them still unsolved, such as #15452, [this](https://github.com/pandas-dev/pandas/pull/15615#issuecomment-287826997), the fact that ``pd.Series(range(2)).loc[[]]`` does not raise, and the fact that ``pd.DataFrame.ix[[missing_label]]`` doesn't either.

#### Other consequences of 2.

Additionally, it was decided that the behavior with missing labels would be to introduce ``NaN``s (rather than to drop them), and I think this was also not a good choice (and indeed partial indexing ``MultiIndex``es does not behave this way - it couldn't). I think it is also undocumented.

And finally, since the above wouldn't always tell you what to do when there are missing labels in a ``MultiIndex``, it was decided that ``.loc`` would rather behave as ``.reindex`` when there are missing _and incomplete_ labels, which is totally unexpected and, I think, undocumented.

Notice that these further issues (and more in general, the question ""what to do when some labels are missing and you are not raising an error"") would partially still hold with 3, but could be dealt with, I think, more elegantly.

#### For users

I think the current behavior is annoying to users not just because of those ""__Other consequences__"", but also because it is more complicated to describe in terms of set operation on labels/indices. For instance, with options 1. and 3.

``pd.concat([chunk.loc[something] for chunk in chunks])``

and

``pd.concat(chunks).loc[something]``

both return the same result (or raise). Instead with 2. it actually depends on how missing labels are distributed across chunks.

#### (Why this?)

It is worth understanding why 2. was picked in the first place, and I think the answer is ""to be coherent with intervals"". But I think it's not worth the damage - after all, an iterable and an interval are different objects. And moreover, introducing ``NaN``s for missing labels is anyway incoherent with intervals.

#### Backward incompatibility

Option 1. is, I think, the best, because it is also coherent with numpy's behavior with out-of-bounds indices (e.g. ``np.array([[1,2], [3,4]])[0,[1,3]]`` raises an ``IndexError``).

But while clearly both 1. and 3. could break some existing code, 3. would be better from this point of view, in the sense that it would break only code _assuming_ that an error is raised. Although one might even claim that 1., by breaking code which looks for missing labels, can help discover bugs in user code (not a great argument, I know).

So overall I am not sure about what we should pick between 1. and 3. But I really think we should leave 2., and that the later it is done, the worse. @jreback , @jorisvandenbossche if you want to tell me your thoughts about this, I can elaborate on what we could do with the ""__Other consequences__"" in the desired option.

Then if you approve the change, I'm willing to help in implementing it."
614366854,34058,DOC: Examples for DataFrame.from_records,bdice,closed,2020-05-07T21:49:59Z,2020-05-12T16:40:38Z,"This PR adds examples to the documentation for `pandas.DataFrame.from_records`. It also rewords a couple sentences to clarify that the input data can be a sequence of dicts or sequence of tuples (but not a sequence of DataFrames).

- [x] closes #34057
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
506970306,28986,Add arm64 builds to CI,siddhesh,closed,2019-10-15T02:43:57Z,2020-05-12T16:58:34Z,"#### Problem description

Travis now has support for arm64 CI[1] and it would be great to have a CI job in place to ensure that builds and tests are run on arm64 to ensure that it works OK.  This will also help ensure that in future undefined behaviour such as #28918 do not get silently introduced into pandas.

[1] https://blog.travis-ci.com/2019-10-07-multi-cpu-architecture-support"
610391461,33905,Fix #28552 Add test for DataFrame min/max preserve timezone,ghost,closed,2020-04-30T21:15:40Z,2020-05-12T17:04:20Z,"- [x] closes #28552
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry _(not applicable - no functionality added)_

Here's a test discussed in #28552. `mean` and other aggregates except for `min` and `max` are not covered since they don't support datetimes - it's a nan being returned.
"
616787613,34138,BUG: to_numeric doesn't work properly when errors='ignore',Nowam,closed,2020-05-12T16:19:54Z,2020-05-12T19:47:41Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
import pandas as pd

s = pd.Series(['a', '1.0'])
out = pd.to_numeric(s, errors='ignore')
print(list(out))
```
output:
```python
['a', '1.0']
```


#### Problem description

The output should cast '1.0' to a float and it doesn't.
This only happends when there is an error to ""ignore"", so a series containing 
```python
['1.0', '1.0']
```
Will cast correctly to:
```python
[1.0, 1.0]
```
#### Expected Output
```python
['a', 1.0]
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 58 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.0.3
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.10.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.3.3
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
616816046,34139,"REF: move mixins, repr_attrs to liboffsets",jbrockmendel,closed,2020-05-12T17:04:14Z,2020-05-13T00:45:49Z,
284051448,18904,Different behavior of pandas when filter DataFrame by index and by column,xuboying,closed,2017-12-22T01:44:40Z,2020-05-13T02:24:42Z,"python3.5
See:
https://stackoverflow.com/questions/47920397/different-behavior-of-pandas-when-filter-dataframe-by-index-and-by-column
"
303209142,20042,BUG: Incorrect outer join when merging multi-index with a single-index,kylebarron,open,2018-03-07T18:26:04Z,2020-05-13T02:27:40Z,"#### Code Sample

```python
In [1]: import pandas as pd

In [2]: df1 = pd.DataFrame([['a', 'x', 0.123], ['a','x', 0.234],
   ...:                     ['a', 'y', 0.451], ['b', 'x', 0.453]],
   ...:                    columns=['first', 'second', 'value1']
   ...:                    ).set_index(['first', 'second'])
   ...:                    

In [3]: df2 = pd.DataFrame([['a', 10],['b', 20]],
   ...:                    columns=['first', 'value']).set_index(['first'])
   ...: 

In [4]: df3 = pd.DataFrame([['a', 1], ['b', 2], ['c', 3]],
   ...:                    columns=['first', 'final_val']).set_index(['first'])
   ...: 

In [5]: df1
Out[5]: 
              value1
first second        
a     x        0.123
      x        0.234
      y        0.451
b     x        0.453

In [6]: df2
Out[6]: 
       value
first       
a         10
b         20

In [7]: df3
Out[7]: 
       final_val
first           
a              1
b              2
c              3

In [8]: df1.join(df3, how='outer')
Out[8]: 
              value1  final_val
first second                   
a     x        0.123          1
      x        0.234          1
      y        0.451          1
b     x        0.453          2

In [9]: df2.join(df3, how='outer')
Out[9]: 
       value  final_val
first                  
a       10.0          1
b       20.0          2
c        NaN          3
```
#### Problem description

I expect the result of an outer join to have all values of the level of the index on which the join is being performed, similar to how the single-index to single-index outer join includes `c` in the index of the result.

#### Expected Output

I expect the result from `df1.join(df3, how='outer')` to be:
```py
              value1  final_val
first second                   
a     x        0.123          1
      x        0.234          1
      y        0.451          1
b     x        0.453          2
c     NaN      NaN            3
```
or something similar (i.e. maybe `''` instead of `NaN` for `second`, since it's not numeric).

#### Output of ``pd.show_versions()``

<details>

In [10]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.4.final.0
python-bits: 64
OS: Linux
OS-release: 4.13.0-36-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.22.0
pytest: 3.4.0
pip: 9.0.1
setuptools: 38.5.1
Cython: 0.27.3
numpy: 1.14.1
scipy: 1.0.0
pyarrow: 0.8.0
xarray: None
IPython: 6.2.1
sphinx: 1.7.1
patsy: 0.5.0
dateutil: 2.6.1
pytz: 2018.3
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.4
feather: 0.4.0
matplotlib: 2.1.2
openpyxl: 2.5.0
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.2
lxml: 4.1.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.3
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: 0.1.4
pandas_gbq: None
pandas_datareader: None

</details>
"
370827882,23191,"Bitwise operations have inconsistent behavior, different from numpy",cyrusmaher,open,2018-10-16T22:38:55Z,2020-05-13T02:38:16Z,"#### Code Sample

```python
# succeeds
pd.Series([False]) & pd.Series([6.])

# Example: order matters
# fails: ufunc 'bitwise_and' not supported for the input types
pd.Series([6.]) & pd.Series([False])

# Example: behavior is different from numpy
# fails: ufunc 'bitwise_and' not supported for the input types
np.array([False]) & np.array([6.])


```
#### Problem description
Bitwise operations between floats and bools error out in numpy. They error out in pandas too if the first argument is a `float`, but not if the first argument is a `bool`. 

#### Expected Output
ufunc 'bitwise_and' not supported for the input types

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.3.final.0
python-bits: 64
OS: Darwin
OS-release: 17.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: None
pip: 18.0
setuptools: 36.5.0.post20170921
Cython: 0.28.2
numpy: 1.15.2
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.1.0
sphinx: None
patsy: 0.5.0
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.2.2
openpyxl: None
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.999999999
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: 0.1.5
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
414807324,25457,"Grouped dataframe ""name"" attribute overrides column access / not well documented",alkasm,open,2019-02-26T20:37:09Z,2020-05-13T02:45:15Z,"#### Code Sample

```python
>>> df = pd.DataFrame({'val': [9, 10, 3, 6, 2, 3], 'name': list('xxyxyy'), 'group': list('aaabbb')})
>>> df

	val	name	group
0	9	x	a
1	10	x	a
2	3	y	a
3	6	x	b
4	2	y	b
5	3	y	b
```

Works correctly:
```python
>>> df.groupby('group').apply(lambda g: g[g['name'] == 'x'])

		val	name	group
group				
a	0	9	x	a
	1	10	x	a
b	3	6	x	b
```

Errors out:

```python
>>> df.groupby('group').apply(lambda g: g[g.name == 'x'])

---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
```

Rest of the traceback is here:

<details>

```
~/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2655             try:
-> 2656                 return self._engine.get_loc(key)
   2657             except KeyError:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: False

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
~/venv/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in apply(self, func, *args, **kwargs)
    688             try:
--> 689                 result = self._python_apply_general(f)
    690             except Exception:

~/venv/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in _python_apply_general(self, f)
    706         keys, values, mutated = self.grouper.apply(f, self._selected_obj,
--> 707                                                    self.axis)
    708 

~/venv/lib/python3.7/site-packages/pandas/core/groupby/ops.py in apply(self, f, data, axis)
    189             group_axes = _get_axes(group)
--> 190             res = f(group)
    191             if not _is_indexed_like(res, group_axes):

<ipython-input-282-522c70a9fa21> in <lambda>(g)
----> 1 df.groupby('group').apply(lambda g: g[g.name == 'x'])

~/venv/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key)
   2926                 return self._getitem_multilevel(key)
-> 2927             indexer = self.columns.get_loc(key)
   2928             if is_integer(indexer):

~/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2657             except KeyError:
-> 2658                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2659         indexer = self.get_indexer([key], method=method, tolerance=tolerance)

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: False

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
~/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2655             try:
-> 2656                 return self._engine.get_loc(key)
   2657             except KeyError:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: False

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
<ipython-input-282-522c70a9fa21> in <module>
----> 1 df.groupby('group').apply(lambda g: g[g.name == 'x'])

~/venv/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in apply(self, func, *args, **kwargs)
    699 
    700                 with _group_selection_context(self):
--> 701                     return self._python_apply_general(f)
    702 
    703         return result

~/venv/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in _python_apply_general(self, f)
    705     def _python_apply_general(self, f):
    706         keys, values, mutated = self.grouper.apply(f, self._selected_obj,
--> 707                                                    self.axis)
    708 
    709         return self._wrap_applied_output(

~/venv/lib/python3.7/site-packages/pandas/core/groupby/ops.py in apply(self, f, data, axis)
    188             # group might be modified
    189             group_axes = _get_axes(group)
--> 190             res = f(group)
    191             if not _is_indexed_like(res, group_axes):
    192                 mutated = True

<ipython-input-282-522c70a9fa21> in <lambda>(g)
----> 1 df.groupby('group').apply(lambda g: g[g.name == 'x'])

~/venv/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key)
   2925             if self.columns.nlevels > 1:
   2926                 return self._getitem_multilevel(key)
-> 2927             indexer = self.columns.get_loc(key)
   2928             if is_integer(indexer):
   2929                 indexer = [indexer]

~/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2656                 return self._engine.get_loc(key)
   2657             except KeyError:
-> 2658                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2659         indexer = self.get_indexer([key], method=method, tolerance=tolerance)
   2660         if indexer.ndim > 1 or indexer.size > 1:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: False
```

</details>

#### Problem description

When you call a method on a groupby that expects a function, the documentation states that the function should expect a DataFrame. Indeed if you inspect the type, a DataFrame is passed. However, this DataFrame has a new attribute added --- `name` --- and this overwrites the dot accessor for a column `""name""` if it exists. This is a little troubling since doing dotted access will work outside of the groupby and not inside the groupby function.

First, this needs to be documented more explicitly---I found it once in the documentation for *one* of the related functions that the `.name` attribute gets added, but cannot find it again, so I'm not sure which one it was. Edit: It was the `transform()` method's docstring, as shown in [a comment below](https://github.com/pandas-dev/pandas/issues/25457#issuecomment-467998238).

#### Related

https://github.com/pandas-dev/pandas/issues/9545

#### Expected Output

The expected output is what happens when you use the `[]` indexer, instead of dot access.

#### Suggestions

Not necessarily mutually exclusive:

* Give a better error message so that it's known `.name` is an attribute of the group DataFrame
* Improve documentation on all related methods to know the `.name` attribute exists and will override the column dot access
* Only add the `.name` attribute if a column of that name doesn't exist 
* Append an underscore to the attribute so there's a much lower chance of conflicts, e.g. `groupdf.name_`
* Change the attribute name entirely for similar lower chance of conflicts, e.g. `groupdf.grouped_value`
* Move into a method call instead, e.g. `groupdf.get_group_name()`
* Add a kwarg to `apply()` / `transform()` which toggles whether to send a second argument into the function, that second argument being the group name

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.0.beta.4
python-bits: 64
OS: Darwin
OS-release: 17.6.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: en_US.UTF-8
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.1
pytest: None
pip: 19.0.3
setuptools: 39.0.1
Cython: None
numpy: 1.16.1
scipy: None
pyarrow: None
xarray: None
IPython: 7.3.0
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: 1.2.18
pymysql: None
psycopg2: 2.7.7 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
605320523,33743,pandas boxplot props not working,mdf-github,closed,2020-04-23T07:54:18Z,2020-05-13T06:07:33Z,"I cannot change some of the plotting kwargs in `pd.DataFrame.boxplot`. I replot the same in matplotlib with almost the same code and I get the expected results.

Here's a minimum working example:

```
np.random.seed(1234)
df = pd.DataFrame(np.random.randn(10, 4),
                  columns=['Col1', 'Col2', 'Col3', 'Col4'])
df.boxplot(column=['Col1', 'Col2', 'Col3'], showmeans=True,
                     meanprops={'markerfacecolor': 'r'},
                     boxprops={'linewidth':5,'color': 'r'}
                    )
fig2, ax2 = plt.subplots()
ax2.boxplot([df[c] for c in df.columns],showmeans=True,
                     meanprops={'markerfacecolor': 'r'},
                     boxprops={'linewidth':5,'color': 'r'})
```

The resulting plots are different. While the boxes are rendered in the correct color using `ax2.boxplot`, the `df.boxplot` version still has blue boxes (though thicker, which means `linewidth` works here)."
614401513,34060,BUG: df.to_excel returns an error,mukerong,closed,2020-05-07T23:17:07Z,2020-05-13T06:53:54Z,"Hello,

I run the sample script in pandas [to_excel documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html). However, it throws an error.

#### Code Sample, a copy-pastable example

```python
# Your code here
df1 = pd.DataFrame([['a', 'b'], ['c', 'd']],
                   index=['row 1', 'row 2'],
                   columns=['col 1', 'col 2'])
df1.to_excel(""output.xlsx"")  
```

#### Problem description

Here is the screenshot of the issue:
![image](https://user-images.githubusercontent.com/23180334/81353467-01e0a500-907e-11ea-884f-c2dec2dd5b76.png)

Openpyxl installed while installing pandas through pip.
"
490587647,28330,groupby and resample methods do not preserve subclassed data structures,grge,closed,2019-09-07T03:57:43Z,2020-05-13T14:52:10Z,"#### Code sample

```python
class MyDataFrame(pd.DataFrame):
    @property
    def _constructor(self):
        return MyDataFrame

dates = pd.date_range('2019', freq='H', periods=1000)
my_df = MyDataFrame(np.arange(len(dates)), index=dates)

print(type(my_df)) 
# __main__.MyDataFrame (✓)

print(type(my_df.diff())) 
# __main__.MyDataFrame (✓)

print(type(my_df.sample(1))) 
# __main__.MyDataFrame (✓)

print(type(my_df.rolling('5H').mean()))
# __main__.MyDataFrame (✓)

print(type(my_df.groupby(my_df.index.dayofweek).mean()))
# pandas.core.frame.DataFrame (✘)

print(type(my_df.resample('D').mean()))
# pandas.core.frame.DataFrame (✘)

```
#### Problem description

Originally posted on [SO](https://stackoverflow.com/questions/57796464/pandas-groupby-resample-etc-for-subclassed-dataframe?noredirect=1#comment102088835_57796464).

The intended behaviour for chain-able methods on subclassed data structures is clearly that the operation returns an instance of the subclass (i.e. `MyDataFrame`), rather than the native type (i.e. `DataFrame`). This is the current behaviour for most operations (e.g., slicing, sampling, sorting) but not resample and groupby.

Currently groupby and resample both return explicitly constructed pandas datatypes, e.g. here:

https://github.com/pandas-dev/pandas/blob/ac693331400bce4747c63aa76c53c6e3488933ae/pandas/core/groupby/generic.py#L338

To get the expected behaviour, the intermediary classes (e.g. DataFrameGroupBy) would need to retain information about the calling class so that the appropriate constructor can be used (i.e. one of `_constructor` or `_constructor_sliced`  or `_constructor_expanddim`).

Note that operations that use Window and Rolling already appear have the expected behaviour because these assemble their results via a call to `concat` such as this one:

https://github.com/pandas-dev/pandas/blob/171c71611886aab8549a8620c5b0071a129ad685/pandas/core/window.py#L325

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.1
numpy            : 1.17.1
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : 3.5.2
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
617075702,34146,REF: move generate_range from offsets to the one place it is used,jbrockmendel,closed,2020-05-13T01:40:15Z,2020-05-13T15:39:31Z,
616986758,34143,CLN: avoid hasattr check in Timestamp.__richcmp__,jbrockmendel,closed,2020-05-12T21:54:50Z,2020-05-13T15:40:13Z,"The only behavior change should be comparison against zero-dim ndarray, which now lo longer un-packs so ends up returning an np.bool_ object instead of a bool object."
607476482,33820,ENH: Add na_value to DataFrame.to_numpy,TomAugspurger,closed,2020-04-27T11:39:12Z,2020-05-13T15:51:29Z,"#### Is your feature request related to a problem?

Control the NA value used when converting a DataFrame to an ndarray.

#### Describe the solution you'd like

Add an `na_value` argument to `DataFrame.to_numpy()`. Has the same meaning as `na_value` in `Series.to_numpy`, except it applies to all the columns.

#### Additional context

```python
In [8]: df = pd.DataFrame({""A"": pd.array([1, None])})

In [9]: df.to_numpy(na_value=np.nan)
```

This has come up in a few places (cc @jorisvandenbossche @dsaxton)"
576206238,32460,Disallow .__call__() as workaround for non-named functions,alexmojaki,closed,2020-03-05T12:07:04Z,2020-05-13T15:53:48Z,"Currently this script:
```python
import pandas as pd
funcs = [lambda: 1]
pd.eval(""funcs[0]()"")
```

Fails with:

    TypeError: Only named functions are supported

however this can easily be worked around by adding `.__call__`:

```python
pd.eval(""funcs[0].__call__()"")
```

I'm assuming we don't want to allow this workaround. This PR ensures that it will fail with the same error."
608716194,33857,ENH: Add na_value argument to DataFrame.to_numpy,dsaxton,closed,2020-04-29T01:28:37Z,2020-05-13T15:57:03Z,"- [x] closes #33820
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
609438958,33883,"BUG: TimedeltaArray+Period, PeriodArray-TimedeltaArray",jbrockmendel,closed,2020-04-29T23:38:45Z,2020-05-13T15:59:33Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

motivated by the FIXME comment in test_timedelta64.py"
598705421,33516,add a new feature sample() into groupby,echozzy629,closed,2020-04-13T06:34:37Z,2020-05-13T18:33:52Z,"- [ ] closes #33175
- [ ] 0 tests added / 0 passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
516487611,29335,Remove FrozenNDArray,WillAyd,closed,2019-11-02T05:45:46Z,2020-05-13T19:12:59Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
609464044,33884,BUG: Preserve Series/DataFrame subclasses through groupby operations,JBGreisman,closed,2020-04-30T00:30:36Z,2020-05-13T20:28:12Z,"This pull request fixes a bug that caused subclassed Series/DataFrames to revert to `pd.Series` and `pd.DataFrame` after `groupby()` operations. This is a follow-up on a previously abandoned pull request (#28573)

- [x] closes #28330
- [x] tests added / passed
- [x] passes black pandas
- [x] passes git diff upstream/master -u -- ""*.py"" | flake8 --diff
- [x] whatsnew entry"
616853404,34140,CLN: Splits Moments and MomentsConsistency tests into separate files,charlesdong1991,closed,2020-05-12T18:05:06Z,2020-05-13T20:43:49Z,"Due to previous cleaning as well as the large size of those files, we could now split the `Moments` and `MomentsConsistency` test classes into two different files for `rolling` and `ewm`. And since current `test_moments_expanding` is actually MomentsConsistencyExpanding class, so rename it to `test_moments_consistency_expanding` to keep consistency.

And I think this will be the second last PR to close the original test clean issue #30486 . The next PR will be fixturizing the `_create_data` so that we could remove the Base entirely (which currently only creates those data). But since this `base` is also used in other several files like `test_api.py`, `test_ewm.py` etc in window tests, I am afraid the next PR will be quite big.

Please let me your thoughts on the plan (or if you think the change is quite big then no need to proceed since the original issue is to only clean the `moments`)? 

cc @jreback 

"
616987807,34144,REF: Implement NullFrequencyError in pd.errors,jbrockmendel,closed,2020-05-12T21:56:48Z,2020-05-13T22:24:10Z,
30595559,6756,DOC: documented that .apply(func) executes func twice on the first time,anton-d,closed,2014-04-01T12:50:45Z,2020-05-14T09:34:31Z,"Related issues are #2656, #2936 and #6753.
"
581848994,32738,DataFrame.plot docstring missing a few parameters,tdpetrou,closed,2020-03-15T22:22:33Z,2020-05-14T10:10:08Z,"Looking at the [plot accessor docstring][1], it's missing 

* `ax`
* `subplots`
* `sharex`
* `sharey`
* `secondary_y`
* `sort_columns`

Maybe others.

Also would be nice to have documentation of all parameters for all the accessor methods i.e. `df.plot.bar`.


[1]: https://github.com/pandas-dev/pandas/blob/master/pandas/plotting/_core.py#L579"
618102928,34173,"Backport PR #33645, #33632 and #34087 on branch 1.0.x",simonjayhawkins,closed,2020-05-14T10:10:42Z,2020-05-14T11:42:02Z,"xref #33645, #33632 and #34087

fix test_s3_roundtrip failure on branch 1.0.x https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=35271&view=logs&j=98c84d08-34d8-513e-80be-2c581992dd5a&t=f75024c1-aff0-57fb-4f0e-35923b654e09"
615930240,34115,Backport PR #34048 on branch 1.0.x (Bug in DataFrame.replace casts columns to ``object`` dtype if items in ``to_replace`` not in values),simonjayhawkins,closed,2020-05-11T14:29:38Z,2020-05-14T12:26:47Z,"xref #34048, #32988 

fixes regression in 1.0.0 "
580664937,32682,Add track_times flag for HDFStore put method,krysma,closed,2020-03-13T15:10:45Z,2020-05-14T12:38:24Z,"When adding table to HDF using put method of HDFStore, it would be good to have option to set track_times flag used in pytables in method create_table. Thanks to this flag it is possible to stop tracking of times the file was changed. This is causing problems when versioning the HDF files and using checksums.

For this, it is necessary to propagate this flag from: https://github.com/pandas-dev/pandas/blob/master/pandas/io/pytables.py#L973 to this line: https://github.com/pandas-dev/pandas/blob/master/pandas/io/pytables.py#L4144"
617856549,34171,DEPR: DateOffset.__call__,jbrockmendel,closed,2020-05-14T01:26:28Z,2020-05-14T16:43:22Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
613649729,34038,"BUG: Pandas Cython pd.sort_values() function, ignore_index DOESN'T WORK",edin0,closed,2020-05-06T22:22:27Z,2020-05-14T18:40:00Z,"- [ ] I have checked that this issue has not already been reported.
yes
- [ ] I have confirmed this bug exists on the latest version of pandas.
yes
- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.
idk
<img width=""646"" alt=""Capture d’écran 2020-05-07 à 00 12 24"" src=""https://user-images.githubusercontent.com/60808981/81234096-635e3080-8ff8-11ea-9f53-3c9709a66f34.png"">




---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here

```

#### Problem description
The code works on python3, but not on Cython. It works when I delete ""ignore_index=True"" from my code, but then the output is obviously wrong :(
(sorry for censoring, I don't want to leak anything)
[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output
ignore_index should work on Cython 
#### Output of ``pd.show_versions()``
1.0.3
<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>"
617985969,34172,BUG: GroupBy sum on an Int64 column with min_count=1 is wrong when all values are null,ali-tny,closed,2020-05-14T07:17:30Z,2020-05-15T00:50:53Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
```python
import pandas as pd

df = pd.DataFrame(
    [
        {""key"": 1, ""value"": None},
    ],
    dtype=""Int64"",
)

>>> df[""value""].sum(min_count=1)
<NA>
>>> df.groupby(""key"")[""value""].sum(min_count=1)
key
1    -9223372036854775808
Name: value, dtype: Int64
```

#### Problem description

When summing with min_count=1 (so that groups with all nulls sum to null), DataFrame.sum exhibits expected behaviour (ie, all nulls sum to null), but GroupBy.sum returns the minimum Int64 -9223372036854775808 rather than `<NA>`. 

#### Expected Output
```python
>>> df.groupby(""key"")[""value""].sum(min_count=1)
key
1    <NA>
Name: value, dtype: Int64
```
#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 17.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.3
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.14.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
617754449,34163,CLN: remove methods of ExtensionIndex that duplicate base Index,jorisvandenbossche,closed,2020-05-13T21:05:59Z,2020-05-15T07:58:24Z,"xref https://github.com/pandas-dev/pandas/pull/34159

cc @jbrockmendel "
559453375,31641,DOC: Removed numeric_only parameter from pd.DataFrame.mad docs,r0cketr1kky,closed,2020-02-04T02:45:15Z,2020-05-15T08:47:48Z,"- closes #29079 
"
615781218,34113,REGR: exceptions not caught in _call_map_locations,simonjayhawkins,closed,2020-05-11T10:38:47Z,2020-05-15T09:33:18Z,"- [ ] xref #32409
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

# NOTE: issue 'fixed' on master. This PR against 1.0.x branch

cc @jbrockmendel Could there still be issue on master with the exceptions not being caught?"
542555018,30486,REF: window/test_moments.py,jreback,closed,2019-12-26T13:22:32Z,2020-05-15T12:53:25Z,"https://github.com/pandas-dev/pandas/blob/master/pandas/tests/window/test_moments.py

this includes testing for ewm/rolling and others. let's put into separate files to make this simpler."
339158745,21793,BUG: PeriodIndex comparisons break on listlike,jbrockmendel,closed,2018-07-07T16:40:30Z,2020-05-15T12:58:18Z,"```
idx = pd.period_range('2016', periods=3, freq='M')

>>> idx == idx.values
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/site-packages/pandas/core/indexes/period.py"", line 107, in wrapper
    other = Period(other, freq=self.freq)
  File ""pandas/_libs/tslibs/period.pyx"", line 1777, in pandas._libs.tslibs.period.Period.__new__
TypeError: unhashable type: 'numpy.ndarray'
```"
618289299,34175,Update test_period_range.py,jnecus,closed,2020-05-14T14:43:32Z,2020-05-15T12:58:21Z,"Attempting to implement test for resolved issue #21793

- [x] closes #21793"
619009679,34190,Backport PR #33983 on branch 1.0.x (BUG: Use args and kwargs in Rolling.apply),simonjayhawkins,closed,2020-05-15T14:32:12Z,2020-05-15T15:01:26Z,"xref #33983, #33433

fixes regression in 1.0.0 "
618379375,34176,CLN: update cimports,jbrockmendel,closed,2020-05-14T16:45:40Z,2020-05-15T15:13:17Z,"some cosmetic, some ordering conventions, some using cimportable classes instead of py-classes"
617129129,34148,REF: move more of Tick into liboffsets._Tick,jbrockmendel,closed,2020-05-13T04:21:03Z,2020-05-15T15:53:02Z,
616947280,34142,May 2020 Dev Meeting,TomAugspurger,closed,2020-05-12T20:43:56Z,2020-05-15T16:35:17Z,"The monthly dev meeting is tomorrow, Wednesday 13th, at 18:00 UTC. Our calendar is at https://pandas.pydata.org/docs/development/meeting.html#calendar to check your local time.

Video Call: https://zoom.us/j/942410248
Minutes: https://docs.google.com/document/u/1/d/1tGbTiYORHiSPgVMXawiweGJlBw5dOkVJLY-licoBmBU/edit?ouid=102771015311436394588&usp=docs_home&ths=true

Please add items you'd like to see discussed to the agenda."
612244510,33983,BUG: Use args and kwargs in Rolling.apply,mproszewska,closed,2020-05-05T00:02:04Z,2020-05-15T20:28:12Z,"- [x] closes #33433
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
572264142,32302,TYP: Groupby sum|prod|min|max|first|last methods,topper-123,closed,2020-02-27T18:14:32Z,2020-05-15T21:21:23Z,"This makes two things:
* makes mypy know the signatures of the agg methods (``sum``, ``prod``, etc.) on Groupby
* gives the agg methods on Groupby objects a return type

This ensues that mypy and  IDE's always know that the return type of e.g. ``df.groupby('a').sum()`` is a DataFrame, which is nice when chaining."
54430781,9261,read_sql_query does not convert invalid dates to NaT,jorisvandenbossche,open,2015-01-15T09:58:56Z,2020-05-16T03:38:40Z,"From: http://stackoverflow.com/questions/27908071/pandas-interprets-timestamp-without-timezones-columns-as-different-types/27960779#27960779

Using:

```
df = pd.DataFrame({'col1':[0,1], 'col2':pd.date_range('2012-01-01', periods=2, freq='1h')})
df.to_sql('test_invalid_date', engine_postgres, if_exists='replace', index=False)
engine_postgres.execute(""INSERT INTO test_invalid_date VALUES (2, '3012-01-01 00:00:00');"")
```

it gives a different handling of the invalid date depending on using `read_sql_table` or `read_sql_query`:

```
In [34]: df1 = pd.read_sql_table('test_invalid_date', engine_postgres)

In [35]: df1
Out[35]:
   col1                col2
0     0 2012-01-01 00:00:00
1     1 2012-01-01 01:00:00
2     2                 NaT

In [36]: df1.dtypes
Out[36]:
col1             int64
col2    datetime64[ns]
dtype: object

In [37]: df2 = pd.read_sql_query('SELECT * FROM test_invalid_date', engine_postgres)

In [38]: df2
Out[38]:
   col1                 col2
0     0  2012-01-01 00:00:00
1     1  2012-01-01 01:00:00
2     2  3012-01-01 00:00:00

In [39]: df2.dtypes
Out[39]:
col1     int64
col2    object
dtype: object

In [40]: df2.col2.values
Out[40]:
array([datetime.datetime(2012, 1, 1, 0, 0),
       datetime.datetime(2012, 1, 1, 1, 0),
       datetime.datetime(3012, 1, 1, 0, 0)], dtype=object)
```
"
118488878,11683,Params not properly supported for pymssql,litchfield,open,2015-11-23T22:34:54Z,2020-05-16T03:45:19Z,"Pandas read SQL functions internally convert params to a list in pandas.io.sql._convert_params().

However on pymssql, params must be a tuple or dictionary. Stacktrace below. 

```
Traceback (most recent call last):
  File ""/Users/simon/.virtualenvs/rfcapital/lib/python3.4/site-packages/pandas/io/sql.py"", line 1539, in execute
    cur.execute(*args)
  File ""pymssql.pyx"", line 447, in pymssql.Cursor.execute (pymssql.c:6274)
  File ""_mssql.pyx"", line 998, in _mssql.MSSQLConnection.execute_query (_mssql.c:10085)
  File ""_mssql.pyx"", line 1029, in _mssql.MSSQLConnection.execute_query (_mssql.c:9964)
  File ""_mssql.pyx"", line 1146, in _mssql.MSSQLConnection.format_and_run_query (_mssql.c:11010)
  File ""_mssql.pyx"", line 1168, in _mssql.MSSQLConnection.format_sql_command (_mssql.c:11219)
  File ""_mssql.pyx"", line 1799, in _mssql._substitute_params (_mssql.c:18045)
ValueError: 'params' arg (<class 'list'>) can be only a tuple or a dictionary.
```
"
172571818,14067,to_sql: Does not produce a hierarchical index when using SQLAlchemy as the dbAPI,ifiddes,open,2016-08-22T22:38:26Z,2020-05-16T03:46:56Z,"Based on this post here:

http://stackoverflow.com/questions/39089513/pandas-sqlalchemy-engine-does-not-produce-hierarchical-index-but-legacy-mode-do
#### data ('test.csv'):

```
AlignmentId,Classifier,Value
aln1,classifier1,True
aln1,classifier2,False
aln2,classifier1,True
```
#### Sample code:

```
import pandas as pd
import sqlalchemy
import sqlite3

sqla_con = sqlalchemy.create_engine('sqlite:///test.db')
legacy_con = sqlite3.connect('test.db')

df = pd.read_csv('test.csv', index_col=[0, 1])

df.to_sql('legacy', legacy_con)
df.to_sql('sqlalchemy', sqla_con)
```
#### Output (inspecting the resulting sqlite database):

```
$sqlite3
SQLite version 3.13.0 2016-05-18 10:57:30
Enter "".help"" for usage hints.
Connected to a transient in-memory database.
Use "".open FILENAME"" to reopen on a persistent database.
sqlite> .open test.db
sqlite> .schema
CREATE TABLE ""legacy"" (
""AlignmentId"" TEXT,
  ""Classifier"" TEXT,
  ""Value"" INTEGER
);
CREATE INDEX ""ix_legacy_AlignmentId_Classifier""ON ""legacy"" (""AlignmentId"",""Classifier"");
CREATE TABLE sqlalchemy (
    ""AlignmentId"" TEXT,
    ""Classifier"" TEXT,
    ""Value"" BOOLEAN,
    CHECK (""Value"" IN (0, 1))
);
CREATE INDEX ""ix_sqlalchemy_Classifier"" ON sqlalchemy (""Classifier"");
CREATE INDEX ""ix_sqlalchemy_AlignmentId"" ON sqlalchemy (""AlignmentId"");
```
#### output of `pd.show_versions()`
## INSTALLED VERSIONS

commit: None
python: 2.7.11.final.0
python-bits: 64
OS: Linux
OS-release: 2.6.32-504.3.3.el6.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.18.1
nose: 1.3.7
pip: 8.1.2
setuptools: 23.0.0
Cython: 0.24
numpy: 1.11.1
scipy: 0.17.1
statsmodels: 0.6.1
xarray: None
IPython: 4.2.0
sphinx: 1.4.1
patsy: 0.4.1
dateutil: 2.5.3
pytz: 2016.4
blosc: None
bottleneck: 1.0.0
tables: 3.2.2
numexpr: 2.6.0
matplotlib: 1.5.1
openpyxl: 2.3.2
xlrd: 1.0.0
xlwt: 1.1.2
xlsxwriter: 0.9.2
lxml: 3.6.0
bs4: 4.4.1
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 1.0.13
pymysql: None
psycopg2: None
jinja2: 2.8
boto: 2.40.0
pandas_datareader: None
"
203226593,15229,ENH: string sql query passed to read_sql_query should be wrapped in sqlalchemy.text when using an sqlalchemy engine,haleemur,open,2017-01-25T21:24:56Z,2020-05-16T03:51:08Z,"#### Code Sample, copy-pastable if proper database credentials are used

```python
import pandas as pd
from sqlalchemy import create_engine, text

# change the connection string as required
engine = create_engine('postgresql+psycopg2://hal@localhost/hal')
engine.execute('CREATE TABLE test_orders (id SERIAL PRIMARY KEY, customer_type VARCHAR)')
engine.execute(""INSERT INTO test_orders (customer_type) VALUES ('urban'), ('urban'), ('rural')"")

statement = ""SELECT COUNT(*) FROM test_orders WHERE customer_type LIKE 'urban%'""
statement2 = ""SELECT COUNT(*) FROM test_orders WHERE customer_type LIKE :c""

# does not work, exception shown in the expected output section
try: 
    df = pd.read_sql_query(statement, engine)
except Exception as e:
    print(str(type(e) + ': ' + str(e))
    

# does not work, exception shown in the expected output section
try: 
    df = pd.read_sql_query(statement, engine, params={'c': 'urban%'})
except Exception as e:
    print(str(e))

# works 
try: 
  df = pd.read_sql_query(text(statement), engine)
  df2 = pd.read_sql_query(text(statement), engine, params={'c': 'urban%'})
except Exception as e:
  print(str(e))
  
```
#### Problem description

Using plain-text sql with sqlalchemy engines don't work
* if the `%` character is present in the query 
* with the argument `params`. 

The queries work as expected if the sql is wrapped with `sqlalchemy.text`. Also, wrapping a plain-text query with `sqlalchemy.text` should not have any effect on the output or performance of the query. 

However that is not an intuitive behavior for many analysts, who don't know what sqlalchemy is or what ORMs are in general, or that different database drivers may use `%` to format parameters.

Here's a stackoverflow Q-A on the same sqlalchemy usage issue:
http://stackoverflow.com/questions/8657508/strange-sqlalchemy-error-message-typeerror-dict-object-does-not-support-inde

We can prevent this error by wrapping the plain-text sql by `text` if an sqlalchemy engine is passed as the `con` argument to the function.

Something like this:

```python
def read_sql_query(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, chunksize=None):
    if isinstance(sql, string) and isinstance(con, Engine):
        sql = text(sql)
    ...
```

I'd be happy to submit a PR for this code change + tests & documentation updates if the maintainers agree this to be an acceptable resolution.

#### Expected Output
```
# output from the first try-except block
<class 'TypeError'>: 'dict' object does not support indexing

# output from the second try-except block
(psycopg2.ProgrammingError) syntax error at or near "":""
LINE 1: ...ELECT COUNT(*) FROM test_orders WHERE customer_type LIKE :c1
                                                                    ^
 [SQL: 'SELECT COUNT(*) FROM test_orders WHERE customer_type LIKE :c1'] [parameters: {'c1': 'urban%'}]
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.0.final.0
python-bits: 64
OS: Darwin
OS-release: 15.6.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_CA.UTF-8
LOCALE: en_CA.UTF-8

pandas: 0.19.2
nose: None
pip: 9.0.1
setuptools: 28.8.0
Cython: 0.25.2
numpy: 1.12.0
scipy: 0.18.1
statsmodels: 0.6.1
xarray: None
IPython: 5.1.0
sphinx: None
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: None
tables: None
numexpr: 2.6.1
matplotlib: 2.0.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.999999999
httplib2: None
apiclient: None
sqlalchemy: 1.1.5
pymysql: 0.7.9.None
psycopg2: 2.6.2 (dt dec pq3 ext lo64)
jinja2: 2.9.4
boto: None
pandas_datareader: None

</details>
"
239639205,16802,to_sql not working with sqlalchemy hana engine,niravchit,open,2017-06-29T23:33:21Z,2020-05-16T03:52:26Z,"#### Code Sample, a copy-pastable example if possible

```python
hanaeng = create_engine('hana://username:password@host_address:port')
my_df.to_sql('table_name', con = hanaeng, index = False, if_exists = 'append')


sqlalchemy.exc.DatabaseError: (pyhdb.exceptions.DatabaseError) invalid column name
```
#### Problem description

I'm trying to push a dataframe into a Hana DB using a sqlalchemy engine (with pyhdb and sqlalchemy-hana installed) however I keep receiving the error above. I created a table in my Hana schema that matches the column names and type of what I'm trying to pass into it from the dataframe.

Has anyone ever come across this error? Or tried connecting to hana using a sqlalchemy engine? I tried using a pyhdb connector to make a connection object and passing that into to_sql but I believe pandas is trying to shift accepting only sqlalchemy engine objects in to_sql versus straight DBAPI connectors? Regardless, any help will be great! Thank you

#### Expected Output

#### Output of ``pd.show_versions()``

<details>
using 0.19 version of pandas
https://stackoverflow.com/questions/44836006/pandas-to-sql-method-work-with-sqlalchemy-hana-connector

</details>
"
258105524,17546,to_sql issue with MS Azure data warehouse,Saravji,open,2017-09-15T17:07:16Z,2020-05-16T03:52:54Z,"#### Code Sample, a copy-pastable example if possible
with the patch to sqlalchemy installed (see below), the following code snippet executes successfully:

```
connection_string = ""mssql+pyodbc://<username>:<password>@<sqlhost>.database.windows.net:<port>/<database>?driver=ODBC+Driver+13+for+SQL+Server""
engn = sqlalchemy.engine.create_engine(connection_string, echo=True)
engn.connect()
```
The moment I add

`df.to_sql(tbl_server_out, engn, if_exists='append', index=False)`

I get the following error stack:

```
2017-09-13 15:07:50,200 INFO sqlalchemy.engine.base.Engine SELECT  SERVERPROPERTY('ProductVersion')
2017-09-13 15:07:50,202 INFO sqlalchemy.engine.base.Engine ()
2017-09-13 15:07:50,246 INFO sqlalchemy.engine.base.Engine SELECT schema_name()
2017-09-13 15:07:50,247 INFO sqlalchemy.engine.base.Engine ()
2017-09-13 15:07:50,522 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1
2017-09-13 15:07:50,522 INFO sqlalchemy.engine.base.Engine ()
2017-09-13 15:07:50,562 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS NVARCHAR(60)) AS anon_1
2017-09-13 15:07:50,563 INFO sqlalchemy.engine.base.Engine ()
2017-09-13 15:07:50,604 INFO sqlalchemy.engine.base.Engine SELECT [INFORMATION_SCHEMA].[COLUMNS].[TABLE_SCHEMA], [INFORMATION_SCHEMA].[COLUMNS].[TABLE_NAME], [INFORMATION_SCHEMA].[COLUMNS].[COLUMN_NAME], [INFORMATION_SCHEMA].[COLUMNS].[IS_NULLABLE], [INFORMATION_SCHEMA].[COLUMNS].[DATA_TYPE], [INFORMATION_SCHEMA].[COLUMNS].[ORDINAL_POSITION], [INFORMATION_SCHEMA].[COLUMNS].[CHARACTER_MAXIMUM_LENGTH], [INFORMATION_SCHEMA].[COLUMNS].[NUMERIC_PRECISION], [INFORMATION_SCHEMA].[COLUMNS].[NUMERIC_SCALE], [INFORMATION_SCHEMA].[COLUMNS].[COLUMN_DEFAULT], [INFORMATION_SCHEMA].[COLUMNS].[COLLATION_NAME] 
FROM [INFORMATION_SCHEMA].[COLUMNS] 
WHERE [INFORMATION_SCHEMA].[COLUMNS].[TABLE_NAME] = CAST(? AS NVARCHAR(max)) AND [INFORMATION_SCHEMA].[COLUMNS].[TABLE_SCHEMA] = CAST(? AS NVARCHAR(max))
2017-09-13 15:07:50,605 INFO sqlalchemy.engine.base.Engine ('dbo.MSODS_DSI', 'dbo')
2017-09-13 15:07:52,151 INFO sqlalchemy.engine.base.Engine 
CREATE TABLE [dbo.MSODS_DSI] (
	[a] DATETIME NULL, 
	[b] VARCHAR(max) NULL, 
	[c] VARCHAR(max) NULL, 
	[d] VARCHAR(max) NULL, 
	[e] VARCHAR(max) NULL, 
	[f] FLOAT(53) NULL, 
	[g] FLOAT(53) NULL
)


2017-09-13 15:07:52,152 INFO sqlalchemy.engine.base.Engine ()
2017-09-13 15:07:54,374 INFO sqlalchemy.engine.base.Engine ROLLBACK
---------------------------------------------------------------------------
ProgrammingError                          Traceback (most recent call last)
/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)
   1181                         parameters,
-> 1182                         context)
   1183         except BaseException as e:

/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/default.py in do_execute(self, cursor, statement, parameters, context)
    469     def do_execute(self, cursor, statement, parameters, context=None):
--> 470         cursor.execute(statement, parameters)
    471 

ProgrammingError: ('42000', ""[42000] [Microsoft][ODBC Driver 13 for SQL Server][SQL Server]The statement failed. Column 'b' has a data type that cannot participate in a columnstore index.\r\nOperation cancelled by user. (35343) (SQLExecDirectW)"")

The above exception was the direct cause of the following exception:

ProgrammingError                          Traceback (most recent call last)
<ipython-input-16-290e9c1020c9> in <module>()
     17 #cnxn = pyodbc.connect(connection_str)
     18 #engn.connect()
---> 19 df.to_sql(tbl_server_out, engn, if_exists='append', index=False)

/home/saravji/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in to_sql(self, name, con, flavor, schema, if_exists, index, index_label, chunksize, dtype)
   1343         sql.to_sql(self, name, con, flavor=flavor, schema=schema,
   1344                    if_exists=if_exists, index=index, index_label=index_label,
-> 1345                    chunksize=chunksize, dtype=dtype)
   1346 
   1347     def to_pickle(self, path, compression='infer'):

/home/saravji/anaconda3/lib/python3.6/site-packages/pandas/io/sql.py in to_sql(frame, name, con, flavor, schema, if_exists, index, index_label, chunksize, dtype)
    469     pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index,
    470                       index_label=index_label, schema=schema,
--> 471                       chunksize=chunksize, dtype=dtype)
    472 
    473 

/home/saravji/anaconda3/lib/python3.6/site-packages/pandas/io/sql.py in to_sql(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype)
   1148                          if_exists=if_exists, index_label=index_label,
   1149                          schema=schema, dtype=dtype)
-> 1150         table.create()
   1151         table.insert(chunksize)
   1152         if (not name.isdigit() and not name.islower()):

/home/saravji/anaconda3/lib/python3.6/site-packages/pandas/io/sql.py in create(self)
    596                     ""'{0}' is not valid for if_exists"".format(self.if_exists))
    597         else:
--> 598             self._execute_create()
    599 
    600     def insert_statement(self):

/home/saravji/anaconda3/lib/python3.6/site-packages/pandas/io/sql.py in _execute_create(self)
    581         # Inserting table into database, add to MetaData object
    582         self.table = self.table.tometadata(self.pd_sql.meta)
--> 583         self.table.create()
    584 
    585     def create(self):

/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/sql/schema.py in create(self, bind, checkfirst)
    754         bind._run_visitor(ddl.SchemaGenerator,
    755                           self,
--> 756                           checkfirst=checkfirst)
    757 
    758     def drop(self, bind=None, checkfirst=False):

/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _run_visitor(self, visitorcallable, element, connection, **kwargs)
   1927                      connection=None, **kwargs):
   1928         with self._optional_conn_ctx_manager(connection) as conn:
-> 1929             conn._run_visitor(visitorcallable, element, **kwargs)
   1930 
   1931     class _trans_ctx(object):

/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _run_visitor(self, visitorcallable, element, **kwargs)
   1536     def _run_visitor(self, visitorcallable, element, **kwargs):
   1537         visitorcallable(self.dialect, self,
-> 1538                         **kwargs).traverse_single(element)
   1539 
   1540 

/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/sql/visitors.py in traverse_single(self, obj, **kw)
    119             meth = getattr(v, ""visit_%s"" % obj.__visit_name__, None)
    120             if meth:
--> 121                 return meth(obj, **kw)
    122 
    123     def iterate(self, obj):

/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/sql/ddl.py in visit_table(self, table, create_ok, include_foreign_key_constraints, _is_metadata_operation)
    765             CreateTable(
    766                 table,
--> 767                 include_foreign_key_constraints=include_foreign_key_constraints
    768             ))
    769 

/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py in execute(self, object, *multiparams, **params)
    943             raise exc.ObjectNotExecutableError(object)
    944         else:
--> 945             return meth(self, multiparams, params)
    946 
    947     def _execute_function(self, func, multiparams, params):

/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/sql/ddl.py in _execute_on_connection(self, connection, multiparams, params)
     66 
     67     def _execute_on_connection(self, connection, multiparams, params):
---> 68         return connection._execute_ddl(self, multiparams, params)
     69 
     70     def execute(self, bind=None, target=None):

/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _execute_ddl(self, ddl, multiparams, params)
   1000             compiled,
   1001             None,
-> 1002             compiled
   1003         )
   1004         if self._has_events or self.engine._has_events:

/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)
   1187                 parameters,
   1188                 cursor,
-> 1189                 context)
   1190 
   1191         if self._has_events or self.engine._has_events:

/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _handle_dbapi_exception(self, e, statement, parameters, cursor, context)
   1400                 util.raise_from_cause(
   1401                     sqlalchemy_exception,
-> 1402                     exc_info
   1403                 )
   1404             else:

/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/util/compat.py in raise_from_cause(exception, exc_info)
    201     exc_type, exc_value, exc_tb = exc_info
    202     cause = exc_value if exc_value is not exception else None
--> 203     reraise(type(exception), exception, tb=exc_tb, cause=cause)
    204 
    205 if py3k:

/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/util/compat.py in reraise(tp, value, tb, cause)
    184             value.__cause__ = cause
    185         if value.__traceback__ is not tb:
--> 186             raise value.with_traceback(tb)
    187         raise value
    188 

/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)
   1180                         statement,
   1181                         parameters,
-> 1182                         context)
   1183         except BaseException as e:
   1184             self._handle_dbapi_exception(

/home/saravji/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/default.py in do_execute(self, cursor, statement, parameters, context)
    468 
    469     def do_execute(self, cursor, statement, parameters, context=None):
--> 470         cursor.execute(statement, parameters)
    471 
    472     def do_execute_no_params(self, cursor, statement, context=None):

ProgrammingError: (pyodbc.ProgrammingError) ('42000', ""[42000] [Microsoft][ODBC Driver 13 for SQL Server][SQL Server]The statement failed. Column 'b' has a data type that cannot participate in a columnstore index.\r\nOperation cancelled by user. (35343) (SQLExecDirectW)"") [SQL: '\nCREATE TABLE [dbo.MSODS_DSI] (\n\t[a] DATETIME NULL, \n\t[b] VARCHAR(max) NULL, \n\t[c] VARCHAR(max) NULL, \n\t[d] VARCHAR(max) NULL, \n\t[e] VARCHAR(max) NULL, \n\t[f] FLOAT(53) NULL, \n\t[g] FLOAT(53) NULL\n)\n\n']
```

#### Problem description

In the configuration python 3.6.1, pandas, pyodbc, sqlalchemy and Azure SQL DataWarehouse the df.to_sql(..., if_exists='append') call actually executes a create table sql statement (with deviating from the existing table column definition).
As the table exists, this is supposed to fail.
It appears that the create table statement should not be executed if the option ""if_exists='append'"" is set?
There is no issue with the installation / database connection.
I can execute sql queries with raw pyodbc (insert, delete, select) as well as execute read_sql.

Note: A patch to sqlalchemy needs to be applied to prevent an additional error from happening.
see this thread for details: [google mailing list sqlalchemy thread pandas to MS SQL DataWarehouse (to_sql)](https://groups.google.com/forum/#!topic/sqlalchemy/KescPeeokfc)

**Note**: We receive a lot of issues on our GitHub tracker: I have checked and could not find related issues.

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you: I'm on very recent version, the updates to 0.20.2 and 0.20.3 do not list any changes to the to_sql() functionality


#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.1.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-93-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.20.1
pytest: 3.0.7
pip: 9.0.1
setuptools: 27.2.0
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.19.0
xarray: None
IPython: 5.3.0
sphinx: 1.5.6
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.3.0
numexpr: 2.6.2
feather: None
matplotlib: 2.0.2
openpyxl: 2.4.7
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: 0.9.6
lxml: 3.7.3
bs4: 4.6.0
html5lib: 0.999
sqlalchemy: 1.1.13
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
pandas_gbq: None
pandas_datareader: None

</details>
"
463104902,27177,dataframe to_sql is inserting raw text into oracle database instead of actual text when utf-8 text is present ,jonanem,open,2019-07-02T08:55:56Z,2020-05-16T03:56:42Z,"
#### Problem description

When trying to insert data into oracle database through dataframe to_sql it is taking long time so i have made changes type(sqlalchemy types) of object to VARCHAR using below code


```python
dtyp = {c:types.VARCHAR(df[c].str.len().max())
    for c in df.columns[df.dtypes == 'object'].tolist()}

df.to_sql(name='TEST_TABLE', con=engine, if_exists='append', index=False, chunksize=1000, dtype=dtyp)
```
Column of the dataframe contains text as utf-8, eg data is b'This is oracle'

b'This is oracle' is actual data present in the dataframe but the data is inserted in the oracle database is '415041432053656D69636F6E647563746F7' (example values) not the actual text 'This is oracle'. Seems like 'b' present before the text is causing the issue when converting it to VARCHAR if i trim out 'b' using decode UTF-8 i am getting UnicodeEncodeError: 'ascii' codec can't encode character error while inserting into database

Seems like to_sql having issue when utf-8 text is inserted as VARCHAR using dtypes



#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.8.final.0
python-bits: 64
OS: Linux
OS-release: 3.10.0-957.1.3.el7.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.24.2


</details>
"
475417254,27684,`to_sql` does not correctly load data into snowflake,lucasludewigsamaro,closed,2019-08-01T00:44:10Z,2020-05-16T03:59:43Z,"#### Code Sample, a copy-pastable example if possible

```python
"""""" Insert dataframe into table.""""""
    if not df1.empty:
        df1['source'].str.lower()
        df1 = df1.replace(r'^\s*$', 0, regex=True)
        df1.to_sql(config['table_name'], schema='ecs_marketing',
                   con=conn, chunksize=200, index=False, if_exists='replace',
                   dtype={'date': sqlalchemy.types.Date,
                          'cost': sqlalchemy.types.Numeric(8, 3)})

```
#### Problem description

I am importing data from Google Sheets. 
So I create a dataframe with the spreadsheet content.
Then, I perform some data transformations. Until here, everything normal.

The problem appears when I try to load the data into Snowflake using to_sql command.
I receive the error:

```
(snowflake.connector.errors.ProgrammingError) 090105 (22000): Cannot perform SELECT. This session does not have a current database. Call 'USE DATABASE', or use a qualified name.
[SQL: 
SELECT /* sqlalchemy:get_columns */
       ic.table_name,
       ic.column_name,
       ic.data_type,
       ic.character_maximum_length,
       ic.numeric_precision,
       ic.numeric_scale,
       ic.is_nullable,
       ic.column_default,
       ic.is_identity,
       ic.comment
  FROM information_schema.columns ic
 WHERE ic.table_schema=%(table_schema)s
   AND ic.table_name=%(table_name)s
 ORDER BY ic.ordinal_position
]
[parameters: {'table_schema': 'dev_raw.ecs_marketing', 'table_name': 'DIRECT_AFFILIATES'}]
(Background on this error at: http://sqlalche.me/e/f405)
```

I immediately noted that on error ""parameters"" dictionary, table_schema is in lower case and table_name is upper case. **Would that be a bug?**

Then I started to dig in. And figured out that to_sql looks for table DDL when `if_exists`='replace'.
If `if_exists`='append', I don't face the problem. Unfortunately, I need to replace.

If I call the function with schema=<database_name>.ecs_marketing also doesn't work.

So, **should to_sql accept also a database parameter, in case of snowflake?**

#### Expected Output

Able to insert my data into Snowflake, without snowflake errors.

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-55-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : pt_BR.UTF-8

pandas           : 0.25.0
numpy            : 1.17.0
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.2.1
setuptools       : 41.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.6
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
485403722,28153,`chunksize` in `read_sql_table` sometimes is not accurate,vvaezian,open,2019-08-26T19:24:58Z,2020-05-16T04:00:35Z,"I have a table containing about 28 million rows. I read the data in 1 million row chunks:
```python
params = urllib.parse.quote_plus(""DRIVER={SQL Server Native Client 11.0};""
                                 ""SERVER=xxxx;""
                                 ""DATABASE=xxxx;""
                                 ""Trusted_Connection=yes"")
engine = sa.create_engine(""mssql+pyodbc:///?odbc_connect={}"".format(params))

pd_frame_generator = pd.read_sql_table(table_name='myTable',
                                        con=engine,
                                        columns=['x', 'y', 'z'] ,
                                        chunksize=1000000
                                       )
```
But sometime one or two more rows are read. The following is the number of lines after exporting the read data into csv (one extra row is normal because of the header. Abnormal ones are morked in red):
![pd_read_sql](https://user-images.githubusercontent.com/15974735/63714998-c1930c80-c7f7-11e9-9dae-9b3fbfd8a7f7.png)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.0.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : None.None

pandas           : 0.25.1
numpy            : 1.15.1
pytz             : 2018.5
dateutil         : 2.7.3
pip              : 18.0
setuptools       : 39.0.1
Cython           : None
pytest           : 3.9.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.2.5
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.7 (dt dec pq3 ext lo64)
jinja2           : 2.10
IPython          : 7.0.1
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.5
matplotlib       : 2.2.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : 1.3.1
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
531292804,29974,pandas.read_sql() fails when the select is not the sole part of the query,djrscally,open,2019-12-02T16:57:03Z,2020-05-16T04:06:53Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import pyodbc
import sqlalchemy as sa
import urllib

query = """"""
drop table if exists #MyTempTable;

select
	1 ID
	, 'A' Letter
into #MyTempTable
union
	select 2, 'B' union
	select 3, 'C' union
	select 4, 'D' union
	select 5, 'E'
;

select
	*
from #MyTempTable
""""""

connection_string = urllib.parse.quote_plus(
    'DRIVER={SQL Server Native Client 11.0};\
    SERVER={0},{1};\
    trusted_Connection=yes\
    ;encrypt=yes;\
    trustServerCertificate=yes;'.format(server, port)
)

eng = sa.create_engine('mssql+pyodbc:///?odbc_connect={0}'.format(connection_string))

df = pd.read_sql(query, con=eng)
```
#### Problem description

The problem is that SQLAlchemy throws an exception when running the snippet of code above:

```Python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
c:\users\dscally\appdata\local\programs\python\python37\lib\site-packages\sqlalchemy\engine\result.py in _fetchall_impl(self)
   1162         try:
-> 1163             return self.cursor.fetchall()
   1164         except AttributeError:

AttributeError: 'NoneType' object has no attribute 'fetchall'

During handling of the above exception, another exception occurred:

ResourceClosedError                       Traceback (most recent call last)
<ipython-input-2-018f2f8f6b89> in <module>
----> 1 df = pd.read_sql(query, con=eng)

c:\users\dscally\appdata\local\programs\python\python37\lib\site-packages\pandas\io\sql.py in read_sql(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)
    395             sql, index_col=index_col, params=params,
    396             coerce_float=coerce_float, parse_dates=parse_dates,
--> 397             chunksize=chunksize)
    398 
    399 

c:\users\dscally\appdata\local\programs\python\python37\lib\site-packages\pandas\io\sql.py in read_query(self, sql, index_col, coerce_float, parse_dates, params, chunksize)
   1106                                         parse_dates=parse_dates)
   1107         else:
-> 1108             data = result.fetchall()
   1109             frame = _wrap_result(data, columns, index_col=index_col,
   1110                                  coerce_float=coerce_float,

c:\users\dscally\appdata\local\programs\python\python37\lib\site-packages\sqlalchemy\engine\result.py in fetchall(self)
   1216         except BaseException as e:
   1217             self.connection._handle_dbapi_exception(
-> 1218                 e, None, None, self.cursor, self.context
   1219             )
   1220 

c:\users\dscally\appdata\local\programs\python\python37\lib\site-packages\sqlalchemy\engine\base.py in _handle_dbapi_exception(self, e, statement, parameters, cursor, context)
   1458                 util.raise_from_cause(sqlalchemy_exception, exc_info)
   1459             else:
-> 1460                 util.reraise(*exc_info)
   1461 
   1462         finally:

c:\users\dscally\appdata\local\programs\python\python37\lib\site-packages\sqlalchemy\util\compat.py in reraise(tp, value, tb, cause)
    275         if value.__traceback__ is not tb:
    276             raise value.with_traceback(tb)
--> 277         raise value
    278 
    279 

c:\users\dscally\appdata\local\programs\python\python37\lib\site-packages\sqlalchemy\engine\result.py in fetchall(self)
   1211 
   1212         try:
-> 1213             l = self.process_rows(self._fetchall_impl())
   1214             self._soft_close()
   1215             return l

c:\users\dscally\appdata\local\programs\python\python37\lib\site-packages\sqlalchemy\engine\result.py in _fetchall_impl(self)
   1163             return self.cursor.fetchall()
   1164         except AttributeError:
-> 1165             return self._non_result([])
   1166 
   1167     def _non_result(self, default):

c:\users\dscally\appdata\local\programs\python\python37\lib\site-packages\sqlalchemy\engine\result.py in _non_result(self, default)
   1168         if self._metadata is None:
   1169             raise exc.ResourceClosedError(
-> 1170                 ""This result object does not return rows. ""
   1171                 ""It has been closed automatically.""
   1172             )

ResourceClosedError: This result object does not return rows. It has been closed automatically.
```
 The exception implies that the problem is caused by the fact that the `DROP` and `SELECT...INTO` statements are both causing the server to return messages along the lines of `(5 rows affected)`, and Pandas is picking up the first of these and trying to treat it as the results of the query. This is backed up by the fact that you can split out the `SELECT` statement, and then things work nicely:

```Python

prelim = """"""
drop table if exists #MyTempTable;

select
	1 ID
	, 'A' Letter
into #MyTempTable
union
	select 2, 'B' union
	select 3, 'C' union
	select 4, 'D' union
	select 5, 'E'
;
""""""

query = """"""
select
    *
from #MyTempTable
""""""

connection_string = urllib.parse.quote_plus(
    'DRIVER={SQL Server Native Client 11.0};\
    SERVER={0},{1};\
    trusted_Connection=yes\
    ;encrypt=yes;\
    trustServerCertificate=yes;'.format(server, port)
)

eng = sa.create_engine('mssql+pyodbc:///?odbc_connect={0}'.format(connection_string))

eng.execute(prelim)

df = pd.read_sql(query, con=eng)
```

And that then returns everything all hunky-dory. I think this is a little user unfriendly, and could be improved.

#### Expected Output

More like ""desired"" than ""expected"", but the simplest method that occurs to me would be to iterate over the returned resultsets using SQLAlchemy's `cursor.nextset()`, and inspection of the results using `cursor.returns_rows` to see if the use of `cursor.fetchall()` would be legal, and returning the first available set where that condition is met.


#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.4.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.1
pytest: None
pip: 19.0.3
setuptools: 41.0.1
Cython: 0.29.6
numpy: 1.16.1
scipy: 1.2.1
pyarrow: 0.14.0
xarray: 0.14.0
IPython: 7.2.0
sphinx: None
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: 1.2.1
lxml.etree: 4.3.4
bs4: None
html5lib: None
sqlalchemy: 1.2.18
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

Not included in `pd.show_versions()` but relevant here:

```Python
>>> print(pyodbc.version)
'4.0.26'
```

</details>
"
555665157,31351,read_sql routes sql query as table name snowflake-sqlalchemy,norton120,open,2020-01-27T15:51:27Z,2020-05-16T04:11:56Z,"#### Code Sample
with [snowflake-sqlalchemy ](https://github.com/snowflakedb/snowflake-sqlalchemy) installed:

```python
from sqlalchemy import create_engine
import pandas as pd

CONN_STRING='snowflake://user:password@account/database'
engine=create_engine(CONN_STRING)

conn=engine.connect()
sql= 'SELECT COUNT(*) FROM ""DATABASE_NAME"".""SCHEMA_NAME"".""TABLE_NAME""'
##this errors (stack below)
r=pd.read_sql(sql)

## this does not 
r=pd.read_sql_query(sql)

```
stack:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/pandas/io/sql.py"", line 420, in read_sql
    pandas_sql.meta.reflect(only=[sql])
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/sql/schema.py"", line 4261, in reflect
    ""in %r%s: (%s)"" % (bind.engine, s, "", "".join(missing))
sqlalchemy.exc.InvalidRequestError: Could not reflect: requested table(s) not available in Engine(snowflake://ethan:***@yxxxxxx.us-east-1/REDACTED_DB_NAME): (SELECT COUNT(*) FROM ""REDACTED_DB_NAME"".""REDACTED_DB_SCHEMA"".""REDACTED_DB_TABLE"")

```
#### Problem description
`read_sql` attempts to read sql queries as tables when running against snowflake. Calling `read_sql_query` is a workaround. 

looks like [this line](https://github.com/pandas-dev/pandas/blob/5c37a0b9caaed81ff3999d05b16be1303d184d77/pandas/io/sql.py#L415):
```
   try:
        _is_table_name = pandas_sql.has_table(sql)
    except Exception:
        # using generic exception to catch errors from sql drivers (GH24988)
        _is_table_name = False

```
is incorrectly determining statements to be table names. 

#### Expected Output
pd dataframe result of query.

#### Output of ``pd.show_versions()``
<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.76-linuxkit
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 44.0.0
Cython           : None
pytest           : 5.3.0
hypothesis       : None
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.12
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
589879466,33129,Feature request: anchoring suffix for hours,yohplala,open,2020-03-29T20:33:53Z,2020-05-16T16:50:08Z,"#### Code Sample, a copy-pastable example if possible

```python
# It is possible to use anchor suffix for weeks
off1 = pd.tseries.frequencies.to_offset('1W-SUN')

# The anchor does effect when using period_range() or creating a period
ts_start = pd.Timestamp('2020-03-25 10:00') # ts_start is a Wednesday

p1 = pd.Period(ts_start, freq=off1)
# Anchor is working: p1 with weekly frequency starts a Monday
p1.start_time
>>> Out: Timestamp('2020-03-23 00:00:00')

# But it doesn't work with '4H' frequency
off2 = pd.tseries.frequencies.to_offset('4H')

# No anchor (expecting one with reference being midnight)
off2.is_anchored()
>>> False

p2 = pd.Period(ts_start, freq=off2)

# Expecting '2020-03-25 08:00:00'
p2.start_time
>>> Timestamp('2020-03-25 10:00:00')

# Trying syntax similar to weekly anchoring
off3 = pd.tseries.frequencies.to_offset('4H-0')
>>> ValueError: Invalid frequency: 4H-0
```
#### Problem description
Please, can anchoring suffix be made available for hours, using midnight as the reference point?

Thanks for your feebdack,
Have a good day,
Bests,"
613367495,34026,DOC: Series.str.rstrip() and Series.str.lstrip() have incorrect description at the top,Dr-Irv,closed,2020-05-06T14:32:01Z,2020-05-16T19:13:46Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.rstrip.html
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.lstrip.html

#### Documentation problem

At the top of the documentation for the `rstrip()` and `lstrip()` methods, it says ""Remove leading and trailing characters.""  

#### Suggested fix for documentation

It should say ""Remove trailing characters"" for `rstrip()` and ""Remove leading characters"" for `lstrip()`.

This is due to the way that the shared docs are set up between `str.lstrip()`, `str.rstrip()` and `str.strip()`
"
250743653,17267,'A/B' type of keys disappear after store 'A' key using to)_hdf command,zhhhaolong,open,2017-08-16T20:05:10Z,2020-05-16T20:34:09Z,"



#### Code Sample, a copy-pastable example if possible

```python
# Your code here
o2='test.h5'
pd.Series(np.zeros(20)).to_hdf(o2,key='All/atest')
with pd.HDFStore(o2) as store:
    print(store.keys())
pd.Series(np.zeros(10)).to_hdf(o2,key='All')
with pd.HDFStore(o2) as store:
    print(store.keys())

```
#### Problem description

I am writing to see if this is the intended feature or a bug. 

Running the following code in python 3.5, Pandas 0.19.2

o2='test.h5'
pd.Series(np.zeros(20)).to_hdf(o2,key='All/atest')
with pd.HDFStore(o2) as store:
    print(store.keys())
pd.Series(np.zeros(10)).to_hdf(o2,key='All')
with pd.HDFStore(o2) as store:
    print(store.keys())

output is:

['/All/atest']
['/All']

I am expecting:
['/All/atest']
['/All','All/atest']

My colleagues have similar problems for pandas 0.20.1

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
366401174,22977,Reading pandas DataFrame with categorical columns from an HDF file with 'where',vss888,open,2018-10-03T15:36:58Z,2020-05-16T20:36:56Z,"It seems pandas DataFrame's with categorical columns are read incorrectly from HDF files with a 'where' argument.

Here is what I see:

```python
>>> import pandas as pd
>>> d = pd.read_hdf(path, columns={'ex'}, where='(ex in [""LN"",""GY"",""FP""])')
>>> d['ex'].unique()
[LN, ID]
Categories (2, object): [LN, ID]
```

For the **same** data saved without conversion of data to Categorical, I get:

```python
>>> import pandas as pd
>>> d = pd.read_hdf(path, columns={'ex'}, where='(ex in [""LN"",""GY"",""FP""])')
>>> d['ex'].unique()
array(['LN'], dtype=object)
```

#### Problem description

I requested only `[""LN"",""GY"",""FP""]` values of `'ex'` but got values with `d['ex']=='ID'`.

#### Expected Output

``` python
>>> d['ex'].unique()
```
should give some subset of `{""LN"",""GY"",""FP""}` values

#### Output of ``pd.show_versions()``

<details>
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.3.final.0
python-bits: 64
OS: Linux
OS-release: 3.10.0-693.17.1.el7.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: en_US.utf-8
LANG: en_US.utf-8
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: 3.3.2
pip: 18.0
setuptools: 39.0.1
Cython: 0.27.3
numpy: 1.14.5
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: 1.6.6
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: None
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 3.0.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.9999999
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>


Thank you for your help!"
454290522,26771,HDFStore performance issue of .select_as_multiple,BeforeFlight,open,2019-06-10T17:55:03Z,2020-05-16T20:38:12Z,"```python
def init_and_store(sh_0, sh_1):
    'Init DataFrame and store it using HDFStore into 2 parts'
    
    df = pd.DataFrame(np.random.rand(sh_0, sh_1))
    cols1 = list(df.columns[:sh_1//2])
    cols2 = list(df.columns[sh_1//2:])
    with pd.HDFStore('test.h5') as store:
        store.put('df1', df[cols1], 't')
        store.put('df2', df[cols2], 't')
    print ('DataFrame shape is', df.shape)
    
def get_select():
    'Retrieve stored df with .select method' 
    
    start = pd.Timestamp.now()
    with pd.HDFStore('test.h5') as store:
        df1 = store.select('df1')
        df2 = store.select('df2')
        out = pd.concat([df1, df2], axis=1)
    print ('SLCT: Select done in %5.2f seconds. Out shape is'% 
           (pd.Timestamp.now()-start).total_seconds(), out.shape)

def get_select_as_multiple():
    'Retrieve stored df with .select_as_multiple method' 
    
    start = pd.Timestamp.now()
    with pd.HDFStore('test.h5') as store:
        out = store.select_as_multiple(['df1', 'df2'])
    print ('MULT: Select done in %5.2f seconds. Out shape is'% 
           (pd.Timestamp.now()-start).total_seconds(), out.shape)

def testing(sh_0, sh_1):
    'Compare time of .select and .select_as_multiple by sh_0, sh_1'
    
    init_and_store(sh_0, sh_1)
    get_select()
    get_select_as_multiple()
    print()
```
Now some tests:
```python
for sh_1 in [4, 8, 24]:
    testing(10**6, sh_1)
```
```python
DataFrame shape is (1000000, 4)
SLCT: Select done in  0.07 seconds. Out shape is (1000000, 4)
MULT: Select done in 25.20 seconds. Out shape is (1000000, 4)

DataFrame shape is (1000000, 8)
SLCT: Select done in  0.10 seconds. Out shape is (1000000, 8)
MULT: Select done in 14.81 seconds. Out shape is (1000000, 8)

DataFrame shape is (1000000, 24)
SLCT: Select done in  0.37 seconds. Out shape is (1000000, 24)
MULT: Select done in  6.45 seconds. Out shape is (1000000, 24)
```
So problems:
1. `.select_as_multiple` is _much_ slower comparatively to plain `.select`.  
- Expected - at least nearly same performance (because in opposite case one may use only `.select` with some `.concat` overhead).
2. Time of `.select_as_multiple` _decreasing_ in case of increasing number of columns (but still far slower than `.select`). 
- Expected is also increasing time of execution, because we increase our data size (up to 6 times).

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Linux
OS-release: 5.0.0-16-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 4.5.0
pip: 19.1.1
setuptools: 41.0.1
Cython: 0.29.7
numpy: 1.16.4
scipy: 1.2.1
pyarrow: None
xarray: 0.12.1
IPython: 7.2.0
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: None
tables: 3.5.1
numexpr: 2.6.9
feather: None
matplotlib: 3.0.3
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
493406982,28430,HDF store not closed on AssertionError,adimascio,open,2019-09-13T16:09:35Z,2020-05-16T20:39:54Z,"#### Code Sample, a copy-pastable example if possible

Code adapted from https://github.com/ToucanToco/peakina/blob/master/peakina/cache.py#L84

```python
        try:
            metadata = pd.read_hdf(filepath)
        except Exception:  # catch all, on purpose
            metadata = pd.DataFrame(columns=['key', 'mtime', 'created_at'])
            metadata.to_hdf(filepath, key, mode='w')

        return metadata
        
```

#### Problem description

In `read_hdf`, if the `store.select()` call throws either a `ValueError`, a `TypeError` or a `KeyError` then the store is closed.

However, if any other exception is thrown (e.g. an `AssertionError` if there are gaps in blocks ref_loc) , the store is not closed and some client code could end up trying to reopen the store and hit an error like: `the file XXX is already opened. Please close it before
reopening in write mode`. 


#### Expected Output

No reentrant exception.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.0-5-amd64
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : fr_FR.UTF-8
LOCALE           : fr_FR.UTF-8

pandas           : 0.25.1
numpy            : 1.17.2
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : 5.1.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : 2.7.0
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.14.1
pytables         : None
s3fs             : 0.3.4
scipy            : None
sqlalchemy       : None
tables           : 3.5.2
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

</details>
"
187143627,14579,"Non-breaking spaces are printed as whitespace, but handled internally as `\xa0`",aechase,open,2016-11-03T17:53:08Z,2020-05-16T21:00:22Z,"If a column name contains a non-breaking space, _pandas_  will print it as normal whitespace, but represent it internally as `\xa0`. So, if you print out the column names, then copy/paste a name into a `.loc[]`, you will get a `KeyError`.

```python
import pandas as pd

df = pd.DataFrame(data={""Space column"": [1, 2, 3, 4, 5],
                        ""Non-breaking\xa0space column"": [1, 2, 3, 4, 5]})
print(df.columns)

# Index(['Non-breaking space column', 'Space column'], dtype='object')

df.loc[:, ""Non-breaking space column""]

# ... KeyError: 'the label [Non-breaking space column] is not in the [columns]' ...
```
For obvious reasons, this error is not helpful for diagnosing the actual problem, because all you can do to check the key is print out the column names again...

Of course, it's not just a problem for column names, but for all strings.
```python
s = pd.Series(data=[""Foo bar"", ""Bar\xa0foo""])
print(s)

# 0    Foo bar
# 1    Bar foo
# dtype: object

print(s.str.contains(""Bar foo""))

# 0    False
# 1    False
# dtype: bool
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Darwin
OS-release: 16.1.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.19.0
nose: 1.3.7
pip: 8.1.2
setuptools: 27.2.0
Cython: 0.25.1
numpy: 1.11.2
scipy: 0.18.1
statsmodels: 0.6.1
xarray: None
IPython: 5.1.0
sphinx: 1.4.8
patsy: 0.4.1
dateutil: 2.5.3
pytz: 2016.7
blosc: None
bottleneck: 1.1.0
tables: 3.3.0
numexpr: 2.6.1
matplotlib: 1.5.3
openpyxl: 2.4.0
xlrd: 1.0.0
xlwt: 1.1.2
xlsxwriter: 0.9.3
lxml: 3.6.4
bs4: 4.5.1
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 1.1.3
pymysql: 0.7.9.None
psycopg2: None
jinja2: 2.8
boto: 2.43.0
pandas_datareader: None</details>
"
291626443,19394,ENH: Include dtypes in DataFrame repr column headers,sullivanrs,open,2018-01-25T16:14:01Z,2020-05-16T21:08:57Z,"In R notebooks and consoles, dtypes are included below column names in plain text and rich renderings of data frames/tibbles (see images below). This is quite helpful in identifying the data type of a column without having to call df.dtypes or df.info(). Is there any appetite for introducing a similar mechanism for the plain text and HTML reprs of Pandas DataFrames?

**R notebook**
![r_notebook](https://user-images.githubusercontent.com/4974928/35398775-a99152f6-01ea-11e8-8988-aab06d7e66d1.PNG)

**R console**
![r_console](https://user-images.githubusercontent.com/4974928/35398777-a9ba8806-01ea-11e8-8513-027483c79b60.PNG)



"
371382037,23220,Option to change pandas.Timestamp format when printing dataframe,teto,open,2018-10-18T06:40:51Z,2020-05-16T21:09:53Z,"

#### Problem description
To use pandas.rolling, I had to convert my float64 nanoseconds timestamps to pandas.Timestamp. 
Timestamps now appear nicer on matplotlib plots but when debugging and plotting `df.head()`, i would like to see the timestamp in nanosecond format rather than `Timestamp.__repr()__` that prints the day/month/year (in my case the same for every row).

Every (stackoverflow) I could find recommand to create a new column with the right format. I would like to avoid that since I find it overkill.
As it's a temporary/printing issue, I think it falls into https://pandas.pydata.org/pandas-docs/stable/options.html#available-options and I would be happy with that.
What do you think ?

#### Expected Output

Version 0.23.4"
426957008,25919,display.precision not honored for float values in an object index,simonjayhawkins,open,2019-03-29T12:11:43Z,2020-05-16T21:11:22Z,"see also #17280, #25917

#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
float_val = 0.55555555
df = pd.DataFrame([float_val, 'foo'], index=[float_val, 'foo'])
pd.set_option('display.precision', 3)
df
```

<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0.55555555</th>
      <td>0.556</td>
    </tr>
    <tr>
      <th>foo</th>
      <td>foo</td>
    </tr>
  </tbody>
</table>
</div>

#### Problem description

the float valued index values are not displayed with a precision of 3. The expected behavior is observed with a `Float64Index` when all index values are floats.

#### Expected Output

<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0.556</th>
      <td>0.556</td>
    </tr>
    <tr>
      <th>foo</th>
      <td>foo</td>
    </tr>
  </tbody>
</table>
</div>

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.2.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.25.0.dev0+332.g96a128eaa
pytest: 4.3.0
pip: 19.0.3
setuptools: 40.6.3
Cython: 0.29.5
numpy: 1.16.2
scipy: 1.2.1
pyarrow: 0.11.1
xarray: 0.11.3
IPython: 7.3.0
sphinx: 1.8.4
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.9
feather: None
matplotlib: 3.0.2
openpyxl: 2.6.0
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.5
lxml.etree: 4.3.1
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.2.18
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: 0.2.0
fastparquet: 0.2.1
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
615155646,34092,"CLN, TYP: Factor out part of info",MarcoGorelli,closed,2020-05-09T11:30:37Z,2020-05-17T09:17:58Z,"Precursor to #31796, which still has a rather large diff. This should help make it easier to read.

On master, `cols` holds the DataFrame's columns. If we were to use this method on a Series, then it would hold an Index containing the Series' name (`Index([data.name])`). Is there a good name for a variable that could store either? At the moment I've called it `ids`"
611086519,33935,TYP: annotations in nanops,jbrockmendel,closed,2020-05-02T02:04:45Z,2020-05-06T20:39:59Z,"tightening types a little bit.

Some of the other functions in this file have inaccurate return types, saving that for a separate pass"
600405347,33568,DOC: Fix heading capitalization in doc/source/whatsnew - part5 (#32550),cleconte987,closed,2020-04-15T15:53:50Z,2020-05-06T21:14:57Z,"-Quite a lot of complicated issues here, need reviewing. Such as 'Groupby' transformed to 'GroupBy', 'I/O' to 'IO', and some others.

- [ ] Modify files v0.18.0.rst, v0.18.1.rst, v0.19.0.rst, v0.19.1.rst, v0.19.2.rst, v0.20.0.rst, v0.20.2.rst, v0.20.3.rst, v0.21.0.rst,  v0.21.1.rst

-File v0.20.0.rst has bad file name as it is actually talking about version v0.20.1.rst"
595372118,33337,DOC/PLT: Add `stacked` in doc and doc example for barh and bar plot,charlesdong1991,closed,2020-04-06T19:32:42Z,2020-05-06T21:31:00Z,"- [x] closes #32759
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
613520757,34033,CLN: address TODOs and FIXMEs,jbrockmendel,closed,2020-05-06T18:24:44Z,2020-05-06T21:31:09Z,
613487975,34032,PERF: use is_tick_offset to check for Tick,jbrockmendel,closed,2020-05-06T17:27:42Z,2020-05-06T21:36:47Z,
613619966,34036,CI: Pin Cython on Numpy Dev,alimcmaster1,closed,2020-05-06T21:19:23Z,2020-05-06T22:04:33Z,"- [x] ref https://github.com/pandas-dev/pandas/issues/34014

Until we fix the above issue with cython 3.0a4

cc @jreback "
612831972,34008,CLN: avoid getattr checks in liboffsets,jbrockmendel,closed,2020-05-05T19:13:30Z,2020-05-07T00:39:07Z,
612980540,34016,CLN: remove normalize_datetime,jbrockmendel,closed,2020-05-06T00:47:22Z,2020-05-07T00:39:29Z,No longer necessary following #34006
612976450,34015,REF: update iNaT imports,jbrockmendel,closed,2020-05-06T00:32:35Z,2020-05-07T00:41:59Z,
612796409,34007,CLN: avoid _typ checks in Period ops,jbrockmendel,closed,2020-05-05T18:11:34Z,2020-05-07T00:42:25Z,
610529861,33908,REF: de-duplicate listlike validation in DTA._validate_foo,jbrockmendel,closed,2020-05-01T03:05:05Z,2020-05-07T01:31:47Z,Implements a helper `_validate_listlike` that is re-used across 4 of the _validate_foo methods.  We can then smooth out the remaining differences by passing the same flags in each of the four usages.
613696902,34041,"CLN: remove normalize_date, but really this time",jbrockmendel,closed,2020-05-07T00:41:29Z,2020-05-07T03:01:41Z,"#34016 removed all the references to it, but I forgot the actual function"
60188886,9609,API: Plotting kwargs consistency,TomAugspurger,open,2015-03-07T03:48:07Z,2020-05-07T03:37:21Z,"Starting this master issue for things I see while going through the plotting kwargs for the `.plot` accessor.
### hist_frame
- [ ] `xlabelsize`, `ylabelsize`: nothing specific to hist. Could be for all (non-pie) plot types
"
123673222,11889,API: different default for `header` kwarg in read_csv vs read_excel,jorisvandenbossche,open,2015-12-23T14:59:11Z,2020-05-07T03:42:18Z,"From https://github.com/pydata/pandas/pull/11874#issuecomment-166915826

The default for `hader` in `read_csv` is `'infer'`, while for `read_excel` this is `0`. In most cases this does not matter I think (as the infer will use 0 as the default). But, in the case of specifying the `names` explicitly, this makes a difference:

For `read_csv`, you need to explicitly pass header=0 to replace header existing in file:

```
In [35]: s = """"""a,b,c
   ....: 1,2,3
   ....: 4,5,6""""""

In [48]: pd.read_csv(StringIO(s), names=list('ABC'))
Out[48]:
   A  B  C
0  a  b  c
1  1  2  3
2  4  5  6

In [49]: pd.read_csv(StringIO(s), header=None, names=list('ABC'))
Out[49]:
   A  B  C
0  a  b  c
1  1  2  3
2  4  5  6

In [51]: pd.read_csv(StringIO(s), header=0, names=list('ABC'))
Out[51]:
   A  B  C
0  1  2  3
1  4  5  6
```

while for `read_excel`, you need to explicitly pass header=None if the file contains no header row:

```
In [31]: pd.read_excel('test_names.xlsx')
Out[31]:
   a  b  c
0  1  2  3
1  4  5  6

In [46]: pd.read_excel('test_names.xlsx', names=['A', 'B', 'C'])     <--- ignoring the data of the first row
Out[46]:
   A  B  C
0  1  2  3
1  4  5  6

In [45]: pd.read_excel('test_names.xlsx', header=False, names=['A', 'B', 'C'])
Out[45]:
   A  B  C
0  1  2  3
1  4  5  6

In [47]: pd.read_excel('test_names.xlsx', header=None, names=['A', 'B', 'C'])
Out[47]:
   A  B  C
0  a  b  c
1  1  2  3
2  4  5  6
```

Would it make sense to change the `read_excel` behaviour to make it more consistent to the `read_csv` one?
"
178551708,14278,Inconsistent use of `_` to separate words in method names,Anaphory,open,2016-09-22T08:36:24Z,2020-05-07T03:45:38Z,"Sometimes, method names contain underscores to separate words (`to_string`) and sometimes not (`tolist`). This catches me of regularly. Particulary annoying are the cases where the same prefix (eg. `to`) sometimes takes an underscore and sometimes not.

Is this going to change for future major versions?

Example: `dir(Index)`
- Without underscore between words, but the compound is well-established: 'argmax', 'argmin', 'argsort', 'nbytes', 'ndim', 'nlevels', 'nunique', 'putmask'
- Without underscore: 'astype', 'fillna', 'hasnans', 'isin', 'itemsize', 'searchsorted', 'sortlevel', 'tolist'
- With underscore: 'drop_duplicates', 'get_duplicates', 'get_indexer', 'get_indexer_for', 'get_indexer_non_unique', 'get_level_values', 'get_loc', 'get_slice_bound', 'get_value', 'get_values', 'groupby', 'has_duplicates', 'holds_integer', 'inferred_type', 'is_all_dates', 'is_boolean', 'is_categorical', 'is_floating', 'is_integer', 'is_lexsorted_for_tuple', 'is_mixed', 'is_monotonic', 'is_monotonic_decreasing', 'is_monotonic_increasing', 'is_numeric', 'is_object', 'is_type_compatible', 'is_unique', 'memory_usage', 'set_names', 'set_value', 'slice_indexer', 'slice_locs', 'sort_values', 'sym_diff', 'symmetric_difference', 'to_datetime', 'to_native_types', 'to_series', 'value_counts'
"
509519490,29105,Should Series.diff() have an axis argument?,vfilimonov,open,2019-10-19T22:55:06Z,2020-05-07T03:52:02Z,"I suggest that `Series.diff()` method should have `axis` argument to align its signature with the signature of `DataFrame.diff()`. This way it'd be easier to write polymorphic functions that could take both series and dataframes.
```python
# Series
x.diff(periods=1)
# DataFrame
df.diff(periods=1, axis=0)
```

Many other `Series` methods already have signatures identical to those of dataframes (and obviously when calling with `axis=1` would raise `ValueError`), e.g.:
```python
# Series
x.shift(periods=1, freq=None, axis=0, fill_value=None)
# DataFrame
df.shift(periods=1, freq=None, axis=0, fill_value=None)

# Series
x.cumsum(axis=None, skipna=True, *args, **kwargs)
# DataFrame
df.cumsum(axis=None, skipna=True, *args, **kwargs)

# Series
x.add(other, level=None, fill_value=None, axis=0)
# DataFrame
df.add(other, axis='columns', level=None, fill_value=None)

# Here axis is not an explicit argument, but properly passed from **kwargs
# Series
x.pct_change(periods=1, fill_method='pad', limit=None, freq=None, **kwargs)
# DataFrame
df.pct_change(periods=1, fill_method='pad', limit=None, freq=None, **kwargs)
```"
512980469,29241,merge vs. join - unify parameter names,stefansimik,open,2019-10-27T15:23:52Z,2020-05-07T03:53:05Z,"#### Problem description

`pandas.DataFrame.merge` function is conceptually simillar like `pandas.DataFrame.join` function.
(first one one merges on specified columns, second merges on index).

These 2 functions use various parameters to do the same thing:
* `join` function has 2 params:  `lsuffix + rsuffix`
* `merge` function has only 1 param: `suffixes`

We should unify names and usage of these parameter(s).
Let's use first way or second way, and deprecate the other way.

_(Personally, I prefer to have only 1 param `suffixes` as it is shorter and there is 1 param for 1 purpose)._
#### Expected Output

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 7
machine          : AMD64
processor        : Intel64 Family 6 Model 79 Stepping 1, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.2
numpy            : 1.16.5
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : 0.14.1
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

</details>
"
526564688,29771,Same parameter name for specifying column names in pd.read_csv and pd.DataFrame,regstrtn,open,2019-11-21T12:29:59Z,2020-05-07T03:53:52Z,"The two most common ways of creating a new dataframe use different parameters for taking a list of column names as input. 
In pd.read_csv, the syntax is: 
`pandas.read_csv(filepath, sep=',', delimiter=None, header='infer', names=None)`

In pd.DataFrame, the syntax is: 
`pandas.DataFrame(data=None, index=None, columns=None)`

And then again, in df.to_csv, we use the name `columns`. 
`df.to_csv(self, path_or_buf=None, sep=', ', na_rep='', float_format=None, columns=None)`

Can they both be changed to `columns`? "
601957814,33608,DOC/CLN: Clean/Fix documentation for Window module,dilex42,closed,2020-04-17T12:58:55Z,2020-05-07T07:59:10Z,"- xref #31661
- xref #28792

- Added '.' before dataframe/series reference to fix missing link problem. Don't know if it's the best approach.
- Changed base template that is used by some functions to return better See Also section. Hope it's ok.


- Also question about Window class. It doesn't has its counterpart from Series/DF modules, its doc containing examples from Rolling part and there're only 4 functions. Is it internal base class for all windows and maybe shoul be excluded from docs or is it used somewhere maybe for creating custom windows?"
594749967,33311,Update citation webpage,ivalaginja,closed,2020-04-06T01:45:50Z,2020-05-07T10:10:11Z,"Follow-up of #32388, addressing #24036 

I will leave it to the pandas team to decide whether to put in there a BibTeX entry with the concept DOI or a specific version, some options of dealing with this are described [in this comment](https://github.com/sherpa/sherpa/pull/634#issuecomment-553668211).

Note how [this comment thread](https://github.com/pandas-dev/pandas/pull/32388#discussion_r386392799) on the previous PR asked to replace the author list by ""The pandas development team"". However, if users go to Zenodo to get the correct BibTeX entry of the version they're actually using, their citation will contain the full author list provided in Zenodo."
613325052,34023,Backport PR #33089 on branch 1.0.x (BUG: Don't cast nullable Boolean to float in groupby),simonjayhawkins,closed,2020-05-06T13:34:58Z,2020-05-07T10:10:26Z,"xref #33089

regression in 1.0.0, xref https://github.com/pandas-dev/pandas/issues/32194#issuecomment-604664466

# NOTE: same code fix, different location

cc @dsaxton "
601424613,33596,Update _core.py with missing parameters,loudlemon,closed,2020-04-16T20:15:12Z,2020-05-07T11:19:22Z,"added missing parameters:
 - ax
 - subplots
 - sharex
 - sharey
 - secondary_y
 - sort_columns
 - stacked

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
609034430,33867,BUG: Max aggregator strange behavour,RavimoShark,closed,2020-04-29T12:55:01Z,2020-05-07T13:28:44Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
dataset_soft_cl = dataset_soft.loc[y_pred_gyr==0].abs()
max_y_g = dataset_soft_cl.groupby('event_id')[y_g_col].max().max(axis=1)
max_y_g_2=dataset_soft.loc[dataset_soft.event_id.isin(dataset_soft_cl.event_id)].abs().groupby('event_id')[y_g_col].max().max(axis=1)

```

#### Problem description

max_y_g is different from max_y_g_2 basically if the max per line is in the first line of the group max_y_g does not see it as opposed to max_y_g_2

#### Expected Output
max_y_g == max_y_g_2
#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-46-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : 0.9.3
psycopg2         : None
jinja2           : None
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy             : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
613723747,34043,PERF: cdef UTC,jbrockmendel,closed,2020-05-07T02:13:31Z,2020-05-07T16:17:25Z,"This won't make a big difference, but it will avoid needing to go to python-space for these lookups."
609696879,33891,pandas interpolate inconsistent results with axis and method ffill,saarahrasheed,closed,2020-04-30T08:12:05Z,2020-05-07T20:00:20Z,"**Problem:**
I am trying to interpolate a dataframe of mxn with na values. The default axis is 0 (index), so if I were to interpolate at axis 0, df should fill missing values (say at index n) for each column with values from index n-1 and n+1 (or n+p, where n+p is the closest index with a valid value). This holds for default linear method but not for ffill method. 

**Code:**
`data = np.array([[1,2,3,4, np.nan, 5], [2,4,6,np.nan, 8, 10], [3, 6, 9, np.nan, np.nan, 30]]).T`
`d = pd.DataFrame(data, columns=['A', 'B', 'C'])`
`d`
     A     B     C
0  1.0   2.0   3.0
1  2.0   4.0   6.0
2  3.0   6.0   9.0
3  4.0   NaN   NaN
4  NaN   8.0   NaN
5  5.0  10.0  30.0

`d.interpolate(method='ffill', axis=1)`   

  A     B     C
0  1.0   2.0   3.0
1  2.0   4.0   6.0
2  3.0   6.0   9.0
3  4.0   6.0   9.0
4  4.0   8.0   9.0
5  5.0  10.0  30.0
`d.interpolate(method='ffill')`

     A     B     C
0  1.0   2.0   3.0
1  2.0   4.0   6.0
2  3.0   6.0   9.0
3  4.0   4.0   4.0
4  NaN   8.0   8.0
5  5.0  10.0  30.0
`d.interpolate()`

     A     B     C
0  1.0   2.0   3.0
1  2.0   4.0   6.0
2  3.0   6.0   9.0
3  4.0   7.0  16.0
4  4.5   8.0  23.0
5  5.0  10.0  30.0"
484631373,28117,Enable pd.options.display.html.table_schema = True By Default,playermanny2,open,2019-08-23T17:17:30Z,2020-05-07T21:42:33Z,"#### Problem description

**This is an enhancement request**

In order to enable a pandas to provide a JSON schema representation of the dataframe utilizing ```application/vnd.dataresource+json```, ```pd.options.display.html.table_schema``` must be set to ```True```.

I'm just wondering if this could be set to True by default? That way front-ends that support it, can ingest it without having to enable it from the import. Not sure on the implications, but wanted to start the convo...


#### Code Sample

```
import pandas as pd
pd.options.display.html.table_schema = True

```
"
75678078,10112,Pandas ExcelWriter set_column fails to format numbers after DataFrame.to_excel used,phaefele,closed,2015-05-12T17:33:21Z,2020-05-07T22:16:26Z,"I have tried the example code found on the xlsxwriter webpage at http://xlsxwriter.readthedocs.org/en/latest/example_pandas_column_formats.html

However it does not format the columns as expected - they simply appear unformatted (no numeric rounding, percentages.)

It seems that calling set_column is being ignored. Is this because perhaps not to_excel() is doing some formatting itself which can't be overwritten?
"
254883920,17426,adding charts to a workbook created with dataframe.to_excel() results in a corrupt workbook,MattWoodhead,open,2017-09-03T16:11:07Z,2020-05-07T22:40:44Z,"#### Code Sample, a copy-pastable example if possible

```python
import io
import pandas as pd
from openpyxl.chart import ScatterChart, Reference, Series


writer = pd.ExcelWriter(""out.xlsx"", engine='openpyxl')

x_data = [""x_data"", 0, 3.75, 7.5, 11.25, 15, 15, 15.5, 16, 16.5, 17, 17,
          22, 27, 32, 37, 37, 71, 105, 139, 173, 173, 178, 183, 188, 193,
          193, 193.5, 194, 194.5, 195, 195, 198.75, 202.5, 206.25, 210]
x_data = [str(i) for i in x_data]
x_data_buffer = io.StringIO(""\n"".join(x_data))

y_data = [""y_data"", -2704.92, -2704.75, -2704.59, -2704.43, -2704.26,
          -5595.72, -5595.69, -5595.67, -5595.65, -5595.63, 843.285,
          843.503, 843.722, 843.94, 844.159, 13.8079, 15.294, 16.78,
          18.266, 19.7521, 9.42089, 9.63942, 9.85795, 10.0765, 10.295,
          -5548.31, -5548.29, -5548.27, -5548.24, -5548.22, -1372.58,
          -1372.42, -1372.26, -1372.09, -1371.93]
y_data = [str(i) for i in y_data]
y_data_buffer = io.StringIO(""\n"".join(y_data))

x_data = pd.read_csv(x_data_buffer)
y_data = pd.read_csv(y_data_buffer)

data = pd.concat([x_data, y_data], axis=1)

# Excel interaction below:
data_rows = len(data.index)

# Send dataframe to excel
data.to_excel(writer, sheet_name=""sheet1"", startcol=0, startrow=3)
ws = writer.sheets[""sheet1""]  # worksheet

# Add Graph
chart_1 = ScatterChart()  # first y axis
chart_1.style = 13
x_values = Reference(ws, min_col=2, min_row=5, max_row=4 + data_rows)
y_values = Reference(ws, min_col=3, min_row=4, max_row=4 + data_rows)
chart_1.series.append(Series(y_values, x_values, title_from_data=True))
chart_1.width = 30
chart_1.height = 15

ws.add_chart(chart_1, ""L6"")  # insert graph in cell L6

# Save workbook
writer.save()  # Using this after adding a chart causes the workbook to be corrupt
writer.close()

#wb = writer.book  # Using the internal pyopenxl save and close methods works correctly
#wb.save(""out.xlsx"")
#wb.close()
```
#### Problem description

When adding charts to an excel workbook created using the DataFrame.to_excel() method (using pyopenxl as the writer) the created workbook is corrupt, and often is unrecoverable by excel.

by accessing the openpyxl save and close methods using writer.book.save instead of writer.save, the workbook can be opened by excel with no issues. Using the openpyxl save method, and the writer.close method in conjucntion with each other also causes a corrupt workbook to be produced.

[related issue on openpyxl dev platform](https://bitbucket.org/openpyxl/openpyxl/issues/886/using-scatter-charts-corrupts-the-excel#comment-39603120)

#### Expected Output

Update the ExcelWriter.save and ExcelWriter.close methods so that they do not produce corrupt workbooks when adding charts using the openpyxl engine.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.1.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: AMD64 Family 21 Model 2 Stepping 0, AuthenticAMD
byteorder: little
LC_ALL: None
LANG: en
LOCALE: None.None

pandas: 0.20.1
pytest: 3.0.7
pip: 9.0.1
setuptools: 27.2.0
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.19.0
xarray: None
IPython: 5.3.0
sphinx: 1.5.6
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.2.2
numexpr: 2.6.2
feather: None
matplotlib: 2.0.2
openpyxl: 2.4.7
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: 0.9.6
lxml: 3.7.3
bs4: 4.6.0
html5lib: 0.999
sqlalchemy: 1.1.9
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
pandas_gbq: None
pandas_datareader: 0.5.0
</details>
"
614317802,34054,"REF: implement ABCTimestamp, ABCTimedelta, ABCTick",jbrockmendel,closed,2020-05-07T20:17:41Z,2020-05-07T22:46:42Z,"Motivation: to make more progress on getting DateOffset into cython, we need to be able to import Timestamp into liboffsets.  ATM that would induce circular dependencies.

This allows us to merge c_timestamp back into timestamps.pyx; the initial motivation for that was to be able to cimport _Timestamp into libconversion.

In a follow-up we can use this pattern to make a faster `is_interval` check (that doesnt have to go through python-space)"
473897551,27634,pandas.ExcelWriter has abstract methods,mhooreman,closed,2019-07-29T07:37:42Z,2020-05-07T23:08:25Z,"#### Code Sample, a copy-pastable example if possible

```python
""""""Lorem ipsum""""""

import pandas as pd


def demo():
    """"""Demo for the false positive""""""
    with pd.ExcelWriter(""demo.xlsx"") as writer:
        print(writer)
```



#### Problem description

The code shown above gives pylint: ``abstract-class-instantiated(E0110): test.py:8:9: demo: Abstract class 'ExcelWriter' with abstract methods instantiated``

#### Expected Output

No pylint issue: no abstract method left

#### Output of ``pd.show_versions()``


<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-54-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.0
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.2.1
setuptools       : 40.8.0
Cython           : 0.29.12
pytest           : 5.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.3.4
html5lib         : None
pymysql          : None
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.6.1
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.1.1
numexpr          : 2.6.9
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.1.8

```
</details>
"
523956701,29668,"parameter ""na_values"" in read_excel() not effective when reading #DIV/0!",scienceunivers,open,2019-11-17T08:58:17Z,2020-05-08T00:41:39Z,"#### Code Sample, a copy-pastable example if possible

```python
naValues = ''
testdf = pd.read_excel('read_excel_bug.xlsx',
                        sheet_name=0,header=0, names=None, index_col=None, 
                        usecols=None, squeeze=False, dtype=None, engine=None, 
                        converters=None, true_values=None, false_values=None,
                       skiprows=None, nrows=None, 
                        na_values=naValues, keep_default_na=False, verbose=False, 
                        parse_dates=False, date_parser=None, thousands=None, comment=None, 
                        skip_footer=0, skipfooter=0, convert_float=True, mangle_dupe_cols=True)
testdf

```
#### Problem description
I set the na_values param to None, and make keep_default_na=False, according to the doc, after execution cells of the “数据” column should be #DIV/0!, but in fact they are all NaN.


 index   |机构	|全称	|社会信用代码	|账号|	数据
-----|-----|---------|----------------|--------|------
0	|机构1|	机构1的全称|	9.144030e+17	|alading	|NaN
1	|NaN	|NaN	|NaN	|NaN	|NaN


#### Expected Output
 index   |机构	|全称	|社会信用代码	|账号|	数据
-----|-----|---------|----------------|--------|------
0	|机构1|	机构1的全称|	9.144030e+17	|alading	|#DIV/0!
1	|NaN	|NaN	|NaN	|NaN	|#DIV/0!

[read_excel_bug.xlsx](https://github.com/pandas-dev/pandas/files/3855400/read_excel_bug.xlsx)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.2
numpy            : 1.14.2
pytz             : 2017.3
dateutil         : 2.6.1
pip              : 19.0.3
setuptools       : 38.4.0
Cython           : 0.27.3
pytest           : 3.3.2
hypothesis       : None
sphinx           : 1.6.6
blosc            : None
feather          : None
xlsxwriter       : 1.0.2
lxml.etree       : 4.1.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 6.2.1
pandas_datareader: None
bs4              : 4.6.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.1.1
matplotlib       : 2.1.2
numexpr          : 2.6.4
odfpy            : None
openpyxl         : 2.4.10
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.0.0
sqlalchemy       : 1.2.1
tables           : 3.4.2
xarray           : None
xlrd             : 1.1.0
xlwt             : 1.3.0
xlsxwriter       : 1.0.2

</details>
"
531746812,29990,Pandas is not able to append dataframe to an existing excel sheet.Although openpyxl can be used for it but openpyxl is also destroying pivot table and all other realted information.So we do not have another way available except openpyxl,vishavgupta97,closed,2019-12-03T06:08:06Z,2020-05-08T00:43:27Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here

```
#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
563710469,31908,pd.read_excel() ignores converter functions over na_filter argument,theSuiGenerisAakash,open,2020-02-12T03:02:22Z,2020-05-08T00:46:10Z,"Please consider the code below.

```python
import pandas as pd
__to_str = lambda x: '' if pd.isnull(x) == True else str(x)

# Let's say that the excel to be read has a column with some empty strings or nulls in them
# by default na_filter is True
df = pd.read_excel('./file1.xlsx', converters = {'Name': __to_str}) # -> Leads to empty strings being read as nan

# when na_filter is False
df = pd.read_excel('./file1.xlsx', converters = {'Name': __to_str}, na_filter=False) # -> Leads to empty strings be read as empty strings (according to __to_str function) 

```
#### Why this behaviour?

I believe that if there's an explicitly provided argument(`converters`), its behaviour should override any other argument that has not been set(has a default value -> `na_filter`), **in case of such conflict or overlap of effect arises**. 

#### Expected output

When one argument is specified which operates at a certain specificity level, the other arguments having default values should give in to the effect caused by the explicitly set one.
For instance, if `converters` is provided with a function to handle cell values of a certain column, then `na_filter` or `keep_default_na` values which are applied by default and have not been passed explicitly.



Please correct me if my understanding is incorrect.

#### Output of `pd.show_versions()`
<details>
INSTALLED VERSIONS
------------------

commit           : None

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0
Cython           : 0.29.14
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : 2.7.7 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.13
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.7
numba            : None
</details>
"
579107613,32617,improve read_excel performance,Dookie78,closed,2020-03-11T09:04:07Z,2020-05-08T00:47:33Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here

```
#### Problem description
I have a spreadsheet of 600,000 rows and the read_excel function takes around 10mins to process. 

Is there any way to improve the performance?

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
453725033,26723,DOC/CI: Doc build is failing with sphinx 2.1,datapythonista,closed,2019-06-07T23:15:03Z,2020-05-08T03:19:05Z,"The doc build is failing with:
```
Recursion error:
maximum recursion depth exceeded while calling a Python object

This can happen with very large or deeply nested source files.  You can carefully increase the default Python recursion limit of 1000 in conf.py with e.g.:
    import sys; sys.setrecursionlimit(1500)
```

I tried increasing the limit from 1,000 to 20,000 but still the same error, so I guess the problem is something else.

I think this is the first job that failed: https://travis-ci.org/pandas-dev/pandas/jobs/542714631 (PR #26697)"
611364038,33952,BUG: adding a value not in the Categories does not raise a ValueError on a Series when adding the value to a new index,arnaudlegout,closed,2020-05-03T08:47:38Z,2020-05-08T10:51:41Z,"- [x ] I have checked that this issue has not already been reported.

- [ x] I have confirmed this bug exists on the latest version of pandas.

- [  ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
s = pd.Series(['a' , 'b'], dtype='category')
>>>
0    a
1    b
dtype: category
Categories (2, object): [a, b]

# this is expected, cannot assign a value not in the category
s.loc[1] = 'c'
>>>
ValueError: Cannot setitem on a Categorical with a new category, set the categories first

# this is NOT expected. not only it assigns a value not the in the category, but it also implicitly
# change the dtype
s.loc[2] = 'c'
>>>
0    a
1    b
2    c
dtype: object

```

#### Problem description

Assigning a value not in the category to an index already in the Series show a different behavior than when the index is not already in the Series. 

This looks like an inconsistent behavior that is hard to explain. 

The expected behavior is that assigning a value not in a category will always raise a ValueError Exception whether the assignement is in an existing index or in a new index. 

#### Expected Output

#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : 0.29.15
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: 0.8.1
bs4              : None
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
533593036,30092,Can't use named aggregation with resample. ,alexjacobson95,closed,2019-12-05T20:53:33Z,2020-05-08T11:03:34Z,"#### Code Sample, a copy-pastable example if possible
```python
import pandas as pd
import numpy as np
from functools import partial

df = pd.DataFrame(
    np.random.randn(1000, 3), 
    index=pd.date_range('1/1/2012', freq='S', periods=1000), 
    columns=['A', 'B', 'C']
)
dfg = df.resample('3T').agg(
    {'A': [
        partial(np.quantile, q=.9999), 
        partial(np.quantile, q=.90),
    ]}
)
```
#### Problem description
Resample, unlike groupby, has no ability to do named aggregation. This means if you use a function twice on the same column you get an error saying:

``pandas.core.base.SpecificationError: Function names must be unique, found multiple named quantile``. 

It would be very useful to make the resample agg interface similar to the groupby agg interface. An example of the named aggregation is here: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#named-aggregation

#### Expected Output

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_US.UTF-8
pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 18.1
setuptools       : 40.6.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
590823913,33167,CLN: Replace DataFrame() with empty_frame in tests,jason3804,closed,2020-03-31T06:09:22Z,2020-05-08T15:58:33Z,"- [ ] closes #33161
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
545214673,30670,Fix errors='ignore' being ignored in astype #30324,naomi172839,closed,2020-01-04T00:19:24Z,2020-05-08T16:01:38Z,"- [x] closes #30324 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
574671399,32410,BUG: fixes plotting with nullable integers (#32073),jeandersonbc,closed,2020-03-03T13:33:42Z,2020-05-08T16:07:39Z,"- [x] closes #32073
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
545663837,30728,Fix read_json category dtype,taoufik07,closed,2020-01-06T10:49:08Z,2020-05-08T16:44:35Z,"- [X] closes #21892
- [ ] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] Add support for ""category"" dtype in read_json
"
496867501,28573,BUG: Preserve subclassing with groupby operations,alkasm,closed,2019-09-23T02:20:01Z,2020-05-08T17:15:10Z,"Preserve subclassing with groupby operations

- [x] closes #28330
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
614328017,34055,Standardize datetimelike casting behavior where/setitem/searchsorted/comparison,jbrockmendel,closed,2020-05-07T20:36:58Z,2020-05-08T17:16:06Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Addresses #30699 for eq/ne, but not for inequalities."
613662085,34039,CLN: spelling fixes in docstrings,akosfurton,closed,2020-05-06T22:55:07Z,2020-05-08T17:36:22Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
405786543,25076,ENH: rolling apply should behave like groupby and allow us to return a dict,KIC,open,2019-02-01T16:55:08Z,2020-05-08T19:39:17Z,"#### Problem description

I really think that `rolling(x).apply(f)` should behave like groupby as it is essentially a group by function just where the groups are overlapping. Therefore one should be allowed to return a dict as a result. i.e. something like this:

```
df = pd.DataFrame({'num_legs': [2, 4, 8, 0],
                    'num_wings': [2, 0, 0, 0],
                    'num_specimen_seen': [10, 2, 1, 8]},
                   index=['falcon', 'dog', 'spider', 'fish'])

df[""num_legs""].rolling(2).apply(lambda x: {""first"": x[0], ""last"": x[-1]}, raw=True)
```

#### Expected Output
| index | | num_legs |
| ------ |-| ---------- |
|dog	|first | 2|
||         last| 4|
|spider | first| 4|
||           last|  8|
|fish|	first| 8|
||        last |0|
"
533202684,30078,EWM functions fail with superclasses of pd.DataFrame,rusiano,closed,2019-12-05T08:39:11Z,2020-05-08T20:07:32Z,"I've built a class that is a superclass of pd.DataFrame and I am trying to apply an exponential weighted mean along the columns, but I get a TypeError saying: ```TypeError: __init__() got an unexpected keyword argument 'index'```

Here is a minimal reproducible example.
```
import pandas as pd
import numpy as np

class Foo(pd.DataFrame):
    
    def __init__(self, df):
        super().__init__(df)

f = Foo(pd.DataFrame(np.random.randn(10,10)))

f.ewm(alpha=1/7).mean()
```

The full exception goes like this:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-39-223b6d63e220> in <module>
      6 f = Foo(pd.DataFrame(np.random.randn(10,10)))
      7 
----> 8 f.ewm(alpha=1/7).mean()

~\Anaconda3\lib\site-packages\pandas\core\window.py in mean(self, *args, **kwargs)
   2323         """"""
   2324         nv.validate_window_func('mean', args, kwargs)
-> 2325         return self._apply('ewma', **kwargs)
   2326 
   2327     @Substitution(name='ewm')

~\Anaconda3\lib\site-packages\pandas\core\window.py in _apply(self, func, **kwargs)
   2309             results.append(np.apply_along_axis(func, self.axis, values))
   2310 
-> 2311         return self._wrap_results(results, blocks, obj)
   2312 
   2313     @Substitution(name='ewm')

~\Anaconda3\lib\site-packages\pandas\core\window.py in _wrap_results(self, results, blocks, obj)
    262         for result, block in zip(results, blocks):
    263 
--> 264             result = self._wrap_result(result, block=block, obj=obj)
    265             if result.ndim == 1:
    266                 return result

~\Anaconda3\lib\site-packages\pandas\core\window.py in _wrap_result(self, result, block, obj)
    242                 return Series(result, index, name=obj.name)
    243 
--> 244             return type(obj)(result, index=index, columns=block.columns)
    245         return result
    246 

TypeError: __init__() got an unexpected keyword argument 'index'
```

#### Output of ``pd.show_versions()``
<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.5.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.2
pytest: None
pip: 19.3.1
setuptools: 41.4.0
Cython: None
numpy: 1.15.4
scipy: 1.3.1
pyarrow: None
xarray: None
IPython: 7.8.0
sphinx: 2.2.0
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2019.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.1.1
openpyxl: None
xlrd: 1.2.0
xlwt: None
xlsxwriter: None
lxml.etree: 4.4.1
bs4: 4.8.1
html5lib: 1.0.1
sqlalchemy: 1.3.5
pymysql: 0.9.3
psycopg2: None
jinja2: 2.10.3
s3fs: 0.1.6
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>


Any ideas?"
551493249,31107,Non-fixed/variable size moving windows in .rolling and .ewm,benjamin-dirtydirty-be,closed,2020-01-17T15:54:15Z,2020-05-08T20:13:53Z,"Hi everyone.

I'm trying to code adaptive lookback indicators in Python/pandas.

```python
import pandas as pd
import numpy as np

def ALB(src, period, TrendParam):
  change = src.diff(period).abs()
  volatility = src.diff().abs().rolling(window=period).sum()
  ER = change / volatility
  Alpha = 2.0 / (period + 1)
  VER = np.power((ER - (2 * ER - 1) / 2.0 * (1 - TrendParam) + 0.5), 2)
  VAlpha = Alpha * VER
  VLength = round((2.0 / VAlpha) - 1)
  return VLength

def percentrank(src, period):
  count = src.rolling(window=period, min_periods=1).apply(lambda x: np.count_nonzero(np.where(x[-1] > x[:-1])), raw=True)
  return 100 * (count / period)

time = pd.date_range(start='2020-01-17', periods=1000, freq='min', name='time')
price = pd.Index(['price'])
df = pd.DataFrame(np.random.rand(1000, 1), time, price)
df['ALB'] = ALB(src=df['price'], period=60, TrendParam=1).fillna(0).astype(int)
df['PCTRNK'] = percentrank(df['price'], period=df['ALB'])
print(df)
```
The code above results in a ""ValueError: window must be an integer""

```python
Traceback (most recent call last):
  File ""<input>"", line 22, in <module>
  File ""<input>"", line 15, in percentrank
  File ""C:\Users\dd\PycharmProjects\rolling\venv\lib\site-packages\pandas\core\generic.py"", line 10769, in rolling
    closed=closed,
  File ""C:\Users\dd\PycharmProjects\rolling\venv\lib\site-packages\pandas\core\window.py"", line 2849, in rolling
    return Rolling(obj, **kwds)
  File ""C:\Users\dd\PycharmProjects\rolling\venv\lib\site-packages\pandas\core\window.py"", line 93, in __init__
    self.validate()
  File ""C:\Users\dd\PycharmProjects\rolling\venv\lib\site-packages\pandas\core\window.py"", line 1738, in validate
    raise ValueError(""window must be an integer"")
ValueError: window must be an integer
```

When using a fixed size window of 60
```python
df['PCTRNK'] = percentrank(df['price'], period=60)
```
I get the following output:

```python
                        price  ALB     PCTRNK
time                                         
2020-01-17 00:00:00  0.728757    0   0.000000
2020-01-17 00:01:00  0.532295    0   0.000000
2020-01-17 00:02:00  0.899849    0   1.666667
2020-01-17 00:03:00  0.742664    0   1.666667
2020-01-17 00:04:00  0.334157    0   0.000000
                       ...  ...        ...
2020-01-17 16:35:00  0.781244  226  75.000000
2020-01-17 16:36:00  0.190402  240  16.666667
2020-01-17 16:37:00  0.505205  220  50.000000
2020-01-17 16:38:00  0.791318  233  80.000000
2020-01-17 16:39:00  0.241293  215  18.333333
```
I would like to use ALB as _window_ (and _min_periods_) input for the .rolling part of the percentrank function, row by row.

I think it would make sense if we were able to use a non-fixed size moving window in .rolling and also in .ewm.
So ideally the parameters _window_ and _min_periods_ (in .rolling) could also accept Series.

This has already been mentioned in issue #26920. but the issue has been closed by @TomAugspurger, stating it looks like a duplicate of #23002.

I guess there is a (inefficient, naive) way of doing it, but I'm puzzled how to make it work.
A 'workaround' is mentioned [here](https://stackoverflow.com/questions/57064501/pandas-calculate-rolling-cumulative-product-with-variable-window-size) I think, but I'm still struggling.

Maybe someone can help?


"
204678405,15284,BUG: pd.read_json with numpy=True coerces columns to int if the first column is an int,Michael-J-Ward,closed,2017-02-01T19:20:03Z,2020-05-08T20:43:50Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

print(""pandas version"", pd.__version__)

df = pd.DataFrame({'a': [0,0,0], 'b': [1.9995, 2.0005, 3.9995]})
print(df.dtypes)
df_json = df.to_json()

print(""note that the `b` column is now a `int`"")
df_from_json = pd.read_json(df_json, numpy=True)
print(df_from_json)
print(df_from_json.dtypes)

# and using dtype='float64' still rounds
df_from_json = pd.read_json(df_json, numpy=True, dtype='float64')
print(df_from_json)
print(df_from_json.dtypes)

# however, if the first column is a float, then the value is maintained
df2_json = df[['b', 'a']].to_json()
df2_from_json = pd.read_json(df2_json, numpy=True)
print(df2_from_json)
print(df2_from_json.dtypes)

```
#### Problem description

Setting `numpy=True` coerces column dtypes to `int64` if the ""left / first"" column is an int. This causes unexpected rounding errors and the behavior is not consistent: if the first column is a float then subsequent columns are not coerced to float.

#### Expected Output
```
   a       b
0  0  1.9995
1  0  2.0005
2  0  3.9995
```
#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-59-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.19.2
nose: 1.3.7
pip: 9.0.1
setuptools: 27.2.0
Cython: 0.25.2
numpy: 1.11.3
scipy: 0.18.1
statsmodels: 0.6.1
xarray: None
IPython: 5.1.0
sphinx: 1.5.1
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: 1.2.0
tables: 3.3.0
numexpr: 2.6.1
matplotlib: 2.0.0
openpyxl: 2.4.1
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: 0.9.6
lxml: 3.7.2
bs4: 4.5.3
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 1.1.5
pymysql: None
psycopg2: None
jinja2: 2.9.4
boto: 2.45.0
pandas_datareader: None

</details>
"
460024468,27022,Unexpected ValueError when using a json reader to read file from disk using chunksize,vlbrown,closed,2019-06-24T18:06:31Z,2020-05-08T21:05:05Z,"Sent to pandas-dev@python.org on June 23 (vlb@c....com); posted here as recommended by Marc Garcia.

#### Code Sample, a copy-pastable example if possible

```python
path = 'file://localhost/Users/vlb/Learn/DSC_Intro/'
filename = path + 'yelp_dataset/review_test.json'

# read the entire file -- this works
reviews = pd.read_json(filename, lines=True)
reviews.info()

# create a reader to read in chunks -- this part seems to work
review_reader = pd.read_json(StringIO(filename), lines=True, chunksize=1)
type(review_reader)

# But trying to read from the reader throws an error
# ValueError: Unexpected character found when decoding 'false'
for chunk in review_reader:
    print(chunk)

```

#### Data Samples

Either or both of the following records can be used

```python
{""review_id"":""rEITo90tpyKmEfNDp3Ou3A"",""user_id"":""6Fz_nus_OG4gar721OKgZA"",""business_id"":""6lj2BJ4tJeu7db5asGHQ4w"",""stars"":5.0,""useful"":0,""funny"":0,""cool"":0,""text"":""We've been a huge Slim's fan since they opened one up in Texas about two years ago when we used to live there. This place never disappoints. They even have great salads and grilled chicken. Plus they have fresh brewed sweet tea, it's the best!"",""date"":""2017-05-26 01:23:19""}
```

```python
{""review_id"":""Amo5gZBvCuPc_tZNpHwtsA"",""user_id"":""DzZ7piLBF-WsJxqosfJgtA"",""business_id"":""qx6WhZ42eDKmBchZDax4dQ"",""stars"":5.0,""useful"":1,""funny"":0,""cool"":0,""text"":""Our family LOVES the food here. Quick, friendly, delicious, and a great restaurant to take kids to. 5 stars!"",""date"":""2017-03-27 01:14:37""}
```

#### Problem description


#### Problem description

I am working a tutorial that uses a  [JSON data file from Yelp](https://www.yelp.com/dataset/documentation/main). The file is huge, so it needs to be read in chunks.

I get an unexpected error: ValueError: Unexpected character found when decoding 'false'

For testing purposes, I have reduced the dataset to a much smaller file with only 3 lines. I can reproduce the error with that file as well as with a file containing only one (any one) of the three lines. 

Note that if I simply read in the entire (test) data set in one go, that works. It's only when I create a reader and try to review the chunks that I get the error.

#### Expected Output

No errors. A chunk should print.

If there is an error, it should be less opaque than ""Unexpected character found when decoding 'false'"".

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Darwin
OS-release: 15.6.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 4.3.1
pip: 19.0.3
setuptools: 40.8.0
Cython: 0.29.6
numpy: 1.16.2
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 7.4.0
sphinx: 1.8.5
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: 1.2.1
tables: 3.5.1
numexpr: 2.6.9
feather: None
matplotlib: 3.0.3
openpyxl: 2.6.1
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.5
lxml.etree: 4.3.2
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.3.1
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.7.0
gcsfs: None
</details>
"
507791132,29025,Incorrect json round-trip with orient='table' when dataframe contains duplicate index values,afoda,open,2019-10-16T11:30:18Z,2020-05-08T21:18:22Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
df = pd.DataFrame({""col1"": [1, 2]}, index=[0, 0])
print(pd.read_json(df.to_json(orient='table'), orient='table'))
print(pd.read_json(df.to_json(orient='split'), orient='split'))
```
Output:
```python
   index  col1
0      0     1
1      0     2
   col1
0     1
0     2
```

#### Problem description

I would expect the json roundtrip to output a dataframe that is identical to the input, regardless of the value of the orient parameter.

#### Expected Output

```python
   col1
0     1
0     2
   col1
0     1
0     2
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-50-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.16.4
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : None
pytest           : 5.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : 0.9.3
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.7.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : 1.3.5
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
572674697,32326,"Unexpected behaviour of df.to_json(compression=""gzip"")",MartinThoma,open,2020-02-28T10:44:27Z,2020-05-08T21:26:01Z,"#### Code Sample

```python
import pandas as pd

df = pd.DataFrame({""a"": [1, 2, 3, 4], ""b"": [""A"", ""B"", ""C"", ""D""],})
json_df = df.to_json(lines=True, orient=""records"", compression=""gzip"")
print(json_df)
```
#### Problem description

The behavior of the ""compression"" argument is unexpected. It doesn't have any effect. Although [it is documented](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_json.html) that it is only applied if there is a filename argument, there should at very least be a warning if compression is set, but no filename is given.

Also: Why doesn't it have an effect? Couldn't we output a bytes object?"
545096804,30652,ENH: .equals for Extension Arrays,dwhu,closed,2020-01-03T18:01:16Z,2020-05-09T07:57:18Z,"- [x] closes #27081
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

--- Jan 3 10:00 PST

Hey team,

Looking for a bit of feedback on this API design and appropriate testing for this PR before I get too deep into it. Could you provide some guidance regarding how to appropriately handle updating this test? https://github.com/pandas-dev/pandas/blob/6f03e76f9d47ecfcfdd44641de6df1fc7dd57a01/pandas/tests/extension/base/ops.py#L162 It appears to contradict the issue #27081.

Appreciate the help,
Dave
"
461644999,27081,API: .equals for Extension Arrays,jreback,closed,2019-06-27T17:04:14Z,2020-05-09T07:57:33Z,"We don't have a good method of testing for equality between EA's

```
In [19]: from pandas.tests.extension.decimal.array import DecimalArray                                                                                                                                                                                           

In [20]: from decimal import Decimal                                                                                                                                                                                                                             

In [21]: x = DecimalArray([Decimal('1'),Decimal('Nan')])                                                                                                                                                                                                         

In [22]: x == x                                                                                                                                                                                                                                                  
Out[22]: array([ True, False])

In [23]: x = pd.Series([1,np.nan], dtype='Int64').array                                                                                                                                                                                                          

In [24]: x == x                                                                                                                                                                                                                                                  
Out[24]: array([ True, False])

```

These *happen* to work with Series now because the null checks are handled at a higher level.
```
In [26]: x.equals(x)                                                                                                                                                                                                                                             
Out[26]: True

In [27]: x = pd.Series(DecimalArray([Decimal('1'),Decimal('Nan')]))                                                                                                                                                                                              

In [28]: x.equals(x)                                                                                                                                                                                                                                             
Out[28]: True
```
we could provide a default implementation that should just work in the presence of NA.

```
def equals(self, other):
    return ((self == other) | (self.isna() == other.isna())).all()
```

actually we should *also* implement a default ``__eq__`` "
582607000,32759,keyword stacked for bar plots not documented anywhere.,joooeey,closed,2020-03-16T21:22:55Z,2020-05-09T09:13:46Z,"#### Code Sample, a copy-pastable example if possible

```python
df.plot.bar(stacked=True)
df.plot(kind='bar', stacked=True)
```
#### Problem description

There is a keyword argument to create a stacked bar graph. This kwarg is not documented anywhere in the Pandas documentation.

#### Expected Output

The documentations for the methods `df.plot.bar` and `df.plot` should include a description of the kwd `stacked`

#### Version

Pandas 1.0.1
"
589021519,33064,ENH/PERF: use mask in factorize for nullable dtypes,jorisvandenbossche,closed,2020-03-27T10:06:19Z,2020-05-09T11:41:19Z,"xref https://github.com/pandas-dev/pandas/issues/30037

This adds the option to use a mask in the `HashTable.factorize` (implementation itself is in `HashTable._unique`).

That allows eg IntegerArray to use its mask and avoid the need to convert to object dtype or float dtype (which also avoids a copy of the data), and gives a nice speed-up (so checking for na_value or nan, only checking the mask):

Small example (for easier testing purposes I just added `factorize2` that uses the mask, vs existing `factorize`, but that is not meant to stay of course):

```python
In [1]: a = np.random.randint(0, 10000, 100000) 
   ...: mask = np.zeros(len(a), dtype=bool)  
   ...: mask[np.random.randint(0, len(a), 1000)] = True 
   ...: arr = pd.arrays.IntegerArray(a, mask)   

In [2]: arr.factorize()
Out[2]: 
(array([   0,    1,    2, ..., 3726, 2673, 5358]), <IntegerArray>
 [2532, 8355, 1885, 2253, 8517, 5615, 3146,  386, 9183, 6497,
  ...
   794, 8600,  823, 1541, 4373, 1205, 9605, 4576,  443, 2070]
 Length: 10000, dtype: Int64)

# using mask gives the same result
In [3]: arr.factorize2()  
Out[3]: 
(array([   0,    1,    2, ..., 3726, 2673, 5358]), <IntegerArray>
 [2532, 8355, 1885, 2253, 8517, 5615, 3146,  386, 9183, 6497,
  ...
   794, 8600,  823, 1541, 4373, 1205, 9605, 4576,  443, 2070]
 Length: 10000, dtype: Int64)

# on master we actually convert to object dtype with NaNs, which is very slow
In [4]: %timeit arr.factorize() 
12.3 ms ± 849 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

# on this PR, for comparision, I changed this to convert to float dtype with NaNs
# (however this is not robust for large ints)
In [4]: %timeit arr.factorize()
2.83 ms ± 51.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

# on this PR using the mask is faster than both approaches above
In [5]: %timeit arr.factorize2() 
771 µs ± 13.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

Since this is adding an extra if branch to `factorize`, this can impact existing cases that don't use the mask, but I didn't see any noticeable effect (with few times switching between branches and rebuilding):

```python
In [1]: a = np.random.randint(0, 10000, 100000) 
   ...: mask = np.zeros(len(a), dtype=bool)  
   ...: mask[np.random.randint(0, len(a), 1000)] = True 
   ...:  
   ...: a2 = a.copy().astype(float) 
   ...: a2[mask] = np.nan 

# factorize on an integer array
In [4]: %timeit pd.factorize(a) 
769 µs ± 31.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  <- PR
745 µs ± 39.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) <- master
779 µs ± 60.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  <- PR
759 µs ± 50 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  <- master

# factorize on a float array with nans
In [5]: %timeit pd.factorize(a2) 
2.12 ms ± 18.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  <- PR
2.2 ms ± 116 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) <- master
2.2 ms ± 102 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  <- PR
2.13 ms ± 47.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  <- master
```

So for the int array there seems to be a small slowdown (as expected), although it is still within the +/- bounds. And I would say it is also small enough (around 2-3%) to be OK with this slowdown given the benefits of being able to use the mask for other cases.
"
462328114,27133,Add Index.__array_ufunc__,TomAugspurger,closed,2019-06-29T18:31:17Z,2020-05-09T14:17:58Z,"We can probably move `Series.__array_ufunc__` to `core/base.py` (once it's in)

The primary difference is that `ufunc(Index, Series)` should defer to `Series` by returning NotImplemented. This may be as easy as excluding `Series` from `Index._HANDLED_TYPES` and including `Index` in `Series._HANDLED_TYPES`."
540472035,30353,Resampler sometimes adds extra bin with NaN,vapoto,closed,2019-12-19T18:10:33Z,2020-05-09T19:23:30Z,"#### Code Sample

```python
import pandas as pd
idx = pd.timedelta_range(start='8H', freq='10S', periods=5040)
s = pd.Series(range(len(idx)), index=idx)
s.resample('3H').min()
```
#### Observed Output
```
08:00:00       0.0
11:00:00    1080.0
14:00:00    2160.0
17:00:00    3240.0
20:00:00    4320.0
23:00:00       NaN
Freq: 3H, dtype: float64
```
#### Expected Output
```
08:00:00       0.0
11:00:00    1080.0
14:00:00    2160.0
17:00:00    3240.0
20:00:00    4320.0
Freq: 3H, dtype: float64
```
#### Problem description
Downsampling and then applying (any) function to the Resampler object seems to add an extra bin in some cases. It is unclear when this exactly occurs, but these two conditions seem necessary (but not sufficient) to reproduce the bug:
- the resampling period does not exactly fit in the original index
- the original index starts with a non-zero Timedelta

This guess is based on the fact that changing `'8H'` to `'0H'` in the code above returns an expected output. Equally so does changing the resampling period to `'2H'`.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.0.0-37-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.2.post20191203
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.10.2
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
614851140,34072,"CLN: update imports, avoid hasattr checks",jbrockmendel,closed,2020-05-08T16:36:38Z,2020-05-09T19:29:58Z,
615028002,34084,CLN: avoid _typ checks in _libs,jbrockmendel,closed,2020-05-08T22:58:06Z,2020-05-09T19:30:30Z,Sits on top of #34072
613696095,34040,"PERF: use isinstance(obj, Foo) instead of ABCFoo",jbrockmendel,closed,2020-05-07T00:38:35Z,2020-05-09T19:31:07Z,"per discussion in #27353, isinstance checks with the ABCFoo are about 4x as expensive as the Foo checks.  Individually they are still very small, but they add up."
615148851,34090,Add regression test for nested meta path in json_normalize,LTe,closed,2020-05-09T10:53:05Z,2020-05-09T19:50:51Z,"We want to add regression test for #27220 and make sure that this kind of issues will not be present in the future.

Thanks to @yanglinlee [code](https://github.com/pandas-dev/pandas/pull/27667) I created test that works with current `master` branch.

- [x] closes #27220
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
602401968,33623,"BUG: Create empty dataframe with string dtype fails ""data type not understood""",selik,closed,2020-04-18T07:45:49Z,2020-05-09T19:56:15Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
In [1]: import pandas as pd

In [2]: pd.DataFrame({'a': ['b']}, dtype='string')
Out[2]:
   a
0  b

In [3]: pd.DataFrame(columns=['a'], dtype='string')
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-3-2e577cc30885> in <module>
----> 1 pd.DataFrame(columns=['a'], dtype='string')

~/lib/miniconda/envs/xport/lib/python3.8/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy)
    433             )
    434         elif isinstance(data, dict):
--> 435             mgr = init_dict(data, index, columns, dtype=dtype)
    436         elif isinstance(data, ma.MaskedArray):
    437             import numpy.ma.mrecords as mrecords

~/lib/miniconda/envs/xport/lib/python3.8/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype)
    232         # no obvious ""empty"" int column
    233         if missing.any() and not is_integer_dtype(dtype):
--> 234             if dtype is None or np.issubdtype(dtype, np.flexible):
    235                 # GH#1783
    236                 nan_dtype = object

~/lib/miniconda/envs/xport/lib/python3.8/site-packages/numpy/core/numerictypes.py in issubdtype(arg1, arg2)
    391     """"""
    392     if not issubclass_(arg1, generic):
--> 393         arg1 = dtype(arg1).type
    394     if not issubclass_(arg2, generic):
    395         arg2_orig = arg2

TypeError: data type not understood

In [4]: pd.DataFrame(columns=['a'], dtype='float')
Out[4]:
Empty DataFrame
Columns: [a]
Index: []
```

#### Problem description

When trying to create an empty dataframe, one can specify some dtypes, like `'float'`, but `dtype='string'` raises `TypeError: data type not understood`.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.4.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : 3.0.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
481720431,27953,DataFrame constructor fails with extension dtype and columns,TomAugspurger,closed,2019-08-16T17:59:09Z,2020-05-09T19:56:15Z,"I *think* this should be supported.

```python
In [8]: pd.DataFrame(columns=['a'], dtype=pd.CategoricalDtype())
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-8-a44ecacab92a> in <module>
----> 1 pd.DataFrame(columns=['a'], dtype=pd.CategoricalDtype())

~/sandbox/pandas/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy)
    405             )
    406         elif isinstance(data, dict):
--> 407             mgr = init_dict(data, index, columns, dtype=dtype)
    408         elif isinstance(data, ma.MaskedArray):
    409             import numpy.ma.mrecords as mrecords

~/sandbox/pandas/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype)
    224         # no obvious ""empty"" int column
    225         if missing.any() and not is_integer_dtype(dtype):
--> 226             if dtype is None or np.issubdtype(dtype, np.flexible):
    227                 # GH#1783
    228                 nan_dtype = object

~/Envs/pandas-dev/lib/python3.7/site-packages/numpy/core/numerictypes.py in issubdtype(arg1, arg2)
    391     """"""
    392     if not issubclass_(arg1, generic):
--> 393         arg1 = dtype(arg1).type
    394     if not issubclass_(arg2, generic):
    395         arg2_orig = arg2

TypeError: data type not understood
```

**Expected Output**

```python
In [10]: pd.DataFrame(columns=['a']).astype('category')
Out[10]:
Empty DataFrame
Columns: [a]
Index: []
```"
602699355,33651,BUG: Create empty dataframe with string dtype fails,kotamatsuoka,closed,2020-04-19T12:10:29Z,2020-05-09T19:56:19Z,"- [ ] closes #33623, closes #27953
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
587308526,32988,REGR: `replace` casts columns to `object`,gituser768,closed,2020-03-24T22:06:12Z,2020-05-09T20:07:43Z,"#### Calling `df.replace` casts columns to `object`

```python
pd.DataFrame(np.eye(2)).replace(to_replace=[None, -np.inf, np.inf], value=pd.NA).dtypes
# 0    object
# 1     object
# dtype: object
```
#### Problem description
I'd expect that the dtypes of the columns remain the same after replacing with pd.NA (especially considering no values are replaced in the above call). We do not get this issue if `to_replace` is any subset of `[None, -np.inf, np.inf]`. We get the same issue if `value` is instead `np.nan`.

#### Expected Output
```python
# 0    float64
# 1     float64
# dtype: object
```
#### Output of ``pd.show_versions()``

<details>
Installed with `pip install -e .` on current master. Was also able to reproduce on `1.0.1`.

INSTALLED VERSIONS
------------------
commit           : bed9103e52ae412774c22b8e198f7b66708981c7
python           : 3.7.0.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.26.0.dev0+2701.gbed9103e5
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.1.post20200323
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
614954081,34079,CLN: de-duplicate ops code,jbrockmendel,closed,2020-05-08T19:57:48Z,2020-05-09T20:12:10Z,
551716117,31117,REF: IntervalIndex.get_value partial slicing,jbrockmendel,closed,2020-01-18T03:46:12Z,2020-05-09T20:22:04Z,"In trying to refactor IntervalIndex.get_loc/get_value to more closely follow the other implementations, I stumbled on a behavior in a test case that surprised me:

```
idx = IntervalIndex.from_tuples([(1, 5), (3, 7)])
s = Series(range(len(idx)), index=idx)
key = [4]

>>> idx.get_loc(key)  # <-- behaves like all the other get_locs in requiring scalar
KeyError: [4]

>>> idx.get_value(s, key)   # <-- surprising
(1, 5]    0
(3, 7]    1
dtype: int64
```

All of the other FooIndex.get_value methods raise InvalidIndexError on non-scalar.

AFAICT this is behaving sort of like a slicing operation, since 4 is in the interior of both of these intervals.  @jschendel is that correct?  If so, we should try to refactor IntervalIndex.get_value to explicitly dispatch for this case, kind of like DatetimeIndex._get_string_slice"
610641931,33911,API: make min/max on empty datetime df consistent with datetime serie…,CloseChoice,closed,2020-05-01T08:41:32Z,2020-05-09T20:31:32Z,"…s (#33704)

- [x] closes #33704
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Fixes the following issue:
```python
import pandas as pd
df = pd.DataFrame(dict(x=pd.to_datetime([])))
df.max()
```
throws a `ValueError` but 
```python
pd.Series(pd.to_datetime([])).max()
```
results in `NaT`.
With this change, calling an DataFrame on en empty `pd.to_datetime([])`, results in
```
In[1]: df = pd.DataFrame(dict(x=pd.to_datetime([])))
In[2]: df.max()
Out[5]: 
x   NaT
dtype: datetime64[ns]
```
"
598524527,33498,BUG: Fix a bug in 'timedelta_range' that produced an extra point on a edge case (fix #30353),hasB4K,closed,2020-04-12T16:44:54Z,2020-05-09T20:41:27Z,"The issue from #30353 came actually from `timedelta_range`.

```python
import pandas as pd

def mock_timedelta_range(start=None, end=None, periods=None, freq=None, name=None, closed=None):
    epoch = pd.Timestamp(0)
    if start is not None:
        start = epoch + pd.Timedelta(start)
    if end is not None:
        end = epoch + pd.Timedelta(end)
    res = pd.date_range(start=start, end=end, periods=periods, freq=freq, name=name, closed=closed)
    res -= epoch
    res.freq = freq
    return res

print(mock_timedelta_range(""1day"", ""10day"", freq=""2D""))
print(pd.timedelta_range(""1day"", ""10day"", freq=""2D""))
```

The outputs from `mock_timedelta_range` and from `pd.timedelta_range` are supposed to equivalent, but are not on pandas v1.0.0:
Outputs without this PR:
```
TimedeltaIndex(['1 days', '3 days', '5 days', '7 days', '9 days'], dtype='timedelta64[ns]', freq='2D')
TimedeltaIndex(['1 days', '3 days', '5 days', '7 days', '9 days', '11 days'], dtype='timedelta64[ns]', freq='2D')
```

Outputs with this PR:
```
TimedeltaIndex(['1 days', '3 days', '5 days', '7 days', '9 days'], dtype='timedelta64[ns]', freq='2D')
TimedeltaIndex(['1 days', '3 days', '5 days', '7 days', '9 days'], dtype='timedelta64[ns]', freq='2D')
```

----
It also solve an issue (that fail on master) related to this comment: https://github.com/pandas-dev/pandas/issues/13022#issuecomment-238382933

```python
import pandas as pd
rng = pd.timedelta_range(start='1d', periods=10, freq='d')
df = pd.Series(range(10), index=rng)
df.resample('2D').count()
```
Outputs with this PR:
```
1 days    2
3 days    2
5 days    2
7 days    2
9 days    2
Freq: 2D, dtype: int64
```

----

On the firsts commits I just fixed the issue in `_generate_regular_range`, then I decided to do a refactor and to use the same code that generate ranges in `core/arrays/_ranges.py` for `date_range` and `timedelta_range`.

Linked with #10887.

----

- [X] closes #30353
       closes #13022
       closes #7687 (for loffset when resampling Timedelta)
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
609495259,33885,"BUG: recognize M8[ns, tzstr]",jbrockmendel,closed,2020-04-30T01:42:14Z,2020-05-09T21:38:34Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

ATM `dtype == ""M8[ns, UTC]""` fails while `dtype == ""datetime64[ns, UTC]""` succeeds"
612863734,34009,REF: avoid importing pd.NA in tslibs,jbrockmendel,closed,2020-05-05T20:09:23Z,2020-05-09T21:43:48Z,"The existing cimport of pd.NA into tslibs.nattype is a circular dependency (its not actually clear to me why it doesnt raise).  I'm finding that its presence is making other refactoring around tslibs.offsets more difficult.  This PR moves the import of pd.NA to tslib, breaking the circular dependency.

This _does_ change some behavior, but none that is tested, so we should discuss what places we want to recognize pd.NA.  In particular, `pd.Timedelta(pd.NA)` returns `NaT` in master and raises `ValueError` in this PR.  `pd.Timestamp(pd.NA)` and `pd.Period(pd.NA)` raise TypeError and ValueError, respectively, in both master and this PR.

cc @jorisvandenbossche what is your preferred behavior?"
614860158,34073,TST/CLN: Break out groupby function tests,dsaxton,closed,2020-05-08T16:53:10Z,2020-05-09T21:51:36Z,tests/groupby/test_function.py is still huge (1000+ lines) but this breaks out a few pieces (all copy / paste with no logic changes)
596602731,33395,BUG: Test failures with CPython master,TomAugspurger,closed,2020-04-08T14:00:27Z,2020-05-10T00:58:42Z,"We have some failures on CPython master (commit c6e5c1123bac6cbb4c85265155af5349dcea522e to be precise):

```
===================================================================================== FAILURES =====================================================================================
pandas/_libs/hashtable_class_helper.pxi:1614: KeyError: 'one'
pandas/_libs/hashtable_class_helper.pxi:1614: KeyError: 'one'
pandas/_libs/hashtable_class_helper.pxi:1614: KeyError: 'one'
pandas/_libs/hashtable_class_helper.pxi:1614: KeyError: 'one'
/Users/taugspurger/sandbox/cpython-dev/pandas/pandas/util/testing.py:394: AssertionError: Series Expected type <class 'pandas.core.series.Series'>, found <class 'pandas.core.arrays.datetimes.DatetimeArray'> instead
pandas/_libs/hashtable_class_helper.pxi:1614: KeyError: 'one'
pandas/_libs/hashtable_class_helper.pxi:1614: KeyError: 'one'
pandas/_libs/hashtable_class_helper.pxi:1614: KeyError: 'one'
pandas/_libs/hashtable_class_helper.pxi:1614: KeyError: 'one'
pandas/_libs/hashtable_class_helper.pxi:1614: KeyError: 'one'
pandas/_libs/hashtable_class_helper.pxi:1614: KeyError: 'one'
pandas/_libs/hashtable_class_helper.pxi:1614: KeyError: 'one'
pandas/_libs/hashtable_class_helper.pxi:1614: KeyError: 'one'
pandas/_libs/groupby_helper.pxi:1014: TypeError: Expected dict, got int
pandas/_libs/groupby_helper.pxi:951: TypeError: Expected dict, got int
pandas/_libs/groupby_helper.pxi:1014: TypeError: Expected dict, got int
pandas/_libs/groupby_helper.pxi:951: TypeError: Expected dict, got int
pandas/_libs/groupby_helper.pxi:1014: TypeError: Expected dict, got int
pandas/_libs/groupby_helper.pxi:951: TypeError: Expected dict, got int
pandas/_libs/groupby_helper.pxi:1014: TypeError: Expected dict, got int
pandas/_libs/groupby_helper.pxi:951: TypeError: Expected dict, got int
pandas/_libs/groupby_helper.pxi:1014: TypeError: Expected dict, got int
pandas/_libs/groupby_helper.pxi:1014: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:750: TypeError: Expected dict, got int
pandas/_libs/join.pyx:819: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
pandas/_libs/join.pyx:681: TypeError: Expected dict, got int
================================================================================= warnings summary =================================================================================
pandas/tests/frame/test_alter_axes.py:241
pandas/tests/frame/test_alter_axes.py:241
  /Users/taugspurger/sandbox/cpython-dev/pandas/pandas/tests/frame/test_alter_axes.py:241: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?
    False if (keys[0] is ""A"" and keys[1] is ""A"") else drop  # noqa: F632

pandas/tests/frame/test_missing.py::TestDataFrameMissingData::test_fillna_categorical_nan
  /Users/taugspurger/sandbox/cpython-dev/numpy/numpy/lib/nanfunctions.py:1114: RuntimeWarning: All-NaN slice encountered
    r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,

pandas/tests/groupby/test_filters.py::test_filter_and_transform_with_non_unique_float_index
  /Users/taugspurger/sandbox/cpython-dev/pandas/pandas/core/indexes/base.py:2821: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
    return self._engine.get_loc(key)

pandas/tests/io/json/test_json_table_schema.py: 12 tests with warnings
pandas/tests/io/json/test_pandas.py: 3 tests with warnings
  /Users/taugspurger/sandbox/cpython-dev/pandas/pandas/io/json/_json.py:148: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
    return dumps(

pandas/tests/io/json/test_pandas.py::TestPandasContainer::test_tz_is_utc
  /Users/taugspurger/sandbox/cpython-dev/pandas/pandas/tests/io/json/test_pandas.py:1324: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
    assert dumps(ts, iso_dates=True) == exp

pandas/tests/io/json/test_pandas.py::TestPandasContainer::test_tz_is_utc
  /Users/taugspurger/sandbox/cpython-dev/pandas/pandas/tests/io/json/test_pandas.py:1326: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
    assert dumps(dt, iso_dates=True) == exp

pandas/tests/io/json/test_pandas.py::TestPandasContainer::test_tz_is_utc
  /Users/taugspurger/sandbox/cpython-dev/pandas/pandas/tests/io/json/test_pandas.py:1329: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
    assert dumps(ts, iso_dates=True) == exp

pandas/tests/io/json/test_pandas.py::TestPandasContainer::test_tz_is_utc
  /Users/taugspurger/sandbox/cpython-dev/pandas/pandas/tests/io/json/test_pandas.py:1331: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
    assert dumps(dt, iso_dates=True) == exp

pandas/tests/io/json/test_pandas.py::TestPandasContainer::test_tz_is_utc
  /Users/taugspurger/sandbox/cpython-dev/pandas/pandas/tests/io/json/test_pandas.py:1334: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
    assert dumps(ts, iso_dates=True) == exp

pandas/tests/io/json/test_pandas.py::TestPandasContainer::test_tz_is_utc
  /Users/taugspurger/sandbox/cpython-dev/pandas/pandas/tests/io/json/test_pandas.py:1336: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
    assert dumps(dt, iso_dates=True) == exp

pandas/tests/io/json/test_pandas.py::TestPandasContainer::test_tz_range_is_utc
pandas/tests/io/json/test_pandas.py::TestPandasContainer::test_tz_range_is_utc
  /Users/taugspurger/sandbox/cpython-dev/pandas/pandas/tests/io/json/test_pandas.py:1353: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
    result = dumps(df, iso_dates=True)

pandas/tests/io/json/test_pandas.py::TestPandasContainer::test_tz_range_is_utc
pandas/tests/io/json/test_pandas.py::TestPandasContainer::test_tz_range_is_utc
  /Users/taugspurger/sandbox/cpython-dev/pandas/pandas/tests/io/json/test_pandas.py:1361: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
    assert dumps(df, iso_dates=True) == dfexp

pandas/tests/io/json/test_pandas.py::TestPandasContainer::test_tz_range_is_utc
pandas/tests/io/json/test_pandas.py::TestPandasContainer::test_tz_range_is_utc
  /Users/taugspurger/sandbox/cpython-dev/pandas/pandas/tests/io/json/test_pandas.py:1368: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
    assert dumps(df, iso_dates=True) == dfexp

pandas/tests/sparse/test_pivot.py::TestPivotTable::test_pivot_table
  /Users/taugspurger/sandbox/cpython-dev/pandas/pandas/core/arrays/sparse.py:1645: RuntimeWarning: invalid value encountered in double_scalars
    return sp_sum / ct

-- Docs: https://docs.pytest.org/en/latest/warnings.html
============================================================================= short test summary info ==============================================================================
FAILED pandas/tests/test_multilevel.py::TestMultiLevel::test_frame_group_ops[True-True-1-1-min] - KeyError: 'one'
FAILED pandas/tests/test_multilevel.py::TestMultiLevel::test_frame_group_ops[True-True-1-1-max] - KeyError: 'one'
FAILED pandas/tests/test_multilevel.py::TestMultiLevel::test_frame_group_ops[False-True-1-1-min] - KeyError: 'one'
FAILED pandas/tests/test_multilevel.py::TestMultiLevel::test_frame_group_ops[False-True-1-1-max] - KeyError: 'one'
FAILED pandas/tests/groupby/test_timegrouper.py::TestGroupBy::test_groupby_with_timezone_selection - AssertionError: Series Expected type <class 'pandas.core.series.Series'>, fo...
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[True-True-1-1-min] - KeyError: 'one'
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[True-True-1-1-max] - KeyError: 'one'
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[True-False-1-1-min] - KeyError: 'one'
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[True-False-1-1-max] - KeyError: 'one'
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[False-True-1-1-min] - KeyError: 'one'
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[False-True-1-1-max] - KeyError: 'one'
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[False-False-1-1-min] - KeyError: 'one'
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[False-False-1-1-max] - KeyError: 'one'
FAILED pandas/tests/groupby/aggregate/test_cython.py::test__cython_agg_general[min-amin] - TypeError: Expected dict, got int
FAILED pandas/tests/groupby/aggregate/test_cython.py::test__cython_agg_general[max-amax] - TypeError: Expected dict, got int
FAILED pandas/tests/groupby/aggregate/test_cython.py::test_cython_agg_empty_buckets[True-min-amin] - TypeError: Expected dict, got int
FAILED pandas/tests/groupby/aggregate/test_cython.py::test_cython_agg_empty_buckets[True-max-amax] - TypeError: Expected dict, got int
FAILED pandas/tests/groupby/aggregate/test_cython.py::test_cython_agg_empty_buckets[False-min-amin] - TypeError: Expected dict, got int
FAILED pandas/tests/groupby/aggregate/test_cython.py::test_cython_agg_empty_buckets[False-max-amax] - TypeError: Expected dict, got int
FAILED pandas/tests/groupby/aggregate/test_cython.py::test_cython_agg_empty_buckets[None-min-amin] - TypeError: Expected dict, got int
FAILED pandas/tests/groupby/aggregate/test_cython.py::test_cython_agg_empty_buckets[None-max-amax] - TypeError: Expected dict, got int
FAILED pandas/tests/resample/test_datetime_index.py::test_custom_grouper - TypeError: Expected dict, got int
FAILED pandas/tests/resample/test_datetime_index.py::test_resample_frame_basic - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_examples2 - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_basic - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_basic_categorical - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_basic_left_index - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_basic_right_index - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_basic_left_index_right_index - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_basic_left_by_right_by - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_missing_right_by - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_multiby - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_multiby_heterogeneous_types - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_multiby_indexed - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_basic2 - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_with_duplicates - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_valid_tolerance - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_non_sorted - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_tolerance[pd.Timedelta] - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_index_tolerance - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_allow_exact_matches - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_allow_exact_matches_and_tolerance - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_forward_by - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_nearest_by - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_by_int - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_on_specialized_type_by_int[float] - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_on_specialized_type_by_int[float32] - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_on_specialized_type_by_int[float64] - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_on_specialized_type_by_int[uint8] - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_on_specialized_type_by_int[uint16] - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_on_specialized_type_by_int[uint32] - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_on_specialized_type_by_int[uint64] - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_on_specialized_type_by_int[int] - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_on_specialized_type_by_int[int8] - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_on_specialized_type_by_int[int16] - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_on_specialized_type_by_int[int32] - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_on_specialized_type_by_int[int64] - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_on_float_by_int - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_merge_by_col_tz_aware - TypeError: Expected dict, got int
FAILED pandas/tests/reshape/merge/test_merge_asof.py::TestAsOfMerge::test_by_mixed_tz_aware - TypeError: Expected dict, got int
======================================== 60 failed, 54929 passed, 3971 skipped, 832 xfailed, 10 xpassed, 32 warnings in 1229.76s (0:20:29) =========================================
```

Short tracebacks in the details


<details>

```
===================================================================================== FAILURES =====================================================================================
______________________________________________________________ TestMultiLevel.test_frame_group_ops[True-True-1-1-min] ______________________________________________________________
pandas/core/indexes/base.py:2821: in get_loc
    return self._engine.get_loc(key)
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'

During handling of the above exception, another exception occurred:
pandas/tests/test_multilevel.py:1031: in test_frame_group_ops
    rightside = getattr(frame, op)(level=level, axis=axis, skipna=skipna)
pandas/core/generic.py:11600: in stat_func
    return self._agg_by_level(name, axis=axis, level=level, skipna=skipna)
pandas/core/generic.py:10415: in _agg_by_level
    return getattr(grouped, name)(**kwargs)
pandas/core/groupby/groupby.py:1383: in f
    result[col] = self._try_cast(result[col], self.obj[col])
pandas/core/frame.py:2846: in __getitem__
    return self._getitem_multilevel(key)
pandas/core/frame.py:2896: in _getitem_multilevel
    loc = self.columns.get_loc(key)
pandas/core/indexes/multi.py:2662: in get_loc
    loc = self._get_level_indexer(key, level=0)
pandas/core/indexes/multi.py:2928: in _get_level_indexer
    code = level_index.get_loc(key)
pandas/core/indexes/base.py:2823: in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'
______________________________________________________________ TestMultiLevel.test_frame_group_ops[True-True-1-1-max] ______________________________________________________________
pandas/core/indexes/base.py:2821: in get_loc
    return self._engine.get_loc(key)
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'

During handling of the above exception, another exception occurred:
pandas/tests/test_multilevel.py:1031: in test_frame_group_ops
    rightside = getattr(frame, op)(level=level, axis=axis, skipna=skipna)
pandas/core/generic.py:11600: in stat_func
    return self._agg_by_level(name, axis=axis, level=level, skipna=skipna)
pandas/core/generic.py:10415: in _agg_by_level
    return getattr(grouped, name)(**kwargs)
pandas/core/groupby/groupby.py:1383: in f
    result[col] = self._try_cast(result[col], self.obj[col])
pandas/core/frame.py:2846: in __getitem__
    return self._getitem_multilevel(key)
pandas/core/frame.py:2896: in _getitem_multilevel
    loc = self.columns.get_loc(key)
pandas/core/indexes/multi.py:2662: in get_loc
    loc = self._get_level_indexer(key, level=0)
pandas/core/indexes/multi.py:2928: in _get_level_indexer
    code = level_index.get_loc(key)
pandas/core/indexes/base.py:2823: in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'
_____________________________________________________________ TestMultiLevel.test_frame_group_ops[False-True-1-1-min] ______________________________________________________________
pandas/core/indexes/base.py:2821: in get_loc
    return self._engine.get_loc(key)
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'

During handling of the above exception, another exception occurred:
pandas/tests/test_multilevel.py:1031: in test_frame_group_ops
    rightside = getattr(frame, op)(level=level, axis=axis, skipna=skipna)
pandas/core/generic.py:11600: in stat_func
    return self._agg_by_level(name, axis=axis, level=level, skipna=skipna)
pandas/core/generic.py:10415: in _agg_by_level
    return getattr(grouped, name)(**kwargs)
pandas/core/groupby/groupby.py:1383: in f
    result[col] = self._try_cast(result[col], self.obj[col])
pandas/core/frame.py:2846: in __getitem__
    return self._getitem_multilevel(key)
pandas/core/frame.py:2896: in _getitem_multilevel
    loc = self.columns.get_loc(key)
pandas/core/indexes/multi.py:2662: in get_loc
    loc = self._get_level_indexer(key, level=0)
pandas/core/indexes/multi.py:2928: in _get_level_indexer
    code = level_index.get_loc(key)
pandas/core/indexes/base.py:2823: in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'
_____________________________________________________________ TestMultiLevel.test_frame_group_ops[False-True-1-1-max] ______________________________________________________________
pandas/core/indexes/base.py:2821: in get_loc
    return self._engine.get_loc(key)
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'

During handling of the above exception, another exception occurred:
pandas/tests/test_multilevel.py:1031: in test_frame_group_ops
    rightside = getattr(frame, op)(level=level, axis=axis, skipna=skipna)
pandas/core/generic.py:11600: in stat_func
    return self._agg_by_level(name, axis=axis, level=level, skipna=skipna)
pandas/core/generic.py:10415: in _agg_by_level
    return getattr(grouped, name)(**kwargs)
pandas/core/groupby/groupby.py:1383: in f
    result[col] = self._try_cast(result[col], self.obj[col])
pandas/core/frame.py:2846: in __getitem__
    return self._getitem_multilevel(key)
pandas/core/frame.py:2896: in _getitem_multilevel
    loc = self.columns.get_loc(key)
pandas/core/indexes/multi.py:2662: in get_loc
    loc = self._get_level_indexer(key, level=0)
pandas/core/indexes/multi.py:2928: in _get_level_indexer
    code = level_index.get_loc(key)
pandas/core/indexes/base.py:2823: in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'
_________________________________________________________________ TestGroupBy.test_groupby_with_timezone_selection _________________________________________________________________
pandas/tests/groupby/test_timegrouper.py:673: in test_groupby_with_timezone_selection
    tm.assert_series_equal(df1, df2)
pandas/util/testing.py:394: in _check_isinstance
    raise AssertionError(
E   AssertionError: Series Expected type <class 'pandas.core.series.Series'>, found <class 'pandas.core.arrays.datetimes.DatetimeArray'> instead
_______________________________________________________________ test_regression_whitelist_methods[True-True-1-1-min] _______________________________________________________________
pandas/core/indexes/base.py:2821: in get_loc
    return self._engine.get_loc(key)
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'

During handling of the above exception, another exception occurred:
pandas/tests/groupby/test_whitelist.py:209: in test_regression_whitelist_methods
    result = getattr(grouped, op)()
pandas/core/groupby/groupby.py:1383: in f
    result[col] = self._try_cast(result[col], self.obj[col])
pandas/core/frame.py:2846: in __getitem__
    return self._getitem_multilevel(key)
pandas/core/frame.py:2896: in _getitem_multilevel
    loc = self.columns.get_loc(key)
pandas/core/indexes/multi.py:2662: in get_loc
    loc = self._get_level_indexer(key, level=0)
pandas/core/indexes/multi.py:2928: in _get_level_indexer
    code = level_index.get_loc(key)
pandas/core/indexes/base.py:2823: in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'
_______________________________________________________________ test_regression_whitelist_methods[True-True-1-1-max] _______________________________________________________________
pandas/core/indexes/base.py:2821: in get_loc
    return self._engine.get_loc(key)
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'

During handling of the above exception, another exception occurred:
pandas/tests/groupby/test_whitelist.py:209: in test_regression_whitelist_methods
    result = getattr(grouped, op)()
pandas/core/groupby/groupby.py:1383: in f
    result[col] = self._try_cast(result[col], self.obj[col])
pandas/core/frame.py:2846: in __getitem__
    return self._getitem_multilevel(key)
pandas/core/frame.py:2896: in _getitem_multilevel
    loc = self.columns.get_loc(key)
pandas/core/indexes/multi.py:2662: in get_loc
    loc = self._get_level_indexer(key, level=0)
pandas/core/indexes/multi.py:2928: in _get_level_indexer
    code = level_index.get_loc(key)
pandas/core/indexes/base.py:2823: in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'
______________________________________________________________ test_regression_whitelist_methods[True-False-1-1-min] _______________________________________________________________
pandas/core/indexes/base.py:2821: in get_loc
    return self._engine.get_loc(key)
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'

During handling of the above exception, another exception occurred:
pandas/tests/groupby/test_whitelist.py:209: in test_regression_whitelist_methods
    result = getattr(grouped, op)()
pandas/core/groupby/groupby.py:1383: in f
    result[col] = self._try_cast(result[col], self.obj[col])
pandas/core/frame.py:2846: in __getitem__
    return self._getitem_multilevel(key)
pandas/core/frame.py:2896: in _getitem_multilevel
    loc = self.columns.get_loc(key)
pandas/core/indexes/multi.py:2662: in get_loc
    loc = self._get_level_indexer(key, level=0)
pandas/core/indexes/multi.py:2928: in _get_level_indexer
    code = level_index.get_loc(key)
pandas/core/indexes/base.py:2823: in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'
______________________________________________________________ test_regression_whitelist_methods[True-False-1-1-max] _______________________________________________________________
pandas/core/indexes/base.py:2821: in get_loc
    return self._engine.get_loc(key)
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'

During handling of the above exception, another exception occurred:
pandas/tests/groupby/test_whitelist.py:209: in test_regression_whitelist_methods
    result = getattr(grouped, op)()
pandas/core/groupby/groupby.py:1383: in f
    result[col] = self._try_cast(result[col], self.obj[col])
pandas/core/frame.py:2846: in __getitem__
    return self._getitem_multilevel(key)
pandas/core/frame.py:2896: in _getitem_multilevel
    loc = self.columns.get_loc(key)
pandas/core/indexes/multi.py:2662: in get_loc
    loc = self._get_level_indexer(key, level=0)
pandas/core/indexes/multi.py:2928: in _get_level_indexer
    code = level_index.get_loc(key)
pandas/core/indexes/base.py:2823: in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'
______________________________________________________________ test_regression_whitelist_methods[False-True-1-1-min] _______________________________________________________________
pandas/core/indexes/base.py:2821: in get_loc
    return self._engine.get_loc(key)
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'

During handling of the above exception, another exception occurred:
pandas/tests/groupby/test_whitelist.py:209: in test_regression_whitelist_methods
    result = getattr(grouped, op)()
pandas/core/groupby/groupby.py:1383: in f
    result[col] = self._try_cast(result[col], self.obj[col])
pandas/core/frame.py:2846: in __getitem__
    return self._getitem_multilevel(key)
pandas/core/frame.py:2896: in _getitem_multilevel
    loc = self.columns.get_loc(key)
pandas/core/indexes/multi.py:2662: in get_loc
    loc = self._get_level_indexer(key, level=0)
pandas/core/indexes/multi.py:2928: in _get_level_indexer
    code = level_index.get_loc(key)
pandas/core/indexes/base.py:2823: in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'
______________________________________________________________ test_regression_whitelist_methods[False-True-1-1-max] _______________________________________________________________
pandas/core/indexes/base.py:2821: in get_loc
    return self._engine.get_loc(key)
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'

During handling of the above exception, another exception occurred:
pandas/tests/groupby/test_whitelist.py:209: in test_regression_whitelist_methods
    result = getattr(grouped, op)()
pandas/core/groupby/groupby.py:1383: in f
    result[col] = self._try_cast(result[col], self.obj[col])
pandas/core/frame.py:2846: in __getitem__
    return self._getitem_multilevel(key)
pandas/core/frame.py:2896: in _getitem_multilevel
    loc = self.columns.get_loc(key)
pandas/core/indexes/multi.py:2662: in get_loc
    loc = self._get_level_indexer(key, level=0)
pandas/core/indexes/multi.py:2928: in _get_level_indexer
    code = level_index.get_loc(key)
pandas/core/indexes/base.py:2823: in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'
______________________________________________________________ test_regression_whitelist_methods[False-False-1-1-min] ______________________________________________________________
pandas/core/indexes/base.py:2821: in get_loc
    return self._engine.get_loc(key)
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'

During handling of the above exception, another exception occurred:
pandas/tests/groupby/test_whitelist.py:209: in test_regression_whitelist_methods
    result = getattr(grouped, op)()
pandas/core/groupby/groupby.py:1383: in f
    result[col] = self._try_cast(result[col], self.obj[col])
pandas/core/frame.py:2846: in __getitem__
    return self._getitem_multilevel(key)
pandas/core/frame.py:2896: in _getitem_multilevel
    loc = self.columns.get_loc(key)
pandas/core/indexes/multi.py:2662: in get_loc
    loc = self._get_level_indexer(key, level=0)
pandas/core/indexes/multi.py:2928: in _get_level_indexer
    code = level_index.get_loc(key)
pandas/core/indexes/base.py:2823: in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'
______________________________________________________________ test_regression_whitelist_methods[False-False-1-1-max] ______________________________________________________________
pandas/core/indexes/base.py:2821: in get_loc
    return self._engine.get_loc(key)
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'

During handling of the above exception, another exception occurred:
pandas/tests/groupby/test_whitelist.py:209: in test_regression_whitelist_methods
    result = getattr(grouped, op)()
pandas/core/groupby/groupby.py:1383: in f
    result[col] = self._try_cast(result[col], self.obj[col])
pandas/core/frame.py:2846: in __getitem__
    return self._getitem_multilevel(key)
pandas/core/frame.py:2896: in _getitem_multilevel
    loc = self.columns.get_loc(key)
pandas/core/indexes/multi.py:2662: in get_loc
    loc = self._get_level_indexer(key, level=0)
pandas/core/indexes/multi.py:2928: in _get_level_indexer
    code = level_index.get_loc(key)
pandas/core/indexes/base.py:2823: in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
pandas/_libs/index.pyx:103: in pandas._libs.index.IndexEngine.get_loc
    cpdef get_loc(self, object val):
pandas/_libs/index.pyx:127: in pandas._libs.index.IndexEngine.get_loc
    return self.mapping.get_item(val)
pandas/_libs/hashtable_class_helper.pxi:1607: in pandas._libs.hashtable.PyObjectHashTable.get_item
    cpdef get_item(self, object val):
pandas/_libs/hashtable_class_helper.pxi:1614: in pandas._libs.hashtable.PyObjectHashTable.get_item
    raise KeyError(val)
E   KeyError: 'one'
============================================================================= short test summary info ==============================================================================
FAILED pandas/tests/test_multilevel.py::TestMultiLevel::test_frame_group_ops[True-True-1-1-min] - KeyError: 'one'
FAILED pandas/tests/test_multilevel.py::TestMultiLevel::test_frame_group_ops[True-True-1-1-max] - KeyError: 'one'
FAILED pandas/tests/test_multilevel.py::TestMultiLevel::test_frame_group_ops[False-True-1-1-min] - KeyError: 'one'
FAILED pandas/tests/test_multilevel.py::TestMultiLevel::test_frame_group_ops[False-True-1-1-max] - KeyError: 'one'
FAILED pandas/tests/groupby/test_timegrouper.py::TestGroupBy::test_groupby_with_timezone_selection - AssertionError: Series Expected type <class 'pandas.core.series.Series'>, fo...
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[True-True-1-1-min] - KeyError: 'one'
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[True-True-1-1-max] - KeyError: 'one'
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[True-False-1-1-min] - KeyError: 'one'
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[True-False-1-1-max] - KeyError: 'one'
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[False-True-1-1-min] - KeyError: 'one'
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[False-True-1-1-max] - KeyError: 'one'
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[False-False-1-1-min] - KeyError: 'one'
FAILED pandas/tests/groupby/test_whitelist.py::test_regression_whitelist_methods[False-False-1-1-max] - KeyError: 'one'
```

</details>

"
598589638,33505,CI: Setup 3.9 Travis Build,alimcmaster1,closed,2020-04-12T22:57:21Z,2020-05-10T01:01:13Z,"- [x] closes https://github.com/pandas-dev/pandas/issues/33395

- In a very similar fashion to @jbrockmendel work for 3.8 - might be useful to have start a 3.9 build so we can start eliminating the issues @TomAugspurger mentioned in https://github.com/pandas-dev/pandas/issues/33395.

This is using currently using `Python 3.9.0a5+`

Currently failing with
```
ImportError while loading conftest '/home/travis/build/pandas-dev/pandas/pandas/conftest.py'.
pandas/conftest.py:1123: in <module>
    (""period"", [pd.Period(2013), pd.NaT, pd.Period(2018)]),
pandas/_libs/tslibs/period.pyx:2470: in pandas._libs.tslibs.period.Period.__new__
    freq = Resolution.get_freq(reso)
pandas/_libs/tslibs/resolution.pyx:240: in pandas._libs.tslibs.resolution.Resolution.get_freq
    return cls._reso_freq_map[resostr]
E   AttributeError: type object 'type' has no attribute '_reso_freq_map'
The command ""ci/run_tests.sh"" exited with 4.
cache.2
store build cache
```"
611095633,33938,TST: raise InvalidIndexError with IntervalIndex.get_value and get_loc,primaprashant,closed,2020-05-02T03:13:12Z,2020-05-10T01:54:45Z,"ensure InvalidIndexError is raised when non-scalar is passed as key to
get_loc and get_value methods for IntervalIndex

- [X] closes #31117 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
615220692,34094,CLN: Change old string formatting syntax into f-strings,proost,closed,2020-05-09T17:11:58Z,2020-05-10T10:00:58Z,"- [x] xref #29547
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
588920768,33061,DateTimeIndex.tz_convert() does not apply DST from 2038 onward,obeavers,closed,2020-03-27T06:37:37Z,2020-05-10T14:41:06Z,"#### Code Sample, a copy-pastable example if possible

```python
assert np.all(pd.date_range('1/1/2037', periods=8760, freq='H', tz='EST').time == pd.date_range('1/1/2037', periods=8760, freq='H', tz='US/Eastern').time) == False

assert np.all(pd.date_range('1/1/2038', periods=8760, freq='H', tz='EST').time == pd.date_range('1/1/2038', periods=8760, freq='H', tz='US/Eastern').time) == False # fails

assert np.all(pd.date_range('1/1/2039', periods=8760, freq='H', tz='EST').time == pd.date_range('1/1/2039', periods=8760, freq='H', tz='US/Eastern').time) == False # fails

```
#### Problem description

Wow, this one hurt. US/Eastern timezone is DST-adjusted (blend of EST/EDT) whereas EST is just EST. 

The second and third assert statements above should both return False.

Surprised this hasn't come up before. 

This is apparently related to a UNIX issue: https://en.wikipedia.org/wiki/Year_2038_problem. With that said, it seems the dtype is datetime64 with some pandas customizations on timezone. Supposedly 64 bit should have solved this.

#### Expected Output

Both of the following should pass:
assert np.all(pd.date_range('1/1/2038', periods=8760, freq='H', tz='EST').time == pd.date_range('1/1/2038', periods=8760, freq='H', tz='US/Eastern').time) == False # fails

assert np.all(pd.date_range('1/1/2039', periods=8760, freq='H', tz='EST').time == pd.date_range('1/1/2039', periods=8760, freq='H', tz='US/Eastern').time) == False # fails

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
NSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
555287616,31331,BUG: Increased support for subclassed types.,EmilianoJordan,closed,2020-01-26T21:03:41Z,2020-05-10T15:02:08Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
615272441,34096,TST: Mark groupby.nunique test as slow,dsaxton,closed,2020-05-09T22:29:37Z,2020-05-10T15:05:30Z,"This test takes about 40 seconds to run for me while shrinking the size of the data brings the runtime down to 2 seconds, presumably without changing coverage (do we even need to parametrize over n and m here?)."
614489411,34062,REF: move apply_wraps up into liboffstes,jbrockmendel,closed,2020-05-08T04:10:18Z,2020-05-10T15:36:24Z,
611227931,33943,TST/REF: Fixturize constant functions in ConsistencyBase,charlesdong1991,closed,2020-05-02T16:41:45Z,2020-05-10T15:45:59Z,"This PR fixturizes the last constants (`base_functions` and `no_nan_functions`) in ConsistencyBase, and take corresponding tests out of class. 

After this one, only constants in `Base` class needs to be fixed (if `Base` is also wanted to be cleaned)

cc @jreback "
408123638,25226,Extending the grouper `base` argument,LucaAmerio,closed,2019-02-08T12:02:47Z,2020-05-10T15:52:54Z,"#### Problem description

`pd.Grouper`'s  `base` argument is almost undocumented ([link](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.Grouper.html)). Despite this, the argument is extremely useful when grouping a dataframe with uneven sampling rate with an arbitrary start time.

Take for example the case of a dataframe representing a 60 minutes experiment with uneven sampling rate starting at '12-01-2019 11:55:23.01938' that we want to split in six 10-minutes group. `base` is our best ally here.

Despite this, in the current implementation `base` represents a floating number of **minutes**. It's however quite hard to understand what this number refers to, is quite cumbersome to convert the above '11:55:23.01938' into a number of minutes and gives you little flexibility regarding when the grouping will start.

This can also cause rounding error issues as descibed [here](https://github.com/pandas-dev/pandas/issues/25161).

I suggest replacing (or extend) this allowing it to be a datetime object so that the edge of each group is simply
`base + N * freq`
where N is an integer number. This is much cleaner and easy to understand.
"
499995075,28675,"resample becomes non-deterministic, depending on DateTimeIndex values",haeusser,closed,2019-09-30T00:27:12Z,2020-05-10T15:52:55Z,"#### Minimal Example
```python
import datetime as dt
import numpy as np
import pandas as pd


def np_to_df(data, start_time):
    index = pd.DatetimeIndex(
        [start_time + dt.timedelta(milliseconds=t) for t in range(len(data))])
    df = pd.DataFrame(data, index=index)
    return df


# generate sample data
data = np.sin(np.arange(1000) / 30)

# create DataFrames with DateTimeIndices
df_1 = np_to_df(data, dt.datetime(2019, 9, 30, 9, 41))
df_2 = np_to_df(data, dt.datetime(2019, 9, 30, 9, 42))

# print difference before resampling
print(""error_1-2:"", np.mean(np.abs(df_1.values - df_2.values)))

# resample
df_1 = df_1.resample(""19ms"").mean()
df_2 = df_2.resample(""19ms"").mean()

# print difference after resampling
print(""error_1-2:"", np.mean(np.abs(df_1.values - df_2.values)))
```

Output:
```
error_1-2: 0.0
error_1-2: 0.04119868246404099
```
#### Problem description
When you give the exact same data to the resample function, it becomes non-deterministic if the `DateTimeIndex` has differing values - even though the frequency is the same.

#### Expected Output
The values of the two `DataFrames` should be exactly the same.

#### Output of ``pd.show_versions()``
<details>

- commit           : None
- python           : 3.6.8.final.0
- python-bits      : 64
- OS               : Linux
- OS-release       : 4.15.0-51-generic
- machine          : x86_64
- processor        : x86_64
- byteorder        : little
- LC_ALL           : None
- LANG             : C.UTF-8
- LOCALE           : en_US.UTF-8
- pandas           : 0.25.1
- numpy            : 1.17.2
- pytz             : 2019.2
- dateutil         : 2.8.0
- pip              : 9.0.1
- setuptools       : 41.0.1
- Cython           : None
- pytest           : 4.4.0
- hypothesis       : None
- sphinx           : None
- blosc            : None
- feather          : None
- xlsxwriter       : None
- lxml.etree       : 4.3.3
- html5lib         : 0.999999999
- pymysql          : None
- psycopg2         : 2.7.7 (dt dec pq3 ext lo64)
- jinja2           : 2.10.1
- IPython          : 7.1.1
- pandas_datareader: None
- bs4              : None
- bottleneck       : None
- fastparquet      : None
- gcsfs            : None
- lxml.etree       : 4.3.3
- matplotlib       : 3.1.1
- numexpr          : None
- odfpy            : None
- openpyxl         : None
- pandas_gbq       : None
- pyarrow          : None
- pytables         : None
- s3fs             : None
- scipy            : 1.3.1
- sqlalchemy       : 1.3.7
- tables           : None
- xarray           : None
- xlrd             : None
- xlwt             : None
- xlsxwriter       : None

</details>

Happy about any help, @jreback ?"
45371660,8521,"ENH: resample(..., base='start') for automaticly determining base.",dimbeto,closed,2014-10-09T14:19:41Z,2020-05-10T15:52:55Z,"(edit):


```python
import pandas as pd
import datetime as dt
import numpy as np

datetime_start = dt.datetime(2014, 9, 1, 9, 30)
datetime_end = dt.datetime(2014, 9, 1, 16, 0)

tt = pd.date_range(datetime_start, datetime_end, freq='1Min')
df = pd.DataFrame(np.arange(len(tt)), index=tt, columns=['A'])
```

The first item is at 9:30, which is not divisible by 8 minutes. We'd like `df.resample('8T', base='start').first()` to be equivalent to `df.resample('8T', base=2)`.

```python
In [13]: df.resample(""8T"", base=2).first()
Out[13]:
                       A
2014-09-01 09:30:00    0
2014-09-01 09:38:00    8
2014-09-01 09:46:00   16
2014-09-01 09:54:00   24
2014-09-01 10:02:00   32
```


---

It seems that for 1Min bar data, resample() with sampling frequency of any multiple of 8 has a bug. The code below illustrates the bug when resampling is done at [3, 5, 6, 8, 16] Min. For both 3 and 5 frequency, the first entry of the resampled dataframe index starts at the base timestamp (9:30 in this case) while for frequencies 8 and 16, the resampled index starts at 9:26 and 9:18 respectively.

``` Python
import pandas as pd
import datetime as dt
import numpy as np

datetime_start = dt.datetime(2014, 9, 1, 9, 30)
datetime_end = dt.datetime(2014, 9, 1, 16, 0)

tt = pd.date_range(datetime_start, datetime_end, freq='1Min')
df = pd.DataFrame(np.arange(len(tt)), index=tt, columns=['A'])

for freq in [3, 5, 6, 8, 16]:
    print(freq)
    print(df.resample(str(freq) + 'Min', how='first', base=30).head(2))
```

produces the following output:

```
3
                     A
2014-09-01 09:30:00  0
2014-09-01 09:33:00  3
5
                     A
2014-09-01 09:30:00  0
2014-09-01 09:35:00  5
6
                     A
2014-09-01 09:30:00  0
2014-09-01 09:36:00  6
8
                     A
2014-09-01 09:26:00  0
2014-09-01 09:34:00  4
16
                     A
2014-09-01 09:18:00  0
2014-09-01 09:34:00  4
```
"
615000470,34081,REF: do length-checks in boilerplate decorator,jbrockmendel,closed,2020-05-08T21:37:36Z,2020-05-10T15:56:15Z,
562064369,31809,ENH: add 'origin' and 'offset' arguments to 'resample' and 'pd.Grouper',hasB4K,closed,2020-02-08T19:11:11Z,2020-05-10T16:45:31Z,"**EDIT:** this PR has changed, now instead of adding `adjust_timestamp` we are adding `origin` and `offset` arguments to `resample` and `pd.Grouper` (see https://github.com/pandas-dev/pandas/pull/31809#issuecomment-583884772)

----

Hello,

This enhancement is an alternative to the `base` argument present in `pd.Grouper` or in the method `resample`. It adds the `adjust_timestamp` argument to change the current behavior of: https://github.com/pandas-dev/pandas/blob/master/pandas/core/resample.py#L1728

-  `adjust_timestamp` is the timestamp on which to adjust the grouping. If None is passed, the first day of the time series at midnight is used.

Currently the bins of the grouping are adjusted based on the beginning of the day of the time series starting point. This works well with frequencies that are multiples of a day (like `30D`) or that divides a day (like `90s` or `1min`). But it can create inconsistencies with some frequencies that do not meet this criteria.

Here is a simple snippet from a test that I added that proves that the current behavior can lead to some inconsistencies. Inconsistencies that can be fixed if we use `adjust_timestamp`:

```python
import pandas as pd
import numpy as np
import pandas._testing as tm
import pytest


freq = ""1399min""  # prime number that is smaller than 24h
start, end = ""1/1/2000 00:00:00"", ""1/31/2000 00:00""
middle = ""1/15/2000 00:00:00""

rng = pd.date_range(start, end, freq=""1231min"")  # prime number
ts = pd.Series(np.random.randn(len(rng)), index=rng)
ts2 = ts[middle:end]

# proves that grouper without a fixed adjust_timestamp does not work
# when dealing with unusual frequencies
simple_grouper = pd.Grouper(freq=freq)
count_ts = ts.groupby(simple_grouper).agg(""count"")
count_ts = count_ts[middle:end]
count_ts2 = ts2.groupby(simple_grouper).agg(""count"")
with pytest.raises(AssertionError):
    tm.assert_index_equal(count_ts.index, count_ts2.index)

# test adjusted_timestamp on 1970-01-01 00:00:00
adjust_timestamp = pd.Timestamp(0)
adjusted_grouper = pd.Grouper(freq=freq, adjust_timestamp=adjust_timestamp)
adjusted_count_ts = ts.groupby(adjusted_grouper).agg(""count"")
adjusted_count_ts = adjusted_count_ts[middle:end]
adjusted_count_ts2 = ts2.groupby(adjusted_grouper).agg(""count"")
tm.assert_series_equal(adjusted_count_ts, adjusted_count_ts2)
```

I think this PR is ready to be merged, but I am of course open to any suggestions or criticism. :wink: 
For instance, I am not sure if the naming of `adjust_timestamp` is correct. An alternative could be `base_timestamp` or `ref_timestamp` :thinking:?

Cheers,

----

- [X] closes #25226
closes #28302
closes #28675
closes #4197
closes #8521
- [X] Add 'origin' and 'offset' arguments to 'resample' and 'pd.Grouper'
- [X] tests added / passed
- [X] Add deprecation warning for `loffset` and `base` in the code
- [X] Add deprecation warning for `loffset` and `base` in the doc
- [x] Add examples in the doc for `origin` and `offset`
- [x] whatsnew entry (add deprecation notice with `offset` example)
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
602747636,33655,BUG: pandas.options.mode.use_inf_as_na disables NA checking for StringArray,dsaxton,closed,2020-04-19T15:42:11Z,2020-05-10T17:12:46Z,"1.0.3 or master:

```python
import pandas as pd

pd.options.mode.use_inf_as_na = True
arr = pd.array([""a"", ""b"", None])
arr.isna()
# array([False, False, False])
```

Setting use_inf_as_na to True breaks isna for StringArray as shown above; it seems to work fine for other ExtensionArrays I checked (IntegerArray, BooleanArray)."
613470100,34030,TST: Add Series.update ExtensionArray tests,dsaxton,closed,2020-05-06T16:58:23Z,2020-05-10T17:39:22Z,"cc @simonjayhawkins 

xref https://github.com/pandas-dev/pandas/issues/25744 https://github.com/pandas-dev/pandas/issues/34020

I think the dtype issue for DataFrame.update with categorical columns mentioned in the first issue is still unaddressed, so likely don't want to only close and lose that (perhaps break into its own issue)?"
602750949,33656,BUG: Fix StringArray use_inf_as_na bug,dsaxton,closed,2020-04-19T15:54:24Z,2020-05-10T17:40:00Z,"- [x] closes #33655
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
615004006,34082,CLN: Delete no-op check,dsaxton,closed,2020-05-08T21:46:34Z,2020-05-10T17:48:50Z,"Maybe the note is somehow useful, otherwise I think this can be cleaned?"
608951174,33863,DOC: timezone warning for DST beyond 2038-01-18,telferm57,closed,2020-04-29T10:38:36Z,2020-05-10T18:09:58Z,"Add warning to time_series user guide that after 2038-01-18 DST will not be respected in tz aware dates 

- [x ] closes #33061
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
591996882,33209,API/BUG: isna_old on EAs?,jbrockmendel,closed,2020-04-01T15:17:51Z,2020-05-10T18:50:58Z,"isna_new has a code path that handles EAs that isna_old doesnt, so the following will raise:

```
pd.set_option(""mode.use_inf_as_na"", True)
pd.isna(my_decimal_array)
```

Most of this can be fixed by adding to isna_ndarray_old:

```
if is_extension_array_dtype(values.dtype):
    result = values.isna()
```

Then we can add into the interface tests:

```
    def test_isna_old_extension_array(self, data_missing):
        # TODO: do we need EA.isna to handle `inf`?
        expected = data_missing.isna()
        with cf.option_context(""mode.use_inf_as_na"", True):
            result = pd.isna(data_missing)

        tm.assert_equal(result, expected)
```

The two problems here are 1) as in the comment, this may be wrong/ambiguous if the EA has a concept of ""inf"" and 2) we still get a test failure on `StringArray([<NA>, 'A'])`
"
613555158,34034,setting values in a dataframe with duplicated keys,c-foschi,closed,2020-05-06T19:23:19Z,2020-05-11T00:33:39Z,"I have a dataframe with duplicated keys, and I need to assign values to some column of that dataframe, for some different keys. Dataframe indexes support duplicated keys, so I assume that this kind of work should be easy, if not I don't see why dataframes should be allowed to have duplicated keys. Anyway, setting the values like this:

    df.loc[key, 'column']= vector

gives me the following error:

    ValueError: Must have equal len keys and value when setting with an iterable

even if the number of times `key` appears in the index of `df` is equal to the length of `vector`. I think this should be fixed.

Thank you,
c. foschi"
324650353,21136,0.23.0 no longer produces TypeError when calling DataFrame.sort_index(level=x) on multiindex of mixed types,bschreck,open,2018-05-19T18:30:35Z,2020-05-11T00:58:21Z,"#### Code Sample, a copy-pastable example if possible

```python
# Column 'a' has both Timestamps and a string
df = pd.Series([pd.Timestamp('2011/4/9'), pd.Timestamp('2010/4/9'), '2009/4/9'], dtype='object', name='a').to_frame()
df['b'] = [1,2,3]
df['c'] = [2,1,3]
df.set_index('a', inplace=True)
df.set_index('c', append=True, inplace=True)
# No TypeError
df.sort_index(level='a')
# Raises TypeError: Cannot compare type 'Timestamp' with type 'str'
df.reset_index(level='c').sort_index(level='a')

```
#### Problem description

Sorting by mixed types should (and did in 0.22) raise a TypeError. Now it looks like it does raise a TypeError if there is only a single level in the index, but does not if there is another level that you're not sorting on.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.14.final.0
python-bits: 64
OS: Darwin
OS-release: 17.4.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.23.0
pytest: 3.2.3
pip: 10.0.1
setuptools: 36.5.0.post20170921
Cython: 0.27.3
numpy: 1.13.3
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 5.4.1
sphinx: 1.6.4
patsy: None
dateutil: 2.7.2
pytz: 2017.3
blosc: None
bottleneck: None
tables: None
numexpr: 2.5.1
feather: None
matplotlib: 2.1.1
openpyxl: None
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.9999999
sqlalchemy: 1.2.7
pymysql: None
psycopg2: 2.7.4 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: 0.1.2
fastparquet: 0.1.5
pandas_gbq: None
pandas_datareader: None

</details>
"
330690928,21386,regression: bar plot with multi-column category doesn't work anymore,soxofaan,open,2018-06-08T14:54:15Z,2020-05-11T01:00:28Z,"#### Code Sample, a copy-pastable example if possible

Doing a bar plot using a multi-column category used to work
```python
df.plot.barh(x=['fruit', 'animal'], y='size')
```

In pandas 0.22.0:
<img width=""575"" alt=""screen shot 2018-06-08 at 16 34 51"" src=""https://user-images.githubusercontent.com/44946/41164650-fa16b13e-6b3b-11e8-8586-1da29dca3688.png"">

since pandas 0.23.0:
<img width=""1113"" alt=""screen shot 2018-06-08 at 16 39 36"" src=""https://user-images.githubusercontent.com/44946/41164687-0a2b8004-6b3c-11e8-83ab-62e930ef643f.png"">

Is it intended that this stopped working? 
I kind of liked the feature. "
147568799,12871,Period Resampling Master Issue,jreback,closed,2016-04-11T22:05:34Z,2020-05-11T02:08:31Z,"Various issue surrounding resampling with a `PeriodIndex`
- [x] #12884 Return PeriodIndex if possible
- [x] #12770 recursion error on existing freq (#12874)
- [x] #12774 count (#12874)
- [x] #12868 empty PeriodIndex (#12874)
- [x] #13212 empty dataframe (#13079)
- [x] #11983 with Panels?
- [ ] #7744 convention issues
- [x] #12883 recursion with 'W' freq when upsampling (#12874)
- [x] #13083 ohlc resampling
- [x] #13224 with NaT
- [ ] #14008 MultiIndex with TimeGrouper
- [x] #14213 Verify ``loffset`` works.
- [ ] #15146 Timestamp freq -> Period"
198845666,15063,Constructing a Period having a multiple of the freq ignores the freq multiplier,dhirschfeld,open,2017-01-05T00:22:54Z,2020-05-11T02:15:20Z,"When using a multiple of the freq, the `period_range` function behaves as expected - e.g. for a  half-yearly frequency (with a `DEC` anchor) it constructs a `PeriodIndex` with periods starting on the 1st and 3rd quarters:
```python
In [27]: pd.period_range('01-Jan-2017', '01-Jan-2019', freq='2Q-DEC')[:-1]
Out[27]: PeriodIndex(['2017Q1', '2017Q3', '2018Q1', '2018Q3'], dtype='period[2Q-DEC]', freq='2Q-DEC')
```
I would expect that when constructing a `Period` with a freq of `2Q-DEC` that you should only be able to get periods starting on the 1st and 3rd quarters - i.e. any date in[Q1, Q3) would give a period starting on Q1 and any date in [Q3, Q1) would give a period starting on Q3. What currently happens is that it appears to ignore the frequency multiplier and treat the periods if they were simply straight quarters:
```python
In [31]: [pd.Period('01-Feb-2017', freq='2Q'),
    ...: pd.Period('01-May-2017', freq='2Q'),
    ...: pd.Period('01-Aug-2017', freq='2Q'),
    ...: pd.Period('01-Nov-2017', freq='2Q')]
Out[31]: 
[Period('2017Q1', '2Q-DEC'),
 Period('2017Q2', '2Q-DEC'),
 Period('2017Q3', '2Q-DEC'),
 Period('2017Q4', '2Q-DEC')]
```
i.e. I would expect instead:
```python
[Period('2017Q1', '2Q-DEC'),
 Period('2017Q1', '2Q-DEC'),
 Period('2017Q3', '2Q-DEC'),
 Period('2017Q3', '2Q-DEC')]
```"
615309059,34098,"BUG:pd.value_counts() When the values are equal, the order of equal values is invalid",sun1638650145,closed,2020-05-10T03:41:53Z,2020-05-11T02:59:44Z,"```python
import pandas as pd
df = pd.DataFrame([('a', 1), ('b', 1), ('c', 3), ('a', 1), ('b', 1)])
print(pd.value_counts(df[0], sort=True))
# you can see two situations
# b    2
# a    2
# c    1
# Name: 0, dtype: int64
# a    2
# b    2
# c    1
# Name: 0, dtype: int64
```

#### Problem description

When the values are equal, the order of equal values is invalid, Sorting is only for values, no index is processed

#### Expected Output

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.4.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : zh_CN.UTF-8

pandas           : 1.0.3
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1
setuptools       : 46.1.3
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
None

</details>
"
614801946,34071,add test for setitem from duplicate axis,CloseChoice,closed,2020-05-08T15:08:06Z,2020-05-11T07:38:44Z,"- [x] closes #34034
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
615154637,34091,DOC: use new numpy doc url,simonjayhawkins,closed,2020-05-09T11:24:47Z,2020-05-11T08:45:52Z,"- [ ] xref #33953
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
613952453,34045,BUG: eval not working with fillna,meakbiyik,open,2020-05-07T10:37:59Z,2020-05-17T18:54:13Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

```python
import pandas as pd
d = pd.DataFrame([1, 2], columns=['a'])
d.eval('((a-1)/(a-1)).fillna(0)')
```

#### Problem description
With eval, fillna function raises an obscure error

```python
>>> d.eval('(a-1)/(a-1)')
0    NaN
1    1.0
Name: a, dtype: float64
>>> d.eval('((a-1)/(a-1)).fillna(0)')
Traceback (most recent call last):
  File ""C:\Users\ErenAkbiyik\.conda\envs\work\lib\site-packages\pandas\core\frame.py"", line 3346, in eval
    return _eval(expr, inplace=inplace, **kwargs)
  File ""C:\Users\ErenAkbiyik\.conda\envs\work\lib\site-packages\pandas\core\computation\eval.py"", line 332, in eval
    parsed_expr = Expr(expr, engine=engine, parser=parser, env=env)
  File ""C:\Users\ErenAkbiyik\.conda\envs\work\lib\site-packages\pandas\core\computation\expr.py"", line 764, in __init__
    self.terms = self.parse()
  File ""C:\Users\ErenAkbiyik\.conda\envs\work\lib\site-packages\pandas\core\computation\expr.py"", line 781, in parse
    return self._visitor.visit(self.expr)
  File ""C:\Users\ErenAkbiyik\.conda\envs\work\lib\site-packages\pandas\core\computation\expr.py"", line 375, in visit
    return visitor(node, **kwargs)
  File ""C:\Users\ErenAkbiyik\.conda\envs\work\lib\site-packages\pandas\core\computation\expr.py"", line 381, in visit_Module
    return self.visit(expr, **kwargs)
  File ""C:\Users\ErenAkbiyik\.conda\envs\work\lib\site-packages\pandas\core\computation\expr.py"", line 375, in visit
    return visitor(node, **kwargs)
  File ""C:\Users\ErenAkbiyik\.conda\envs\work\lib\site-packages\pandas\core\computation\expr.py"", line 384, in visit_Expr
    return self.visit(node.value, **kwargs)
  File ""C:\Users\ErenAkbiyik\.conda\envs\work\lib\site-packages\pandas\core\computation\expr.py"", line 375, in visit
    return visitor(node, **kwargs)
  File ""C:\Users\ErenAkbiyik\.conda\envs\work\lib\site-packages\pandas\core\computation\expr.py"", line 622, in visit_Call
    res = self.visit_Attribute(node.func)
  File ""C:\Users\ErenAkbiyik\.conda\envs\work\lib\site-packages\pandas\core\computation\expr.py"", line 607, in visit_Attribute
    resolved = self.visit(value).value
AttributeError: 'Div' object has no attribute 'value'
```

#### Expected Output

```python
>>> d.eval('(a-1)/(a-1)')
0    0.0
1    1.0
Name: a, dtype: float64
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : 0.8.3
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
616121719,34123,BUG: Index with tuples comparison not working anymore,charlesdong1991,closed,2020-05-11T19:21:02Z,2020-05-17T21:16:51Z,"It seems trace back to #34081

with Index we could do something like:
```python
>>> pd.Index(['a', 'b', 'c']) == 'c'
array([False, False,  True])
```
And it's also possible to do the same for Index with tuples:
```python
>>> pd.Index([('a', 'b'), ('b', 'c'), ('c', 'a')]) == ('c', 'a')
array([False, False, True])
```
But seems not working this way anymore after #34081 , is this length check here correct (so it is designed this way on purpose)? or we should consider it a bug?

cc @jbrockmendel 

currently master:
```python
>>> pd.Index([('a', 'b'), ('b', 'c'), ('c', 'a')]) == ('c', 'a')
ValueError: ('Lengths must match', (3,), (2,), <class 'tuple'>)
```
"
619530844,34214,PERF: faster indexing for non-fastpath groupby ops,jbrockmendel,closed,2020-05-16T18:29:31Z,2020-05-17T21:35:13Z,"Per discussions about removing libreduction code, this is part of an effort to make the non-libreduction path more performant.

Performance comparisons are done by disabling fast_apply entirely and taking the two most-affected asvs:

```
import numpy as np
from pandas import DataFrame

N = 10 ** 4
labels = np.random.randint(0, 2000, size=N)
labels2 = np.random.randint(0, 3, size=N)
df = DataFrame(
    {
        ""key"": labels,
        ""key2"": labels2,
        ""value1"": np.random.randn(N),
        ""value2"": [""foo"", ""bar"", ""baz"", ""qux""] * (N // 4),
    }
)

%prun -s cumtime df.groupby([""key"", ""key2""]).apply(lambda x: 1)
PR -> 0.263 s
No optimization -> 0.308 s
master -> .039 s

%prun -s cumtime df.groupby(""key"").apply(lambda x: 1)
PR -> 0.083 s
No optimization -> 0.127 s
master -> .012 s
```
"
619576473,34219,CLN: simplify freq/nanos checks,jbrockmendel,closed,2020-05-16T23:44:31Z,2020-05-17T22:19:41Z,
619534098,34215,CLN: remove no-longer-used reverse_ops,jbrockmendel,closed,2020-05-16T18:49:43Z,2020-05-17T22:20:36Z,
617839457,34168,DOC: Fix comment,CitizenB,closed,2020-05-14T00:30:16Z,2020-05-17T22:24:23Z,"The code clears the O_NONBLOCK flag (that is, makes IO blocking), so make the comment match.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
619659957,34221,Header updates,pikamegan,closed,2020-05-17T09:59:00Z,2020-05-17T22:25:19Z,"Capitalisation changes to headers on line 33, 118, 139, 150, 190, 252, 257, and 387."
618554697,34184,REF: inherit BaseOffset in WeekOfMonthMixin,jbrockmendel,closed,2020-05-14T21:45:13Z,2020-05-17T22:27:08Z,Analogous to #34181
618520078,34181,REF: inherit BaseOffset in BusinessMixin,jbrockmendel,closed,2020-05-14T20:37:09Z,2020-05-17T22:27:35Z,"Cython's cdef classes dont play nicely with multiple inheritance, so we're going to have to turn our mixins into a chain of base classes.  This does that for BusinessMixin and BusinessHourMixin, de-duplicating some `__init__` methods along the way."
619197928,34196,REF: remove redundant get_freq function,jbrockmendel,closed,2020-05-15T19:38:12Z,2020-05-17T22:28:26Z,"We have multiple functions named get_freq/get_freq_group, trying to de-duplicate these"
619222579,34198,CLN: let Index use general concat machinery - remove Index._concat_same_dtype,jorisvandenbossche,closed,2020-05-15T20:20:25Z,2020-05-18T06:42:04Z,"Following up on the recent changes in the `concat_compat` machinery (to use the `_get_common_dtype` protocol for EAs), we can also use this in the Index class. 

Doing this also makes the `_concat_same_dtype` method unnecessary (as this is handled in `concat_compat` to use `EA._concat_same_type` when appropriate)

cc @jbrockmendel "
617354485,34154,DOC: add black badge,MarcoGorelli,closed,2020-05-13T11:03:30Z,2020-05-18T06:46:53Z,"IMO pandas benefits greatly from `black`, so I thought I'd add their badge to the readme

- [x] passes `black pandas`"
619905251,34230,CI: Fix lint error on master,mroeschke,closed,2020-05-18T04:48:33Z,2020-05-18T07:05:58Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
614652040,34064,BUG: Pandas named aggregation not working with resample agg,MarcoGorelli,closed,2020-05-08T10:21:05Z,2020-05-18T11:08:36Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame(
    {""group"": [""a""], ""col"": [1.0]}, index=[pd.to_datetime(""2019-11-04 10:32:09.737"")]
)
df.groupby(""group"").resample(""1D"").agg(open=pd.NamedAgg(""col"", ""first""))
```

#### Problem description

This returns
```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-6-34479fc13c88> in <module>
----> 1 df.groupby('group').resample('1D').agg(open=pd.NamedAgg(""col"", ""first""))

TypeError: aggregate() missing 1 required positional argument: 'func'
```


#### Expected Output
I think this would be
```python
                 group  open
group                       
a     2019-11-04     a   1.0
```


#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : 3ed7dff48bb4e8c7c0129283ff51eccea3a0f861
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-51-generic
Version          : #44~18.04.2-Ubuntu SMP Thu Apr 23 14:27:18 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.0.dev0+1502.g3ed7dff48
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200325
Cython           : 0.29.16
pytest           : 5.4.1
hypothesis       : 5.8.0
sphinx           : 3.0.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : 1.3.2
fastparquet      : 0.3.3
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.16
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0

</details>
"
422617154,25777,Summing a sparse boolean series throws an exception `TypeError: sum() got an unexpected keyword argument 'min_count'`,tsoernes,closed,2019-03-19T09:12:53Z,2020-05-18T13:03:05Z,"#### Code Sample, a copy-pastable example if possible

```python

In [202]: sparse_series.dtype
Out[203]: Sparse[bool, False]

In [208]: sparse_series.value_counts()
Out[208]: 
False    51386
True        13
Name: C_3D Printing, dtype: int64

In [209]: sparse_series.sum()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-209-8f6162710840> in <module>
----> 1 sparse_series.sum()

~/anaconda3/envs/idp/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, min_count, **kwargs)
  10929                                       skipna=skipna, min_count=min_count)
  10930         return self._reduce(f, name, axis=axis, skipna=skipna,
> 10931                             numeric_only=numeric_only, min_count=min_count)
  10932 
  10933     return set_function_name(stat_func, name, cls)

~/anaconda3/envs/idp/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)
   3613         # dispatch to ExtensionArray interface
   3614         if isinstance(delegate, ExtensionArray):
-> 3615             return delegate._reduce(name, skipna=skipna, **kwds)
   3616         elif is_datetime64_dtype(delegate):
   3617             # use DatetimeIndex implementation to handle skipna correctly

~/anaconda3/envs/idp/lib/python3.6/site-packages/pandas/core/arrays/sparse.py in _reduce(self, name, skipna, **kwargs)
   1439         kwargs.pop('numeric_only', None)
   1440         kwargs.pop('op', None)
-> 1441         return getattr(arr, name)(**kwargs)
   1442 
   1443     def all(self, axis=None, *args, **kwargs):

~/anaconda3/envs/idp/lib/python3.6/site-packages/pandas/core/arrays/sparse.py in sum(self, axis, *args, **kwargs)
   1491         sum : float
   1492         """"""
-> 1493         nv.validate_sum(args, kwargs)
   1494         valid_vals = self._valid_sp_values
   1495         sp_sum = valid_vals.sum()

~/anaconda3/envs/idp/lib/python3.6/site-packages/pandas/compat/numpy/function.py in __call__(self, args, kwargs, fname, max_fname_arg_count, method)
     54                 validate_args_and_kwargs(fname, args, kwargs,
     55                                          max_fname_arg_count,
---> 56                                          self.defaults)
     57             else:
     58                 raise ValueError(""invalid validation method ""

~/anaconda3/envs/idp/lib/python3.6/site-packages/pandas/util/_validators.py in validate_args_and_kwargs(fname, args, kwargs, max_fname_arg_count, compat_args)
    216 
    217     kwargs.update(args_dict)
--> 218     validate_kwargs(fname, kwargs, compat_args)
    219 
    220 

~/anaconda3/envs/idp/lib/python3.6/site-packages/pandas/util/_validators.py in validate_kwargs(fname, kwargs, compat_args)
    154     """"""
    155     kwds = kwargs.copy()
--> 156     _check_for_invalid_keys(fname, kwargs, compat_args)
    157     _check_for_default_values(fname, kwds, compat_args)
    158 

~/anaconda3/envs/idp/lib/python3.6/site-packages/pandas/util/_validators.py in _check_for_invalid_keys(fname, kwargs, compat_args)
    125         raise TypeError((""{fname}() got an unexpected ""
    126                          ""keyword argument '{arg}'"".
--> 127                          format(fname=fname, arg=bad_arg)))
    128 
    129 

TypeError: sum() got an unexpected keyword argument 'min_count'

In [210]: 

```
#### Problem description

Summing a sparse boolean series throws an exception `TypeError: sum() got an unexpected keyword argument 'min_count'`

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
In [198]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.8.final.0
python-bits: 64
OS: Linux
OS-release: 4.20.15-200.fc29.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: nb_NO.UTF-8
LOCALE: nb_NO.UTF-8

pandas: 0.24.1+0.g1700680.dirty
pytest: None
pip: 10.0.1
setuptools: 39.0.1.post20180504
Cython: None
numpy: 1.16.1
scipy: 1.2.0
pyarrow: None
xarray: None
IPython: 7.3.0
sphinx: None
patsy: None
dateutil: 2.6.0
pytz: 2018.4
blosc: None
bottleneck: None
tables: None
numexpr: 2.6.8
feather: None
matplotlib: 3.0.1
openpyxl: 2.6.1
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: 2.7.7 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
619648695,34220,BUG: Summing a sparse boolean series raises TypeError,simonjayhawkins,closed,2020-05-17T08:59:41Z,2020-05-18T14:05:13Z,"- [ ] closes #25777
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
619338949,34203,"REF: move get_freq_group to libfreqs, de-duplicate with Resolution.get_freq_group",jbrockmendel,closed,2020-05-16T00:32:09Z,2020-05-18T14:10:38Z,"Having both libresolution.get_freq_group and libresolution.Resolution.get_freq_group is confusing, this adds 1 line to the module-level function to make it handle the method, removes the method, and moves the function to libfrequencies, which is a better fit."
435540633,26179,BUG: Resolving fallback from skipna flag in groupby().sum(),mukundm19,closed,2019-04-21T19:37:43Z,2020-05-18T14:32:33Z,"- [x] closes #20824 
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Issue #20824 found that a fallback was occuring when df.Groupby().sum() was called with the skipna flag. This was occurring because the _cython_agg_general function was not accepting the argument, which has now been fixed. The fallback still occurs with strings in the df, however this is a deeper issue stemming from the _aggregate() call in groupby/ops.py (line 572)."
617005659,34145,"REF: separate to_time, avoid runtime imports",jbrockmendel,closed,2020-05-12T22:31:35Z,2020-05-18T15:23:52Z,
135297217,12412,Add type stub files to python's typeshed,max-sixty,closed,2016-02-22T02:41:47Z,2020-05-18T17:09:57Z,"Same issue as xarray: https://github.com/pydata/xarray/issues/771, resolved with https://github.com/pydata/xarray/pull/773.

OK to do the same to `NDFrame`?
"
617816320,34167,REF: make BaseOffset a cdef class,jbrockmendel,closed,2020-05-13T23:24:00Z,2020-05-18T18:04:15Z,"@jreback i could use help here with one last stubborn pickle/pytables test `test_read_py2_hdf_file_in_py3`

Within this legacy file we have the substring (sub-bytes?) 

`blob = b""ccopy_reg\n_reconstructor\np1\n(cpandas.tseries.offsets\nBusinessDay\np2\nc__builtin__\nobject\np3\nNtRp4\n(dp5\nS'normalize'\np6\nI00\nsS'n'\nI1\nsS'kwds'\np7\n(dp8\nsS'offset'\np9\ncdatetime\ntimedelta\np10\n(I0\nI0\nI0\ntRp11\nsb.""`

AFAICT it is doing `pickle.loads(blob)` and getting:

```
>>> pickle.loads(blob)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/copyreg.py"", line 43, in _reconstructor
    obj = object.__new__(cls)
TypeError: object.__new__(BusinessDay) is not safe, use BusinessDay.__new__()
```

Then it is falling back and for reasons not clear to me is calling `to_offset(blob)` which raises `ValueError` which is what pytest shows us.

IIUC there is a way to register functions for unpickling, but _none_ of the functions in pd.compat.pickle_compat are getting called in this process, so I'm at a loss for how to do this.  Any ideas?"
620101830,34234,BUG: `astype` on index drops name,giuliobeseghi,closed,2020-05-18T10:38:08Z,2020-05-18T18:59:41Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

Index name is dropped when using `astype`

#### Code Sample, a copy-pastable example

```python
import pandas as pd

index = pd.Float64Index(range(10), name='my_index')
int_index = index.astype(int)
print(int_index.name)  # doesn't print anything
```

#### Problem description

From the docs of `astype`, I wouldn't expect the `name` attribute to be dropped:

> Cast a pandas object to a specified dtype ``dtype``.

#### Expected Output
`map` works.

```python
int_index = index.map(int)
print(int_index.name)  # prints original index name
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : 0.29.17
pytest           : 5.4.1
hypothesis       : 5.11.0
sphinx           : 3.0.3
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.16
tables           : 3.6.1
tabulate         : 0.8.3
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.8
numba            : 0.49.0

</details>
"
620464960,34239,CI: ValueError: Must pass 2-d input in constructor,charlesdong1991,closed,2020-05-18T19:52:32Z,2020-05-18T19:54:44Z,"seems df construction fail at some special cases, have seen the errors in multiple PRs

```
@pytest.mark.xfail(_is_numpy_dev, reason=""Interprets list of frame as 3D"")
    def test_constructor_list_frames(self):
        # see gh-3243
>       result = DataFrame([DataFrame()])

pandas/tests/frame/test_constructors.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pandas/core/frame.py:514: in __init__
    mgr = init_ndarray(data, index, columns, dtype=dtype, copy=copy)
pandas/core/internals/construction.py:190: in init_ndarray
    values = _prep_ndarray(values, copy=copy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

values = array([], shape=(1, 0, 0), dtype=float64), copy = False

    def _prep_ndarray(values, copy: bool = True) -> np.ndarray:
        if not isinstance(values, (np.ndarray, ABCSeries, Index)):
            if len(values) == 0:
                return np.empty((0, 0), dtype=object)
            elif isinstance(values, range):
                arr = np.arange(values.start, values.stop, values.step, dtype=""int64"")
                return arr[..., np.newaxis]
    
            def convert(v):
                return maybe_convert_platform(v)
    
            # we could have a 1-dim or 2-dim list here
            # this is equiv of np.asarray, but does object conversion
            # and platform dtype preservation
            try:
                if is_list_like(values[0]) or hasattr(values[0], ""len""):
                    values = np.array([convert(v) for v in values])
                elif isinstance(values[0], np.ndarray) and values[0].ndim == 0:
                    # GH#21861
                    values = np.array([convert(v) for v in values])
                else:
                    values = convert(values)
            except (ValueError, TypeError):
                values = convert(values)
    
        else:
    
            # drop subclass info, do not copy data
            values = np.asarray(values)
            if copy:
                values = values.copy()
    
        if values.ndim == 1:
            values = values.reshape((values.shape[0], 1))
        elif values.ndim != 2:
>           raise ValueError(""Must pass 2-d input"")
E           ValueError: Must pass 2-d input
```"
569474273,32197,TYP: Add annotation for df.pivot,charlesdong1991,closed,2020-02-23T10:31:51Z,2020-05-18T22:53:40Z,"- [ ] xref #30928 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
619575934,34218,CLN: remove ABCPeriod,jbrockmendel,closed,2020-05-16T23:40:03Z,2020-05-18T23:20:50Z,
619768310,34227,REF: make Tick entirely a cdef class,jbrockmendel,closed,2020-05-17T18:29:13Z,2020-05-18T23:48:29Z,Sits on top of #34167
616777930,34137,Revert: REF: do length-checks in boilerplate decorator,charlesdong1991,closed,2020-05-12T16:05:52Z,2020-05-19T08:08:40Z,"closes #34123 

As discussed in #34123 
let's revert this PR34081 first since it has potentially big impact on some cases in ops.

cc @jbrockmendel @jorisvandenbossche "
618465531,34179,CLN/TST: Remove Base Class and all subclasses and fixturize data,charlesdong1991,closed,2020-05-14T19:00:01Z,2020-05-19T08:11:25Z,"- [x] closes #30486

This PR will be able to finally close 30486 in my opinion (even a little beyond the scope ^^), and after this, all classes adhered to Base in `tests/window` folder is removed.

Although the PR is huge, it actually is quite simple, basically it does:
1. Turn needed data from `_create_data()` in `Base` class as fixtures
2. Remove Base class
3. Change the shared methods which need to inherit data from Base class to use arguments, and in the real tests, assign corresponding fixtures to those arguments accordingly.
4. Remove all sub-classes which inherit from Base class, and now they all turn to independent test functions.

cc @jreback 

And there is no test missing or added, so number of tests remain unchanged for all:
This is the result after i did a quick run for all changed tests to check the number of tests:
```
test_api: 15
test_ewm: 6
test_expanding: 21
test_rolling: 150
test_window: 23
test_moments_ewm: 22
test_moments_rolling: 253
test_moments_consitency_ewm: 4310
test_moments_consistenty_expanding: 1340
test_moments_consistency_rolling: 10881
```
"
595352325,33336,PLT: Order of plots does not preserve the column orders in df.hist,charlesdong1991,closed,2020-04-06T19:00:09Z,2020-05-19T08:12:15Z,"- [x] closes #29235
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
620743552,34245,"DOC, ENH: Clarify ExcelFile's available engine compatibility with file types in...",miguelmarques1904,closed,2020-05-19T07:26:04Z,2020-05-19T08:14:24Z,"the docstring. Add check to the ExcelFile class that verifies it. GH34237.

DOC: I edited the available ExcelFile's docstring to add information about the recent pyxlsb engine. I also added some compatibility information between the available engines and file types.

ENH: Passing an incompatible filetype-engine combo to the read_excel method would result in an error at line 824/845 (`self._reader = self._engines[engine](self._io)`), which would be engine-dependent and provide little information to the user about what went wrong. The changes proposed check for compatibility in-class and provide some feedback in case of error.

- [x] closes #34237 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
589774903,33115,REGR: unhelpful error message with np.min on unordered Categorical,simonjayhawkins,closed,2020-03-29T11:53:30Z,2020-05-19T08:41:32Z,"xref #28949

on master

```
>>> import numpy as np
>>> import pandas as pd
>>>
>>> pd.__version__
'1.1.0.dev0+1008.g60b0e9fbc'
>>>
>>> cat = pd.Categorical([""a"", ""b"", ""c"", ""b""], ordered=False)
>>> cat
[a, b, c, b]
Categories (3, object): [a, b, c]
>>>
>>> np.min(cat)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<__array_function__ internals>"", line 6, in amin
  File ""C:\Users\simon\Anaconda3\envs\pandas-dev\lib\site-packages\numpy\core\fromnumeric.py"", line 2793, in amin
    keepdims=keepdims, initial=initial, where=where)
  File ""C:\Users\simon\Anaconda3\envs\pandas-dev\lib\site-packages\numpy\core\fromnumeric.py"", line 88, in _wrapreduction
    return reduction(axis=axis, out=out, **passkwargs)
  File ""C:\Users\simon\pandas\pandas\util\_decorators.py"", line 212, in wrapper
    return func(*args, **kwargs)
TypeError: min() got an unexpected keyword argument 'axis'
```

on 0.25.3

```
>>> import numpy as np
>>> import pandas as pd
>>>
>>> pd.__version__
'0.25.3'
>>>
>>> cat = pd.Categorical([""a"", ""b"", ""c"", ""b""], ordered=False)
>>> cat
[a, b, c, b]
Categories (3, object): [a, b, c]
>>>
>>> np.min(cat)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\simon\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py"", line 2618, in amin
    initial=initial)
  File ""C:\Users\simon\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py"", line 84, in _wrapreduction
    return reduction(axis=axis, out=out, **passkwargs)
  File ""C:\Users\simon\Anaconda3\lib\site-packages\pandas\core\arrays\categorical.py"", line 2278, in min
    self.check_for_ordered(""min"")
  File ""C:\Users\simon\Anaconda3\lib\site-packages\pandas\core\arrays\categorical.py"", line 1586, in check_for_ordered
    ""Categorical to an ordered one\n"".format(op=op)
TypeError: Categorical is not ordered for operation min
you can use .as_ordered() to change the Categorical to an ordered one
```

"
620843293,34250,Backport PR #33968 on branch 1.0.x (CI: Bump numpydev URL),meeseeksmachine,closed,2020-05-19T09:58:07Z,2020-05-19T11:02:18Z,Backport PR #33968: CI: Bump numpydev URL
611858652,33968,CI: Bump numpydev URL,TomAugspurger,closed,2020-05-04T13:02:42Z,2020-05-19T11:02:37Z,"NumPy and scipy are uploading their wheels to a new URL.
"
88354046,10355,BUG: STD modifies groupby target column when as_index=False,jxrossel,closed,2015-06-15T08:14:27Z,2020-05-19T12:56:49Z,"xref #14547 for other tests

In pandas 0.16.2 (and already in 0.16.0), using std() for aggregation after a groupby( 'my_column', as_index=False) modifies 'my_column' by taking its sqrt(). Example:

``` python
df = pandas.DataFrame({
               'a' : [1,1,1,2,2,2,3,3,3],
               'b' : [1,2,3,4,5,6,7,8,9],
})
df.groupby('a',as_index=False).std()
Out[5]: 
          a  b
0  1.000000  1
1  1.414214  1
2  1.732051  1
```

The square root values of 'a' are returned instead of 1, 2, 3.
## INSTALLED VERSIONS

commit: None
python: 2.7.9.final.0
python-bits: 32
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 45 Stepping 7, GenuineIntel
byteorder: little
LC_ALL: None
LANG: fr_CH

pandas: 0.16.2
nose: 1.3.4
Cython: 0.22
numpy: 1.9.2
scipy: 0.15.1
statsmodels: None
IPython: 3.0.0
sphinx: 1.3.1
patsy: 0.3.0
dateutil: 2.4.0
pytz: 2015.2
bottleneck: None
tables: 3.1.1
numexpr: 2.4
matplotlib: 1.4.3
openpyxl: None
xlrd: 0.9.3
xlwt: None
xlsxwriter: 0.7.1
lxml: None
bs4: 4.3.2
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 0.9.9
pymysql: None
psycopg2: None
"
600253481,33564,DOC: rename_axis 'columns' param,Toussaic,closed,2020-04-15T12:16:54Z,2020-05-19T14:57:52Z,"Example :
```python
import pandas as pd
import numpy as np

s1 = pd.Series(np.random.randn(3), name=""First"", index=['a', 'b', 'c'])
s1.rename(columns=""My Column"")
```

Except :
TypeError: rename_axis() got an unexpected keyword argument ""columns""

Description :
The documentation shows a ""columns"" param which is not allowed for a Series. There should be a note to explain this parameter only apply for DataFrame.

Link :
https://github.com/pandas-dev/pandas/blob/3adf3340453d6704d4a2cb47058214cc697a7d29/pandas/core/generic.py#L1110-L1279

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : fr_FR.cp1252

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>"
619159698,34194,DOC: Add note about columns parameter not relevant for Series in rename_axis(),gabsmoreira,closed,2020-05-15T18:26:59Z,2020-05-19T15:15:05Z,"- [x] closes #33564
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
583161305,32779,PERF: block-wise arithmetic for frame-with-frame,jbrockmendel,closed,2020-03-17T17:01:08Z,2020-05-19T15:48:31Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
621095479,34260,ENH: One-hot decoding,clbarnes,closed,2020-05-19T15:57:09Z,2020-05-19T16:09:20Z,"#### Is your feature request related to a problem?

`pd.get_dummies` provides a way to turn a sequence of category-like data into a one-hot encoded data frame. However, there is no easy way (to my knowledge) of going in the other direction: given a boolean dataframe where the row sums are all 1, produce a categorical series. This task is particularly valuable for serialisation.

#### Describe the solution you'd like

Some way of constructing a `Categorical` array from a one-hot encoded dataframe (view). To avoid piling extra functionality into the existing constructor, a class method could be used.

Scratch implementation:

```python
import numpy as np 
import pandas as pd

class Categorical:
    ...
    
    @classmethod
    def from_dummies(cls, df: pd.DataFrame, **kwargs):
        onehot = df.astype(bool)

        if (onehot.sum(axis=1) > 1).any():
            raise ValueError(""Some rows belong to >1 category"")

        index_into = pd.Series([np.nan] + list(onehot.columns))
        mult_by = np.arange(1, len(index_into))

        indexes = (onehot.astype(int) * mult_by).sum(axis=1)
        values = index_into[indexes]

        return cls(values, df.columns, **kwargs)
```

#### Describe alternatives you've considered

- A free function (less discoverable, less self-documenting)
- Importing scikit-learn

#### Additional context

[sklearn.preprocessing.OneHotEncoder.inverse_transform](https://github.com/scikit-learn/scikit-learn/blob/fd237278e/sklearn/preprocessing/_encoders.py#L473)
"
599846521,33549,[WIP] Add remote file io using fsspec.,jrderuiter,closed,2020-04-14T20:42:38Z,2020-05-19T20:10:05Z,"- [x] closes #33452
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
621071148,34258,CI: temporarily disable interval in feather tests (pyarrow 0.17.1 regression),jorisvandenbossche,closed,2020-05-19T15:24:38Z,2020-05-19T20:26:17Z,"xref https://github.com/pandas-dev/pandas/issues/34255

Temporary solution to get green CI. "
614368385,34059,DOC:GH34026,moaraccounts,closed,2020-05-07T21:53:34Z,2020-05-19T20:54:34Z,"- [x] closes #34026 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
615026067,34083,CLN: .values -> ._values,jbrockmendel,closed,2020-05-08T22:51:37Z,2020-05-19T21:32:32Z,
618671289,34185,"BUG: Inconsistent behaviour when averaging Decimals, floats and ints",cuchoi,closed,2020-05-15T03:27:26Z,2020-05-19T22:09:37Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

---

```python
from decimal import Decimal

df = pd.DataFrame({'col_1': [Decimal(1.5), Decimal(4.0)], 'col_2': [5.0, 10.0]})
df.mean(axis=1) # returns 5, 10 -- ignoring the decimals types in the averaging

df2 = pd.DataFrame({'col_1': [Decimal(1.5), Decimal(4.0)], 'col_2': [5, 10]})
df2.mean(axis=1) # returns 3.25, 7 -- includes the decimals types in the averaging
```

#### Problem description

There is inconsistent behaviour on how Decimal is being averaging depending if it is averaged to an int vs a float. Is it expected that the two dataframes above return different results?

#### Expected Output

I would expect in both cases to see `3.25` and  `7` as the mean of the rows.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.16
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.8
numba            : None

</details>
"
519316057,29463,"""Cannot compare tz-naive and tz-aware datetime-like objects"" when querying DatetimeIndex",hjfreyer,closed,2019-11-07T14:54:00Z,2020-05-20T05:40:36Z,"## Code Sample, a copy-pastable example if possible

```python
import pandas as pd

df = pd.DataFrame({
    'val': range(10),
    'time': pd.date_range(start='2019-01-01', freq='1d', periods=10, tz='UTC')
})

# This works.
df.query('""2019-01-03 00:00:00+00"" < time')

# This raises an exception.
df.set_index('time').query('""2019-01-03 00:00:00+00"" < time')

# Perhaps interestingly, this works.
df.set_index('time').reset_index().query('""2019-01-03 00:00:00+00"" < time')

```
#### Problem description

query shouldn't behave differently when the thing being queried is an index vs a column. Also, the error just seems objectively incorrect, as both values are tz-aware to start with (perhaps it's getting lost along the way).

May be related to #26236 , but the repro is different.

#### Expected Output
Not an exception.


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.7.final.0
python-bits: 64
OS: Linux
OS-release: 4.3.5-smp-821.23.0.0
machine: x86_64
processor: 
byteorder: little
LC_ALL: en_US.UTF-8
LANG: None
LOCALE: en_US.UTF-8

pandas: 0.24.1
pytest: None
pip: None
setuptools: unknown
Cython: None
numpy: 1.16.4
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 2.0.0
sphinx: None
patsy: 0.4.1
dateutil: 2.8.0
pytz: 2019.3
blosc: None
bottleneck: None
tables: 3.5.2
numexpr: 2.6.10dev0
feather: None
matplotlib: 3.0.3
openpyxl: None
xlrd: 1.2.0
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.3
s3fs: None
fastparquet: None
pandas_gbq: 0+unknown
pandas_datareader: None
gcsfs: None


</details>
"
613239575,34021,TST: query with timezone aware index & column,vipulr8,closed,2020-05-06T11:14:28Z,2020-05-20T05:47:18Z,"- [X] closes #29463
- [X] tests added / passed
- [X] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
301782026,19965,Missing labels with Timedelta on x-axis ,dniku,open,2018-03-02T14:23:19Z,2020-05-20T10:13:23Z,"#### Code Sample, a copy-pastable example if possible

Please see the entire sample [in this Gist](https://gist.github.com/Pastafarianist/e61f9ace6dfe849072c7bd45a546e640). Here I'm only copying the code and the output (no data).

```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('pandas_timedelta_plot.csv')
df['deltas'] = pd.to_timedelta(df['deltas'])

df.plot(x='deltas', y='data')
plt.show()
```

![image](https://user-images.githubusercontent.com/1315874/36903231-29592102-1e3e-11e8-8cf5-e84af561706c.png)

#### Problem description

The labels are missing with Timedelta on x-axis.

#### Expected Output

The labels should be there.

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.4.final.0
python-bits: 64
OS: Linux
OS-release: 4.14.21-1-MANJARO
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.22.0
pytest: None
pip: None
setuptools: 38.5.1
Cython: 0.27.3
numpy: 1.14.0
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2018.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.1.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
```

</details>
"
620803652,34246,Backport PR #34053 on branch 1.0.x (more informative error message with np.min or np.max on unordered Categorical),simonjayhawkins,closed,2020-05-19T08:58:42Z,2020-05-20T12:13:39Z,"xref #34053, #33115

regression in 1.0.0 "
620811559,34247,Backport PR #34049 on branch 1.0.x (Bug in Series.groupby would raise ValueError when grouping by PeriodIndex level),simonjayhawkins,closed,2020-05-19T09:10:08Z,2020-05-20T12:14:19Z,"xref #34049, #34010

regression in 1.0.0"
595237874,33327,BUG: User-facing AssertionError in Series.to_timestamp,TomAugspurger,closed,2020-04-06T16:03:45Z,2020-05-20T14:05:36Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python

In [5]: import pandas as pd

In [6]: pd.Series([0]).to_timestamp()
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-6-1a9d08d7daba> in <module>
----> 1 pd.Series([0]).to_timestamp()

~/sandbox/pandas/pandas/core/series.py in to_timestamp(self, freq, how, copy)
   4565             new_values = new_values.copy()
   4566
-> 4567         assert isinstance(self.index, (ABCDatetimeIndex, ABCPeriodIndex))
   4568         new_index = self.index.to_timestamp(freq=freq, how=how)
   4569         return self._constructor(new_values, index=new_index).__finalize__(

AssertionError:
```

#### Problem description

Users shouldn't see AssertionErrors like this, since they may be disabled when the interpreter starts up.

#### Expected Output

We probably want this to be a `TypeError`.
"
614724454,34067,BUG : Series.to_timestamp and Series.to_period raise user-facing AssertionError,vipulr8,closed,2020-05-08T12:52:34Z,2020-05-20T14:05:42Z,"- [X] closes #33327
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
246542822,17114,API documentation should state return types,fulldecent,closed,2017-07-29T20:13:26Z,2020-05-20T14:20:51Z,"As an example, let's see the documentation for `pandas.DataFrame.dtypes`:

Documentation:

> `DataFrame.dtypes`
> Return the dtypes in this object.
>
> https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dtypes.html

This documentation is insufficient to describe the function. Perhaps a better documentation would state something like:

> `DataFrame.dtypes`
>
> Return a series of dtypes in this object.
> Return type is `pandas.core.series.Series` with elements of `numpy.dtype`.

And those two types should link to related to documentation if available."
620661943,34243,CLN: remove get_base_alias,jbrockmendel,closed,2020-05-19T04:10:54Z,2020-05-20T15:12:15Z,There are way too many really-similar functions like this.  This trims one of them and de-duplicates 4 near-identical usages in the plotting code.
620638955,34241,CLN: Remove util.is_offset_object,jbrockmendel,closed,2020-05-19T02:59:48Z,2020-05-20T15:12:58Z,Do isinstance checks on _Timestamp instead of ABCTimestamp where possible (less mro traversal)
621098694,34262,REF: FY5253Mixin to share methods,jbrockmendel,closed,2020-05-19T16:01:33Z,2020-05-20T15:14:16Z,
621166436,34264,REF: make YearOffset a cdef class,jbrockmendel,closed,2020-05-19T17:42:39Z,2020-05-20T15:14:41Z,
619196887,34195,"CLN: tslibs typing, avoid private funcs",jbrockmendel,closed,2020-05-15T19:36:20Z,2020-05-20T15:15:08Z,
621057770,34257,ENH: DataFrame.replace does not work with dictionary subclasses,timgeb,open,2020-05-19T15:08:56Z,2020-05-20T15:18:28Z,"#### Is your feature request related to a problem?

I want to use `DataFrame.replace` with a dictionary that has a custom `__getitem__` logic. The current behavior is easy to demonstrate with a `defaultdict` (see code below).

#### Describe the solution you'd like

I would like `DataFrame.replace` to honor the `__getitem__` method of dict-like arguments.

#### API breaking implications

I don't know.

#### Code

```python
>>> from collections import defaultdict
>>> import pandas as pd
>>> df = pd.DataFrame([1, 2, 3])
>>> d = defaultdict(int, {1: -1, 2: -2})
>>> df.replace(d)
   0
0 -1
1 -2
2  3
>>> d[3]
0
```
Expected behavior: the `3` inside the DataFrame should be replaced with `0`, because `d[3]` returns `0`.
"
582636732,32762,"TypeError: searchsorted requires compatible dtype or scalar, not Series",MislavSag,closed,2020-03-16T22:32:45Z,2020-05-20T19:06:04Z,"#### Code Sample, a copy-pastable example if possible

```
x = ['1998-01-07 13:17:00', '1998-01-07 13:18:00', '1998-01-07 13:19:00', '1998-01-07 13:20:00',
     '1998-01-07 13:21:00', '1998-01-07 13:22:00', '1998-01-07 13:23:00', '1998-01-07 13:24:00',
     '1998-01-07 13:25:00', '1998-01-07 13:26:00']
df = pd.DataFrame(x)
df.set_index(df.iloc[:, 0], inplace=True)
df.index = pd.DatetimeIndex(df.index.values)
```
#### Problem description

I want to apply `searchsorted` function to df index:
`df.index.searchsorted(pd.Series(np.array([df.index[0], df.index.max()])))`
but it returns an error:

> TypeError: searchsorted requires compatible dtype or scalar, not Series

Why it doesn't find the nearest datetimeindex in time? I think this works with older versions of pandas.

I use the lastest version of pandas (1.0.3.)

"
620273992,34238,API: implement ObjectIndex to take place of Index,jbrockmendel,closed,2020-05-18T14:51:54Z,2020-05-21T01:19:44Z,"Then we would never actually return an Index object.

We could then make ABCIndex behave like ABCIndexClass, and get rid of the latter.

Briefly discussed in #34159"
482938906,28038,DOC: Remove docs code header and use explicit imports in the user guide,datapythonista,open,2019-08-20T15:41:15Z,2020-05-21T09:33:31Z,"I've been discussing with the Binder team, and looks like it'll be possible soon to make the pandas examples in the docs runnable directly from the docs.

I think this will be amazing, but I think it's one more reason to start being explicit and self-contained with the docs. One option is of course that the trickiness we have now on importing things translate to Binder, and we automatically inject code for imports (and may be seeds...).

But my opinion is that the right thing is to start being explicit on what we run in the examples."
619458830,34207,TYP: add return types to read_sql|read_sql_query|read_sql_table,topper-123,closed,2020-05-16T12:12:04Z,2020-05-21T09:58:06Z,"Adds return types to read_sql|read_sql_query|read_sql_table.

I'd like to take on the other pd.read_* functions in follow-ups."
622020451,34279,"Reopened: ""TypeError: searchsorted requires compatible dtype or scalar, not ndarray """,Baynez,closed,2020-05-20T19:10:55Z,2020-05-21T13:53:43Z,"""TypeError: searchsorted requires compatible dtype or scalar, not Series""
appears again when applying timeindex data:

``` 
#sample data
data = ['1998-01-07 13:17:00', '1998-01-07 13:18:00', '1998-01-07 13:19:00', '1998-01-07 13:20:00',
     '1998-01-07 13:21:00', '1998-01-07 13:22:00', '1998-01-07 13:23:00']
df = pd.DataFrame(x)
df.set_index(df.iloc[:, 0], inplace=True)
df.index = pd.DatetimeIndex(df.index.values) 
 
# this is the line
df.index.searchsorted(pd.Series(np.array([df.index[0], df.index.max()]))) 
  
```
How to solve this?"
618993964,34187,CLN: deduplicate __setitem__ and _reduce on masked arrays,jorisvandenbossche,closed,2020-05-15T14:08:30Z,2020-05-21T14:02:29Z,"For `__setitem__`, the only difference was that they both call to their own ""coercing"" function, so I made this available as a helper method, so it the rest of the implementation of `__setitem__` can be shared.

For `_reduce`, everything was the same, BooleanArray just has an additional check for `any` and `all`."
622243707,34287,REF: put cast_from_unit in conversion to de-circularize cimports,jbrockmendel,closed,2020-05-21T04:47:19Z,2020-05-21T16:01:12Z,
621475407,34269,CLN: Move rolling helper functions to where they are used,mroeschke,closed,2020-05-20T05:39:14Z,2020-05-21T16:26:48Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Added some docstrings as well"
614790624,34070,DOC: clarify boxplot whiskers extension,neutrinoceros,closed,2020-05-08T14:49:49Z,2020-05-21T18:17:42Z,"As I'm getting started with box plots, I found confusing that the `DataFrame.boxplot` docstring didn't mention that whiskers extend _at most_ to 1.5 * IQR away from the box's edges.

- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry -> is it required for such a minor change ?
"
622715823,34300,ENH: add an option to not install the tests,jamestwebber,closed,2020-05-21T19:07:30Z,2020-05-21T19:26:12Z,"We've been using pandas as part of a project running on AWS lambda, which means we need it installed as a layer, and layers need to have a total size (when zipped) of 50MB. This is tricky with libraries like pandas which are pretty big (installed size for me is 41 MB by itself, although it compresses to less).

There are existing layers for pandas which we've been using, but recently I looked into getting all of our dependencies into a single layer that we generated ourselves. When I looked into it I was surprised to find a lot of space in pandas is from the tests: 17 MB, so nearly half, is in `pandas/tests`.

#### Describe the solution you'd like

The feature request is to add some logic to `setup.py` to make the tests an optional installation, which will save a lot of space for applications that need it.

#### API breaking implications

This shouldn't break the API unless tests are being imported/used in the main code (I certainly hope they aren't)

#### Describe alternatives you've considered

I can manually remove the tests when I package up the layer, as can anyone else who wants to save the space, but it'd be nice if this was a simple option."
551648180,31113,BUG: pd.concat fails when handling MultiIndex if repeated index exists,charlesdong1991,closed,2020-01-17T21:49:53Z,2020-05-21T19:35:00Z,"xref #20565 

- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
313932206,20674,Fixture for np.* Functions Used in Transform,WillAyd,open,2018-04-13T00:37:37Z,2020-05-22T03:24:33Z,xref #20655 ideally want to set up a fixture for all of the np.* aggregation functions and make sure there is at least a test which consumes those functions in a transformation
495398002,28506,TST: DataFrame operations with ExtensionArrays,jorisvandenbossche,open,2019-09-18T18:54:50Z,2020-05-22T03:37:02Z,"@jbrockmendel the use case I was trying to explain in words is this (which currently works):

```
In [4]: df1 = pd.DataFrame({'a': DecimalArray(make_data()[:5]), 'b': DecimalArray(make_data()[:5])}) 

In [5]: df1 
Out[5]: 
                                                   a                                                  b
0  Decimal: 0.12461374221703280795736645814031362...  Decimal: 0.52335889201915997137604108502273447...
1  Decimal: 0.31309497234009509014640570967458188...  Decimal: 0.00133572791466307627672449598321691...
2  Decimal: 0.84547604466523051947035582998069003...  Decimal: 0.73944523257987115893996588056324981...
3  Decimal: 0.78411203581007593577112402272177860...  Decimal: 0.60960149820278708432397252181544899...
4  Decimal: 0.73864636517230020107405152884894050...  Decimal: 0.53212511863152778257557429242297075...

In [6]: df1.dtypes 
Out[6]: 
a    decimal
b    decimal
dtype: object

In [7]: df2 = pd.DataFrame({'a': np.arange(5), 'b': np.arange(5)})

In [8]: df1 + df2 
Out[8]: 
                                         a                                        b
0  Decimal: 0.1246137422170328079573664581  Decimal: 0.5233588920191599713760410850
1   Decimal: 1.313094972340095090146405710   Decimal: 1.001335727914663076276724496
2   Decimal: 2.845476044665230519470355830   Decimal: 2.739445232579871158939965881
3   Decimal: 3.784112035810075935771124023   Decimal: 3.609601498202787084323972522
4   Decimal: 4.738646365172300201074051529   Decimal: 4.532125118631527782575574292

In [9]: (df1 + df2).dtypes 
Out[9]: 
a    decimal
b    decimal
dtype: object
```

As you mentioned, in those case the ops code would keep doing that column by column (or 1D block by 1D block) and the right integer 2D block will be splitted. 
But I thought you mentioned something about such a case currently not being covered by the tests? (in which case it would be good to add some)"
620208030,34236,BUG: Fix validation script not printing docstring is correct (#34228),rtm010,closed,2020-05-18T13:25:48Z,2020-05-22T09:41:49Z,"- [x] closes #34228 
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
503298543,28813,Diff on sparse dtype,thedimlebowski,closed,2019-10-07T08:17:59Z,2020-05-22T09:58:31Z,"Correct behaviour without Sparse[int]:

```python
a = [[0, 1], [1, 0]]
print(pd.DataFrame(a).diff())
```
yields:
```
     0    1
0  NaN  NaN
1  1.0 -1.0
```

Current behaviour (doesn't work):
```python
a = [[0, 1], [1, 0]]
print(pd.DataFrame(a, dtype='Sparse[int]').diff())
```
yields:

```
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-34-3cc99aac7c69> in <module>
      3 print(pd.DataFrame(a).diff())
      4 
----> 5 print(pd.DataFrame(a, dtype='Sparse[int]').diff())

~/.local/lib/python3.6/site-packages/pandas/core/frame.py in diff(self, periods, axis)
   6592         """"""
   6593         bm_axis = self._get_block_manager_axis(axis)
-> 6594         new_data = self._data.diff(n=periods, axis=bm_axis)
   6595         return self._constructor(new_data)
   6596 

~/.local/lib/python3.6/site-packages/pandas/core/internals/managers.py in diff(self, **kwargs)
    564 
    565     def diff(self, **kwargs):
--> 566         return self.apply(""diff"", **kwargs)
    567 
    568     def interpolate(self, **kwargs):

~/.local/lib/python3.6/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)
    436                     kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy)
    437 
--> 438             applied = getattr(b, f)(**kwargs)
    439             result_blocks = _extend_blocks(applied, result_blocks)
    440 

~/.local/lib/python3.6/site-packages/pandas/core/internals/blocks.py in diff(self, n, axis)
   1328     def diff(self, n, axis=1):
   1329         """""" return block for the diff of the values """"""
-> 1330         new_values = algos.diff(self.values, n, axis=axis)
   1331         return [self.make_block(values=new_values)]
   1332 

~/.local/lib/python3.6/site-packages/pandas/core/algorithms.py in diff(arr, n, axis)
   1943 
   1944     na_indexer = [slice(None)] * arr.ndim
-> 1945     na_indexer[axis] = slice(None, n) if n >= 0 else slice(n, None)
   1946     out_arr[tuple(na_indexer)] = na
   1947 

IndexError: list assignment index out of range```"
619808002,34228,DOC: Validation script doesn't report that the docstring is correct,datapythonista,closed,2020-05-17T22:08:37Z,2020-05-22T10:00:22Z,"When executing for example:
```
./scripts/validate_docstrings.py pandas.DataFrame.head
```
The script, after the docstring, should report:
```
################################################################################
################################## Validation ##################################
################################################################################
Docstring for DataFrame.head correct. :)
```
But for some reason, the sentence `Docstring for DataFrame.head correct. :)` is not being printed.

This is the line that is not being executed: https://github.com/pandas-dev/pandas/blob/master/scripts/validate_docstrings.py#L364"
622578937,34294,fix to is_string,ieaves,closed,2020-05-21T15:26:06Z,2020-05-22T10:00:40Z,"- [X] closes #34295
- [x] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
622584271,34295,BUG: `is_string_dtype` incorrectly identifies categorical data,ieaves,closed,2020-05-21T15:30:07Z,2020-05-22T10:01:14Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

```python
s = pd.Series(['a', 'b', 'c'], dtype='category')
pdt.is_string_dtype(s)
>>> True
```

#### Problem description

The current implementation of `is_string_dtype` incorrectly evaluates to True for categorical series.

#### Expected Output

```python
s = pd.Series(['a', 'b', 'c'], dtype='category')
pdt.is_string_dtype(s)
>>> False
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.0.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.4.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 44.0.0.post20200106
Cython           : 0.29.14
pytest           : 5.3.2
hypothesis       : None
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
590725557,33160,DOC: Make doc decorator a class and replace Appender by doc,HH-MWB,closed,2020-03-31T01:58:25Z,2020-05-22T10:05:05Z,"- [x] work for #31942
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
598993626,33528,BUG: Grouped-by column loses name when empty list of aggregations is specified,echozzy629,closed,2020-04-13T16:39:41Z,2020-05-22T10:22:12Z,"- [ ] closes #32580
- [ ] 0 tests added / 0 passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
598652162,33514,DOC: Add missing descriptions in See Also section,devjeetr,closed,2020-04-13T03:41:51Z,2020-05-22T10:29:05Z,"Fixed SR04 for `pandas/core/window/rolling.py`
- [X] xref #28792
- [ ] 0 tests added / 0 passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
600783643,33581,Issue 30999 fix,Dxin-code,closed,2020-04-16T06:34:05Z,2020-05-22T10:39:09Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
585763364,32908,"WIP: Draft strawman implementation of draft strawman data frame ""__dataframe__"" interchange protocol for discussion",simonjayhawkins,closed,2020-03-22T16:54:47Z,2020-05-22T10:44:08Z,xref https://github.com/wesm/dataframe-protocol/pull/1
583385125,32796,BUG: created check and warning for merging dataframes on unequal inde…,SeeminSyed,closed,2020-03-18T00:41:29Z,2020-05-22T10:52:20Z,"…x levels (#13094)

- [ ] closes #13094
- [ ] tests added / passed
- [ ] warn users when merging between different index levels
"
599214168,33536,ENH: Clear index cache after reindex,BaiBaiHi,closed,2020-04-14T00:26:37Z,2020-05-22T10:56:50Z,"Add flag to clear index cache after reindex.

By default, reindex causes index to cache values which potentially increases the memory consumption significantly in the case of multiindexes. 

```
In [2]: idx = pd.MultiIndex.from_product([pd.date_range('2010-01-01', '2015-01-01'), range(1000)], names=['date', 'id'])
In [3]: idx2 = pd.MultiIndex.from_product([pd.date_range('2010-01-01', '2015-01-01'), range(500)], names=['date', 'id'])
In [4]: df =  pd.DataFrame({'a': 1}, index=idx)
In [5]: df.memory_usage(deep=True, index=True) # Original Memory Usage
Out[5]:
Index     7453600
a        14616000
dtype: int64
In [6]: df.reindex(idx2)  # df is still the same as original. 
In [7]: df.memory_usage(deep=True, index=True)  # Memory usage after reindex
Out[7]:
Index    91339680
a        14616000
dtype: int64
```

With  clear_cache=True

```
In [20]: idx = pd.MultiIndex.from_product([pd.date_range('2010-01-01', '2015-01-01'), range(1000)], names=['date', 'id'])
In [21]: idx2 = pd.MultiIndex.from_product([pd.date_range('2010-01-01', '2015-01-01'), range(500)], names=['date', 'id'])
In [22]: df =  pd.DataFrame({'a': 1}, index=idx)
In [23]: df.memory_usage(deep=True, index=True) # Original Memory Usage
Out[23]:
Index     7453600
a        14616000
dtype: int64
In [24]: df.reindex(idx2)  # df is still the same as original.
In [25]: df.memory_usage(deep=True, index=True)  # Memory usage after reindex
Out[25]:
Index     7453600
a        14616000
dtype: int64

```
"
603944814,33697,fix bug of overflow validaion in 'reshape',chjinche,closed,2020-04-21T12:05:23Z,2020-05-22T11:03:07Z,"- [x] closes #26314
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
539039346,30304,BUG: fix DataFrame.apply returning wrong result when dealing with dtype (#28773),Reksbril,closed,2019-12-17T12:35:06Z,2020-05-22T11:22:41Z,"- [x] closes #28773
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The DataFrame.apply was sometimes returning wrong result when we passed
function, that was dealing with dtypes. It was caused by retrieving
the DataFrame.values of whole DataFrame, and applying the function
to it: values are represented by NumPy array, which has one
type for all data inside. It sometimes caused treating objects
in DataFrame as if they had one common type. What's worth mentioning,
the problem only existed, when we were applying function on columns.

The implemented solution ""cuts"" the DataFrame by columns and applies
function to each part, as it was whole DataFrame. After that, all
results are concatenated into final result on whole DataFrame.
The ""cuts"" are done in following way: the first column is taken, and
then we iterate through next columns and take them into first cut
while their dtype is identical as in the first column. The process
is then repeated for the rest of DataFrame."
621396928,34268,TST: GH28813 test .diff() on Sparse dtype,matteosantama,closed,2020-05-20T01:46:51Z,2020-05-22T11:59:55Z,"- [x] closes #28813 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Add a quick test to confirm that .diff() function handles sparse dataframes the same way as regular dataframes. 

First time making a pandas PR so comments, advice, critiques are encouraged."
19229919,4793,read_csv does not parse in header with BOM utf-8,johnclinaa,closed,2013-09-09T23:37:06Z,2020-05-22T12:35:10Z,"I am using Pandas version 0.12.0 on a Mac.

I noticed that when there is a BOM utf-8 file, and if the header row is in the first line, the read_csv() method will leave a leading quotation mark in the first column's name.  However, if the header row is further down the file and I use the ""header="" option, then the whole header row gets parsed correctly.

Here is an example code:

bing_kw = pd.read_csv('../../data/sem/Bing-Keyword_daily.csv', header=9,   thousands=',', encoding='utf-8')

Parses the header correctly.

bing_kw = pd.read_csv('../../data/sem/Bing-Keyword_daily.csv', thousands=',', encoding='utf-8')

Parses the first header column name incorrectly by leaving the leading quotation mark.
"
588014856,33023,Fix 25648 improve read csv stacktrace on bad argument,roberthdevries,closed,2020-03-25T21:55:52Z,2020-05-22T14:24:40Z,"- [x] closes #25648
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
564973376,31965,"drawstyle with ""step"" or ""step-mid"" display error",mankoff,closed,2020-02-13T21:55:28Z,2020-05-22T14:36:26Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
df = pd.DataFrame(index=pd.date_range(start='2020-01-01', periods=3), data=[1,2,3])
df.plot(drawstyle='steps')
df.plot(drawstyle='steps-mid')
```

`df` is:

```
|                     | 0 |
|---------------------+---|
| 2020-01-01 00:00:00 | 1 |
| 2020-01-02 00:00:00 | 2 |
| 2020-01-03 00:00:00 | 3 |
```
#### Problem description

For the first plot (`step`), the first data point (value 1) is not displayed, and I question the x-axis labeling. It implies that the value throughout the day of 01 Jan is 2, while the datafram suggests the value is 1 (assuming the value is carried forward from the timestamp until the next observation).

![step](https://user-images.githubusercontent.com/145117/74480648-34577900-4e66-11ea-99bb-e9ca3add03c1.png)

![step-mid](https://user-images.githubusercontent.com/145117/74480702-46391c00-4e66-11ea-897f-039bde2d418f.png)

For the second plot (`step-mid`), all data is visible, but the x-axis is incorrect. It implies that  the value of 1 that began on 01 Jan changed midway through the day. Visually, both values 1 and 3 are graphically underrepresented and half-width, while value 2 is full-width.

I would expect that when `drawstyle=""step""` is used, that the graph looks like this:

```
import pandas as pd
df = pd.DataFrame(index=pd.date_range(start='2020-01-01', periods=3), data=[1,2,3])
df.loc[df.index[-1] + (df.index[-1]-df.index[-2])] = df.iloc[-1]
df.plot(drawstyle='steps-post')

```

![better](https://user-images.githubusercontent.com/145117/74482932-4affcf00-4e6a-11ea-8a71-426f3fd01b56.png)


#### Expected Output

A graph that shows all data with correct x-axis labels.

#### Output of ``pd.show_versions()``

<details>

pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-76-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.3
pytz             : 2018.7
dateutil         : 2.7.5
pip              : 18.1
setuptools       : 40.5.0
Cython           : None
pytest           : 5.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.1 (dt dec pq3 ext lo64)
jinja2           : 2.10
IPython          : 7.1.1
pandas_datareader: None
bs4              : None
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.0.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : 1.2.15
tables           : None
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

 

</details>
"
621763136,34275,ENH: fix arrow roundtrip for ExtensionDtypes in absence of pandas metadata,jorisvandenbossche,closed,2020-05-20T13:28:20Z,2020-05-22T14:40:44Z,"An oversight in our Arrow Extension Types for period and interval to also define the conversion to the equivalent pandas dtype. This ensures that the arrow->pandas conversion also works if there is no ""pandas_metadata"" in the schema's metadata, eg when this metadata was lost during some operations (up to now, the pandas ExtensionDtype was reconstructed based on the string name in the stored metadata).

See https://arrow.apache.org/docs/python/extending_types.html#conversion-to-pandas"
522628416,29612,Line plot is not displaying correctly using pandas,Ashutosh341,closed,2019-11-14T05:10:00Z,2020-05-22T15:16:11Z,"I have installed latest pandas version

Now I am trying to plot a bar plot by using pandas but it is giving me this Figure

![Capture line plot](https://user-images.githubusercontent.com/54778459/68828240-07f75980-06cb-11ea-8c50-5c2ffd53cb9b.PNG)
.png)

I am learning pandas from Udemy and when trainer was plotting bar plot , it was plotting correctly
![Capture112](https://user-images.githubusercontent.com/54778459/68828495-e2b71b00-06cb-11ea-8704-9e6cb45eae26.PNG)
 "
513804310,29264,Inconsistent results between `rolling.corr` and `corr`,evanzd,closed,2019-10-29T09:52:23Z,2020-05-22T16:09:24Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
x = pd.Series([0.125, 0.625, 0.5, 0.8125, 0.1875])
y = pd.Series([0.2, 0.2, 0.2, 0.2, 0.2])
print('rolling.corr:', x.rolling(5).corr(y).iloc[-1])
print('corr:', x.corr(y))
```

Output:

```
rolling.corr: -inf
corr: nan
```

#### Problem description

For constant vector (e.g., [0.2, 0.2, 0.2, 0.2, 0.2]), `corr` should be `nan` but not `inf`.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.1
numpy            : 1.16.5
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
531189672,29966,Type conversion to boolean for integers after concat,stevesmit,closed,2019-12-02T15:08:42Z,2020-05-22T16:58:24Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
# when concatenating on Multi-index, 1 and 0 are automatically converted to booleans if there are other booleans
s1 = pd.Series([10,20],index=pd.MultiIndex.from_tuples( [('is true', True), ('is true', False)],names=['Question', 'Answer']))
s2 = pd.Series([51,12,123],index=pd.MultiIndex.from_tuples( [('how many', 0), ('how many', 1), ('how many', 2),],names=['Question', 'Answer']))
to_concat =[s1,s2]
print('with issue:')
print(pd.concat(to_concat))
#no issue
s1 = pd.Series([10,20],index=pd.MultiIndex.from_tuples( [('is true', 1), ('is true', 0)],names=['Question', 'Answer']))
to_concat =[s1,s2]
print('\nwithout issue:')
print(pd.concat(to_concat))
```
#### Problem description

When concatenating on Multi-index, 1 and 0 are automatically converted to Booleans if there are other Booleans. In my view the type should remain the same after concatenating.

For documentation-related issues, you can check the latest versions of the docs on `master` here:


#### Expected Output
```with issue:
Question  Answer
is true   True       10
          False      20
how many  False      51
          True       12
          2         123
dtype: int64

without issue:
Question  Answer
is true   1          10
          0          20
how many  0          51
          1          12
          2         123
dtype: int64
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
'0.25.3'

</details>
"
622785759,34301,CLN: de-duplicate tzlocal conversion function,jbrockmendel,closed,2020-05-21T21:01:31Z,2020-05-22T17:28:27Z,
623240428,34314,DEPR: pd.wide_to_long,simonjayhawkins,open,2020-05-22T14:14:59Z,2020-05-22T17:52:29Z,xref https://github.com/pandas-dev/pandas/pull/33418#pullrequestreview-390789474
622088846,34281,REF: make operate_blockwise into a BlockManager method,jbrockmendel,closed,2020-05-20T21:14:46Z,2020-05-22T20:27:56Z,"More appropriate abstraction.
"
622101625,34282,REF: Make QuarterOffset a cdef class,jbrockmendel,closed,2020-05-20T21:40:33Z,2020-05-22T20:30:07Z,
621909431,34278,REF: Make Period arith mirror PeriodArray arith,jbrockmendel,closed,2020-05-20T16:33:50Z,2020-05-22T20:30:35Z,In follow-ups can use is_any_tdlike_scalar elsewhere
607876579,33832,DEPR: Timestamp.freq,jbrockmendel,closed,2020-04-27T21:48:45Z,2020-05-22T20:33:06Z,"- [x] closes #15146
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The motivation here is that I've concluded that DatetimeArray.freq _must_ be removed, so for internal consistency Timestamp.freq should be removed too.

The main user-facing behavior this will affect is Timestamp.is_month_start, is_month_end, is_quarter_start, is_quarter_end, is_year_start, is_year_end, for which I guess we'll offer users an alternative(?)"
623473421,34321,CI: Numpy compat,alimcmaster1,closed,2020-05-22T21:01:36Z,2020-05-22T23:39:58Z,"- [x] closes #34306

Likely related to: https://github.com/pandas-dev/pandas/pull/32510.

Seems doc builds have started using: 
numpy                     1.17.5           py38h95a1406_0    conda-forge
https://github.com/pandas-dev/pandas/pull/34317/checks?check_run_id=700337761

(Previous pipelines where using 1.18)

In numpy 1.17.5 BitGenerator path is:
`np.random.bit_generator.BitGenerator`

In numpy 1.18.0 path is
`np.random.BitGenerator`"
623501645,34325,REF: get str_rep in numexpr code,jbrockmendel,closed,2020-05-22T22:16:23Z,2020-05-23T00:48:33Z,

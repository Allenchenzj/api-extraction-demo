id,number,title,user,state,created_at,updated_at,body
638266003,34762,"CLN: day->day_opt, remove unused case in liboffsets.get_day_of_month",jbrockmendel,closed,2020-06-14T01:46:21Z,2020-06-14T15:20:08Z,"Removing this unused case turns out to be a blocker to making get_day_of_month nogil, which in turn will allow a bunch of de-duplication in this file."
638210839,34752,REF: refactor NDFrame.interpolate to avoid dispatching to fillna,simonjayhawkins,closed,2020-06-13T18:41:21Z,2020-06-14T15:25:57Z,"xref https://github.com/pandas-dev/pandas/pull/31048#issuecomment-643658604, #33959"
574314386,32401,HDFStore: Fix empty result of keys() method on non-pandas hdf5 file,roberthdevries,closed,2020-03-02T23:26:12Z,2020-06-14T15:32:11Z,"First try to get pandas style tables, if there are none, return the list of non-pandas (hdf5 native) tables in the file (if any)

- [x] closes #29916
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
542657619,30492,BUG: sql different offsets (1/2 modify infer_dtype),ThibTrip,closed,2019-12-26T20:00:00Z,2020-06-14T15:34:25Z,"- [x] closes #30207
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

# Explanations

For to_sql no errors are raised anymore when we have Series containing datetimes with timezone and different offsets (unlike in GH30207) and the data is saved as expected (I tested with postgres which does not save offsets anyways).

Concerning read_sql however there are several points to discuss:

1. if we want to have datetime64 instead of ""object"" (as described in [GH30207](https://github.com/pandas-dev/pandas/issues/30207)) we lose the offsets. So this would be a breaking change and would require first a warning in the next version then we implement it in the version after, right?

2. is this patch I made acceptable?

The probem is that read_sql uses pd.DataFrame.from_records and it reads datetimes with different offsets as ""object"" instead of datetime64[ns,...] (e.g. datetime64[ns, psycopg2.tz.FixedOffsetTimezone(offset=60, name=None)]).

If we wanted to modify this behavior we would have to modify the C extension _libs.lib. I don't know how to help with that and I am not sure we want to do this. Even if we could pack the datetimes with differents offsets into a Series with a datetime64 dtype the methods of the .dt accessor are not compatible anyways: currently a ValueError is thrown if one attempts to use the accessor in this case (see traceback in the example in ""Before PR"" below).



# Before PR


```python
import datetime
import psycopg2
import sqlalchemy
import pandas as pd
from pandas.io.sql import read_sql, to_sql

# CREATE ENGINE
# engine = sqlalchemy.create_engine(""CONNECTION_STRING"")

# create datetimes with timezone that have different offsets
data = [[datetime.datetime(2019, 11, 14, 16, 12, 0, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=60, name=None))],
        [datetime.datetime(2019, 8, 7, 15, 37, 4, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=120, name=None))]]

df = pd.DataFrame(data, columns = ['ts_tz'])

# save data
to_sql(frame = df, 
       name = 'timezone_diff_offsets_test',
       con = engine,
       schema = 'public',
       if_exists = 'replace',
       index = False)
```

<details>
 <summary>Traceback</summary>


```python-traceback

    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    ~/GitHub/pandas/pandas/core/arrays/datetimes.py in objects_to_datetime64ns(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)
       1968         try:
    -> 1969             values, tz_parsed = conversion.datetime_to_datetime64(data)
       1970             # If tzaware, these values represent unix timestamps, so we


    ~/GitHub/pandas/pandas/_libs/tslibs/conversion.pyx in pandas._libs.tslibs.conversion.datetime_to_datetime64()


    ValueError: Array must be all same time zone

    
    During handling of the above exception, another exception occurred:


    ValueError                                Traceback (most recent call last)

    <ipython-input-2-a66775d04fe3> in <module>
         20        schema = 'public',
         21        if_exists = 'replace',
    ---> 22        index = False)
    

    ~/GitHub/pandas/pandas/io/sql.py in to_sql(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)
        519         chunksize=chunksize,
        520         dtype=dtype,
    --> 521         method=method,
        522     )
        523 


    ~/GitHub/pandas/pandas/io/sql.py in to_sql(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method)
       1318             index_label=index_label,
       1319             schema=schema,
    -> 1320             dtype=dtype,
       1321         )
       1322         table.create()


    ~/GitHub/pandas/pandas/io/sql.py in __init__(self, name, pandas_sql_engine, frame, index, if_exists, prefix, index_label, schema, keys, dtype)
        620         if frame is not None:
        621             # We want to initialize based on a dataframe
    --> 622             self.table = self._create_table_setup()
        623         else:
        624             # no data provided, read-only mode


    ~/GitHub/pandas/pandas/io/sql.py in _create_table_setup(self)
        869         from sqlalchemy import Table, Column, PrimaryKeyConstraint
        870 
    --> 871         column_names_and_types = self._get_column_names_and_types(self._sqlalchemy_type)
        872 
        873         columns = [


    ~/GitHub/pandas/pandas/io/sql.py in _get_column_names_and_types(self, dtype_mapper)
        861         column_names_and_types += [
        862             (str(self.frame.columns[i]), dtype_mapper(self.frame.iloc[:, i]), False)
    --> 863             for i in range(len(self.frame.columns))
        864         ]
        865 


    ~/GitHub/pandas/pandas/io/sql.py in <listcomp>(.0)
        861         column_names_and_types += [
        862             (str(self.frame.columns[i]), dtype_mapper(self.frame.iloc[:, i]), False)
    --> 863             for i in range(len(self.frame.columns))
        864         ]
        865 


    ~/GitHub/pandas/pandas/io/sql.py in _sqlalchemy_type(self, col)
        971             # timezone information
        972             try:
    --> 973                 if col.dt.tz is not None:
        974                     return TIMESTAMP(timezone=True)
        975             except AttributeError:


    ~/GitHub/pandas/pandas/core/accessor.py in _getter(self)
         83         def _create_delegator_property(name):
         84             def _getter(self):
    ---> 85                 return self._delegate_property_get(name)
         86 
         87             def _setter(self, new_values):


    ~/GitHub/pandas/pandas/core/indexes/accessors.py in _delegate_property_get(self, name)
         60         from pandas import Series
         61 
    ---> 62         values = self._get_values()
         63 
         64         result = getattr(values, name)


    ~/GitHub/pandas/pandas/core/indexes/accessors.py in _get_values(self)
         51                 return PeriodArray(data, copy=False)
         52             if is_datetime_arraylike(data):
    ---> 53                 return DatetimeIndex(data, copy=False, name=self.name)
         54 
         55         raise TypeError(


    ~/GitHub/pandas/pandas/core/indexes/datetimes.py in __new__(cls, data, freq, tz, normalize, closed, ambiguous, dayfirst, yearfirst, dtype, copy, name)
        266             dayfirst=dayfirst,
        267             yearfirst=yearfirst,
    --> 268             ambiguous=ambiguous,
        269         )
        270 


    ~/GitHub/pandas/pandas/core/arrays/datetimes.py in _from_sequence(cls, data, dtype, copy, tz, freq, dayfirst, yearfirst, ambiguous)
        418             dayfirst=dayfirst,
        419             yearfirst=yearfirst,
    --> 420             ambiguous=ambiguous,
        421         )
        422 


    ~/GitHub/pandas/pandas/core/arrays/datetimes.py in sequence_to_dt64ns(data, dtype, copy, tz, dayfirst, yearfirst, ambiguous)
       1864             #  or M8[ns] to denote wall times
       1865             data, inferred_tz = objects_to_datetime64ns(
    -> 1866                 data, dayfirst=dayfirst, yearfirst=yearfirst
       1867             )
       1868             tz = maybe_infer_tz(tz, inferred_tz)


    ~/GitHub/pandas/pandas/core/arrays/datetimes.py in objects_to_datetime64ns(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)
       1972             return values.view(""i8""), tz_parsed
       1973         except (ValueError, TypeError):
    -> 1974             raise e
       1975 
       1976     if tz_parsed is not None:


    ~/GitHub/pandas/pandas/core/arrays/datetimes.py in objects_to_datetime64ns(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)
       1963             dayfirst=dayfirst,
       1964             yearfirst=yearfirst,
    -> 1965             require_iso8601=require_iso8601,
       1966         )
       1967     except ValueError as e:


    ~/GitHub/pandas/pandas/_libs/tslib.pyx in pandas._libs.tslib.array_to_datetime()


    ~/GitHub/pandas/pandas/_libs/tslib.pyx in pandas._libs.tslib.array_to_datetime()


    ValueError: Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True


```

</details>


```python
# save datetimes with UTC timezone in different periods of the year
## create test data
data = [[datetime.datetime(2019, 11, 14, 15, 12, 0, tzinfo = datetime.timezone.utc)],
        [datetime.datetime(2019, 8, 7, 13, 37, 4,  tzinfo = datetime.timezone.utc)]]

df = pd.DataFrame(data, columns = ['ts_tz'])

## save test data
to_sql(frame = df, 
       name = 'timezone_diff_offsets_test',
       con = engine,
       schema = 'public',
       if_exists = 'replace',
       index = False)

# upon reading if you have for instance ""Europe/Paris""
# configured as timezone on your local machine then you get datetimes with timezone
# that have different offsets (because they are in different periods of the year)
# due to this you have ""object"" dtype and not datetime64[...]
df = read_sql('SELECT * FROM timezone_diff_offsets_test', engine)
print(df,'\n')
print(df.dtypes)
```

                           ts_tz
    0  2019-11-14 16:12:00+01:00
    1  2019-08-07 15:37:04+02:00 
    
    ts_tz    object
    dtype: object


# After PR


```python
import datetime
import psycopg2
import sqlalchemy
import pandas as pd
from pandas.io.sql import read_sql, to_sql

# CREATE ENGINE
# engine = sqlalchemy.create_engine(""CONNECTION_STRING"")

# create datetimes with timezone that have different offsets
data = [[datetime.datetime(2019, 11, 14, 16, 12, 0, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=60, name=None))],
        [datetime.datetime(2019, 8, 7, 15, 37, 4, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=120, name=None))]]

df = pd.DataFrame(data, columns = ['ts_tz'])

# save data - no exceptions occur
to_sql(frame = df, 
       name = 'timezone_diff_offsets_test',
       con = engine,
       schema = 'public',
       if_exists = 'replace',
       index = False)

# read data - offsets are dropped and dtype of column ts_tz is datetime64[ns, UTC]
df = read_sql('SELECT * FROM timezone_diff_offsets_test', engine)
print(df,'\n')
print(df.dtypes)
```

                          ts_tz
    0 2019-11-14 15:12:00+00:00
    1 2019-08-07 13:37:04+00:00 
    
    ts_tz    datetime64[ns, UTC]
    dtype: object

"
581849711,32739,BUG: Fix join on MultiIndex for mixed Datetimelike and string levels,nrebena,closed,2020-03-15T22:25:01Z,2020-06-14T15:34:59Z,"# Problem
From #26558, a minimal example is:
```python
import pandas as pd
import datetime

i1 = pd.Index(['2019-05-31'])
i2 = pd.Index([datetime.datetime(2019, 5, 31)])

mi1 = pd.MultiIndex.from_tuples([('2019-05-31',)])
mi2 = pd.MultiIndex.from_tuples([(datetime.datetime(2019, 5, 31),)])

print(i1.join(i2, return_indexers=True))
print(mi1.join(mi2, return_indexers=True))
   
# (DatetimeIndex(['2019-05-31'], dtype='datetime64[ns]', freq=None), None, None)
# (MultiIndex([('2019-05-31',)], ), None, array([-1]))
```
Here we can see that one of the indexers for multiindex is wrong.
 
# Proposed fix 🩹
The difference of handling for single index is explain by the join method for DatetimeTimedeltaMixin.
I did some refactoring in pandas/core/indexes/datetimelike.py to be able to reuse the same datetime handling method.
Then in the Index.join method, where MultiIndex are handled, I apply this method to each level of the MultiIndex that is a DatetimeTimedeltaMixin.

# PR checkboxes :ballot_box_with_check:  

- [x] closes #26558
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
510049894,29131,[BUG] Fixed behavior of DataFrameGroupBy.apply to respect _group_selection_context,christopherzimmerman,closed,2019-10-21T15:27:45Z,2020-06-14T15:37:45Z,"- [x] closes #28549
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This issue needs to be addressed before #28541 can be merged
"
527405093,29800,BUG: add reset logic for Grouper if new obj is passed in (#26564),alichaudry,closed,2019-11-22T20:41:55Z,2020-06-14T15:41:34Z,"- [ ] closes #26564
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
559223704,31626,BUG: qcut can fail for highly discontinuous data distributions,puneet29,closed,2020-02-03T17:23:26Z,2020-06-14T15:44:48Z,"Fixes #15069. Needs refactoring

- [x] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
636226397,34689,TYP: check_untyped_defs pandas.core.nanops,simonjayhawkins,closed,2020-06-10T13:00:05Z,2020-06-14T15:51:36Z,"pandas\core\nanops.py:565: error: Item ""bool"" of ""Union[bool, Any]"" has no attribute ""any"""
636441939,34704,BUG: CI builds failing at compilation of _sas,martindurant,closed,2020-06-10T17:58:18Z,2020-06-14T15:57:50Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

See, for example, the build line https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=37068&view=logs&j=a3a13ea8-7cf0-5bdb-71bb-6ac8830ae35c&t=2296ffae-5e67-52bf-66a8-6160145a0f80&l=660 
(and this appears to be failing other PRs too)

#### Problem description

CI builds are failing, see link above

#### Expected Output

"
638197873,34750,Bump up minimum numpy version in windows37 job,bharatr21,closed,2020-06-13T17:14:27Z,2020-06-14T16:04:55Z,"- [x] closes #34724 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
577555032,32546,BUG: Dataframe.groupby aggregations with categorical columns lead to incorrect results.,MarcoGorelli,closed,2020-03-08T19:51:00Z,2020-06-14T16:31:04Z,"- [x] closes #32494 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
623701754,34344,BUG: don't plot colorbar if c is column containing colors,MarcoGorelli,closed,2020-05-23T17:09:49Z,2020-06-14T16:31:23Z,"- [x] closes #34316
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
611778522,33964,REGR: string indexing on PeriodIndex raises KeyError on master (not yet released),simonjayhawkins,closed,2020-05-04T10:48:50Z,2020-06-14T16:39:59Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
>>> import pandas as pd
>>>
>>> pd.__version__
'1.1.0.dev0+1466.ga3477c769'
>>>
>>> index = pd.period_range(start=""2000"", periods=20, freq=""B"")
>>> series = pd.Series(range(20), index=index)
>>>
>>> series.index
PeriodIndex(['2000-01-03', '2000-01-04', '2000-01-05', '2000-01-06',
             '2000-01-07', '2000-01-10', '2000-01-11', '2000-01-12',
             '2000-01-13', '2000-01-14', '2000-01-17', '2000-01-18',
             '2000-01-19', '2000-01-20', '2000-01-21', '2000-01-24',
             '2000-01-25', '2000-01-26', '2000-01-27', '2000-01-28'],
            dtype='period[B]', freq='B')
>>>
>>> series.loc[""2000-01-14""]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\simon\pandas\pandas\core\indexing.py"", line 871, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File ""C:\Users\simon\pandas\pandas\core\indexing.py"", line 1102, in _getitem_axis
    return self._get_label(key, axis=axis)
  File ""C:\Users\simon\pandas\pandas\core\indexing.py"", line 1051, in _get_label
    return self.obj.xs(label, axis=axis)
  File ""C:\Users\simon\pandas\pandas\core\generic.py"", line 3512, in xs
    loc = self.index.get_loc(key)
  File ""C:\Users\simon\pandas\pandas\core\indexes\period.py"", line 519, in get_loc
    raise KeyError(key)
KeyError: '2000-01-14'
>>>

```

#### Problem description

regression in #31172 (i.e. not yet released)

9a211aae9f710db23c9113aea0251e2758904755 is the first bad commit
commit 9a211aae9f710db23c9113aea0251e2758904755
Author: jbrockmendel <jbrockmendel@gmail.com>
Date:   Sat Jan 25 08:07:15 2020 -0800

    BUG: inconsistency between PeriodIndex.get_value vs get_loc (#31172)

cc @jbrockmendel 

#### Expected Output

```
>>> import pandas as pd
>>>
>>> pd.__version__
'1.0.3'
>>>
>>> index = pd.period_range(start=""2000"", periods=20, freq=""B"")
>>> series = pd.Series(range(20), index=index)
>>>
>>> series.index
PeriodIndex(['2000-01-03', '2000-01-04', '2000-01-05', '2000-01-06',
             '2000-01-07', '2000-01-10', '2000-01-11', '2000-01-12',
             '2000-01-13', '2000-01-14', '2000-01-17', '2000-01-18',
             '2000-01-19', '2000-01-20', '2000-01-21', '2000-01-24',
             '2000-01-25', '2000-01-26', '2000-01-27', '2000-01-28'],
            dtype='period[B]', freq='B')
>>>
>>> series.loc[""2000-01-14""]
9
>>>
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
637976756,34736,BUG: Fixed regression in PeriodIndex loc,TomAugspurger,closed,2020-06-12T20:04:21Z,2020-06-14T16:40:03Z,"Closes https://github.com/pandas-dev/pandas/issues/33964.

This is the smallest change I could get. I'm not sure what a more comprehensive fix would look like.

Regression only on master, so no need for a whatsnew."
638273579,34764,REF: make get_day_of_month nogil,jbrockmendel,closed,2020-06-14T03:08:24Z,2020-06-14T16:53:00Z,
638266241,34763,"REF: remove roll_check, use roll_convention",jbrockmendel,closed,2020-06-14T01:49:07Z,2020-06-14T17:01:22Z,Avoid bespoke logic for these two cases.  This will make it feasible to collapse shift_months down to a single case (following #34762 and the one after that that makes get_day_of_month nogil)
638197243,34749,[WIP] fork of #31048 for CI testing DO NOT MERGE,simonjayhawkins,closed,2020-06-13T17:10:22Z,2020-06-14T17:03:52Z,fork of #31048 for CI testing
612351285,33988,Issue33955 Properly checking whether post_processing is callable or not in Class GroupBy,KenilMehta,closed,2020-05-05T05:38:06Z,2020-06-14T17:14:22Z,- [x ] closes #33955 
587035963,32979,Add error message for tz_localize,sumanau7,closed,2020-03-24T15:15:46Z,2020-06-14T17:26:22Z,"- [x] closes #32967
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
630236652,34558,DOC: Add bcpandas to Ecosystem in docs,yehoshuadimarsky,closed,2020-06-03T18:46:57Z,2020-06-14T18:01:51Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Hi, I wrote this utility which I use in production almost daily at my job, and it's been super helpful for me personally to fill a gap - slow writes from Pandas to MS SQL. Figured I'd suggest it in the pandas ecosystem if it could be useful to others.

Wasn't sure where to put it, and didn't want to start a new section, so I put it in `Out-of-core`. It appeared to be sorted alphabetically, so I put it in that order.
"
599697658,33545,BUG: index name is lost when doing groupby where by is a dict and at least one value in the by dict is a list of of size zero,jasonaue,closed,2020-04-14T16:22:59Z,2020-06-14T18:04:54Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

To reproduce:

```
example = pd.DataFrame({'foo':[1,2,3],'bar1':[1,2,3],'bar2':[1,2,3]})

#index name is foo
display(example.groupby('foo').agg({'bar1': ['min'], 'bar2':['max']}))

#index name is None
display(example.groupby('foo').agg({'bar1': ['min'], 'bar2':[]}))
```

#### Output of ``pd.show_versions()``
```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-7642-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.2.1
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.1
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.9
tables           : 3.5.2
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.1
```"
578686978,32580,[BUG] Grouped-by column loses name when empty list of aggregations is specified.,shwina,closed,2020-03-10T15:57:43Z,2020-06-14T18:04:54Z,"```
In [53]: a = pd.DataFrame({'a': [1, 1, 2], 'b': [1, 2, 3], 'c': [1, 2, 4]})

In [54]: a.groupby('a').agg({'c': ['min']}) # name preserved in index
Out[54]:
    c
  min
a
1   1
2   4

In [55]: a.groupby('a').agg({'b': [], 'c': ['min']}) # name lost in index
Out[55]:
    c
  min
1   1
2   4
```

In the above snippet, the name of the grouped by column (`'a'`) is preserved in the former case, and lost in the latter case."
638389167,34770,CLN/TYPE: EWM,mroeschke,closed,2020-06-14T16:53:17Z,2020-06-14T18:36:26Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

1. Move `min_periods` validation to constructor
2. Stronger cython type defintions
3. Typing EWM methods"
638160241,34746,API: validate `limit_direction` parameter of NDFrame.interpolate,simonjayhawkins,closed,2020-06-13T13:14:10Z,2020-06-14T18:43:12Z,"broken off #31048
"
614774968,34069,ENH: Implement groupby.sample,dsaxton,closed,2020-05-08T14:23:50Z,2020-06-14T19:05:08Z,"- [x] closes #31775
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
638397714,34772,REF: inline get_day_of_month,jbrockmendel,closed,2020-06-14T17:45:56Z,2020-06-14T19:52:45Z,"Makes the logic in all cases of shift_months identical, same for shift_quarters.  Next pass does de-duplication."
630756501,34570,Export InvalidIndexError,horta,closed,2020-06-04T11:52:41Z,2020-06-14T20:37:30Z,"- [x] closes #xxxx (there is no such an issue)
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
638260965,34760,REF: De-duplicate roll_yearday/roll_qtrday,jbrockmendel,closed,2020-06-14T00:55:42Z,2020-06-14T21:23:21Z,"
"
589469318,33086,"BUG: Groupby lost index, when one of the agg keys had no function all…",phofl,closed,2020-03-28T00:07:06Z,2020-06-14T21:35:59Z,"…ocated

- [x] closes #32580
- [x] closes #33545 (is a duplicate of 32580)
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The issue was the concatenation with an empty DataFrame and the result of the min function. This resulted in the lost index. 

I changed the input for the concatenation, so that only non empty DataFrames would be concatenated. We have to catch the case, that all DataFrames are empty, because this would result in an error."
589081471,33067,BUG: Fix unwanted type casting while replacing values in a DataFrame,phofl,closed,2020-03-27T11:54:17Z,2020-06-14T21:37:18Z,"- [x] closes #32988
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

As mentioned in #32988 [here](https://github.com/pandas-dev/pandas/issues/32988#issuecomment-604727057) I think, that I have found a way to fix this, but I don't know, if this results in a desired behavior. 

I would add tests after ensuring, that this does not break anything else. 

Any thoughts about this?"
589275833,33072,"BUG: Add test to ensure, that bug will not occur again. #33058",phofl,closed,2020-03-27T17:04:59Z,2020-06-14T21:38:00Z,"- [x] xref #33058
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Should I add a whatsnew entry? It's not really new, if it is backported to 1.0.1."
589571052,33096,TEST: Add test for #22541 to ensure that error does not occur again,phofl,closed,2020-03-28T12:48:06Z,2020-06-14T21:40:14Z,"- [x] closes #22541
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

While searching for related issue for my other pull request, I found that this issue must have been fixed with another commit in the past (don't know when this issue was fixed).

I added a test to ensure that this error won't know occur in the future."
529934413,29916,HDF5: empty groups and keys,st-bender,closed,2019-11-28T14:09:53Z,2020-06-14T22:20:19Z,"Hi,

With some of the hdf5 files I have, `pandas.HDFStore.groups()` returns an empty list. (as does `.keys()` which iterates over the groups). However, the data are accessible via `.get()` or `.get_node()`.

This is related to #21543 and #21372 where the `.groups()` logic was changed, in particular using `self._handle.walk_groups()` instead of `self._handle.walk_nodes()`, now to be found here:
https://github.com/pandas-dev/pandas/blob/ea2e26ae7d700d7fd363ea5bfc05d2fe3fb8a5ee/pandas/io/pytables.py#L1212


#### Current Output

```python
>>> hdf.groups()
[]
```
```python
>>> hdf.keys()
[]
```

#### Expected Ouptut

List of groups and keys as visible with e.g. `h5dump`.
**Note:** Changing the aforementioned line back to use `.walk_nodes()` fixes the issue and lists the groups and keys properly:

```python
>>> hdf.groups()
[/Data/Table Layout (Table(69462,), zlib(4)) ''
   description := {
...
/Data/Array Layout/2D Parameters/Data Parameters (Table(15,)) ''
   description := {
   ""mnemonic"": StringCol(itemsize=8, shape=(), dflt=b'', pos=0),
   ""description"": StringCol(itemsize=48, shape=(), dflt=b'', pos=1),
   ""isError"": Int64Col(shape=(), dflt=0, pos=2),
   ""units"": StringCol(itemsize=7, shape=(), dflt=b'', pos=3),
   ""category"": StringCol(itemsize=31, shape=(), dflt=b'', pos=4)}
   byteorder := 'little'
   chunkshape := (642,)]]
```
```python
>>> hdf.keys()
['/Data/Table Layout',
 '/Metadata/Data Parameters',
 '/Metadata/Experiment Notes',
 '/Metadata/Experiment Parameters',
 '/Metadata/Independent Spatial Parameters',
 '/Metadata/_record_layout',
 '/Data/Array Layout/Layout Description',
 '/Data/Array Layout/1D Parameters/Data Parameters',
 '/Data/Array Layout/2D Parameters/Data Parameters']
```

#### Fix

One solution would be (I guess) to revert #21543, another to fix at least `.keys()` to use `._handle.walk_nodes()` instead of `.groups()` in
https://github.com/pandas-dev/pandas/blob/ea2e26ae7d700d7fd363ea5bfc05d2fe3fb8a5ee/pandas/io/pytables.py#L562

Could also be that it is a bug in `pytables`.

#### Problem background

I was trying to figure out why some hdf5 files open fine with `pandas` but fail with `dask`.
The reason is that `dask` allows wildcards and iterates over the keys to find valid ones. If `.keys()` is empty, reading the files with `dask` fails.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-957.27.2.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.1.post20191125
Cython           : None
pytest           : 5.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.10.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : 2.7.0
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : None
tables           : 3.6.1
xarray           : 0.14.1
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
32481715,7002,BUG/TST: verify that groupby apply with a column aggregation does not return the column,jreback,closed,2014-04-29T20:26:56Z,2020-06-14T22:20:51Z,"related https://github.com/pydata/pandas/pull/7000

```
In [1]:  df = DataFrame({'foo1' : ['one', 'two', 'two', 'three', 'one', 'two'],
                                      'foo2' : np.random.randn(6)})

In [2]: df
Out[2]: 
    foo1      foo2
0    one  1.006666
1    two  0.002063
2    two  1.507785
3  three  1.865921
4    one  0.141202
5    two -1.079792

[6 rows x 2 columns]
```

```
In [3]: df.groupby('foo1').mean()
Out[3]: 
           foo2
foo1           
one    0.573934
three  1.865921
two    0.143352

[3 rows x 1 columns]

In [4]: df.groupby('foo1').apply(lambda x: x.mean())
Out[4]: 
           foo2
foo1           
one    0.573934
three  1.865921
two    0.143352

[3 rows x 1 columns]
```

This should return the foo1 column as well

```
[6]: df.groupby('foo1',as_index=False).apply(lambda x: x.mean())
Out[6]: 
       foo2
0  0.573934
1  1.865921
2  0.143352

[3 rows x 1 columns]

In [7]: df.groupby('foo1',as_index=False).mean()
Out[7]: 
    foo1      foo2
0    one  0.573934
1  three  1.865921
2    two  0.143352

[3 rows x 2 columns]

```
"
638414864,34775,DOC: updated multi.py docstring for SS06 errors,willpeppo,closed,2020-06-14T19:29:09Z,2020-06-14T22:21:51Z,
583976252,32812,Nested list multi index (#14467),harri471,closed,2020-03-18T20:13:50Z,2020-06-14T22:53:10Z,"- [ ] closes #14467 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
638264803,34761,REF: implement shift_bday,jbrockmendel,closed,2020-06-14T01:33:30Z,2020-06-14T23:06:50Z,"Avoids depending on DatetimeArray/PeriodArray methods, also avoids a couple of array allocations

```
In [2]: dti = pd.date_range(""2016-01-01"", periods=10**5, freq=""S"")                                                                     

In [3]: off = pd.offsets.BDay()                                                                                                        

In [4]: %timeit dti + off                                                                                                              
24.1 ms ± 202 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)  # <-- master
20.1 ms ± 664 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  # <-- PR
```"
638438618,34778,REF: de-duplicate code in liboffsets,jbrockmendel,closed,2020-06-14T21:58:07Z,2020-06-15T00:59:10Z,"```
In [2]: dti = pd.date_range(""2016-01-01"", periods=10000, freq=""S"")              
In [3]: off = pd.offsets.BMonthEnd(2)                                           
In [4]: %timeit dti + off                                                       
3.36 ms ± 14 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  # <-- master
998 µs ± 12.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  # <-- PR
```"
638398425,34774,CI: move arm to non-failure builds,jreback,closed,2020-06-14T17:50:10Z,2020-06-15T00:59:29Z,"make pre-commit language generic
"
636436151,34703,CLN: disallow tuple in to_offset,jbrockmendel,closed,2020-06-10T17:48:15Z,2020-06-15T01:14:28Z,"This is technically an API change, but AFAICT this isnt documented behavior anywhere."
638466789,34780,TST: remove super slow cases on upsample_nearest_limit,jreback,closed,2020-06-15T00:42:37Z,2020-06-15T02:19:40Z,make multi_thread csv parsing test slow
634732767,34647,TST: groupby apply with indexing and column aggregation returns the column #7002,fangchenli,closed,2020-06-08T15:46:45Z,2020-06-15T03:11:17Z,"Adds test for #7002 

- [x] closes #7002 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
638475156,34781,REF: reuse roll_qtrday in liboffsets,jbrockmendel,closed,2020-06-15T01:18:58Z,2020-06-15T03:13:37Z,Last one in this sequence.
638479760,34782,REF: avoid using DTA/PA methods in Week.apply_index,jbrockmendel,closed,2020-06-15T01:35:35Z,2020-06-15T03:14:31Z,"We avoid a couple of array allocations in the process.

```
In [2]: dti = pd.date_range(""2016-01-01"", periods=10000, freq=""S"")              
In [3]: off = pd.offsets.Week(1, False, 3)                                      
In [4]: %timeit dti + off                                                       
3.22 ms ± 30.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  # <-- master
344 µs ± 4.97 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  # <-- PR
```"
635480722,34667,BLD: pyproject.toml for Py38,TomAugspurger,closed,2020-06-09T14:29:53Z,2020-06-15T09:17:23Z,Closes https://github.com/pandas-dev/pandas/issues/34666
637354950,34721,Debug CI Issue,WillAyd,closed,2020-06-11T22:17:59Z,2020-06-15T09:25:07Z,
638688619,34788,Backport PR #34721 on branch 1.0.x (Debug CI Issue),simonjayhawkins,closed,2020-06-15T09:24:33Z,2020-06-15T09:58:01Z,xref #34721
638654358,34786,Backport PR #34667 on branch 1.0.x (BLD: pyproject.toml for Py38),simonjayhawkins,closed,2020-06-15T08:35:45Z,2020-06-15T11:23:53Z,"xref #34667

@jorisvandenbossche another set of eyes needed as this was not a clean cherry-pick"
638673222,34787,Backport Test Only from PR #34500 on branch 1.0.x (REG: Fix read_parquet from file-like objects),simonjayhawkins,closed,2020-06-15T09:02:42Z,2020-06-15T11:25:47Z,xref #34500
613237433,34020,REGR: Series.update raises ValueError with tz-aware types,simonjayhawkins,closed,2020-05-06T11:10:28Z,2020-06-15T11:44:42Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

print(pd.__version__)

dti = pd.date_range(""1/1/2000"", periods=3, tz=""Europe/London"")

a = pd.Series([dti[0], None, dti[2]])
b = pd.Series([None, dti[1], None])
a.update(b)

print(a)
```

#### Problem description

on master

```
1.1.0.dev0+1468.g3cebe3024
Traceback (most recent call last):
  File ""C:/Users/simon/test.py"", line 10, in <module>
    a.update(b)
  File ""c:\users\simon\pandas\pandas\core\series.py"", line 2855, in update
    self._mgr = self._mgr.putmask(mask=mask, new=other)
  File ""c:\users\simon\pandas\pandas\core\internals\managers.py"", line 544, in putmask
    return self.apply(
  File ""c:\users\simon\pandas\pandas\core\internals\managers.py"", line 397, in apply
    applied = getattr(b, f)(**kwargs)
  File ""c:\users\simon\pandas\pandas\core\internals\blocks.py"", line 1604, in putmask
    new_values[mask] = new
  File ""c:\users\simon\pandas\pandas\core\arrays\datetimelike.py"", line 609, in __setitem__
    self._data[key] = value
ValueError: NumPy boolean array indexing assignment cannot assign 3 input values to the 1 output values where the mask is true
```
regression in #27428

d717ad8157d2e0713cde89e10743cf4362294027 is the first bad commit
commit d717ad8157d2e0713cde89e10743cf4362294027
Author: jbrockmendel <jbrockmendel@gmail.com>
Date:   Tue Jul 23 16:55:40 2019 -0700

    CLN/REF: stop allowing iNaT in DatetimeBlock (#27428)

cc @jbrockmendel 

#### Expected Output

on 0.25.3

```
0.25.3
0   2000-01-01 00:00:00+00:00
1   2000-01-02 00:00:00+00:00
2   2000-01-03 00:00:00+00:00
dtype: datetime64[ns, Europe/London]
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
578898136,32599,[BUG] Groupby with as_index=False raises error when type is Category.,amineKammah,closed,2020-03-10T22:28:14Z,2020-06-15T12:29:19Z,"#### Code to reproduce the issue

```python
import pandas as pd

test = pd.DataFrame([[1, 1], [2, 2], [3, 3]], columns=['col1', 'col2'])
test['col1'] = test['col1'].astype('category')

test.groupby(['col1', 'col2'], as_index=False).size()
```
#### Problem description

With pandas 1.0.1, the code throws an error `ValueError: No axis named 1 for object type <class 'pandas.core.series.Series'>`.
With pandas 0.25.3, the code works, but `as_index` argument do not function as already mentioned in #25011.
This happened with categorical type, the output of the new version is similar to 0.25.3 with other types.

#### Expected Output
|    |   col1 |   col2 |   0 |
|---:|-------:|-------:|----:|
|  0 |      1 |      1 |   1 |
|  1 |      1 |      2 |   0 |
|  2 |      1 |      3 |   0 |
|  3 |      2 |      1 |   0 |
|  4 |      2 |      2 |   1 |
|  5 |      2 |      3 |   0 |
|  6 |      3 |      1 |   0 |
|  7 |      3 |      2 |   0 |
|  8 |      3 |      3 |   1 |

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.3.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : 5.3.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : None
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : 0.8.6
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
631752487,34605,BUG: Positional Arguments Passed as a Keyword Argument in Custom Rolling Aggregation,daskol,closed,2020-06-05T16:53:39Z,2020-06-15T12:53:18Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Problem description

According to [docstring][1] of `Rolling.apply` arguments `args` and `kwargs` allow to pass to a custom function positional and keyword arguments enumerated in `kwargs`.  However, `Rolling.apply` passes both of them as keyword arguments. In other word, it passes keyword `args ` consisting of ""positional"" arguments. This is definitely buggy behaviour.

The reason is that a closure function is built in [a wrong way][2]. The issue states for `cython` backend as well as `numba` backend. The possible should looks like the following.
```diff
diff --git a/pandas/core/window/rolling.py b/pandas/core/window/rolling.py
index a01a753e8..32360848a 100644
--- a/pandas/core/window/rolling.py
+++ b/pandas/core/window/rolling.py
@@ -1313,11 +1313,11 @@ class _Rolling_and_Expanding(_Rolling):
 
         window_func = partial(
             self._get_cython_func_type(""roll_generic""),
-            args=args,
-            kwargs=kwargs,
+            *args,
             raw=raw,
             offset=offset,
             func=func,
+            **kwargs,
         )
 
         def apply_func(values, begin, end, min_periods, raw=raw):
```
[1]: https://github.com/pandas-dev/pandas/blob/1.0.x/pandas/core/window/rolling.py#L1237-L1240
[2]: https://github.com/pandas-dev/pandas/blob/1.0.x/pandas/core/window/rolling.py#L1314-L1321"
636480657,34705,"TST: Add test for rolling window, see GH 34605",gglanzani,closed,2020-06-10T19:07:27Z,2020-06-15T12:53:22Z,"- [X] closes #34605 
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Dear maintainers,

I'm opening this PR to address issue #34605.

~I've made the test conditional to Python 3.8 as I'm using the positional only syntax as strong check. If this is not desired, let me know and I will change the function to explicitly check for the length of `*args`.~

Edit: maybe the usage of `parametrize` is a bit exaggerated here.
Edit2: Using Python 3.8 only features does not pleases the tests. Updating."
117469078,11631,CLN: tshift & shift could be consolidated,max-sixty,closed,2015-11-17T22:45:59Z,2020-06-15T13:20:34Z,"Particularly now that `shift` only works on datetime-like indexes (https://github.com/pydata/pandas/pull/11211)

They look almost the same, although not exactly:

``` python
In [11]: df=pd.DataFrame(pd.np.random.rand(5,2), index=pd.date_range(periods=5, start='2000'))

In [12]: df
Out[12]: 
                   0         1
2000-01-01  0.640148  0.781291
2000-01-02  0.261649  0.652372
2000-01-03  0.642422  0.734348
2000-01-04  0.582657  0.601868
2000-01-05  0.848645  0.078437

In [13]: df.shift()
Out[13]: 
                   0         1
2000-01-01       NaN       NaN
2000-01-02  0.640148  0.781291
2000-01-03  0.261649  0.652372
2000-01-04  0.642422  0.734348
2000-01-05  0.582657  0.601868

In [14]: df.tshift()
Out[14]: 
                   0         1
2000-01-02  0.640148  0.781291
2000-01-03  0.261649  0.652372
2000-01-04  0.642422  0.734348
2000-01-05  0.582657  0.601868
2000-01-06  0.848645  0.078437

In [15]: df.shift(freq='D')
Out[15]: 
                   0         1
2000-01-02  0.640148  0.781291
2000-01-03  0.261649  0.652372
2000-01-04  0.642422  0.734348
2000-01-05  0.582657  0.601868
2000-01-06  0.848645  0.078437

In [16]: df.tshift(freq='D')
Out[16]: 
                   0         1
2000-01-02  0.640148  0.781291
2000-01-03  0.261649  0.652372
2000-01-04  0.642422  0.734348
2000-01-05  0.582657  0.601868
2000-01-06  0.848645  0.078437

```
"
638636406,34785,DOC: add release note about revert for 1.0.5,jorisvandenbossche,closed,2020-06-15T08:08:22Z,2020-06-15T13:38:02Z,"Whatsnew for https://github.com/pandas-dev/pandas/pull/34632

cc @simonjayhawkins @alimcmaster1 "
584723135,32844,Switch dataframe constructor to use dispatch,saulshanabrook,closed,2020-03-19T21:55:08Z,2020-06-15T14:06:22Z,"This is an attempt to add extensibility to the `DataFrame` constructor so that third party libraries can register their own ways of converting to a Pandas Dataframe. It does this by creating a [`singledispatch` function](https://docs.python.org/3/library/functools.html#functools.singledispatch) that is used in the constructor.

For example, Dask could implement the function like this:

```python
from pandas.core.construction import create_dataframe
import dask.datafame

@create_dataframe.register
def _create_dataframe_dask(data: dask.datafame.DataFrame, *args, **kwargs):
    return create_dataframe(data.compute(), *args, **kwargs)
```

Then, if a downstream library tries to construct a Pandas dataframe from a dask dataframe, it will work:

```python
import dask
import pandas

df = dask.datasets.timeseries()

assert isinstance(pandas.DataFrame(df), pandas.DataFrame)
```


This is response to [the thread about providing a protocol for dataframes](https://discuss.ossdata.org/t/a-dataframe-protocol-for-the-pydata-ecosystem/267) to present an alternative for the underlying use case. The alternative is:

1. Force libraries like sk learn to depend on pandas
2. Have them called `pandas.Dataframe` on their input data to see if it can be turned into a dataframe
3. Have third party libraries with alternative dataframe implementations register themselves with this function provided here.

It doesn't try to solve any sort of out-of-core dataframe API conversation and it does require all libraries to have Pandas as a hard dependency.

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
- [x] Review name of function

"
638860670,34797,Backport PR #34785 on branch 1.0.x (DOC: add release note about revert for 1.0.5),meeseeksmachine,closed,2020-06-15T13:36:51Z,2020-06-15T14:20:08Z,Backport PR #34785: DOC: add release note about revert for 1.0.5
637957833,34735,API: Remove PeriodDtype.dtype_code from public API,TomAugspurger,closed,2020-06-12T19:22:15Z,2020-06-15T14:24:12Z,"On master PeriodDtype has a `.dtype_code` attribute.

```python
In [5]: pd.PeriodDtype('D').dtype_code
Out[5]: 6000
```

That wasn't there in 1.0.x. I think we want it to be private."
638822499,34796,API: Removed PeriodDtype.dtype_code from public API,TomAugspurger,closed,2020-06-15T12:41:52Z,2020-06-15T14:24:13Z,Closes #34735 
628378623,34510,Performance regression in TimedeltaIndexing.time_get_loc,jorisvandenbossche,closed,2020-06-01T11:51:48Z,2020-06-15T14:38:54Z,"Setup:

```python
index = pd.timedelta_range(start=""1985"", periods=1000, freq=""D"")
timedelta = index[500]

%timeit index.get_loc(timedelta)
```

```
# master
7.39 µs ± 78.9 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

# 1.0.3
2.82 µs ± 13.5 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
```

See https://pandas.pydata.org/speed/pandas/#timedelta.TimedeltaIndexing.time_get_loc?commits=9929fca8-62c7dd3e

Happened somewhere between May 11 and May 27 (the gap in the benchmarks)"
637888940,34734,PERF: Fixed perf regression in TimedeltaIndex.get_loc,TomAugspurger,closed,2020-06-12T16:59:19Z,2020-06-15T14:39:04Z,"Closes https://github.com/pandas-dev/pandas/issues/34510

1.0.3

```python
In [10]: index = pd.timedelta_range(start=""1985"", periods=1000, freq=""D"")
    ...: timedelta = index[500]
    ...:
    ...: %timeit index.get_loc(timedelta)

3.7 µs ± 275 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

```

This PR

```
3.25 µs ± 67.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
```"
638514230,34783,REF: avoid DTA/PA methods in SemiMonthOffset.apply_index,jbrockmendel,closed,2020-06-15T03:31:47Z,2020-06-15T15:00:32Z,"I think this gets the last implicit-external-dependencies in tslibs.

```
In [2]: dti = pd.date_range(""2016-01-01"", periods=10000, freq=""S"")              
In [3]: off = pd.offsets.SemiMonthEnd(-2)                                       
In [4]: %timeit dti + off                                                       
3.08 ms ± 108 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  # <-- master
486 µs ± 7.16 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  # <-- PR
```"
281600333,18755,Scatter requires x column to be numeric,naught101,closed,2017-12-13T02:03:18Z,2020-06-15T15:02:31Z,"```py
In [30]: df = pd.DataFrame(dict(a=['A', 'B', 'C'], b=[2, 3, 4]))

In [31]: plt.scatter(df['a'], df['b'])
Out[31]: <matplotlib.collections.PathCollection at 0x7f17e17b41d0>
```

This works, and produces this:

![figure_1](https://user-images.githubusercontent.com/167164/33918097-b35d2262-e005-11e7-89a3-31a47647cf7d.png)


On the other hand, this doesn't:

```py
In [32]: df.plot.scatter(x='a', y='b')
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/data/documents/uni/phd/projects/FluxnetTrafficLights/scripts/plots/predictability_plots.py in <module>()                                                                                                     
----> 1 df.plot.scatter(x='a', y='b')

~/miniconda3/envs/science/lib/python3.6/site-packages/pandas/plotting/_core.py in scatter(self, x, y, s, c, **kwds)                                                                                           
   2803         axes : matplotlib.AxesSubplot or np.array of them
   2804         """"""
-> 2805         return self(kind='scatter', x=x, y=y, c=c, s=s, **kwds)
   2806 
   2807     def hexbin(self, x, y, C=None, reduce_C_function=None, gridsize=None,

~/miniconda3/envs/science/lib/python3.6/site-packages/pandas/plotting/_core.py in __call__(self, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)                                                                                           
   2625                           fontsize=fontsize, colormap=colormap, table=table,
   2626                           yerr=yerr, xerr=xerr, secondary_y=secondary_y,
-> 2627                           sort_columns=sort_columns, **kwds)
   2628     __call__.__doc__ = plot_frame.__doc__
   2629 

~/miniconda3/envs/science/lib/python3.6/site-packages/pandas/plotting/_core.py in plot_frame(data, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)                                                                                         
   1867                  yerr=yerr, xerr=xerr,
   1868                  secondary_y=secondary_y, sort_columns=sort_columns,
-> 1869                  **kwds)
   1870 
   1871 

~/miniconda3/envs/science/lib/python3.6/site-packages/pandas/plotting/_core.py in _plot(data, x, y, subplots, ax, kind, **kwds)
   1650         if isinstance(data, DataFrame):
   1651             plot_obj = klass(data, x=x, y=y, subplots=subplots, ax=ax,
-> 1652                              kind=kind, **kwds)
   1653         else:
   1654             raise ValueError(""plot kind %r can only be used for data frames""

~/miniconda3/envs/science/lib/python3.6/site-packages/pandas/plotting/_core.py in __init__(self, data, x, y, s, c, **kwargs)
    808             # the handling of this argument later
    809             s = 20
--> 810         super(ScatterPlot, self).__init__(data, x, y, s=s, **kwargs)
    811         if is_integer(c) and not self.data.columns.holds_integer():
    812             c = self.data.columns[c]

~/miniconda3/envs/science/lib/python3.6/site-packages/pandas/plotting/_core.py in __init__(self, data, x, y, **kwargs)
    783             y = self.data.columns[y]
    784         if len(self.data[x]._get_numeric_data()) == 0:
--> 785             raise ValueError(self._kind + ' requires x column to be numeric')
    786         if len(self.data[y]._get_numeric_data()) == 0:
    787             raise ValueError(self._kind + ' requires y column to be numeric')

ValueError: scatter requires x column to be numeric
```

Why does pandas require x to be numeric if matplotlib doesn't?

using versions from conda, on Kubuntu 17.10:

```
matplotlib                2.1.0            py36hba5de38_0  
pandas                    0.20.3                   py36_0  
python                    3.6.3                h0ef2715_3  
```
"
638940025,34803,"BUG: `read_parquet` gives ""Passed non-file path"" in v1.0.4 but works in v1.0.3",dmitry-kabanov,closed,2020-06-15T15:23:13Z,2020-06-15T15:32:22Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd
pd.read_parquet(""https://www.tangel.se/wind/wind_frames/wind-2018.parquet"")
```

#### Problem description

There is some regression in the interaction of Pandas v1.0.4 and Pyarrow.

The above code downloads a Parquet file when Pandas is v1.0.3.
However, in v1.0.4 it generates the error:
```
  File ""<python-env>/lib/python3.8/site-packages/pandas/io/parquet.py"", line 315, in read_parquet
    return impl.read(path, columns=columns, **kwargs)
  File ""<python-env>/lib/python3.8/site-packages/pandas/io/parquet.py"", line 130, in read
    parquet_ds = self.api.parquet.ParquetDataset(
  File ""<python-env>/lib/python3.8/site-packages/pyarrow/parquet.py"", line 1171, in __init__
    self.metadata_path) = _make_manifest(
  File ""<python-env>/lib/python3.8/site-packages/pyarrow/parquet.py"", line 1367, in _make_manifest
    raise OSError('Passed non-file path: {}'
OSError: Passed non-file path: https://www.tangel.se/wind/wind_frames/wind-2018.parquet
```

I can confirm that the bug appears with Pyarrow of versions 0.15.1 and 0.17.1 on Python 3.7 and 3.8.

#### Expected Output

```
                     x             y          u          v  ...  altitude  height        date   time
0        361912.978162  6.139641e+06   5.338025   5.958648  ...         5      10  2018-01-01  06:00
1        361912.978162  6.139641e+06  10.008798  11.172465  ...         5      10  2018-01-01  09:00
2        361912.978162  6.139641e+06  14.252365   4.676546  ...         5      10  2018-01-01  12:00
3        361912.978162  6.139641e+06  14.107182   7.549001  ...         5      10  2018-01-01  15:00
4        361912.978162  6.139641e+06   8.650998   6.794132  ...         5      10  2018-01-01  18:00
...                ...           ...        ...        ...  ...       ...     ...         ...    ...
1449984  804603.972910  7.610557e+06  -1.789940   0.190043  ...       330      10  2018-12-31  19:00
1449985  804603.972910  7.610557e+06  -1.646950  -0.947394  ...       330      10  2018-12-31  20:00
1449986  804603.972910  7.610557e+06  -1.078928  -0.725200  ...       330      10  2018-12-31  21:00
1449987  804603.972910  7.610557e+06   0.302022  -1.057725  ...       330      10  2018-12-31  22:00
1449988  804603.972910  7.610557e+06   0.920168  -0.391524  ...       330      10  2018-12-31  23:00

[1449989 rows x 9 columns]
```

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.4
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.4.0.post20200518
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.49.1
```

</details>
"
581317141,32706,CI: Update pipelines config to trigger on PRs,datapythonista,closed,2020-03-14T18:26:00Z,2020-06-15T15:57:58Z,"- [X] closes #32705

"
617389749,34156,"BUG: Pandas import error Python 2.7.6 isinstance() arg 2 must be a class, type, or tuple of classes and types ",ghost,closed,2020-05-13T12:02:05Z,2020-06-15T16:06:45Z,"Hi,

I am obliged to use Python 2.7.6 and pandas below 0.24.2.

The only thing I am trying to do as for now is 'import pandas'.

And I keep getting the error below. I have checked online and tslib was fixed in new versions but I have to stay on 2.7.6.
I have already tried pip uninstall and pip install again.

Any possible fix you might know of ? 

Thanking you very much for your answer,
Regards and take care

ERROR LOG:
---------------------------------------------------------------------------------------------------------------

468"",""import   pandas |  
468"",""  File ""C | \Python27\lib\site-packages\pandas\__init__.py"", line   42, in <module>
  |  
468"",""from pandas.core.api import * |  
  |  
468"",""    File ""C | \Python27\lib\site-packages\pandas\core\api.py"",   line 10, in <module>
  |  
468"",""from   pandas.core.groupby import Grouper |  
  |  
468"",""  File ""C | \Python27\lib\site-packages\pandas\core\groupby.py"",   line 47, in <module>
  |  
468"",""from pandas.core.index import   (Index, MultiIndex, |  
  |  
468"",""    File ""C | \Python27\lib\site-packages\pandas\core\index.py"",   line 2, in <module>
  |  
468"",""from   pandas.core.indexes.api import * |  
  |  
468"",""  File ""C | \Python27\lib\site-packages\pandas\core\indexes\api.py"",   line 9, in <module>
  |  
468"",""from pandas.core.indexes.interval   import IntervalIndex  # noqa |  
  |  
468"",""    File ""C | \Python27\lib\site-packages\pandas\core\indexes\interval.py"",   line 30, in <module>
  |  
468"",""from   pandas.core.indexes.datetimes import date_range |  
  |  
468"",""  File ""C | \Python27\lib\site-packages\pandas\core\indexes\datetimes.py"",   line 39, in <module>
  |  
468"",""from pandas.tseries.frequencies   import ( |  
  |  
468"",""    File ""C | \Python27\lib\site-packages\pandas\tseries\frequencies.py"",   line 18, in <module>
  |  
468"",""from   pandas.tseries.offsets import DateOffset |  
  |  
468"",""  File ""C | \Python27\lib\site-packages\pandas\tseries\offsets.py"",   line 2692, in <module>
  |  
484"",""class Tick(SingleConstructorOffset) |  
  |  
484"",""    File ""C | \Python27\lib\site-packages\pandas\tseries\offsets.py"",   line 2693, in Tick
  |  
484"",""_inc   = Timedelta(microseconds=1000) |  
  |  
484"",""  File ""pandas\_libs\tslib.pyx"",   line 2585, in pandas._libs.tslib.Timedelta.__new__ |  
  |  
484"",""TypeError""
484"",""isinstance()   arg 2 must be a class, type, or tuple of classes and types 
  |  
484"",""Python interpretor failed

"
637031928,34711,BLD: Pin cython for 37-locale build,TomAugspurger,closed,2020-06-11T13:47:58Z,2020-06-15T16:17:38Z,"Seeing if this resolves the build failure.

xref https://github.com/pandas-dev/pandas/issues/34704"
637302742,34718,Removed __div__ impls from Cython,WillAyd,closed,2020-06-11T20:32:36Z,2020-06-15T16:20:31Z,Alternate to #34711 which may close #34704
639005392,34807,Backport PR #34804 on branch 1.0.x (TST: ensure read_parquet filter argument is correctly passed though (pyarrow engine)),simonjayhawkins,closed,2020-06-15T17:04:51Z,2020-06-15T17:49:57Z,xref #34804
638771585,34794,Backport PR #34711 on branch 1.0.x (BLD: Pin cython for 37-locale build),simonjayhawkins,closed,2020-06-15T11:22:27Z,2020-06-15T17:51:00Z,xref #34711
638796131,34795,CI: Update pipelines config to trigger on PRs on 1.0.x,simonjayhawkins,closed,2020-06-15T12:01:56Z,2020-06-15T17:52:30Z,"xref #32706

I think maybe the additions should also go in master. let's see what happens here first."
638913588,34802,Backport PR #34718 on branch 1.0.x (Removed __div__ impls),simonjayhawkins,closed,2020-06-15T14:47:33Z,2020-06-15T17:54:07Z,"https://github.com/pandas-dev/pandas/pull/34711#issuecomment-644148383


@WillAyd on 1.0.x. we have  cython>=0.29.13 and on master cython>=0.29.16. is this an issue?
"
635760829,34678,CLN: dont consolidate in NDFrame._is_numeric_mixed_type,jbrockmendel,closed,2020-06-09T21:09:45Z,2020-06-15T18:12:24Z,
629199366,34530,"BUG: series with dtype ""Int64"" don't work correctly with replace() method",MarcusJellinghaus,closed,2020-06-02T13:22:40Z,2020-06-15T19:21:20Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas. ( Version 1.0.4)

---

I get an AssertionError from the following code:

#### Code Sample

```python
Int_series = pd.Series(data=[1, 2, 3], dtype=""Int64"")
Int_series.replace("""", ""ABC"", inplace=True)  # throws exception
```

```pytb
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/sandbox/pandas/pandas/core/internals/blocks.py in replace(self, to_replace, value, inplace, regex, convert)
    731         try:
--> 732             blocks = self.putmask(mask, value, inplace=inplace)
    733             # Note: it is _not_ the case that self._can_hold_element(value)

~/sandbox/pandas/pandas/core/internals/blocks.py in putmask(self, mask, new, inplace, axis, transpose)
   1608
-> 1609         new_values[mask] = new
   1610         return [self.make_block(values=new_values)]

~/sandbox/pandas/pandas/core/arrays/masked.py in __setitem__(self, key, value)
     93             value = [value]
---> 94         value, mask = self._coerce_to_array(value)
     95

~/sandbox/pandas/pandas/core/arrays/integer.py in _coerce_to_array(self, value)
    417     def _coerce_to_array(self, value) -> Tuple[np.ndarray, np.ndarray]:
--> 418         return coerce_to_array(value, dtype=self.dtype)
    419

~/sandbox/pandas/pandas/core/arrays/integer.py in coerce_to_array(values, dtype, mask, copy)
    241     elif not (is_integer_dtype(values) or is_float_dtype(values)):
--> 242         raise TypeError(f""{values.dtype} cannot be converted to an IntegerDtype"")
    243

TypeError: <U3 cannot be converted to an IntegerDtype

During handling of the above exception, another exception occurred:

AssertionError                            Traceback (most recent call last)
<ipython-input-3-36a959f691c3> in <module>
      1 Int_series = pd.Series(data=[1, 2, 3], dtype=""Int64"")
----> 2 Int_series.replace("""", ""ABC"", inplace=True)  # throws exception

~/sandbox/pandas/pandas/core/series.py in replace(self, to_replace, value, inplace, limit, regex, method)
   4449             limit=limit,
   4450             regex=regex,
-> 4451             method=method,
   4452         )
   4453

~/sandbox/pandas/pandas/core/generic.py in replace(self, to_replace, value, inplace, limit, regex, method)
   6662                 elif not is_list_like(value):  # NA -> 0
   6663                     new_data = self._mgr.replace(
-> 6664                         to_replace=to_replace, value=value, inplace=inplace, regex=regex
   6665                     )
   6666                 else:

~/sandbox/pandas/pandas/core/internals/managers.py in replace(self, value, **kwargs)
    592     def replace(self, value, **kwargs) -> ""BlockManager"":
    593         assert np.ndim(value) == 0, value
--> 594         return self.apply(""replace"", value=value, **kwargs)
    595
    596     def replace_list(

~/sandbox/pandas/pandas/core/internals/managers.py in apply(self, f, align_keys, **kwargs)
    400                 applied = b.apply(f, **kwargs)
    401             else:
--> 402                 applied = getattr(b, f)(**kwargs)
    403             result_blocks = _extend_blocks(applied, result_blocks)
    404

~/sandbox/pandas/pandas/core/internals/blocks.py in replace(self, to_replace, value, inplace, regex, convert)
    743                 raise
    744
--> 745             assert not self._can_hold_element(value), value
    746
    747             # try again with a compatible block

AssertionError: ABC
```

#### Problem description

The code does not work on Pandas Version 1.0.4. It throws an exception. However, it works on Pandas Version 1.0.1, and also with dtype=""int64.

#### Expected Output
No crash. function replace() itself does nothing, since there is no """" in the series.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.0.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.0.4
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.0.3
setuptools       : 40.8.0
Cython           : None
pytest           : 5.4.2
hypothesis       : None
sphinx           : 3.0.3
blosc            : None
feather          : None
xlsxwriter       : None

</details>
"
638986971,34806,"Regression in to_timedelta with errors=""coerce"" and unit",TomAugspurger,closed,2020-06-15T16:33:03Z,2020-06-15T20:16:28Z,"Introduced in https://github.com/pandas-dev/pandas/commit/d3f686bb50c14594087171aa0493cb07eb5a874c

In pandas 1.0.3

```python
In [2]: pd.to_timedelta([1, 2, 'error'], errors=""coerce"", unit=""ns"")
Out[2]: TimedeltaIndex(['00:00:00.000000', '00:00:00.000000', NaT], dtype='timedelta64[ns]', freq=None)
```

In master, we raise.

```pytb
In [2]: pd.to_timedelta([1, 2, 'error'], errors=""coerce"", unit=""ns"")
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-2-a3691c044041> in <module>
----> 1 pd.to_timedelta([1, 2, 'error'], errors=""coerce"", unit=""ns"")

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/tools/timedeltas.py in to_timedelta(arg, unit, errors)
    101         arg = arg.item()
    102     elif is_list_like(arg) and getattr(arg, ""ndim"", 1) == 1:
--> 103         return _convert_listlike(arg, unit=unit, errors=errors)
    104     elif getattr(arg, ""ndim"", 1) > 1:
    105         raise TypeError(

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/tools/timedeltas.py in _convert_listlike(arg, unit, errors, name)
    140
    141     try:
--> 142         value = sequence_to_td64ns(arg, unit=unit, errors=errors, copy=False)[0]
    143     except ValueError:
    144         if errors == ""ignore"":

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/arrays/timedeltas.py in sequence_to_td64ns(data, copy, unit, errors)
    927     if is_object_dtype(data.dtype) or is_string_dtype(data.dtype):
    928         # no need to make a copy, need to convert if string-dtyped
--> 929         data = objects_to_td64ns(data, unit=unit, errors=errors)
    930         copy = False
    931

~/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/arrays/timedeltas.py in objects_to_td64ns(data, unit, errors)
   1037     values = np.array(data, dtype=np.object_, copy=False)
   1038
-> 1039     result = array_to_timedelta64(values, unit=unit, errors=errors)
   1040     return result.view(""timedelta64[ns]"")
   1041

pandas/_libs/tslibs/timedeltas.pyx in pandas._libs.tslibs.timedeltas.array_to_timedelta64()

ValueError: unit must not be specified if the input contains a str
```

This restores the 1.0.3 behavior, and adds an additional test for `errors=""ignore""`, and cleans up the  `to_timedelta` docstring."
638121158,34740,BUG: pd.NA.__format__ fails with format_specs,topper-123,closed,2020-06-13T08:22:28Z,2020-06-15T22:34:22Z,"``pd.NA`` fails if passed to a format string and format parameters are supplied. This is different behaviour than ``np.nan`` and makes converting arrays containing ``pd.NA`` to strings very brittle and annoying.

Examples:

```python
>>> format(pd.NA)
'<NA>'  # master and PR, ok
>>> format(pd.NA, "".1f"")
TypeError  # master
'<NA>'  # this PR
>>> format(pd.NA, "">5"")
TypeError  # master
' <NA>'  # this PR, tries to behave like a string, then falls back to '<NA>', like np.na
```

The new behaviour mirrors the behaviour of ``np.nan``.
"
639042161,34808,"CLN: liboffsets annotate, de-duplicate",jbrockmendel,closed,2020-06-15T18:11:49Z,2020-06-15T22:40:42Z,
499024772,28641,"GroupBy(..., as_index=True).agg() drops index when ",adamhooper,closed,2019-09-26T17:41:22Z,2020-06-15T22:53:27Z,"#### Code Sample, a copy-pastable example if possible

```ipython
In [1]: import pandas as pd

In [2]: pd.DataFrame({""A"": [1997], ""B"": pd.Series([""b""], dtype=""category"").cat.as_ordered()}).groupby(""A"", as_index=True).agg({""B"": ""size""})
Out[2]:
      B
A
1997  1

In [3]: pd.DataFrame({""A"": [1997], ""B"": pd.Series([""b""], dtype=""category"").cat.as_ordered()}).groupby(""A"", as_index=True).agg({""B"": ""min""})
Out[3]:
   B
0  b
```
#### Problem description

When aggregating `min`, `max` or `first` of a categorical column, `.agg()` returns a dataframe with a default index instead of the index returned by `groupby()`.

(This may be related to #13416 ... but I think it's a clear, well-defined bug so maybe easier to resolve?)

In my case, I think I can work around this problem with a hack: if I `.agg({""B"": [""min"", ""size""]})` and then ignore the `(""B"", ""size"")` output column, Pandas will output a dataframe with the correct index.

#### Expected Output

```
Out[3]:
      B
A
1997  b
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.2.11-200.fc30.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.0
numpy            : 1.16.1
pytz             : 2018.9
dateutil         : 2.8.0
pip              : 19.0.3
setuptools       : 40.8.0
Cython           : 0.29.5
pytest           : 4.5.0
hypothesis       : None
sphinx           : 2.1.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.2.5
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.2.0
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : None
fastparquet      : 0.2.1
gcsfs            : None
lxml.etree       : 4.2.5
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.14.1
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
589576182,33098,"TST: GroupBy(..., as_index=True).agg() drops index ",phofl,closed,2020-03-28T13:19:45Z,2020-06-16T07:29:21Z,"- [x] closes #28641
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

While searching for related issue for my other pull request, I found that this issue must have been fixed with another commit in the past (don't know when this issue was fixed).

I added a test to ensure that this error won't know occur in the future."
638941929,34804,TST: ensure read_parquet filter argument is correctly passed though (pyarrow engine),jorisvandenbossche,closed,2020-06-15T15:25:52Z,2020-06-16T09:24:46Z,xref https://github.com/pandas-dev/pandas/issues/26551#issuecomment-643882543
637874359,34733,BUG: Fixed Series.replace for EA with casting,TomAugspurger,closed,2020-06-12T16:30:49Z,2020-06-16T09:46:26Z,Closes https://github.com/pandas-dev/pandas/issues/34530
617267583,34152,CI: Address linting errors in flake8 >= 3.8.1,mgmarino,closed,2020-05-13T08:52:07Z,2020-06-16T10:01:31Z,"xref #34150

This partially addresses the above issue by:

- Ignoring E741
- Fixing other linting errors

The issue with flake8-rst is handled temporarily in #34151, and the build here will fail the flake8-rst part until that is merged in."
639526587,34819,Backport PR #34733 on branch 1.0.x (BUG: Fixed Series.replace for EA with casting),simonjayhawkins,closed,2020-06-16T09:46:06Z,2020-06-16T11:24:42Z,xref #34733
638627528,34784,BUG: pd.Series.rolling - center=True doesnt work when using numba engine,PeterLuenenschloss,closed,2020-06-15T07:53:59Z,2020-06-16T12:47:57Z,"

**Rolling over a Series with center=True gives different results depending on the engine used:**

```python
import pandas as pd
test_ser = pd.Series(np.arange(0,10))
rolled_without_numba =test_ser.rolling(3, center=True).apply(lambda x:x[1],raw=True)

rolled_with_numba = test_ser.rolling(3, center=True).apply(lambda x:x[1],raw=True, engine='numba', engine_kwargs={'no_python':True})
```

#### Problem description

wheras the result for rolling without the engine is as expected:

```python
rolled_without_numba.head()
0    NaN
1    1.0
2    2.0
3    3.0
4    4.0
```
rolling with the numba engine gives:

```python
rolled_with_numba.head()
0    NaN
1    NaN
2    1.0
3    2.0
4    3.0
```
The behavior is problematic at least because its not documented and causes frustrating bug tracing sessions. That the use of numba engine may change the result numerically, is not the first guess.
.
Since rolling with offset is not possible when using numba engine anyway, finding the center labels after rolling is not too big a problem - so maybe instead of a fix, a documentation or a warning would be sufficient.

#### Expected Output

```python
rolled_with_numba.equals(rolled_without_numba)
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-53-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : de_DE.UTF-8
LOCALE           : de_DE.UTF-8

pandas           : 1.0.4
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.0.3
setuptools       : 46.1.3
Cython           : 0.29.17
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.2.1
html5lib         : 0.999999999
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : 4.6.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.1
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : 0.8.3
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
639436226,34817,DOC: move 'Other API changes' under correct section,jorisvandenbossche,closed,2020-06-16T07:27:09Z,2020-06-16T13:00:35Z,"The ""Other API changes"" bullet points were split in two (one part under ""Other API changes"", and one part directly under ""Backwards incompatible API changes""), and in previous whatsnew files, the ""Other API changes"" is also a subsection of ""Backwards incompatible API changes"".

So moved a few things around to make this consistent."
639449199,34818,CLN: remove unused args/kwargs in BlockManager.reduce,jorisvandenbossche,closed,2020-06-16T07:48:52Z,2020-06-16T15:33:28Z,Small clean-up broken off from https://github.com/pandas-dev/pandas/pull/32867 cc @jbrockmendel 
639343611,34815,CLN: liboffsets annotations,jbrockmendel,closed,2020-06-16T03:35:11Z,2020-06-16T15:34:51Z,It looks like rollforward/rollback we have some tests that pass `date` instead of `datetime`.  That and a couple other outliers means there are some things that are not yet annotated.
639353778,34816,BUG: Respect center=True in rolling.apply when numba engine is used,mroeschke,closed,2020-06-16T04:07:34Z,2020-06-16T15:40:30Z,"- [x] closes #34784
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
639313468,34813,REF: move Resolution to tslibs.dtypes,jbrockmendel,closed,2020-06-16T02:04:17Z,2020-06-16T15:51:10Z,"This is pretty much a clean move, in preparation for making FreqGroup an enum and trying to de-duplicate our 3+ enum-like classes"
581619165,32723,BUG: Fix HDFStore empty keys on native HDF5 file by adding keyword include,roberthdevries,closed,2020-03-15T10:44:05Z,2020-06-16T18:47:24Z,"- [x] closes #29916
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
638358167,34768,"BUG, TST: fix-_check_ticks_props",MarcoGorelli,closed,2020-06-14T14:02:33Z,2020-06-16T19:34:12Z,"Here's something I noticed while working on #34334 : `self._check_ticks_props(ax, ylabelsize=0)` always passes!

This is because of
```python
if ylabelsize
```
instead of
```python
if ylabelsize is not None
```
being used.

The test I've added fails on master:
```python-traceback
(pandas-dev) marco@marco-Predator-PH315-52:~/pandas-dev$ pytest pandas/tests/plotting/test_frame.py::TestDataFramePlots::test_plot_with_rot
============================================================== test session starts ==============================================================
platform linux -- Python 3.8.2, pytest-5.4.1, py-1.8.1, pluggy-0.13.1
rootdir: /home/marco/pandas-dev, inifile: setup.cfg
plugins: cov-2.8.1, xdist-1.31.0, asyncio-0.10.0, hypothesis-5.8.0, forked-1.1.2
collected 1 item                                                                                                                                

pandas/tests/plotting/test_frame.py .                                                                                                     [100%]

=============================================================== 1 passed in 0.33s ===============================================================  
(pandas-dev) marco@marco-Predator-PH315-52:~/pandas-dev$ git checkout upstream/master -- pandas/tests/plotting/common.py
(pandas-dev) marco@marco-Predator-PH315-52:~/pandas-dev$ pytest pandas/tests/plotting/test_frame.py::TestDataFramePlots::test_plot_with_rot
============================================================== test session starts ==============================================================
platform linux -- Python 3.8.2, pytest-5.4.1, py-1.8.1, pluggy-0.13.1
rootdir: /home/marco/pandas-dev, inifile: setup.cfg
plugins: cov-2.8.1, xdist-1.31.0, asyncio-0.10.0, hypothesis-5.8.0, forked-1.1.2
collected 1 item                                                                                                                                

pandas/tests/plotting/test_frame.py F                                                                                                     [100%]

=================================================================== FAILURES ====================================================================
_____________________________________________________ TestDataFramePlots.test_plot_with_rot _____________________________________________________

self = <pandas.tests.plotting.test_frame.TestDataFramePlots object at 0x7f8c7578f3a0>

    def test_plot_with_rot(self):
        # GH 34768
        df = pd.DataFrame({""b"": [0, 1, 0], ""a"": [1, 2, 3]})
        ax = _check_plot_works(df.plot, rot=30)
        ax.yaxis.set_tick_params(rotation=30)
        msg = ""expected 0.00000 but got ""
        with pytest.raises(AssertionError, match=msg):
>           self._check_ticks_props(ax, xrot=0)
E           Failed: DID NOT RAISE <class 'AssertionError'>

pandas/tests/plotting/test_frame.py:3360: Failed
============================================================ short test summary info ============================================================
FAILED pandas/tests/plotting/test_frame.py::TestDataFramePlots::test_plot_with_rot - Failed: DID NOT RAISE <class 'AssertionError'>
=============================================================== 1 failed in 0.46s ===============================================================
```"
639847242,34828,REF: remove libfrequencies,jbrockmendel,closed,2020-06-16T17:30:02Z,2020-06-16T20:51:28Z,
628709621,34519,BUG: read_csv stopped working with s3 file system,hellocoldworld,closed,2020-06-01T20:36:00Z,2020-06-16T21:16:59Z,"- [yes ] I have checked that this issue has not already been reported.

- [ yes ] I have confirmed this bug exists on the latest version of pandas.

- [ yes] (optional) I have confirmed this bug exists on the master branch of pandas.
Checked against pandas 1.1.0.dev0+1732.g2428cdda3


#### Problem description

read_csv in pandas1.0.4 has stopped working with s3fs.

On pandas1.0.3

```python
import pandas as pd; import s3fs
s3fs.S3FileSystem(anon=False, key=os.environ.get(""STORE_USERNAME""), secret=os.environ.get(""STORE_PASSWORD""))
df = pd.read_csv(filepath_or_buffer=""s3://my-private-bucket/my_dataframe.csv"")
print(df.shape)
```
prints the correct output, whilst using pandas1.0.4 it raises the following exception

```python
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/nico/.local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""/home/nico/.local/lib/python3.7/site-packages/pandas/io/parsers.py"", line 431, in _read
    filepath_or_buffer, encoding, compression
  File ""/home/nico/.local/lib/python3.7/site-packages/pandas/io/common.py"", line 212, in get_filepath_or_buffer
    filepath_or_buffer, encoding=encoding, compression=compression, mode=mode
  File ""/home/nico/.local/lib/python3.7/site-packages/pandas/io/s3.py"", line 52, in get_filepath_or_buffer
    file, _fs = get_file_and_filesystem(filepath_or_buffer, mode=mode)
  File ""/home/nico/.local/lib/python3.7/site-packages/pandas/io/s3.py"", line 42, in get_file_and_filesystem
    file = fs.open(_strip_schema(filepath_or_buffer), mode)
  File ""/home/nico/.local/lib/python3.7/site-packages/fsspec/spec.py"", line 775, in open
    **kwargs
  File ""/home/nico/.local/lib/python3.7/site-packages/s3fs/core.py"", line 378, in _open
    autocommit=autocommit, requester_pays=requester_pays)
  File ""/home/nico/.local/lib/python3.7/site-packages/s3fs/core.py"", line 1097, in __init__
    cache_type=cache_type)
  File ""/home/nico/.local/lib/python3.7/site-packages/fsspec/spec.py"", line 1065, in __init__
    self.details = fs.info(path)
  File ""/home/nico/.local/lib/python3.7/site-packages/s3fs/core.py"", line 530, in info
    Key=key, **version_id_kw(version_id), **self.req_kw)
  File ""/home/nico/.local/lib/python3.7/site-packages/s3fs/core.py"", line 200, in _call_s3
    return method(**additional_kwargs)
  File ""/home/nico/.local/lib/python3.7/site-packages/botocore/client.py"", line 316, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/home/nico/.local/lib/python3.7/site-packages/botocore/client.py"", line 622, in _make_api_call
    operation_model, request_dict, request_context)
  File ""/home/nico/.local/lib/python3.7/site-packages/botocore/client.py"", line 641, in _make_request
    return self._endpoint.make_request(operation_model, request_dict)
  File ""/home/nico/.local/lib/python3.7/site-packages/botocore/endpoint.py"", line 102, in make_request
    return self._send_request(request_dict, operation_model)
  File ""/home/nico/.local/lib/python3.7/site-packages/botocore/endpoint.py"", line 132, in _send_request
    request = self.create_request(request_dict, operation_model)
  File ""/home/nico/.local/lib/python3.7/site-packages/botocore/endpoint.py"", line 116, in create_request
    operation_name=operation_model.name)
  File ""/home/nico/.local/lib/python3.7/site-packages/botocore/hooks.py"", line 356, in emit
    return self._emitter.emit(aliased_event_name, **kwargs)
  File ""/home/nico/.local/lib/python3.7/site-packages/botocore/hooks.py"", line 228, in emit
    return self._emit(event_name, kwargs)
  File ""/home/nico/.local/lib/python3.7/site-packages/botocore/hooks.py"", line 211, in _emit
    response = handler(**kwargs)
  File ""/home/nico/.local/lib/python3.7/site-packages/botocore/signers.py"", line 90, in handler
    return self.sign(operation_name, request)
  File ""/home/nico/.local/lib/python3.7/site-packages/botocore/signers.py"", line 160, in sign
    auth.add_auth(request)
  File ""/home/nico/.local/lib/python3.7/site-packages/botocore/auth.py"", line 357, in add_auth
    raise NoCredentialsError
botocore.exceptions.NoCredentialsError: Unable to locate credentials
```

#### Output of ``pd.show_versions()``
using pandas 1.0.3

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-46-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : es_AR.UTF-8
LOCALE           : es_AR.UTF-8

pandas           : 1.0.3
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 9.0.1
setuptools       : 39.0.1
Cython           : None
pytest           : 5.4.0
hypothesis       : None
sphinx           : 1.6.7
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 0.999999999
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.0
pyxlsb           : None
s3fs             : 0.4.2
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>

using pandas 1.0.4
<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-46-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : es_AR.UTF-8
LOCALE           : es_AR.UTF-8

pandas           : 1.0.4
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 9.0.1
setuptools       : 39.0.1
Cython           : None
pytest           : 5.4.0
hypothesis       : None
sphinx           : 1.6.7
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 0.999999999
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.0
pyxlsb           : None
s3fs             : 0.4.2
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
639734508,34825,"BUG: pd.concat(..., copy=False) still causes copy on block consolidation.",DamianBarabonkovQC,open,2020-06-16T14:54:27Z,2020-06-16T21:36:23Z,"#### Problem description

When concatenating columns of the same dtype, even with `copy=False` option, the columns are consolidated together which involves a copy and a costly `vstack`. The performance is actually worse for `copy=False` than the default `copy=True` which is misleading.

There are use cases where consolidated data is not required for an application, so this unneeded performance penalty is undesired.

#### Sample Program

```python
import time
import pandas as pd

template_series = pd.Series(list(range(10000)))

series_ls = []
for i in range(1000):
    series_ls.append(template_series.copy())

start_time = time.time()
df_no_copy = pd.concat(series_ls, copy=False)
print(""No copy elapsed"", time.time() - start_time)

start_time = time.time()
df_copy = pd.concat(series_ls, copy=True) # The default setting
print(""Copy elapsed"", time.time() - start_time)
```

#### Execution Time

```
No copy elapsed 0.07740044593811035
Copy elapsed 0.05434751510620117
```

Execution time is in seconds.

#### Problem Trace

The consolidation occurs as a result of:

```python
if not self.copy:
    new_data._consolidate_inplace()
```

located near ""pandas/core/reshape/concat.py:499""."
638758449,34793,CI testing for s3 reads from public buckets on 1.0.x - DO NOT MERGE,simonjayhawkins,closed,2020-06-15T11:01:53Z,2020-06-17T07:29:33Z,xref #34626
458421410,26959,"API? how to do a ""rolling groupby"" or groupby with overlapping groups?",jorisvandenbossche,open,2019-06-20T08:21:41Z,2020-06-17T10:24:22Z,"Sparked by @betatim's question on twitter: https://twitter.com/betatim/status/1141321049918906368

Suppose you have the following data:

```
repo_id = np.random.choice(np.arange(1000), 10000)
index = pd.Timestamp(""2019-01-01"") + pd.to_timedelta(np.random.randint(0, 24*30*6, size=10000), unit='H')
df = pd.DataFrame({'repo_id': repo_id}, index=index).sort_index()
```

```
In [111]: df.head() 
Out[111]: 
                     repo_id
2019-01-01 01:00:00      162
2019-01-01 01:00:00      850
2019-01-01 01:00:00      414
2019-01-01 02:00:00      753
2019-01-01 02:00:00      125
```

Data at somewhat randomly points in time (here hourly, but precision doesn't matter much).

Assume now you want to calculate a rolling 30D (monthly) statistic for each day.  
For example the rolling number of unique values over a period of time (the elegant but somewhat verbose python code to do this: https://gist.github.com/betatim/c59039682d92fab89859358e8c585313)

A rolling operation does not exactly give what you want, because `df.rolling()` will calculate this 24H mean for each row, while there might be many rows for a single day, and you are only interested in 1 value for that day.
A groupby/resample operation can also not achieve this goal, as that could give monthly number of unique values, but it cannot shift this month period one day at a time, since the groups cannot be overlapping.

With rolling you can do something like `df.rolling(""30D"").nunique().resample('D').last()`. But this is 1) very slow, as the `rolling` in the first step is doing way too many calculations that you afterwards throw away with `resample().last()`, and 2) I am not sure the binning for timestamps within the same day happens fully as desired.

I found this an interesting use case, to think about if there are ways to better handle this in pandas, or how such an API could look like."
640279833,34845,DOC: 1.0.5 release date,simonjayhawkins,closed,2020-06-17T09:15:02Z,2020-06-17T11:00:19Z,
537563519,30257,DOC: .get_slice_bound in MultiIndex needs documentation.,proost,closed,2019-12-13T13:56:19Z,2020-06-17T11:31:50Z,"- [x] closes #29967
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I'm not good at English. So if there are wrong sentences or awkward sentences, please let me know.
"
640348317,34846,Backport PR #34845 on branch 1.0.x (DOC: 1.0.5 release date),meeseeksmachine,closed,2020-06-17T10:57:34Z,2020-06-17T11:45:01Z,Backport PR #34845: DOC: 1.0.5 release date
628521607,34516,CI: Linux py37_np_dev failing on 1.0.x,simonjayhawkins,closed,2020-06-01T15:14:54Z,2020-06-02T07:22:02Z,xref https://github.com/pandas-dev/pandas/pull/34503#issuecomment-636651219
559076127,31613,BUG: Ensure same index is returned for slow and fast path in groupby.apply,fjetter,closed,2020-02-03T13:15:28Z,2020-06-02T09:31:04Z,"This _fixes_ the internal check to be consistent with the slow apply path

- [x] closes #31612
- [x] closes #14927
closes #13056 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
617115725,34147,INT: take DateOffset out of the inheritance tree for BaseOffset subclasses,jbrockmendel,closed,2020-05-13T03:39:56Z,2020-06-02T11:37:32Z,"Use a `__instancecheck__` override to keep isinstance checks working.

This will allow us to start moving other BaseOffset subclasses up into liboffsets without having to get DateOffset (which will be one of the trickiest) first."
628846884,34523,CLN: stronger typing for Period.freq,jbrockmendel,closed,2020-06-02T02:14:35Z,2020-06-02T14:39:22Z,
628864172,34524,CLN: remove Resolution.get_attrname_from_abbrev,jbrockmendel,closed,2020-06-02T03:05:17Z,2020-06-02T14:39:40Z,"rename get_str -> attrname, make it a property instead of a classmethod"
627615601,34475,BLD/PERF: bump cython to 0.29.19 for searchsorted,jbrockmendel,closed,2020-05-30T00:56:28Z,2020-06-02T18:31:03Z,"cython 0.29.19 fixed a bug that now allows us to use `cnp.PyArray_SearchSorted` to make `ndarray.searchsorted` into a C call.  As a demonstration, this PR makes that change in `normalize_i8_timestamps` and shaves about 12% off of it for the scalar case (the non-scalar case is not noticeably affected)

```
In [4]: ts2 = pd.Timestamp.now(""US/Pacific"")                                    
In [5]: %timeit ts2.normalize()                                                 
59.3 µs ± 197 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)   # <-- master
51.8 µs ± 362 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)   # <-- PR
```

Not a particularly big deal on its own, _but_ this makes viable passing `int64_t*` instead of `int64_t[:]`, which in turn makes it so we can skip a wrapping/unwrapping step in the scalar code _and_ ideally share code between Timestamp/DatetimeArray, Period/PeriodArray, Timedelta/TimedeltaArray."
559140740,31620,Does #28827 also apply to applymap?,MarcoGorelli,closed,2020-02-03T15:02:57Z,2020-06-02T21:04:15Z,"In  #28854, the note 

> In the current implementation applymap calls func twice on the first column/row to decide whether it can take a fast or slow code path. This can lead to unexpected behavior if func has side-effects, as they will take effect twice for the first column/row.

was removed from the `DataFrame.apply` docstring.

However, it is still in the [docstring](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.applymap.html?highlight=applymap#pandas.DataFrame.applymap) of `DataFrame.applymap`.

Does it need to be removed there too?

EDIT
----

As far as I can tell, this behaviour (calling the function twice to decide which path to take) still happens for `groupby.apply` and is what a user was asking about in #31111. Does it need to be documented in Groupby.Apply? Do end-users need to know about such internals?"
618536933,34183,BUG/ENH: Fix apply to only call `func` once on the first column/row,alonme,closed,2020-05-14T21:08:49Z,2020-06-02T21:04:27Z,"closes #33879
closes #30815
closes #31620
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
629265128,34533,DOC: remove an extra colon,partev,closed,2020-06-02T14:47:48Z,2020-06-02T21:30:44Z,"fix a typo by removing an extra colon

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
359159958,22663,Operation between DataFrame with non-numeric types and incomplete series,toobaz,closed,2018-09-11T18:09:20Z,2020-06-02T22:21:04Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=list('abc'))

In [3]: df + pd.Series([-1, -2], index=list('ab'))
Out[3]: 
     a    b   c
0  0.0  0.0 NaN
1  3.0  3.0 NaN

In [4]: df['c'] = 'c'

In [5]: df + pd.Series([-1, -2], index=list('ab'))
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/nobackup/repo/pandas/pandas/core/ops.py in na_op(x, y)
   1714         try:
-> 1715             result = expressions.evaluate(op, str_rep, x, y, **eval_kwargs)
   1716         except TypeError:

~/nobackup/repo/pandas/pandas/core/computation/expressions.py in evaluate(op, op_str, a, b, use_numexpr, **eval_kwargs)
    204     if use_numexpr:
--> 205         return _evaluate(op, op_str, a, b, **eval_kwargs)
    206     return _evaluate_standard(op, op_str, a, b)

~/nobackup/repo/pandas/pandas/core/computation/expressions.py in _evaluate_numexpr(op, op_str, a, b, truediv, reversed, **eval_kwargs)
    119     if result is None:
--> 120         result = _evaluate_standard(op, op_str, a, b)
    121 

~/nobackup/repo/pandas/pandas/core/computation/expressions.py in _evaluate_standard(op, op_str, a, b, **eval_kwargs)
     64     with np.errstate(all='ignore'):
---> 65         return op(a, b)
     66 

TypeError: Can't convert 'float' object to str implicitly

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-5-c60f1e54c070> in <module>()
----> 1 df + pd.Series([-1, -2], index=list('ab'))

~/nobackup/repo/pandas/pandas/core/ops.py in f(self, other, axis, level, fill_value)
   1737             return _combine_series_frame(self, other, na_op,
   1738                                          fill_value=fill_value, axis=axis,
-> 1739                                          level=level, try_cast=True)
   1740         else:
   1741             if fill_value is not None:

~/nobackup/repo/pandas/pandas/core/ops.py in _combine_series_frame(self, other, func, fill_value, axis, level, try_cast)
   1655         # default axis is columns
   1656         return self._combine_match_columns(other, func, level=level,
-> 1657                                            try_cast=try_cast)
   1658 
   1659 

~/nobackup/repo/pandas/pandas/core/frame.py in _combine_match_columns(self, other, func, level, try_cast)
   4832         new_data = left._data.eval(func=func, other=right,
   4833                                    axes=[left.columns, self.index],
-> 4834                                    try_cast=try_cast)
   4835         return self._constructor(new_data)
   4836 

~/nobackup/repo/pandas/pandas/core/internals/managers.py in eval(self, **kwargs)
    527 
    528     def eval(self, **kwargs):
--> 529         return self.apply('eval', **kwargs)
    530 
    531     def quantile(self, **kwargs):

~/nobackup/repo/pandas/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)
    421 
    422             kwargs['mgr'] = self
--> 423             applied = getattr(b, f)(**kwargs)
    424             result_blocks = _extend_blocks(applied, result_blocks)
    425 

~/nobackup/repo/pandas/pandas/core/internals/blocks.py in eval(self, func, other, errors, try_cast, mgr)
   1416         try:
   1417             with np.errstate(all='ignore'):
-> 1418                 result = get_result(other)
   1419 
   1420         # if we have an invalid shape/broadcast error

~/nobackup/repo/pandas/pandas/core/internals/blocks.py in get_result(other)
   1384                 result = func(values, other)
   1385             else:
-> 1386                 result = func(values, other)
   1387 
   1388             # mask if needed

~/nobackup/repo/pandas/pandas/core/ops.py in na_op(x, y)
   1715             result = expressions.evaluate(op, str_rep, x, y, **eval_kwargs)
   1716         except TypeError:
-> 1717             result = masked_arith_op(x, y, op)
   1718 
   1719         result = missing.fill_zeros(result, x, y, op_name, fill_zeros)

~/nobackup/repo/pandas/pandas/core/ops.py in masked_arith_op(x, y, op)
    819             # errors in Py3 (TypeError) vs Py2 (ValueError)
    820             # Note: Only = an issue in DataFrame case
--> 821             raise ValueError('Cannot broadcast operands together.')
    822 
    823         if mask.any():

ValueError: Cannot broadcast operands together.
```

#### Problem description

``df + df[['a', 'b']]`` works, and the above also worked until recently (it works for instance in 0.19, although it raises a ``VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 1 but corresponding boolean dimension is 2`` - which however makes no reference to the mixed dtype).

#### Expected Output

In 0.19, ``In [3]`` and ``In [5]`` would yield the same result.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.0-7-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: it_IT.UTF-8
LOCALE: it_IT.UTF-8

pandas: 0.24.0.dev0+465.ge775f9ab6
pytest: 3.5.0
pip: 9.0.1
setuptools: 39.2.0
Cython: 0.28.4
numpy: 1.14.3
scipy: 0.19.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: 1.5.6
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: 1.2.0dev
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.2.2.post1634.dev0+ge8120cf6d
openpyxl: 2.3.0
xlrd: 1.0.0
xlwt: 1.3.0
xlsxwriter: 0.9.6
lxml: 4.1.1
bs4: 4.5.3
html5lib: 0.999999999
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.2.1
gcsfs: None

</details>
"
611480089,33956,BUG: throwing error in interpolate depending on dtype of column names,CloseChoice,closed,2020-05-03T18:47:40Z,2020-06-02T22:42:50Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas. (on 862db6421256, last commit where build works as of 20:42 2020-05-03).

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
df
Out[6]: 
     A     B     C
0  1.0   2.0   3.0
1  2.0   4.0   6.0
2  3.0   6.0   9.0
3  4.0   NaN   NaN
4  NaN   8.0   NaN
5  5.0  10.0  30.0
df.interpolate(method='ffill', axis=1)
```
Throws an error `ValueError: Index column must be numeric or datetime type when using ffill method other than linear. Try setting a numeric or datetime index column before interpolating.` But the following code works:

```python
df.columns = [1, 2, 3]
df
Out[9]: 
     1     2     3
0  1.0   2.0   3.0
1  2.0   4.0   6.0
2  3.0   6.0   9.0
3  4.0   NaN   NaN
4  NaN   8.0   NaN
5  5.0  10.0  30.0
df.interpolate(method='ffill', axis=1)
Out[10]: 
     1     2     3
0  1.0   2.0   3.0
1  2.0   4.0   6.0
2  3.0   6.0   9.0
3  4.0   6.0   9.0
4  4.0   8.0   9.0
5  5.0  10.0  30.0
```
#### Problem description

Throwing an error should not depend on the dtype of column names.

#### Expected Output

```python
    A     B     C
0  1.0   2.0   3.0
1  2.0   4.0   6.0
2  3.0   6.0   9.0
3  4.0   6.0   9.0
4  4.0   8.0   9.0
5  5.0  10.0  30.0
```

#### Output of ``pd.show_versions()``

```
pd.show_versions()
INSTALLED VERSIONS
------------------
commit           : 862db6421256cb7a00ae3e88a4a6999347b76271
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-51-generic
Version          : #44~18.04.2-Ubuntu SMP Thu Apr 23 14:27:18 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.1.0.dev0+1463.g862db6421
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1
setuptools       : 46.1.3.post20200325
Cython           : 0.29.17
pytest           : 5.4.1
hypothesis       : 5.10.4
sphinx           : 3.0.3
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : 1.3.2
fastparquet      : 0.3.3
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.17.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.16
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0
```

</details>
"
510535395,29146,df.interpolate(method='pad')  axis is not consistent with df.fillna(method='pad'),markxwang,closed,2019-10-22T09:30:38Z,2020-06-02T22:42:51Z,"```df.interpolate(method='pad')``` behaves weirdly regarding the axis. 

```python
df = pd.DataFrame([[1,np.nan,3],[np.nan, 2,np.nan],[2,3,np.nan]])

     0    1    2
0  1.0  NaN  3.0
1  NaN  2.0  NaN
2  2.0  3.0  NaN


df.fillna(method='pad', axis=0)

  0    1    2
0  1.0  NaN  3.0
1  1.0  2.0  3.0
2  2.0  3.0  3.0

df.interpolate(method='pad', axis=0)

   0    1    2
0  1.0  1.0  3.0
1  NaN  2.0  2.0
2  2.0  3.0  3.0

df.interpolate(method='linear', axis=0)
     0    1    2
0  1.0  NaN  3.0
1  1.5  2.0  3.0
2  2.0  3.0  3.0
```

Besides, ```df.interpolate(method='pad')``` does not seem to respond to `limit_area`

```python
df.interpolate(method='pad',limit_area='inside',axis=1)
     0    1    2
0  1.0  NaN  3.0
1  1.0  2.0  3.0
2  2.0  3.0  3.0

df.interpolate(method='linear',limit_area='inside', axis=0)

     0    1    2
0  1.0  NaN  3.0
1  1.5  2.0  NaN
2  2.0  3.0  NaN
```

#### Output of ``pd.show_versions()``

<details>

pandas           : 0.25.2
numpy            : 1.16.5
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.4.0
Cython           : None
pytest           : 5.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: 0.8.0
bs4              : None
bottleneck       : None
fastparquet      : 0.3.0
gcsfs            : 0.3.1
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
s3fs             : 0.3.5
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : 0.14.0
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
149027682,12918,DataFrame.ffill behaves different than DataFrame.interpolate(method='ffill') along axes,EVaisman,closed,2016-04-18T03:07:51Z,2020-06-02T22:42:51Z,"It looks like `df.ffill(axis=0)` has the same behavior as `test_df.interpolate(method='ffill', axis=1)`.

```
from pandas.util.testing import assert_frame_equal
import numpy as np
import pandas as pd

n = np.nan
test_df = pd.DataFrame([[0, 2, n, n],
                        [1, n, 4, 6],
                        [n, 3, 5, n]])

assert_frame_equal(
    test_df.interpolate(method='ffill', axis=1),
    test_df.ffill(axis=0),
)
```

Is this the desired behavior?

```
In [2]: pandas.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.11.final.0
python-bits: 64
OS: Darwin
OS-release: 15.3.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.18.0
nose: 1.3.7
pip: 8.1.1
setuptools: 20.3.1
Cython: 0.23.4
numpy: 1.11.0
scipy: 0.17.0
statsmodels: 0.6.1
xarray: None
IPython: 4.1.2
sphinx: 1.3.5
patsy: 0.4.0
dateutil: 2.5.2
pytz: 2016.3
blosc: None
bottleneck: 1.0.0
tables: 3.2.2
numexpr: 2.5
matplotlib: 1.4.3
openpyxl: 2.3.2
xlrd: 0.9.4
xlwt: 1.0.0
xlsxwriter: 0.8.4
lxml: 3.6.0
bs4: 4.4.1
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 1.0.12
pymysql: None
psycopg2: 2.6.1 (dt dec pq3 ext lo64)
jinja2: 2.7.3
boto: 2.39.0
```
"
624732202,34383,DOC: Indexing and selecting data and the use of mask/where,psteinb,closed,2020-05-26T09:18:32Z,2020-06-02T22:57:43Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#evaluation-order-matters

#### Documentation problem

The documentation talks about how **not** to use `dfb['c'][dfb['a'].str.startswith('o')] = 42` for assignment of multiple values to a `pandas.Series` or `pandas.DataFrame` column. it does however fail to make a concrete suggestion how to do it given the example. 

#### Suggested fix for documentation

I suggest to mention the `pandas.Series.where` and `pandas.Series.mask` methods here in order to show how the assignment can be done, i.e. by adding an example like:

```python
dfb['c'].mask(dfb['a'].str.startswith('o'), 42)
```
which performs the desired assigment of rows matching `o` to `42`. I am happy to contribute this as a PR myself."
627374658,34461,mask based multi-index assignment of column values described,psteinb,closed,2020-05-29T16:04:08Z,2020-06-02T22:57:49Z,"- [x] closes #34383
- [x] passes `black pandas`
not sure what this means, will look it up.
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`


- ~~[ ] tests added / passed~~ 
this is an PR on documentation only, unit tests are not needed.
- ~~[ ] whatsnew entry~~
I think I don't need this as this is a PR related to documentation only.
"
609373919,33880,Fix Dataframe.apply documentation to include note regarding calling f…,alonme,closed,2020-04-29T21:32:25Z,2020-06-03T04:57:36Z,"…unction twice on first element

- [ ] closes #33879
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
628554065,34517,TST #22663: Operation between DataFrame with non-numeric types and incomplete series,OlivierLuG,closed,2020-06-01T16:05:43Z,2020-06-03T06:48:29Z,"- [ x ] closes #22663
- [ 1 ] tests added / passed
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
629752383,34542,> I propose to create a single document that contains all the coding guidelines.,ttsstack,closed,2020-06-03T07:13:59Z,2020-06-03T07:14:17Z,"> I propose to create a single document that contains all the coding guidelines.

perhaps the google style could be a good template or basis for outline http://google.github.io/styleguide/pyguide.html

personally i prefer a well structured reference style document to a informal prose on the subject. 

I think that the reason we have the two documents is that the contributing guidelines contains the descriptive introduction for first time contributors and the code style document contains more details. This second document was only recently introduced and adding the content and some migration of content from the contributing guidlines is WIP.

contributions and PRs welcome.

_Originally posted by @simonjayhawkins in https://github.com/pandas-dev/pandas/issues/33851#issuecomment-621145606_"
560315969,31695,DOC: Removed Notes from DataFrame.applymap,r0cketr1kky,closed,2020-02-05T11:40:05Z,2020-06-03T07:53:19Z,"- closes #31620 

Documentation screenshot: 
![Screenshot from 2020-02-05 17-08-33](https://user-images.githubusercontent.com/39258575/73838709-5172c300-483a-11ea-96b4-6faa1918b73f.png)
"
610751068,33917,Performance regression in timeseries.SortIndex.time_sort_index,TomAugspurger,closed,2020-05-01T13:29:19Z,2020-06-03T09:04:43Z,"```python
import pandas as pd
import numpy as np

N = 10 ** 5
idx = pd.date_range(start=""1/1/2000"", periods=N, freq=""s"")
s = pd.Series(np.random.randn(N), index=idx)
%timeit s.sort_index()
```

```
# 1.0.2
108 µs ± 8.27 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
# master
225 µs ± 8.36 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

According to https://pandas.pydata.org/speed/pandas/index.html#timeseries.SortIndex.time_sort_index?p-monotonic=True&commits=f683473a156f032a64a1d7edcebde21c42a8702d-085860a49f3a87aa4e24b3115b50b85c4b3c5676, the first slow commit is https://github.com/pandas-dev/pandas/pull/33755, which just bumps Cython in numpydev... So probably not actually that commit."
619068978,34192,PERF: Remove unnecessary copies in sorting functions (#33917),mproszewska,closed,2020-05-15T15:43:10Z,2020-06-03T09:04:54Z,"- [x] closes #33917
- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This PR fixes pereformance regression after commit `dec736f3f` by removing unnecessary copies if `key=None` in sorting functions.

```
setup = """"""
import pandas as pd
import numpy as np
N = 10 ** 5
idx = pd.date_range(start=""1/1/2000"", periods=N, freq=""s"")
s = pd.Series(np.random.randn(N), index=idx)
""""""

import timeit
timeit.timeit(""s.sort_index()"",setup=setup, number=100000)

# master
# 41.245621485984884
# now
# 14.826273362035863
```"
435482633,26171,TypeError: __init__() got an unexpected keyword argument 'max_rows',ShahWaseem,closed,2019-04-21T07:11:56Z,2020-06-03T12:00:16Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import pystan
from fbprophet import Prophet

plt.style.use('fivethirtyeight')
y = [t1_tmax['date'], t1_tmax['value']]
import pandas as pd
df = pd.DataFrame({'date': y[0], 'value': y[1]})

df.dtypes

df = df.rename(columns={'date': 'ds','value': 'y'})
df

my_model = Prophet(interval_width=0.95)
my_model.fit(df) 



```
#### Problem description

I have installed the latest Pystan and fbprophet versions:
pystan                    2.17.1.0
fbprophet              0.4.post2

Currently, I'm using conda 4.6.14 with Python 3.6.7 version as suggested after uninstalling 3.7 in other threads!

#### Expected Output

#### Output of ``
my_model.fit(df) 
``

<details>
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-134-2eba425e3c88> in <module>
      2 #with weekly_seasonality=True
      3 #instantiate Prophet
----> 4 my_model.fit(df) #fit the model with your dataframe

~\Anaconda3\Anaconda\lib\site-packages\fbprophet\forecaster.py in fit(self, df, **kwargs)
   1010         """"""
   1011         if self.history is not None:
-> 1012             raise Exception('Prophet object can only be fit once. '
   1013                             'Instantiate a new object.')
   1014         if ('ds' not in df) or ('y' not in df):

Exception: Prophet object can only be fit once. Instantiate a new object.

</details>"
628789817,34522,DEPR: tz kwarg in Period.to_timestamp,jbrockmendel,closed,2020-06-01T23:20:39Z,2020-06-03T16:29:07Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

inconsistent with PeriodArray/PeriodIndex methods
"
623329370,34318,"BUG: ""IndexingError: Too many indexers"" when accessing a None value using .loc through a MultiIndex",dechamps,closed,2020-05-22T16:26:51Z,2020-06-03T18:02:23Z,"#### Steps to reproduce

```python
print(pd.Series(
    [None],
    pd.MultiIndex.from_arrays([['Level1'], ['Level2']]))
    .loc[('Level1', 'Level2')])
```

#### Expected output

```
None
```

#### Actual output

```
/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py in __getitem__(self, key)
   1760                 except (KeyError, IndexError, AttributeError):
   1761                     pass
-> 1762             return self._getitem_tuple(key)
   1763         else:
   1764             # we by definition only have the 0th axis

/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py in _getitem_tuple(self, tup)
   1275 
   1276         # no multi-index, so validate all of the indexers
-> 1277         self._has_valid_tuple(tup)
   1278 
   1279         # ugly hack for GH #836

/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py in _has_valid_tuple(self, key)
    699         for i, k in enumerate(key):
    700             if i >= self.ndim:
--> 701                 raise IndexingError(""Too many indexers"")
    702             try:
    703                 self._validate_key(k, i)

IndexingError: Too many indexers
```

#### Additional information

If any value other than `None` (even `np.nan`) is used, the code behaves correctly.

If a single index level is used, the code behaves correctly.

#### Workaround

Seems to work if `.loc(axis=0)[('Level1', 'Level2')]` is used instead.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.candidate.1
python-bits      : 64
OS               : Linux
OS-release       : 5.6.0-1-amd64
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3
Cython           : None
pytest           : 4.6.9
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 4.6.9
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
626950308,34450,"Fix MultiIndex .loc ""Too Many Indexers"" with None as return value",pedrooa,closed,2020-05-29T02:21:59Z,2020-06-03T18:02:27Z,"- [x] closes #34318 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Three changes in indexing.py : 
- function _handle_lowerdim_multi_index_axis0 default return value was None, now it raises an Exception.
- function _getitem_lowerdim checks for a raised exception instead of a value None.
- function _getitem_nested_tuple checks for a raised exception instead of a value None(had to be changed because it also uses _handle_lowerdim_multi_index_axis0) 

"
506886820,28981,Possible regression in comparison operation for interval dtypes,dsaxton,closed,2019-10-14T21:36:57Z,2020-06-03T18:04:40Z,"It seems that this comparison is now failing on master when it was working in 0.25.1.  Need to look a bit more, but I don't think it's specific to this operation.

```python
import pandas as pd

s = pd.Series([pd.Interval(0, 1), pd.Interval(1, 2)], dtype=""interval"")  
s == ""a""
```

#### 0.25.1

```
0    False
1    False
dtype: bool
```

#### master

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-4-3a654234a428> in <module>
----> 1 s == ""a""

~/pandas/pandas/core/ops/__init__.py in wrapper(self, other)
    527         rvalues = extract_array(other, extract_numpy=True)
    528 
--> 529         res_values = comparison_op(lvalues, rvalues, op)
    530 
    531         return _construct_result(self, res_values, index=self.index, name=res_name)

~/pandas/pandas/core/ops/array_ops.py in comparison_op(left, right, op)
    253 
    254     if should_extension_dispatch(lvalues, rvalues):
--> 255         res_values = dispatch_to_extension_op(op, lvalues, rvalues)
    256 
    257     elif is_scalar(rvalues) and isna(rvalues):

~/pandas/pandas/core/ops/dispatch.py in dispatch_to_extension_op(op, left, right, keep_null_freq)
    124     #  a Series or Index.
    125 
--> 126     if left.dtype.kind in ""mM"" and isinstance(left, np.ndarray):
    127         # We need to cast datetime64 and timedelta64 ndarrays to
    128         #  DatetimeArray/TimedeltaArray.  But we avoid wrapping others in

TypeError: 'in <string>' requires string as left operand, not NoneType
```

@jbrockmendel Is the fix for this as simple as (say) setting the `kind` to `""O""` here https://github.com/pandas-dev/pandas/blob/master/pandas/core/dtypes/dtypes.py#L975?  It looks like we're assuming the `dtype` has a `kind` attribute when it doesn't."
619697536,34224,BUG: pd.DataFrame.transform recursively loops in some cases,adamczykm,closed,2020-05-17T13:01:45Z,2020-06-03T18:05:38Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
pd.DataFrame({""a"":[None]}).transform({""a"":int})
```

#### Problem description

Executing the above causes recursion depth limit exception. This is confusing and it is harder to pinpoint/debug than the expected exception.

#### Expected Output

Something akin to the output of `int(None)`

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.6.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.0.3
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : 3.10.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 3.10.1
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
624553285,34377,BUG: pd.DataFrame.transform recursively loops in some cases #34224,pedrooa,closed,2020-05-26T02:02:16Z,2020-06-03T18:05:44Z,"- [x] closes #34224 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Error handling within try/except in aggregate function of frame.py file
"
630184660,34554,CLN: Update imports,jbrockmendel,closed,2020-06-03T17:30:20Z,2020-06-03T18:11:35Z,
630162982,34553,DOC: fix PR06 (parameter type) errors in Timestamp docstrings,willpeppo,closed,2020-06-03T16:56:44Z,2020-06-03T20:37:50Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
630103340,34552,DOC: fix PR06 errors in Timedeltas docstrings (parameter type),willpeppo,closed,2020-06-03T15:39:46Z,2020-06-03T20:37:50Z,"- [x] closes #28253 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
626197784,34422,BUG: SeriesGroupBy works with any column name specified in NamedAgg,gurukiran07,closed,2020-05-28T04:09:08Z,2020-06-03T22:26:16Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python3
s = pd.Series([1,1,2,2,3,3,4,5])

s.groupby(s.values).agg(one = pd.NamedAgg(column='anything',aggfunc='sum'))
   one
1    2
2    4
3    6
4    4
5    5

s.groupby(s.values).agg(one=('something','sum'))

   one
1    2
2    4
3    6
4    4
5    5
```

#### Problem description

For `SeriesGroupBy.agg` named aggregation, it accepts any column name as mentioned in the above examples but Series doesn't have any columns.

#### Expected Output
After discussing with @TomAugspurger and @MarcoGorelli in another issue #34380 regarding this issue in the comments, We came to a solution that 
> So for SeriesGroupBy.agg, if there are any tuples or NamedAgg present in kwargs then I think we should raise.
Disallowing `NamedAgg` and tuples with `SeriesGroupBy`.
```python3
s.groupby(s.values).agg(one = pd.NamedAgg(column='anything',aggfunc='sum'))
# Error should be raised.
s.groupby(s.values).agg(one=('something','sum'))
# Error should be raised.
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 46.1.3
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
630311820,34559,CLN: remove ABCTimedelta,jbrockmendel,closed,2020-06-03T20:41:27Z,2020-06-03T22:27:45Z,"Hoping we can eventually remove ABCTimestamp too, not sure."
630199348,34555,REF: simplify wrapping in apply_index,jbrockmendel,closed,2020-06-03T17:49:42Z,2020-06-03T22:28:14Z,"Make the caller (DatetimeArray._add_offset) responsible for wrapping, so we can move the liboffsets methods towards only needing the ndarrays."
630337228,34560,BUG: fix resolution_string docstring,jbrockmendel,closed,2020-06-03T21:15:55Z,2020-06-03T22:32:13Z,broken off from #34499
629524567,34538,REF: add to_offset to tslibs namespace,jbrockmendel,closed,2020-06-02T21:22:59Z,2020-06-03T22:32:53Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
554544958,31269,"ENH: allow 'pad', 'backfill' and 'cumcount' in groupby.transform",fujiaxiang,closed,2020-01-24T04:59:08Z,2020-06-03T23:28:11Z,"#### Summary
Currently on master, `groupby.transform` with ``func`` equals one of `('fillna', 'pad', 'backfill', 'ffill', etc.)` yields wrong results (See #30918).

With updates from PR (#31101), the incorrect outputs are fixed. However, when ``func`` is one of `('pad', 'backfill', 'cumcout')`, `groupby.transform` then raises ``AttributeError`` similar to what's reported in #27472.

#### Code Sample

```python
# On branch of PR31101
>>> pd.__version__
'1.0.0rc0+162.g56c70234e'

>> df = pd.DataFrame(
...     {
...         ""A"": [""foo"", ""foo"", ""foo"", ""foo"", ""bar"", ""bar"", ""baz""],
...         ""B"": [1, 2, np.nan, 3, 3, np.nan, 4],
...     }
... )
>>> df
     A    B
0  foo  1.0
1  foo  2.0
2  foo  NaN
3  foo  3.0
4  bar  3.0
5  bar  NaN
6  baz  4.0

>>> df.groupby(""A"").pad()  # This is ok
     B
0  1.0
1  2.0
2  2.0
3  3.0
4  3.0
5  3.0
6  4.0

>>> df.groupby(""A"").transform(""pad"")  # This raises AttributeError
Traceback (most recent call last):
...
...
AttributeError: 'Series' object has no attribute 'pad'

>>> df.groupby(""A"").transform(""cumcount"")
Traceback (most recent call last):
...
...
AttributeError: 'Series' object has no attribute 'cumcount'
```

Ideally we want to allow all of the above 3 functions in `groupby.transform`"
627168828,34453,"[ENH] Allow pad, backfill and cumcount in groupby.transform",fujiaxiang,closed,2020-05-29T10:32:25Z,2020-06-04T04:06:48Z,"- [x] closes #31269
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
630369634,34561,DOC: Fixed PR06 (wrong parameter type) in pandas.Timestamp,willpeppo,closed,2020-06-03T22:18:19Z,2020-06-04T07:04:57Z,
588221849,33035,Can't Seem to install pandas in python,kartikdutt18,closed,2020-03-26T07:50:34Z,2020-06-04T08:53:09Z,"I tried running
```
sudo pip install pandas
```
#### Problem description

Tried Installing pandas for python 2.7 . Can't seem to figure it out. Checked similar issues, but they didn't seem to help me.

Numpy version : 1.16.6 (tried other versions as well)

#### Expected Output

Pandas installed successfully.


####Got this output instead:

```
DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support
Defaulting to user installation because normal site-packages is not writeable
Collecting pandas
  Using cached pandas-1.0.3.tar.gz (5.0 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  ERROR: Command errored out with exit status 1:
   command: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python /Users/kd/Library/Python/2.7/lib/python/site-packages/pip/_vendor/pep517/_in_process.py get_requires_for_build_wheel /var/folders/30/00757twx74l3jppj5c4nq7380000gn/T/tmpUhhGpr
       cwd: /private/var/folders/30/00757twx74l3jppj5c4nq7380000gn/T/pip-install-6SkeqV/pandas
  Complete output (19 lines):
  Traceback (most recent call last):
    File ""/Users/kd/Library/Python/2.7/lib/python/site-packages/pip/_vendor/pep517/_in_process.py"", line 257, in <module>
      main()
    File ""/Users/kd/Library/Python/2.7/lib/python/site-packages/pip/_vendor/pep517/_in_process.py"", line 240, in main
      json_out['return_val'] = hook(**hook_input['kwargs'])
    File ""/Users/kd/Library/Python/2.7/lib/python/site-packages/pip/_vendor/pep517/_in_process.py"", line 91, in get_requires_for_build_wheel
      return hook(config_settings)
    File ""/private/var/folders/30/00757twx74l3jppj5c4nq7380000gn/T/pip-build-env-IK81LY/overlay/lib/python2.7/site-packages/setuptools/build_meta.py"", line 146, in get_requires_for_build_wheel
      return self._get_build_requires(config_settings, requirements=['wheel'])
    File ""/private/var/folders/30/00757twx74l3jppj5c4nq7380000gn/T/pip-build-env-IK81LY/overlay/lib/python2.7/site-packages/setuptools/build_meta.py"", line 127, in _get_build_requires
      self.run_setup()
    File ""/private/var/folders/30/00757twx74l3jppj5c4nq7380000gn/T/pip-build-env-IK81LY/overlay/lib/python2.7/site-packages/setuptools/build_meta.py"", line 243, in run_setup
      self).run_setup(setup_script=setup_script)
    File ""/private/var/folders/30/00757twx74l3jppj5c4nq7380000gn/T/pip-build-env-IK81LY/overlay/lib/python2.7/site-packages/setuptools/build_meta.py"", line 142, in run_setup
      exec(compile(code, __file__, 'exec'), locals())
    File ""setup.py"", line 42
      f""numpy >= {min_numpy_ver}"",
                                ^
  SyntaxError: invalid syntax
  ----------------------------------------
ERROR: Command errored out with exit status 1: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python /Users/kd/Library/Python/2.7/lib/python/site-packages/pip/_vendor/pep517/_in_process.py get_requires_for_build_wheel /var/folders/30/00757twx74l3jppj5c4nq7380000gn/T/tmpUhhGpr Check the logs for full command output
```"
611534813,33959,"fix bfill, ffill and pad when calling with df.interpolate with column…",CloseChoice,closed,2020-05-03T23:38:37Z,2020-06-04T11:25:02Z,"… dtype string (#33956)

- [x] closes #33956 
- [x] closes #12918 
- [x] closes #29146 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
558557031,31544,Reading with read_stata in chunks messes up categories,toobaz,closed,2020-02-01T15:39:57Z,2020-06-04T11:38:48Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: df = pd.DataFrame({'col{}'.format(k) : pd.Categorical(['a_label'] +
                                                              ['another_label']*500)
                           for k in range(2)})                                                                           

In [3]: df.dtypes                                                                                                                                                                                
Out[3]: 
col0    category
col1    category
dtype: object

In [4]: df.dtypes[0]                                                                                                                                                                             
Out[4]: CategoricalDtype(categories=['a_label', 'another_label'], ordered=False)

In [5]: df.to_stata('/tmp/stata_test.dta', write_index=False)                                                                                                                                    

In [6]: pd.read_stata('/tmp/stata_test.dta').dtypes                                                                                                                                              
Out[6]: 
col0    category
col1    category
dtype: object
# ... that's good

In [7]: reader = pd.read_stata('/tmp/stata_test.dta', chunksize=100)                                                                                                                             

In [8]: reader.value_labels()                                                                                                                                                                    
Out[8]: 
{'col0': {0: 'a_label', 1: 'another_label'},
 'col1': {0: 'a_label', 1: 'another_label'}}
# ... still all good

In [9]: out_chunks = [chunk for chunk in reader]                                                                                                                                                 

In [10]: out_chunks[1].dtypes[0]                                                                                                                                                                 
Out[10]: CategoricalDtype(categories=['another_label'], ordered=True)
# Ooops... where's the other label gone?

In [11]: reader.close()                                                                                                                                                                          

In [12]: all_together = pd.concat(out_chunks)                                                                                                                                                    

In [13]: all_together.dtypes[0]                                                                                                                                                                  
Out[13]: dtype('O')
# Ouch!
```

#### Problem description

My data has categories, but they are lost only because I'm reading it in chunks. I noticed this because I was reading in chunks a large database of which I only needed a subset of columns: ironically, precisely the fact that I was reading it in chunks made memory usage explode when I reattached them.

An by the way, ``Out[8]:`` shows that pandas _is_ aware of the actual categories, even before iterating... so this is the information that should be used to consistently recreate them, and all chunks should have exactly the _same_ (as in ``is``) categorical dtype.

#### Expected Output

``Out[10]`` should feature both categories, and ``Out[13]`` should still be a categorical.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.0-6-amd64
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : it_IT.UTF-8
LOCALE           : it_IT.UTF-8

pandas           : 1.1.0.dev0+276.g2495068ad
numpy            : 1.16.4
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 18.1
setuptools       : 41.0.1
Cython           : 0.29.13
pytest           : 4.6.3
hypothesis       : 3.71.11
sphinx           : 1.8.4
blosc            : 1.7.0
feather          : None
xlsxwriter       : 0.9.3
lxml.etree       : 4.3.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.7 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.2
matplotlib       : 3.0.2
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.4.9
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 4.6.3
pyxlsb           : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : 1.2.18
tables           : 3.4.4
tabulate         : 0.8.3
xarray           : 0.11.3
xlrd             : 1.1.0
xlwt             : 1.3.0
xlsxwriter       : 0.9.3
numba            : 0.45.0


</details>
"
617560940,34158,REGR: use dtype.fill_value in ExtensionBlock.fill_value where available,scottgigante,closed,2020-05-13T15:50:08Z,2020-06-04T12:02:27Z,"- [X] closes #27781, closes #29563
- [X] tests added / passed
- [X] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
627989539,34499,REF/PERF: PeriodDtype decouple from DateOffset,jbrockmendel,closed,2020-05-31T15:49:01Z,2020-06-04T16:50:19Z,"ATM we define PeriodDtype in terms of DateOffsets, but this is a misnomer.  In fact, virtually every Period/PeriodArray method has to start off by taking its `.freq` and finding the corresponding integer code.  This makes the integer code itself into a dtype.  We lose a little bit of ground on the constructor, then make it back up in subsequent calls.

```
In [2]: per = pd.Period(""2016Q1"")                                                                                                                                                                   

In [3]: %timeit per.year
556 ns ± 13.3 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)    # <-- master
94.5 ns ± 1.36 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)  # <-- PR

In [4]: %timeit pd.Period(""2016Q1"")                   
21.2 µs ± 176 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- master
25.8 µs ± 457 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- PR
```

The constructor perf I think we can improve by eventually cutting the DateOffset out of the process altogether.

We'll also be able to de-duplicate a _bunch_ of other stuff: FreqGroup can be defined in terms of PeriodDypeCode, Resolution can be defined in terms of FreqGroup (xref #34462), we can avoid redundant definitions of the dtype codes in period.pyx, and a lot of the rest of libfrequencies becomes unnecessary.

In a follow-up I plan to mix the cython-space PeriodDtype into the core.dtypes PeriodDtype and we can get the same perf improvements in the PeriodArray methods."
631017255,34578,Solve ci issue related to #34555,hasnain2808,closed,2020-06-04T17:29:12Z,2020-06-04T17:30:34Z,"- [x] closes #34575 
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
solved issues with benchmarks
benchmarks running fine now

Current right output
```
· Discovering benchmarks
· Running 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)
[  0.00%] ·· Benchmarking existing-py_home_moha_venv_pandas-dev_bin_python
[ 50.00%] ··· arithmetic.ApplyIndex.time_apply_indexgit                                                                                                ok
[ 50.00%] ··· =================================== ==========
                             offset                         
              ----------------------------------- ----------
                      <YearEnd: month=12>          2.05±0ms 
                      <YearBegin: month=1>         1.56±0ms 
                 <QuarterEnd: startingMonth=3>     4.41±0ms 
                <QuarterBegin: startingMonth=3>    2.50±0ms 
                           <MonthEnd>              4.32±0ms 
                          <MonthBegin>             2.31±0ms 
                 <DateOffset: days=2, months=2>    5.67±0ms 
                         <BusinessDay>             10.4±0ms 
                <SemiMonthEnd: day_of_month=15>    14.5±0ms 
               <SemiMonthBegin: day_of_month=15>   14.2±0ms 
              =================================== ==========
```"
595716834,33360,BUG: Can't disable progress bar for read_gbq,de-code,closed,2020-04-07T09:13:24Z,2020-06-04T17:41:34Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd

pd.read_gbq(..., progress_bar_type=None)
```

(Due to the BigQuery dependency it's not copy-pastable)

Warning message displayed or tqdm is used:

```
/path/to/python/site-packages/pandas_gbq/gbq.py:555: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.
  progress_bar_type=progress_bar_type,
```

That is because if `progress_bar_type` is set to `None` it is not passed to `pandas_gbq.read_gbq` (see https://github.com/pandas-dev/pandas/pull/29858#discussion_r352260559). But the default for `pandas_gbq.read_gbq` is `tqdm`.

#### Problem description

It should be possible to disable the progress bar.

#### Expected Output

No progress bar.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-91-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 39.0.1
Cython           : None
pytest           : 4.6.9
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : 0.13.1
pyarrow          : None
pytables         : None
pytest           : 4.6.9
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
598244751,33477,Send None parameter to pandas-gbq to set no progress bar,sumanau7,closed,2020-04-11T11:12:16Z,2020-06-04T18:10:02Z,"- [x] closes #33360 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
630419981,34563,CLN: circular/runtime imports in tslibs,jbrockmendel,closed,2020-06-04T00:31:13Z,2020-06-04T18:15:40Z,"ATM tzconversion depends on timedeltas (for delta_to_nanoseconds), which throws a wrench in the erstwhile dependency hierarchy.  By making the one usage of delta_to_nanoseconds a runtime import, we make it possible to remove a bunch of other runtime imports."
631039005,34579,solves ci issues with #34575,hasnain2808,closed,2020-06-04T18:04:59Z,2020-06-04T19:04:33Z,"- [x] closes #34575 
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
solved issues with benchmarks
benchmarks running fine now

Current right output
```
· Discovering benchmarks
· Running 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)
[  0.00%] ·· Benchmarking existing-py_home_moha_venv_pandas-dev_bin_python
[ 50.00%] ··· arithmetic.ApplyIndex.time_apply_indexgit                                                                                                ok
[ 50.00%] ··· =================================== ==========
                             offset                         
              ----------------------------------- ----------
                      <YearEnd: month=12>          2.05±0ms 
                      <YearBegin: month=1>         1.56±0ms 
                 <QuarterEnd: startingMonth=3>     4.41±0ms 
                <QuarterBegin: startingMonth=3>    2.50±0ms 
                           <MonthEnd>              4.32±0ms 
                          <MonthBegin>             2.31±0ms 
                 <DateOffset: days=2, months=2>    5.67±0ms 
                         <BusinessDay>             10.4±0ms 
                <SemiMonthEnd: day_of_month=15>    14.5±0ms 
               <SemiMonthBegin: day_of_month=15>   14.2±0ms 
              =================================== ==========
```"
630934297,34575,CI is failing due to benchmark failing at  arithmetic.ApplyIndex.time_apply_index (0.65%),hasnain2808,closed,2020-06-04T15:31:09Z,2020-06-04T19:04:33Z,"ASV benchmarks are failing on the benchmarks for arithmetic.ApplyIndex.time_apply_index 
I believe the benchmarks are failing after #34499  was merged.

The latest benchmark failed [link](https://github.com/pandas-dev/pandas/pull/33962/checks?check_run_id=738856351)

Related errors

```
##[error][  0.65%] ··· arithmetic.ApplyIndex.time_apply_index                 3/10 failed
[  0.65%] ··· =================================== ==========
                             offset                         
              ----------------------------------- ----------
                      <YearEnd: month=12>          1.55±0ms 
                      <YearBegin: month=1>         1.37±0ms 
                 <QuarterEnd: startingMonth=3>     1.71±0ms 
                <QuarterBegin: startingMonth=3>    1.59±0ms 
                           <MonthEnd>              2.25±0ms 
                          <MonthBegin>             1.34±0ms 
                 <DateOffset: days=2, months=2>    3.16±0ms 
                         <BusinessDay>              failed  
                <SemiMonthEnd: day_of_month=15>     failed  
               <SemiMonthBegin: day_of_month=15>    failed  
              =================================== ==========

[  0.65%] ···· For parameters: <BusinessDay>
               Traceback (most recent call last):
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 1184, in main_run_server
                   main_run(run_args)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 1058, in main_run
                   result = benchmark.do_run()
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 537, in do_run
                   return self.run(*self._current_params)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 627, in run
                   samples, number = self.benchmark_timing(timer, min_repeat, max_repeat,
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 694, in benchmark_timing
                   timing = timer.timeit(number)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/timeit.py"", line 177, in timeit
                   timing = self.inner(it, self.timer)
                 File ""<timeit-src>"", line 6, in inner
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 599, in <lambda>
                   func = lambda: self.func(*param)
                 File ""/home/runner/work/pandas/pandas/asv_bench/benchmarks/arithmetic.py"", line 469, in time_apply_index
                   offset.apply_index(self.rng)
                 File ""pandas/_libs/tslibs/offsets.pyx"", line 87, in pandas._libs.tslibs.offsets.apply_index_wraps.wrapper
                 File ""pandas/_libs/tslibs/offsets.pyx"", line 1397, in pandas._libs.tslibs.offsets.BusinessDay.apply_index
               AttributeError: 'PeriodIndex' object has no attribute '_addsub_int_array'
               
               For parameters: <SemiMonthEnd: day_of_month=15>
               Traceback (most recent call last):
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 1184, in main_run_server
                   main_run(run_args)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 1058, in main_run
                   result = benchmark.do_run()
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 537, in do_run
                   return self.run(*self._current_params)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 627, in run
                   samples, number = self.benchmark_timing(timer, min_repeat, max_repeat,
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 694, in benchmark_timing
                   timing = timer.timeit(number)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/timeit.py"", line 177, in timeit
                   timing = self.inner(it, self.timer)
                 File ""<timeit-src>"", line 6, in inner
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 599, in <lambda>
                   func = lambda: self.func(*param)
                 File ""/home/runner/work/pandas/pandas/asv_bench/benchmarks/arithmetic.py"", line 469, in time_apply_index
                   offset.apply_index(self.rng)
                 File ""pandas/_libs/tslibs/offsets.pyx"", line 87, in pandas._libs.tslibs.offsets.apply_index_wraps.wrapper
                 File ""pandas/_libs/tslibs/offsets.pyx"", line 2319, in pandas._libs.tslibs.offsets.SemiMonthOffset.apply_index
               AttributeError: 'PeriodIndex' object has no attribute '_addsub_int_array'
               
               For parameters: <SemiMonthBegin: day_of_month=15>
               Traceback (most recent call last):
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 1184, in main_run_server
                   main_run(run_args)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 1058, in main_run
                   result = benchmark.do_run()
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 537, in do_run
                   return self.run(*self._current_params)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 627, in run
                   samples, number = self.benchmark_timing(timer, min_repeat, max_repeat,
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 694, in benchmark_timing
                   timing = timer.timeit(number)
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/timeit.py"", line 177, in timeit
                   timing = self.inner(it, self.timer)
                 File ""<timeit-src>"", line 6, in inner
                 File ""/home/runner/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/asv/benchmark.py"", line 599, in <lambda>
                   func = lambda: self.func(*param)
                 File ""/home/runner/work/pandas/pandas/asv_bench/benchmarks/arithmetic.py"", line 469, in time_apply_index
                   offset.apply_index(self.rng)
                 File ""pandas/_libs/tslibs/offsets.pyx"", line 87, in pandas._libs.tslibs.offsets.apply_index_wraps.wrapper
                 File ""pandas/_libs/tslibs/offsets.pyx"", line 2319, in pandas._libs.tslibs.offsets.SemiMonthOffset.apply_index
               AttributeError: 'PeriodIndex' object has no attribute '_addsub_int_array'
``` 

I am not sure what was the right way to report this thus created this issue. Do tell me if this is an incorrect place, incorrect way or incorrect language
"
630928491,34574,DOC: fixed PR06 in pandas.Timedeltas,willpeppo,closed,2020-06-04T15:25:37Z,2020-06-04T19:06:45Z,
630814397,34572,TST: Added test to check that the freqstr attribute of the index is p…,ghost,closed,2020-06-04T13:18:07Z,2020-06-04T19:10:00Z,"…reserved after a shift operation. Run black and flake8.

- [x] closes #21275
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry (omitted, test only)
"
630496359,34566,CLN: address FIXMEs in liboffsets,jbrockmendel,closed,2020-06-04T04:12:53Z,2020-06-04T19:38:38Z,
627796284,34487,"TST, TYP: _use_dynamic_x",MarcoGorelli,closed,2020-05-30T18:54:09Z,2020-06-04T19:52:48Z,"This condition (which, from the current plotting test suite, is unreachable) was introduced in #9814, when `_use_dynamic_x` was written.

Is there any reason why `get_period_alias(freq)` would return `None` if `freq` is a valid time frequency? If so, I'll add a test - else, this PR removes unreachable code.

Added an annotation for `data` and the return type while I was here - is there a way to annotate `ax`?

UPDATE
------

As it turns out (thanks jbrockmendel and jorisvandenbossche), this line can be hit, so I've included a test which does"
611609631,33962,Add nrows to read json.,hasnain2808,closed,2020-05-04T05:19:02Z,2020-06-04T20:45:50Z,"- [x] closes #33916
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Add the nrows to read_json parameter that returns only the required number of json from the line delimited json "
629717388,34541,DOC: documentation link to whatsnew v1.0.4 not working,CloseChoice,closed,2020-06-03T06:03:24Z,2020-06-04T20:55:17Z,"#### Location of the documentation

full whatsnew link on
https://github.com/pandas-dev/pandas/releases/tag/v1.0.4

#### Documentation problem

The link to https://pandas.pydata.org/docs/whatsnew/v1.0.4.html is not working though https://pandas.pydata.org/docs/whatsnew/v1.0.3.html works
"
594387930,33300,RLS: 1.0.4,simonjayhawkins,closed,2020-04-05T10:44:42Z,2020-06-04T20:56:03Z,"we have several regressions reported since 1.0.3 (not due to 1.0.3, since 0.25.3) and a couple that had not been fixed.

https://github.com/pandas-dev/pandas/issues?page=1&q=is%3Aopen+is%3Aissue+label%3ARegression

Is there any that warrant a 1.0.4 release?

@pandas-dev/pandas-core "
630988599,34577,BUG: read_table cannot handle multi-character separators in memory_map mode,emptyVoid,open,2020-06-04T16:45:40Z,2020-06-05T04:52:14Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample

```python
import pandas
dataframe = pandas.read_table('file.txt',
                              header=None,
                              sep=' - ',
                              names=['key', 'value'],
                              memory_map=True)
```

#### Data Sample
```
key1 - value1
key2 - value2
key3 - value3
```

#### Problem description

I'm getting an exception: 
> **Exception has occurred: TypeError**
cannot use a string pattern on a bytes-like object

from this line:
https://github.com/pandas-dev/pandas/blob/14eda586582513c68f32f0a1f00ecfe8d6c7f8f3/pandas/io/parsers.py#L2472
since `f.readline()` returns a byte-string.

And if I change separator to `sep=b' - '`, a similar exception:
> **Exception has occurred: TypeError**
cannot use a bytes pattern on a string-like object

gets raised from the following line:
https://github.com/pandas-dev/pandas/blob/14eda586582513c68f32f0a1f00ecfe8d6c7f8f3/pandas/io/parsers.py#L2475
since `for line in f:` yields normal strings.

#### Expected Output

Exception should not be raised either with `sep=' - '` or with `sep=b' - '`.
Not sure which one is the correct, although I see no issues with `sep=' - '` when `memory_map=True` is not set.

#### Workaround
Changing lines
https://github.com/pandas-dev/pandas/blob/14eda586582513c68f32f0a1f00ecfe8d6c7f8f3/pandas/io/parsers.py#L2474-L2475
to
```python
while True:
    line = f.readline()
    if not line:
        break
    yield pat.split(line.strip())
```
fixes `read_table` for byte-string separators (e.g. `sep=b' - '`).

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 63 Stepping 2, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : None.None

pandas           : 1.0.4
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.17
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
486323081,28188,Slow (and weird) empty dataframe creation,astyonax,closed,2019-08-28T11:31:07Z,2020-06-05T17:26:58Z,"I measured the creation of an empty dataframe with 3 similar arguments:

```python
import pandas as pd
cols = np.arange(100)
index = np.arange(1000)

%timeit pd.DataFrame(columns=cols, index=index)
# 100 loops, best of 3: 18.8 ms per loop
%timeit pd.DataFrame({}, columns=cols, index=index)
100 loops, best of 3: 18.7 ms per loop
%timeit pd.DataFrame(np.nan ,columns=cols, index=index)
1000 loops, best of 3: 434 µs per loop

z1 = pd.DataFrame(columns=cols,index=index) # dtype -> object
z2 = pd.DataFrame({},columns=cols,index=index) # dtype -> object
z3 = pd.DataFrame(np.nan,columns=cols,index=index) # dtype -> float
```

If I understand correctly the code in here:
https://github.com/pandas-dev/pandas/blob/171c71611886aab8549a8620c5b0071a129ad685/pandas/core/frame.py#L399

the first 2 constructions are the same: the default value of data is an empty dictionary. 
The output is a dataframe of type object. 

The 3rd, with np.nan as initial value, is about 40 times faster, and the dtype is float, as expected.

So I would change:
1. the default value of data to np.nan. If one creates an empty dataframe with no specific  dtype, then the best pandas can do is to return the cheapest and fastest dataframe that can contain ""empty"" values. A dataframe of type float filled with np.nan is so a good candidate.


2. the documentation to report the current behavior.

What is your opinion?
"
626728968,34435,BUG: Fix to GH34422 SeriesGroupBy works only with 'func' now,gurukiran07,closed,2020-05-28T18:42:25Z,2020-06-05T17:30:50Z,"- [X] closes #34422 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] whatsnew entry

This PR tries to fix the bug pointed in #34422 where `SeriesGroupBy.agg` works with any given column name in `NamedAgg`.

After discussing with @TomAugspurger  and @MarcoGorelli  in another issue #34380 regarding this issue in the comments, We came to a solution that

> So for SeriesGroupBy.agg, if there are any tuples or NamedAgg present in kwargs then I think we should raise.
> Disallowing `NamedAgg` and tuples with `SeriesGroupBy`.

#### Before fix:
```python3
s = pd.Series([1,1,2,2,3,3,4,5])
s.groupby(s.values).agg(one = pd.NamedAgg(column='anything',aggfunc='sum'))
  one
1    2
2    4
3    6
4    4
5    5

s.groupby(s.values).agg(one=('something','sum'))
   one
1    2
2    4
3    6
4    4
5    5
```

#### After fix:
```python3
s = pd.Series([1,1,2,2,3,3,4,5])
s.groupby(s.values).agg(one = pd.NamedAgg(column='anything',aggfunc='sum'))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""d:\#gh34422\pandas\pandas\core\groupby\generic.py"", line 243, in aggregate
    raise TypeError(tuple_given_message.format(type(kwargs[col]).__name__))
TypeError: 'func' is expected but recieved NamedAgg in **kwargs.

s.groupby(s.values).agg(one=('something','sum'))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""d:\#gh34422\pandas\pandas\core\groupby\generic.py"", line 243, in aggregate
    raise TypeError(tuple_given_message.format(type(kwargs[col]).__name__))
TypeError: 'func' is expected but recieved tuple in **kwargs.
```"
625495701,34403,pandas.core.base.DataError: No numeric types to aggregate,ishgupta,closed,2020-05-27T08:37:00Z,2020-06-05T18:49:23Z,"aggregating a boolean fields doesn't allow averaging the data column in the latest version. Is there a new alternative of doing this for boolean attributes, or it should only be handled by transforming it to a int/float only?

data[ group_fields + [ bool_field ]].groupby( group_fields ).mean() produces the error mentioned in subject.

"
600104117,33560,"ENH: plotting function for ""2D histograms""/""dynamic spectra"" (similar to heatmap)",johan12345,open,2020-04-15T08:06:03Z,2020-06-05T18:54:29Z,"#### Is your feature request related to a problem?

I often need to plot a heatmap of a DataFrame which uses an `IntervalIndex` as its columns (and, usually, time as its index). Such a plot could also be called a ""dynamic spectrum"" or ""2D histogram"" and is used to quickly get an idea of how a spectrum develops over time.

<img src=""https://user-images.githubusercontent.com/5310424/79311884-6042b980-7efe-11ea-86d9-1a24b0f73038.png"" width=400/>


This is slightly different from what is usually considered as a heatmap (see #19008 for an example) as the bins are not necessarily equidistant and there is not necessarily a separate label for each bin. The y axis (which is used for the `IntervalIndex`) could even have logarithmic scaling.

#### Describe the solution you'd like

This could use the same API `df.plot(type='heatmap')` as suggested in #19008 and switch between appropriate axis scaling/labeling modes depending on whether a `CategoricalIndex`, `IntervalIndex` or other types of indices are used.

#### Describe alternatives you've considered

My current implementation (see below) uses matplotlib's `pcolormesh`, but needs to do some fiddling with the bin edges to work correctly.

Matplotlib's `hist2d` does not work for this use case, because the data is already stored in histogrammed form - the histogram and its bins don't need to be calculated, just plotted.

Seaborn's `heatmap` function seems to be limited to plotting categorical data, so both `IntervalIndex` and `DatetimeIndex` are displayed as categorical data with one label per bin, equidistant spacing, and values on the y axis sorted from top to bottom instead of bottom to top:

<img src=""https://user-images.githubusercontent.com/5310424/79312520-4d7cb480-7eff-11ea-80c2-9d0c120c68a2.png"" width=400/>

#### Additional context

My current implementation looks similar to this:

```python
binedges = np.append(df.columns.left, df.columns.right[-1])
X, Y = np.meshgrid(df.index, binedges)
pcm = ax.pcolormesh(X, Y, df.values.T)

# then add labels, colorbar etc.
```

This only works if the `IntervalIndex` has no gaps and is non-overlapping, which would have to be checked first.
"
595927035,33368,PERF/DISC: restrict inputs to is_foo_dtype,jbrockmendel,closed,2020-04-07T14:49:11Z,2020-06-05T19:58:12Z,"xref #33364

A lot of the overhead in is_foo_dtype (which we call _a lot_) comes from checking for dtype objects, arrays, and strings.  We could tighten these to be e.g.

```
def is_extension_arrray_dtype(obj):
     return isinstance(obj, ExtensionDtype)
```

The trouble with this is some of these are user-facing, so this would require a deprecation cycle, and in the interim the `warnings` call would actually hurt performance."
615443692,34101,BUG: Inconsistency in week anchoring?,yohplala,open,2020-05-10T18:01:42Z,2020-06-05T20:34:58Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

---

#### Code Sample, a copy-pastable example

```python

ts = pd.Timestamp('2020-05-13 15:30:00')

freqstr = 'W-MON'
period_start1 = pd.tseries.frequencies.to_offset(freqstr).rollback(ts.normalize())

freqstr = 'W-SUN'
period_start2 = pd.Period(ts, freq=freqstr).start_time

period_start1 == period_start2

```

#### Problem description

Result is `True` where it should be `False`
I would like to raise what seem to me 2 ""troubles"".

#####Trouble 1
I don't understand why it is not possible to use the same `freqstr` between `to_offset` and `Period` to get the same result (which would be according my understanding `W-SUN` here)

#####Trouble 2
To get `period_start1` anchored to beginning of the week, which starts at midnight, I have to `normalize()` it, either the `Timestamp` or the `Timestamp` provided by the `rollback`.
On the other hand `rollback` documentation states: 'Rolled timestamp if not on offset'
For me the offset is at period beginning, i.e. midnight on Sunday.

#### Expected Output
I would expect the following code to give equivalent result (no `normalize` and same `freqstr`).

```python

ts = pd.Timestamp('2020-05-13 15:30:00')

freqstr = 'W-SUN'
period_start1 = pd.tseries.frequencies.to_offset(freqstr).rollback()

freqstr = 'W-SUN'
period_start2 = pd.Period(ts, freq=freqstr).start_time
```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-51-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : fr_FR.UTF-8
LOCALE           : fr_FR.UTF-8

pandas           : 1.0.3
numpy            : 1.16.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.3.3
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.0.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
612689257,34002,BUG: read_csv() wrong reading if comment-character included in na_value,Waljakov,open,2020-05-05T15:24:16Z,2020-06-05T20:36:18Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
from io import StringIO

dataframe_file = (
    ""# this is a comment\n""
    ""1,2,3,4\n""
    ""1,2,3,4#inline comment\n""
    ""1,2#,3,4\n""
    ""1,2,#N/A,4\n""
)

dataframe = pd.read_csv(StringIO(dataframe_file), comment=""#"", na_values=""#N/A"")

print(dataframe)
```

Output:
```
   1  2    3    4
0  1  2  3.0  4.0
1  1  2  NaN  NaN
2  1  2  NaN  NaN
```

#### Problem description
On the last row, it should read the 3rd value (`#N/A`) as NA-Value (`NaN`) and not as a comment.
But because the value `#N/A` starts with the comment-character `#` the rest of the line is interpreted as comment and the last value is not read.

#### Expected Output
```
   1  2    3    4
0  1  2  3.0  4.0
1  1  2  NaN  NaN
2  1  2  NaN  4.0
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.6.8-arch1-1
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : 0.29.17
pytest           : 5.4.1
hypothesis       : 5.8.3
sphinx           : 3.0.3
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.16
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.8
numba            : 0.49.0

</details>
"
631946085,34609,I can't get this to work,CarlosAVTX,closed,2020-06-05T20:47:48Z,2020-06-05T22:00:33Z,"https://github.com/pandas-dev/pandas/blob/1ce1c3c1ef9894bf1ba79805f37514291f52a9da/pandas/core/frame.py#L6253-L6323

if I run the lines I get a syntax error with the "":"" after column. If I try to import and test on a dataframe i get an error that .explode is not an attribute. :'("
626422343,34427,CLN: is pandas/tests/data used?,MarcoGorelli,closed,2020-05-28T11:07:59Z,2020-06-05T23:04:24Z,"In `pandas/tests/data` there are two files:
```
iris.csv
tips.csv
```
Both of these files are also in `pandas/tests/io/data/csv`.

Do they need to be in both places? If not, can they be removed from one of them?"
347990366,22218,Flesh Out Period Benchmarks,jbrockmendel,closed,2018-08-06T16:17:25Z,2020-06-06T02:34:14Z,https://github.com/pandas-dev/pandas/pull/22196#issuecomment-410666276
605184626,33735,BUG: escapechar option of to_csv method does not function properly. ,garylavayou,open,2020-04-23T02:04:37Z,2020-06-06T02:53:07Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
t = pd.DataFrame({0:['""key"":""value""'],  1:['mno,'],  2:['abc\\'],  3:['ijk']})
print(t)
#               0     1     2    3
#0  ""key"":""value""  mno,  abc\  ijk
t.to_csv('test.csv', header=None, escapechar='\\')
# cat test.csv
# 0,""""""key"""":""""value"""""",""mno,"",""abc\"",ijk
t = pd.read_csv('test.csv', header=None, escapechar='\\')
```

#### Problem description

The `read_csv` cannot parse the file, and produce an `ParseError`
```sh
....ParserError: Error tokenizing data. C error: EOF inside string starting at row 0
```
This is because the third column with value `""abc\""`, in which the `\` is a normal character in the string, is not handled properly. With `escapechar='\\'` set for `to_csv`, the value  `abc\` is quoted in the output `test.csv` file as:
```
0,""""""key"""":""""value"""""",""mno,"",""abc\"",ijk
```
As a result the escapechar can be parsed as normal character if we read it by using `read_csv` without  `escapechar='\\'` . **However, this is not compatible with `read_csv` with  `escapechar='\\'` , as it will render the `\""` as a single normal character `""`, thus the quoting pair of this column is missing the end quote char, and hence the above error occurs.**  

#### Expected Output

When writing file with`to_csv`, the `\` specified as escapechar should be rendered as `\\` in the output file, instead of quote the whole string with quotechar `""...""`. In this way, the meaning of `escapechar` option  in `to_csv` and `read_csv` would be consistent, i.e., if we write with `escapechar` , the we read with `escapechar` ( otherwise the `\\`  is read as two normal characters, without error occuring).

#### Output of ``pd.show_versions()``

<details>

python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1

</details>
"
627213244,34454,TST/REF: refactor the arithmetic tests for IntegerArray,jorisvandenbossche,closed,2020-05-29T11:53:24Z,2020-06-06T08:29:54Z,"This is an attempt to make the arithmetic tests for the masked arrays more understandable and maintainable (doing it here for IntegerArray as a start, but the idea would be to do the same for BooleanArray, and later FloatingArray as well, and at that point also share some of those tests).

The problem with the current `arrays/integer/test_arithmetic.py` is that is quite complex to see what is going on (which I experienced when making a version for FloatingArray in https://github.com/pandas-dev/pandas/pull/34307): for example, there is a huge `_check_op` method that has all the logic to created the expected result, but so this also has all the special cases of all ops combined, making it very difficult to see what is going on or to know what is now exactly tested for a certain op. 
The reason for this structure is that those tests originally came from the base extension tests in `tests/extension/`, where the tests needed to be very generic. However, we already moved out those tests for IntgerArray, since they were all customized (nothing was still being inherited from the base class), but so we also don't need to maintain the original class structure then.

The logic how I constructed the new tests:

- I first test each `op` with an explicitly constructed expected result. Here, we test the IntegerArray-specific special cases (like things as `1 ** pd.NA`, or division resulting in a float numpy array, ..). Explicitly writing it down makes it a lot easier to read and to see what is tested (no complex `if op == ...: ..,` constructs, and no complex correcting of expected results based on ndarray)
- Those first tests are all with EAs. In a next set of tests, I added tests for array+scalar ops, array+ndarray ops, ... But here, I just rely on creating the expected result with the EA itself (since EA+EA ops were already tested before)
- Similar tests for frame+scalar, series+scalar, series+array with the EA dtypes -> rely on the actual EA op for the expected result instead of having `_check_op` handle that as well.


cc @dsaxton @jreback "
631856117,34607,PERF: isinstance ABCIndexClass and ABCExtensionArray,simonjayhawkins,closed,2020-06-05T19:02:18Z,2020-06-06T09:16:17Z,"```
%timeit isinstance(pd.Series, pd.core.dtypes.generic.ABCIndexClass)
# 607 ns ± 12.4 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) - PR
# 937 ns ± 37.5 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) - Master


%timeit isinstance(pd.Series, pd.core.dtypes.generic.ABCExtensionArray)
# 634 ns ± 26.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) - PR
# 759 ns ± 40.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) - Master


%timeit isinstance(pd.Series, pd.core.dtypes.generic.ABCSeries)
# 638 ns ± 49.1 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) - PR
# 644 ns ± 20.1 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) - Master
```"
631544322,34597,BUG: Reindex doesn't add NaN when using level=1,gurukiran07,closed,2020-06-05T12:12:34Z,2020-06-06T09:56:45Z,"- [ ] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
index = pd.MultiIndex.from_tuples([('bar', 'one'),
            ('bar', 'two'),
            ('baz', 'one'),
            ('baz', 'two'),
            ('foo', 'one'),
            ('foo', 'two'),
            ('qux', 'one'),
            ('qux', 'two')],
           names=['first', 'second'])
np.random.seed(0)
s = pd.Series(np.random.randn(8), index=index)
first  second
bar    one       1.764052
       two       0.400157
baz    one       0.978738
       two       2.240893
foo    one       1.867558
       two      -0.977278
qux    one       0.950088
       two      -0.151357
dtype: float64
```
When I reindex by level 1.
Current output: 
```python
s.reindex(['two','one','three'],level=1)
first  second
bar    two       0.400157
       one       1.764052
baz    two       2.240893
       one       0.978738
foo    two      -0.977278
       one       1.867558
qux    two      -0.151357
       one       0.950088
dtype: float64
```
#### Expected Output
This output can reproduced by doing `s.unstack().reindex(['two','one','three'],axis=1).stack(dropna=False) ` or `s.reindex(pd.MultiIndex.from_product([s.index.levels[0],['two','one','three']]))`
```python
first  second
bar    two       0.400157
       one       1.764052
       three       NaN
baz    two       2.240893
       one       0.978738
       three        NaN
foo    two      -0.977278
       one       1.867558
       three        NaN
qux    two      -0.151357
       one       0.950088
       three        NaN
dtype: float64
```
#### Problem description

`reindex` is dropping `NaN` while using it on a `MultiIndex` over level 1. The normal behaviour of `reindex` produces `NaN` for new indices, It should also in the above case.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 46.1.3
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.0.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
129167608,12155,BUG: GroupBy.apply iterates over first group twice,tomderuijter,closed,2016-01-27T14:57:47Z,2020-06-06T13:54:33Z,"When using the `GroupBy.apply()` method with a custom function handle, the first group is iterated over twice. Explicitly looping over each group and applying the function directly does not yield the error.

Error reproduction

``` python
>>> import pandas as pd
>>> df = pd.DataFrame({'one':[1,2,3,1,2,3],'two':[4,5,6,7,8,9]})
>>> df
   one  two
0    1    4
1    2    5
2    3    6
3    1    7
4    2    8
5    3    9

>>> gp = df.groupby('one')
>>> def foo(group):
...   print(group.iloc[[0]].index[0])
...
>>> gp.apply(foo)
0
0
1
2
Empty DataFrame
Columns: []
Index: []

>>> len(gp)
3
```

The example prints the row index of the first group member. The '0' is printed twice.

Versions:

``` python
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.1.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None

pandas: 0.17.1
nose: None
pip: 8.0.1
setuptools: 19.4
Cython: None
numpy: 1.10.1
scipy: None
statsmodels: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.4.2
pytz: 2015.7
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: None
pymysql: None
psycopg2: None
Jinja2: None
```
"
407486692,25200,CLN: Remove pd.util._depr_module?,jbrockmendel,closed,2019-02-07T00:00:49Z,2020-06-06T16:08:42Z,AFAICT it isn't used anywhere
384421868,23929,Dtype checking in reshape.merge,jbrockmendel,closed,2018-11-26T16:55:48Z,2020-06-06T16:10:05Z,"There is a check (equivalent to)

```
elif (issubclass(lk.dtype.type, (np.integer, np.timedelta64, np.datetime64)) and
      issubclass(rk.dtype.type, (np.integer, np.timedelta64, np.datetime64))):
```

which I'm told is incorrect:

> this is not right, these should be in separate branches here, the datetime / timedeltas can be in a single elif clause.

https://github.com/pandas-dev/pandas/pull/23917#discussion_r236262558"
620642353,34242,BUG: NaT+Period doesnt match Period+NaT,jbrockmendel,closed,2020-05-19T03:10:44Z,2020-06-06T16:13:52Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Most of the edits in `__add__` and `__sub__` are just an un-indent to get rid of an unreachable branch, but found this incorrect case along the way."
632658828,34624,DOC: updated core/arrays/base.py for PR08 errors,willpeppo,closed,2020-06-06T18:42:01Z,2020-06-06T22:23:59Z,
578766283,32589,regex with word boundary in pandas query expression doesn't work correctly,vale46,open,2020-03-10T18:05:34Z,2020-06-07T00:34:21Z,"Minimal example to highlight the issue I'm seeing when using pandas query to filter rows:

#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame(['A test case', 'Another Testing Case'], columns=list('A'))
df.query(""A.str.contains(r'\btest\b', regex=True, case=False)"", engine='python')
```
(incorrectly) returns
```python
Empty DataFrame
Columns: [A]
Index: []
```
whereas
#### Expected Output

```python
df[df.A.str.contains(r'\btest\b', regex=True, case=False)]
             A
0  A test case
```

#### Output of ``pd.show_versions()``
<details>
INSTALLED VERSIONS

------------------

commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : 0.29.15
pytest           : 5.3.5
hypothesis       : 5.5.4
sphinx           : 2.4.0
blosc            : None
feather          : 0.4.0
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.48.0
</details>
"
629992301,34549,Ci #34131 rm slow fixtures,OlivierLuG,closed,2020-06-03T13:25:02Z,2020-06-07T08:59:19Z,"- [ x ] Part of #34131 
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ FAIL ] tests passed for 11 of 12 files changed. 6 tests fail inside the file `pandas/core/arrays/datetimes.py` , but there are not concerned by this PR. So I decide to push anyway.

I went through tests tagged with `@pytest.mark.slow` and take a look at the duration. The test is unmarked if the test duration is below 1 second.
note: I considered the total time for each combination in case of parameters

A total of 76 flags `slow` were unmark, so we can focus on ""real"" slow tests.


"
626834225,34443,TST #28981 comparison operation for interval dtypes,OlivierLuG,closed,2020-05-28T21:41:00Z,2020-06-07T09:03:01Z,"As a complement of #28980, one test was modified to take into account this issue:
- [ x ] closes #28981
- [ 1 ] test modified
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
626802781,34439, TST #24444 converting Period to Timestamp,OlivierLuG,closed,2020-05-28T20:44:49Z,2020-06-07T09:03:17Z,"I've tested intervals `ms`, `s` and `min`.
Note: the test raises an error for `ms` on v1.0.3, but is successful on master
Note: I did not choose to close this issue has the test fail for `ns` unit

[ ] #24444
[ 1 ] tests added / passed
[ x ] passes `black pandas`
[ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
623656516,34338,BUG: fix concat of Sparse with non-sparse dtypes,jorisvandenbossche,closed,2020-05-23T13:02:59Z,2020-06-07T09:37:06Z,"Closes #34336

This adds tests with the behaviour as it was on 0.25.3 / 1.0.3, and some changes to get back to that behaviour.  
(but whether this behaviour is fully ""sane"", I am not sure ..)"
200149803,15111,ENH: Dropping outliers,ashishsingal1,closed,2017-01-11T17:00:56Z,2020-06-07T10:13:10Z,"Create a new function to remove outliers.

http://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-dataframe

I find myself using the code from SO quite often to remove outliers in a particular column when preprocessing data and it seems this is a common issue. It would be nice to have a function that operates on a Series to do this automatically.

#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame(np.random.randn(100, 3))

# from SO answer by tanemaki
from scipy import stats
df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]

# instead, would prefer
df.drop_outliers(3)

```

Alternatively, instead of a new function, we could modify .clip(), though I think a new function makes more sense.

The implementation could be similar to the SO implementation.

If there is agreement that this would be useful and the implementation makes sense, happy to do the PR.

"
631657745,34600,DOC: Fixed docstring in Series .isin() method,MarianD,closed,2020-06-05T14:50:52Z,2020-06-07T10:59:10Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
394248030,24444,Imprecision when converting Period to timestamp,mkuklik,closed,2018-12-26T21:57:30Z,2020-06-07T17:19:57Z,"#### Code Sample, a copy-pastable example if possible

```python
pd.Period(""2000-12-31 23:59:59.999"").to_timestamp()
```
returns: 2000-12-31 23:59:59.998999999
instead of 2000-12-31 23:59:59.999
It works correctly for other dates, e.g.
```python
pd.Period(""2000-12-31 23:59:59.99"").to_timestamp()
```
returns correctly ""2000-12-31 23:59:59.990000""

#### Problem description
incorrect conversion of Period to Timestamp, see above.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None

pandas: 0.23.4
pytest: None
pip: 18.1
setuptools: 39.1.0
Cython: None
numpy: 1.15.4
scipy: 1.2.0
pyarrow: None
xarray: None
IPython: 7.2.0
sphinx: None
patsy: 0.5.1
dateutil: 2.7.5
pytz: 2018.7
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
633209119,34627,TST #24444 added tests,OlivierLuG,closed,2020-06-07T08:53:04Z,2020-06-07T18:53:01Z,"At last, there are no issues at all. I've made a new PR.

- [x] closes #24444
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
627819305,34490,DOC: Added documentation for building using pyenv,rorcores,closed,2020-05-30T20:52:29Z,2020-06-07T20:10:05Z,"Added documentation for building using pyenv. Discussed this PR with @WillAyd on gitter on May 20.

Also fixed up another line he mentioned (python setup.py build_ext --inplace -j 4
instead of:
python setup.py build_ext --inplace -j 0) for conda/venv/pyenv.

Have rebuilt the docs and ensured they build properly.
"
377094435,23475,Clarify (or de-duplicate) to_offset vs Period._maybe_convert_freq,jbrockmendel,closed,2018-11-03T21:22:26Z,2020-06-07T20:11:13Z,"Period._maybe_convert_freq calls to_offset, so I guess is a strict superset of its functionality?  The difference should be documented along with what use case calls for which.

Also it isn't clear that _maybe_convert_freq needs to be a class method, could just be a function."
627690998,34480,DOC: `merge` doc for `indicator` parameter can be improved,ianozsvald,closed,2020-05-30T09:43:21Z,2020-06-07T20:26:14Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html#pandas.DataFrame.merge


#### Documentation problem

The documentation is correct but hard to read, currently:

```... If string, column with information on source of each row will be added to output DataFrame, and column will be named value of string. Information column is Categorical-type and ...```

#### Suggested fix for documentation

I believe the following paragraph will be easier to read:

```
If True, adds a column to the output DataFrame called “_merge” with information on 
the source of each row. The column can be given a different name by providing a 
string argument. The column will have a Categorical type with the value of “left_only” 
for observations whose merge key only appears in ‘left’ DataFrame, “right_only” for 
observations whose merge key only appears in ‘right’ DataFrame, and “both” if 
the observation’s merge key is found in both.
```
"
627786163,34485,DOC: updating the `indicator` wording in `merge` doc,nnick14,closed,2020-05-30T18:00:46Z,2020-06-07T20:26:21Z,"- [ ] closes #34480 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
632564541,34619,"DOC: docstring, closes #23475",jbrockmendel,closed,2020-06-06T16:18:29Z,2020-06-07T20:34:38Z,"- [x] closes #23475
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
630615124,34568,DOC: The argument of pandas.DataFrame.to_markdown is unfriendly,nuka137,closed,2020-06-04T08:23:22Z,2020-06-07T20:59:52Z,"#### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.to_markdown.html


#### Documentation problem

The document of `pandas.DataFrame.to_markdown` is unfriendly because many options are shown as `**kwargs`.
I think these arguments should be documented explicitly like a `to_csv`.

https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.to_csv.html


#### Suggested fix for documentation

All arguments supported by `to_markdown` should be shown at document.
If don't mind, I would like to tackle this improvement."
631424917,34594,Improve document for **kwargs argument of pandas.DataFrame.to_markdown,nuka137,closed,2020-06-05T08:48:21Z,2020-06-07T21:00:00Z,"- [x] closes #34568 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
623092084,34305,"DOC: ""Setting layouts"" plots are hard to read",MarcoGorelli,closed,2020-05-22T09:50:44Z,2020-06-07T21:23:10Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/dev/user_guide/visualization.html#using-layout-and-targeting-multiple-axes

**Note**: You can check the latest versions of the docs on `master` [here](https://pandas.pydata.org/docs/dev/).

#### Documentation problem

This is hard to read:
![image](https://user-images.githubusercontent.com/33491632/82655302-e2b74b00-9c19-11ea-9bdb-52c1e9d86f45.png)

Same with the plot immediately below it:
![image](https://user-images.githubusercontent.com/33491632/82655376-fcf12900-9c19-11ea-9e02-54eedf7c1600.png)


#### Suggested fix for documentation

Could the plots be made clearer, so the labels don't cover each other?
"
625172492,34394,"DOC: ""Setting layouts"" plots' label size GH34305",najann,closed,2020-05-26T20:19:42Z,2020-06-07T21:23:16Z,"- [X] closes #34305
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
631315132,34591,DOC: updated base.py and datetimes.py in core/indexes for PR08,willpeppo,closed,2020-06-05T05:07:34Z,2020-06-07T21:23:30Z,
605268319,33740,BUG: pandas merge suffixes accepting `set` can interchange right and left suffix order,PuneethaPai,closed,2020-04-23T06:14:49Z,2020-06-08T01:36:52Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

```python
df_b = pd.DataFrame(dict(p=[1,2,3], q=[2,3,4]))
df_a = pd.DataFrame(dict(p=[2,3,4], q=[1,2,3]))
pd.merge(left=df_b, right=df_a, on=['p'], how='outer', suffixes={'_b', '_a'})
```

#### Current output:
  | p | q_a | q_b
-- | -- | -- | --
0 | 1 | 2.0 | NaN
1 | 2 | 3.0 | 1.0
2 | 3 | 4.0 | 2.0
3 | 4 | NaN | 3.0

#### Expected Output:

  | p | q_b | q_a
-- | -- | -- | --
0 |1 | 2.0 | NaN
1 | 2 | 3.0 | 1.0
2 | 3 | 4.0 | 2.0
3 | 4 | NaN | 3.0


#### Problem description
This [line](https://github.com/pandas-dev/pandas/blob/837daf18d480cce18c25844c591c39da19437252/pandas/core/reshape/merge.py#L670
) is causing the issue:
```
lsuf, rsuf = self.suffixes
```
When you unpack set it returns elements in sorted order:
```
l, r = (2, 1); print(l, r) # l=2, r=1
l, r = [2, 1]; print(l, r) # l=2, r=1
l, r = {2, 1}; print(l, r) # l=1, r=2
```

#### Output of ``pd.show_versions()``

<details>
[INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.165-133.209.amzn2.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None]
</details>
"
627619982,34477,CLN: Removed duplicated test data,alimcmaster1,closed,2020-05-30T01:17:43Z,2020-06-08T01:45:31Z,"Remove duplicated test data.

""test1.csv"" & ""tips.csv"" are present in both ""tests/io/parser/data"" & ""tests/io/data/csv"".

Move the ""*.csv.bz2"" & ""*.csv.gz"" files since the s3_resource fixture in tests/io/conftest requires these.

Follow up from: https://github.com/pandas-dev/pandas/issues/29439

Think we should move all the data in io/parser/data/* into io/data/ so it can be re-used. Will save for follow up."
631749982,34604,DOC: updated io/pytables.py to fix PR08,willpeppo,closed,2020-06-05T16:50:48Z,2020-06-08T01:52:36Z,
624589609,34379,BUG/API: to_timedelta unit-argument ignored for string input,pdnm,closed,2020-05-26T04:15:42Z,2020-06-08T01:56:49Z,"Supersedes #23025
- [x] closes #12136
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
- [x] merge master
"
625894910,34413,TST: slow test_nearest_upsample_with_limit with tzlocal,jbrockmendel,closed,2020-05-27T17:40:13Z,2020-06-08T02:01:41Z,"Doing `pytest pandas/tests --skip-slow --durations=10` the top two entries are

```
71.76s call     pandas/tests/resample/test_datetime_index.py::test_nearest_upsample_with_limit[tzlocal()-30S-Y]
59.86s call     pandas/tests/resample/test_datetime_index.py::test_nearest_upsample_with_limit[tzlocal()-30S-10M]
```

Because this uses the tz_aware_fixture, there isn't a trivial way to add a pytest.mark.slow to this, but figuring something out may be worthwhile."
631947966,34610,"TST: mark tzlocal tests as slow, closes #34413",jbrockmendel,closed,2020-06-05T20:50:23Z,2020-06-08T02:05:00Z,"- [x] closes #34413
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
632002664,34612,REF: make DateOffset apply_index methods operate on ndarrays where feasible,jbrockmendel,closed,2020-06-05T22:01:04Z,2020-06-08T02:05:24Z,
631315320,34592,BUG: Series.where doesn't work with empty lists,jluttine,closed,2020-06-05T05:08:05Z,2020-06-08T02:15:44Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas. (Checked 1.0.3)

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
>>> pd.Series([], dtype=float).where([])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/nix/store/vqd2pmsja84h36wz6c878bgbpz46cy2h-python3.7-pandas-1.0.3/lib/python3.7/site-packages/pandas/core/generic.py"", line 8919, in where
    cond, other, inplace, axis, level, errors=errors, try_cast=try_cast
  File ""/nix/store/vqd2pmsja84h36wz6c878bgbpz46cy2h-python3.7-pandas-1.0.3/lib/python3.7/site-packages/pandas/core/generic.py"", line 8673, in _where
    raise ValueError(msg.format(dtype=cond.dtype))
ValueError: Boolean array expected for the condition, not float64
```

For comparison, if the list isn't empty, it works:

```python
>>> pd.Series([42], dtype=float).where([True])
0    42
dtype: int64
```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

If `where` accepts lists as its mask argument, it should work for empty lists too. Either don't support lists at all or support empty lists too.

#### Expected Output

```
>>> pd.Series([], dtype=float).where([])
Series([], dtype: float64)
```

#### Output of ``pd.show_versions()``

<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.41
machine          : x86_64
processor        :
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : None
setuptools       : None
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : None
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : None
numba            : None

</details>
"
632043130,34614,CLN: update tslibs/tseries test locations/imports,jbrockmendel,closed,2020-06-05T23:08:30Z,2020-06-08T02:30:39Z,
631204368,34585,REF: ensure we have offset objects in plotting functions,jbrockmendel,closed,2020-06-04T23:18:01Z,2020-06-08T02:31:24Z,
632225706,34616,Improve document for **kwargs argument of pandas.Series.to_markdown,nuka137,closed,2020-06-06T05:38:54Z,2020-06-08T02:38:31Z,"This is a follow-up PR of #34594 .  
`pandas.Series.to_markdown` has also same argument.

This PR adds example for `pandas.Series.to_markdown` with tabulate option.

Related Issue: #34568 "
631477314,34595,BUG: fix Series.where(cond) when cond is empty,simonjayhawkins,closed,2020-06-05T10:13:34Z,2020-06-08T08:31:23Z,"- [ ] closes #34592
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref #21947 for fix for DataFrame.where
"
633492813,34631,TYP: some type annotations for interpolate,simonjayhawkins,closed,2020-06-07T15:08:39Z,2020-06-08T08:32:06Z,pre-cursor to #34628
634262103,34636,CLN: EWMA cython code and function dispatch,mroeschke,closed,2020-06-08T07:02:17Z,2020-06-08T15:28:52Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
632777572,34625,REF: re-use existing conversion functions,jbrockmendel,closed,2020-06-06T22:38:16Z,2020-06-08T16:11:14Z,
631304999,34590,REF: mix PeriodPseudoDtype into PeriodDtype,jbrockmendel,closed,2020-06-05T04:37:11Z,2020-06-08T16:25:09Z,This will allow us to start getting rid of `get_freq_code` usages and related libfrequencies functions that are unnecessarily roundabout.
128613727,12136,BUG/API: to_timedelta unit-argument ignored for string input,belteshassar,closed,2016-01-25T19:11:18Z,2020-06-08T22:10:04Z,"So, I don't know if this is by design, but it sure confused me. `to_timedelta()` operates differently on the number `1000` compared to the string `'1000'`:

``` python
In[3]: pd.to_timedelta(1000, unit='ms')
Out[3]: Timedelta('0 days 00:00:01')
In[4]: pd.to_timedelta(1000.0, unit='ms')
Out[4]: Timedelta('0 days 00:00:01')
```

and

``` python
In[6]: pd.to_timedelta('1000ms')
Out[6]: Timedelta('0 days 00:00:01')
```

while

``` python
In[5]: pd.to_timedelta('1000', unit='ms')
Out[5]: Timedelta('0 days 00:00:00.000001')
```

I've tried my best to come up with a reason for why this behaviour would be desirable, but I can't think of any.
"
633599746,34634,BUG/API: Disallow unit if input to Timedelta and to_timedelta is/contains str,pdnm,closed,2020-06-07T17:28:50Z,2020-06-08T22:10:15Z,"- [x] closes #12136
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
634832957,34653,DOC: updated _testing.py for PR08 errors,willpeppo,closed,2020-06-08T18:21:23Z,2020-06-08T22:57:02Z,
634715867,34646,DOC: updated io/sql.py for PR08 errors,willpeppo,closed,2020-06-08T15:23:27Z,2020-06-08T22:58:15Z,
634753669,34649,REF: simplify libperiod.get_yq,jbrockmendel,closed,2020-06-08T16:16:09Z,2020-06-08T23:10:47Z,
631292220,34588,"REF: implement c_FreqGroup, FreqGroup in tslibs.dtypes",jbrockmendel,closed,2020-06-05T03:56:42Z,2020-06-08T23:11:06Z,"These are already used in libperiod, better to be explicit about it.

Coming up: make FreqGroup into an enum with helper methods to convert between FreqGroup/PeriodDtype/DateOffset/Resolution"
357296073,22610,to_csv() surrogates not allowed,obilodeau,closed,2018-09-05T15:56:17Z,2020-06-08T23:36:05Z,"#### Code Sample

```python
s = '\ud800'
srs = pd.Series()
srs.loc[ 0 ] = s
srs.to_csv('testcase.csv')
```

Stack trace:

```
---------------------------------------------------------------------------
UnicodeEncodeError                        Traceback (most recent call last)
<ipython-input-50-769583baba38> in <module>()
      4 srs = pd.Series()
      5 srs.loc[ 0 ] = s
----> 6 srs.to_csv('testcase.csv')

/opt/conda/lib/python3.6/site-packages/pandas/core/series.py in to_csv(self, path, index, sep, na_rep, float_format, header, index_label, mode, encoding, compression, date_format, decimal)
   3779                            index_label=index_label, mode=mode,
   3780                            encoding=encoding, compression=compression,
-> 3781                            date_format=date_format, decimal=decimal)
   3782         if path is None:
   3783             return result

/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py in to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)
   1743                                  doublequote=doublequote,
   1744                                  escapechar=escapechar, decimal=decimal)
-> 1745         formatter.save()
   1746 
   1747         if path_or_buf is None:

/opt/conda/lib/python3.6/site-packages/pandas/io/formats/csvs.py in save(self)
    169                 self.writer = UnicodeWriter(f, **writer_kwargs)
    170 
--> 171             self._save()
    172 
    173         finally:

/opt/conda/lib/python3.6/site-packages/pandas/io/formats/csvs.py in _save(self)
    284                 break
    285 
--> 286             self._save_chunk(start_i, end_i)
    287 
    288     def _save_chunk(self, start_i, end_i):

/opt/conda/lib/python3.6/site-packages/pandas/io/formats/csvs.py in _save_chunk(self, start_i, end_i)
    311 
    312         libwriters.write_csv_rows(self.data, ix, self.nlevels,
--> 313                                   self.cols, self.writer)

pandas/_libs/writers.pyx in pandas._libs.writers.write_csv_rows()

UnicodeEncodeError: 'utf-8' codec can't encode character '\ud800' in position 2: surrogates not allowed
```

#### Problem description

The presence of Unicode surrogates in a dataframe (or Series) causes an error in `.to_csv()`. This has already been fixed in `.to_hdf()` by allowing the `errors=` argument to be used where we can use the `surrogatepass` or `surrogateescape` error handler.

See the [original bug report](https://github.com/pandas-dev/pandas/issues/20835) and the [PR that fixed it](https://github.com/pandas-dev/pandas/pull/20873).

#### Expected Output

No error.

#### Output of ``pd.show_versions()``

I forgot to grab this before the end of my workshop and I destroyed the cloud instance. Sorry. It was Python 3.6 and pandas 0.23.4 I think."
634752278,34648,REF: de-duplicate month/year rolling in libperiod,jbrockmendel,closed,2020-06-08T16:14:05Z,2020-06-09T00:55:19Z,
634978502,34658,CLN: disallow passing tuples for Period freq,jbrockmendel,closed,2020-06-08T22:35:33Z,2020-06-09T01:57:10Z,Really looking forward to killing off `get_freq_code`
633519706,34632,Revert backport of #33632: Parquet & s3 I/O changes,alimcmaster1,closed,2020-06-07T15:43:26Z,2020-06-09T07:36:51Z,"Ref: https://github.com/pandas-dev/pandas/issues/34626#issuecomment-640225867

Revert backport of https://github.com/pandas-dev/pandas/pull/33632
Backport: https://github.com/pandas-dev/pandas/pull/34173

cc @jorisvandenbossche @simonjayhawkins 
"
627234276,34458,CLN: Clean csv files in test data GH34427,najann,closed,2020-05-29T12:30:26Z,2020-06-09T11:51:08Z,"- [x] closes #34427
- [x] tests passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I deleted the `iris.csv` and `tips.csv` files which are unused duplicates (i.e. those in `pandas/tests/io/data/csv`). 
"
629896644,34546,BUG: to_datetime do not keep the date format throughout the column when using inferred format,lviani,closed,2020-06-03T10:54:00Z,2020-06-09T12:32:44Z,"- [ X] I have checked that this issue has not already been reported.

- [X ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd

# day|month first (no error is raised, and it shouldnt continue normally)
print('First case - year last | no error raised')
df = pd.DataFrame({'timestamp':['11-05-2020','12-05-2020','14-05-2020','13-05-2020','01-06-2020','02-06-2020']})
print(pd.to_datetime(df['timestamp']))

# Year first (an error is raised, thus it is what I would expect)
print('\n\nSecond case - year first | error raised')
df = pd.DataFrame({'timestamp':['2020-11-05','2020-12-05','2020-14-05','2020-13-05','2020-01-06','2020-02-06']})
pd.to_datetime(df['timestamp'])

```

#### Problem description

The function to_datetime do not keep the same date format throughout the column when using inferred format. This happens when the dates to parse do NOT start with the year.

Using the table below as input ('test.csv'):
timestamp
2020-11-05
2020-12-05
2020-14-05
2020-13-05
2020-01-06
2020-02-06

Problem:
1) The problem happens when the dates do not start with the year. In this case, pandas switch the month and day (see row 2 and 3), thus changing the format detected.
0   2020-11-05
1   2020-12-05
2   2020-05-14
3   2020-05-13
4   2020-01-06
5   2020-02-06

2) When the dates start with the year and error is raised (for me this should also happen in the previous case).
ValueError: month must be in 1..12


#### Expected Output
raise an error if the datetime format of the entry values changes in the same column

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.18.0-25-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 5.3.2
pip: 19.0.3
setuptools: 40.8.0
Cython: 0.29.6
numpy: 1.16.2
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 7.4.0
sphinx: 1.8.5
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: 1.2.1
tables: 3.5.1
numexpr: 2.6.9
feather: None
matplotlib: 3.0.3
openpyxl: 2.6.1
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.5
lxml.etree: 4.3.2
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.3.1
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
634389469,34638,CLN: deduplicate in core.internals.blocks.interpolate,simonjayhawkins,closed,2020-06-08T08:52:32Z,2020-06-09T12:44:41Z,broken off #34628
623292601,34317,BUG: GH29461 strftime() with nanoseconds for Timestamp,matteosantama,closed,2020-05-22T15:32:58Z,2020-06-09T14:41:34Z,"- [x] closes #29461 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The new method displays %f to the full precision, including trailing zeroes. I could add a check that only displays nanoseconds if they are non-zero."
605052368,33729,BLD: bump numpy min version to 1.15.4,jbrockmendel,closed,2020-04-22T20:31:06Z,2020-06-09T14:44:08Z,"- [x] closes #33718
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
628073205,34507,REF: use standard pattern in normalize_i8_timestamps,jbrockmendel,closed,2020-06-01T00:04:20Z,2020-06-09T15:00:58Z,From here the plan is to move to passing `int64_t*` which will allow us to share code between the scalar/vector versions of this function
635263194,34660,BUG: ExtensionArray.equals expects ExtensionArray.all to return a numpy.bool_,xhochy,closed,2020-06-09T09:20:44Z,2020-06-09T15:37:45Z,"- [x] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Problem description

The current implementation of `ExtensionArray.equals` expects that the implementation of `ExtensionArray.all` always returns a numpy scalar, see https://github.com/pandas-dev/pandas/blob/942beba1e8f49c931b7c501bb1eec0e383313ac7/pandas/core/arrays/base.py#L741. If you don't use `numpy` as the backing memory store, I don't it is reasonable to stick to NumPy scalars here.

Instead, the method should unpack `numpy` scalars if necessary but passthrough otherwise.

Context: This appeared while running the unit tests with `fletcher` and `pandas`' master."
632039109,34613,REF: RelativeDeltaOffset.apply_index operate on ndarray,jbrockmendel,closed,2020-06-05T23:01:28Z,2020-06-09T16:36:52Z,
635268810,34661,BUG: Allow plain bools in ExtensionArray.equals,xhochy,closed,2020-06-09T09:28:51Z,2020-06-09T16:50:12Z,"- [x] closes #34660
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
634806188,34652,DOC: updated plotting/_misc.py for PR08 errors,willpeppo,closed,2020-06-08T17:34:57Z,2020-06-09T17:36:00Z,
634981239,34659,REF: avoid get_freq_code,jbrockmendel,closed,2020-06-08T22:42:17Z,2020-06-09T17:49:43Z,"We'll be able to rip it out entirely after this, #34658, and #34587."
635602875,34672,PERF: normalize_i8_timestamps,jbrockmendel,closed,2020-06-09T16:47:38Z,2020-06-09T18:03:23Z,"xref #34507

Based on the asv mentioned in that thread:

```
In [3]: N = 10**5                                                                                                                                                                                              
In [4]: tz = ""UTC""                                                              
In [5]: series = Series(date_range(start=""1/1/2000"", periods=N, freq=""T"", tz=tz))

In [6]: %timeit series.dt.normalize()                                                                                                                                                                          
947 µs ± 4.33 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)   # <-- PR
3.26 ms ± 43.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)   # <-- master
```"
619270637,34201,ENH: Implement __iter__ for Rolling and Expanding,charlesdong1991,closed,2020-05-15T21:33:29Z,2020-06-09T18:17:49Z,"- [x] closes #11704
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
598235371,33476,BUG: read_excel: incorrect multi-index values - 1 (empty or equal values),kuraga,closed,2020-04-11T10:09:19Z,2020-06-09T18:34:03Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

pd.read_excel('df.xlsx', sheet_name='Sheet1', index_col=0, header=[0, 1])
```

Note: `engine`s could be different.

#### Problem description

Exception on `pd.read_excel()`:
```
$ ~/miniconda3/bin/python read.py 
Traceback (most recent call last):
  File ""read.py"", line 4, in <module>
    df = pd.read_excel('df_empty.xlsx', engine='openpyxl', sheet_name='Sheet1', index_col=0, header=[0, 1])
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 334, in read_excel
    **kwds,
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 888, in parse
    **kwds,
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 521, in parse
    ].columns.set_names(header_names)
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 1325, in set_names
    idx._set_names(names, level=level)
  File ""/home/sasha/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 1239, in _set_names
    raise ValueError(f""Length of new names must be 1, got {len(values)}"")
ValueError: Length of new names must be 1, got 2
```

on XLSX like:
![image](https://user-images.githubusercontent.com/1063219/79040944-b2c96080-7bf4-11ea-8f70-c09899f76611.png)
(C2 is empty, [df_empty.xlsx](https://github.com/pandas-dev/pandas/files/4464392/df_empty.xlsx))

or this:

![image](https://user-images.githubusercontent.com/1063219/79040983-f91ebf80-7bf4-11ea-8aea-b1755309bd3e.png)
(B2 == C2, [df_equals.xlsx](https://github.com/pandas-dev/pandas/files/4464393/df_equals.xlsx))

#### Expected Output

No exceptions.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.25-calculate
machine          : x86_64
processor        : Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz
byteorder        : little
LC_ALL           : None
LANG             : ru_RU.utf8
LOCALE           : ru_RU.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : 0.29.15
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : 1.2.8
numba            : None
</details>
"
118942708,11704,ENH: implement __iter__ for window objects,jreback,closed,2015-11-25T23:45:11Z,2020-06-09T19:09:07Z,"xref #11603 

might be slightly non-trivial as the current impl does NOT use an explict iterator, rather does a sliding window in cython and marginal calcs.
"
581264820,32702,BUG: Add errors argument to to_csv() call to enable error handling for encoders,roberthdevries,closed,2020-03-14T15:59:58Z,2020-06-09T19:13:07Z,"- [x] closes #22610 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
633340946,34629,QST: How to plot hist not in stacked style for different columns by df.plot.hist()?,futurehome,closed,2020-06-07T11:50:02Z,2020-06-09T20:29:26Z,"-  I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

When using pd.plot.hist(), how to plot different columns in the parallel style(1st pic below), rather than in the stacked style(2nd pic below)?

Thanks,
Alex

![image](https://user-images.githubusercontent.com/6230304/83967855-b416c100-a8f7-11ea-8491-17763ca13384.png)

![image](https://user-images.githubusercontent.com/6230304/83967844-9d706a00-a8f7-11ea-8f86-a439ade13dab.png)


---

#### Question about pandas

**Note**: If you'd still like to submit a question, please read [this guide](
https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your question.

```python
# Your code here, if applicable

```
"
266632267,17914,PERF: enable caching on expensive offsets,chris-b1,closed,2017-10-18T20:32:22Z,2020-06-09T20:54:43Z,"xref  #16463

We have a set of seemingly unused but also seemingly functional caching logic for generating ranges of offsets.  Example
```python
In [63]: BM = pd.offsets.BusinessMonthEnd()

In [64]: %timeit pd.date_range('1990-01-01', '2020-01-01', freq=BM)
18.1 ms ± 366 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [65]: BM._cacheable
Out[65]: False

In [66]: BM._cacheable = True

In [67]: %timeit pd.date_range('1990-01-01', '2020-01-01', freq=BM)
686 µs ± 17.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

In [68]: a = pd.date_range('1990-01-01', '2020-01-01', freq='BM')

In [69]: b = pd.date_range('1990-01-01', '2020-01-01', freq=BM)

In [70]: (a == b).all()
Out[70]: True
```
Possibly should enable or expose an API for this?  Especially for slower offsets.

cc @jbrockmendel"
634404405,34639,ENH: Add max_results to `read_gbq`,charlesdong1991,closed,2020-06-08T09:03:57Z,2020-06-09T22:17:20Z,"For queries for which the purpose is to create/replace tables: `CREATE OR REPLACE TABLE`, if the table is huge, it will eat up a lot of memory currently using `read_gbq`.

And it turns out that since `pandas-gbq==0.12.0`, a new argument `max_results` was implemented to limit the number of rows in result dataframe for e.g. setting it to `0` for DDL queries, which will help save memory.

So it will be nice for pandas to add it for `read_gbq` as `pandas-gbq` does."
634543059,34641,ENH: Add max_results kwarg to read_gbq,JohnPaton,closed,2020-06-08T11:53:34Z,2020-06-09T22:17:24Z,"Adds support for the new `max_results` kwarg from pandas-gbq (added in [0.12.0](https://github.com/pydata/pandas-gbq/releases/tag/0.12.0)). Since `max_results` is a new kwarg, it is handled and tested in the same way as the `use_bqstorage_api` kwarg to maintain backwards compatibility.

❓ **Open question:** Setting `max_results=0` causes `pandas_gbq.read_gbq` to return `None` instead of a DataFrame. I've kept this behaviour the same in this PR, but maintainers may prefer to always return a (empty) DataFrame instead of `None`.

- [x] closes #34639
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

"
630826826,34573,BUG: Pandas changes dtypes of columns when no float (or other) assignments are done to this column,volsend,closed,2020-06-04T13:30:19Z,2020-06-09T22:31:41Z,"```python
df = pd.DataFrame()
df[""a""] = int(0)

for col_name in [""b"", ""c""]:
    df[col_name] = 0.

print(df.dtypes)
    
for idx, b in enumerate([1,2,3]):
    df.loc[df.shape[0]] = {
        'a' : int(idx),
        'b' : float(b),
        'c' : float(b),
    }

print(df.dtypes)
```
#### Problem description

In the above example, pandas changes the dtype of the column `a` of dataframe `df` from _int64_ to _float64_, while there is no operation, that assigns non-integer value to this column. 

#### Expected Output

Currently, the output of this code is:
```
a      int64
b    float64
c    float64
dtype: object
a    float64
b    float64
c    float64
dtype: object
```

while the expected one is:
```
a      int64
b    float64
c    float64
dtype: object
a      int64
b    float64
c    float64
dtype: object
```

<details>
  <summary>pd.show_versions() output</summary>
  
```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-101-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.4
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 44.0.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None
```
</details>
"
606509974,33775,BUG: Changing way of applying numpy functions on Series,mproszewska,closed,2020-04-24T18:41:29Z,2020-06-09T22:49:54Z,"- [x] closes #33492 
- [x] tests added and passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] numpy functions on Series are called like other functions"
584866378,32855,BUG: Improved error msg,a-y-khan,closed,2020-03-20T06:18:23Z,2020-06-09T22:50:28Z,"- [x] closes #9259
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Hopefully this is a more informative message. The original bug report mentions that the index key also fails for Series but didn't report the error type, so I took my best guess at the reporter's intent in the test."
631236943,34587,ENH: Resolutions for month/qtr/year,jbrockmendel,closed,2020-06-05T00:59:20Z,2020-06-09T23:39:59Z,"This does three things

1) Use Resolution objects instead of strings in the places in core.indexes.  This makes whats going on much clearer.
2) To make 1) possible, extend Resolution to include month, quarter, and year.  This means that `pd.period_range(""2017"", periods=3, freq=""A"").resolution` is now ""year"" instead of ""day"", which is more accurate.
3) with year/quarter/month added, `Resolution` and `FreqGroup` will be re-labellings of each other, so we can de-duplicate them.  They will end up living in tslibs.dtypes."
635662379,34674,CLN: remove get_freq_code,jbrockmendel,closed,2020-06-09T18:24:02Z,2020-06-10T01:39:11Z,Sits on top of #34587
520461464,29512,DEPS: Minimum numpy version for pandas 1.0,datapythonista,closed,2019-11-09T15:43:01Z,2020-06-10T01:43:29Z,"There has been some discussion about dropping numpy 1.13, and require 1.15 for pandas 1.0. See https://github.com/pandas-dev/pandas/pull/29212#issuecomment-551856248

I think that'd be nice to reduce our matrix of builds for the CI, other than that I'm not sure what advantages this will have in our code base.

Any opinions @pandas-dev/pandas-core ?"
328133209,21275,shift does not always preserve freqstr if series contains only one or two elements,kdebrab,closed,2018-05-31T13:13:29Z,2020-06-10T02:26:39Z,"#### Problem description

The freqstr information is lost if series contains only one or two elements AND the freqstr by which is shifted is not the same as the freqstr of the series. In all other cases it works fine.

```python
import pandas as pd
s1 = pd.Series([1], index=pd.date_range('2016-1-1', periods=1, freq='H'))
s2 = pd.Series([1, 2], index=pd.date_range('2016-1-1', periods=2, freq='H'))
s3 = pd.Series([1, 2, 3], index=pd.date_range('2016-1-1', periods=3, freq='H'))

# following DOES NOT preserve freqstr
print(s1.shift(1, '2H'))
print(s2.shift(1, '2H'))
print(s1.shift(1, 'min'))
print(s2.shift(1, 'min'))

# for comparison, following is all OK (preserves freqstr)
print(s1.shift(1, 'H'))
print(s2.shift(1, 'H'))
print(s3.shift(1, 'H'))
print(s3.shift(1, '2H'))
print(s3.shift(1, 'min'))
print(s3.shift(1, 'min'))
```
#### Expected Output

Preservation of the freqstr in all above cases.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.13.final.0
python-bits: 32
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None
pandas: 0.23.0
pytest: None
pip: 10.0.1
setuptools: 36.4.0
Cython: None
numpy: 1.14.3
scipy: 1.1.0
pyarrow: None
xarray: 0.10.0
IPython: 5.3.0
sphinx: 1.6.3
patsy: 0.5.0
dateutil: 2.6.1
pytz: 2018.3
blosc: None
bottleneck: 1.2.1
tables: None
numexpr: None
feather: None
matplotlib: 2.2.2
openpyxl: 2.5.3
xlrd: 1.0.0
xlwt: 1.3.0
xlsxwriter: 1.0.4
lxml: 4.2.1
bs4: 4.6.0
html5lib: 0.999999999
sqlalchemy: 1.2.3
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
563576426,31903,PERF: Index.__getitem__ should not need to do dtype inference,jorisvandenbossche,closed,2020-02-11T23:04:31Z,2020-06-10T06:57:39Z,"Currently, the `__getitem__` call in a snippet like

```
idx = pd.Index([1, 2, 3, 4])
idx[np.array([0, 3])]
```

does type inference when creating the new Index object (using `_shallow_copy_with_infer`), while I think we can always know for sure the result will be another Int64Index (for the above example) ?"
635765188,34680,CLN: dont consolidate in groupby,jbrockmendel,closed,2020-06-09T21:17:54Z,2020-06-10T07:47:26Z,"full asv run looks like pure noise

```
       before           after         ratio
     [03f5066c]       [f2cae354]
     <master~3>       <cln-consolidate-less-3>
+      38.4±0.1μs         45.7±6μs     1.19  algorithms.SortIntegerArray.time_argsort(1000)
+      11.5±0.2ms       13.3±0.4ms     1.16  groupby.Nth.time_series_nth('float64')
+     7.09±0.02ms      8.02±0.04ms     1.13  io.hdf.HDFStoreDataFrame.time_query_store_table
+      12.3±0.1μs       13.7±0.3μs     1.11  tslibs.timestamp.TimestampOps.time_replace_None('US/Eastern')
+     3.96±0.03μs      4.36±0.04μs     1.10  period.Indexing.time_get_loc
-        1.25±0ms         1.13±0ms     0.91  arithmetic.OffsetArrayArithmetic.time_add_series_offset(<DateOffset: days=2, months=2>)
-        1.22±0ms         1.10±0ms     0.90  arithmetic.OffsetArrayArithmetic.time_add_dti_offset(<DateOffset: days=2, months=2>)
-      1.54±0.1μs      1.39±0.06μs     0.90  index_cached_properties.IndexCache.time_is_all_dates('MultiIndex')
-      12.9±0.5μs       11.6±0.5μs     0.90  tslibs.timedelta.TimedeltaConstructor.time_from_components
-     3.75±0.06ms         3.35±0ms     0.89  rolling.ForwardWindowMethods.time_rolling('Series', 1000, 'float', 'min')
-     1.22±0.01ms         1.08±0ms     0.89  arithmetic.ApplyIndex.time_apply_index(<DateOffset: days=2, months=2>)
-      4.47±0.2μs      3.95±0.08μs     0.88  tslibs.timedelta.TimedeltaConstructor.time_from_datetime_timedelta
-     12.2±0.06ms      10.7±0.04ms     0.88  groupby.MultiColumn.time_col_select_numpy_sum
-      3.98±0.5μs      3.45±0.07μs     0.87  index_cached_properties.IndexCache.time_shape('DatetimeIndex')
-      30.2±0.2ms      26.1±0.08ms     0.86  frame_methods.Dropna.time_dropna_axis_mixed_dtypes('any', 0)
-        2.25±1μs      1.78±0.06μs     0.79  index_cached_properties.IndexCache.time_shape('UInt64Index')
-      5.76±0.9μs       4.51±0.3μs     0.78  index_cached_properties.IndexCache.time_shape('IntervalIndex')
-      2.56±0.5μs       1.99±0.2μs     0.77  index_cached_properties.IndexCache.time_values('IntervalIndex')
-     3.67±0.02ms       2.66±0.1ms     0.73  rolling.ForwardWindowMethods.time_rolling('Series', 1000, 'int', 'min')
-        4.34±1μs       2.95±0.2μs     0.68  index_cached_properties.IndexCache.time_shape('CategoricalIndex')
-        2.48±2μs      1.65±0.09μs     0.66  index_cached_properties.IndexCache.time_shape('Float64Index')
-        3.04±1μs       1.96±0.1μs     0.64  index_cached_properties.IndexCache.time_values('CategoricalIndex')
-      1.88±0.6μs      1.09±0.06μs     0.58  index_cached_properties.IndexCache.time_values('Float64Index')
-        3.14±1μs      1.16±0.05μs     0.37  index_cached_properties.IndexCache.time_values('UInt64Index')


```"
631589267,34599,BUG: Pandas changes dtypes of columns when no float (or other) assignments are done to this column #34573,simonjayhawkins,closed,2020-06-05T13:28:05Z,2020-06-10T08:04:13Z,"- [ ] closes #34573
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
633452892,34630,TYP: some type annotations in core\tools\datetimes.py,simonjayhawkins,closed,2020-06-07T14:17:17Z,2020-06-10T09:25:02Z,
633229899,34628,WIP: avoid internals for Series.interpolate,simonjayhawkins,closed,2020-06-07T09:20:44Z,2020-06-10T09:30:00Z,"This may be premature while discussion about removing the block manager is ongoing

most of the diff is type annotations and collecting the method validation. will break off into pre-cursor PRs"
635361137,34664,Backport PR #34458 on branch 1.0.x (CLN: Clean csv files in test data GH34427),simonjayhawkins,closed,2020-06-09T11:50:34Z,2020-06-10T10:49:18Z,xref #34458
636264277,34690,DOC: Add note about shallow clones in contributing guide,gglanzani,closed,2020-06-10T13:48:43Z,2020-06-10T14:54:15Z,"- [ ] closes #xxxx
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I didn't open a new issue for this, let me know if I should, I'll gladly do it (I got the impression that that's not mandatory by reading the contributing guidelines).

Feel also free to close without merging if I'm the only person who got confused by this."
596823112,33406,BUG: to_period() freq was not infered,ShaharNaveh,closed,2020-04-08T19:56:23Z,2020-06-10T15:15:38Z,"- [x] closes #33358
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

---

When running ```pytest -q --cache-clear pandas/tests/ -k ""period""``` some tests in ```pandas/tests/indexing/``` are failing, but when running ```pytest -q --cache-clear pandas/tests/indexing/``` __all__ the tests are passing (on my local machine), so I figured and debug from here, thoughts?"
636275212,34691,BUG: Unpredictable behaviour of `Series.replace` when filling with `None` values.,complexsum,closed,2020-06-10T14:03:17Z,2020-06-10T16:18:13Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [1.0, 2.0, 'a', 45.0, 0.0]})
df['B'] = df['B'].replace('a', None)
print(df)
```

#### Problem description

While replacing the values in dataframe with `None` using `.replace` method  the values to be replaced get forward filled from the previous values which is *unpredictable behaviour here*.

#### Expected Output

```
   A     B
0  1     1
1  2     2
2  3  None
3  4    45
4  5     0
```

#### Actual Output

```
   A   B
0  1   1
1  2   2
2  3   2
3  4  45
4  5   0
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-33-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.3
numpy            : 1.18.3
pytz             : 2019.3
dateutil         : 2.7.3
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.8
numba            : None

</details>
"
636336813,34698,QST: should pandas series isin raise an exception with a nested sequence? ,giordafrancis,closed,2020-06-10T15:18:06Z,2020-06-10T16:34:24Z,"- [x ] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ x] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).
---

#### Question about pandas

Using a nested structure in series isin() does not rasie an exception, instead no match is found? Should this pass silently? 

```python
# Your code here, if applicable
test = pd.DataFrame({'a': [1,2,3]})
test['a'].isin([1])

0     True
1    False
2    False

test['a'].isin([[1]])

0    False
1    False
2    False

pd.__version__
'0.25.0'

```
"
576592306,32474,pandas documentation error,752525721,open,2020-03-05T23:26:51Z,2020-06-10T22:47:34Z,"pandas documentation
Date: Feb 05, 2020 Version: 1.0.1

Download documentation: PDF Version

Why this PDF Version have no contents>
"
71893554,10017,assigning column values to empty dataframe with loc,paolini,closed,2015-04-29T14:37:29Z,2020-06-11T09:07:33Z,"I'm trying to add a column to an existing dataframe. The dataframe can be empty (0 rows) but I want the column to be added anyway. 

```
import pandas as pd
import numpy as np

df = pd.DataFrame(index=np.arange(4))
df.loc[:,'col'] = 42 # this works fine!

df = pd.DataFrame(index=np.arange(0)) # empty dataframe
df.loc[:,'col'] = 42 # ERROR: ValueError: cannot set a frame with no defined index and a scalar
df['col'] = 42 # this works fine but seems deprecated

pd.__version__ is '0.15.2'
```
"
635479190,34666,BUG: Source build issue with with py38,TomAugspurger,closed,2020-06-09T14:28:06Z,2020-06-11T15:29:02Z,"We don't have a py38 entry in our `pyproject.toml`, so when building from source for 3.8 we fetch NumPy 1.15.4, which isn't compatible with py38. Fix incoming."
600540482,33570,Reverted cython pin,WillAyd,closed,2020-04-15T19:37:17Z,2020-06-11T15:40:31Z,reverts #33534 and closes #33507 
636302785,34694,DOC: updated doctorings for SS06 errors,willpeppo,closed,2020-06-10T14:37:45Z,2020-06-11T16:50:02Z,
607947698,33836,BUG: preserve freq in DTI/TDI factorize,jbrockmendel,closed,2020-04-28T00:58:46Z,2020-06-11T17:27:14Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref #33830"
626023731,34417,TST closes #28980 Certain comparison operations misbehaving for period dtype,OlivierLuG,closed,2020-05-27T20:54:29Z,2020-06-11T17:30:42Z,"I've added test to compare `pd.Series([...], dtype=""period[A-DEC]"")`  to dtype `str`, `bool`, `int`, `float` and `None`.

- [ x ] closes #28980
- [ 5 ] tests added / passed
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
555353303,31336,json normalize max level not working,jphilippou27,closed,2020-01-27T04:44:32Z,2020-06-11T23:07:38Z,"#### Code Sample, a copy-pastable example if possible

```python
from pandas.io.json import json_normalize

data = [{'id': 1,
          'name': ""Cole Volk"",
          'fitness': {'height': 130, 'weight': 60}},
         {'name': ""Mose Reg"",
          'fitness': {'height': 130, 'weight': 60}},
        {'id': 2, 'name': 'Faye Raker',
          'fitness': {'height': 130, 'weight': 60}}]
json_normalize(data, max_level=0)
```
#### Problem description

The code above is from the pandas docs. I'm running python 3.6.4 and pandas==0.22.0


**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
636308727,34696,ENH: DataFrame or Series to code,ELToulemonde,closed,2020-06-10T14:44:33Z,2020-06-12T01:16:02Z,"**Request**

Have a method to transform a pandas object to code. 

**Example**

The given situation

`given_df = pd.DataFrame({""a"": [1, 2],
                                 ""b"": pd.to_datetime([""2019-01-01"", ""2019-12-31""]),
                                 ""c"": [""a"", ""b""]})`

Call to the method : 

`code_string = given_df.to_code()`

Expected output : 
`pd.DataFrame({'a': [1, 2], 'b': pd.to_datetime(['2019-01-01', '2019-12-31'], format='%Y-%m-%d'), 'c': ['a', 'b']})`


**Is your feature request related to a problem?**

The usage I would have of such a method would be when I'm writing integration tests. 

GIven `my_function`, I want to write a integration test to make sure that result doesn't changes. 
The output of my function is a DataFrame which is a bit long a few lines, and a few columns. 


    class test(unitest.testcase): 
        def test_integration(self):
             # Given
             given_some_args
             
             # When
             result_df: pd.DataFrame = my_function(given_some_args)
             
             # Then
             pd.testing.assert_frame_equals(expected_df, result_df)


Writing the `expected_df` manually is time consuming. I would like to be able to get it by executed `result_df.to_code() `once and then copy past the generated code in my integration test.

**API breaking implications**

I don't think it will have impact on existing API. 

**Describe alternatives you've considered**

A proposition of implementation would be `""df = pd.DataFrame( %s )"" % (str(df.to_dict()))`

Although it is a simple implementation, 

- it doesn't work for every types of series (for example datetimes), 
- it doesn't provide executable code in case of `nan`, 
- it doesn't include needed imports

**Additional context**

This need has already been raised in[ this question](https://stackoverflow.com/questions/41769882/pandas-dataframe-to-code) on stackoverflow."
637370579,34722,CLN: remove verbose and private_key reference #34640,lathamri,closed,2020-06-11T22:59:41Z,2020-06-12T06:52:13Z,"- [ ] closes #34640 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
637532582,34726,DOC: changed 'providing' to 'that provides',ObliviousParadigm,closed,2020-06-12T07:01:20Z,2020-06-12T08:47:26Z,"Changed this as it sounds more clear.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
634759676,34650,June 2020 Meeting,TomAugspurger,closed,2020-06-08T16:25:19Z,2020-06-12T20:18:24Z,"The monthly dev meeting is Wednesday June 10th, at 18:00 UTC. Our calendar is at https://pandas.pydata.org/docs/development/meeting.html#calendar to check your local time.

Video Call: https://zoom.us/j/942410248
Minutes: https://docs.google.com/document/u/1/d/1tGbTiYORHiSPgVMXawiweGJlBw5dOkVJLY-licoBmBU/edit?ouid=102771015311436394588&usp=docs_home&ths=true

Please add items you'd like to see discussed to the agenda.

All are welcome to attend."
637262172,34716,DOC: updated pandas/core/series.py for SS06 errors,willpeppo,closed,2020-06-11T19:23:29Z,2020-06-12T20:58:35Z,
637243019,34715,DOC: updated core/groupby/generic.py for SS06 errors,willpeppo,closed,2020-06-11T18:46:50Z,2020-06-12T20:59:56Z,
637591953,34729,Changed the way a few sentences were written,ObliviousParadigm,closed,2020-06-12T08:46:42Z,2020-06-12T21:05:20Z,"Added comma before 'and'

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
637176477,34713,DOC: updated core/indexes/base.py for SS06 errors,willpeppo,closed,2020-06-11T16:54:01Z,2020-06-12T21:51:07Z,
638018181,34738,BUG: pd.NA in format strings with formatting parameters,topper-123,closed,2020-06-12T21:44:49Z,2020-06-12T22:06:31Z,"``pd.NA`` raises if passed to a format string and format parameters are supplied. This is different behaviour than ``np.nan`` and makes converting arrays containing ``pd.NA`` to strings very brittle and annoying.

Examples:

```python
>>> format(pd.NA)
'<NA>'  # master and PR, ok
>>> format(pd.NA, "".1f"")
TypeError  # master
'<NA>'  # this PR
>>> format(pd.NA, "">5"")
TypeError  # master
' <NA>'  # this PR, tries to behave like a string, then falls back to '<NA>', like np.na
```

The new behaviour mirrors the behaviour of ``np.nan``."
628001759,34500,REG: Fix read_parquet from file-like objects,alimcmaster1,closed,2020-05-31T16:58:23Z,2020-06-12T22:33:50Z,"- [x] xref #34467
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry - waiting on https://github.com/pandas-dev/pandas/pull/34481

Use arrow parquet.read_table opposed to ParquetDataset
"
619477047,34208,Restrict Pandas merge suffixes param type to list/tuple to avoid interchange in right and left suffix order,PuneethaPai,closed,2020-05-16T13:38:03Z,2020-06-13T09:33:31Z,"- [x] closes #33740
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
638155864,34745,DOC: updated strings.py for SS06 errors,willpeppo,closed,2020-06-13T12:43:38Z,2020-06-13T17:04:18Z,
636197940,34688,TYP: update setup.cfg,simonjayhawkins,closed,2020-06-10T12:16:47Z,2020-06-13T17:57:49Z,
636322215,34697,TYP: type annotations for read_sas,simonjayhawkins,closed,2020-06-10T15:00:17Z,2020-06-13T18:01:59Z,
612218063,33981,QST:,Gautamshahi,closed,2020-05-04T22:50:05Z,2020-06-13T18:27:39Z,"I am using the below code to merger the multiple CSV files in the folder. The code is working fine for smaller dataset but for bigger data.

As the total number of rows is 51163632 but as a result, in big_frame I am getting only 20883611 rows.

Is there a limit of pandas dataframe to store the values?

big_frame = pd.DataFrame()

filename= []
num_comments=0
for file in os.listdir():
    print(file)
    if file.endswith('.CSV'):
        filename.append(file)
        num_lines = sum(1 for line in open(file, encoding=""utf8""))
        #print(num_lines)
        num_comments = num_comments + num_lines
        print(num_comments)
        try:
        #df = pd.read_csv(file, error_bad_lines=False, sep=',', header=None, encoding='utf-8')
            df = pd.read_csv(file, error_bad_lines=False, delimiter=',', header=None, encoding='utf-8')
        except:
            continue
        big_frame = big_frame.append(df, ignore_index=True)"
638135612,34742,CLN: remove the old 'nature_with_gtoc' sphinx doc theme,jorisvandenbossche,closed,2020-06-13T10:11:10Z,2020-06-13T18:31:48Z,Noticed we still have the old theme customizations in the doc sources.
636278016,34692,TYP: check_untyped_defs pandas.core.resample,simonjayhawkins,closed,2020-06-10T14:07:09Z,2020-06-13T18:49:41Z,"pandas\core\resample.py:969: error: Too many arguments for ""__init__"" of ""object""
pandas\core\resample.py:1572: error: ""DatetimeIndex"" has no attribute ""tzinfo""
pandas\core\resample.py:1573: error: ""DatetimeIndex"" has no attribute ""tzinfo""
pandas\core\resample.py:1585: error: ""PeriodIndex"" has no attribute ""asfreq""; maybe ""freq""?
pandas\core\resample.py:1867: error: ""PeriodIndex"" has no attribute ""asfreq""; maybe ""freq""?
pandas\core\resample.py:1902: error: ""PeriodIndex"" has no attribute ""asfreq""; maybe ""freq""?
"
312181907,20627,Modifying dataframe with float values using .iloc and boolean indexing raises ValueError,karih,closed,2018-04-07T07:37:18Z,2020-06-13T19:58:21Z,"There seems to be inconsistent behavior between updating parts of a dataframe using `.iloc` depending on whether the index is accessed using an integer list or a boolean array. This does not seem to happen if all dataframe values are integer (but still happens if `np.nan` is replaced by `7.0`). Example:

#### Code Sample

```import pandas as pd
import numpy as np

data = [
    [0, 2, 13], 
    [1, 6, 21], 
    [2, np.nan, 0],
    [3, 5, 3]
]
df = pd.DataFrame(data, columns=(""idx"", ""val1"", ""val2"")).set_index(""idx"")

print(""updating with ints"")
df.iloc[np.nonzero(df.index >= 2)[0],0] *= 2
print(""updating with bool"")
try:
    df.iloc[df.index >= 2,0] *= 2 # raises ValueError
except ValueError as e:
    print(""ValueError"", e)
try:
    df.iloc[df.index < 2,0] *= 2 # raises ValueError
except ValueError as e:
    print(""ValueError"", e)
print(""iloc with bool"")
print(df.iloc[df.index >= 2,0])
print(""iloc with ints"")
print(df.iloc[np.nonzero(df.index >= 2)[0],0])
```
Output is:
```
updating with ints
updating with bool
ValueError Must have equal len keys and value when setting with an iterable
ValueError Must have equal len keys and value when setting with an iterable
iloc with bool
idx
2     NaN
3    10.0
Name: val1, dtype: float64
iloc with ints
idx
2     NaN
3    10.0
Name: val1, dtype: float64
```
#### Problem Description
Anticipated behavior is that they should be equivalent when updating, particuarly when one considers that the return value of `.iloc` is the same. Instead, we receieve `ValueError: Must have equal len keys and value when setting with an iterable`. 

#### Output of ``pd.show_versions()``
<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.4.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.15-1-ARCH
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: en_US.utf8
LOCALE: en_US.UTF-8

pandas: 0.22.0
pytest: None
pip: 9.0.3
setuptools: 38.5.2
Cython: 0.25.2
numpy: 1.14.2
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: None
patsy: None
dateutil: 2.7.2
pytz: 2018.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.2.0
openpyxl: None
xlrd: 1.0.0
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0.1
sqlalchemy: 1.2.5
pymysql: None
psycopg2: 2.7.4 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
638140466,34744,CLN: clean and deduplicate in core.missing.interpolate_1d,simonjayhawkins,closed,2020-06-13T10:48:26Z,2020-06-13T20:34:32Z,broken-off #34728
638251937,34758,typo: pivot_table -> pivot,hartzell,closed,2020-06-13T23:28:48Z,2020-06-14T01:29:40Z,"I'm new to pandas and reading the docs for the first time.

I believe that the reference in the *pivot* section should be to `pivot`, not `pivot_table`, to match the code in the example.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
636304216,34695,TYP: check_untyped_defs pandas.io.json._table_schema,simonjayhawkins,closed,2020-06-10T14:39:23Z,2020-06-14T09:51:51Z,"pandas\io\json\_table_schema.py:112: error: Incompatible types in assignment (expression has type ""Dict[str, List[Any]]"", target has type ""str"")
pandas\io\json\_table_schema.py:264: error: Incompatible types in assignment (expression has type ""str"", target has type ""List[Any]"")


"
638227760,34757,typo: rows -> columns,hartzell,closed,2020-06-13T20:32:47Z,2020-06-14T13:56:04Z,"I'm new to pandas and reading the docs for the first time.  By my reading of the first part of this paragraph, there's a dictionary of lists and the lists are `[""Braund..."", ""Allen..."", ...]`, `[22, 35, 58]`, and `[""male"", ""male"", ""female""]`.

The original sentence says: ""... and the values in each list as rows of the DataFrame"", but the values in the first list (`[""Baund...]`) are the values in the first **column** of the illustration.  Likewise the list `[22,...]` is the second column and `[""male"",...]` is the third.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
636583212,34708,BUG: DataFrame.unstack on non-consolidated frame,jbrockmendel,closed,2020-06-10T22:15:53Z,2020-06-14T14:13:01Z,"```
df = pd.DataFrame({""x"": [1, 2, np.NaN], ""y"": [3.0, 4, np.NaN]})

df2 = df[[""x""]]
df2[""y""] = df[""y""]
assert len(df2._mgr.blocks) == 2

>>> df2.unstack()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pandas/core/frame.py"", line 7031, in unstack
    return unstack(self, level, fill_value)
  File ""pandas/core/reshape/reshape.py"", line 419, in unstack
    return _unstack_frame(obj, level, fill_value=fill_value)
  File ""pandas/core/reshape/reshape.py"", line 435, in _unstack_frame
    unstacker = _Unstacker(obj.index, level=level)
  File ""pandas/core/reshape/reshape.py"", line 93, in __init__
    self.index = index.remove_unused_levels()
AttributeError: 'RangeIndex' object has no attribute 'remove_unused_levels'
```

"
637395052,34724,CI: window 37_141 jobs needs to update numpy version,jreback,closed,2020-06-11T23:58:43Z,2020-06-14T14:29:35Z,"https://github.com/pandas-dev/pandas/blob/master/ci/deps/azure-windows-37.yaml

we no longer support this, I guess it *still* works ok. But we should advance this job to be numpy 1.15* at least, i would say we should actually test numpy 1.18* here.

might also need to actually rename the job to reflect the update."
616688325,34136,Performance regression in replace.ReplaceList.time_replace_list_one_match,TomAugspurger,closed,2020-05-12T14:09:49Z,2020-06-14T14:34:35Z,"https://pandas.pydata.org/speed/pandas/index.html#replace.ReplaceList.time_replace_list_one_match?p-inplace=False&commits=6388370eb313244ba0e4cd8215dcdbfdabb7004b-88d5f1264bf58b518b7cce39230d018db59b3c72 points to https://github.com/pandas-dev/pandas/compare/6388370eb313244ba0e4cd8215dcdbfdabb7004b...88d5f1264bf58b518b7cce39230d018db59b3c72.


https://github.com/pandas-dev/pandas/pull/34048 mentions DataFrame.replace. cc @simonjayhawkins.


https://pandas.pydata.org/speed/pandas/index.html#replace.FillNa.time_replace?p-inplace=True&commits=6388370eb313244ba0e4cd8215dcdbfdabb7004b-88d5f1264bf58b518b7cce39230d018db59b3c72 is probably the same cause."
638005576,34737,PERF: avoid copy in replace,TomAugspurger,closed,2020-06-12T21:11:45Z,2020-06-14T14:34:41Z,"Closes #34136.

Hopefully this preserves the right behavior. I could imagine breaking something if a caller was relying on `putmask(., inplace=False)` returning a copy.

```
import pandas as pd
import numpy as np

df = pd.DataFrame({""A"": 0, ""B"": 0}, index=range(4 * 10 ** 7))

# the 1 can be held in self._df.blocks[0], while the inf and -inf cant
%timeit df.replace([np.inf, -np.inf, 1], np.nan, inplace=False)



# 1.0.3
483 ms ± 10.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

# master
900 ms ± 18.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

# PR
490 ms ± 8.64 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```"
623277600,34316,BUG: don't plot colorbar if c is column containing colors ,MarcoGorelli,closed,2020-05-22T15:11:32Z,2020-06-14T14:46:35Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import matplotlib as mpl
import numpy as np

import pandas as pd

CMAP = ""viridis""
df = pd.DataFrame(
    [[5.1, 3.5], [4.9, 3.0], [7.0, 3.2], [6.4, 3.2], [5.9, 3.0]],
    columns=[""length"", ""width""],
)
df[""species""] = ['r', 'r', 'g', 'g', 'b']
df.plot.scatter(
    x=0, y=1, c=""species"", cmap=CMAP,
)
```

#### Problem description

This produces
![image](https://user-images.githubusercontent.com/33491632/82682171-c087f200-9c46-11ea-8636-0c070ec21695.png)

The colorbar to the right of the plot doesn't make any sense here

#### Expected Output
Same plot, but without the colorbar

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 6ad157e937624854312934a3e2df8437083cf957
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-51-generic
Version          : #44~18.04.2-Ubuntu SMP Thu Apr 23 14:27:18 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.0.dev0+1623.g6ad157e93
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200325
Cython           : 0.29.16
pytest           : 5.4.1
hypothesis       : 5.8.0
sphinx           : 3.0.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : 1.3.2
fastparquet      : 0.3.3
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.16
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0

</details>
"
576967058,32494,Dataframe.groupby aggregations with categorical columns lead to incorrect results.,tv3141,closed,2020-03-06T14:28:25Z,2020-06-14T15:02:05Z,"#### Code Sample

```python
# In[2]:

import pandas as pd

def create_df():
    df = pd.DataFrame(
        {
            'major_id': [1, 2, 1, 2, 2],
            'minor_id': ['a', 'b', 'c', 'd', 'e'],
            'values': [1, 2, 3, 4, 5]
        }
    )
    return df

def groupby(df):
    df['max_value'] = (
        df
        .groupby(['major_id', 'minor_id'])
        ['values']
        .transform('max')
    )
    
    return df


# In[3]:

# correct result
df = create_df()
groupby(df)

# Out[3]
       ""   major_id minor_id  values  max_value\n"",
       ""0         1        a       1          1\n"",
       ""1         2        b       2          2\n"",
       ""2         1        c       3          3\n"",
       ""3         2        d       4          4\n"",
       ""4         2        e       5          5""

# In[4]:

# incorrect result: groupby with one non-categorical column and one categorical column
df = create_df()
df = df.astype({'minor_id': 'category'})
groupby(df)

# Out[4]

       ""   major_id minor_id  values  max_value\n"",
       ""0         1        a       1        1.0\n"",
       ""1         2        b       2        3.0\n"",
       ""2         1        c       3        NaN\n"",
       ""3         2        d       4        NaN\n"",
       ""4         2        e       5        NaN""
```
#### Problem description

`groupby` with one non-categorical column and one categorical column leads to incorrect aggregations (wrong values, or `NAN`s).

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.1
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.2
setuptools       : 41.0.1
Cython           : None
pytest           : 5.2.1
hypothesis       : 5.5.4
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.7.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.3.3
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.2.1
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.3.1
sqlalchemy       : 1.3.10
tables           : None
tabulate         : 0.8.5
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0
</details>
"
636575150,34707,PERF: is_date_array_normalized,jbrockmendel,closed,2020-06-10T21:57:37Z,2020-06-14T15:09:14Z,Same optimization we made in normalize_i8_timestamps the other day.
636634414,34709,BUG: DataFrame.unstack with non-consolidated,jbrockmendel,closed,2020-06-11T00:32:04Z,2020-06-14T15:10:18Z,"- [x] closes #34708
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
636400433,34701,CLN: remove libfrequencies.get_freq_group,jbrockmendel,closed,2020-06-10T16:47:53Z,2020-06-14T15:11:04Z,The next (hopefully last) step after this is to move Resolution to dtypes.pyx and de-duplicate it with FreqGroup.
636382977,34700,CLN: remove usages of base_and_stride,jbrockmendel,closed,2020-06-10T16:19:53Z,2020-06-14T15:12:19Z,
632654487,34623,TST/REF: arithmetic tests for BooleanArray + consolidate with integer masked tests,jorisvandenbossche,closed,2020-06-06T18:32:47Z,2020-06-14T15:18:13Z,"Follow-up on https://github.com/pandas-dev/pandas/pull/34454

- Move a bunch of tests from `integer/test_arithmetic.py` to `masked/test_arithmetic.py` so those can be shared with boolean (and once merged I can add float as well in the FloatingArray PR)
- Added some more arithmetic tests to `boolean/test_arithmetic.py` to follow the same pattern as we have in `integer/test_arithmetic.py`. 
- The commong integer/boolean tests uncovered a bug in the BooleanArray implementation when dealing with a `pd.NA` scalar as other operand."
598063292,33465,API: more permissive conversion to StringDtype,topper-123,closed,2020-04-10T19:50:25Z,2020-05-27T05:56:38Z,"- [x] closes #31204
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This is a proposal to make using ``StringDtype`` more permissive and be usable inplace of ``dtype=str``.

ATM converting to StringDtype will only accept arrays that are ``str`` already, meaning you will often have to use ``astype(str).astype(""string"")`` to be sure not to get errors, which can be tedious. For example these fail in master and work in this PR:

```python
>>> pd.Series([1,2, np.nan], dtype=""string"")
0       1
1       2
2    <NA>
dtype: string
>>> pd.array([1,2, np.nan], dtype=""string"")
<StringArray>
['1', '2', <NA>]
Length: 3, dtype: string
>>> pd.Series([1,2, np.nan]).astype(""string"")
0     1.0
1     2.0
2    <NA>
dtype: string
>>> pd.Series([1,2, np.nan], dtype=""Int64"").astype(""string"")
0       1
1       2
2    <NA>
dtype: string
```

etc. now work. Previously the above all gave errors.

## The proposed solution:
``ExtensionArray._from_sequence`` is in master explicitly stated to expect a sequence of scalars of the *correct* type. This makes it not usable for type conversion.

I've therefore added a new ``ExtensionArray._from_sequence_of_any_type`` that accepts scalars of any type and may change the scalars to the correct type before passing them on to ``_from_sequence``. Currently it just routes everything through to ``ExtensionArray._from_sequence``, except in ``StringArray._from_sequence_of_any_type``, where it massages the input scalars into strings before passing them on.

Obviously tests and doc updates are still missing, but I would appreciate feedback if this solution looks ok and I'll add tests and doc updates if this is ok."
603543030,33686,BLD: Set max version of numba to 0.48.0,mgmarino,closed,2020-04-20T21:20:36Z,2020-05-27T08:23:27Z,"This should fix the errors in the CI (Travis) builds, e.g.

```
E       AttributeError: module 'numba' has no attribute 'targets'
```"
599869172,33551,BUG: Fix behavior of isocalendar with timezones,mgmarino,closed,2020-04-14T21:25:14Z,2020-05-27T08:23:27Z,"- If timezone is not UTC, then convert to UTC
- This bug was found while deprecating 'week' and 'weekofyear'"
599100983,33533,CLN: Change isocalendar to be a method,mgmarino,closed,2020-04-13T19:58:13Z,2020-05-27T08:23:27Z,"For consistency with `Timestamp.isocalendar`, `Series.dt.isocalendar` and
`DatetimeIndex.isocalendar` should rather be methods and not attributes.

Followup of #33220, see the discussions following the merge of that PR"
617264096,34151,CI: Pin flake8 < 3.8.0,mgmarino,closed,2020-05-13T08:47:20Z,2020-05-27T08:23:30Z,"xref #34150

This handles multiple issues:
- flake8 >= 3.8.1 introduces a new version of pycodestyle which introduces
  extra checking
- flake8-rst is broken with the newest version of flake8
    - See kataev/flake8-rst#22

A follow-up PR handling the first of these is forth-coming"
616588298,34129,BUG: melt MultiIndex columns using index columns as identifier variables,ashtou,closed,2020-05-12T11:40:27Z,2020-05-27T12:31:24Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
df = pd.DataFrame(
    [['p', 'q', 'r'],
     ['s', 't', 'u'],
     ['v', 'w', 'x'],
    ],
    index=pd.MultiIndex.from_arrays(
        [list('123'), list('456')],
        names=['ind1', 'ind2']
        ),
    columns=pd.MultiIndex.from_arrays(
        [list('ABC'), list('DEF')])
)

# melt using level1 (or above in other cases) fails
df_l1 = df.reset_index(col_level=1)
print(""\nL1 index insert:\n"", df_l1)
# NOTE: THIS FAILS!
df_l1 = pd.melt(df_l1, col_level=1, 
    id_vars=['ind1'], value_vars=['D','E'])
print(""\nL1 melt:\n"", df_l1)
```

#### Problem description
Suppose that we have multi-index columns and we would like to `melt`, using the index as the `id_vars`: 
In this example, if we `reset_index(col_level=1)` and then `melt()` will fail as shown below:

```python

L1 index insert:
              A  B  C
  ind1 ind2  D  E  F
0    1    4  p  q  r
1    2    5  s  t  u
2    3    6  v  w  x

FAILS: KeyError: 'ind1'
```

<details>

```python-traceback
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/Repos/spec17/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2896             try:
-> 2897                 return self._engine.get_loc(key)
   2898             except KeyError:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'ind1'

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
 in 
     17 print(""\nL1 index insert:\n"", df_l1)
     18 # NOTE: THIS FAILS!
---> 19 df_l1 = pd.melt(df_l1, col_level=1, 
     20     id_vars=['ind1'], value_vars=['D','E'])
     21 print(""\nL1 melt:\n"", df_l1)

~/Repos/spec17/venv/lib/python3.8/site-packages/pandas/core/reshape/melt.py in melt(frame, id_vars, value_vars, var_name, value_name, col_level)
    102     mdata = {}
    103     for col in id_vars:
--> 104         id_data = frame.pop(col)
    105         if is_extension_type(id_data):
    106             id_data = concat([id_data] * K, ignore_index=True)

~/Repos/spec17/venv/lib/python3.8/site-packages/pandas/core/generic.py in pop(self, item)
    860         3  monkey        NaN
    861         """"""
--> 862         result = self[item]
    863         del self[item]
    864         try:

~/Repos/spec17/venv/lib/python3.8/site-packages/pandas/core/frame.py in __getitem__(self, key)
   2993             if self.columns.nlevels > 1:
   2994                 return self._getitem_multilevel(key)
-> 2995             indexer = self.columns.get_loc(key)
   2996             if is_integer(indexer):
   2997                 indexer = [indexer]

~/Repos/spec17/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2897                 return self._engine.get_loc(key)
   2898             except KeyError:
-> 2899                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2900         indexer = self.get_indexer([key], method=method, tolerance=tolerance)
   2901         if indexer.ndim > 1 or indexer.size > 1:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'ind1'

```

</details>

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.0.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-51-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.3
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.14.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
578683056,32579,[QST] Clarify when `agg()` will return groups as index,shwina,closed,2020-03-10T15:52:14Z,2020-05-27T12:43:34Z,"```
In [44]: a = pd.DataFrame({'a': [1, 1, 2], 'b': [1, 2, 3]})

In [45]: a.groupby('a').agg('sum') # 'a' is returned as index
Out[45]:
   b
a
1  3
2  3

In [46]: a.groupby('a').agg('nunique') # 'a' is returned both as index and column
Out[46]:
   a  b
a
1  1  2
2  1  1
```

The groupby [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#aggregation) notes:

> Aggregation functions will not return the groups that you are aggregating over if they are named columns, when as_index=True, the default. The grouped columns will be the indices of the returned object.

However, in the snippet above, it looks like the `nunique` aggregation behaves differently in this respect from the `sum` aggregation.

Is it possible to predict when an aggregation will return groups as index columns v/s a ""SQL-style output"" v/s a combination of the two (as above)?"
323726103,21090,Different return type when using groupby with nunique,RaulPL,closed,2018-05-16T17:59:26Z,2020-05-27T12:43:34Z,"#### Code Sample

I have the following code

```python
import pandas as pd
print(pd.__version__) # 0.22
df = pd.DataFrame(
    {'A': ['Jane', 'Jane', 'Charles', 'Charles'], 
     'B': ['red', 'blue', 'green', 'green']})

# here I would like to group by one of the columns (A in this case), and aggregate the other. 
# These two lines return a pandas DataFrame
df.groupby('A', as_index=False).agg({'B': pd.Series.count})  # pd.DataFrame
df.groupby('A', as_index=False).agg({'B': pd.Series.nunique})  # pd.DataFrame

# But when I do it in this way I don't know why I am getting a pandas Series in the last line
df.groupby('A', as_index=False).B.count()  # pd.DataFrame
df.groupby('A', as_index=False).B.nunique()  # pd.Series
```
#### Problem description

I am getting a pandas Series when trying to aggregate using ""col.nunique()"" notation with as_index set to False. Also, the pandas Series that is returned drops the values of the grouped column.

#### Expected Output
I think that the last line of code should return a pandas DataFrame in order to be consistent. 

I am happy to help with this issue if its possible, I am not an expert but I would like to contribute.

Thanks a lot, this is an awesome library =).



#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.5.final.0
python-bits: 64
OS: Linux
OS-release: 4.13.0-41-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.22.0
pytest: 3.5.1
pip: 10.0.1
setuptools: 39.1.0
Cython: 0.28.2
numpy: 1.14.2
scipy: 1.1.0
pyarrow: 0.9.0
xarray: 0.10.3
IPython: 6.4.0
sphinx: 1.7.4
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: 1.2.1
tables: 3.4.3
numexpr: 2.6.5
feather: 0.4.0
matplotlib: 2.2.2
openpyxl: 2.5.3
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.4
lxml: 4.2.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.7
pymysql: 0.8.0
psycopg2: None
jinja2: 2.10
s3fs: 0.1.4
fastparquet: 0.1.5
pandas_gbq: None
pandas_datareader: None

</details>
"
622612713,34297,BUG: Subsequent calls to df.sub() are much faster than the first call,philippegr,closed,2020-05-21T16:12:37Z,2020-05-27T13:11:12Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import numpy as np
import pandas as pd

# Building some general structure 
my_date_range = pd.date_range('20200101 00:00', '20200102 0:00', freq='S')
level_0_names = list(str(i) for i in range(30))
#level_0_names = list(range(30))
index = pd.MultiIndex.from_product([level_0_names, my_date_range])
column_names = ['col_1', 'col_2']

# Building a df that represents some value over time (think sensors)
# Indexed by sensor and time 
value_df = pd.DataFrame(np.random.rand(len(index),2), index=index, columns=column_names)

# Build a reference df for the reference value the sensor can take (like its max)
# Indexed by sensor
ref_df = pd.DataFrame(np.random.randint(1, 10, (len(level_0_names), 2)), 
                   index = level_0_names, 
                   columns=column_names)

# We now want to consider for each time index in value_df what is the deviation of the value observed wrt to the ref value 

# In a notebook, this first execution will be slow: 8-10s on my machine
# %%time 
value_df.sub(ref_df, level=0)

# This second execution will be fast: 100-150ms 
# %%time 
value_df.sub(ref_df, level=0)

# For reference, this is NOT the problem, the following lines would produce the same output
# On my machine it takes ~2s 
# %%time 
same_w_merge = pd.merge(left = value_df.reset_index(level=1), right = ref_df, right_index=True, left_index=True) 
same_w_merge['col_1_x'] -= same_w_merge['col_1_y']
same_w_merge['col_2_x'] -= same_w_merge['col_2_y']
same_w_merge = same_w_merge.drop(columns = ['col_1_y', 'col_2_y'])
same_w_merge = same_w_merge.rename({'col_1_x':'col_1', 'col_2_x': 'col_2'})
same_w_merge = same_w_merge.set_index('level_1', append=True).sort_index()
```

#### Problem description

There is a significant difference in speed between the first and second call to `sub` (which are the same instruction) in the code above. I don't understand where this is coming from. In particular why this is notably slower than merge (whose performance remains consistent).

Upon investigation, I noticed that the difference between runs is much smaller if value_df.index.level[0] is of type int (80ms for the first run 60ms for the subsequent) 

#### Expected Output
Current output is correct, speed of first call is the issue here 

#### Output of ``pd.show_versions()``

<details>
Bug reproduced here on a conda/ OS X install for simplicity but can confirm it exists as well in a Ubuntu Based Docker 

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_CA.UTF-8
LOCALE           : en_CA.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.4.0.post20200518
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
625003234,34389,CLN: _consolidate_inplace less,jbrockmendel,closed,2020-05-26T16:07:54Z,2020-05-27T14:10:39Z,
581880543,32740,Reindexing two tz-aware (UTC) indices gives 'DatetimeArray subtraction must have the same timezones or no timezones',andrewcooke,closed,2020-03-15T23:56:49Z,2020-05-27T14:33:46Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import datetime as dt

print(pd.__version__)
a = pd.date_range('2010-01-01', '2010-01-02', periods=24, tz='utc')
b = pd.date_range('2010-01-01', '2010-01-02', periods=23, tz='utc')
a.reindex(b, method='nearest', tolerance=dt.timedelta(seconds=20))
```
#### Problem description

Output is:
```
Python 3.7.4 (default, Jul 27 2019, 21:25:02) 
[GCC 7.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas as pd
>>> import datetime as dt
>>> 
>>> print(pd.__version__)
1.0.2
>>> a = pd.date_range('2010-01-01', '2010-01-02', periods=24, tz='utc')
>>> b = pd.date_range('2010-01-01', '2010-01-02', periods=23, tz='utc')
>>> a.reindex(b, method='nearest', tolerance=dt.timedelta(seconds=20))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/andrew/project/ch2/choochoo-1/py/env/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 3144, in reindex
    target, method=method, limit=limit, tolerance=tolerance
  File ""/home/andrew/project/ch2/choochoo-1/py/env/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 2740, in get_indexer
    indexer = self._get_nearest_indexer(target, limit, tolerance)
  File ""/home/andrew/project/ch2/choochoo-1/py/env/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 2830, in _get_nearest_indexer
    indexer = self._filter_indexer_tolerance(target, indexer, tolerance)
  File ""/home/andrew/project/ch2/choochoo-1/py/env/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 2834, in _filter_indexer_tolerance
    distance = abs(self.values[indexer] - target)
  File ""/home/andrew/project/ch2/choochoo-1/py/env/lib/python3.7/site-packages/pandas/core/indexes/extension.py"", line 147, in method
    result = meth(_maybe_unwrap_index(other))
  File ""/home/andrew/project/ch2/choochoo-1/py/env/lib/python3.7/site-packages/pandas/core/arrays/datetimelike.py"", line 1458, in __rsub__
    return -(self - other)
  File ""/home/andrew/project/ch2/choochoo-1/py/env/lib/python3.7/site-packages/pandas/core/ops/common.py"", line 64, in new_method
    return method(self, other)
  File ""/home/andrew/project/ch2/choochoo-1/py/env/lib/python3.7/site-packages/pandas/core/arrays/datetimelike.py"", line 1406, in __sub__
    result = self._sub_datetime_arraylike(other)
  File ""/home/andrew/project/ch2/choochoo-1/py/env/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py"", line 667, in _sub_datetime_arraylike
    f""{type(self).__name__} subtraction must have the same ""
TypeError: DatetimeArray subtraction must have the same timezones or no timezones
```

The problem is that I don't see how to reindex and / or the error message appears to be nonsensical since both have UTC timezones.

#### Expected Output

With 0.25.3 I see:
```
(DatetimeIndex([          '2010-01-01 00:00:00+00:00',
               '2010-01-01 01:05:27.272727272+00:00',
               '2010-01-01 02:10:54.545454545+00:00',
               '2010-01-01 03:16:21.818181818+00:00',
               '2010-01-01 04:21:49.090909090+00:00',
               '2010-01-01 05:27:16.363636363+00:00',
               '2010-01-01 06:32:43.636363636+00:00',
               '2010-01-01 07:38:10.909090909+00:00',
               '2010-01-01 08:43:38.181818181+00:00',
               '2010-01-01 09:49:05.454545454+00:00',
               '2010-01-01 10:54:32.727272727+00:00',
                         '2010-01-01 12:00:00+00:00',
               '2010-01-01 13:05:27.272727272+00:00',
               '2010-01-01 14:10:54.545454545+00:00',
               '2010-01-01 15:16:21.818181818+00:00',
               '2010-01-01 16:21:49.090909090+00:00',
               '2010-01-01 17:27:16.363636363+00:00',
               '2010-01-01 18:32:43.636363636+00:00',
               '2010-01-01 19:38:10.909090909+00:00',
               '2010-01-01 20:43:38.181818181+00:00',
               '2010-01-01 21:49:05.454545454+00:00',
               '2010-01-01 22:54:32.727272727+00:00',
                         '2010-01-02 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', freq=None), array([ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
       -1, -1, -1, -1, -1, 23]))
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.12.14-lp151.28.36-default
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.2
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.0
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
623872084,34354,34297 sub slow in first call,phofl,closed,2020-05-24T14:09:46Z,2020-05-27T16:30:00Z,"- [x] closes #34297
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

@jbrockmendel 

I added a lenght check as suggested by you. I could not find another method to check the indices (nlevels for example does not work). I'm open to recommendations about adding additional checks before running into the values call.

I have added a test, which measures the execution time of both calls to `sub`. Is there a better way to test, if they are equally fast?"
623641900,34335,BUG: Fix render as column name in to_excel,phofl,closed,2020-05-23T11:44:13Z,2020-05-27T16:30:02Z,"- [x] closes #34331
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Should I add a whats new enty? Under which section?

https://github.com/phofl/pandas/blob/34331_render_column_name_raises_error_in_to_excel/pandas/io/formats/excel.py#L389

The previous code checked if the `df` had a attribute `render`. This check was True, if there was a `DataFrame` given as input with a column `render`. I think that this check should only be True if a `Styler` object is given, so I added an `isinstace` check for `Styler`. 

Does the `Styler` object always an attribute `render`? If not, we would run into problems, if we get a `Styler` object with a column named `render` but without the `render` attribute. If a `Styler` object has always an attribute `render`, we could delete the `hasattr` check.

Edit: I changed the check to `not isinstance(df, DataFrame)` to avoid the jinja issues."
623658216,34340,"DOC: Fix reference, old file was separated last year",phofl,closed,2020-05-23T13:13:23Z,2020-05-27T16:30:04Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

The comment referenced a file deleted last year after separating the tests into a few files. I referenced the correct folder again"
625803626,34409,"Initialize variables in pqyear, pquarter",TomAugspurger,closed,2020-05-27T15:38:05Z,2020-05-27T16:30:08Z,"This resolves the `-Wmaybe-uninitialized` warning observed when building with the manylinux1 docker image

```
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -DNPY_NO_DEPRECATED_API=0 -Ipandas/_libs/tslibs -I./pandas/_libs/tslibs -I/opt/python/cp38-cp38/lib/python3.8/site-packages/numpy/core/include -I/opt/python/cp38-cp38/include/python3.8 -c pandas/_libs/tslibs/period.c -o build/temp.linux-x86_64-3.8/pandas/_libs/tslibs/period.o
pandas/_libs/tslibs/period.c: In function ‘__pyx_f_6pandas_5_libs_6tslibs_6period_pqyear’:
pandas/_libs/tslibs/period.c:12464:3: warning: ‘__pyx_v_year’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   return __pyx_r;
   ^
pandas/_libs/tslibs/period.c: In function ‘__pyx_f_6pandas_5_libs_6tslibs_6period_pquarter’:
pandas/_libs/tslibs/period.c:12512:3: warning: ‘__pyx_v_quarter’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   return __pyx_r;
```

Hopefully this doesn't break anything. IIUC, these pointers are just used to set the result, so it shouldn't matter what the value is going in? 

Closes https://github.com/pandas-dev/pandas/issues/34114
"
625329495,34400,CLN: de-duplicate paths in tslibs,jbrockmendel,closed,2020-05-27T02:34:59Z,2020-05-27T17:32:17Z,
625291984,34398,REF: move remaining offsets into liboffsets,jbrockmendel,closed,2020-05-27T00:51:00Z,2020-05-27T17:32:54Z,this _doesnt_ make them all cdef classes yet; need to get #34345 figured out before that is doable.
625228324,34396,DOC: added D-Tale to the visualizations section of Ecosystem page,aschonfeld,closed,2020-05-26T22:02:51Z,2020-05-27T18:34:13Z,"- [x] closes #34376

Added [D-Tale](https://github.com/man-group/dtale) to the visualizations section of [Pandas Ecosystem](https://pandas.pydata.org/docs/ecosystem.html) doc
"
623750538,34348,BUG: Clipboard tests failing in Dockerfile,matteosantama,closed,2020-05-23T22:04:09Z,2020-05-27T19:26:09Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
pytest pandas/tests/io/test_clipboard.py 
```

#### Problem description
Last three clipboard tests are failing in a Docker image running on Mac. Dug into the pyperclip code and the issue appears to be that the Docker container does not have a DISPLAY environment variable set correctly. Pyperclip code does not recognize my machine as a Mac, so it tries to proceed as if its a Linux.

I've tried manually setting it, and the tests run through completion but the assertions fail and I get `Could not open display`. Does anyone have an idea how to fix this? Or is Dockerfile not supported for Macs? "
625673591,34405,DOC: add link to benchmarks page in Development docs,jreback,closed,2020-05-27T12:55:18Z,2020-05-27T23:12:06Z,"link is: https://pandas.pydata.org/speed/pandas/#/
also could add somewhere: https://github.com/pandas-dev/pandas/blob/master/README.md"
625800290,34408,DOC: add link to benchmarks page in developer docs,SwordKnight6216,closed,2020-05-27T15:33:41Z,2020-05-27T23:12:11Z,"- [x] closes #34405
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew  

@jreback 

"
619514093,34212,BUG: Executing SQL containing non-escaped percent sign fails without parameters,john-bodley,closed,2020-05-16T16:52:47Z,2020-05-28T02:10:15Z,"- [x] closes https://github.com/pandas-dev/pandas/issues/34211
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
626337305,34424,DOC: intersphinx inventory link for statsmodels,simonjayhawkins,closed,2020-05-28T08:53:27Z,2020-05-28T09:41:45Z,"intersphinx inventory has moved: http://www.statsmodels.org/devel/objects.inv -> https://www.statsmodels.org/devel/objects.inv

EDIT: this is already change on master. this PR directly against 1.0.x"
626402909,34425,DOC: 1.0.4 release notes and date,simonjayhawkins,closed,2020-05-28T10:35:29Z,2020-05-28T11:38:23Z,
626440270,34428,DOC: 1.0.4 whatsnew,TomAugspurger,closed,2020-05-28T11:38:59Z,2020-05-28T12:55:01Z,"Brings 1.0.4.rst to master & sets the release date (for tomorrow)

cc @simonjayhawkins."
623854530,34353,ENH: should users be able to adjust subplot parameters?,MarcoGorelli,closed,2020-05-24T12:26:22Z,2020-05-28T15:38:20Z,"Noticed this while reviewing #34343

Currently, the example in the doc would look like this:
![image](https://user-images.githubusercontent.com/33491632/82753854-0799f280-9dc1-11ea-9675-7716dd75162f.png)

IMO there's insufficient separation between the subplots. If we change [this line](https://github.com/pandas-dev/pandas/blob/cb35d8a938c9222d903482d2f66c62fece5a7aae/pandas/plotting/_matplotlib/boxplot.py#L427) to
```python
        fig.subplots_adjust(bottom=0.15, top=0.9, left=0.1, right=0.9, wspace=0.2, hspace=.5)
```
then this example would look like this
![image](https://user-images.githubusercontent.com/33491632/82754046-44b2b480-9dc2-11ea-97ad-a13b58d0d218.png)


It seems these subplot adjustments aren't adjustable by the user.

So, should the user be allowed to pass in these parameters (e.g. `hspace`), or should the fix here be to just hardcode `hspace=.5` into the line mentioned above, and if users want extra customisability they can use matplotlib directly?"
589756759,33114,"concatenating frame and series with identical keys returns "" int() argument must be a string""",MarcoGorelli,closed,2020-03-29T09:53:51Z,2020-05-28T15:39:59Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import pandas as pd
>>> s = pd.Series([1, 2])                                                                                                                                                                                        
>>> df = pd.DataFrame([[1, 2], [3, 4]])                                                                                                                                                                          
>>> pd.concat([df, s], axis=1, keys=['a', 'a']) 
```
#### Problem description

This returns

<details>

<summary>Traceback</summary>

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-4-262b5243c6cf> in <module>
----> 1 pd.concat([df, s], axis=1, keys=['a', 'a'])

~/pandas/pandas/core/reshape/concat.py in concat(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    278         verify_integrity=verify_integrity,
    279         copy=copy,
--> 280         sort=sort,
    281     )
    282 

~/pandas/pandas/core/reshape/concat.py in __init__(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)
    447         self.copy = copy
    448 
--> 449         self.new_axes = self._get_new_axes()
    450 
    451     def get_result(self):

~/pandas/pandas/core/reshape/concat.py in _get_new_axes(self)
    510         return [
    511             self._get_concat_axis() if i == self.axis else self._get_comb_axis(i)
--> 512             for i in range(ndim)
    513         ]
    514 

~/pandas/pandas/core/reshape/concat.py in <listcomp>(.0)
    510         return [
    511             self._get_concat_axis() if i == self.axis else self._get_comb_axis(i)
--> 512             for i in range(ndim)
    513         ]
    514 

~/pandas/pandas/core/reshape/concat.py in _get_concat_axis(self)
    566         else:
    567             concat_axis = _make_concat_multiindex(
--> 568                 indexes, self.keys, self.levels, self.names
    569             )
    570 

~/pandas/pandas/core/reshape/concat.py in _make_concat_multiindex(indexes, keys, levels, names)
    648 
    649         return MultiIndex(
--> 650             levels=levels, codes=codes_list, names=names, verify_integrity=False
    651         )
    652 

~/pandas/pandas/core/indexes/multi.py in __new__(cls, levels, codes, sortorder, names, dtype, copy, name, verify_integrity, _set_identity)
    281         # we've already validated levels and codes, so shortcut here
    282         result._set_levels(levels, copy=copy, validate=False)
--> 283         result._set_codes(codes, copy=copy, validate=False)
    284 
    285         result._names = [None] * len(levels)

~/pandas/pandas/core/indexes/multi.py in _set_codes(self, codes, level, copy, validate, verify_integrity)
    864             new_codes = FrozenList(
    865                 _coerce_indexer_frozen(level_codes, lev, copy=copy).view()
--> 866                 for lev, level_codes in zip(self._levels, codes)
    867             )
    868         else:

~/pandas/pandas/core/indexes/multi.py in <genexpr>(.0)
    864             new_codes = FrozenList(
    865                 _coerce_indexer_frozen(level_codes, lev, copy=copy).view()
--> 866                 for lev, level_codes in zip(self._levels, codes)
    867             )
    868         else:

~/pandas/pandas/core/indexes/multi.py in _coerce_indexer_frozen(array_like, categories, copy)
   3597         Non-writeable.
   3598     """"""
-> 3599     array_like = coerce_indexer_dtype(array_like, categories)
   3600     if copy:
   3601         array_like = array_like.copy()

~/pandas/pandas/core/dtypes/cast.py in coerce_indexer_dtype(indexer, categories)
    856     length = len(categories)
    857     if length < _int8_max:
--> 858         return ensure_int8(indexer)
    859     elif length < _int16_max:
    860         return ensure_int16(indexer)

~/pandas/pandas/_libs/algos_common_helper.pxi in pandas._libs.algos.ensure_int8()
     59             return arr
     60         else:
---> 61             return arr.astype(np.int8, copy=copy)
     62     else:
     63         return np.array(arr, dtype=np.int8)
```

</details>

```
TypeError: int() argument must be a string, a bytes-like object or a number, not 'slice'
```

#### Expected Output

I would either expect this to work, or for it to return
```
InvalidIndexError: Reindexing only valid with uniquely valued Index objects
```
, which is what happens when you do
```
pd.concat([df, df], axis=1, keys=['a', 'a'])
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 36d6583cf51ff6de5b6d82a63391561056a696e6
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-91-generic
Version          : #92-Ubuntu SMP Fri Feb 28 11:09:48 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.26.0.dev0+2746.g36d6583cf
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200119
Cython           : 0.29.16
pytest           : 5.4.1
hypothesis       : 5.8.0
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : 0.3.3
gcsfs            : None
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.0
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0

</details>
"
623716585,34345,REF: make CustomMixin a cdef class,jbrockmendel,closed,2020-05-23T18:31:29Z,2020-05-28T15:53:12Z,
625018957,34390,BUG: Result of rolling mean depends on more observations than are in window,cpaulik,closed,2020-05-26T16:31:34Z,2020-05-28T16:39:15Z,"#### Code Sample, a copy-pastable example

```python
import numpy as np
import pandas as pd
import datetime as dt

data = np.array([0.1552, 0.1746, 0.1932, 0.234 , np.nan, 0.2423, 0.1648,
                    np.nan, 0.2148, 0.2081, 0.2313, 0.2011, np.nan, 0.2076,
                    0.2096, np.nan, 0.1801, 0.1872, 0.1878, 0.1949, np.nan,
                    0.1608, np.nan, np.nan, 0.1793, np.nan, 0.1689, 0.1631,
                    np.nan, 0.1586, 0.1531, np.nan, 0.149 , 0.1434, np.nan,
                    0.1526, np.nan, 0.1293, 0.1268, np.nan])
dates_pandas = pd.date_range(start=dt.date(2020, 3, 24), periods=data.shape[0])
ser = pd.Series(data, index=dates_pandas)
full_mean = ser.rolling(window=""20D"",
                        min_periods=2,
                        center=False,
                        closed='both').mean()
shorter_mean = ser['2020-03-25':].rolling(window=""20D"",
                                            min_periods=2,
                                            center=False,
                                            closed='both').mean()
print(full_mean['2020-05-02'], full_mean['2020-05-02'].round(4))
print(shorter_mean['2020-05-02'], shorter_mean['2020-05-02'].round(4))
```

##### Output

```
0.15664999999999998 0.1566
0.15665000000000004 0.1567
```

#### Problem description

The rolling mean in the example should only take the last 20 values into account. The output for the last day `2020-05-02` does however depend on the inclusion of a value on `2020-03-24`.

This is especially visible if we round the output to the 4 digit precision that the input data has.

#### Expected Output

Both running means should be the same value.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.6.12-arch1-1
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.6.0.post20191101
Cython           : 0.29.13
pytest           : 5.3.0
hypothesis       : None
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.0
pyxlsb           : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : 1.3.17
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
None


</details>
"
541832622,30429,Suggestion: add feature to show in detail changes in 1 df over time,christina-zhou-96,closed,2019-12-23T17:01:06Z,2020-05-28T17:08:46Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np
from tabulate import tabulate

def print_table(df, doc=False):
    """"""
    Print out a nice looking table.
    
    Set doc to True if you are printing this out for a Microsoft Word (11 x 8) document.

    When doc is False:
    
    Input:
       A  B
    0  1  0
    1  2  0
    2  3  0

    Output:
    +----+-----+-----+
    |    |   A |   B |
    |----+-----+-----|
    |  0 |   1 |   0 |
    |  1 |   2 |   0 |
    |  2 |   3 |   0 |
    +----+-----+-----+
    
    When doc is True:
    Input:
         a   b   c   d   e   f   g   h   i   j   k   l   m   n   p
    0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    1    1   1   1   1   1   1   1   1   1   1   1   1   1   1   1
    2    2   2   2   2   2   2   2   2   2   2   2   2   2   2   2
    3    3   3   3   3   3   3   3   3   3   3   3   3   3   3   3
    4    4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
    5    5   5   5   5   5   5   5   5   5   5   5   5   5   5   5
    6    6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
    7    7   7   7   7   7   7   7   7   7   7   7   7   7   7   7
    8    8   8   8   8   8   8   8   8   8   8   8   8   8   8   8
    9    9   9   9   9   9   9   9   9   9   9   9   9   9   9   9
    10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10
    11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11
    12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12
    13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13
    14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14

    Output:
    
    Part 1/3
    +----+-----+-----+-----+-----+-----+-----+
    |    |   a |   b |   c |   d |   e |   f |
    |----+-----+-----+-----+-----+-----+-----|
    |  0 |   0 |   0 |   0 |   0 |   0 |   0 |
    |  1 |   1 |   1 |   1 |   1 |   1 |   1 |
    |  2 |   2 |   2 |   2 |   2 |   2 |   2 |
    |  3 |   3 |   3 |   3 |   3 |   3 |   3 |
    |  4 |   4 |   4 |   4 |   4 |   4 |   4 |
    +----+-----+-----+-----+-----+-----+-----+
    Part 2/3
    +----+-----+-----+-----+-----+-----+-----+
    |    |   g |   h |   i |   j |   k |   l |
    |----+-----+-----+-----+-----+-----+-----|
    |  0 |   0 |   0 |   0 |   0 |   0 |   0 |
    |  1 |   1 |   1 |   1 |   1 |   1 |   1 |
    |  2 |   2 |   2 |   2 |   2 |   2 |   2 |
    |  3 |   3 |   3 |   3 |   3 |   3 |   3 |
    |  4 |   4 |   4 |   4 |   4 |   4 |   4 |
    +----+-----+-----+-----+-----+-----+-----+
    Part 3/3
    +----+-----+-----+-----+
    |    |   m |   n |   p |
    |----+-----+-----+-----|
    |  0 |   0 |   0 |   0 |
    |  1 |   1 |   1 |   1 |
    |  2 |   2 |   2 |   2 |
    |  3 |   3 |   3 |   3 |
    |  4 |   4 |   4 |   4 |
    +----+-----+-----+-----+

    :param df: DataFrame
    :param doc: bool
    :return: None
    """"""
    def _simple_print(df):
        """"""
        Single line helper function to print a dataframe.
        :param df: DataFrame
        :return: None
        """"""
        print(tabulate(df, headers='keys', tablefmt='psql'))

    # Just print with no extra slicing.
    if not doc:
        _simple_print(df)

    # Slice the dataframe to show to Word.
    else:
        # Set the number of columns Word can handle. It typically can handle 6 columns.
        max_columns = 6
        # Find how many tables we will need to print if we cut the columns by 6ths.
        max_tables = int(np.ceil(df.shape[1] / max_columns))
        # For each 6th, print the table.
        for iteration in range(1, max_tables + 1):
            # Print which iteration we're on.
            print(f'Part {iteration}/{max_tables}')
            # Slice the larger dataframe to obtain the current dataframe.
            current_df = df.iloc[0:5, (iteration * max_columns) - max_columns:iteration * max_columns]
            # Print current dataframe.
            _simple_print(current_df)

def test(old_df, new_df, desired_columns, index=None, verbose=False, print_full=True):
    """"""
    Description
    -----------
    Check for differences (across time, business rule changes, etc.) over the same
    dataframe.

    Recommended to start without verbose, then add it if needed.

    You can toggle whether to print out the sample dataframes (head and tail),
    or not.


    Sample Usage
    ------------
    old_data = {'ID': ['CHA','COC','COF'],
            'Name': ['Chai Tea', 'Cocoa', 'Coffee'],
            'Description': ['Chai Kcup','Hot Chocolate Kcup','Coffee Kcup'],
            'Cost': [2,2,3]}

    new_data = {'ID': ['CHA','COC','COF'],
                'Name': ['Chai Tea', 'Cocoa', 'Coffee'],
                'Description': ['Chai','Hot Chocolate Kcup','Coffee Kcup'],
                'Cost': [2,3,3]}

    old_df = (pd.DataFrame(old_data)
              .set_index('ID'))
    new_df = (pd.DataFrame(new_data)
              .set_index('ID'))

  test(old_df=old_df,
                   new_df=new_df,
                   desired_columns=old_df.columns)


    Output In Console
    -----------------
    Match? False

    Quick Head Test
    Old Records:
    +------+----------+--------------------+--------+
    | ID   | Name     | Description        |   Cost |
    |------+----------+--------------------+--------|
    | CHA  | Chai Tea | Chai Kcup          |      2 |
    | COC  | Cocoa    | Hot Chocolate Kcup |      2 |
    | COF  | Coffee   | Coffee Kcup        |      3 |
    +------+----------+--------------------+--------+
    New Records:
    +------+----------+--------------------+--------+
    | ID   | Name     | Description        |   Cost |
    |------+----------+--------------------+--------|
    | CHA  | Chai Tea | Chai               |      2 |
    | COC  | Cocoa    | Hot Chocolate Kcup |      3 |
    | COF  | Coffee   | Coffee Kcup        |      3 |
    +------+----------+--------------------+--------+
    Quick Tail Test
    Old Records:
    +------+----------+--------------------+--------+
    | ID   | Name     | Description        |   Cost |
    |------+----------+--------------------+--------|
    | CHA  | Chai Tea | Chai Kcup          |      2 |
    | COC  | Cocoa    | Hot Chocolate Kcup |      2 |
    | COF  | Coffee   | Coffee Kcup        |      3 |
    +------+----------+--------------------+--------+
    New Records:
    +------+----------+--------------------+--------+
    | ID   | Name     | Description        |   Cost |
    |------+----------+--------------------+--------|
    | CHA  | Chai Tea | Chai               |      2 |
    | COC  | Cocoa    | Hot Chocolate Kcup |      3 |
    | COF  | Coffee   | Coffee Kcup        |      3 |
    +------+----------+--------------------+--------+


    Quick Cell to Cell Differences
    All differences: (2, 2)
                           Old   New
    ID
    CHA Description  Chai Kcup  Chai
    COC Cost                 2     3


    Parameters
    ----------
    :param old_df: DataFrame
        This could be cases you know to be correct, or cases from last week, etc.

    :param new_df: DataFrame
        This could be the current output created by code you wish to debug or double
        check, or cases from today, etc.

    :param desired_columns: list of strings
        <old_df.columns>

    :param index: string
        name of column to match cases against (currently not implemented)
        <Student ID>

    :return: None
    """"""
    new_df = new_df[desired_columns]
    old_df = old_df[desired_columns]

    # overall test (returns true or false)
    print(f'Match? {old_df.equals(new_df)}')
    print('\n')

    if print_full:
        # quick head and tail test
        print('Quick Head Test')
        print('Old Records:')
        print_table(old_df.head(5))

        print('New Records:')
        print_table(new_df.head(5))
        print('\n')

        print('Quick Tail Test')
        print('Old Records:')
        print_table(old_df.tail(5))

        print('New Records:')
        print_table(new_df.head(5))
        print('\n')

    def _diff_table(old_df, new_df):
        """"""
        Create the table that only shows differences (or ""errors"").


        This is code that manesioz on stackoverflow wrote as a response to my question
        for how to achieve this.

        :param old_df: pandas DataFrame
        :param new_df: pandas DataFrame
        :return: pandas DataFrame
        """"""
        # create frame of comparison bools
        bool_df = (old_df != new_df).stack()
        diff_table = pd.concat([old_df.stack()[bool_df],
                                new_df.stack()[bool_df]],
                               axis=1)
        diff_table.columns = [""Old"", ""New""]
        return diff_table

    diff_table = _diff_table(old_df,new_df)
    
    if verbose==False:
        # specific cell by cell test for differences
        print('Quick Cell to Cell Differences')
        print(f'All differences: {diff_table.shape}')
        print(diff_table.head())
        print('\n')

    # Offer a verbose option. Because the following code can produce errors if the
    # dataframes are too different (eg. if the column names differ), we offer the
    # non-verbose option.
    else:
        print('Cell to Cell Differences: All')
        print(f'All differences: {diff_table.shape}')
        print(diff_table)
        print('\n')
```
#### Problem description

Often I need to do a quick analysis of the same dataframe over some time. I need to look at the specific cells that changed, and how. Frequently, it's at a volume where it's impossible to just eyeball this. I'm surprised that under pandas' testing suite there's not currently an in-built method to do this.

I ended up using manesioz's code from my stackoverflow question [here](https://stackoverflow.com/questions/57224294/more-specific-pandas-testing-for-equality-between-two-dataframes) in my own utility function which is included here. But, I think a feature like this (specifically the output under Quick Cell to Cell Differences) could be really useful for other data analysts too.

More sample output of other times I've used this feature:

- Ad hoc, timed request to find any differences in ordering exams over a time period of a month from schools
```
>>>last_week_df.columns
Index(['BCO', 'Superintendent', 'DBN', 'OrderedExams', 'FLOrderedExams',
       'Algebra I', 'ELA', 'Chemistry', 'Earth Science', 'Global Transition',
       'Global II', 'Algebra II', 'Living Environment', 'US History',
       'Physics', 'Geometry', 'All Exams'],
      dtype='object')
>>>last_week_df.shape
(616, 17)


Cell to Cell Differences: All
All differences: (66, 2)
                        Old  New
36  FLOrderedExams      Yes   No
40  FLOrderedExams      Yes   No
50  FLOrderedExams      Yes   No
66  FLOrderedExams      Yes   No
    ELA                  60   10
    All Exams           110   60
81  OrderedExams         No  Yes
    Algebra I             0   30
    Living Environment    0   15
    Geometry              0   25
    All Exams             0   70
90  FLOrderedExams      Yes   No
165 OrderedExams        Yes   No
    Algebra I            50    0
    ELA                 170    0
    Earth Science        25    0
    Global II            25    0
    Algebra II           25    0
    Living Environment   25    0
    Geometry             15    0
    All Exams           335    0
200 Physics              35   34
    All Exams          1010 1009
282 Living Environment  250  150
    All Exams           750  650
286 ELA                 615  440
    All Exams          1640 1465
432 Algebra I           695  620
    All Exams          1841 1766
540 OrderedExams         No  Yes
    Algebra I             0  130
    ELA                   0   50
    Global Transition     0   30
    Living Environment    0  140
    US History            0   40
    All Exams             0  390
552 OrderedExams         No  Yes
    Algebra I             0   15
    ELA                   0   25
    Chemistry             0    5
    Earth Science         0   15
    Global Transition     0   20
    Algebra II            0   20
    Living Environment    0   10
    US History            0   10
    Geometry              0   10
    All Exams             0  130
560 OrderedExams         No  Yes
    Algebra I             0  200
    Global Transition     0  100
    All Exams             0  300
561 OrderedExams         No  Yes
    Algebra I             0   12
    Living Environment    0   25
    All Exams             0   37
576 OrderedExams         No  Yes
    FLOrderedExams       No  Yes
    Algebra I             0   30
    ELA                   0  140
    All Exams             0  170
606 OrderedExams         No  Yes
    FLOrderedExams       No  Yes
    Earth Science         0   15
    Living Environment    0   15
    Geometry              0   25
    All Exams             0   55
```


- Ad hoc, timed request to update a data file dependency for another file on locations of foreign language personnel at sites
```
MATHI
Match? False


Cell to Cell Differences: All
All differences: (5, 2)
                     Old  New
Exams go to?                 
14J558       MATHI_K   10    2
             MATHI_R    1    2
20J445       MATHI_K    1    3
28W440       MATHI_K    1    4
29W283       MATHI_K    1    4


USH
Match? False


Cell to Cell Differences: All
All differences: (3, 2)
                     Old  New
Exams go to?                 
02N600       USH_K    1    0
             USH_R    0    1
13J499       USH_K    2    1


PHYS
Match? False


Cell to Cell Differences: All
All differences: (8, 2)
                     Old  New
Exams go to?                 
02N475       PHYS_K    1    0
             PHYS_R    0    1
15J519       PHYS_K    4    2
             PHYS_R    1    2
28W690       PHYS_K    1    3
30W445       PHYS_K    1    2
31T450       PHYS_K    1    0
             PHYS_R    0    1


GLOBALI
Match? False


Cell to Cell Differences: All
All differences: (6, 2)
                     Old  New
Exams go to?                 
19J660       GLOBALI_K    3    1
             GLOBALI_R    1    2
21J540       GLOBALI_K    3    2
             GLOBALI_R    1    2
29W272       GLOBALI_K    1    3
             GLOBALI_R    2    1


LIVING
Match? True


Cell to Cell Differences: All
All differences: (0, 2)
Empty DataFrame
Columns: [Old, New]
Index: []


GLOBALII
Match? False


Cell to Cell Differences: All
All differences: (7, 2)
                     Old  New
Exams go to?                 
13J499       GLOBALII_K    6    2
             GLOBALII_R    1    2
19J660       GLOBALII_K    4    2
21J540       GLOBALII_K    2    3
28W620       GLOBALII_K    2    4
             GLOBALII_R    2    1
29W272       GLOBALII_K    2    4
```
"
547576898,30852,ENH: Added DataFrame.compare and Series.compare (GH30429),fujiaxiang,closed,2020-01-09T16:14:08Z,2020-05-28T17:08:53Z,"Added `DataFrame.differences` and `Series.differences` methods. 

Have not yet added whatsnew entries. Will do so after review of API and behavior design.

A few design considerations open for discussion (among other things):
1. Index/column names: (self, other) vs (left, right) vs (old, new)
I used the (self, other) pair as I think they naturally match with the parameter names and the concept of comparing `self` with `other`. Although (left, right) or (old, new) may be slight more intuitive for users in some use cases, these new functions not necessarily compare two objects that are left and right (may be stacked on `axis=0`, or old and new.

1. Parameter name: axis
I feel axis may not be a very clear name, since we are stacking the output along this axis, rather than comparing objects along this axis. I considered using the name `stack_axis` or `orient` like `DataFrame.to_json` does.

1. I made the methods such that two NaNs are considered equal, hence not different. I think this should be the desired behavior since we are trying to compare two objects. However, under current implementation, a `None` will be considered equal to `np.nan` too. This is probably something we need to be careful for.

Let me know what you guys think. I'm ready to be scrutinized and criticized!

- [x] closes #30429
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
506884323,28980,Certain comparison operations misbehaving for period dtype,dsaxton,closed,2019-10-14T21:30:42Z,2020-05-28T17:17:44Z,"This is closely related to https://github.com/pandas-dev/pandas/issues/28930.

```python
import pandas as pd

s = pd.Series([pd.Period(""2019""), pd.Period(""2020"")], dtype=""period[A-DEC]"") 
s == ""a""                                                                                                                                                     
```

yields

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-4-3a654234a428> in <module>
----> 1 s == ""a""

~/pandas/pandas/core/ops/__init__.py in wrapper(self, other)
    527         rvalues = extract_array(other, extract_numpy=True)
    528 
--> 529         res_values = comparison_op(lvalues, rvalues, op)
    530 
    531         return _construct_result(self, res_values, index=self.index, name=res_name)

~/pandas/pandas/core/ops/array_ops.py in comparison_op(left, right, op)
    253 
    254     if should_extension_dispatch(lvalues, rvalues):
--> 255         res_values = dispatch_to_extension_op(op, lvalues, rvalues)
    256 
    257     elif is_scalar(rvalues) and isna(rvalues):

~/pandas/pandas/core/ops/dispatch.py in dispatch_to_extension_op(op, left, right, keep_null_freq)
    134 
    135     try:
--> 136         res_values = op(left, right)
    137     except NullFrequencyError:
    138         # DatetimeIndex and TimedeltaIndex with freq == None raise ValueError

~/pandas/pandas/core/arrays/period.py in wrapper(self, other)
     98             result.fill(nat_result)
     99         else:
--> 100             other = Period(other, freq=self.freq)
    101             result = ordinal_op(other.ordinal)
    102 

~/pandas/pandas/_libs/tslibs/period.pyx in pandas._libs.tslibs.period.Period.__new__()
   2459                 value = str(value)
   2460             value = value.upper()
-> 2461             dt, _, reso = parse_time_string(value, freq)
   2462             if dt is NaT:
   2463                 ordinal = NPY_NAT

~/pandas/pandas/_libs/tslibs/parsing.pyx in pandas._libs.tslibs.parsing.parse_time_string()
    265             yearfirst = get_option(""display.date_yearfirst"")
    266 
--> 267     res = parse_datetime_string_with_reso(arg, freq=freq,
    268                                           dayfirst=dayfirst,
    269                                           yearfirst=yearfirst)

~/pandas/pandas/_libs/tslibs/parsing.pyx in pandas._libs.tslibs.parsing.parse_datetime_string_with_reso()
    291 
    292     if not _does_string_look_like_datetime(date_string):
--> 293         raise ValueError('Given date string not likely a datetime.')
    294 
    295     parsed, reso = _parse_delimited_date(date_string, dayfirst)

ValueError: Given date string not likely a datetime.
```

but the output should be `Series([False, False])`.  The other comparison operators `<`, `<=`, `>`, and `>=` yield the same error, which also seems incorrect (the error message should probably be different, and the error should be a `TypeError` rather than a `ValueError`).  This is happening on 0.25.1 and master."
624427084,34367,REGR: frame/frame op with unaligned blocks + non-slice-like placement failing with assertion error,jorisvandenbossche,closed,2020-05-25T18:06:00Z,2020-05-28T17:19:47Z,"```
In [7]: arr = np.random.randint(0, 1000, (100, 10))  

In [8]: df1 = pd.DataFrame(arr)  

In [9]: df2 = df1.copy() 
   ...: df2.iloc[0, [1, 3, 7]] =  np.nan    

In [10]: df1 + df2  
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-10-fa4784095cc3> in <module>
----> 1 df1 + df2

~/scipy/pandas/pandas/core/ops/__init__.py in f(self, other, axis, level, fill_value)
    663         if isinstance(other, ABCDataFrame):
    664             # Another DataFrame
--> 665             new_data = self._combine_frame(other, na_op, fill_value)
    666 
    667         elif isinstance(other, ABCSeries):

~/scipy/pandas/pandas/core/frame.py in _combine_frame(self, other, func, fill_value)
   5734                 return func(left, right)
   5735 
-> 5736         new_data = ops.dispatch_to_series(self, other, _arith_op)
   5737         return new_data
   5738 

~/scipy/pandas/pandas/core/ops/__init__.py in dispatch_to_series(left, right, func, axis)
    283 
    284         array_op = get_array_op(func)
--> 285         bm = left._mgr.operate_blockwise(right._mgr, array_op)
    286         return type(left)(bm)
    287 

~/scipy/pandas/pandas/core/internals/managers.py in operate_blockwise(self, other, array_op)
    358         Apply array_op blockwise with another (aligned) BlockManager.
    359         """"""
--> 360         return operate_blockwise(self, other, array_op)
    361 
    362     def apply(self: T, f, align_keys=None, **kwargs) -> T:

~/scipy/pandas/pandas/core/internals/ops.py in operate_blockwise(left, right, array_op)
     34             right_ea = not isinstance(rblk.values, np.ndarray)
     35 
---> 36             lvals, rvals = _get_same_shape_values(blk, rblk, left_ea, right_ea)
     37 
     38             res_values = array_op(lvals, rvals)

~/scipy/pandas/pandas/core/internals/ops.py in _get_same_shape_values(lblk, rblk, left_ea, right_ea)
     84 
     85     # Require that the indexing into lvals be slice-like
---> 86     assert rblk.mgr_locs.is_slice_like, rblk.mgr_locs
     87 
     88     # TODO(EA2D): with 2D EAs pnly this first clause would be needed

AssertionError: BlockPlacement([0 2 4 5 6 8 9])
```

cc @jbrockmendel I suppose caused by the frame-frame blockwise PR (https://github.com/pandas-dev/pandas/pull/32779), but didn't yet look into detail

Just removing the assertion seems to still work (for this case), but I don't know if that `_get_same_shape_values` functino relies on the `is_slice_like` characteristic for its implementation to be correct"
626160397,34420,REF: move to_offset to liboffsets,jbrockmendel,closed,2020-05-28T02:16:20Z,2020-05-28T17:20:37Z,
626122785,34419,REF: make remaining offset classes cdef,jbrockmendel,closed,2020-05-28T00:20:04Z,2020-05-28T17:21:51Z,"After this its just down to

1) a pass to clean up pickle-based kludges that were needed when these were mixed python/cython
2) privatize things in liboffsets no longer needed externally
3) move to_offset int liboffsets"
544220829,30584,ENH: Add dropna in groupby to allow NaN in keys,charlesdong1991,closed,2019-12-31T16:07:32Z,2020-05-28T18:48:46Z,"- [x] closes #3729 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Note that this PR will *NOT* fix the issue for `pivot_table` for now, the reason is that there is already an argument called `dropna` in `pivot_table` and it has slightly different meaning, currently it means: `Do not include columns whose entries are all NaN`.

I would propose a change in the follow-up PR for this since this is an api change: change the name of current `dropna` to `drop_all_na` maybe? and then add `dropna` to it and it is aligned with the `dropna` in groupby.


Summary:
After this PR, it will optional to inlcude NaN in group keys, e.g. below, and i also add example in docstring as well:
```python
a = [['a', 'b', 12, 12, 12], ['a', None, 12.3, 233., 12], ['b', 'a', 123.23, 123, 1], ['a', 'b', 1, 1, 1.]]
df = pd.DataFrame(a, columns=['a', 'b', 'c', 'd', 'e'])

df.groupby(by=['a', 'b']).sum()
```
will get
![Screen Shot 2020-01-01 at 11 01 16 AM](https://user-images.githubusercontent.com/9269816/71640222-19e68900-2c86-11ea-8749-c223729eec99.png)

with `dropna=False`,
```python
df.groupby(by=['a', 'b'], dropna=False).sum()
```
![Screen Shot 2020-01-01 at 11 01 23 AM](https://user-images.githubusercontent.com/9269816/71640225-2ff44980-2c86-11ea-9c66-a18147179c12.png)

For Series, it is the same:
```python
s = pd.Series([1, 2, 3, 3], index=[""a"", 'a', 'b', np.nan])

s.groupby(level=0).sum()
s.groupby(level=0, dropna=False).sum()
```
![Screen Shot 2020-01-01 at 11 01 32 AM](https://user-images.githubusercontent.com/9269816/71640228-4d291800-2c86-11ea-9d66-4a4be1a7347d.png)
"
625323995,34399,CLN: GH29547 format with f-strings,matteosantama,closed,2020-05-27T02:20:12Z,2020-05-28T19:17:24Z,"Sorry all, new PR. Didn't branch off master for the last one. 

#29547 "
592339763,33232,merge() outer with left_on column and right_index=True produces unexpected results ,cwkwong,open,2020-04-02T03:54:22Z,2020-05-28T20:36:30Z,"Apologies if this has been reported before, I searched through the issues but couldn't find a duplicate, IMHO.

#### Code Sample:

```python
import pandas as pd
import numpy as np

df1 = pd.DataFrame({'lkey': [0, np.nan, np.nan, 3], 'value': [1, 2, 3, 5]})
>>> df1
   lkey  value
0   0.0      1
1   NaN      2
2   NaN      3
3   3.0      5


df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'rock', 'baz'], 'value': [0, 1, 2, 3]})
>>> df2
   rkey  value
0   foo      0
1   bar      1
2  rock      2
3   baz      3


res_1 = pd.merge(df1, df2, how='outer', suffixes=('_left', '_right'), left_on='lkey', right_index=True)
>>> res_1
     lkey  value_left  rkey  value_right
0.0   0.0         1.0   foo          0.0
1.0   NaN         2.0   NaN          NaN
2.0   NaN         3.0   NaN          NaN
3.0   3.0         5.0   baz          3.0
NaN   1.0         NaN   bar          1.0
NaN   2.0         NaN  rock          2.0


res_2 = pd.merge(df1, df2, how='outer', suffixes=('_left', '_right'), left_on='lkey', right_on='value')
>>> res_2
   lkey  value_left  rkey  value_right
0   0.0         1.0   foo          0.0
1   NaN         2.0   NaN          NaN
2   NaN         3.0   NaN          NaN
3   3.0         5.0   baz          3.0
4   NaN         NaN   bar          1.0
5   NaN         NaN  rock          2.0
```
#### Problem description

- When merging with `how=outer` and `right_index=True`,  result `res_1` produces the last 2 rows with some unexpected values for the `index` and especially the `lkey`.
  - Last 2 rows have `lkey` values that don't seem to exist in the original left side `df1`.
  - Last 2 rows have `np.nan` for `index`. Was expecting perhaps `[4.0, 5.0]`
- Compare this to `res_2`. Which is almost identical `merge` except now instead of `right_index=True` we use a column `right_on='value'`
  - the `df2` `index` and `value` column have the same type and values.
- I would expect seeing `res_2` instead of `res_1` when merging with `right_index=True` above.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_AU.UTF-8
LOCALE           : en_AU.UTF-8

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0
Cython           : 0.29.11
pytest           : 5.1.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 0.9.6
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.9.6
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 1.8.6
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : 1.3.12
tables           : 3.5.1
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 0.9.6

</details>
"
625108884,34393,DOC: add a semicolon to hide long matplotlib axes return value,partev,closed,2020-05-26T18:51:37Z,2020-05-28T21:17:00Z,"add a semicolon to hide debug messages like:
Out[85]: 
array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f3d0ad6e3d0>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3d0ad4a350>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3d0aa10250>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3d0a6367d0>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f3d088c2d50>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3d089ea310>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3d08d89890>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3d0a76ae10>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f3d0a756090>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3d0a8dd750>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3d096aaed0>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3d0937d490>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f3d09f49a10>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3d0a0551d0>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3d0a418550>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3d0a6f0ad0>]],
      dtype=object)
which you can see here:

https://pandas.pydata.org/docs/user_guide/visualization.html

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
626613093,34432,CLN: assorted tslibs cleanups,jbrockmendel,closed,2020-05-28T15:41:33Z,2020-05-28T21:28:52Z,
624474164,34370,CLN Replace old string formatting syntax with f-strings #29547: 2 files w…,OlivierLuG,closed,2020-05-25T20:32:04Z,2020-05-28T21:44:46Z,"Hello, i'm new to github / opensource. Let me know if I'm doing things correctly

pandas/_libs/tslibs/timedeltas.pyx
pandas/_libs/tslibs/timestamps.pyx


Replace old string formatting syntax with f-strings #29547

- [ x ] passes `black pandas`
- [  ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
592575885,33235,TST: Use try/except block to properly catch and handle the exception,roberthdevries,closed,2020-04-02T11:58:29Z,2020-05-28T22:10:39Z,"Now no more warnings

- [x] closes #32951
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
626822570,34442,CLN: clearer lookups for period accessors,jbrockmendel,closed,2020-05-28T21:20:28Z,2020-05-28T22:20:24Z,
626858509,34444,CLN: simplify to_offset,jbrockmendel,closed,2020-05-28T22:33:51Z,2020-05-29T01:09:34Z,
626894250,34447,REF: move offset_to_period_map from liboffsets,jbrockmendel,closed,2020-05-28T23:43:06Z,2020-05-29T01:10:38Z,"its only used in tseries.frequencies, so this puts it back there"
626098711,34418,DEPR: deprecate non keyword arguments in read_excel,topper-123,closed,2020-05-27T23:15:39Z,2020-05-29T06:12:00Z,"Follow-up to #27573.

Allows two non-keyword arguments, ``io`` and ``sheet_name``. I think ``sheet_name`` is quite often (e.g. Interactively) supplied without being a keyword argument and requiring it will just be needlessly annoying.

Also some clean-up in pandas/tests/io/excel."
626986530,34451,BUG:pandas use all() or any() get wrong result,sin-en-2009,closed,2020-05-29T04:21:17Z,2020-05-29T06:24:30Z,"- [ ] I have checked that this issue has not already been reported.
yes
- [ ] I have confirmed this bug exists on the latest version of pandas.
yes
- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.
yes
---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
print(data['shift1'].values.all() > 0,data['shift1'].values)
print(data['shift1'].values.any() > 0,data['shift1'].values[0],data['shift1'].values[1],-3680.0>0)
```

#### Problem description
True [-3680. -3680.]
True -3680.0 -3680.0 False
The value of series is clearly less than 0, but using all () and any () gives the wrong result

#### Expected Output
True [-3680. -3680.]
True -3680.0 -3680.0 False

#### Output of ``pd.show_versions()``
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 58 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.4
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 39.1.0
<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
626714361,34434,TST: additional regression cases for slicing blockwise op (GH34421),jorisvandenbossche,closed,2020-05-28T18:17:28Z,2020-05-29T06:38:58Z,"Adding the other case from https://github.com/pandas-dev/pandas/issues/34367 to the test as well (although also fixed by the PR, it was failing with a different error originally, so might be worth it to add as test case as well).

cc @jbrockmendel  "
572074011,32294,ENH: Move corrwith from transformation to reduction kernels in groupby.base,fujiaxiang,closed,2020-02-27T13:10:59Z,2020-05-29T10:47:56Z,"- [x] closes #31270
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
559798040,31653,ENH: Timestamp constructor now raises more explanatory error message,fujiaxiang,closed,2020-02-04T15:33:34Z,2020-05-29T10:48:00Z,"Timestamp constructor now raises explanatory error message when year, month or day is missing

- [x] closes #31200
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
551378225,31101,BUG: groupby transform fillna produces wrong result,fujiaxiang,closed,2020-01-17T12:05:33Z,2020-05-29T10:48:03Z,"- [x] closes #30918
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
626754593,34437,BUG: pivot_table calculates incorrect standard deviation when np.std passed to aggfunc,dcsaba89,closed,2020-05-28T19:27:21Z,2020-05-29T12:46:05Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

`
import numpy as np
import pandas as pd

df = pd.DataFrame({'A': ['a', 'a'], 'X': [2.8597, 2.8503]})
pivot = pd.pivot_table(df, index='A', values='X', aggfunc=(np.mean, np.std))
my_std = np.std(df.X)  # 0.0047000000000001485
pivot_std = pivot.loc['a', 'std']  # 0.006646803743153757
`


#### Problem description

pivot_table calculates incorrect standard deviation when np.std passed to aggfunc.

#### Expected Output
Numpy calculates the correct std in the given example.
"
626892987,34446,CLN: liboffsets de-duplicate pickle code,jbrockmendel,closed,2020-05-28T23:40:59Z,2020-05-29T14:45:18Z,privatize/cdef functions that are now only used in liboffsets
623644938,34336,REGR: concat of Sparse with incompatible dtype now gives Sparse[object] instead of object,jorisvandenbossche,closed,2020-05-23T12:00:32Z,2020-05-29T16:03:30Z,"Regression on master related to the `common_dtype` mechanism, I suppose:

```
In [5]: pd.__version__    
Out[5]: '1.0.3'

In [6]: s1 = pd.Series([1, 0, 2], dtype=pd.SparseDtype(""int64"", 0)) 

In [7]: s2 = pd.Series([""a"", ""b"", ""c""], dtype=""category"") 

In [8]: pd.concat([s1, s2]) 
Out[8]: 
0    1
1    0
2    2
0    a
1    b
2    c
dtype: object

In [9]: pd.concat([s2, s1]) 
Out[9]: 
0    a
1    b
2    c
0    1
1    0
2    2
dtype: object
```

(and the same on v0.25.3)

But on master:

```
# raising in SparseDtype._get_common_dtype
In [3]: pd.concat([s1, s2])  
...
TypeError: data type not understood

In [4]: pd.concat([s2, s1])   
Out[4]: 
0    a
1    b
2    c
0    1
1    0
2    2
dtype: Sparse[object, 0]
```"
417863419,25571,tricky timestamp conversion,randomgambit,closed,2019-03-06T15:28:44Z,2020-05-29T16:30:50Z,"Hello there, its me the bug hunter again :)

I have this massive 200 million rows dataset, and I encountered some very annoying behavior. I wonder if this is a bug.

I load my csv using

```
mylog = pd.read_csv('/mydata.csv',
                    names = ['mydatetime',  'var2', 'var3', 'var4'],
                    dtype = {'mydatetime' : str},
                    skiprows = 1)
```

and the `datetime` column really look like regular timestamps (tz aware)

```
mylog.mydatetime.head()
Out[22]: 
0    2019-03-03T20:58:38.000-0500
1    2019-03-03T20:58:38.000-0500
2    2019-03-03T20:58:38.000-0500
3    2019-03-03T20:58:38.000-0500
4    2019-03-03T20:58:38.000-0500
Name: mydatetime, dtype: object
```

Now, I take extra care in converting these string into proper timestamps:

`mylog['mydatetime'] = pd.to_datetime(mylog['mydatetime'] ,errors = 'coerce', format = '%Y-%m-%dT%H:%M:%S.%f%z', infer_datetime_format = True, cache = True)`

That takes a looong time to process, but seems OK. The output is

```
mylog.mydatetime.head()
Out[23]: 
0    2019-03-03 20:58:38-05:00
1    2019-03-03 20:58:38-05:00
2    2019-03-03 20:58:38-05:00
3    2019-03-03 20:58:38-05:00
4    2019-03-03 20:58:38-05:00
Name: mydatetime, dtype: object

```
What is puzzling is that so far I thought I had full control of my `dtypes`. However, running the simple

```
mylog['myday'] = pd.to_datetime(mylog['mydatetime'].dt.date, errors = 'coerce')

  File ""pandas/_libs/tslib.pyx"", line 537, in pandas._libs.tslib.array_to_datetime

ValueError: Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True
```

The only way I was able to go past this error was by running

`mylog['myday'] = pd.to_datetime(mylog['mydatetime'].apply(lambda x: x.date()))
`

Is this a bug? Before upgrading to `24.1` I was not getting the `tz` error above. What do you think? I cant share the data but I am happy to try some things to help you out!

Thanks!"
596234519,33384,FEATURE: pandas.Series.query() #22347,yaolan4,closed,2020-04-08T00:46:16Z,2020-05-29T18:03:02Z,"
"
625845600,34410,32-bit compat issues in MacPython/pandas-wheels,TomAugspurger,closed,2020-05-27T16:29:45Z,2020-05-29T19:05:06Z,"https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=36222&view=logs&j=c0130b29-789d-5a3c-6978-10796a508a7f&t=e120bc6c-1f5e-5a41-8f0a-1d992cd2fbfb&l=1400

A couple classes of failures

```
2020-05-27T03:46:00.4640450Z ______________ test_multiple_agg_funcs[rolling-2-expected_vals0] ______________
2020-05-27T03:46:00.4640874Z [gw0] win32 -- Python 3.7.7 D:\a\1\s\test_venv\Scripts\python.exe
2020-05-27T03:46:00.4641216Z 
2020-05-27T03:46:00.4641446Z func = 'rolling', window_size = 2
2020-05-27T03:46:00.4641974Z expected_vals = [[nan, nan, nan, nan], [15.0, 20.0, 25.0, 20.0], [25.0, 30.0, 35.0, 30.0], [nan, nan, nan, nan], [20.0, 30.0, 35.0, 30.0], [35.0, 40.0, 60.0, 40.0], ...]
2020-05-27T03:46:00.4642426Z 
2020-05-27T03:46:00.4642646Z     @pytest.mark.parametrize(
2020-05-27T03:46:00.4642970Z         ""func,window_size,expected_vals"",
2020-05-27T03:46:00.4643468Z         [
2020-05-27T03:46:00.4643722Z             (
2020-05-27T03:46:00.4643997Z                 ""rolling"",
2020-05-27T03:46:00.4644235Z                 2,
2020-05-27T03:46:00.4644500Z                 [
2020-05-27T03:46:00.4644818Z                     [np.nan, np.nan, np.nan, np.nan],
2020-05-27T03:46:00.4645140Z                     [15.0, 20.0, 25.0, 20.0],
2020-05-27T03:46:00.4645482Z                     [25.0, 30.0, 35.0, 30.0],
2020-05-27T03:46:00.4645799Z                     [np.nan, np.nan, np.nan, np.nan],
2020-05-27T03:46:00.4646157Z                     [20.0, 30.0, 35.0, 30.0],
2020-05-27T03:46:00.4646503Z                     [35.0, 40.0, 60.0, 40.0],
2020-05-27T03:46:00.4646811Z                     [60.0, 80.0, 85.0, 80],
2020-05-27T03:46:00.4647113Z                 ],
2020-05-27T03:46:00.4647372Z             ),
2020-05-27T03:46:00.4647589Z             (
2020-05-27T03:46:00.4647855Z                 ""expanding"",
2020-05-27T03:46:00.4648104Z                 None,
2020-05-27T03:46:00.4648376Z                 [
2020-05-27T03:46:00.4648680Z                     [10.0, 10.0, 20.0, 20.0],
2020-05-27T03:46:00.4648983Z                     [15.0, 20.0, 25.0, 20.0],
2020-05-27T03:46:00.4649326Z                     [20.0, 30.0, 30.0, 20.0],
2020-05-27T03:46:00.4649670Z                     [10.0, 10.0, 30.0, 30.0],
2020-05-27T03:46:00.4649972Z                     [20.0, 30.0, 35.0, 30.0],
2020-05-27T03:46:00.4650322Z                     [26.666667, 40.0, 50.0, 30.0],
2020-05-27T03:46:00.4650683Z                     [40.0, 80.0, 60.0, 30.0],
2020-05-27T03:46:00.4650946Z                 ],
2020-05-27T03:46:00.4651208Z             ),
2020-05-27T03:46:00.4651414Z         ],
2020-05-27T03:46:00.4651646Z     )
2020-05-27T03:46:00.4651997Z     def test_multiple_agg_funcs(func, window_size, expected_vals):
2020-05-27T03:46:00.4652328Z         # GH 15072
2020-05-27T03:46:00.4652605Z         df = pd.DataFrame(
2020-05-27T03:46:00.4652839Z             [
2020-05-27T03:46:00.4653114Z                 [""A"", 10, 20],
2020-05-27T03:46:00.4653414Z                 [""A"", 20, 30],
2020-05-27T03:46:00.4653669Z                 [""A"", 30, 40],
2020-05-27T03:46:00.4653966Z                 [""B"", 10, 30],
2020-05-27T03:46:00.4654266Z                 [""B"", 30, 40],
2020-05-27T03:46:00.4654520Z                 [""B"", 40, 80],
2020-05-27T03:46:00.4654822Z                 [""B"", 80, 90],
2020-05-27T03:46:00.4655056Z             ],
2020-05-27T03:46:00.4655354Z             columns=[""stock"", ""low"", ""high""],
2020-05-27T03:46:00.4655642Z         )
2020-05-27T03:46:00.4655833Z     
2020-05-27T03:46:00.4656116Z         f = getattr(df.groupby(""stock""), func)
2020-05-27T03:46:00.4656545Z         if window_size:
2020-05-27T03:46:00.4656817Z             window = f(window_size)
2020-05-27T03:46:00.4657104Z         else:
2020-05-27T03:46:00.4657337Z             window = f()
2020-05-27T03:46:00.4657591Z     
2020-05-27T03:46:00.4657872Z         index = pd.MultiIndex.from_tuples(
2020-05-27T03:46:00.4658235Z             [(""A"", 0), (""A"", 1), (""A"", 2), (""B"", 3), (""B"", 4), (""B"", 5), (""B"", 6)],
2020-05-27T03:46:00.4658625Z             names=[""stock"", None],
2020-05-27T03:46:00.4658902Z         )
2020-05-27T03:46:00.4659155Z         columns = pd.MultiIndex.from_tuples(
2020-05-27T03:46:00.4659548Z             [(""low"", ""mean""), (""low"", ""max""), (""high"", ""mean""), (""high"", ""min"")]
2020-05-27T03:46:00.4659860Z         )
2020-05-27T03:46:00.4660195Z         expected = pd.DataFrame(expected_vals, index=index, columns=columns)
2020-05-27T03:46:00.4660526Z     
2020-05-27T03:46:00.4660750Z         result = window.agg(
2020-05-27T03:46:00.4661122Z >           OrderedDict(((""low"", [""mean"", ""max""]), (""high"", [""mean"", ""min""])))
2020-05-27T03:46:00.4661464Z         )
2020-05-27T03:46:00.4661630Z 
2020-05-27T03:46:00.4661965Z test_venv\lib\site-packages\pandas\tests\window\test_api.py:346: 
2020-05-27T03:46:00.4662381Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2020-05-27T03:46:00.4662906Z test_venv\lib\site-packages\pandas\core\window\rolling.py:2033: in aggregate
2020-05-27T03:46:00.4663339Z     return super().aggregate(func, *args, **kwargs)
2020-05-27T03:46:00.4663730Z test_venv\lib\site-packages\pandas\core\window\rolling.py:603: in aggregate
2020-05-27T03:46:00.4664169Z     result, how = self._aggregate(func, *args, **kwargs)
2020-05-27T03:46:00.4664599Z test_venv\lib\site-packages\pandas\core\base.py:417: in _aggregate
2020-05-27T03:46:00.4664941Z     result = _agg(arg, _agg_1dim)
2020-05-27T03:46:00.4665315Z test_venv\lib\site-packages\pandas\core\base.py:384: in _agg
2020-05-27T03:46:00.4665658Z     result[fname] = func(fname, agg_how)
2020-05-27T03:46:00.4666056Z test_venv\lib\site-packages\pandas\core\base.py:368: in _agg_1dim
2020-05-27T03:46:00.4666433Z     return colg.aggregate(how)
2020-05-27T03:46:00.4666793Z test_venv\lib\site-packages\pandas\core\window\rolling.py:2033: in aggregate
2020-05-27T03:46:00.4667225Z     return super().aggregate(func, *args, **kwargs)
2020-05-27T03:46:00.4667657Z test_venv\lib\site-packages\pandas\core\window\rolling.py:603: in aggregate
2020-05-27T03:46:00.4668054Z     result, how = self._aggregate(func, *args, **kwargs)
2020-05-27T03:46:00.4668471Z test_venv\lib\site-packages\pandas\core\base.py:475: in _aggregate
2020-05-27T03:46:00.4668902Z     return self._aggregate_multiple_funcs(arg, _axis=_axis), None
2020-05-27T03:46:00.4669286Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2020-05-27T03:46:00.4669590Z 
2020-05-27T03:46:00.4669874Z self = RollingGroupby [window=2,center=False,axis=0], arg = ['mean', 'max']
2020-05-27T03:46:00.4670218Z _axis = 0
2020-05-27T03:46:00.4670378Z 
2020-05-27T03:46:00.4670690Z     def _aggregate_multiple_funcs(self, arg, _axis):
2020-05-27T03:46:00.4671087Z         from pandas.core.reshape.concat import concat
2020-05-27T03:46:00.4671345Z     
2020-05-27T03:46:00.4671605Z         if _axis != 0:
2020-05-27T03:46:00.4671974Z             raise NotImplementedError(""axis other than 0 is not supported"")
2020-05-27T03:46:00.4672267Z     
2020-05-27T03:46:00.4672562Z         if self._selected_obj.ndim == 1:
2020-05-27T03:46:00.4672862Z             obj = self._selected_obj
2020-05-27T03:46:00.4673151Z         else:
2020-05-27T03:46:00.4673452Z             obj = self._obj_with_exclusions
2020-05-27T03:46:00.4673697Z     
2020-05-27T03:46:00.4673941Z         results = []
2020-05-27T03:46:00.4674209Z         keys = []
2020-05-27T03:46:00.4674412Z     
2020-05-27T03:46:00.4674664Z         # degenerate case
2020-05-27T03:46:00.4674921Z         if obj.ndim == 1:
2020-05-27T03:46:00.4675282Z             for a in arg:
2020-05-27T03:46:00.4675640Z                 colg = self._gotitem(obj.name, ndim=1, subset=obj)
2020-05-27T03:46:00.4675954Z                 try:
2020-05-27T03:46:00.4676277Z                     new_res = colg.aggregate(a)
2020-05-27T03:46:00.4676528Z     
2020-05-27T03:46:00.4676805Z                 except TypeError:
2020-05-27T03:46:00.4677111Z                     pass
2020-05-27T03:46:00.4677359Z                 else:
2020-05-27T03:46:00.4677673Z                     results.append(new_res)
2020-05-27T03:46:00.4677958Z     
2020-05-27T03:46:00.4678210Z                     # make sure we find a good name
2020-05-27T03:46:00.4678584Z                     name = com.get_callable_name(a) or a
2020-05-27T03:46:00.4678891Z                     keys.append(name)
2020-05-27T03:46:00.4679164Z     
2020-05-27T03:46:00.4679405Z         # multiples
2020-05-27T03:46:00.4679630Z         else:
2020-05-27T03:46:00.4679957Z             for index, col in enumerate(obj):
2020-05-27T03:46:00.4680401Z                 colg = self._gotitem(col, ndim=1, subset=obj.iloc[:, index])
2020-05-27T03:46:00.4680756Z                 try:
2020-05-27T03:46:00.4681075Z                     new_res = colg.aggregate(arg)
2020-05-27T03:46:00.4681407Z                 except (TypeError, DataError):
2020-05-27T03:46:00.4681737Z                     pass
2020-05-27T03:46:00.4682119Z                 except ValueError as err:
2020-05-27T03:46:00.4682422Z                     # cannot aggregate
2020-05-27T03:46:00.4682818Z                     if ""Must produce aggregated value"" in str(err):
2020-05-27T03:46:00.4683257Z                         # raised directly in _aggregate_named
2020-05-27T03:46:00.4683562Z                         pass
2020-05-27T03:46:00.4683915Z                     elif ""no results"" in str(err):
2020-05-27T03:46:00.4684337Z                         # raised directly in _aggregate_multiple_funcs
2020-05-27T03:46:00.4684653Z                         pass
2020-05-27T03:46:00.4684954Z                     else:
2020-05-27T03:46:00.4685220Z                         raise
2020-05-27T03:46:00.4685512Z                 else:
2020-05-27T03:46:00.4685823Z                     results.append(new_res)
2020-05-27T03:46:00.4686109Z                     keys.append(col)
2020-05-27T03:46:00.4686378Z     
2020-05-27T03:46:00.4686628Z         # if we are empty
2020-05-27T03:46:00.4686893Z         if not len(results):
2020-05-27T03:46:00.4687219Z >           raise ValueError(""no results"")
2020-05-27T03:46:00.4687514Z E           ValueError: no results
2020-05-27T03:46:00.4687767Z 
2020-05-27T03:46:00.4688056Z test_venv\lib\site-packages\pandas\core\base.py:540: ValueError
2020-05-27T03:46:00.4900327Z ________________ test_rolling_apply_args_kwargs[args_kwargs0] _________________
2020-05-27T03:46:00.4910069Z [gw0] win32 -- Python 3.7.7 D:\a\1\s\test_venv\Scripts\python.exe
2020-05-27T03:46:00.4913683Z 
2020-05-27T03:46:00.4916587Z args_kwargs = [None, {'par': 10}]
2020-05-27T03:46:00.4920984Z 
2020-05-27T03:46:00.4932595Z     @pytest.mark.parametrize(""args_kwargs"", [[None, {""par"": 10}], [(10,), None]])
2020-05-27T03:46:00.4934841Z     def test_rolling_apply_args_kwargs(args_kwargs):
2020-05-27T03:46:00.4935692Z         # GH 33433
2020-05-27T03:46:00.4936381Z         def foo(x, par):
2020-05-27T03:46:00.4937029Z             return np.sum(x + par)
2020-05-27T03:46:00.4937700Z     
2020-05-27T03:46:00.4938401Z         df = DataFrame({""gr"": [1, 1], ""a"": [1, 2]})
2020-05-27T03:46:00.4939052Z     
2020-05-27T03:46:00.4939885Z         idx = Index([""gr"", ""a""])
2020-05-27T03:46:00.4940561Z         expected = DataFrame([[11.0, 11.0], [11.0, 12.0]], columns=idx)
2020-05-27T03:46:00.4941155Z     
2020-05-27T03:46:00.4941805Z         result = df.rolling(1).apply(foo, args=args_kwargs[0], kwargs=args_kwargs[1])
2020-05-27T03:46:00.4942523Z         tm.assert_frame_equal(result, expected)
2020-05-27T03:46:00.4943122Z     
2020-05-27T03:46:00.4943684Z         result = df.rolling(1).apply(foo, args=(10,))
2020-05-27T03:46:00.4945113Z     
2020-05-27T03:46:00.4945760Z         midx = MultiIndex.from_tuples([(1, 0), (1, 1)], names=[""gr"", None])
2020-05-27T03:46:00.4946452Z         expected = Series([11.0, 12.0], index=midx, name=""a"")
2020-05-27T03:46:00.4947078Z     
2020-05-27T03:46:00.4947685Z         gb_rolling = df.groupby(""gr"")[""a""].rolling(1)
2020-05-27T03:46:00.4948255Z     
2020-05-27T03:46:00.4949192Z >       result = gb_rolling.apply(foo, args=args_kwargs[0], kwargs=args_kwargs[1])
2020-05-27T03:46:00.4949774Z 
2020-05-27T03:46:00.4950430Z test_venv\lib\site-packages\pandas\tests\window\test_apply.py:165: 
2020-05-27T03:46:00.4951199Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2020-05-27T03:46:00.4951938Z test_venv\lib\site-packages\pandas\core\window\rolling.py:2067: in apply
2020-05-27T03:46:00.4952625Z     kwargs=kwargs,
2020-05-27T03:46:00.4953353Z test_venv\lib\site-packages\pandas\core\window\rolling.py:1391: in apply
2020-05-27T03:46:00.4954036Z     kwargs=kwargs,
2020-05-27T03:46:00.4954689Z test_venv\lib\site-packages\pandas\core\window\rolling.py:2196: in _apply
2020-05-27T03:46:00.4955365Z     **kwargs,
2020-05-27T03:46:00.4956044Z test_venv\lib\site-packages\pandas\core\window\rolling.py:589: in _apply
2020-05-27T03:46:00.4956693Z     result = calc(values)
2020-05-27T03:46:00.4957386Z test_venv\lib\site-packages\pandas\core\window\rolling.py:575: in calc
2020-05-27T03:46:00.4958198Z     closed=self.closed,
2020-05-27T03:46:00.4958846Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2020-05-27T03:46:00.4959456Z 
2020-05-27T03:46:00.4960053Z self = <pandas.core.window.indexers.GroupbyRollingIndexer object at 0x3A34FA30>
2020-05-27T03:46:00.4960788Z num_values = 2, min_periods = None, center = False, closed = None
2020-05-27T03:46:00.4961381Z 
2020-05-27T03:46:00.4961909Z     @Appender(get_window_bounds_doc)
2020-05-27T03:46:00.4962523Z     def get_window_bounds(
2020-05-27T03:46:00.4963100Z         self,
2020-05-27T03:46:00.4963652Z         num_values: int = 0,
2020-05-27T03:46:00.4964290Z         min_periods: Optional[int] = None,
2020-05-27T03:46:00.4964946Z         center: Optional[bool] = None,
2020-05-27T03:46:00.4965552Z         closed: Optional[str] = None,
2020-05-27T03:46:00.4966208Z     ) -> Tuple[np.ndarray, np.ndarray]:
2020-05-27T03:46:00.4966908Z         # 1) For each group, get the indices that belong to the group
2020-05-27T03:46:00.4967605Z         # 2) Use the indices to calculate the start & end bounds of the window
2020-05-27T03:46:00.4968322Z         # 3) Append the window bounds in group order
2020-05-27T03:46:00.4968991Z         start_arrays = []
2020-05-27T03:46:00.4969583Z         end_arrays = []
2020-05-27T03:46:00.4970143Z         window_indicies_start = 0
2020-05-27T03:46:00.4970823Z         for key, indicies in self.groupby_indicies.items():
2020-05-27T03:46:00.4971539Z             if self.index_array is not None:
2020-05-27T03:46:00.4972196Z                 index_array = self.index_array.take(indicies)
2020-05-27T03:46:00.4972843Z             else:
2020-05-27T03:46:00.4973467Z                 index_array = self.index_array
2020-05-27T03:46:00.4974075Z             indexer = self.rolling_indexer(
2020-05-27T03:46:00.4974759Z                 index_array=index_array, window_size=self.window_size,
2020-05-27T03:46:00.4975407Z             )
2020-05-27T03:46:00.4975981Z             start, end = indexer.get_window_bounds(
2020-05-27T03:46:00.4976661Z                 len(indicies), min_periods, center, closed
2020-05-27T03:46:00.4977281Z             )
2020-05-27T03:46:00.4977908Z             # Cannot use groupby_indicies as they might not be monotonic with the object
2020-05-27T03:46:00.4978599Z             # we're rolling over
2020-05-27T03:46:00.4979224Z             window_indicies = np.arange(
2020-05-27T03:46:00.4979816Z                 window_indicies_start,
2020-05-27T03:46:00.4980472Z                 window_indicies_start + len(indicies),
2020-05-27T03:46:00.4981202Z                 dtype=np.int64,
2020-05-27T03:46:00.4981747Z             )
2020-05-27T03:46:00.4982364Z             window_indicies_start += len(indicies)
2020-05-27T03:46:00.4983051Z             # Extend as we'll be slicing window like [start, end)
2020-05-27T03:46:00.4983789Z             window_indicies = np.append(window_indicies, [window_indicies[-1] + 1])
2020-05-27T03:46:00.4984532Z >           start_arrays.append(window_indicies.take(start))
2020-05-27T03:46:00.4985326Z E           TypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'
2020-05-27T03:46:00.4986013Z 
```"
615918777,34114,MacPython linux build failing ,TomAugspurger,closed,2020-05-11T14:14:48Z,2020-05-29T19:05:40Z,"There are build failures with the manylinux1 32-bit docker image. 

```
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -DNPY_NO_DEPRECATED_API=0 -Ipandas/_libs/tslibs -I./pandas/_libs/tslibs -I/opt/python/cp38-cp38/lib/python3.8/site-packages/numpy/core/include -I/opt/python/cp38-cp38/include/python3.8 -c pandas/_libs/tslibs/period.c -o build/temp.linux-x86_64-3.8/pandas/_libs/tslibs/period.o
pandas/_libs/tslibs/period.c: In function ‘__pyx_f_6pandas_5_libs_6tslibs_6period_pqyear’:
pandas/_libs/tslibs/period.c:12464:3: warning: ‘__pyx_v_year’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   return __pyx_r;
   ^
pandas/_libs/tslibs/period.c: In function ‘__pyx_f_6pandas_5_libs_6tslibs_6period_pquarter’:
pandas/_libs/tslibs/period.c:12512:3: warning: ‘__pyx_v_quarter’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   return __pyx_r;


building 'pandas._libs.hashtable' extension
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wl,-strip-all -I/usr/local/include -fPIC -DNPY_NO_DEPRECATED_API=0 -Ipandas/_libs/src/klib -I/opt/python/cp38-cp38/lib/python3.8/site-packages/numpy/core/include -I/opt/python/cp38-cp38/include/python3.8 -c pandas/_libs/hashtable.c -o build/temp.linux-i686-3.8/pandas/_libs/hashtable.o -Werror
pandas/_libs/hashtable.c: In function ‘__pyx_f_6pandas_5_libs_9hashtable_value_count_float64’:
pandas/_libs/hashtable.c:27378:41: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
pandas/_libs/hashtable.c:29050:41: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
         for (__pyx_t_10 = 0; __pyx_t_10 < __pyx_t_9; __pyx_t_10+=1) {
                                         ^
pandas/_libs/hashtable.c: In function ‘__pyx_pf_6pandas_5_libs_9hashtable_8duplicated_uint64’:
pandas/_libs/hashtable.c:29800:38: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
             __pyx_t_10 = ((__pyx_v_k != __pyx_v_table->n_buckets) != 0);
                                      ^
pandas/_libs/hashtable.c: In function ‘__pyx_pf_6pandas_5_libs_9hashtable_10ismember_uint64’:
pandas/_libs/hashtable.c:30271:177: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
           *__Pyx_BufPtrStrided1d(__pyx_t_5numpy_uint8_t *, __pyx_pybuffernd_result.rcbuffer->pybuffer.buf, __pyx_t_17, __pyx_pybuffernd_result.diminfo[0].strides) = (__pyx_v_k != __pyx_v_table->n_buckets);
                                                                                                                                                                                 ^
pandas/_libs/hashtable.c: In function ‘__pyx_f_6pandas_5_libs_9hashtable_value_count_object’:
pandas/_libs/hashtable.c:30731:33: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
   for (__pyx_t_8 = 0; __pyx_t_8 < __pyx_t_7; __pyx_t_8+=1) {
                                 ^
pandas/_libs/hashtable.c: In function ‘__pyx_pf_6pandas_5_libs_9hashtable_14duplicated_object’:
pandas/_libs/hashtable.c:31389:31: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
       __pyx_t_9 = ((__pyx_v_k != __pyx_v_table->n_buckets) != 0);
                               ^
pandas/_libs/hashtable.c: In function ‘__pyx_pf_6pandas_5_libs_9hashtable_16ismember_object’:
pandas/_libs/hashtable.c:31825:171: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
     *__Pyx_BufPtrStrided1d(__pyx_t_5numpy_uint8_t *, __pyx_pybuffernd_result.rcbuffer->pybuffer.buf, __pyx_t_16, __pyx_pybuffernd_result.diminfo[0].strides) = (__pyx_v_k != __pyx_v_table->n_buckets);
                                                                                                                                                                           ^
pandas/_libs/hashtable.c: In function ‘__pyx_f_6pandas_5_libs_9hashtable_value_count_int64’:
pandas/_libs/hashtable.c:32258:39: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
         for (__pyx_t_9 = 0; __pyx_t_9 < __pyx_t_8; __pyx_t_9+=1) {
                                       ^
pandas/_libs/hashtable.c: In function ‘__pyx_pf_6pandas_5_libs_9hashtable_20duplicated_int64’:
pandas/_libs/hashtable.c:33007:38: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
             __pyx_t_10 = ((__pyx_v_k != __pyx_v_table->n_buckets) != 0);
                                      ^
pandas/_libs/hashtable.c: In function ‘__pyx_pf_6pandas_5_libs_9hashtable_22ismember_int64’:
pandas/_libs/hashtable.c:33478:177: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
           *__Pyx_BufPtrStrided1d(__pyx_t_5numpy_uint8_t *, __pyx_pybuffernd_result.rcbuffer->pybuffer.buf, __pyx_t_17, __pyx_pybuffernd_result.diminfo[0].strides) = (__pyx_v_k != __pyx_v_table->n_buckets);
                                                                                                                                                                                 ^
pandas/_libs/hashtable.c: In function ‘__pyx_pf_6pandas_5_libs_9hashtable_24mode_float64’:
pandas/_libs/hashtable.c:33800:41: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
         for (__pyx_t_13 = 0; __pyx_t_13 < __pyx_t_12; __pyx_t_13+=1) {
                                         ^
pandas/_libs/hashtable.c: In function ‘__pyx_pf_6pandas_5_libs_9hashtable_26mode_int64’:
pandas/_libs/hashtable.c:34225:41: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
         for (__pyx_t_13 = 0; __pyx_t_13 < __pyx_t_12; __pyx_t_13+=1) {
                                         ^
pandas/_libs/hashtable.c: In function ‘__pyx_pf_6pandas_5_libs_9hashtable_28mode_uint64’:
pandas/_libs/hashtable.c:34650:41: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
         for (__pyx_t_13 = 0; __pyx_t_13 < __pyx_t_12; __pyx_t_13+=1) {
                                         ^
pandas/_libs/hashtable.c: In function ‘__pyx_pf_6pandas_5_libs_9hashtable_30mode_object’:
pandas/_libs/hashtable.c:35077:35: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
   for (__pyx_t_13 = 0; __pyx_t_13 < __pyx_t_12; __pyx_t_13+=1) {
                                   ^
cc1: all warnings being treated as errors

```

To reproduce

```Dockerfile
FROM quay.io/pypa/manylinux1_i686:latest

RUN git clone --depth=1 https://github.com/pandas-dev/pandas \
 && cd pandas \
 && /opt/python/cp38-cp38/bin/python -m pip install numpy cython python-dateutil pytz
WORKDIR pandas

RUN /opt/python/cp38-cp38/bin/python setup.py build_ext -i
```

I've uploaded the generated hashtable.c file [here][hashtable].


[hashtable]: https://gist.github.com/1f60b2059a103e6d36c02a0d07200e59


---

#34409 fixed the following issues.

https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=35250&view=logs&j=e9afaa34-1a0f-534e-78fc-fae528ccd915&t=4685e914-c192-594f-c25f-e6cef98b7ba4

```
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wl,-strip-all -I/usr/local/include -fPIC -DNPY_NO_DEPRECATED_API=0 -Ipandas/_libs/tslibs -I./pandas/_libs/tslibs -I/opt/python/cp38-cp38/lib/python3.8/site-packages/numpy/core/include -I/opt/python/cp38-cp38/include/python3.8 -c pandas/_libs/tslibs/period.c -o build/temp.linux-x86_64-3.8/pandas/_libs/tslibs/period.o -Werror
pandas/_libs/tslibs/period.c: In function ‘__pyx_f_6pandas_5_libs_6tslibs_6period_pqyear’:
pandas/_libs/tslibs/period.c:12479:3: error: ‘__pyx_v_year’ may be used uninitialized in this function [-Werror=maybe-uninitialized]
   return __pyx_r;
   ^
pandas/_libs/tslibs/period.c: In function ‘__pyx_f_6pandas_5_libs_6tslibs_6period_pquarter’:
pandas/_libs/tslibs/period.c:12527:3: error: ‘__pyx_v_quarter’ may be used uninitialized in this function [-Werror=maybe-uninitialized]
   return __pyx_r;
   ^
cc1: all warnings being treated as errors
warning: pandas/_libs/groupby.pyx:1101:26: Unreachable code
```

https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=34768&view=results from May 3rd may be the first one with this warning. https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=34727&view=results from May 2nd built successfully."
625991196,34416,Macpython 32 bit build fixup,TomAugspurger,closed,2020-05-27T20:00:26Z,2020-05-29T19:05:45Z,"Closes #34114 

cc @jbrockmendel and @WillAyd. I have no idea if this is correct, but it builds."
626204029,34423,BUG: Fix failing MacPython 32bit wheels for groupby rolling,mroeschke,closed,2020-05-28T04:26:34Z,2020-05-29T19:08:49Z,"- [x] closes #34410
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

What would be the best way to validate that this fixes the MacPython/pandas-wheels @TomAugspurger "
625724558,34406,PERF: asv runner is down,jreback,closed,2020-05-27T14:03:30Z,2020-05-29T19:09:54Z,"see https://github.com/pandas-dev/pandas/pull/34389#issuecomment-634641337

@TomAugspurger said the asv runner has segfaulted, this is a tracking issue."
487785450,28243,BUG: Left join on index and column gives incorrect output,dsaxton,open,2019-08-31T16:14:16Z,2020-05-29T21:45:31Z,"```python
import numpy as np
import pandas as pd

df_left = pd.DataFrame(index=[""a"", ""b""])
df_right = pd.DataFrame({""x"": [""a"", ""c""]})

pd.merge(df_left, df_right, left_index=True, right_on=""x"", how=""left"")
#      x
# 0.0  a
# NaN  b
```
#### Problem description

This is closely related to https://github.com/pandas-dev/pandas/issues/28220 but deals with the values of the `DataFrame` rather than the index itself.  When left joining on an index and a column it looks like the value `""b""` from the index of `df_left` is somehow getting carried over to the column `x`, but `""a""` should be the only value in this column since it's the only one that matches the index from `df_left`.  This is happening on 0.25.1 and master, and has been a bug for some time according to https://github.com/pandas-dev/pandas/issues/28220.

#### Expected Output

```python
     x
a    a
b  NaN
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.16.4
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : None
pytest           : 5.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.7.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>

Seems related to this issue: https://github.com/pandas-dev/pandas/issues/17257"
602492739,33632,IO: Fix parquet read from s3 directory,alimcmaster1,closed,2020-04-18T16:01:56Z,2020-05-29T22:14:08Z,"- [x] closes https://github.com/pandas-dev/pandas/issues/26388 
- [x] closes https://github.com/pandas-dev/pandas/issues/27596
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

(Seems to have also fixed the xfailing test in https://github.com/pandas-dev/pandas/issues/33077)

NOTE: lets merge https://github.com/pandas-dev/pandas/pull/33645 first - since that fixes up a crucial bit of error handling around this functionality."
586347958,32936,"Pandas 1.0.3 on Windows 7 64bit throws error ""DLL load failed while importing aggregations"" on import.  Version 1.0.1 works fine.",mborus,closed,2020-03-23T16:39:48Z,2020-05-30T09:46:35Z,"_TLDR;  2 DLLs are missing that were included in 1.0.1_

#### Code Sample, a copy-pastable example if possible

```python

import pandas as pd

```
#### Problem description

I'm using Windows 7, German 64bit with a fresh installation of Python 3.8.2 64bit.
I updated pip, created a fresh virtual environment with ""py -3 -m venv venv"" and activated it.
This installation is in a restricted area with no direct, unfiltered internet access.

I installed Pandas 1.0.3 from a via ""python -m pip download pandas"" download made on an unrestricted similar Windows7 machine. I moved the download folder behind a firewall and installed Pandas like this: ""python -m pip install pandas --no-index --find-links /path/to/downloads"":

Pandas installs fine. It crashes on import with the error below (The last sentence says in English: The requested Module was not found)

As recommended by this stack overflow thread (https://stackoverflow.com/questions/60763529/unable-to-import-pandas-pandas-libs-window-aggregations) going back to 1.0.1 fixes the problem. (More below, after the error)

Error:

(venv) C:\my_program>python
Python 3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 23:03:10) [MSC v.1916 64 bit (AM
D64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas as pd
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\my_program\venv\lib\site-packages\pandas\__init__.py"", line 5
5, in <module>
    from pandas.core.api import (
  File ""C:\my_program\venv\lib\site-packages\pandas\core\api.py"", line 2
9, in <module>
    from pandas.core.groupby import Grouper, NamedAgg
  File ""C:\my_program\venv\lib\site-packages\pandas\core\groupby\__init_
_.py"", line 1, in <module>
    from pandas.core.groupby.generic import DataFrameGroupBy, NamedAgg, SeriesGr
oupBy
  File ""C:\my_program\venv\lib\site-packages\pandas\core\groupby\generic
.py"", line 60, in <module>
    from pandas.core.frame import DataFrame
  File ""C:\my_program\venv\lib\site-packages\pandas\core\frame.py"", line
 124, in <module>
    from pandas.core.series import Series
  File ""C:\my_program\venv\lib\site-packages\pandas\core\series.py"", lin
e 4572, in <module>
    Series._add_series_or_dataframe_operations()
  File ""C:\my_program\venv\lib\site-packages\pandas\core\generic.py"", li
ne 10349, in _add_series_or_dataframe_operations
    from pandas.core.window import EWM, Expanding, Rolling, Window
  File ""C:\my_program\venv\lib\site-packages\pandas\core\window\__init__
.py"", line 1, in <module>
    from pandas.core.window.ewm import EWM  # noqa:F401
  File ""C:\my_program\venv\lib\site-packages\pandas\core\window\ewm.py"",
 line 5, in <module>
    import pandas._libs.window.aggregations as window_aggregations
ImportError: DLL load failed while importing aggregations: Das angegebene Modul
wurde nicht gefunden.

I compared the installation of version 1.0.1 to 1.0.3;

The 1.0.3 installs these files in
""\venv\Lib\site-packages\pandas\_libs\window\""
```
    __init__.py
    aggregations.cp38-win_amd64.pyd
    indexers.cp38-win_amd64.pyd
```
The 1.0.1 version also installs these two DLLs in the same folder.
```
    concrt140.dll
    msvcp140.dll
```

When I **manually place these DLLs** into the above folder of the 1.0.3 version,
the import works **without an error**.

#### Expected Output

No error message

#### Output of ``pd.show_versions()``

Not available, since pd did not import


"
627587675,34470,ENH: Handing missing data in scatter_matrix when using hue/color,amueller,open,2020-05-29T22:59:10Z,2020-05-30T10:33:46Z,"#### Is your feature request related to a problem?

I would like to use scatter_matrix on a dataframe with missing entries when using the c parameter.

This works:
```python
import pandas as pd
import numpy as np
X = pd.DataFrame(np.random.normal(size=(150, 4)))
y = pd.Series(np.random.randint(0, 4, size=150))
pd.plotting.scatter_matrix(X, c=y)
```
this works:
```python
X[X > 1] = np.NaN
pd.plotting.scatter_matrix(X)
```
This breaks:
```python
X[X > 1] = np.NaN
pd.plotting.scatter_matrix(X, c=y)
```
```pythontb

ValueError: Invalid RGBA argument: 1.0

During handling of the above exception, another exception occurred:

ValueError: 'c' argument has 150 elements, which is inconsistent with 'x' and 'y' with size 103.
```
(using pandas 1.0.3)

#### Describe the solution you'd like
scatter_matrix should make sure the shapes are consistent. I haven't looked into the internals. Calling ``plt.scatter`` with missing values actually does the right thing.

#### API breaking implications

Don't think it'll break anything.

#### Describe alternatives you've considered

I could use seaborn but seaborn doesn't allow me to pass a series as 'c'. I can also use ``dabl`` ;)
"
612246345,33984,BUG: Fix Series.update ExtensionArray issue,dsaxton,closed,2020-05-05T00:07:40Z,2020-05-30T16:30:29Z,"- [x] closes #33980
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
627813169,34489,Added documentation for building using pyenv. ,rorcores,closed,2020-05-30T20:24:03Z,2020-05-30T20:50:34Z,"Added documentation for building using pyenv. Discussed this PR with @WillAyd on gitter on May 20. 

Also fixed up another line he mentioned (python setup.py build_ext --inplace -j 4
instead of:
python setup.py build_ext --inplace -j 0) for conda/venv/pyenv.

Have rebuilt the docs and ensured they build properly."
118604913,11690,ENH: rounding for datetimelike Indexes/Scalars,jreback,closed,2015-11-24T13:07:34Z,2020-05-31T03:23:24Z,"closes #4314 
"
457500347,26921,pandas.DataFrame.plot error for time series data,alexgawrilow,open,2019-06-18T14:00:13Z,2020-05-31T08:18:13Z,"#### Code Sample

```python
import pandas as pd

idx = pd.date_range(start=""2018"", freq=pd.DateOffset(days=1), periods=2)
ts = pd.DataFrame({""A"": [1, 2]}, idx)
ts.plot()
```
#### Problem description

I created the index of a time series using pd.date_range and provided the frequency as a `DateOffset` object. This index was passed to the `DataFrame` constructor to create a time series. When calling the `plot` method on this time series, pandas is not able to parse the frequency correctly:
```
233     if isinstance(freq, DateOffset):
--> 234         freq = freq.rule_code
    235     else:
    236         freq = get_base_alias(freq)

~/venv/lib/python3.6/site-packages/pandas/tseries/offsets.py in rule_code(self)
    372     @property
    373     def rule_code(self):
--> 374         return self._prefix
    375 
    376     @cache_readonly

~venv/lib/python3.6/site-packages/pandas/tseries/offsets.py in _prefix(self)
    368     @property
    369     def _prefix(self):
--> 370         raise NotImplementedError('Prefix not defined')
    371 
    372     @property
```

#### Expected Output
When passing the frequency to `pd.date_range` as a string, everything works fine, however both indices are equal.
```python
idx2 = pd.date_range(start=""2018"", freq=""D"", periods=2)
ts = pd.DataFrame({""A"": [1, 2]}, idx2)
ts.plot()
idx.equals(idx2)
```
Output: `True`

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.7.final.0
python-bits: 64
OS: Linux
OS-release: 4.18.0-10-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 4.3.1
pip: 19.1.1
setuptools: 41.0.1
Cython: 0.29.6
numpy: 1.16.2
scipy: None
pyarrow: None
xarray: None
IPython: 7.3.0
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.2.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: 4.7.1
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
627903454,34494,ENH: Add read_logfmt(),link2xt,closed,2020-05-31T06:31:31Z,2020-05-31T21:10:22Z,"- [x] closes #34501 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
627870425,34492,CLN: remove unused JsonReader.path_or_buf,link2xt,closed,2020-05-31T01:53:57Z,2020-05-31T22:04:02Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
627860716,34491,CLN: de-duplicate bits lib timestamps,jbrockmendel,closed,2020-05-31T00:50:29Z,2020-05-31T22:32:59Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
628024584,34505,CLN: unreachable branch in tzconversion,jbrockmendel,closed,2020-05-31T19:11:27Z,2020-05-31T22:34:18Z,
628018030,34504,CLN: stronger typing in tzconversion,jbrockmendel,closed,2020-05-31T18:33:03Z,2020-05-31T22:39:12Z,
627619435,34476,REF: make normalize_i8_timestamps cpdef,jbrockmendel,closed,2020-05-30T01:15:05Z,2020-05-31T23:06:19Z,"there's a tiny perf bump, but it all comes from doing `own_tz = self.tzinfo` up-front instead of accessing `self.tz` multiple times within `Timestamp.normalize`.  With just the cimport, this is actually slightly slower, which is something of a mystery.

The main upside is that we now only cimport from libconversion, which im hopeful will make some other circular-imports easier to simplify.the perf improvement "
627583884,34469,TYP: annotations in plotting code,jbrockmendel,closed,2020-05-29T22:46:13Z,2020-05-31T23:07:50Z,Trying to sort out what is getting passed to the tseries.frequencies funcs in these modules.
627430638,34462,REF: make Resolution an enum,jbrockmendel,closed,2020-05-29T17:41:57Z,2020-06-01T00:02:06Z,I find this much clearer than just using an int.  No measured performance impact.
605282747,33741,BUG: DatetimeIndex.get_indexer wrong result on mixed-type argument,batterseapower,closed,2020-04-23T06:45:30Z,2020-06-01T00:28:41Z,"- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of pandas.
- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

#### Code Sample, a copy-pastable example

```python
import datetime
import pandas as pd

dates = pd.bdate_range('2020-04-20', '2020-04-25')
print(dates.get_indexer([datetime.date(2020, 4, 20), datetime.date(2020, 4, 21)]))
# [0, 1]
print(dates.get_indexer([pd.Timestamp('2020-04-20'), datetime.date(2020, 4, 21)]))
# [0, -1]
```

#### Problem description

As a general rule I would expect that `index.get_indexer(as + bs) == np.concat([index.get_indexer(as), index.get_indexer(bs)])` for any lists `as` and `bs`, i.e. `index.get_indexer` operates elementwise. However in this case the type of a previous argument is surprisingly affecting the index that is assigned to a later one.

#### Expected Output

`[0, 1]` in both cases

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 63 Stepping 2, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United Kingdom.936

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
605707197,33749,BUG: Fix mixed datetime dtype inference,dsaxton,closed,2020-04-23T17:16:03Z,2020-06-01T00:29:55Z,"- [x] closes #33741
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
620231932,34237,BUG:pd.read_excel does not read xlsb,ShayHa,closed,2020-05-18T13:57:10Z,2020-06-01T01:41:17Z,"In the docs it says:
```code
Supports xls, xlsx, xlsm, xlsb, and odf file extensions read from a local filesystem or URL. Supports an option to read a single sheet or a list of sheets.
```
When I try to read a xlsb file with specifying engine='openpyxl'
I get the next error:

```code
df = pd.read_excel(r""file.xlsb"",engine='openpyxl')
```

```code
openpyxl does not support binary format .xlsb, please convert this file to .xlsx format if you want to open it with openpyxl
```
I do have the openpyxl module installed

Thank you"
621095664,34261,DOC: Clarify ExcelFile's available engine compatibility with file types in...,miguelmarques1904,closed,2020-05-19T15:57:26Z,2020-06-01T01:41:29Z,"information about engine compatibility and also reflect the support of the pyxlsb engine. GH34237.

DOC: I edited the available ExcelFile's docstring to add information about the recent pyxlsb engine. I also added some compatibility information between the available engines and file types.


- [x] closes #34237
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
610818026,33924,Performance regression in DataFrame[bool_indexer],TomAugspurger,closed,2020-05-01T15:45:02Z,2020-06-01T01:42:30Z,"```python
import numpy as np
import pandas as pd

idx_dupe = np.array(range(30)) * 99
df = pd.DataFrame(np.random.randn(10000, 5))
bool_indexer = [True] * 5000 + [False] * 5000


%timeit df[np.array(bool_indexer)]
```


```
# 1.0.2
2.58 ms ± 108 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

# master
5.47 ms ± 192 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

Note that this only affects the case where the indexer is a *list*. Performance looks fine when an ndarray is passed.

https://pandas.pydata.org/speed/pandas/index.html#indexing.DataFrameNumericIndexing.time_bool_indexer?commits=80d37adcc3d9bfbbe17e8aa626d6b5873465ca98-4f89c261f624305fc7bae6c43ae862663994be34 points to somewhere in 80d37adc..4f89c261, which has a bunch of commits.
"
521365272,29563,Sparse DataFrame with all-zeros column returns NA for fancy index,memeplex,closed,2019-11-12T06:58:02Z,2020-06-01T01:46:31Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here

import pandas as pd

df = pd.DataFrame({""x"": [0, 0, 0],
                   ""y"": [4, 5, 6]})
df = df.astype(pd.SparseDtype(""float"", fill_value=0))
df.iloc[[1]]
```
#### Problem description

Executing the above code results in:

```
        x	y
1	NaN	5.0
```

Instead of

```
        x	y
1	0	5.0
```

This could be related to https://github.com/pandas-dev/pandas/issues/27781, although the interpretation of the bug is different.

#### Expected Output

Mentioned above.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.candidate.1
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-19-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.1.0
Cython           : 0.29.14
pytest           : None
hypothesis       : None
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : 1.3.11
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
619227057,34199,PERF: Fixes performance regression in DataFrame[bool_indexer] (#33924),mproszewska,closed,2020-05-15T20:25:45Z,2020-06-01T01:51:04Z,"- [x] closes #33924
- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Insted of calling `construction.array` in `check_array_indexer`, creates array with `dtype=bool` before calling `check_array_indexer`.

Fixes performance regression after commit b9bcdc307.

```
setup = """"""
import numpy as np
import pandas as pd
df = pd.DataFrame(np.random.randn(100000, 5))
bool_indexer = [True] * 50000 + [False] * 50000
""""""
import timeit
timeit.timeit(""df[bool_indexer]"",setup=setup, number=1000)

# master
# 27.29123323399108
# now
# 8.814320757053792
```
"
595962328,33371,ENH: Add query functionality to skiprows param in read_csv,slimnsour,closed,2020-04-07T15:37:36Z,2020-06-01T01:55:28Z,"This allows users to pass in strings to query through the ``skiprows`` parameter in ``pandas.read_csv``. This is done through querying the Dataframe as soon as it is created, and returning a Dataframe with the desired columns to the user. 

Included in this PR are the relevant whatsnew entry, documentation changes, and new tests.

- [X] closes #32072
- [x] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry"
627795543,34486,"ENH: mul(Tick, float); simplify to_offset",jbrockmendel,closed,2020-05-30T18:50:31Z,2020-06-01T03:22:12Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This will let us remove Resolution.get_stride_from_decimal"
628216595,34509,BUG: fix BooleanArray.astype('string') (GH34110),jorisvandenbossche,closed,2020-06-01T07:27:09Z,2020-06-01T08:29:49Z,"This should fix CI (bug caused by https://github.com/pandas-dev/pandas/pull/34110, due to another PR that was merged a few days ago adding this capability)"
505529066,28909,DOC: diff() when using unsigned ints,jjlkant,closed,2019-10-10T21:49:32Z,2020-06-01T10:32:23Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import numpy as np
>>> import pandas as pd
>>> 
>>> s = pd.Series([1, 0], dtype=np.uint8)
>>> s.diff()
0      NaN
1    255.0
dtype: float64
```
#### Problem description

The current version of the docs don't reflect on what happens when a Series or DataFrame containing unsigned ints are used with the `diff()` function, especially for cases where integer overflow occurs.

NumPy handles this in their docs for `numpy.diff()`, stating:

> For unsigned integer arrays, the results will also be unsigned. This should not be surprising, as the result is consistent with calculating the difference directly:
```python
>>> u8_arr = np.array([1, 0], dtype=np.uint8)
>>> np.diff(u8_arr)
array([255], dtype=uint8)
>>> u8_arr[1,...] - u8_arr[0,...]
255
```

The behaviour of `pandas.Series.diff()`  and `pandas.DataFrame.diff()` during calculation is the same as the NumPy equivalent, but the actual return type is no longer an unsigned integer; it will always return with a `float64` type.

#### Expected Output

The current output _can_ be expected when the user is perfectly aware that integer overflow might possibly occur during the calculation of the `diff()` output, even though it returns as a `float64` dtype. I do think it is desired to mention this in docs similarly to the NumPy equivalent though.

However, since the output dtype of `diff()` will always be `float64`, this also allows to handle the case differently: Instead of storing the output of the calculation:
https://github.com/pandas-dev/pandas/blob/0c4404b5b85c085e5b6cd26be42e81e5a197fd8d/pandas/core/algorithms.py#L1964
into a float64 dtyped array, it would also be possible to process the values as float instead of integers? This would generate the output:
```
0    NaN
1   -1.0
dtype: float64
```

I understand that this might go against the absolute difference of the elements as they are defined inside of the Series or DataFrame, but it would better fit the output type? Maybe just a warning for the overflow is the best middle ground instead? Would love to hear your thoughts on the case...

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : None.None

pandas           : 0.25.1
numpy            : 1.17.2
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
581198576,32699,DOC: Clarify output of diff (returned type and possible overflow),mproszewska,closed,2020-03-14T12:56:25Z,2020-06-01T10:33:00Z,"- [x] closes #28909
- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] note about dtype in `diff` in Dataframa and Series
"
615480047,34110,CLN: Don't use astype_nansafe after to_numpy,dsaxton,closed,2020-05-10T21:27:06Z,2020-06-01T13:22:42Z,"I'm not 100% sure about the change in the tests, but it seems to align with numpy:
```python
[ins] In [1]: np.array([1, 2]).astype(str)                                                                                                                                                                   
Out[1]: array(['1', '2'], dtype='<U21')

[ins] In [2]: pd.array([1, 2]).astype(str)                                                                                                                                                                   
Out[2]: array(['1', '2'], dtype='<U21')
```
```python
[ins] In [1]: np.array([True, False]).astype(str)                                                                                                                                                            
Out[1]: array(['True', 'False'], dtype='<U5')

[ins] In [2]: pd.array([True, False]).astype(str)                                                                                                                                                            
Out[2]: array(['True', 'False'], dtype='<U5')
```"
628013052,34503,Backport PR #34481 on branch 1.0.x (DOC: start 1.0.5),meeseeksmachine,closed,2020-05-31T18:05:26Z,2020-06-01T14:25:15Z,Backport PR #34481: DOC: start 1.0.5
577330855,32516,Deprecate Aliases as orient Argument in DataFrame.to_dict,elmonsomiat,closed,2020-03-07T12:53:26Z,2020-06-01T14:52:06Z,"- [ x] closes #32515
- [x ] tests added / passed
- [x ] passes `black pandas`
- [x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x ] whatsnew entry"
627740421,34481,DOC: start 1.0.5,simonjayhawkins,closed,2020-05-30T14:13:52Z,2020-06-01T15:43:56Z,xref https://github.com/pandas-dev/pandas/pull/33970#issuecomment-624029963
626465323,34430,DOC: cleanup whatsnew post 1.0.4 release,simonjayhawkins,closed,2020-05-28T12:21:33Z,2020-06-01T15:47:51Z,
627536299,34464,CLN: drop **kwds from pd.read_excel,topper-123,closed,2020-05-29T21:02:10Z,2020-06-01T16:06:50Z,"Drop ``**kwds`` from ``pd.read_excel``.

Wrt. ""new"" keyword parameter ``na_filter``: This parameter already exists in the doc string and is just masked in the ``kwds`` in the signature. If you follow the code paths all the way down to parsers.py, you can see that it indeed has a default of `True``."
626937509,34449,BUG: Period.to_timestamp not matching PeriodArray.to_timestamp,jbrockmendel,closed,2020-05-29T01:39:17Z,2020-06-01T16:34:25Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

With this change, I _think_ we can move to share some methods between Period and PeriodArray."
604250165,33709,DOC: Improve to_parquet documentation,larroy,closed,2020-04-21T20:03:37Z,2020-06-01T20:05:15Z,"fixes #29476

- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry  / NA
"
627612988,34474,BUG: fix origin epoch when freq is Day and harmonize epoch between timezones,hasB4K,closed,2020-05-30T00:44:46Z,2020-06-01T22:18:07Z,"Follow-up of #31809.

The purpose of this PR is to fix the current behavior on master:

```python
import pandas as pd
import numpy as np

start, end = ""2000-08-02 23:30:00+0500"", ""2000-12-02 00:30:00+0500""
rng = pd.date_range(start, end, freq=""7min"")
ts = pd.Series(np.random.randn(len(rng)), index=rng)

result_1 = ts.resample(""D"", origin=""epoch"").count()
result_2 = ts.resample(""24H"", origin=""epoch"").count()

print(f""result_1:\n\n{result_1}\n"")
print(f""result_2:\n\n{result_2}\n"")
```

Outputs on master:
```
result_1:

2000-08-02 00:00:00+05:00      5
2000-08-03 00:00:00+05:00    205
2000-08-04 00:00:00+05:00    206
2000-08-05 00:00:00+05:00    206
2000-08-06 00:00:00+05:00    206
                            ... 
2000-11-28 00:00:00+05:00    206
2000-11-29 00:00:00+05:00    206
2000-11-30 00:00:00+05:00    205
2000-12-01 00:00:00+05:00    206
2000-12-02 00:00:00+05:00      5
Freq: D, Length: 123, dtype: int64

result_2:

2000-08-02 05:00:00+05:00     48
2000-08-03 05:00:00+05:00    205
2000-08-04 05:00:00+05:00    206
2000-08-05 05:00:00+05:00    206
2000-08-06 05:00:00+05:00    205
                            ... 
2000-11-27 05:00:00+05:00    206
2000-11-28 05:00:00+05:00    206
2000-11-29 05:00:00+05:00    206
2000-11-30 05:00:00+05:00    205
2000-12-01 05:00:00+05:00    168
Freq: 24H, Length: 122, dtype: int64
```

Expected Outputs (with this PR):
```
result_1:

2000-08-02 00:00:00+05:00      5
2000-08-03 00:00:00+05:00    205
2000-08-04 00:00:00+05:00    206
2000-08-05 00:00:00+05:00    206
2000-08-06 00:00:00+05:00    206
                            ... 
2000-11-28 00:00:00+05:00    206
2000-11-29 00:00:00+05:00    206
2000-11-30 00:00:00+05:00    205
2000-12-01 00:00:00+05:00    206
2000-12-02 00:00:00+05:00      5
Freq: D, Length: 123, dtype: int64

result_2:

2000-08-02 00:00:00+05:00      5
2000-08-03 00:00:00+05:00    205
2000-08-04 00:00:00+05:00    206
2000-08-05 00:00:00+05:00    206
2000-08-06 00:00:00+05:00    206
                            ... 
2000-11-28 00:00:00+05:00    206
2000-11-29 00:00:00+05:00    206
2000-11-30 00:00:00+05:00    205
2000-12-01 00:00:00+05:00    206
2000-12-02 00:00:00+05:00      5
Freq: 24H, Length: 123, dtype: int64
```

------

I thought of two possible solutions while fixing that:
1. Consider 'epoch' as a UNIX epoch: `pd.Timestamp(0, tz=index_tz)`
2. Consider that 'epoch' is just an helper to fix the origin: `pd.Timestamp(""1970-01-01"", tz=index_tz)`

To have similar results between timezones, I thought the solution 2 was a better choice. The solution 2 could also help us to set the behavior `origin=epoch` as the default one in a future version since the results would be quite similar with `origin=start_day` (that is not quite the case with a DST timezone, I can provide further details if needed on why this is the case).

(The solution 1 is correct in theory but is giving results that could be hard to explain to the end user.)

------

- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry (follow-up PR, not necessary I think)
"
628517550,34515,CLN: remove Resolution.get_stride_from_decimal,jbrockmendel,closed,2020-06-01T15:11:32Z,2020-06-01T22:57:15Z,"as it is no longer used; update its tests

rename Resolution.get_reso -> Resolution.from_attrname"
628697688,34518,REF: add Resolution to tslibs.__init__,jbrockmendel,closed,2020-06-01T20:15:58Z,2020-06-02T01:21:46Z,rename _resolution -> _resolution_obj
640371916,34848,CI: Failing np.bool on numpy master,TomAugspurger,closed,2020-06-17T11:36:44Z,2020-06-17T17:26:57Z,"```
=================================== FAILURES ===================================

_________________________ test_dataframe_div_silenced __________________________

[gw0] linux -- Python 3.9.0 /home/travis/virtualenv/python3.9-dev/bin/python

    def test_dataframe_div_silenced():

        # GH#26793

        pdf1 = pd.DataFrame(

            {

                ""A"": np.arange(10),

                ""B"": [np.nan, 1, 2, 3, 4] * 2,

                ""C"": [np.nan] * 10,

                ""D"": np.arange(10),

            },

            index=list(""abcdefghij""),

            columns=list(""ABCD""),

        )

        pdf2 = pd.DataFrame(

            np.random.randn(10, 4), index=list(""abcdefghjk""), columns=list(""ABCX"")

        )

        with tm.assert_produces_warning(None):

>           pdf1.div(pdf2, fill_value=0)

pandas/tests/arithmetic/test_numeric.py:1311: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7f00d965da90>

type = None, value = None, traceback = None

    def __exit__(self, type, value, traceback):

        if type is None:

            try:

>               next(self.gen)

E               AssertionError: Caused unexpected warning(s): [('DeprecationWarning', DeprecationWarning('`np.bool` is a deprecated alias for the builtin `bool`. Use `bool` by itself, which is identical in behavior, to silence this warning. If you specifically wanted the numpy scalar type, use `np.bool_` here.'), '/home/travis/build/pandas-dev/pandas/pandas/core/indexes/base.py', 377), ('DeprecationWarning', DeprecationWarning('`np.bool` is a deprecated alias for the builtin `bool`. Use `bool` by itself, which is identical in behavior, to silence this warning. If you specifically wanted the numpy scalar type, use `np.bool_` here.'), '/home/travis/build/pandas-dev/pandas/pandas/core/indexes/base.py', 377), ('DeprecationWarning', DeprecationWarning('`np.bool` is a deprecated alias for the builtin `bool`. Use `bool` by itself, which is identical in behavior, to silence this warning. If you specifically wanted the numpy scalar type, use `np.bool_` here.'), '/home/travis/build/pandas-dev/pandas/pandas/core/indexes/base.py', 377), ('DeprecationWarning', DeprecationWarning('`np.bool` is a deprecated alias for the builtin `bool`. Use `bool` by itself, which is identical in behavior, to silence this warning. If you specifically wanted the numpy scalar type, use `np.bool_` here.'), '/home/travis/build/pandas-dev/pandas/pandas/core/indexes/base.py', 377)]
```

https://travis-ci.org/github/pandas-dev/pandas/jobs/699262221"
621349822,34267,"Fix .isin Considers ""1"" and 1 equal",gabrielvf1,closed,2020-05-19T23:30:12Z,2020-06-17T18:41:14Z,"- [X ] ref #34125
- [X ] tests added / passed
- [X ] passes `black pandas`
- [X ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
601137835,33591,BUG: groupby first()/last() change datatype of all NaN columns to float; nth() preserves datatype,jdmarino,closed,2020-04-16T15:01:32Z,2020-06-17T18:54:45Z,"- [x ] I have checked that this issue has not already been reported.
- [x ] I have confirmed this bug exists on the latest version of pandas.  
Version 1.0.3 from the conda distro.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
print('pandas version is', pd.__version__, '\n')

df = pd.DataFrame({'id':['a','b','b','c'], 'sym':['ibm','msft','msft','goog'], 'idx':range(4), 'sectype':['E','E','E','E']})
df['osi'] = df.sym.where(df.sectype=='O')  # add a column of NaNs that are object/str
print(df)
print(df.info())

# .nth(-1) does the right thing
x = df.groupby('id').nth(-1)
print('\nnth(-1)')
print(x.info())

# .last() converts the osi col to floats
x = df.groupby('id').last()
print('\nlast()')
print(x.info())

# .nth(0) does the right thing
x = df.groupby('id').nth(-1)
print('\nnth(0)')
print(x.info())

# .first() converts the osi col to floats
x = df.groupby('id').first()
print('\nfirst()')
print(x.info())

```

#### Problem description
Given a dataframe with an all-NaN column of str/objects, performing a groupby().first() will convert the all-NaN column to float.  This is true for .last() as well, but not for .nth(0) and .nth(-1), so these are workarounds.

This is a problem for me as the groupby().first() is in general code that is iteratively called and the results either pd.concat'd (resulting in a column with mixed types) or appended to an hdf file (causing failure on the write).

#### Expected Output
The resulting dataframe from a groupby().first()/.last() should have the same metadata (column structure and datatypes) as the input dataframe.  The result of .first()/.last() should match that of .nth(0)/.nth(-1) .

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 85 Stepping 4, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : 0.29.15
pytest           : 5.4.1
hypothesis       : 5.8.3
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.8
numba            : 0.48.0
</details>
"
547539535,30849,read_fwf reading large numbers as object dtype,TonyFhh,closed,2020-01-09T15:13:53Z,2020-06-17T21:42:13Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
>>> pd.read_fwf(r'D:\Other\ggg.txt', header=None, widths =[26,3,3,26])
                            0   1    2                     3
0  +0000000000000000000000000 NaN  CCY        30000000000000
1  +0077800000000000000000000 NaN  ABF  18244030000000000000

>>> pd.read_fwf(r'D:\Other\ggg.txt', header=None, widths =[26,3,3,26])
                            0   1    2                           3
0  +0000000000000000000000000 NaN  CCY  +0000000000030000000000000
1  +0077800000000000000000000 NaN  ABF  +0000018544030000000000000

>>> pd.read_fwf(r'D:\Other\ggg.txt', header=None, widths =[26,3,3,26])[3]
0    +0000000000030000000000000
1    +0000018544030000000000000
Name: 3, dtype: object
```
#### Problem description
I have a fixed width file with numbers specifications up to 25 characters with a 1 character sign in front. Normally for small numbers these numbers read properly. As the numbers get bigger as shown above, read_fwf starts to read them as objects even though they are numbers. Attempting to convert said column series with bigger number results in overflow error.

This may cause incompatibility of dtypes when the field is expected to be numerical. read_fwf could process the column as np.float64 to maintain the numerical dtype if np.int64 is not compaitable.

#### Expected Output
```python
>>> pd.read_fwf(r'D:\Other\ggg.txt', header=None, widths =[26,3,3,26])[3].astype(np.float64)
0    3.000000e+13
1    1.854403e+19
Name: 3, dtype: float64
```

</details>
"
631361774,34593,DOC: Can't figure out how to get exponential working,tavurth,closed,2020-06-05T07:02:26Z,2020-06-18T00:11:58Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

to_test = pd.DataFrame({ ""data"": [1,2,3,4,5,6,7,8,9,10] })
exponent = to_test.rolling((3,4), win_type=""exponential"").mean()

print(exponent)
```

```error
ValueError: exponential window requires tau
```

```python
import pandas as pd

to_test = pd.DataFrame({ ""data"": [1,2,3,4,5,6,7,8,9,10] })
exponent = to_test.rolling(3, win_type=""exponential"", tau=4).mean()

print(exponent)
```

```error
TypeError: rolling() got an unexpected keyword argument 'tau'
```

#### Problem description

I can't get exponential rolling working, and there's no documentation on this.

#### Expected Output
The exponential mean of the first 4 values

#### Output of ``pd.show_versions()``
<details>

INSTALLED VERSIONS

------------------

commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.3.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.4
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 46.0.0
Cython           : 0.29.17
pytest           : 5.4.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : 0.4.1
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.14.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.0
pytables         : None
pytest           : 5.4.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
632100659,34615,DOC: Clarify where to the additional arguments for some win_types,linnndachen,closed,2020-06-06T00:49:42Z,2020-06-18T00:12:10Z,"For example, std needs to specify when win_types is gaussian. However, std should be specified in the operation argument, not as one of the rolling arguments. This change is to clarify this point. 

- [ ] closes #34593
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
640237336,34843,BUG: Series(..) coerces datetime64[non-ns] array differently depending on writable flag,jorisvandenbossche,closed,2020-06-17T08:13:37Z,2020-06-18T06:48:00Z,"Normally, if we create a Series or DataFrame from a numpy array of non-ns datetime64, and the values are within the ns-range, we convert to datetime64[ns] (in `sanitize_array`). 

However, this does not seem to happen if the array is not writeable:

```
In [9]: arr = np.array(['2020-01-01T00:00:00.000000'], dtype=""datetime64[us]"") 

In [10]: pd.Series(arr) 
Out[10]: 
0   2020-01-01
dtype: datetime64[ns]

In [11]: arr.flags.writeable = False  

In [12]: pd.Series(arr)  
Out[12]: 
0    2020-01-01 00:00:00
dtype: object
```

I *suppose* this is a bug? (@jbrockmendel ?)

It seems this is due to `tslibs.conversion.ensure_datetime64ns` failing on a read-only buffer, and it started ""failing"" since pands 0.25"
640991515,34858,1.0 as boolean dtype cause ValueError in pd.read_csv(),aauss,closed,2020-06-18T07:43:32Z,2020-06-18T08:35:24Z,"When I read the following ""example_csv"" using pd.read_csv() without assigning dtypes, no error is raised:

```python
import io
import pandas as pd

example_csv = """"""
boolean_column, string_column
1.0, a
0.0, b
,c""""""

csv = io.StringIO(example_csv)
df = pd.read_csv(csv)
df.info()

>>> <class 'pandas.core.frame.DataFrame'>
RangeIndex: 3 entries, 0 to 2
Data columns (total 2 columns):
 #   Column          Non-Null Count  Dtype  
---  ------          --------------  -----  
 0   boolean_column  2 non-null      float64
 1    string_column  3 non-null      object 
dtypes: float64(1), object(1)
memory usage: 176.0+ bytes
```

However, I want ""boolean_column"" to have a nullable boolean dtype instead of float64. Therefore, I assign a dtype when reading the data which casues an error.

```python
csv = io.StringIO(example_csv)
df = pd.read_csv(csv, dtype={""boolean_column"": ""boolean""})

>>> ValueError: 1.0 cannot be cast to bool
```
Is this intended?

I am using Windows 10, Python 3.7.6 and pandas 1.0.4"
640271950,34844,BUG: fix construction from read-only non-ns datetime64 numpy array,jorisvandenbossche,closed,2020-06-17T09:03:34Z,2020-06-18T10:26:49Z,Closes #34843
636038728,34684,RLS: 1.0.5,simonjayhawkins,closed,2020-06-10T08:20:04Z,2020-06-18T13:09:20Z,"xref https://github.com/pandas-dev/pandas/pull/34664#issuecomment-641355926

>>> @simonjayhawkins do you have time one of those days to do a 1.0.5 release?

>> can do. do we want to include anything else, wait a few weeks before doing the release and see if anything else surfaces or just get a patch release done with the S3 changes?

> I would personally just do a patch release with the S3 changes, and not wait a few weeks with that. But maybe open a RLS issue for it to get some feedback from others?

cc @pandas-dev/pandas-core "
640760623,34854,QST: Events generated when dataframes are changed,sgt101,closed,2020-06-17T21:42:36Z,2020-06-18T14:39:33Z,"- [x ] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [x ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

here is a link to this question (similar) 
https://stackoverflow.com/questions/48209811/how-to-notify-pandas-dataframe-modification


---

#### Question about pandas

What events are generated when a dataframe is updated in pandas? for example if a column is dropped or a value is changed in a cell? I am not interested in accesses, but I'd like to know if a dataframe in my environment goes away or if it is altered. 

"
505640743,28917,DataFrame.to_html col_space parameter to change width of a specific column only,questglobal-bom,closed,2019-10-11T05:08:34Z,2020-06-18T15:21:38Z,"#### Code Sample, a copy-pastable example if possible


#### Problem description

Currently `col_space` changes width of all columns that are present in the dataframe. There must be a way to change the width of a specific column only while converting dataframe to html.

#### Expected Output
`col_space` to accept a list or dict where we can mention the column name to change only a specific column's width."
640529388,34853,DEPR: to_perioddelta,jbrockmendel,closed,2020-06-17T15:19:24Z,2020-06-18T16:47:15Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The only usage was in liboffsets and that has now been removed."
630055373,34550,BUG: Series.unique segfaults on invalid unicode,marco-neumann-by,closed,2020-06-03T14:41:34Z,2020-06-18T23:08:41Z," **Not present anymore, might be fixed by accident, but no could not find a PR that did that.**

#### Code Sample, a copy-pastable example

```python
import pandas as pd

ser = pd.Series([""\ud83d""])
ser.unique()
```

#### Problem description
```
UnicodeEncodeError: 'utf-8' codec can't encode character '\ud83d' in position 0: surrogates not allowed
Exception ignored in: 'pandas._libs.tslibs.util.get_c_string_buf_and_size'                                             
UnicodeEncodeError: 'utf-8' codec can't encode character '\ud83d' in position 0: surrogates not allowed
Segmentation fault (core dumped)             
```

#### Expected Output
Not crashing.

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-33-generic
machine          : x86_64
processor        :
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.4
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : 3.0.3
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
640442219,34851,base/test_unique.py: regression test for bad unicode string,suvayu,closed,2020-06-17T13:27:02Z,2020-06-18T23:08:46Z,"- [x] closes #34550 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I ran the tests with pandas 1.0.4, where it segfaults as expected (shown below), and `master` passes the test.

    tests/test_mytest.py::test_unique2[idx_or_series_w_bad_unicode0] Fatal Python error: Segmentation fault

    Current thread 0x00007f09d2dc4740 (most recent call first):
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pandas/core/algorithms.py"", line 382 in unique
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pandas/core/base.py"", line 1246 in unique
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 1974 in unique
      File ""/home/suvali/tmp/pandas-tests/tests/test_mytest.py"", line 16 in test_unique2
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/_pytest/python.py"", line 182 in pytest_pyfunc_call
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/callers.py"", line 187 in _multicall
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/manager.py"", line 84 in <lambda>
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/manager.py"", line 93 in _hookexec
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/hooks.py"", line 286 in __call__
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/_pytest/python.py"", line 1477 in runtest
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/_pytest/runner.py"", line 135 in pytest_runtest_call
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/callers.py"", line 187 in _multicall
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/manager.py"", line 84 in <lambda>
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/manager.py"", line 93 in _hookexec
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/hooks.py"", line 286 in __call__
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/_pytest/runner.py"", line 217 in <lambda>
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/_pytest/runner.py"", line 244 in from_call
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/_pytest/runner.py"", line 216 in call_runtest_hook
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/_pytest/runner.py"", line 186 in call_and_report
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/_pytest/runner.py"", line 100 in runtestprotocol
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/_pytest/runner.py"", line 85 in pytest_runtest_protocol
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/callers.py"", line 187 in _multicall
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/manager.py"", line 84 in <lambda>
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/manager.py"", line 93 in _hookexec
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/hooks.py"", line 286 in __call__
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/_pytest/main.py"", line 272 in pytest_runtestloop
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/callers.py"", line 187 in _multicall
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/manager.py"", line 84 in <lambda>
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/manager.py"", line 93 in _hookexec
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/hooks.py"", line 286 in __call__
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/_pytest/main.py"", line 247 in _main
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/_pytest/main.py"", line 191 in wrap_session
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/_pytest/main.py"", line 240 in pytest_cmdline_main
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/callers.py"", line 187 in _multicall
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/manager.py"", line 84 in <lambda>
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/manager.py"", line 93 in _hookexec
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/pluggy/hooks.py"", line 286 in __call__
      File ""/opt/conda/envs/py38/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 124 in main
      File ""/opt/conda/envs/py38/bin/pytest"", line 11 in <module>
    Segmentation fault (core dumped)"
641524575,34864,Series and Index have inconsistent repr,dsaxton,closed,2020-06-18T20:20:19Z,2020-06-19T08:27:46Z,"Series and Index format differently when printed to the console (e.g., `dtype: object` vs. `dtype='object'`), but likely they should look the same (Index seems better IMO since it matches the constructor and agrees with numpy).

```python
In [11]: pd.Series([], dtype=object, name=""abc"")                                                                                                                                                             
Out[11]: Series([], Name: abc, dtype: object)

In [12]: pd.Index([], dtype=object, name=""abc"")                                                                                                                                                              
Out[12]: Index([], dtype='object', name='abc')
```"
626691566,34433,DOC: reduce API docs for offset aliases,jorisvandenbossche,closed,2020-05-28T17:38:54Z,2020-06-19T11:01:29Z,Test to see if this works with sphinx
635683508,34675,BUG: pandas.groupby.apply does not work with floating point indexed dataframe since 1.0.0,r-luo,closed,2020-06-09T18:56:46Z,2020-06-19T15:06:35Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd  # pandas==1.0.4, but this issue exists since 1.0.0
import numpy as np  # numpy==1.16.3

toy_df = pd.DataFrame(
    {
        'group': np.random.randint(0, 5, 10), 
        'value': np.random.random(10)
    }, 
    index=np.random.random(10),
)

# fails with floating point index
toy_df.groupby('group').apply(sum)
toy_df.groupby('group').apply(lambda x: x['value'].sum())

# with integer index then it works:
toy_df.index = np.random.randint(0, 5, 10)
toy_df.groupby('group').apply(lambda x: x['value'].sum())
toy_df.groupby('group').apply(sum)
```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

I found this peculiar issue with the new version of pandas. When I have floating point index in a pandas DataFrame, then I cannot perform any `groupby.apply` operation on it. It will always give the below error:

```python
ValueError: index must be monotonic increasing or decreasing
During handling of the above exception, another exception occurred:
...
pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()
pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()
pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Float64HashTable.get_item()
pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Float64HashTable.get_item()
KeyError: 0.0
```

When I used `ipdb` to trace inside the error, I found that the following line caused the problem:
```python
> pandas/core/groupby/ops.py(930)fast_apply()
    929 
--> 930         sdata = self._get_sorted_data()
    931         return libreduction.apply_frame_axis0(sdata, f, names, starts, ends)
```

after this line, `sdata` still does not have a sorted index, and then the next line raises the `ValueError: index must be monotonic increasing or decreasing`.

I tested that this error exists in `pandas==1.0.4` and `pandas==1.0.0` but does not exist in `pandas==0.25.1`.

#### Expected Output

Expected output from groupby apply should be the same in the example with either floating point index or integer index.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.0.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.173-137.229.amzn2.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.4
numpy            : 1.16.3
pytz             : 2020.1
dateutil         : 2.8.0
pip              : 20.0.2
setuptools       : 47.1.1.post20200604
Cython           : 0.29.2
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
593667996,33277,CLN 31942/replace appender with doc 3,smartvinnetou,closed,2020-04-03T22:12:25Z,2020-06-19T16:36:01Z,"- [x] ref #31942
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] replaces `Appender` decorator with `doc`
"
629178128,34529,BUG: apply() fails on some value types,simondsmart,closed,2020-06-02T12:51:59Z,2020-06-19T22:33:21Z,"We have some existing code that manipulates data that is decoded into numpy arrays (by a C powered backend). This code has stopped working.

I've tried to strip it down to a reduced case

```
import pandas as pd
import numpy as np
df = pd.DataFrame(np.array([b'abcd', b'efgh']), columns=['col'])
df.apply(lambda x: x.astype('object'))
```
This fails with an error inside an internal function of apply:
```---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-88-a5fa9cabd101> in <module>
----> 1 df.apply(lambda x: x.astype('object'))

~/local/pkg/miniconda3/envs/odc/lib/python3.8/site-packages/pandas/core/frame.py in apply(self, func, axis, raw, result_type, args, **kwds)
   6876             kwds=kwds,
   6877         )
-> 6878         return op.get_result()
   6879 
   6880     def applymap(self, func) -> ""DataFrame"":

~/local/pkg/miniconda3/envs/odc/lib/python3.8/site-packages/pandas/core/apply.py in get_result(self)
    184             return self.apply_raw()
    185 
--> 186         return self.apply_standard()
    187 
    188     def apply_empty_result(self):

~/local/pkg/miniconda3/envs/odc/lib/python3.8/site-packages/pandas/core/apply.py in apply_standard(self)
    293 
    294             try:
--> 295                 result = libreduction.compute_reduction(
    296                     values, self.f, axis=self.axis, dummy=dummy, labels=labels
    297                 )

pandas/_libs/reduction.pyx in pandas._libs.reduction.compute_reduction()

pandas/_libs/reduction.pyx in pandas._libs.reduction.Reducer.__init__()

pandas/_libs/reduction.pyx in pandas._libs.reduction.Reducer._check_dummy()

ValueError: Dummy array must be same dtype
```
If we use a different dtype, then it works.
```
df = pd.DataFrame(np.array(['abcd', 'efgh']), columns=['col'])
df.apply(lambda x: x.astype('object'))
print(df)
```
which gives the expected result
```
    col
0  abcd
1  efgh
```"
639260931,34812,BUG: apply() fails on some value types,Veronur,closed,2020-06-15T23:47:32Z,2020-06-19T22:33:26Z,"- [x] closes #34529
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
573195477,32357,DOC: Update the pandas.Index.is_ docstring,garyteofanus,closed,2020-02-29T05:51:37Z,2020-06-20T08:24:55Z,"- [x] closes https://github.com/pandanistas/pandanistas_sprint_ui2020/issues/6
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Output of `python scripts/validate_docstrings.py pandas.Index.is_`:
```
################################################################################
################################## Validation ##################################
################################################################################

1 Errors found:
        No examples section found
```"
639974012,34831,CLN: GH29547 change string formatting with f-strings (6 files changed),MatteoFelici,closed,2020-06-16T21:09:49Z,2020-06-20T09:01:29Z,"Replace old string formatting syntax with f-strings #29547

- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Changed 6 files:
- pandas/tests/io/parser/test_header.py
-  pandas/tests/io/test_sql.py
- pandas/tests/io/test_html.py
- pandas/tests/reductions/test_reductions.py
- pandas/tests/reshape/test_melt.py
- pandas/tests/scalar/timedelta/test_timedelta.py

"
585653161,32903,"EHN: to_{html, string} col_space col specific",quangngd,closed,2020-03-22T06:07:56Z,2020-06-20T11:08:08Z,"- [ ] closes #28917
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

In to_html, to_latex and to_string, ```col_space``` arg now accepts a list or dict where we can mention the column name to change only a specific column's width.
Haven't add test for to_latex and to_string since they too don't have test for the old col_space and I cannot think of anything less cumbersome than to count added whitespace."
640922575,34856,DOC: Fix syntax in 1.1.0 whatsnew,quangngd,closed,2020-06-18T05:37:34Z,2020-06-20T11:08:40Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
642353104,34885,DOC: Fixed table formatting in box plot section,pvanhauw,closed,2020-06-20T10:40:06Z,2020-06-20T12:19:13Z,"[DOC]fixed table formatting in the box plot section keeing simple rst table format

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
642342482,34882,DOC: Fix validation issues with Index.is_ docstring,cvanweelden,closed,2020-06-20T09:19:57Z,2020-06-20T12:19:17Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Fix validation issues with the `Index.is_ docstring` and add `Index.identical` to see also section. Trying to finish what was started in #32357 "
642345799,34883,TST: Feather RoundTrip Column Ordering,alimcmaster1,closed,2020-06-20T09:44:58Z,2020-06-20T12:30:47Z,"Looks like this in fixed from pyarrow 0.17.1 -> Added a Test.

- [x] closes #https://github.com/pandas-dev/pandas/issues/33878
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
642364237,34891,Plotting Tables x-axis labels display,marydmit,closed,2020-06-20T12:05:13Z,2020-06-20T12:49:08Z,Added x-axis labels on top. Adjusted figure sizes in order to have the complete table visible in the user guide.
642369977,34894,move 3.9 travis build to allowed failures,jreback,closed,2020-06-20T12:47:03Z,2020-06-20T13:07:53Z,"xref #34881
 "
609353456,33878,BUG: read_feather doesn't work when columns are shuffle,Benjamin15,closed,2020-04-29T20:54:57Z,2020-06-20T13:46:16Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd

df = pd.DataFrame({
    ""A"": [1, 2],
    ""B"": [""x"", ""y""],
    ""C"": [True, False]
})
df.to_feather(""./test_data.feather"")

df2 = pd.read_feather(""./test_data.feather"", columns=['B', 'A'])
```

#### Error message

```python
ArrowInvalid                              Traceback (most recent call last)
<ipython-input-4-1e23cf201732> in <module>
     15 
     16 
---> 17 df2 = pd.read_feather(""/misc/labshare/datasets3/rating/data/preprocessing/tests/test_data.feather"", columns=['B', 'A'])

~/.conda/envs/venv/lib/python3.6/site-packages/pandas/io/feather_format.py in read_feather(path, columns, use_threads)
    101     path = stringify_path(path)
    102 
--> 103     return feather.read_feather(path, columns=columns, use_threads=bool(use_threads))

~/.conda/envs/venv/lib/python3.6/site-packages/pyarrow/feather.py in read_feather(source, columns, use_threads, memory_map)
    206     """"""
    207     _check_pandas_version()
--> 208     return (read_table(source, columns=columns, memory_map=memory_map)
    209             .to_pandas(use_threads=use_threads))
    210 

~/.conda/envs/venv/lib/python3.6/site-packages/pyarrow/feather.py in read_table(source, columns, memory_map)
    237         return reader.read_indices(columns)
    238     elif all(map(lambda t: t == str, column_types)):
--> 239         return reader.read_names(columns)
    240 
    241     column_type_names = [t.__name__ for t in column_types]

~/.conda/envs/venv/lib/python3.6/site-packages/pyarrow/feather.pxi in pyarrow.lib.FeatherReader.read_names()

~/.conda/envs/venv/lib/python3.6/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowInvalid: Schema at index 0 was different: 
B: string
A: int64
vs
A: int64
B: string
```
#### Problem description

We don't always know the order in which our columns are. 
The issue is when we update pyarrow to 0.17.0

This line work fine:
```python
df2 = pd.read_feather(""./test_data.feather"", columns=['B', 'A'])
```

Should we apply a fix here or in the pyarrow repository ?

#### Expected Output

df2 = pd.DataFrame({
    ""A"": [1, 2],
    ""B"": [""x"", ""y""],
})

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-91-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3
Cython           : 0.29.15
pytest           : 5.3.2
hypothesis       : 5.5.4
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : 0.3.3
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.17.0
pytables         : None
pytest           : 5.3.2
pyxlsb           : None
s3fs             : None
scipy            : 1.2.3
sqlalchemy       : 1.3.13
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.48.0

</details>
"
489362379,28287,DataFrame.dropna() not working with sparse columns,rjboczar,closed,2019-09-04T19:50:41Z,2020-06-20T13:46:25Z,"```python
import numpy as np
import pandas as pd

A = pd.DataFrame({'a': [0,1], 'b': pd.SparseArray([np.nan, 1])})

# Prints empty DataFrame
A.dropna()

```
Not sure if I'm using this correctly, but I'd expect only the first row to be dropped.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.16.4
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.2
setuptools       : 41.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.7.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None


</details>
"
642374895,34900,"TST: xref 34893, html bank url tests",jreback,closed,2020-06-20T13:21:40Z,2020-06-20T13:49:28Z,xref #34893 
642366675,34893,CI/TST: html tests are failing with 404 for fdic.gov url,jorisvandenbossche,closed,2020-06-20T12:22:58Z,2020-06-20T13:49:37Z,See eg https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=37674&view=logs&jobId=a67b4c4c-cd2e-5e3c-a361-de73ac9c05f9&j=a67b4c4c-cd2e-5e3c-a361-de73ac9c05f9&t=33d2fdd0-c376-5f94-e6d3-957bdd23a3b8
642371383,34895,"TST: skip gbq integration tests, xref #34779",jreback,closed,2020-06-20T12:56:48Z,2020-06-20T14:43:35Z,xref #34779 
640349074,34847,Remove redundant lists in array.py,cool-RR,closed,2020-06-17T10:58:46Z,2020-06-20T14:47:24Z,As far as I know these lists do nothing but taking up memory.
642336935,34879,TST: Add test to verify 'dropna' behaviour on SparseArray,MBrouns,closed,2020-06-20T08:38:43Z,2020-06-20T15:08:10Z,"verify that calling `dropna` on a pd.SparseArray does not inadvertently drop non-na records on both DataFrames and the SparseArray itself.

- [x] closes #28287
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry (not applicable?)
"
389036769,24171,IntegerNA Support for DataFrame.diff(),ededovic,closed,2018-12-09T16:03:59Z,2020-06-20T16:50:33Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd, numpy as np

df = pd.DataFrame({'a': np.arange(1,6),
                   'b': np.arange(1,6)+2,
                   'c': np.arange(1,6)**2})
print(f""all columns are int: \n{df}"")
df.loc[2,'b'] = np.nan
print(f""if one value in column b is set to NA, then the whole column in set to float:\n{df}"")
print(f'column b is float, and column a is int, then there is no result for anything for column b-a:\n{df.diff(axis=1)}')
print(f'if the whole dataframe is casted to float, then diff works:\n{df.astype(np.float).diff(axis=1)}')

```
#### Problem description
If for example, in the dataframe of integers, we set (2,b) as NA, then the entire column b becomes float. When we attempt to use df.diff(axis=1), then column b-a gives all NaN because float - int gives NaN. 
In addition, c-b does not even happen, c-a is done instead as only c and a columns are integers. 
Both of these appear unexpected results as in regular python, float - int is a float, and providing results in column c as c-a instead of c-b is conflicting with the definition of the diff function period = 1. 

The workaround to this problem is to cast the entire dataframe to float ```df.astype(np.float).diff(axis=1) ``` and it works. I think this should be fixed in pandas or at least a warning should be given. 


Here is steps by step. Simple dataframe of integers:
```
   a  b   c
0  1  3   1
1  2  4   4
2  3  5   9
3  4  6  16
4  5  7  25
```

If one value in column b is set to NA, then the whole column in set to float:
```
   a    b   c
0  1  3.0   1
1  2  4.0   4
2  3  NaN   9
3  4  6.0  16
4  5  7.0  25
```
when we attempt to do ```df.diff(axis=1)```, then column b-a gives all NaN, however, instead of doing c-b, c-a is done instead, it is only doing difference between the same type columns. For example, in the row 4, 25 - 5 = 20, which means c-a was done instead of c-b:
```
    a   b     c
0 NaN NaN   0.0
1 NaN NaN   2.0
2 NaN NaN   6.0
3 NaN NaN  12.0
4 NaN NaN  20.0
```

if the whole dataframe is casted to float, then diff works as expected:
```
    a    b     c
0 NaN  2.0  -2.0
1 NaN  2.0   0.0
2 NaN  NaN   NaN
3 NaN  2.0  10.0
4 NaN  2.0  18.0
```
panda version 0.23.4. This issue appears in prior versions as well."
642360579,34889,TST: IntegerNA Support for DataFrame.diff(),Marvzinc,closed,2020-06-20T11:36:49Z,2020-06-20T16:50:37Z,"- [x] closes #24171
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Test for IntegerNA support DataFrame.diff().
``` bash
pytest tests/frame/methods/test_diff.py::TestDataFrameDiff::test_diff_integer_na
```
"
642363503,34890,DOC: add mention of recommended dependencies in users guide,annamontare-edu,closed,2020-06-20T11:59:46Z,2020-06-20T17:14:05Z,"First-time contributor here!

Ian Ozsvald mentioned installing the optional dependencies for pandas in his talk at PyData Amsterdam 2020. Although these are mentioned in the Getting Started section, I though they should be mentioned in the Users Guide, which is where I typically go for pandas information. It seems like many users aren't aware that these dependencies are available to increase performance.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
642375114,34901,DOC: explain EWM,KrishnaSai2020,closed,2020-06-20T13:23:25Z,2020-06-20T17:25:41Z,"- [ ] closes #34867
- [ ] tests  passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew - Made docs more clear and renamed files
"
642352505,34884,"DOC: fixed labels in ""Plotting with error bars""",marydmit,closed,2020-06-20T10:35:51Z,2020-06-20T18:26:53Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

x-labels should now be visible in the plot."
628010894,34502,CLN: GH29547 format with f-strings,DanBasson,closed,2020-05-31T17:52:39Z,2020-06-20T19:59:26Z,"replace .format() for f-strings in the following:

1. pandas/tests/series/indexing/test_numeric.py
2. pandas/tests/series/indexing/test_take.py
3. pandas/tests/series/indexing/test_where.py

"
630460039,34564,BUG: pd.Series/DataFrame.truncate() drops multiindex names,iyer,closed,2020-06-04T02:24:56Z,2020-06-20T20:04:28Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
# create multiindex with names L1/L2
mi = pd.MultiIndex.from_product([[1, 2, 3, 4], ['A', 'B']], names=['L1', 'L2'])
s1 = pd.Series(range(mi.shape[0]), index=mi)
# truncate drops the names in the index of s2
s2 = s1.truncate(before=2, after=3)

```

#### Problem description

The current version seems to drop index level names. This was not the case in pandas version 0.23.4

#### Expected Output
```
s1
Out[1]: 
L1  L2
1   A     0
    B     1
2   A     2
    B     3
3   A     4
    B     5
4   A     6
    B     7
dtype: int64

# In pandas 0.23.4
s2
Out[2]: 
L1  L2
2  A    2
   B    3
3  A    4
   B    5
dtype: int64

# In pandas 1.0.3 (drops axis labels L1, L2 along truncation axis)
s2
Out[2]: 
2  A    2
   B    3
3  A    4
   B    5
dtype: int64
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.13-1.el7.elrepo.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.4.0.post20200518
Cython           : 0.29.17
pytest           : 5.4.2
hypothesis       : None
sphinx           : 3.0.3
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : None
numba            : 0.49.1

</details>
"
590811175,33166,DOC: document support for in-memory HDFStore,choucavalier,closed,2020-03-31T05:39:30Z,2020-06-20T20:08:40Z,"It seems to be currently imposible to have an in-memory HDFStore. 

Any of my attempts at using `io.BytesIO` failed.

This can be useful for applications where the bytes of the resulting HDF file need to be sent over the network."
642354740,34888,DOC: document support for in-memory HDFStore GH33166,dennisbakhuis,closed,2020-06-20T10:52:30Z,2020-06-20T20:08:44Z,"I currently have added to the docstring that **kwargs passes its parameters to
PyTables. Maybe this is too much but I also added an example using **kwargs,
passing the driver paramter to create an in-memory HDFStore.

Furthermore, I have added HDFStore class to the reference api, as it was not
autogenerated and also made **kwargs more clear that it passes its parameters
to PyTables.

Added in the cookbook the method of creating in-memory HDFStores, including
an example.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

closes #33166 "
642414087,34911,TST: Verify whether Datetime subclasses are also of dtype datetime,avinashpancham,closed,2020-06-20T17:23:13Z,2020-06-20T22:17:52Z,"- [x] closes #21142
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
324706061,21142,BUG: Datetime subclasses are no longer datetime64 types,fjetter,closed,2018-05-20T12:34:06Z,2020-06-20T22:17:52Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import pandas as pd
>>> from datetime import datetime
>>> class MyDatetime(datetime):
...             pass
>>> df_subclass = pd.DataFrame({""datetime"": [MyDatetime(2018, 1, 1, 1, 1)]})
>>> df_subclass.dtypes
datetime    object
dtype: object

>>> df_dt = pd.DataFrame({""datetime"": [datetime(2018, 1, 1, 1, 1)]})
>>> df_dt.dtypes
datetime    datetime64[ns]
dtype: object
```
#### Problem description

Since version 0.23.0 pandas treats subclasses of datetime  no longer as `datetime64[ns]`

#### Expected Output

Same behavior as in pre 0.23.0 versions, i.e. subclasses of datetime are of type `datetime64[ns]`

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit: None
python: 3.6.4.final.0
python-bits: 64
OS: Darwin
OS-release: 16.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.0
pytest: 3.4.0
pip: 9.0.1
setuptools: 38.5.1
Cython: 0.27.3
numpy: 1.14.3
scipy: None
pyarrow: 0.8.1.dev163+gb33dfd9
xarray: None
IPython: 6.2.1
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
641568717,34867,DOC: pandas/pandas/core/window/ewm.py,db-baron,closed,2020-06-18T21:46:47Z,2020-06-20T22:20:59Z,"#### Location of the documentation
Docstring for `class EWM(_Rolling)` in ewm.py.

#### Documentation problem
The purposes of the `EWM` class is unclear because there's no documentation on what 'ewm' is abbreviating. One reason this is confusing is that `ewm` can easily be interpreted as either ""exponentially weighted mean"" or ""exponentially weighted moving"". The `EWM` class docstring actually muddies the water further by stating that the class provides ""exponential weighted **(EW)** functions"".

#### Suggested fix for documentation
If it's absolutely necessary to use an abbreviation like `ewm` then the docstring needs to clearly state what it's abbreviating. "
551562154,31111,Strange groupby apply behaviour,endremborza,closed,2020-01-17T18:18:39Z,2020-06-20T22:24:25Z,"#### Minimal reproducible example

```python
import pandas as pd
import numpy as np

tdf = pd.DataFrame({""tree"": [0, 0, 0, 0, 1, 1, 1, 1],
                    ""into"": [""0-2"", ""0-4"", ""0-10"", np.nan, ""1-2"", ""1-3"", ""1-7"", np.nan],
                    ""leaf_value"": [0,0,0,3,0,0,0,4]},
                   index=[""0-0"", ""0-2"", ""0-4"", ""0-10"", ""1-0"", ""1-1"", ""1-2"", ""1-7""])

def deduce_tree(df):
    print(""DEDUCING TREE WITH INDEX:\n"",df.index)
    next_id = df.index[0]
    while isinstance(next_id, str):
        print(next_id)
        next_node = df.loc[next_id, :]
        next_id = df.loc[next_id, ""into""]
    print(""RETURNING:\n"", next_node)
    return next_node

tdf.groupby(""tree"").apply(deduce_tree)

```
<details>

DEDUCING GREE WITH INDEX:
 Index(['0-0', '0-2', '0-4', '0-10'], dtype='object')
0-0
0-2
0-4
0-10
RETURNING:
 tree            0
into          NaN
leaf_value      3
Name: 0-10, dtype: object
DEDUCING GREE WITH INDEX:
 Index(['1-0', '1-1', '1-2', '1-7'], dtype='object')
1-0
DEDUCING GREE WITH INDEX:
 Index(['0-0', '0-2', '0-4', '0-10'], dtype='object')
0-0
0-2
0-4
0-10
RETURNING:
 tree            0
into          NaN
leaf_value      3
Name: 0-10, dtype: object
DEDUCING GREE WITH INDEX:
 Index(['1-0', '1-1', '1-2', '1-7'], dtype='object')
1-0
1-2
1-7
RETURNING:
 tree            1
into          NaN
leaf_value      4
Name: 1-7, dtype: object

</details>

#### Problem description

This came up when trying to analyze a boosted tree internals. As the apply function gets called with a print statement, when it gets to the line `next_node = df.loc[next_id, :]` it just calls the deduce_tree function again, with the group 0.
it prints out `DEDUCING TREE WITH INDEX` 3 times as opposed to 2, and for some reason interrupts the function.

the result of `tdf.groupby(""tree"").apply(deduce_tree)` is correct, but it seems to do some unnecessary work and if I want to implement some side effects into `deduce_tree` it gets messed up.

Can anyone explain why it works like this? Is this some bug? How can a .loc interrupt a function?

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.2.1-1.el7.elrepo.x86_64
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2018.4
dateutil         : 2.7.3
pip              : 19.2.3
setuptools       : 41.0.1
Cython           : None
pytest           : 4.1.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.1 (dt dec pq3 ext lo64)
jinja2           : 2.10
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.2.8
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
642354623,34886,TST: Ensure dtypes are set correctly for empty integer columns #24386,avinashpancham,closed,2020-06-20T10:51:35Z,2020-06-20T22:26:14Z,"- [x ] closes #24386
- [x ] tests added / passed
- [x ] passes `black pandas`
- [x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x ] added tests to verify whether empty integer columns are loaded in as integer columns
"
642374823,34899,DOC: add note about the values of unit for pd.to_datetime,annamontare-edu,closed,2020-06-20T13:21:13Z,2020-06-20T23:53:59Z,"The `unit` option for `pandas.to_datetime` takes different format strings than the `format` option,
which makes sense, but caught me off-guard as a new user. It would be helpful if the documentation
mentioned this.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
631300718,34589,DataFrame.truncate drops MultiIndex names,arw2019,closed,2020-06-05T04:23:41Z,2020-06-21T03:24:54Z,"- [x] closes #34564
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
- [x] added tests
- [ ] passes tests (fixed for `DataFrame`, still issue for `Series`)"
642373803,34897,TST: groupby apply called multiple times,Rohith295,closed,2020-06-20T13:14:04Z,2020-06-21T06:06:14Z,"closes #31111 
xref https://github.com/pandas-dev/pandas/pull/24748
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ Added test for testing func should be called len(set(groupby_column)) times when passed to groupby.apply(func) ] whatsnew entry
"
642413656,34910,DOC: explain EWM ,KrishnaSai2020,closed,2020-06-20T17:20:06Z,2020-06-21T08:14:13Z,"- [ ] closes #34867
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
597316581,33422,ENH: update feather IO for pyarrow 0.17 / Feather V2,jorisvandenbossche,closed,2020-04-09T14:19:31Z,2020-06-21T17:26:14Z,"Upcoming pyarrow 0.17 release will include an upgraded feather format. 

This PRs updates pandas for that, more specifically ensures the new keywords can be passed through (the basics should keep working out of the box, since the public API did not change), and small update to the tests "
619808951,34229,DOC: Improve docstring of Series/DataFrame.bool,datapythonista,closed,2020-05-17T22:14:29Z,2020-06-22T06:32:50Z,"Not a big fan of this method, I'd deprecate it, in favor of `bool(df.squeeze())`. But since it's quite tricky, let's at least have the docstring as clear as possible."
629853948,34544,DOC: DatetimeIndex day says the same as month,VictorGerritsen,closed,2020-06-03T09:46:35Z,2020-06-22T07:23:32Z,"#### Location of the documentation

https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.day.html#pandas.DatetimeIndex.day

#### Documentation problem

Right now it says the same as month, `The month as of January=1, December=12`

#### Suggested fix for documentation

Something along the lines of `Day of the month, from 1 through 31` 
"
640019943,34833,BUG: resample seems to convert hours to 00:00 ,liverpool1026,closed,2020-06-16T22:47:30Z,2020-06-22T14:35:51Z,"- [x] I have checked that this issue has not already been reported. (As far as I can see by using the search)

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import datetime as dt
import pandas as pd

start_time = dt.datetime(year=2018, month=6, day=7, hour=18)
end_time = dt.datetime(year=2018, month=6, day=27, hour=18)
time_index_df = pd.date_range(
    start_time,
    end_time,
    freq=""12H"",
    name=""datetime"",
).to_frame(index=False)

time_index_df[""test""] = 1
time_index_df[""name""] = ""Name""

time_index_df = time_index_df.resample(""2d"", convention=""end"", on=""datetime"").mean().reset_index()

print(time_index_df[""datetime""].values)

time_index_df.to_csv(f""test_resample_{pd.__version__}"")

```

#### Problem description
On pandas 0.23 the behaviour of resample will keep the correct datetime (18:00 in this case) after resample.
But starting from 0.24, after resample, the datetime is now converted to (00:00).
[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

#### Expected Output
On. 0.23
['2018-06-07T18:00:00.000000000' '2018-06-09T18:00:00.000000000'
 '2018-06-11T18:00:00.000000000' '2018-06-13T18:00:00.000000000'
 '2018-06-15T18:00:00.000000000' '2018-06-17T18:00:00.000000000'
 '2018-06-19T18:00:00.000000000' '2018-06-21T18:00:00.000000000'
 '2018-06-23T18:00:00.000000000' '2018-06-25T18:00:00.000000000'
 '2018-06-27T18:00:00.000000000']

But Starting from 0.24 it is giving me
['2018-06-07T00:00:00.000000000' '2018-06-09T00:00:00.000000000'
 '2018-06-11T00:00:00.000000000' '2018-06-13T00:00:00.000000000'
 '2018-06-15T00:00:00.000000000' '2018-06-17T00:00:00.000000000'
 '2018-06-19T00:00:00.000000000' '2018-06-21T00:00:00.000000000'
 '2018-06-23T00:00:00.000000000' '2018-06-25T00:00:00.000000000'
 '2018-06-27T00:00:00.000000000']


#### Output of ``pd.show_versions()``


<details>


0.23
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.9.final.0
python-bits: 64
OS: Linux
OS-release: 5.3.0-59-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_AU.UTF-8
LOCALE: en_AU.UTF-8

pandas: 0.23.0
pytest: None
pip: 20.0.2
setuptools: 46.0.0
Cython: None
numpy: 1.18.5
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.8.1
pytz: 2020.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: 2.5.8
xlrd: 0.9.4
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: 1.2.19
pymysql: None
psycopg2: 2.8.5 (dt dec pq3 ext lo64)
jinja2: 2.11.2
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
None


0.24 
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.9.final.0
python-bits: 64
OS: Linux
OS-release: 5.3.0-59-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_AU.UTF-8
LOCALE: en_AU.UTF-8

pandas: 0.24.2
pytest: None
pip: 20.0.2
setuptools: 46.0.0
Cython: None
numpy: 1.18.5
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.8.1
pytz: 2020.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: 2.5.8
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: 1.2.19
pymysql: None
psycopg2: 2.8.5 (dt dec pq3 ext lo64)
jinja2: 2.11.2
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
None


</details>
"
642436127,34913,REF: dont use compute_reduction,jbrockmendel,closed,2020-06-20T19:55:20Z,2020-06-22T16:37:30Z,"Full asv run shows no change.

This gets rid of one usage of libreduction.compute_reduction; #34909 gets rid of the other."
643338796,34938,Remove exploded variable,erfannariman,closed,2020-06-22T20:25:37Z,2020-06-23T12:21:23Z,"- [x] closes #34934
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
597825279,33452,"ENH: Use fsspec for reading/writing from/to S3, GCS, Azure Blob, etc.",jrderuiter,closed,2020-04-10T10:42:26Z,2020-06-23T13:43:25Z,"#### Is your feature request related to a problem?

Currently pandas has some support for S3 and GCS using the pandas.io.{gcs,s3} modules, which are based on S3fs and gcsfs. 

It seems like we could easily broaden the support for different filesystems by leveraging the fsspec library (https://pypi.org/project/fsspec/) and its interface implementations (see https://github.com/intake/filesystem_spec/blob/master/fsspec/registry.py for some examples) to read/write files in pandas. 

This way, we would also be able to use filesystems such as Azure-based storage systems directly from pandas. 

#### Describe the solution you'd like

I'd like to be able to use the different file systems supported by fsspec in pandas with something like:

```
import pandas as pd

df1 = pd.read_csv(""abfs://my_container/my_file.csv"")
df1.to_json(""file:///some/local/path.json"") # Also works without file:// prefix.

df2 = pd.read_csv(""s3://my_bucket/my_file.csv"")
...
```

#### API breaking implications

In principle, it looks as if we could cover most of the work by adapting get_filepath_or_buffer in pandas/io/common.py to use fsspec. We would of course have to test if fsspec doesn't break anything compared to the current implementations.

One challenge is that some storage systems require extra arguments (called storage options in fsspec). For example, Azure blob requires the user to pass two storage options (account_name and account_key) to be able to access the storage. We would need to consider how to pass these options to the correct methods, either by (a) setting these options globally for a given type of storage or (b) passing the options through the pd.read_* functions and pd.DataFrame.to_* methods. 

#### Describe alternatives you've considered

This seems like a change that would have a small impact, as pandas already uses S3fs and gcsfs, which are both implementations of the broader fsspec interface. It should also provide support for a great number of different filesystems with minimal changes, compared to adding support for each filesystem ourselves. As such, I would think it is the preferred approach. 

Another approach would be to add support for each additional filesystem as it comes along. This however would require adding and maintaining code for each filesystem, which seems less preferable.

#### Additional context

I recently implemented similar wrappers for pandas code at one of our clients, and am therefore somewhat familiar with fsspec etc. I would be happy to see if we can contribute these ideas + code to the pandas project.
"
621247327,34266,ENH: add fsspec support,martindurant,closed,2020-05-19T19:59:53Z,2020-06-23T13:57:28Z,"Supersedes https://github.com/pandas-dev/pandas/pull/33549
closes #33452

- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
641552924,34866,[WIP]REGR: Fixed reading from public S3 buckets with credentials,TomAugspurger,closed,2020-06-18T21:13:41Z,2020-06-23T14:01:32Z,"Closes https://github.com/pandas-dev/pandas/issues/34626

This works in 1.0.4 I think, so no whatsnew.

I'd like to wait for https://github.com/pandas-dev/pandas/pull/34266 to get in first. I'll need to update things.

And in the future, I'd like to deprecate this behavior in favor of something like

```python
pd.read_csv(""s3://path/to/key"", storage_options={""anon"": True})
```

It should be easy to detect the cases when we need to do that. We just don't have the `storage_options` kwarg yet."
643883041,34950,Correct the misprint at advanced.rst,glechic,closed,2020-06-23T14:19:32Z,2020-06-23T16:02:30Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
627566680,34467,BUG: read_parquet no longer supports file-like objects,claytonlemons,closed,2020-05-29T21:56:30Z,2020-06-23T16:04:00Z,"
#### Code Sample, a copy-pastable example

```python
from io import BytesIO
import pandas as pd

buffer = BytesIO()

df = pd.DataFrame([1,2,3], columns=[""a""])
df.to_parquet(buffer)

df2 = pd.read_parquet(buffer)
```

#### Problem description

The current behavior of `read_parquet(buffer)` is that it raises the following exception:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""./working_dir/tvenv/lib/python3.7/site-packages/pandas/io/parquet.py"", line 315, in read_parquet
    return impl.read(path, columns=columns, **kwargs)
  File ""./working_dir/tvenv/lib/python3.7/site-packages/pandas/io/parquet.py"", line 131, in read
    path, filesystem=get_fs_for_path(path), **kwargs
  File ""./working_dir/tvenv/lib/python3.7/site-packages/pyarrow/parquet.py"", line 1162, in __init__
    self.paths = _parse_uri(path_or_paths)
  File ""./working_dir/tvenv/lib/python3.7/site-packages/pyarrow/parquet.py"", line 47, in _parse_uri
    path = _stringify_path(path)
  File ""./working_dir/tvenv/lib/python3.7/site-packages/pyarrow/util.py"", line 67, in _stringify_path
    raise TypeError(""not a path-like object"")
TypeError: not a path-like object
```

#### Expected Output

Instead, `read_parquet(buffer)` should return a new DataFrame with the same contents as the serialized DataFrame stored in `buffer`

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-99-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.4
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 9.0.1
setuptools       : 39.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 0.999999999
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
642832032,34930,CLN: Update Cython data pointers for rolling apply,mroeschke,closed,2020-06-22T07:35:46Z,2020-06-23T16:37:22Z,"- [x] xref #34014
- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Performance looks pretty comparable between master and this PR

```
N = 10 ** 3
arr = 100 * np.random.random(N)
roll = pd.DataFrame(arr).rolling(10)
%timeit roll.apply(lambda x: np.sum(x) + 5, raw=True)

4.48 ms ± 14.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) <--PR
4.34 ms ± 29.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) <--master
```"
541492636,30409,CLN: str.format -> f-strings for `io/sas`,jyscao,closed,2019-12-22T19:32:02Z,2020-06-23T17:09:05Z,"- [x] ref #29547 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
541832770,30430,CLN: use f-string for JSON related files,jyscao,closed,2019-12-23T17:01:30Z,2020-06-23T17:09:06Z,"- [x] ref #29547 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
630680882,34569,DOC: Wrong description of behaviour Series.pop(),bestrandomnameever,closed,2020-06-04T09:54:31Z,2020-06-23T17:35:08Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.pop.html

#### Documentation problem

Documentation describes that the operation would not return the popped column and would instead return the modified version of the Series (making it appear as a non-inplace function). Actual behavior however returns the value of the popped column and executes the operation in-place.

#### Suggested fix for documentation

Use the documentation of the similar and well described method of Dataframe.pop() and adjust it for Series (return a value instead of a Series)."
631835603,34606,#34569 Added proper description for pandas.Series.pop,2796gaurav,closed,2020-06-05T18:35:17Z,2020-06-23T17:35:15Z,"- [x] closes #34569
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Updated documentation will look like this: https://i.imgur.com/Tl9JO5X.png
This is my first commit to the project. If any changes/modifications let me know.
"
323160886,21049,BUG: `Series.shift` fails with non-writable array,evanzd,closed,2018-05-15T10:37:07Z,2020-06-23T17:35:57Z,"### Problem

```python
import numpy as np
import pandas as pd

arr = np.empty(shape=(0,))
arr.setflags(write=0)
pd.Series(arr).shift(1)
```
will raise an Exception:

`ValueError: assignment destination is read-only`.

Update: all pandas versions from 0.16 to current master have this bug.

### Solution

Here `maybe_upcast` will return exactly the same array, the write permission keeps the same (non-writable):
https://github.com/pandas-dev/pandas/blob/186ce4e772c8884b5bc87ab767adc94d6870a4e0/pandas/core/internals.py#L1285

Here since it's an empty array, `np.roll` will not be executed so `new_values` keeps non-writable. That's where the exception is:
https://github.com/pandas-dev/pandas/blob/186ce4e772c8884b5bc87ab767adc94d6870a4e0/pandas/core/internals.py#L1293

Maybe add an `else` condition can fix this:
```
if np.prod(new_values.shape):
    new_values = np.roll(new_values, _ensure_platform_int(periods),
                         axis=axis)
else:
    new_values = self.values.copy()
```"
642529061,34919,TST: Verify whether non writable numpy array is shiftable (21049),avinashpancham,closed,2020-06-21T09:31:55Z,2020-06-23T17:36:04Z,"- [x ] closes #21049
- [x ] tests added / passed
- [x ] passes `black pandas`
- [x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x ] whatsnew entry
"
503553839,28823,Feature Request: Population Covariance Calculation with ddof parameter for DataFrame.cov(),chrisyeh96,closed,2019-10-07T16:16:12Z,2020-06-23T17:37:58Z,"I propose adding a `bias` parameter in [`DataFrame.cov()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cov.html). Currently, Pandas v0.25.1 does not have such a parameter.

The default would be `bias=False`, which would compute sample covariance with a `N-1` factor in the denominator. When `bias=True`, this would compute population covariance with `N` in the denominator.

This follows the same semantics as the [`numpy.cov()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html) function. Since `DataFrame.cov()` uses `numpy.cov()`, I hope this would not be too hard to implement."
631953546,34611,Add ddof to cov methods,blueprintarchitect,closed,2020-06-05T20:57:50Z,2020-06-23T17:38:04Z,"- [ x] closes #28823
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
642542652,34920,TST : Added test for creating empty dataframe with column of type str…,prakhar987,closed,2020-06-21T11:12:13Z,2020-06-23T17:45:33Z,"- [x] closes #34915
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
642455631,34915,BUG: Cannot create empty dataframe having columns with dtype string,impredicative,closed,2020-06-20T22:24:33Z,2020-06-23T17:45:33Z,"- [x] I have checked that this issue has not already been reported.

I have checked, but this requirement is grossly unreasonable since there are currently 3,426 other open issues. It's impossible for anyone to be sure. This requirement should be reworded.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python-traceback
>>> import pandas as pd

>>> pd.DataFrame(columns=['c1'], dtype='string')
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""/home/homeuser/PycharmProjects/.venv/myenv/lib/python3.8/site-packages/pandas/core/frame.py"", line 435, in __init__
    mgr = init_dict(data, index, columns, dtype=dtype)
  File ""/home/homeuser/PycharmProjects/.venv/myenv/lib/python3.8/site-packages/pandas/core/internals/construction.py"", line 234, in init_dict
    if dtype is None or np.issubdtype(dtype, np.flexible):
  File ""/home/homeuser/PycharmProjects/.venv/myenv/lib/python3.8/site-packages/numpy/core/numerictypes.py"", line 388, in issubdtype
    arg1 = dtype(arg1).type
TypeError: Cannot interpret 'StringDtype' as a data type
```

#### Problem description
These three alternatives work, so there is no reason for this bug to continue to not be fixed:
```python-traceback
>>> pd.DataFrame(columns=['c1'], dtype='str')
Empty DataFrame
Columns: [c1]
Index: []

>>> pd.DataFrame([{}], columns=['c1'], dtype='string')
     c1
0  <NA>

>>> pd.DataFrame([{}], columns=['c1'], dtype='string').drop(labels=0)
Empty DataFrame
Columns: [c1]
Index: []
```

#### Expected Output
```python-traceback
Empty DataFrame
Columns: [c1]
Index: []
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-1087-oem
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 40.8.0
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.1
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
642805427,34929,BUILD: make tests discoverable in .devcontainer.json,MarcoGorelli,closed,2020-06-22T06:47:33Z,2020-06-23T17:50:12Z,"I wasn't able to discover tests using the current setup, I had to use ""python.testing.pytestArgs"" instead of ""python.testing.cwd"""
643306188,34934,BUG: unnecessary repetition of variable in explode test,erfannariman,closed,2020-06-22T19:22:25Z,2020-06-23T18:02:35Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

In the test `test_typical_usecase` in `series/methods/test_explode.py` there's 

```python
def test_typical_usecase():

    df = pd.DataFrame(
        [{""var1"": ""a,b,c"", ""var2"": 1}, {""var1"": ""d,e,f"", ""var2"": 2}],
        columns=[""var1"", ""var2""],
    )
    exploded = df.var1.str.split("","").explode()
    exploded
    result = df[[""var2""]].join(exploded)
    expected = pd.DataFrame(
        {""var2"": [1, 1, 1, 2, 2, 2], ""var1"": list(""abcdef"")},
        columns=[""var2"", ""var1""],
        index=[0, 0, 0, 1, 1, 1],
    )
    tm.assert_frame_equal(result, expected)
```

#### Problem description

The variable `exploded` is called unnecessary.

"
643767051,34949,CLN: Removed unnecessary variable call,erfannariman,closed,2020-06-23T11:37:24Z,2020-06-23T18:03:35Z,"- [x] closes #34934
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
642398883,34907,TST: pandas/test/window/ changes for #30999,boweyism,closed,2020-06-20T15:43:54Z,2020-06-23T19:29:03Z,"- [ ] xref  #30999 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I've made changes for:
- tests/window/moments/test_moments_rolling.py
- tests/window/test_dtypes.py
- tests/window/test_ewm.py
- tests/window/test_expanding.py
- tests/window/test_timeseries_windows.py

In a couple of places there were comments stating the raised error's output. The `msg` variable contains the same information. Should I remove these coments?

Belongs to issue: https://github.com/pandas-dev/pandas/issues/30999"
597819256,33451,DOC: Be explicit whether a view or copy is returned,FSpanhel,closed,2020-04-10T10:27:07Z,2020-06-23T19:45:34Z,"#### Location of the documentation
For instance, 
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html 

#### Documentation problem

Sometimes the documentation is explicit that a copy is returned, e.g., https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html -> ""Returns a new object ..."", sometimes it is not or not explicitly stated, e.g., https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html -> ""Drop specified labels from rows or columns.""

#### Suggested fix for documentation

I wrongly assumed that pandas.DataFrame.drop returns a view because the first line of the docstring ""Drop specified labels from rows or columns."" suggests that, in my view. Later it is mentioned what happens if you set inplace = True. But you can only assume what happens if the default argument is used. Thus, the documentation is not clear w.r.t. this point and may be confusing to users, see #33438 or #30484. 

I would suggest that the documentation reads `""Drop specified labels from rows or columns and return a new object""' and/or ""inplace : bool, default False. If False, return a copy. If True, do operation inplace and return None."" This also applies to other methods that have the inplace parameter.

Since the use of inplace = True seems to be discouraged (#30484) it would also make sense to mention that in the documentation?
"
336889932,21675,BUG: unstack() does not always sort index in 0.23,jzwinck,closed,2018-06-29T07:18:15Z,2020-06-23T20:13:52Z,"#### Code Sample
```python
import pandas as pd
index = pd.MultiIndex(levels=[['A','B','C','D','E']] * 2, labels=[[4,4,4,3], [4,2,0,1]])
pd.Series(0, index).unstack()
```

#### Problem description

In Pandas 0.20, 0.21, and 0.22, this gave the expected result:
```
     A    B    C    E
D  NaN  0.0  NaN  NaN
E  0.0  NaN  0.0  0.0
```
But in Pandas 0.23, the result is not sorted:
```
     E    C    A    B
E  0.0  0.0  0.0  NaN
D  NaN  NaN  NaN  0.0
```

The documentation says *""The level involved will automatically get sorted""*, and while I've seen the explanation of confusing implementation details leaking out in #15105 and some other outright bugs in #9514, this seems to be a different bug, and a regression.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.4.final.0
python-bits: 64
OS: Linux
machine: x86_64
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.21.1
numpy: 1.13.1

</details>
"
642643098,34924,DOC: Fix language style,alimcmaster1,closed,2020-06-21T21:19:04Z,2020-06-23T20:50:06Z,"- Ref: https://9to5google.com/2020/06/12/google-android-chrome-blacklist-blocklist-more-inclusive/
- Ref: https://android-review.googlesource.com/c/platform/frameworks/ml/+/970739

cc @pandas-dev/pandas-core "
644050355,34957,QST: Behaviour of `mean()` for a Series of strings.,shwina,closed,2020-06-23T18:21:05Z,2020-06-23T21:41:07Z,"Greetings, Pandas devs! My question is about the behaviour of `mean()` for a Series of strings:

```
In [1]: import pandas as pd

In [2]: pd.Series(['1', '2']).mean()
Out[2]: 6.0
```

From https://github.com/pandas-dev/pandas/issues/34671#issuecomment-641927667, the result is `6` because:

> ""1"" + ""2"" is ""12"", and then we convert to numeric 12, and divide by the count

This feels like somewhat implicit behaviour that makes assumptions about how the mean is computed.

Would it be more desirable to throw here? Or perhaps return `1.5` instead?
"
295634504,19602,reset_index() on MultiIndexed empty dataframe does not preserve dtypes ,alberto-dellera,closed,2018-02-08T19:21:52Z,2020-06-23T22:07:01Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame( data=[[0,0,0]], columns=['level_1','level_2','payload'] )

# make dataframe empty
df = df[ df.payload == -1 ]

# columns are all int64 here

print(df.info())
#output: level_1    0 non-null int64
#output: level_2    0 non-null int64
#output: payload    0 non-null int64

# set MultiIndex - levels are still int64 
df = df.set_index(['level_1','level_2'])

print(str(df.index.levels[0].dtype))
print(str(df.index.levels[1].dtype))
#output: int64
#output: int64

# reset_index - former-levels columns are now float64
df = df.reset_index()

print(df.info())
#output: level_1    0 non-null float64
#output: level_2    0 non-null float64
#output: payload    0 non-null int64
```
#### Problem description
The dtypes are preserved instead if either  
 a) index is not a MultiIndex 
 b) dataframe is not empty

(b) is a big issue for programs that calculate subset of dataframes that sometimes
can be empty, since downstream code might expect a certain dtype and fail when it finds
a float64 instead. 

Real-world scenario: sampling a system (a collection of processes or threads) at regular intervals,
and collecting some measures (cpu used, or other resources or figures); a very common strategy 
in performance investigation software (e.g. check Oracle's v$active_session_history). 
Here, the natural index is (sample_time, process_id), sample_time being datetime64 (a Time Series).
Even more naturally, we want to computes differences of sample_time, yielding a timedelta64,
and divide it by np.timedelta64(1,'s') to get the elapsed time in seconds; but when the 
initial dataframe is empty, we try to divide float64 / np.timedelta64(1,'s') and get an exception.

An obvious workaround is to check for empty dataframes after EVERY reset_index()
and coerce the float64s back to their correct value - but that easily becomes a maintenance/coverage nightmare :O

#### Expected Output
resetted columns having their initial dtype

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.3.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.22.0
pytest: None
pip: 9.0.1
setuptools: 36.5.0.post20170921
Cython: None
numpy: 1.13.3
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: None
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.3
blosc: None
bottleneck: None
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 2.1.1
openpyxl: 2.4.9
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None


</details>
"
643377979,34940,TST: disallow bare pytest raises,boweyism,closed,2020-06-22T21:40:18Z,2020-06-23T22:08:43Z,"xref #30999 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry


Changes for:
-    pandas/tests/frame/methods/test_assign.py
-    pandas/tests/frame/methods/test_at_time.py
-    pandas/tests/frame/methods/test_between_time.py
-    pandas/tests/frame/methods/test_first_and_last.py
-    pandas/tests/frame/methods/test_interpolate.py
-    pandas/tests/frame/methods/test_replace.py
-    pandas/tests/frame/test_query_eval.py


Belongs to https://github.com/pandas-dev/pandas/issues/30999"
642397100,34905,BUG: plotting layout patch,lyashevska,closed,2020-06-20T15:32:42Z,2020-06-23T22:36:25Z,"This fixes a layout problem.  
"
644351054,34964,I had this error please tell me the solution,hamzajamil12,closed,2020-06-24T06:19:22Z,2020-06-24T07:19:28Z,"
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-14-71bf30a97169> in <module>
----> 1 df=pd.read(""datasets_33180_43520_heart_csv.py"")

~\miniconda3\lib\site-packages\pandas\__init__.py in __getattr__(name)
    261             return _SparseArray
    262 
--> 263         raise AttributeError(f""module 'pandas' has no attribute '{name}'"")
    264 
    265 

AttributeError: module 'pandas' has no attribute 'read'
"
643966900,34955,DOC: Clarify in pandas.DataFrame.drop that a copy is returned when inplace = False.,marinomaria,closed,2020-06-23T16:06:59Z,2020-06-24T08:10:19Z,"Clarify in `pandas.DataFrame.drop` that a copy is returned when `inplace = False`.

- [x] closes #33451  
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
572608019,32322,Fix exception causes all over the code,cool-RR,closed,2020-02-28T08:37:25Z,2020-06-24T08:34:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
643325838,34936,BUG: pandas.DataFrame.interpolate fails with high value of  `limit` argument,monstrorivas,open,2020-06-22T20:00:53Z,2020-06-24T08:59:02Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np
df = pd.DataFrame([1]*500000)
df.iloc[1000:50000] =np.nan
df.interpolate(method='linear', limit_direction='both', limit=None)  # This runs fine eventhough the limit is effectively > 5000 datapoints
df.interpolate(method='linear', limit_direction='both', limit=5000)  # This produces an error
```

#### Problem description
An error is produced when specifying a large limit in pandas.DataFrame.Interpolate
~~The error is NOT present in pandas 1.0.1 but it is present at least in 1.0.4 and 1.0.5~~
If the limit is set to None, there is no error... even when the interpolated consecutive nans is larger than the limit that fails

Error:
```
ValueError: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.
```
The error is with Python 3.7 but not with Python 3.6

#### Expected Output
~~The expected output is what pandas v1.0.1 produces.~~

In python 3.6, specifying a large value of limit doesn't result in a ValueError

This runs in v1.0.1
```python
import pandas as pd
import numpy as np
df = pd.DataFrame([1]*500000)
df.iloc[1000:50000] =np.nan
dff = df.interpolate(method='linear', limit_direction='both', limit=5000) 
assert dff.isna().sum().values == 39000
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 32
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : None.None

pandas           : 1.0.5
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.1.1
Cython           : 0.29.20
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
644060376,34960,DOC: misc sphinx directive fixes,simonjayhawkins,closed,2020-06-23T18:38:57Z,2020-06-24T13:05:24Z,
643515105,34946,BUG: Inconsistent behavior: Occasional crash on same code and data,arshadparvez,closed,2020-06-23T04:11:52Z,2020-06-24T13:25:58Z,"[x] I have checked that this issue has not already been reported.
[x]  I have confirmed this bug exists on the latest version of pandas.

**Description**
When reading csv with pd.read_csv, giving both names= and dtypes= arguments, the program occasionally crashes. Surprisingly, sometimes the application completes its execution cycle normally - without any change in code or data.

I had initially used only the _dtype_ argument, but the first row of my data would be considered as column headers (the csv, otherwise has no headers). Instead of using headers=None, I added _names_ argument, which on one hand made all my rows read as data while on the other set the desired column headers in DataFrame.

But after this change, the program's behavior has become inconsistent; occasional crashing while normal execution at other times.

### Reproducing Code Example
**Example Code and Data**
Minimal code along with a small data file, that demonstrate the reported behavior, is:
 
```python
import pandas as pd

df = pd.read_csv('data\\test3.csv', names={'pVal1', 'pClass'}, dtype={'pVal1':int, 'pClass':bool})
print (df)
```
The data file test3.csv contains:
```
1,TRUE
2,FALSE
3,TRUE
```

The problem goes away if we remove either of the two arguments (names, dtype). With any of the following statements, the program executes normally:
```
df = pd.read_csv('data\\test3.csv', names={'pVal1', 'pClass'})
```
or
```
df = pd.read_csv('data\\test3.csv', dtype={'pVal1':int, 'pClass':bool})
```

### Error message:
**Demonstration of Inconsistent Behaviour**

I am posting the result of two consecutive executions; first successful, the second failing with error:

**Successful Execution**
(ml) C:\Users\pyuser\projs\ml>py err.py
   pVal1  pClass
0      1    True
1      2   False
2      3    True

**Crash with Error**
(ml) C:\Users\pyuser\projs\ml>py err.py
Traceback (most recent call last):
  File ""pandas\_libs\parsers.pyx"", line 1152, in pandas._libs.parsers.TextReader
._convert_tokens
TypeError: Cannot cast array from dtype('int64') to dtype('bool') according to t
he rule 'safe'

During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""err.py"", line 3, in <module>
    df = pd.read_csv('data\\test3.csv', names={'pVal1', 'pClass'}, dtype={'pVal1
':int, 'pClass':bool})
  File ""C:\Users\pyuser\projs\ml\lib\site-packages\pandas\io\parsers.py"", line 676
, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""C:\Users\pyuser\projs\ml\lib\site-packages\pandas\io\parsers.py"", line 454
, in _read
    data = parser.read(nrows)
  File ""C:\Users\pyuser\projs\ml\lib\site-packages\pandas\io\parsers.py"", line 113
3, in read
    ret = self._engine.read(nrows)
  File ""C:\Users\pyuser\projs\ml\lib\site-packages\pandas\io\parsers.py"", line 203
7, in read
    data = self._reader.read(nrows)
  File ""pandas\_libs\parsers.pyx"", line 860, in pandas._libs.parsers.TextReader.
read
  File ""pandas\_libs\parsers.pyx"", line 875, in pandas._libs.parsers.TextReader.
_read_low_memory
  File ""pandas\_libs\parsers.pyx"", line 952, in pandas._libs.parsers.TextReader.
_read_rows
  File ""pandas\_libs\parsers.pyx"", line 1084, in pandas._libs.parsers.TextReader
._convert_column_data
  File ""pandas\_libs\parsers.pyx"", line 1160, in pandas._libs.parsers.TextReader
._convert_tokens
ValueError: cannot safely convert passed user dtype of bool for int64 dtyped dat
a in column 0

**Software and Library Versions**
### Numpy/Python version information:
(ml) C:\Users\pyuser\projs\ml>python --version
Python 3.8.2

(ml) C:\Users\pyuser\projs\ml>python -c ""import numpy; print(numpy.version.version
)""
1.18.4

(ml) C:\Users\pyuser\projs\ml>python -c ""import pandas; print(pandas.__version__)""
1.0.5

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.18.4 3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 23:03:10) [MSC v.1916 64 bit (AM
D64)]
"
644585466,34970,BUG: fixtures are not to be called directly,pizzathief,closed,2020-06-24T12:44:04Z,2020-06-24T14:00:50Z,"- [ X] I have checked that this issue has not already been reported.

- [X ] I have confirmed this bug exists on the latest version of pandas.

- [X ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```bash
pytest-3 pandas

========================================================================== test session starts ==========================================================================
platform linux -- Python 3.8.2, pytest-4.6.9, py-1.8.1, pluggy-0.13.0
rootdir: ~/code/pandas, inifile: setup.cfg, testpaths: pandas
collected 27211 items / 3 errors / 4 skipped / 27204 selected                                                                                                           

================================================================================ ERRORS =================================================================================
________________________________________________________ ERROR collecting pandas/tests/groupby/test_whitelist.py ________________________________________________________
Fixture ""df_letters"" called directly. Fixtures are not meant to be called directly,
but are created automatically when test functions request them as parameters.
See https://docs.pytest.org/en/latest/fixture.html for more information about fixtures, and
https://docs.pytest.org/en/latest/deprecations.html#calling-fixtures-directly about how to update your code.
_____________________________________________________ ERROR collecting pandas/tests/indexes/datetimes/test_tools.py _____________________________________________________
Fixture ""epoch_1960"" called directly. Fixtures are not meant to be called directly,
but are created automatically when test functions request them as parameters.
See https://docs.pytest.org/en/latest/fixture.html for more information about fixtures, and
https://docs.pytest.org/en/latest/deprecations.html#calling-fixtures-directly about how to update your code.
________________________________________________________ ERROR collecting pandas/tests/series/test_analytics.py _________________________________________________________
Fixture ""s_main_dtypes"" called directly. Fixtures are not meant to be called directly,
but are created automatically when test functions request them as parameters.
See https://docs.pytest.org/en/latest/fixture.html for more information about fixtures, and
https://docs.pytest.org/en/latest/deprecations.html#calling-fixtures-directly about how to update your code.
=========================================================================== warnings summary ============================================================================
pandas/tests/extension/json/array.py:27
  /home/jason/code/pandas/pandas/tests/extension/json/array.py:27: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working
    type = collections.Mapping

-- Docs: https://docs.pytest.org/en/latest/warnings.html
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 3 errors during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
============================================================ 4 skipped, 1 warnings, 3 error in 9.47 seconds =============================================================


```


#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]
>>> pandas.show_versions()

INSTALLED VERSIONS
------------------
commit: 5a84593d9d02f4c26be18c98bf6637bf0a6f876c
python: 3.8.2.final.0
python-bits: 64
OS: Linux
OS-release: 5.4.0-37-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_AU.UTF-8
LOCALE: en_AU.UTF-8

pandas: 0.24.0.dev0+193.g5a84593d9
pytest: None
pip: 20.0.2
setuptools: 44.0.0
Cython: None
numpy: 1.18.5
scipy: 1.4.1
pyarrow: None
xarray: None
IPython: 7.15.0
sphinx: None
patsy: None
dateutil: 2.8.1
pytz: 2020.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.2.1
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
>>> 

</details>
"
644587414,34971,TST: update incorrectly defined fixtures,pizzathief,closed,2020-06-24T12:47:21Z,2020-06-24T14:03:17Z,"handles the deprecationwarning messages and allows ""pytest pandas"" to proceed. (see issue #34970)"
642692972,34927,Fix DataFrame/Series stack/unstack docs,pdnm,closed,2020-06-22T01:57:55Z,2020-06-24T15:00:52Z,"- [x] closes #21675
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

As discussed in the issue, the sorted level is an implementation detail that does not always happen. Thus I think we should remove the misleading documentation."
463997231,27222,Wish: Write support for Open Document Spreadsheet (ODS),solstag,closed,2019-07-03T23:45:46Z,2020-06-24T15:13:58Z,"Ni!

This is a follow up to #2311 which has been closed after read support was implemented in  #25427 .

People whose work flow involve outputting ODS files would benefit from write support.

.~´"
644587698,34972,CI: test_constructor_list_frames failures on MacPython,TomAugspurger,closed,2020-06-24T12:47:50Z,2020-06-24T15:18:40Z,"https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=38048&view=logs&j=79f3a53a-4a6d-509c-98b5-ff6e9111f67c&t=25a63239-9c30-5cf0-cb4c-3d01da7b47c5&l=1049

```
___________ TestDataFrameConstructors.test_constructor_list_frames ____________
[gw0] win32 -- Python 3.6.8 D:\a\1\s\test_venv\Scripts\python.exe

self = <pandas.tests.frame.test_constructors.TestDataFrameConstructors object at 0x2882F090>

    @pytest.mark.xfail(_is_numpy_dev, reason=""Interprets list of frame as 3D"")
    def test_constructor_list_frames(self):
        # see gh-3243
>       result = DataFrame([DataFrame()])

test_venv\lib\site-packages\pandas\tests\frame\test_constructors.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
test_venv\lib\site-packages\pandas\core\frame.py:488: in __init__
    mgr = init_ndarray(data, index, columns, dtype=dtype, copy=copy)
test_venv\lib\site-packages\pandas\core\internals\construction.py:169: in init_ndarray
    values = prep_ndarray(values, copy=copy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

values = array([], shape=(1, 0, 0), dtype=float64), copy = False

    def prep_ndarray(values, copy=True) -> np.ndarray:
        if not isinstance(values, (np.ndarray, ABCSeries, Index)):
            if len(values) == 0:
                return np.empty((0, 0), dtype=object)
            elif isinstance(values, range):
                arr = np.arange(values.start, values.stop, values.step, dtype=""int64"")
                return arr[..., np.newaxis]
    
            def convert(v):
                return maybe_convert_platform(v)
    
            # we could have a 1-dim or 2-dim list here
            # this is equiv of np.asarray, but does object conversion
            # and platform dtype preservation
            try:
                if is_list_like(values[0]) or hasattr(values[0], ""len""):
                    values = np.array([convert(v) for v in values])
                elif isinstance(values[0], np.ndarray) and values[0].ndim == 0:
                    # GH#21861
                    values = np.array([convert(v) for v in values])
                else:
                    values = convert(values)
            except (ValueError, TypeError):
                values = convert(values)
    
        else:
    
            # drop subclass info, do not copy data
            values = np.asarray(values)
            if copy:
                values = values.copy()
    
>           raise ValueError(""Must pass 2-d input"")
E           ValueError: Must pass 2-d input

test_venv\lib\site-packages\pandas\core\internals\construction.py:295: ValueError
```"
603879235,33696,"DOC: pd.Categorical does not throw ValueError, converts to NaN instead",behrenhoff,closed,2020-04-21T10:14:49Z,2020-06-24T15:30:32Z,"My problem might be a bug or just a documentation issue. Consider this two-liner:

```python
import pandas as pd
print(pd.Categorical([*'abc'], categories=[*'ab']))
```
The output is:
```
[a, b, NaN]
Categories (2, object): [a, b]
```

According to the docs:

> All values of the Categorical are either in categories or np.nan. **Assigning values outside of categories will raise a ValueError.**

And further:
> Raises ValueError  If the categories do not validate.

However, no ValueError is thrown, instead 'c' is converted to NaN silently.

Either this is intended - then the docu needs update.
Or it is not intended - then it is a bug - and it should throw a ValueError(""c not in categories"")

Tested with Pandas 0.25.3 and 1.0.3 on Python 3.6.9.

I tried in a fresh venv:
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-96-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : de_DE.UTF-8
LOCALE           : de_DE.UTF-8

pandas           : 1.0.3
numpy            : 1.18.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 9.0.1
setuptools       : 39.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 0.999999999
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"

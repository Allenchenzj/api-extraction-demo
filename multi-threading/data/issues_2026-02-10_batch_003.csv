id,number,title,user,state,created_at,updated_at,body
558994294,31608,Backport PR #31571 on branch 1.0.x (DOC fix *_option() docstring),meeseeksmachine,closed,2020-02-03T10:39:43Z,2020-02-03T11:27:01Z,Backport PR #31571: DOC fix *_option() docstring
528196106,29836,ENH: XLSB support,Rik-de-Kort,closed,2019-11-25T16:26:33Z,2020-02-03T12:12:35Z,"Hey all, a moderately commonly requested feature is xlsb support. I thought I'd go ahead and make a PR for it, based on Pyxlsb. The library isn't very full-featured: datetimes are loaded in as floats without any indication they're datetimes. Would that be grounds for rejection?

Alternative would be to implement xlsb support in Openpyxl which looks like it will take a long time for someone not familiar with the file formats (as I am).

- [X] closes #8540
- [x] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
558975377,31606,DOC: add back google analytics with the new doc theme,jorisvandenbossche,closed,2020-02-03T10:07:33Z,2020-02-03T12:35:38Z,"WIth the new theme, we didn't have google analytics anymore (how we had it before https://github.com/pandas-dev/pandas/pull/27662), so did a quick PR to add that to the theme with an option: https://github.com/pandas-dev/pydata-bootstrap-sphinx-theme/pull/84"
559041844,31611,Backport PR #31606 on branch 1.0.x (DOC: add back google analytics with the new doc theme),meeseeksmachine,closed,2020-02-03T12:07:39Z,2020-02-03T12:35:58Z,Backport PR #31606: DOC: add back google analytics with the new doc theme
558962816,31604,DOC: pandas.Series.dt.day has incorrect docs,julienprieur,closed,2020-02-03T09:44:59Z,2020-02-03T12:41:37Z,"See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.day.html#pandas.Series.dt.day. 

The documentation shows the description for dt.month and not dt.day."
558998454,31610,Backport PR #31594: TST: troubleshoot npdev build,jorisvandenbossche,closed,2020-02-03T10:46:34Z,2020-02-03T12:53:41Z,Backport of https://github.com/pandas-dev/pandas/pull/31594
548069766,30881,DOC: missing docstring substitution for Series/DataFrame.set_axis(),simonjayhawkins,closed,2020-01-10T13:06:54Z,2020-02-03T13:08:39Z,"https://dev.pandas.io/docs/reference/api/pandas.Series.set_axis.html
https://dev.pandas.io/docs/reference/api/pandas.DataFrame.set_axis.html

![image](https://user-images.githubusercontent.com/13159005/72154883-b527de80-33a9-11ea-8593-91d2cc2a9aaf.png)
"
556104646,31385,pd.read_sas with chunksize option raises IndexError,jicky94,closed,2020-01-28T10:09:38Z,2020-02-03T13:21:52Z,"Good morning,

Using Python 3.6.

My problem seems to be close to issue [#14734](https://github.com/pandas-dev/pandas/issues/14734) but with a different error type though. However, please forgive my lack of competence, but I am not able to understand 1. if my issue is really similar and 2. if the issue that seemed to surround some sas files has been solved or if there might still be some probleùs with some sas files (that i cannot provide for reasons detailed just below).

I have read the rules about posting but i cannot attach a sample of my data or reproduce the entire error message as the data i am working on is located on a server without access to internet. I apologize for this inconvenience. I’ll try to reproduce most of what is requested however below.

I am working with very big sas files (data on each job, hence millions of lines) and got memory error when i was trying to simple read them (they open fine in R or stata strangely). Therefore i searched and find the pandas.read_sas option to work with chunks of the data. My code is now the following:

```python
import pandas as pd
df_chunk = pd.read_sas(r'file.sas7bdat', chunksize=500)

for chunk in df_chunk:  
    chunk_list.append(chunk)

```

At this point i get the following error (I am reproducing it here manually as i cannot copy paste):
```python
line 660, in _chunk_to_dataframe
if self.column_formats[j] in const.sas_date_formats:
IndexError: list index out of range
```
I am aware the exposition of my issue is truncated and probably incomplete but many thanks for any help you could provide,
Axelle
"
556026501,31380,BUG: Timedelta components no longer rounded with high precision integers,mroeschke,closed,2020-01-28T07:16:54Z,2020-02-03T14:06:59Z,"- [x] closes #31354
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
558988483,31607,DOC: combine regressions in section in v1.0.1 whatsnew,jorisvandenbossche,closed,2020-02-03T10:30:05Z,2020-02-03T14:32:14Z,"Similarly how we did it for 0.24.x, gathering all regressions in the first section of the whatsnew (as those are the most important part of 1.0.1)

cc @TomAugspurger "
558331201,31515,"REGR: DataFrame.__setitem__(slice, val) is positional ",jbrockmendel,closed,2020-01-31T19:36:38Z,2020-02-05T12:34:52Z,"- [x] closes #31469 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
559127380,31619,Backport PR #31521: REGR: Fixed slicing DatetimeIndex with date,jorisvandenbossche,closed,2020-02-03T14:41:32Z,2020-02-03T15:39:45Z,backport https://github.com/pandas-dev/pandas/pull/31521
559125733,31618,"Backport PR #31515 on branch 1.0.x (REGR: DataFrame.__setitem__(slice, val) is positional )",meeseeksmachine,closed,2020-02-03T14:38:48Z,2020-02-03T15:39:57Z,"Backport PR #31515: REGR: DataFrame.__setitem__(slice, val) is positional "
467991956,27394,pd.read_csv prefix parameter seems do not works,Sniperq2,closed,2019-07-15T08:06:33Z,2020-02-03T15:52:27Z,"#### Code Sample, a copy-pastable example if possible
```test.csv
field,data,values
text1,34,hh,fail
text2,76,tt,fail2
```
```python
import pandas as pd

if __name__ == ""__main__"":
    data_list = pd.read_csv('test.csv', prefix=""X"")
    print(data_list)

```
#### Problem description
A documentation said: 
```
prefix : str, optional
    Prefix to add to column numbers when no header, e.g. ‘X’ for X0, X1, …
```
If I understood correctly this documentation I should get this

#### Expected Output
```
field    data    values  X0
text1     34    hh        fail
text2     76    tt         fail2
```
But I got this anyway:

#### Current Output
```
          field  data    values
text1     34    hh      fail
text2     76    tt        fail2
```
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.3.final.0
python-bits: 64
OS: Windows
OS-release: 8.1
machine: AMD64
processor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.2
pytest: None
pip: 19.1.1
setuptools: 40.1.0
Cython: None
numpy: 1.15.0
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.7.5
pytz: 2018.5
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.2.3
openpyxl: 2.5.5
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: 4.1.1
bs4: 4.6.0
html5lib: None
sqlalchemy: 1.2.19
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
None


</details>
"
493574252,28441,BUG: Fix for fillna ignoring axis=1 parameter (issues #17399 #17409),jorvis,closed,2019-09-14T02:43:19Z,2020-02-03T16:11:29Z,"- [x] closes #17399 #17409 
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
556088926,31383,Raise error in read_csv when arguments header and prefix both are not None,rushabh-v,closed,2020-01-28T09:40:27Z,2020-02-03T16:13:05Z,"- [x] closes #27394
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
558399940,31521,REGR: Fixed slicing DatetimeIndex with date,TomAugspurger,closed,2020-01-31T22:10:44Z,2020-02-03T16:49:38Z,Closes https://github.com/pandas-dev/pandas/issues/31501
558311867,31514,json_normalize Doc Broken,WillAyd,closed,2020-01-31T18:55:45Z,2020-02-03T19:39:23Z,"First google result for json_normalize will yield:

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.io.json.json_normalize.html

This however is a dead link. I think the issue is we moved json_normalize to the top-level pd.namespace, so this works:

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html

The former should redirect to the latter
"
559143448,31622,DOC: add redirect for moved json_normalize docstring,jorisvandenbossche,closed,2020-02-03T15:07:27Z,2020-02-03T19:39:29Z,Closes https://github.com/pandas-dev/pandas/issues/31514
559121687,31617,Backport PR #31607: DOC: combine regressions in section in v1.0.1 whatsnew,jorisvandenbossche,closed,2020-02-03T14:31:50Z,2020-02-03T19:39:37Z,Backport https://github.com/pandas-dev/pandas/pull/31607
557475177,31449,REGR: Assignment to MultiIndex fails for pandas 1.0.0.,tobiasraabe,closed,2020-01-30T12:54:39Z,2020-02-03T23:20:27Z,"#### Code Sample, a copy-pastable example if possible

This problem occurred with ``pandas 1.0.0 py37he350917_0 conda-forge``, but did not before or at least not since 0.24.

```python
import pandas as pd
import numpy as np

index = pd.MultiIndex.from_tuples([(""a"", ""c""), (""b"", ""x""), (""a"", ""d"")], names=[""l1"", ""l2""])
df = pd.DataFrame(index=index, data=np.arange(3), columns=[""e""])
df.loc[""a"", ""e""] = np.arange(99, 101)
```
#### Problem description

The error message is the following
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-13-b62e661ba0e4> in <module>
----> 1 df.loc[""a"", ""e""] = np.arange(99, 101)

C:\tools\miniconda3\envs\respy\lib\site-packages\pandas\core\indexing.py in __setitem__(self, key, value)
    668             key = com.apply_if_callable(key, self.obj)
    669         indexer = self._get_setitem_indexer(key)
--> 670         self._setitem_with_indexer(indexer, value)
    671 
    672     def _validate_key(self, key, axis: int):

C:\tools\miniconda3\envs\respy\lib\site-packages\pandas\core\indexing.py in _setitem_with_indexer(self, indexer, value)
    927                     # we can directly set the series here
    928                     # as we select a slice indexer on the mi
--> 929                     idx = index._convert_slice_indexer(idx)
    930                     obj._consolidate_inplace()
    931                     obj = obj.copy()

C:\tools\miniconda3\envs\respy\lib\site-packages\pandas\core\indexes\base.py in _convert_slice_indexer(self, key, kind)
   2917 
   2918         # potentially cast the bounds to integers
-> 2919         start, stop, step = key.start, key.stop, key.step
   2920 
   2921         # figure out if this is a positional indexer

AttributeError: 'numpy.ndarray' object has no attribute 'start'
```

If the MultiIndex would have been sorted, ``key`` in the last part of the traceback or ``indexer`` in the first part would have been a slice and everything is fine. Since the MultiIndex is unsorted ``key`` or ``indexer`` are a boolean array which has no start stop attributes.

I am sorry for not digging further, but I have not inspected pandas internals before.

#### Expected Output

```
         e
l1 l2     
a  c    99
b  x     1
a  d   100
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 69 Stepping 1, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.0
numpy            : 1.17.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200119
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.13
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.46.0

</details>
"
559319903,31631,REGR: Fixed setitem with MultiIndex,TomAugspurger,closed,2020-02-03T20:37:44Z,2020-02-03T23:26:28Z,"Closes https://github.com/pandas-dev/pandas/issues/31449
"
559239734,31628,replacing .format with f-strings,drewseibert,closed,2020-02-03T17:56:20Z,2020-02-03T23:46:37Z,"fixing some styling (using f-strings instead of .format)
"
557836370,31475,BUG: Period[us] start_time off by 1 nanosecond,jbrockmendel,closed,2020-01-31T00:19:37Z,2020-02-04T00:13:45Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
558625384,31556,[MRG] f-string updates for issue #29547,abbiepopa,closed,2020-02-02T01:22:48Z,2020-02-04T00:41:26Z,"Addresses, in part, https://github.com/pandas-dev/pandas/issues/29547 

…ape/pivot, reshape/reshape

- [ x] xref #29547 
- [n/a ] tests added / passed
- [x ] passes `black pandas`
- [ x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [n/a ] whatsnew entry
"
559290777,31629,Backport PR #31622 on branch 1.0.x (DOC: add redirect for moved json_normalize docstring),meeseeksmachine,closed,2020-02-03T19:39:53Z,2020-02-04T07:27:29Z,Backport PR #31622: DOC: add redirect for moved json_normalize docstring
559393757,31637,Backport PR #31631 on branch 1.0.x (REGR: Fixed setitem with MultiIndex),meeseeksmachine,closed,2020-02-03T23:20:38Z,2020-02-04T07:28:01Z,Backport PR #31631: REGR: Fixed setitem with MultiIndex
559487626,31642,DOC: Fix typo in Getting Started docs,ghost,closed,2020-02-04T04:55:42Z,2020-02-04T11:20:10Z,"
"
559563806,31645,DOC: extending pandas page is not rendered correctly,charlesdong1991,closed,2020-02-04T08:35:33Z,2020-02-04T12:23:45Z,"https://pandas.pydata.org/pandas-docs/stable/development/extending.html

this is the first result gotten by googling extending pandas, however, seems not rendered correctly, help is welcome!"
556418133,31399,PERF: avoid is_bool_indexer check where possible,jbrockmendel,closed,2020-01-28T19:27:47Z,2020-02-04T16:16:12Z,"xref #30349, not a lot of ground to pick up here, but we can avoid a few calls"
558227835,31499,Pandas 1.0 no longer handles `numpy.str_`s as catgories,flying-sheep,closed,2020-01-31T16:18:34Z,2020-02-04T16:23:59Z,"#### Code Sample

```python
import pandas as pd
pd.Categorical(['1', '0', '1'], [np.str_('0'), np.str_('1')])
```

```pytb
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/angerer/Dev/Python/venvs/env-pandas-1/lib/python3.8/site-packages/pandas/core/arrays/categorical.py"", line 385, in __init__
    codes = _get_codes_for_values(values, dtype.categories)
  File ""/home/angerer/Dev/Python/venvs/env-pandas-1/lib/python3.8/site-packages/pandas/core/arrays/categorical.py"", line 2576, in _get_codes_for_values
    t.map_locations(cats)
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1403, in pandas._libs.hashtable.StringHashTable.map_locations
TypeError: Expected unicode, got numpy.str_
```

#### Problem description
I know that having a list of `numpy.str_`s seems weird, but it easily happens when you use non-numpy algorithms on numpy arrays (e.g. `natsort.natsorted` in our case), or via comprehensions or so:

```py
>>> np.array(['1', '0'])[0].__class__
<class 'numpy.str_'>
>>> [type(s) for s in np.array(['1', '0'])]
[<class 'numpy.str_'>, <class 'numpy.str_'>]
```
#### Expected Output
A normal pd.Categorical

#### Pandas version

pandas 1.0"
558020159,31485,CLN: named parameters for GroupBy.(mean|median|var|std),topper-123,closed,2020-01-31T09:41:52Z,2020-02-04T16:25:32Z,"Drops *args & **kwargs, replace with named parameters for groupby methods mean, median, var & std. Similar to #31473.

This PR has the side effect that the raised error when a parameter is not allowed, is now ``TypeError`` instead of ``UnsupportedFunctionCall``, so technically a API change..."
558472609,31528,REGR: Categorical with np.str_ categories,jbrockmendel,closed,2020-02-01T03:40:31Z,2020-02-04T16:31:28Z,"- [x] closes #31499
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
558245881,31502,BUG: Fixed IntervalArray[int].shift,TomAugspurger,closed,2020-01-31T16:51:03Z,2020-02-04T16:56:08Z,"Closes https://github.com/pandas-dev/pandas/issues/31495

In ExtensionArray.shift, we have the note

```
        # Note: this implementation assumes that `self.dtype.na_value` can be
        # stored in an instance of your ExtensionArray with `self.dtype`.
```

I wonder, should we have a method / property like

```
@property
def _can_hold_na_value(self):
    return True
```

And for IntervalArray, we would do something like

```python
@property
def _can_hold_na_value(self):
    return is_float_dtype(self.dtype.subtype)
```


That would let us deduplicate things, since the call to `_from_sequence` would know to not pass `dtype=self.dtype` and trigger a re-inference."
558149833,31495,Series shift method breaks for series of pandas Intervals in Pandas 1.0 (works in 0.25.3),owenlamont,closed,2020-01-31T14:01:52Z,2020-02-04T16:56:08Z,"#### Code Sample, a copy-pastable example if possible

```python
# Problem code example 1
import pandas as pd
test = pd.Series(index=[1, 2], data=[pd.Interval(pd.Timestamp(""2020-09-04 10:00:00""), pd.Timestamp(""2020-11-30 14:00:00"")), pd.Interval(pd.Timestamp(""2020-08-14 10:00:00""), pd.Timestamp(""2020-09-21 14:00:00""))])
test.shift(1)

# Problem code example 2
import pandas as pd
test = pd.Series(index=[1, 2], data=[pd.Interval(1, 2), pd.Interval(3, 4)])
test.shift(1)
```
#### Problem description

Calling the shift method on an integer indexed Pandas series of Pandas intervals throws opaque exceptions. The same code works as expected in Pandas 0.25.3. This is definitely a breaking change - I'm unsure if it is intentional. I'm assuming it should still work the same as 0.25.3 for now. I tried searching for any documented changes to the shift method behaviour but didn't find any.

Exception traceback for example 1:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-7a1f088b5a14> in <module>
      4 test = pd.Series(index=[1, 2], data=[pd.Interval(pd.Timestamp(""2020-09-04 10:00:00""), pd.Timestamp(""2020-11-30 14:00:00"")),
      5                                      pd.Interval(pd.Timestamp(""2020-08-14 10:00:00""), pd.Timestamp(""2020-09-21 14:00:00""))])
----> 6 test.shift(1)

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\series.py in shift(self, periods, freq, axis, fill_value)
   4183     def shift(self, periods=1, freq=None, axis=0, fill_value=None):
   4184         return super().shift(
-> 4185             periods=periods, freq=freq, axis=axis, fill_value=fill_value
   4186         )
   4187 

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\generic.py in shift(self, periods, freq, axis, fill_value)
   9043         if freq is None:
   9044             new_data = self._data.shift(
-> 9045                 periods=periods, axis=block_axis, fill_value=fill_value
   9046             )
   9047         else:

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\internals\managers.py in shift(self, **kwargs)
    571 
    572     def shift(self, **kwargs):
--> 573         return self.apply(""shift"", **kwargs)
    574 
    575     def fillna(self, **kwargs):

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\internals\managers.py in apply(self, f, filter, **kwargs)
    440                 applied = b.apply(f, **kwargs)
    441             else:
--> 442                 applied = getattr(b, f)(**kwargs)
    443             result_blocks = _extend_blocks(applied, result_blocks)
    444 

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\internals\blocks.py in shift(self, periods, axis, fill_value)
   1908         return [
   1909             self.make_block_same_class(
-> 1910                 self.values.shift(periods=periods, fill_value=fill_value),
   1911                 placement=self.mgr_locs,
   1912                 ndim=self.ndim,

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\arrays\base.py in shift(self, periods, fill_value)
    623 
    624         empty = self._from_sequence(
--> 625             [fill_value] * min(abs(periods), len(self)), dtype=self.dtype
    626         )
    627         if periods > 0:

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\arrays\interval.py in _from_sequence(cls, scalars, dtype, copy)
    243     @classmethod
    244     def _from_sequence(cls, scalars, dtype=None, copy=False):
--> 245         return cls(scalars, dtype=dtype, copy=copy)
    246 
    247     @classmethod

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\arrays\interval.py in __new__(cls, data, closed, dtype, copy, verify_integrity)
    182             copy=copy,
    183             dtype=dtype,
--> 184             verify_integrity=verify_integrity,
    185         )
    186 

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\arrays\interval.py in _simple_new(cls, left, right, closed, copy, dtype, verify_integrity)
    202                 raise TypeError(msg)
    203             elif dtype.subtype is not None:
--> 204                 left = left.astype(dtype.subtype)
    205                 right = right.astype(dtype.subtype)
    206 

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\indexes\numeric.py in astype(self, dtype, copy)
    393         if needs_i8_conversion(dtype):
    394             raise TypeError(
--> 395                 f""Cannot convert Float64Index to dtype {dtype}; integer ""
    396                 ""values are required for conversion""
    397             )

TypeError: Cannot convert Float64Index to dtype datetime64[ns]; integer values are required for conversion
```

Exception traceback for example 2
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-6804e066ee18> in <module>
      4 test = pd.Series(index=[1, 2], data=[pd.Interval(1, 2),
      5                                      pd.Interval(3, 4)])
----> 6 test.shift(1)

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\series.py in shift(self, periods, freq, axis, fill_value)
   4183     def shift(self, periods=1, freq=None, axis=0, fill_value=None):
   4184         return super().shift(
-> 4185             periods=periods, freq=freq, axis=axis, fill_value=fill_value
   4186         )
   4187 

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\generic.py in shift(self, periods, freq, axis, fill_value)
   9043         if freq is None:
   9044             new_data = self._data.shift(
-> 9045                 periods=periods, axis=block_axis, fill_value=fill_value
   9046             )
   9047         else:

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\internals\managers.py in shift(self, **kwargs)
    571 
    572     def shift(self, **kwargs):
--> 573         return self.apply(""shift"", **kwargs)
    574 
    575     def fillna(self, **kwargs):

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\internals\managers.py in apply(self, f, filter, **kwargs)
    440                 applied = b.apply(f, **kwargs)
    441             else:
--> 442                 applied = getattr(b, f)(**kwargs)
    443             result_blocks = _extend_blocks(applied, result_blocks)
    444 

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\internals\blocks.py in shift(self, periods, axis, fill_value)
   1908         return [
   1909             self.make_block_same_class(
-> 1910                 self.values.shift(periods=periods, fill_value=fill_value),
   1911                 placement=self.mgr_locs,
   1912                 ndim=self.ndim,

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\arrays\base.py in shift(self, periods, fill_value)
    623 
    624         empty = self._from_sequence(
--> 625             [fill_value] * min(abs(periods), len(self)), dtype=self.dtype
    626         )
    627         if periods > 0:

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\arrays\interval.py in _from_sequence(cls, scalars, dtype, copy)
    243     @classmethod
    244     def _from_sequence(cls, scalars, dtype=None, copy=False):
--> 245         return cls(scalars, dtype=dtype, copy=copy)
    246 
    247     @classmethod

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\arrays\interval.py in __new__(cls, data, closed, dtype, copy, verify_integrity)
    182             copy=copy,
    183             dtype=dtype,
--> 184             verify_integrity=verify_integrity,
    185         )
    186 

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\arrays\interval.py in _simple_new(cls, left, right, closed, copy, dtype, verify_integrity)
    202                 raise TypeError(msg)
    203             elif dtype.subtype is not None:
--> 204                 left = left.astype(dtype.subtype)
    205                 right = right.astype(dtype.subtype)
    206 

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\indexes\numeric.py in astype(self, dtype, copy)
    399             # TODO(jreback); this can change once we have an EA Index type
    400             # GH 13149
--> 401             arr = astype_nansafe(self.values, dtype=dtype)
    402             return Int64Index(arr)
    403         return super().astype(dtype, copy=copy)

~\Miniconda3\envs\jupyter\lib\site-packages\pandas\core\dtypes\cast.py in astype_nansafe(arr, dtype, copy, skipna)
    866 
    867         if not np.isfinite(arr).all():
--> 868             raise ValueError(""Cannot convert non-finite values (NA or inf) to integer"")
    869 
    870     elif is_object_dtype(arr):

ValueError: Cannot convert non-finite values (NA or inf) to integer
```

#### Expected Output

This is the actual output I got executing with pandas 0.25.3

1                                           NaN
2    (2020-09-04 10:00:00, 2020-11-30 14:00:00]
dtype: object

1       NaN
2    (1, 2]
dtype: object

#### Output of ``pd.show_versions()``

<details>

commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 13, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.0
numpy            : 1.17.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200119
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : 0.2.2
scipy            : 1.3.1
sqlalchemy       : 1.3.13
tables           : None
tabulate         : 0.8.6
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.7
numba            : 0.48.0

</details>
"
559388498,31636,CLN: Unreachable branch in Loc._getitem_iterable,jbrockmendel,closed,2020-02-03T23:07:09Z,2020-02-04T16:58:18Z,
229784751,16386,New Interval / IntervalIndex behavior spec,alexlenail,closed,2017-05-18T19:48:02Z,2020-02-04T17:15:32Z," - [x] part of #16316
 - [x] tests added / passed
 - [x] passes ``git diff upstream/master --name-only -- '*.py' | flake8 --diff``

@jreback "
559235402,31627,Replace .format with f-strings,leandermaben,closed,2020-02-03T17:47:03Z,2020-02-04T18:33:15Z,"- [ ] solves few cases of  #29547

"
559831089,31654,Backport PR #31528 on branch 1.0.x (REGR: Categorical with np.str_ categories),meeseeksmachine,closed,2020-02-04T16:24:38Z,2020-02-04T19:25:21Z,Backport PR #31528: REGR: Categorical with np.str_ categories
559851083,31656,Backport PR #31502 on branch 1.0.x (BUG: Fixed IntervalArray[int].shift),meeseeksmachine,closed,2020-02-04T16:56:19Z,2020-02-04T19:25:31Z,Backport PR #31502: BUG: Fixed IntervalArray[int].shift
559776349,31651,PERF: Cache MultiIndex.levels,TomAugspurger,closed,2020-02-04T15:00:02Z,2020-02-04T20:08:40Z,"Closes https://github.com/pandas-dev/pandas/issues/31648.

This should have been caught by the benchmark MultiIdnex.time_index_slice. Confirming that now.."
559948807,31664,Backport PR #31651: PERF: Cache MultiIndex.levels,TomAugspurger,closed,2020-02-04T20:04:26Z,2020-02-04T21:25:00Z,Manual backport for https://github.com/pandas-dev/pandas/pull/31651.
559950691,31665,whatsnew for MultiIndex levels caching,TomAugspurger,closed,2020-02-04T20:08:16Z,2020-02-04T21:25:04Z,"(cherry picked from commit 103840e974a4c8548fccab86cabc6a3161bd94e8)

Included in https://github.com/pandas-dev/pandas/pull/31664. So this won't need to be backported."
557552566,31455,"jobs failling with error raise RuntimeError(""Cannot cythonize without Cython installed."")\n    RuntimeError: Cannot cythonize without Cython installed.\n",ramneek91,closed,2020-01-30T15:01:56Z,2020-02-04T21:36:03Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here

```
#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
557699732,31471,TypeError when calculating min/max of period column using groupby,fajost,closed,2020-01-30T19:23:28Z,2020-02-04T22:52:27Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
periods = pd.period_range(start=""2019-01"", periods=4, freq=""M"")
groups = [1, 1, 2, 2]
df = pd.DataFrame({""periods"": periods, ""groups"": groups})
result = df.groupby(""groups"")[""periods""].min()
```
#### Problem description

The last line of the example throws a _TypeError: data type not understood_.

<details>
Traceback (most recent call last):
  File ""test.py"", line 6, in <module>
    result = df.groupby(""groups"")[""periods""].min()
  File ""...\site-packages\pandas\core\groupby\groupby.py"", line 1378, in f
    return self._cython_agg_general(alias, alt=npfunc, **kwargs)
  File ""...\site-packages\pandas\core\groupby\groupby.py"", line 889, in _cython_agg_general
    result, agg_names = self.grouper.aggregate(
  File ""...\site-packages\pandas\core\groupby\ops.py"", line 580, in aggregate
    return self._cython_operation(
  File ""...\site-packages\pandas\core\groupby\ops.py"", line 573, in _cython_operation
    result = result.astype(orig_values.dtype)
TypeError: data type not understood
</details>

#### Expected Output

The `result` DataFrame should contain the earliest period (in this case 2019-01) for each grouping slice.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS                                                        
------------------                                                        
commit           : None                                                   
python           : 3.8.1.final.0                                          
python-bits      : 64                                                     
OS               : Windows                                                
OS-release       : 10                                                     
machine          : AMD64                                                  
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel   
byteorder        : little                                                 
LC_ALL           : None                                                   
LANG             : en_US.UTF-8                                            
LOCALE           : English_United Kingdom.1252                            
                                                                          
pandas           : 1.0.0                                                  
numpy            : 1.18.1                                                 
pytz             : 2019.3                                                 
dateutil         : 2.8.1                                                  
pip              : 20.0.2                                                 
setuptools       : 45.1.0.post20200127                                    
Cython           : None                                                   
pytest           : None                                                   
hypothesis       : None                                                   
sphinx           : None                                                   
blosc            : None                                                   
feather          : None                                                   
xlsxwriter       : None                                                   
lxml.etree       : None                                                   
html5lib         : None                                                   
pymysql          : None                                                   
psycopg2         : None                                                   
jinja2           : None                                                   
IPython          : None                                                   
pandas_datareader: None                                                   
bs4              : None                                                   
bottleneck       : None                                                   
fastparquet      : None                                                   
gcsfs            : None                                                   
lxml.etree       : None                                                   
matplotlib       : None                                                   
numexpr          : None                                                   
odfpy            : None                                                   
openpyxl         : None                                                   
pandas_gbq       : None                                                   
pyarrow          : None                                                   
pytables         : None                                                   
pytest           : None                                                   
pyxlsb           : None                                                   
s3fs             : None                                                   
scipy            : None                                                   
sqlalchemy       : None                                                   
tables           : None                                                   
tabulate         : None                                                   
xarray           : None                                                   
xlrd             : None                                                   
xlwt             : None                                                   
xlsxwriter       : None                                                   
numba            : None                                                   

</details>
"
557867832,31477,REGR: Fix TypeError in groupby min / max of period column,dsaxton,closed,2020-01-31T02:06:23Z,2020-02-04T22:54:20Z,"- [x] closes #31471
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
559752450,31649,to_datetime returning numpy.datetime64,ecwootten,closed,2020-02-04T14:22:56Z,2020-02-05T00:22:03Z,"#### Code Sample, a copy-pastable example if possible

This code:
```python
>>> df = pd.DataFrame({'date': ['Aug2020', 'November 2020']})
>>> df['parsed'] = df['date'].apply(pd.to_datetime)
>>> end = df.loc[df['parsed'].idxmax()]
>>> end['parsed'].replace(day=2)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'numpy.datetime64' object has no attribute 'replace'

```
worked in Pandas 0.25.3, but raises since 1.0.0.

I think there might be an issue with unboxing values when there are mixed types in the dataframe:

```python
>>> df = pd.DataFrame({'date': ['Aug2020', 'November 2020']})
>>> new = (
...     df
...     .assign(
...         parsed=lambda x: x['date'].apply(pd.to_datetime),
...         parsed2 = lambda x: x['date'].apply(pd.to_datetime)
...     )
... )
>>> new['parsed'].iloc[0]
Timestamp('2020-08-01 00:00:00')
>>> new.iloc[0]['parsed']
numpy.datetime64('2020-08-01T00:00:00.000000000') # unboxed type
>>> new2 = new.drop(columns=['date'])
>>> new2['parsed'].iloc[0]
Timestamp('2020-08-01 00:00:00')
>>> new2.iloc[0]['parsed']
Timestamp('2020-08-01 00:00:00') # boxed type now that we've dropped the string column

```
#### Problem description

to_datetime can ""sometimes"" result in a np.datetime64 return type.

np.datetime64 is not a valid return type for to_datetime (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html), it should always be a datetimelike.

#### Expected Output

As in previous versions of Pandas:

```python
>>> df = pd.DataFrame({'date': ['Aug2020', 'November 2020']})
>>> df['parsed'] = df['date'].apply(pd.to_datetime)
>>> end = df.loc[df['parsed'].idxmax()]
>>> end['parsed'].replace(day=2)
Timestamp('2020-11-02 00:00:00')
```

#### Output of ``pd.show_versions()``

<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : None.None

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.2.0
Cython           : 0.29.14
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None


</details>
"
560027694,31671,Backport PR #31477 on branch 1.0.x (REGR: Fix TypeError in groupby min / max of period column),meeseeksmachine,closed,2020-02-04T22:53:09Z,2020-02-05T00:22:44Z,Backport PR #31477: REGR: Fix TypeError in groupby min / max of period column
557481348,31450,REGR: groupby().agg fails on categorical column in pandas 1.0.0,MarekOzana,closed,2020-01-30T13:05:54Z,2020-02-05T00:31:25Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
tbl = pd.DataFrame({""col_num"": [1, 1, 2, 3]})
tbl[""col_cat""] = tbl[""col_num""].astype(""category"")

# The following line works in 0.25.3 but throws exception in 1.0.0
df = tbl.groupby(""col_num"").agg({""col_cat"": ""first""})
```
#### Problem description
agg() on categorical column works without any warnings in 0.25.3 but throws exception in pandas 1.0.0:
<details>
NotImplementedError                       Traceback (most recent call last)
<ipython-input-2-663038a86e20> in <module>
----> 1 df = tbl.groupby(""col_num"").agg({""col_cat"": ""first""})

~\AppData\Local\Continuum\miniconda3\envs\dev37\lib\site-packages\pandas\core\groupby\generic.py in aggregate(self, func, *args, **kwargs)
    938         func = _maybe_mangle_lambdas(func)
    939 
--> 940         result, how = self._aggregate(func, *args, **kwargs)
    941         if how is None:
    942             return result

~\AppData\Local\Continuum\miniconda3\envs\dev37\lib\site-packages\pandas\core\base.py in _aggregate(self, arg, *args, **kwargs)
    426 
    427                 try:
--> 428                     result = _agg(arg, _agg_1dim)
    429                 except SpecificationError:
    430 

~\AppData\Local\Continuum\miniconda3\envs\dev37\lib\site-packages\pandas\core\base.py in _agg(arg, func)
    393                 result = {}
    394                 for fname, agg_how in arg.items():
--> 395                     result[fname] = func(fname, agg_how)
    396                 return result
    397 

~\AppData\Local\Continuum\miniconda3\envs\dev37\lib\site-packages\pandas\core\base.py in _agg_1dim(name, how, subset)
    377                         ""nested dictionary is ambiguous in aggregation""
    378                     )
--> 379                 return colg.aggregate(how)
    380 
    381             def _agg_2dim(name, how):

~\AppData\Local\Continuum\miniconda3\envs\dev37\lib\site-packages\pandas\core\groupby\generic.py in aggregate(self, func, *args, **kwargs)
    245 
    246         if isinstance(func, str):
--> 247             return getattr(self, func)(*args, **kwargs)
    248 
    249         elif isinstance(func, abc.Iterable):

~\AppData\Local\Continuum\miniconda3\envs\dev37\lib\site-packages\pandas\core\groupby\groupby.py in f(self, **kwargs)
   1376                 # try a cython aggregation if we can
   1377                 try:
-> 1378                     return self._cython_agg_general(alias, alt=npfunc, **kwargs)
   1379                 except DataError:
   1380                     pass

~\AppData\Local\Continuum\miniconda3\envs\dev37\lib\site-packages\pandas\core\groupby\groupby.py in _cython_agg_general(self, how, alt, numeric_only, min_count)
    888 
    889             result, agg_names = self.grouper.aggregate(
--> 890                 obj._values, how, min_count=min_count
    891             )
    892 

~\AppData\Local\Continuum\miniconda3\envs\dev37\lib\site-packages\pandas\core\groupby\ops.py in aggregate(self, values, how, axis, min_count)
    579     ) -> Tuple[np.ndarray, Optional[List[str]]]:
    580         return self._cython_operation(
--> 581             ""aggregate"", values, how, axis, min_count=min_count
    582         )
    583 

~\AppData\Local\Continuum\miniconda3\envs\dev37\lib\site-packages\pandas\core\groupby\ops.py in _cython_operation(self, kind, values, how, axis, min_count, **kwargs)
    453         # are not setup for dim transforming
    454         if is_categorical_dtype(values) or is_sparse(values):
--> 455             raise NotImplementedError(f""{values.dtype} dtype not supported"")
    456         elif is_datetime64_any_dtype(values):
    457             if how in [""add"", ""prod"", ""cumsum"", ""cumprod""]:

NotImplementedError: category dtype not supported
</details>
#### Expected Output
no error, just groupped dataframe.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.0
numpy            : 1.17.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200119
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.0
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.7
numba            : None
</details>
"
560035835,31674,CLN: assorted cleanups in indexes/,jbrockmendel,closed,2020-02-04T23:14:09Z,2020-02-05T00:45:56Z,
560034183,31673,"CLN: misc tslibs, annotations, unused imports",jbrockmendel,closed,2020-02-04T23:09:40Z,2020-02-05T00:48:50Z,
558728536,31580,Checking plot labels works locally but not during CI,MarcoGorelli,closed,2020-02-02T17:28:53Z,2020-02-05T01:02:58Z,"#### Code Sample, a copy-pastable example if possible

xref #31207 

If I modified the test as follows (with `import re` at the top of the file)

```python
        with tm.assert_produces_warning(None):
            fig, ax = self.plt.subplots()
            ts.plot(ax=ax)
            fig.canvas.draw()
            labels = [i.get_text() for i in ax.get_xticklabels()]
            # Extract H:M component, check first point is in correct timezone.
            # NOTE: this test could be updated once GH 31548 is fixed,
            # so that the last point is checked as well.
            assert re.findall(r""[^:]?(\d{2}:\d{2})"", labels[0])[0] == ""00:00""
            _check_plot_works(ts.plot)
```

then the test worked locally, but not during CI. Looking at the job details for linux py36_local, I could see that `labels` was
```
['01 00:01', ...1 00:51', ...]
```

I don't understand where that extra minute is coming from (nor can I understand why it only appears during CI).

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : 6e2d3aeb6179d40abfb68fdcc5b064a1d6b0bc59
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-74-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.26.0.dev0+2036.g6e2d3aeb6
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 45.0.0.post20200113
Cython           : 0.29.14
pytest           : 5.3.3
hypothesis       : 5.1.5
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.3.3
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.12
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.47.0

</details>
"
559916954,31662,CLN: MultiIndex.get_value is a hive of scum and villainy,jbrockmendel,closed,2020-02-04T19:02:45Z,2020-02-05T01:07:52Z,This will in turn allow for simplification for other code that has built up kludges around MultiIndex.get_value's behavior
560039413,31676,REF: make _convert_scalar_indexer require a scalar,jbrockmendel,closed,2020-02-04T23:23:40Z,2020-02-05T01:08:33Z,
559441621,31639,CLN: Replace format with f-strings in test_repr_info,thomasjpfan,closed,2020-02-04T02:02:09Z,2020-02-05T01:12:15Z,"Updated `pandas/tests/frame/test_repr_info.py` to use f-strings

ref: https://github.com/pandas-dev/pandas/issues/29547

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
559440436,31638,"REF: call _maybe_cast_indexer upfront, better exception messages",jbrockmendel,closed,2020-02-04T01:58:07Z,2020-02-05T01:18:18Z,"This will actually allow us to remove e.g. CategoricalIndex.get_loc by moving the 2 relevant lines into CategoricalIndex._maybe_cast_indexer.  Large parts of DTI/TDI/PI.get_loc will also be simplifiable.

We've got both Index._maybe_cast_indexer and Index._convert_scalar_indexer which hopefully we'll only need one of."
559316294,31630,BUG: Series.xs boxing datetime64 incorrectly,jbrockmendel,closed,2020-02-03T20:30:21Z,2020-02-05T01:19:16Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
558964739,31605,Unexpected TypeError with groupby,fran6w,closed,2020-02-03T09:48:30Z,2020-02-05T01:26:33Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
import numpy as np
import pandas as pd

def fct(group):
    return group[1].values.flatten()

df = pd.DataFrame([('A', 1), ('A', 2), ('A', 3), ('B', 4), ('B', 5), ('C', np.nan),])
df.groupby(0).apply(fct)
```
#### Problem description

This code works with pandas 0.25.1 BUT NOT with pandas 1.0.0.
With pandas 1.0.0, the output is:
TypeError: copy() takes no keyword arguments

#### Expected Output
With pandas 0.25.1, the output is:
Series:
0
A    [1.0, 2.0, 3.0]
B         [4.0, 5.0]
C              [nan]

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 7
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : fr_FR.cp1252

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
558874411,31599,CLN: _convert_list_indexer only called by Loc,jbrockmendel,closed,2020-02-03T06:35:00Z,2020-02-05T01:32:53Z,
558869166,31597,REF: implement tests/indexes/objects/,jbrockmendel,closed,2020-02-03T06:20:01Z,2020-02-05T01:33:37Z,Many of the tests in test_base are about `Index.__new__` cases that return a subclass.  This is intended specifically for Index-only tests.
558786194,31592,REF: parametrize indexing tests,jbrockmendel,closed,2020-02-03T00:43:37Z,2020-02-05T01:41:18Z,
559835113,31655,REF: call _convert_scalar_indexer upfront in Series.__getitem__,jbrockmendel,closed,2020-02-04T16:30:51Z,2020-02-05T02:47:04Z,This simplifies a try/except block and moves us towards being able to tighten what we accept in _convert_scalar_indexer among others.  I think we'll soon be able to get rid of the kludge on L866/868.
560031222,31672,REF: move convert_scalar out of cython,jbrockmendel,closed,2020-02-04T23:01:56Z,2020-02-05T02:50:39Z,There's nothing about this that particularly benefits from being in cython (I think until recently this was used within index.pyx) and its clearer in python.  Plus we get a slightly smaller/faster build.
559488902,31643,BUG: pandas.cut does not give the right answer with nullable integer Series input,khdlim,closed,2020-02-04T05:00:20Z,2020-02-05T02:56:09Z,"#### Code Sample, a copy-pastable example if possible

```python
import numpy
import pandas

input_array = numpy.array([1, 2, numpy.nan, 4, 5])
print('Input array: %s\n' % input_array)

bins = numpy.array([0.5, 2.5, 4.5, 6.5])
print ('Using bins: %s\n' % bins)

test_series = pandas.Series(input_array).astype('Int64')

print('Test series:\n\n%s\n' % test_series)
print('Results of cut:\n\n%s\n'% pandas.cut(test_series, bins))
```

Console output:
````
Input array: [ 1.  2. nan  4.  5.]

Using bins: [0.5 2.5 4.5 6.5]

Test series:

0      1
1      2
2    NaN
3      4
4      5
dtype: Int64

Results of cut:

0    (0.5, 2.5]
1    (0.5, 2.5]
2           NaN
3    (0.5, 2.5]
4    (4.5, 6.5]
dtype: category
Categories (3, interval[float64]): [(0.5, 2.5] < (2.5, 4.5] < (4.5, 6.5]]
````
#### Problem description

`output[3]` should be `(2.5, 4.5]` and not `(0.5, 2.5]` as shown.
Tested in pandas 1.0.0 and 0.25.3.


#### Expected Output
````
Input array: [ 1.  2. nan  4.  5.]

Using bins: [0.5 2.5 4.5 6.5]

Test series:

0      1
1      2
2    NaN
3      4
4      5
dtype: Int64

Results of cut:

0    (0.5, 2.5]
1    (0.5, 2.5]
2           NaN
3    (2.5, 4.5]
4    (4.5, 6.5]
dtype: category
Categories (3, interval[float64]): [(0.5, 2.5] < (2.5, 4.5] < (4.5, 6.5]]
````

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.10.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-173-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.6.1
pip              : 20.0.2
setuptools       : 40.6.3
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
559141344,31621,TST: add regression test for apply case from GH-31605,fjetter,closed,2020-02-03T15:03:58Z,2020-02-05T06:28:31Z,"@jorisvandenbossche you wanted to put more tests in for this issue. I put the user reported code example into a test. Did you have anything else in mind? I didn't merge it with the above one since the above acted on an empty Dataframe.

- [x] closes #31605
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
560069783,31678,Backport PR #31668 on branch 1.0.x (REGR: Fixed handling of Categorical in cython ops),meeseeksmachine,closed,2020-02-05T00:31:37Z,2020-02-05T07:31:27Z,Backport PR #31668: REGR: Fixed handling of Categorical in cython ops
550647197,31069,Fix DataFrame.info() range summary for DatetimeIndex,Ircama,closed,2020-01-16T08:20:16Z,2020-02-05T08:02:10Z,"**Fix `DataFrame.info()` range summary for DatetimeIndex**

This PR aims to provide better description of [DatetimeIndex](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html) within [DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) in relation to the display of the range of unsorted indexes, by replacing the original first to last element values with ""min date"" to ""max date"" values.

Fixes https://github.com/pandas-dev/pandas/issues/31116

<del>
This PR fixes range display errors that might occur when [DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) describes a [DatetimeIndex](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html) of big dataframes: depending on the dataframe structure, related *""min date"" to ""max date""* values can be wrong.

Example of output produced by `DataFrame.info()`:

```python
import pandas as pd
idx = pd.date_range('2020-01-15 01:03:05',
                    periods=1000004, freq='ms')
df = pd.DataFrame({'count': range(len(idx))}, index=idx)
df
df.info()
```

`df` output:
```
                           count
2020-01-15 01:03:05.000        0
2020-01-15 01:03:05.001        1
2020-01-15 01:03:05.002        2
2020-01-15 01:03:05.003        3
2020-01-15 01:03:05.004        4
...                          ...
2020-01-15 01:19:44.999   999999
2020-01-15 01:19:45.000  1000000
2020-01-15 01:19:45.001  1000001
2020-01-15 01:19:45.002  1000002
2020-01-15 01:19:45.003  1000003
```

`df.info()` output:
```
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 1000004 entries, 2020-01-15 01:03:05 to 2020-01-15 01:19:45.003000
Freq: L
Data columns (total 1 columns):
count    1000004 non-null int64
dtypes: int64(1)
memory usage: 15.3 MB
```

This PR fixes possible errors within the description: `""DatetimeIndex: ... entries, `*date_a*` to `*date_b* output, where *date_a* and *date_b* should respectively show minimum and maximum date range values.
</del>"
551710635,31116,Unclear DataFrame.info() range summary info for DatetimeIndex,Ircama,closed,2020-01-18T02:45:57Z,2020-02-05T08:03:37Z,"Within [DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html), the description of the [DatetimeIndex](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html) of unsorted indexes might not be clear enough to users, because just showing the first and last element and not the *""min date"" to ""max date""* values.

Please check the following sample:

```python
import pandas as pd
dfs = []
for day in [30, 1, 2, 5]:
    idx = pd.date_range(f'2020-01-{day} 00:00:00', periods=1000, freq='s')
    dfs.append(pd.DataFrame({'count': range(len(idx)), 'date': idx}))

df = pd.concat(dfs)
df.set_index(['date'], inplace=True)
df
df.info()
df.index.min()
df.index.max()
```

Reated output is the following:
```
>>> df
                     count
date
2020-01-30 00:00:00      0
2020-01-30 00:00:01      1
2020-01-30 00:00:02      2
2020-01-30 00:00:03      3
2020-01-30 00:00:04      4
...                    ...
2020-01-05 00:16:35    995
2020-01-05 00:16:36    996
2020-01-05 00:16:37    997
2020-01-05 00:16:38    998
2020-01-05 00:16:39    999

[4000 rows x 1 columns]
>>> df.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 4000 entries, 2020-01-30 00:00:00 to 2020-01-05 00:16:39
Data columns (total 1 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   count   4000 non-null   int64
dtypes: int64(1)
memory usage: 62.5 KB
>>> df.index.min()
Timestamp('2020-01-01 00:00:00')
>>> df.index.max()
Timestamp('2020-01-30 00:16:39')
```

I would expect `2020-01-01 00:00:00 to 2020-01-30 00:16:39` (min and max range values for DatetimeIndex) and not `2020-01-30 00:00:00 to 2020-01-05 00:16:39` (first and last element).

In case, PR https://github.com/pandas-dev/pandas/pull/31069 fixes the issue.

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-18362-Microsoft
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.6.0
Cython           : 0.29.14
pytest           : 5.3.0
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : None
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
s3fs             : None
scipy            : 1.3.3
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
558273256,31505,"REGR: `copy()` within `apply()` raises ValueError: cannot create a DatetimeTZBlock without a tz, as of 1.0.0",DomKennedy,closed,2020-01-31T17:45:45Z,2020-02-05T08:15:33Z,"#### Code Sample, a copy-pastable example if possible

Minimal example:
```python
import pandas as pd

df = pd.DataFrame({""foo"": [pd.Timestamp(""2020"", tz=""UTC"")]}, dtype=""object"")
df.apply(lambda col: col.copy())  # raises exception below
```

Real-life usage:
```python
def filter_dataframe_by_dict(df, filters):
    """"""
    Filter the specified dataframe to only those rows which match the specified filters

    Parameters
    ----------
    df : pd.DataFrame
    
    filters : Mapping
        dict, keyed by a subset of `df.columns`

    Returns
    -------
    pd.DataFrame
       Same columns as `df`, including only those rows which match `filters` on all specified values.
    """"""
    filters = pd.Series(filters, dtype=""object"")
    mask = df[filters.index].apply(
        # astype(""object"") calls `copy()` internally, and is necessary to ensure dtype-agnostic 
        # comparisons.
        lambda row: row.astype(""object"").equals(filters), axis=""columns""
    )
    return df[mask]

records = pd.DataFrame(columns = [""foo"", ""bar"", ""baz""])

records.loc[0] = {""foo"": pd.Timestamp(""2019"", tz=""UTC""), ""bar"": 1, ""baz"": 6.283}
records.loc[1] = {""foo"": pd.Timestamp(""2020"", tz=""UTC""), ""bar"": 2, ""baz"": 6.283}

filters = {""foo"": pd.Timestamp(""2020"", tz=""UTC"")}

filter_dataframe_by_dict(records, filters)  # raises below exception
```

Exception:
```
Traceback (most recent call last):
  File ""pandas_bug.py"", line 29, in <module>
    df.apply(lambda col: col.copy())
  File "".venv/lib/python3.6/site-packages/pandas/core/frame.py"", line 6875, in apply
    return op.get_result()
  File "".venv/lib/python3.6/site-packages/pandas/core/apply.py"", line 186, in get_result
    return self.apply_standard()
  File "".venv/lib/python3.6/site-packages/pandas/core/apply.py"", line 296, in apply_standard
    values, self.f, axis=self.axis, dummy=dummy, labels=labels
  File ""pandas/_libs/reduction.pyx"", line 617, in pandas._libs.reduction.compute_reduction
  File ""pandas/_libs/reduction.pyx"", line 127, in pandas._libs.reduction.Reducer.get_result
  File ""pandas_bug.py"", line 29, in <lambda>
    df.apply(lambda row: row.copy(), axis=""columns"")
  File "".venv/lib/python3.6/site-packages/pandas/core/generic.py"", line 5810, in copy
    data = self._data.copy(deep=deep)
  File "".venv/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 794, in copy
    res = self.apply(""copy"", deep=deep)
  File "".venv/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 442, in apply
    applied = getattr(b, f)(**kwargs)
  File "".venv/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 696, in copy
    return self.make_block_same_class(values, ndim=self.ndim)
  File "".venv/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 281, in make_block_same_class
    return make_block(values, placement=placement, ndim=ndim, klass=type(self))
  File "".venv/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 3028, in make_block
    return klass(values, ndim=ndim, placement=placement)
  File "".venv/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 1723, in __init__
    values = self._maybe_coerce_values(values)
  File "".venv/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 2306, in _maybe_coerce_values
    raise ValueError(""cannot create a DatetimeTZBlock without a tz"")
ValueError: cannot create a DatetimeTZBlock without a tz
```
#### Problem description

Essentially, the problem arises when `.copy()` is called from with an `.apply` call, on a row/column of a DataFrame which:
* has dtype `object`
* consists solely of tz-aware `Timestamp` objects

While this seems like a fairly artificial set of conditions, as the ""real world example"" above is intended to demonstrate, it can indeed occur ""organically"". Appending rows to an initially empty `DataFrame` results in the dtype defaulting to `object` for all columns, so if the `filters` passed to `filter_dataframe_by_dict` happen to only consist of timestamp-valued columns, the error conditions are met.

Note that the error *only* seems to arise in `.apply` calls; making the same calls on the rows return using `iterrrows()` or `iloc` works just fine.

This problem only occurs as of the latest pandas 1.0.0 release.

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-74-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.0
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.6.0
Cython           : None
pytest           : 5.3.0
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.0
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.11
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```
</details>
"
557698335,31470,BUG: groupby().agg fails on categorical column,charlesdong1991,closed,2020-01-30T19:20:30Z,2020-02-05T08:24:35Z,"- [ ] closes #31450 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
559959592,31666,BUG: Block.iget not wrapping timedelta64/datetime64,jbrockmendel,closed,2020-02-04T20:26:39Z,2020-02-05T08:44:26Z,"- [x] closes #31649
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
560217614,31686,DOC: fixup v1.0.1 whatsnew entries,jorisvandenbossche,closed,2020-02-05T08:36:47Z,2020-02-05T08:40:18Z,Fixing the merge of https://github.com/pandas-dev/pandas/pull/31614
559079820,31614,REGR: fix non-reduction apply with tz-aware objects,jorisvandenbossche,closed,2020-02-03T13:21:47Z,2020-02-05T12:34:41Z,"Closes https://github.com/pandas-dev/pandas/issues/31505
"
512994666,29243,Follow up PR: #28097 Simplify branch statement,proost,closed,2019-10-27T17:15:37Z,2020-02-05T08:47:25Z,"
xref #28097

This is follow up pr. eliminate branch statement. ""grouper"" variable is assigned  when handle NAs."
560214735,31685,Backport PR #31614: REGR: fix non-reduction apply with tz-aware objects,jorisvandenbossche,closed,2020-02-05T08:30:02Z,2020-02-05T09:35:53Z,backport https://github.com/pandas-dev/pandas/pull/31614
560221579,31688,Backport PR #31666: BUG: Block.iget not wrapping timedelta64/datetime64,jorisvandenbossche,closed,2020-02-05T08:45:09Z,2020-02-05T09:47:35Z,Backport of https://github.com/pandas-dev/pandas/pull/31666
560222838,31689,Backport PR #29243 on branch 1.0.x (Follow up PR: #28097 Simplify branch statement),meeseeksmachine,closed,2020-02-05T08:47:51Z,2020-02-05T09:47:57Z,Backport PR #29243: Follow up PR: #28097 Simplify branch statement
560027651,31670,groupby fails when MultiIndex contains Int64Index in an empty DataFrame in 1.0.0,jdfinsf,closed,2020-02-04T22:53:05Z,2020-02-05T10:17:06Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame(
    [[123, ""a"", 1.0], [123, ""b"", 2.0]],
    columns=[""id"", ""category"", ""value""]
)
df = df.set_index([""id"", ""category""])
df[df.value < 0].groupby(""id"").sum()
```

#### Problem description

When groupby is over a `Int64Index` in a `MultiIndex` for an empty DataFrame, the groupby fails with error: `ValueError: Unable to fill values because Int64Index cannot contain NA`

#### Expected Output

The groupby should not raise an error, instead the code above should output an empty DataFrame as would happen for `df[df.value < 0].groupby(""category"").sum()` 

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 18.1
setuptools       : 41.6.0
Cython           : None
pytest           : 5.1.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
pytest           : 5.1.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.6
tables           : None
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None
```

</details>"
560228687,31690,TST: add test for regression in groupby with empty MultiIndex level,jorisvandenbossche,closed,2020-02-05T09:00:05Z,2020-02-05T10:17:11Z,Closes https://github.com/pandas-dev/pandas/issues/31670 (the actual fix was already in #29243)
560270921,31692,Backport PR #31690 on branch 1.0.x (TST: add test for regression in groupby with empty MultiIndex level),meeseeksmachine,closed,2020-02-05T10:17:17Z,2020-02-05T11:09:51Z,Backport PR #31690: TST: add test for regression in groupby with empty MultiIndex level
560287338,31693,Timestamps are coerced to different types in 0.25.3 and 1.0.0 when selecting a row,oalfred,closed,2020-02-05T10:45:52Z,2020-02-05T12:10:34Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
df = pd.DataFrame({'TIME': [pd.to_datetime('2020-01-01 12:00')], 'STRING': ['ABC']})
print(type(df.iloc[0]['TIME']))
```
#### Problem description
In 0.25.3 the code above results in `<class 'pandas._libs.tslibs.timestamps.Timestamp'>` whereas in 1.0.0 it is `numpy.datetime64`. I could not find anything about this specifically in the release notes. Is it intended?"
560013308,31668,REGR: Fixed handling of Categorical in cython ops,TomAugspurger,closed,2020-02-04T22:19:19Z,2020-02-05T13:23:23Z,"Fixed by falling back to the wrapped Python version when the cython
version raises NotImplementedError.

Closes https://github.com/pandas-dev/pandas/issues/31450"
558718261,31575,pandas 1.0.0  read_csv() is broken use `open( buffering=0)` option.,paihu,closed,2020-02-02T16:11:58Z,2020-02-05T13:27:05Z,"#### Code Sample

```python
import os
import pandas
import tempfile

fname = """"
with tempfile.NamedTemporaryFile(delete=False, mode=""w+"", encoding=""shift-jis"") as f:
        f.write(""てすと\nbar"")
        fname = f.name
print(fname)

try:
        with open(fname,mode=""r"", encoding=""shift-jis"") as f:
                result = pandas.read_csv(f)
                print(""read shift-jis"")
                print(result)

        with open(fname,mode=""r"", encoding=""shift-jis"") as f:
                result = pandas.read_csv(f,encoding=""utf-8"")
                print(""open shift-jis file and read_csv with encoding: utf-8"")
                print(result)

        with open(fname,mode=""rb"") as f:
                result = pandas.read_csv(f,encoding=""shift-jis"")
                print(""open binary with buffered and read_csv with encoding: shift-jis"")
                print(result)

        with open(fname,mode=""rb"",buffering=0) as f:
                result = pandas.read_csv(f,encoding=""shift-jis"")
                print(""open binary without burrered and read_csv with encoding: shift-jis"")
                print(result)
except Exception as e:
        print(e)

os.unlink(fname)
```
#### Problem description

Pandas 1.0.0, this sample does not work. But pandas 0.25.3, this sample works fine.

Open file with buffering=0 option, f is `RawIOBase`. This case seems encoding option will be ignored.

https://github.com/pandas-dev/pandas/pull/30771/files#diff-0335ae9037e4eb4747749a9f94cffd32R641

https://github.com/pandas-dev/pandas/pull/30771/files#diff-777d7549579ddc0c6e67596ad87e0d27R1879

#### Expected Output
```
/tmp/tmpxxxxxxxxxxx
read shift-jis
   てすと
0  bar
open shift-jis file and read_csv with encoding: utf-8
   てすと
0  bar
open binary with buffered and read_csv with encoding: shift-jis
   てすと
0  bar
open binary without burrered and read_csv with encoding: shift-jis
   てすと
0  bar
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-18362-Microsoft
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : ja_JP.UTF-8
LOCALE           : ja_JP.UTF-8

pandas           : 1.0.0
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.4.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
560360457,31696,Where did check_bool_array_indexer move to?,achapkowski,closed,2020-02-05T13:07:27Z,2020-02-05T13:39:07Z,"I was leveraging the following:

```
pd.api.indexers.check_bool_array_indexer
```

And at v1.0.0, it is gone, what is the comparable operation to this?

"
560371502,31698,Backport PR #31596 on branch 1.0.x (BUG: read_csv used in file like object RawIOBase is not recognize encoding option),meeseeksmachine,closed,2020-02-05T13:27:18Z,2020-02-05T14:16:21Z,Backport PR #31596: BUG: read_csv used in file like object RawIOBase is not recognize encoding option
559110719,31616,REGR: Fixed AssertionError in groupby,TomAugspurger,closed,2020-02-03T14:13:46Z,2020-02-05T14:56:00Z,"Closes https://github.com/pandas-dev/pandas/issues/31522


cc @jbrockmendel. Just raising a `TypeError` when that assert failed didn't work. The `finally` still runs, which raised an assertion error.

It seemed easier to try to just support this case. IIUC, it only occurs when an `(P, n_rows)` input block gets split into `P` result blocks. I believe that

1. The result blocks should all have the same dtype
2. The input block must not have been an extension block, since it's 2d

So it *should* be safe to just cast the result values into an ndarray. Hopefully...

Are there any edge cases I'm not considering? Some kind of `agg` that returns a result that can't be put in a 2D block? Even something like `.agg(lambda x: pd.Period())` 
won't hit this, since it has to be a Cython function. "
560402352,31700,TYP: remove type:ignore from pandas/io/common.py,simonjayhawkins,closed,2020-02-05T14:19:24Z,2020-02-05T15:20:32Z,
560418070,31702,Backport PR #31699 on branch 1.0.x (DOC: Update 1.0.1 release notes),meeseeksmachine,closed,2020-02-05T14:43:33Z,2020-02-05T15:22:37Z,Backport PR #31699: DOC: Update 1.0.1 release notes
560385260,31699,DOC: Update 1.0.1 release notes,TomAugspurger,closed,2020-02-05T13:51:06Z,2020-02-05T15:28:25Z,cc @jorisvandenbossche
560425863,31703,Backport PR #31616 on branch 1.0.x (REGR: Fixed AssertionError in groupby),meeseeksmachine,closed,2020-02-05T14:55:38Z,2020-02-05T15:40:56Z,Backport PR #31616: REGR: Fixed AssertionError in groupby
559866103,31657,https://pandas.pydata.org/pandas-docs/version/ gives a 403 Forbidden error.,dumbledad,closed,2020-02-04T17:21:49Z,2020-02-05T15:50:44Z,"On the pandas website at https://pandas.pydata.org/ if I click on the drop-down menu titled 'documentation' and then select 'older versions' I am taken to https://pandas.pydata.org/pandas-docs/version/ which gives a 403 Forbidden error.
"
548175508,30891,WEB: Link from the website to the docs,datapythonista,closed,2020-01-10T16:33:39Z,2020-02-05T15:50:58Z,"The navigation of the web was implemented assuming the docs will render with the same layout. Since this is not finally the case, and the website and the docs are finally independent, I guess it makes more sense to simply link to the home of the docs.

Otherwise, we'd need to rethink those links, since the last one is broken, and the rest are inconsistent with the navigation of the docs.

Closes https://github.com/pandas-dev/pandas/issues/31657
"
560446826,31704,DOC: fix contributors listing for v1.0.1,jorisvandenbossche,closed,2020-02-05T15:28:05Z,2020-02-05T15:54:43Z,Follow-up on https://github.com/pandas-dev/pandas/pull/31699
560463853,31706,Backport PR #31704 on branch 1.0.x (DOC: fix contributors listing for v1.0.1),meeseeksmachine,closed,2020-02-05T15:54:50Z,2020-02-05T15:57:47Z,Backport PR #31704: DOC: fix contributors listing for v1.0.1
560479684,31707,Manul backport of pr 31666 on 1.0.x,jbrockmendel,closed,2020-02-05T16:18:53Z,2020-02-05T16:27:11Z,"Kind of confused, it looks like the test implemented by 31666 already got backported?"
553581648,31207,BUG: no longer raise user warning when plotting tz aware time series,MarcoGorelli,closed,2020-01-22T14:35:16Z,2020-02-05T18:54:49Z,"- [x] closes #31205
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
560548177,31714,DOC: Fix whatsnew in 1.0.x,TomAugspurger,closed,2020-02-05T18:22:20Z,2020-02-05T19:08:12Z,"Need to figure out a better system for this :/ I've already tagged and pushed 1.0.1, so I'm applying this to my local doc build."
549199309,30984,DOC: plotting backend is not mentioned in the user guide,jorisvandenbossche,closed,2020-01-13T21:44:44Z,2020-02-05T19:09:23Z,"The new plotting backend option is documented for package authors implementing such a backend: https://dev.pandas.io/docs/development/extending.html#plotting-backends, but we should probably mention this in the user guide (https://dev.pandas.io/docs/user_guide/visualization.html) as well."
550586616,31066,DOC: add plotting backends in visualization.rst,rushabh-v,closed,2020-01-16T05:40:06Z,2020-02-05T19:19:30Z,"- [x] closes #30984 
"
560588761,31719,Change of rolling behavior in Pandas 1.0.0 vs 0.25.3,icexelloss,closed,2020-02-05T19:44:12Z,2020-02-05T19:52:14Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame({
        'a': np.arange(10, dtype=float),
        'b': [3.0, np.NaN] * 5,
        'key': list('ddeefffggh'),
    })

expected = df.sort_values(['key', 'a']).assign(
    rolled=lambda d: d.groupby('key')
    .b.rolling(3, min_periods=1)
    .apply(lambda x: x.mean(), raw=True)
    .reset_index(level=0, drop=True)
)

```
#### Problem description
1.0.0 result:
```
     a    b key  rolled                                                                                                                                                                                                                                                                                                                                                                    
0  0.0  3.0   d     3.0                                                                                                                                                                                                                                                                                                                                                                    
1  1.0  NaN   d     3.0                                                                                                                                                                                                                                                                                                                                                                    
2  2.0  3.0   e     3.0                                                                                                                                                                                                                                                                                                                                                                    
3  3.0  NaN   e     3.0                                                                                                                                                                                                                                                                                                                                                                    
4  4.0  3.0   f     3.0                                                                                                                                                                                                                                                                                                                                                                    
5  5.0  NaN   f     3.0                                                                                                                                                                                                                                                                                                                                                                    
6  6.0  3.0   f     3.0                                                                                                                                                                                                                                                                                                                                                                    
7  7.0  NaN   g     NaN                                                                                                                                                                                                                                                                                                                                                                    
8  8.0  3.0   g     3.0                                                                                                                                                                                                                                                                                                                                                                    
9  9.0  NaN   h     NaN
```

0.25.3 behavior:
```
     a    b key  rolled
0  0.0  3.0   d     3.0
1  1.0  NaN   d     NaN
2  2.0  3.0   e     3.0
3  3.0  NaN   e     NaN
4  4.0  3.0   f     3.0
5  5.0  NaN   f     NaN
6  6.0  3.0   f     NaN
7  7.0  NaN   g     NaN
8  8.0  3.0   g     NaN
9  9.0  NaN   h     NaN
```

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
559905526,31660,Replaced .format with f- strings,leandermaben,closed,2020-02-04T18:39:33Z,2020-02-05T20:22:31Z,"xref #29547
for
- [ ] pandas/util/_print_versions.py
- [ ] pandas/tests/tseries/frequencies/test_to_offset.py
"
558855794,31596,BUG: read_csv used in file like object RawIOBase is not recognize encoding option,paihu,closed,2020-02-03T05:40:21Z,2020-02-05T20:56:20Z,"- [x] closes #31575
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
560577240,31718,fix type errors in pandas/tests/extension/json/array.py,SaturnFromTitan,closed,2020-02-05T19:21:46Z,2020-02-05T20:59:29Z,"Part of #28926

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
560518845,31712,DOC Adds newline to dataframe melt,thomasjpfan,closed,2020-02-05T17:24:05Z,2020-02-05T21:06:32Z,"Adds newline before versionadded in `dataframe.melt` so that sphinx/rst can pick it up and render it correctly.

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
555632626,31343,TST/CLN: remove setup_method from test_dtypes,simonjayhawkins,closed,2020-01-27T15:01:11Z,2020-02-05T23:12:11Z,
560752345,31732,Wrong start datetime for data_range,GYHHAHA,closed,2020-02-06T03:15:27Z,2020-02-06T04:09:48Z,"#### Code Sample, a copy-pastable example if possible

```python
>>>pd.date_range(start='1/1/2018', periods=5, freq='M')
DatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',
               '2018-05-31'],
              dtype='datetime64[ns]', freq='M')
```
#### Problem description

Why datatime start from '2018-01-31' ?"
560690149,31726,pandas.read_excel() close the io stream ,dujiaxin,closed,2020-02-05T23:28:56Z,2020-02-06T05:56:45Z,"#### Code Sample, a copy-pastable example if possible

```python
with open(r'a.xlsx','rb') as f: # input the .xlsx
    df_top = pd.read_excel(f,'Sheet1',encoding = 'utf-8').iloc[:,0:2]
    df_less = pd.read_excel(f,'Sheet2',encoding = 'utf-8').iloc[:,0:2]
```
#### Problem description

The first read_excel() works fine. But the second read_excel() get the following:
Traceback (most recent call last):
  File ""xxx.py"", line 127, in <module>
    df_less = pd.read_excel(f,'Sheet2',encoding = 'utf-8').iloc[:,0:2]
  File ""xxx\pandas\io\excel\_base.py"", line 304, in read_excel
    io = ExcelFile(io, engine=engine)
  File ""xxx\pandas\io\excel\_base.py"", line 821, in __init__
    self._reader = self._engines[engine](self._io)
  File ""xxx\pandas\io\excel\_xlrd.py"", line 21, in __init__
    super().__init__(filepath_or_buffer)
  File ""xxx\pandas\io\excel\_base.py"", line 350, in __init__
    filepath_or_buffer.seek(0)
ValueError: seek of closed file

It seems the read_excel() change the io object ""f"". Is that some new feature in 1.0? Older versions don't have the problem.

I think it should be fixed because a function should not change the outer variable.

#### Expected Output
read_excel() should not change the io stream and should be able to read the same io stream several times.
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 13, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
558005722,31484,BUG: Array.__setitem__ failing with nullable boolean mask,charlesdong1991,closed,2020-01-31T09:14:04Z,2020-02-06T09:08:02Z,"- [x] closes #31446 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
560837349,31737,Backport PR #31723 on branch 1.0.x (DOC: Add 1.0.2 whatsnew),meeseeksmachine,closed,2020-02-06T07:49:29Z,2020-02-06T11:12:07Z,Backport PR #31723: DOC: Add 1.0.2 whatsnew
560927444,31742,Rename mad() in mean_absolute_deviation(),michaeldorner,closed,2020-02-06T10:44:28Z,2020-02-06T13:26:36Z,"#### Problem description

MAD is a overloaded shortcut: It stands for *M*edian *A*bsolute *D*eviation (more common), but also can be used as *M*ean *A*bsolute *D*eviation (which is used in pandas). 

#### Expected Output

To avoid confusion, the method should be renamed from `.mad()` to `.mean_absolute_deviation()`, and when implemented the more commonly used `.median_absolute_deviation()`.

If you are fine with it, I would kick off a PR. 

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-24-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.0
numpy            : 1.17.0
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.7.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.0
matplotlib       : 3.1.1
numexpr          : 2.6.9
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.6
tables           : 3.5.2
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
```
</details>
"
560870782,31739,CI: continuous-integration/travis-ci/pr build fails due to unavailable channel,AlexKirko,closed,2020-02-06T09:06:08Z,2020-02-06T15:44:12Z,"#### Problem description

**continuous-integration/travis-ci/pr** build currently seems to fail for everyone, regardless of PR. Error output below:

```
conda env create -q --file=ci/deps/travis-37.yaml
Collecting package metadata (repodata.json): ...working... failed
UnavailableInvalidChannel: The channel is not accessible or is invalid.
  channel name: c3i_test
  channel url: https://conda.anaconda.org/c3i_test
  error code: 404
You will need to adjust your conda configuration to proceed.
Use `conda config --show channels` to view your configuration's current state,
and use `conda config --show-sources` to view config file locations.
The command ""ci/setup_env.sh"" failed and exited with 1 during .
Your build has been stopped.

```"
561017576,31745,CI: Update travis-37.yaml Conda channel ,alimcmaster1,closed,2020-02-06T13:36:53Z,2020-02-06T15:44:14Z,"Remove unrequired channel

- [x] closes #31739 

Cc @TomAugspurger 
"
549097747,30977,JSON Date Handling 1.0 Regressions,WillAyd,closed,2020-01-13T18:13:51Z,2020-02-06T17:06:46Z,"closes #30904

Came across these adding parametrized tests in #30903 which this pulls some elements from but which will also ultimately refactor to avoid duplication; just getting this to work as is

To be clear the regressions were:

1. ISO dates with datetime.date objects was broken

```python
>>> import pandas as pd
>>> import datetime
>>> data = [datetime.date(year=2020, month=1, day=1), ""a""]
>>> pd.Series(data).to_json(date_format=""iso"")
TypeError: Expected datetime object
```

2. `pd.NaT` labels would now yield the sentinel value instead of writing ""null""

```python
>>> data = [pd.Timestamp(""2020-01-01""), pd.NaT]
>>> pd.Series(data, index=data).to_json()
'{""1577836800000"":1577836800000,""-9223372036854"":null}'
```

Both have been fixed here"
559443357,31640,CLN: prelims for MultiIndex.get_value,jbrockmendel,closed,2020-02-04T02:08:12Z,2020-02-06T17:22:57Z,"Unravelling MultiIndex.get_value and get_loc is shaping up to be a PITA.  This separates out the easiest parts of the get_value cleanup.

@TomAugspurger can you confirm that the change made here to the `__finalize__` call is correct?  If so, suggest a test?
"
561029433,31747,read_parquet gives ArrowIOError,azimgivron,closed,2020-02-06T13:57:12Z,2020-02-06T19:03:16Z,"I am trying to open a .parquet file in pandas but I am failing at it. I tried the solution proposed in https://stackoverflow.com/questions/54201799/pandas-cannot-read-parquet-files-created-in-pyspark but it is not working for me. 

Does anyone have a clue of how to fix this ? 

#### 

python code:

pd.read_parquet('filepath')

#### 

Error received in the consol:


ArrowIOError Traceback (most recent call last)
<ipython-input-32-623a6e60c94a> in <module>
----> 2 pd.read_parquet('filepath'')

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\parquet.py in read_parquet(path, engine, columns, **kwargs)
    308 
    309     impl = get_engine(engine)
--> 310     return impl.read(path, columns=columns, **kwargs)

C:\ProgramData\Anaconda3\lib\site-packages\pandas\io\parquet.py in read(self, path, columns, **kwargs)
    123         kwargs[""use_pandas_metadata""] = True
    124         result = self.api.parquet.read_table(
--> 125             path, columns=columns, **kwargs
    126         ).to_pandas()
    127         if should_close:

C:\ProgramData\Anaconda3\lib\site-packages\pyarrow\parquet.py in read_table(source, columns, use_threads, metadata, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size)
   1272                             read_dictionary=read_dictionary,
   1273                             buffer_size=buffer_size,
-> 1274                             filesystem=filesystem, filters=filters)
   1275     else:
   1276         pf = ParquetFile(source, metadata=metadata,

C:\ProgramData\Anaconda3\lib\site-packages\pyarrow\parquet.py in __init__(self, path_or_paths, filesystem, schema, metadata, split_row_groups, validate_schema, filters, metadata_nthreads, read_dictionary, memory_map, buffer_size)
   1058 
   1059         if validate_schema:
-> 1060             self.validate_schemas()
   1061 
   1062     def equals(self, other):

C:\ProgramData\Anaconda3\lib\site-packages\pyarrow\parquet.py in validate_schemas(self)
   1090                 self.schema = self.common_metadata.schema
   1091             else:
-> 1092                 self.schema = self.pieces[0].get_metadata().schema
   1093         elif self.schema is None:
   1094             self.schema = self.metadata.schema

C:\ProgramData\Anaconda3\lib\site-packages\pyarrow\parquet.py in get_metadata(self)
    558         metadata : FileMetaData
    559         """"""
--> 560         f = self.open()
    561         return f.metadata
    562 

C:\ProgramData\Anaconda3\lib\site-packages\pyarrow\parquet.py in open(self)
    565         Returns instance of ParquetFile
    566         """"""
--> 567         reader = self.open_file_func(self.path)
    568         if not isinstance(reader, ParquetFile):
    569             reader = ParquetFile(reader, **self.file_options)

C:\ProgramData\Anaconda3\lib\site-packages\pyarrow\parquet.py in _open_dataset_file(dataset, path, meta)
    940         read_dictionary=dataset.read_dictionary,
    941         common_metadata=dataset.common_metadata,
--> 942         buffer_size=dataset.buffer_size
    943     )
    944 

C:\ProgramData\Anaconda3\lib\site-packages\pyarrow\parquet.py in __init__(self, source, metadata, common_metadata, read_dictionary, memory_map, buffer_size)
    135         self.reader.open(source, use_memory_map=memory_map,
    136                          buffer_size=buffer_size,
--> 137                          read_dictionary=read_dictionary, metadata=metadata)
    138         self.common_metadata = common_metadata
    139         self._nested_paths_by_prefix = self._build_nested_paths()

C:\ProgramData\Anaconda3\lib\site-packages\pyarrow\_parquet.pyx in pyarrow._parquet.ParquetReader.open()

C:\ProgramData\Anaconda3\lib\site-packages\pyarrow\error.pxi in pyarrow.lib.check_status()

ArrowIOError: Invalid parquet file. Corrupt footer.
####


version:


python 3.7.6
pandas 1.0.1
"
560646987,31723,DOC: Add 1.0.2 whatsnew,alimcmaster1,closed,2020-02-05T21:43:43Z,2020-02-06T21:05:06Z,"- Need this for PR https://github.com/pandas-dev/pandas/issues/31677

cc. @TomAugspurger @jorisvandenbossche "
561162838,31751,F string fixes,drewseibert,closed,2020-02-06T17:33:01Z,2020-02-06T21:43:36Z,Related to issue https://github.com/pandas-dev/pandas/issues/29547
560056886,31677,Pandas excel output specify column names to write is broken in 1.0.0,mikemc3,closed,2020-02-05T00:06:23Z,2020-02-06T23:41:13Z,"#### Example code:
df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),columns=['col_a', 'col_b', 'col_c'])
excelwritename = '/tmp/Test%s.xlsx' %pd.datetime.now(pytz.timezone('US/Pacific')).strftime('%b%d_%I_%M %p')
with pd.ExcelWriter(excelwritename, engine='xlsxwriter',datetime_format='mmm dd yyyy hh:mm AM/PM') as writer:
    df.to_excel(writer,columns=['col_a','col_b'],sheet_name = 'xyz')

#### Problem description:
specifying column names to output to excel is no longer working in pandas 1.0.0 and it outputs all columns in the df. 
Checked different versions of xlsxwriter, but pandas 0.25.3 works fine, while 1.0.0 does not.


#### Expected Output
It should only output columns that are specified, but it outputs all columns of df
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.2.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : 0.29.14
pytest           : 5.3.4
hypothesis       : 4.54.2
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.0
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.4
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.0
numba            : 0.48.0
</details>
"
560693198,31727,TST/CLN: dtype test_construct_from_string,simonjayhawkins,closed,2020-02-05T23:38:33Z,2020-02-06T23:41:32Z,
561097258,31749,Backport PR #31745 on branch 1.0.x (CI: Update travis-37.yaml Conda channel ),meeseeksmachine,closed,2020-02-06T15:44:25Z,2020-02-06T23:41:55Z,Backport PR #31745: CI: Update travis-37.yaml Conda channel 
559215337,31625,CLN: inline indexing 1-liners,jbrockmendel,closed,2020-02-03T17:07:27Z,2020-02-06T23:45:59Z,"The only 2-liners here are _convert_scalar_indexer and _convert_slice_indexer, which are effectively `axis = min(axis, self.ndim-1)`, but that is unnecessary since all places where these are called have axis < self.ndim."
561204302,31753,CLN/TST: organize DatetimeIndex tests,jbrockmendel,closed,2020-02-06T18:54:23Z,2020-02-06T23:53:21Z,1/many
560626630,31722,API/BUG: Inconsistent errors/msgs between loc vs at,jbrockmendel,closed,2020-02-05T21:02:07Z,2020-02-06T23:58:05Z,"```
df = DataFrame({""A"": [1, 2, 3]}, index=list(""abc""))

>>> df.at[0]
ValueError: At based indexing on an non-integer index can only have non-integer indexers
>>> df.loc[0]
TypeError: cannot do label indexing on <class 'pandas.core.indexes.base.Index'> with these indexers [0] of <class 'int'>
```

I would expect these to behave the same."
560527706,31713,REF: Index.get_value call self.get_loc instead of self._engine.get_loc,jbrockmendel,closed,2020-02-05T17:41:31Z,2020-02-06T23:58:07Z,"This makes Index.get_value match ExtensionIndex.get_value, so we can remove the latter.

Along with implementing _should_fallback_to_positional, this allows us to rip out Float64Index.get_value."
504820310,28874,use requests when it is installed,ocefpaf,closed,2019-10-09T18:31:52Z,2020-02-06T23:59:52Z,"closes #16716
closes #28825
closes #28826
solves https://github.com/pandas-dev/pandas/pull/16910

- [ ] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Continuation of https://github.com/pandas-dev/pandas/pull/21504 and https://github.com/pandas-dev/pandas/pull/17087

This PR is not ready but I'd appreciate any early comment/feedback."
560558714,31715,REF: organize MultiIndex indexing tests,jbrockmendel,closed,2020-02-05T18:44:09Z,2020-02-07T00:02:47Z,
560124547,31681,REF: Use _maybe_cast_indexer and dont override Categorical.get_loc,jbrockmendel,closed,2020-02-05T03:55:04Z,2020-02-07T00:05:47Z,
560118124,31680,CLN: remove libindex.get_value_at,jbrockmendel,closed,2020-02-05T03:29:43Z,2020-02-07T00:06:45Z,"There was a usage in MultiIndex.get_value that was just removed, leaving just the one in sparse.array that this changes to handle in python-space."
560129377,31683,Why does CategoricalIndex.get_value call _convert_scalar_indexer?,jbrockmendel,closed,2020-02-05T04:14:54Z,2020-02-07T00:12:53Z,"cc @jreback @jorisvandenbossche @TomAugspurger the meat of CategoricalIndex.get_value reads:

```
def get_value(self, series, key):
        k = key
        try:
            k = self._convert_scalar_indexer(k, kind=""getitem"")
            indexer = self.get_loc(k)
            return series.take([indexer])[0]
        except (KeyError, TypeError):
            pass

        # we might be a positional inexer
        return Index.get_value(self, series, key)
```

The thing is, if we actually track down that _convert_scalar_indexer call, with `kind=""getitem""` it always passes through to the base class method, which ends up checking:

```
            if kind == ""getitem"" and is_float(key):
                if not self.is_floating():
                    self._invalid_indexer(""label"", key)
```

But AFAICT `CategoricalIndex.is_floating()` is always False, so it looks like the call in `get_value` is a no-op, which I expect was not the original intention.

Also possibly undesired:

```
idx = pd.Index(range(4)).astype(""f8"") / 2
cidx = pd.CategoricalIndex(idx)

ser1 = pd.Series(range(4), index=idx)
ser2 = pd.Series(range(4), index=cidx)

>>> ser1[1.0] 
2
>>> ser2[1.0]
[...] TypeError [...]
>>> ser1[2]
[...] KeyError [...]
>>> ser2[2]
2
```

Can anyone confirm if the no-op call and/or different `__getitem__` behavior is intentional?"
561335373,31764,Backport PR #31729 on branch 1.0.x (BUG: Fix to_excel writers handling of cols),meeseeksmachine,closed,2020-02-06T23:41:26Z,2020-02-07T00:38:54Z,Backport PR #31729: BUG: Fix to_excel writers handling of cols
560495041,31709,"CLN: _convert_scalar_indexer only handle ""loc"" and ""getitem""",jbrockmendel,closed,2020-02-05T16:43:15Z,2020-02-07T01:54:35Z,"`_convert_scalar_indexer` is called with kind=""iloc"" from only one place, and in that case

1) the base class method is equivalent to just the 1-liner `self._validate_indexer(""positional"", key, ""iloc"")`
2) all subclasses just call the base class method

So by inlining that 1-liner, we can take the ""iloc"" case out of `_convert_scalar_indexer` altogether.

kind=None is never passed, so we can rip that right out.

Ultimately I want to disentable/de-duplicate/disambiguate `_convert_scalar_indexer` vs `_maybe_cast_indexer`

Partial overlap with #31625."
549067823,30974,DOC: Fix isort output in CI,datapythonista,closed,2020-01-13T17:12:03Z,2020-02-07T06:12:19Z,"In the CI, we're using `isort` to check whether the imports in Python files are sorted in an standard way. See https://github.com/pandas-dev/pandas/blob/master/ci/code_checks.sh#L114

The way it is implemented, we expect isort to not print anything in the terminal if everything is ok, and to show errors for the errors encountered. This is mostly true, except that `isort` is also reporting when there are files being skipped.

The result of this, is that we highlight in the CI as an error when there are files being skipped. See https://github.com/pandas-dev/pandas/pull/30946/checks?check_run_id=385833706#step:6:36

This can be misleading, and we should try to avoid it. Without adding much complexity to the already complex CI. Not sure if there is an `isort` parameter to simply not report the skipped files. Or if there is an easy way to not highlight the skipped files as an error.
"
560764258,31733,Add --quiet option to isort command,d1618033,closed,2020-02-06T04:00:01Z,2020-02-07T06:12:44Z,"- [x] closes #30974
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
558667392,31564,CI: deprecation warning tests failing on linux py37_local pipeline,AlexKirko,closed,2020-02-02T09:13:10Z,2020-02-07T07:41:19Z,"#### Problem description

As of now, the Linux py37_locale pipeline seems to fail independent of PR:

```
_________________ TestRegistration.test_registering_no_warning _________________
2020-02-02T08:55:26.6331296Z [gw1] linux -- Python 3.7.6 /home/vsts/miniconda3/envs/pandas-dev/bin/python

self = <pandas.tests.plotting.test_converter.TestRegistration object at 0x7f97699062d0>

     def test_registering_no_warning(self):
         plt = pytest.importorskip(""matplotlib.pyplot"")
         s = Series(range(12), index=date_range(""2017"", periods=12))
         _, ax = plt.subplots()
     
        # Set to the ""warn"" state, in case this isn't the first test run
        register_matplotlib_converters()
        with tm.assert_produces_warning(DeprecationWarning, check_stacklevel=False):
            # GH#30588 DeprecationWarning from 2D indexing
>            ax.plot(s.index, s.values)
E               AssertionError: Did not see expected warning of class 'DeprecationWarning'
```

#### Full failures output

<details>

```
2020-02-02T08:55:26.6329275Z =================================== FAILURES ===================================
2020-02-02T08:55:26.6329476Z _________________ TestRegistration.test_registering_no_warning _________________
2020-02-02T08:55:26.6331296Z [gw1] linux -- Python 3.7.6 /home/vsts/miniconda3/envs/pandas-dev/bin/python
2020-02-02T08:55:26.6331555Z 
2020-02-02T08:55:26.6331736Z self = <pandas.tests.plotting.test_converter.TestRegistration object at 0x7f97699062d0>
2020-02-02T08:55:26.6331919Z 
2020-02-02T08:55:26.6332094Z     def test_registering_no_warning(self):
2020-02-02T08:55:26.6332272Z         plt = pytest.importorskip(""matplotlib.pyplot"")
2020-02-02T08:55:26.6332458Z         s = Series(range(12), index=date_range(""2017"", periods=12))
2020-02-02T08:55:26.6332629Z         _, ax = plt.subplots()
2020-02-02T08:55:26.6332795Z     
2020-02-02T08:55:26.6333232Z         # Set to the ""warn"" state, in case this isn't the first test run
2020-02-02T08:55:26.6333431Z         register_matplotlib_converters()
2020-02-02T08:55:26.6333618Z         with tm.assert_produces_warning(DeprecationWarning, check_stacklevel=False):
2020-02-02T08:55:26.6333793Z             # GH#30588 DeprecationWarning from 2D indexing
2020-02-02T08:55:26.6333984Z >           ax.plot(s.index, s.values)
2020-02-02T08:55:26.6334129Z 
2020-02-02T08:55:26.6334293Z pandas/tests/plotting/test_converter.py:71: 
2020-02-02T08:55:26.6334479Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-02-02T08:55:26.6334637Z 
2020-02-02T08:55:26.6334800Z self = <contextlib._GeneratorContextManager object at 0x7f9769b95250>
2020-02-02T08:55:26.6334986Z type = None, value = None, traceback = None
2020-02-02T08:55:26.6335129Z 
2020-02-02T08:55:26.6335310Z     def __exit__(self, type, value, traceback):
2020-02-02T08:55:26.6335477Z         if type is None:
2020-02-02T08:55:26.6335637Z             try:
2020-02-02T08:55:26.6335813Z >               next(self.gen)
2020-02-02T08:55:26.6336232Z E               AssertionError: Did not see expected warning of class 'DeprecationWarning'
2020-02-02T08:55:26.6336444Z 
2020-02-02T08:55:26.6336863Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/contextlib.py:119: AssertionError
2020-02-02T08:55:26.6337076Z ___________________ TestRegistration.test_option_no_warning ____________________
2020-02-02T08:55:26.6337500Z [gw1] linux -- Python 3.7.6 /home/vsts/miniconda3/envs/pandas-dev/bin/python
2020-02-02T08:55:26.6337681Z 
2020-02-02T08:55:26.6337867Z self = <pandas.tests.plotting.test_converter.TestRegistration object at 0x7f9769c690d0>
2020-02-02T08:55:26.6338027Z 
2020-02-02T08:55:26.6338188Z     def test_option_no_warning(self):
2020-02-02T08:55:26.6338368Z         pytest.importorskip(""matplotlib.pyplot"")
2020-02-02T08:55:26.6338539Z         ctx = cf.option_context(""plotting.matplotlib.register_converters"", False)
2020-02-02T08:55:26.6338733Z         plt = pytest.importorskip(""matplotlib.pyplot"")
2020-02-02T08:55:26.6338903Z         s = Series(range(12), index=date_range(""2017"", periods=12))
2020-02-02T08:55:26.6339083Z         _, ax = plt.subplots()
2020-02-02T08:55:26.6339242Z     
2020-02-02T08:55:26.6339402Z         # Test without registering first, no warning
2020-02-02T08:55:26.6339586Z         with ctx:
2020-02-02T08:55:26.6339759Z             # GH#30588 DeprecationWarning from 2D indexing on Index
2020-02-02T08:55:26.6339947Z             with tm.assert_produces_warning(DeprecationWarning, check_stacklevel=False):
2020-02-02T08:55:26.6340121Z >               ax.plot(s.index, s.values)
2020-02-02T08:55:26.6340414Z 
2020-02-02T08:55:26.6340592Z pandas/tests/plotting/test_converter.py:105: 
2020-02-02T08:55:26.6340763Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-02-02T08:55:26.6340928Z 
2020-02-02T08:55:26.6341093Z self = <contextlib._GeneratorContextManager object at 0x7f976ac1aa90>
2020-02-02T08:55:26.6341279Z type = None, value = None, traceback = None
2020-02-02T08:55:26.6341440Z 
2020-02-02T08:55:26.6341601Z     def __exit__(self, type, value, traceback):
2020-02-02T08:55:26.6341764Z         if type is None:
2020-02-02T08:55:26.6341941Z             try:
2020-02-02T08:55:26.6342099Z >               next(self.gen)
2020-02-02T08:55:26.6342547Z E               AssertionError: Did not see expected warning of class 'DeprecationWarning'
2020-02-02T08:55:26.6342814Z 
2020-02-02T08:55:26.6343228Z ../../../miniconda3/envs/pandas-dev/lib/python3.7/contextlib.py:119: AssertionError
```

</details>

Saw this in two PRs that have gone through checks recently, and none of them touch anything that relates to the failing tests (#31562 #31559)."
517454962,29404,BUG: GH29310 HDF file compression not working,yp1996,closed,2019-11-04T23:06:06Z,2020-02-07T07:55:31Z,"- [x] closes #29310 
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Re #29310, the complib and complevel parameters were not being passed down all the way previously, hence HDF compression not working.  

I noticed that the implementation of to_hdf() specifies that compression is not allowed for fixed formats:  
` if not s.is_table and complib:
            raise ValueError(""Compression not supported on Fixed format stores"")`

I'm guessing that means the performance comparison section for https://github.com/pandas-dev/pandas/pull/28890/files will also need to be updated to remove the test_fixed_compress test @WuraolaOyewusi? 

Also, after the update, the following test is currently failing: 
![image](https://user-images.githubusercontent.com/13011161/68165200-a4736c00-ff13-11e9-964f-3ebc105e7e46.png)

due to a ValueError for using compression with a fixed format, and I'm not sure as to why the expected behaviour for this test is what it is? Why should setting complib disable compression? I would appreciate any further info on that.




"
560650673,31724,API/BUG: make .at raise same exceptions as .loc,jbrockmendel,closed,2020-02-05T21:51:48Z,2020-02-07T08:49:26Z,"- [x] closes #31722
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This also (very) indirectly addresses #31683 which in turn will let us get rid of `CategoricalIndex.get_value` altogether."
561650453,31776,cannot import name 'OrderedDict',aminemosbah,closed,2020-02-07T13:53:16Z,2020-02-07T17:01:55Z,"#### cannot import name 'OrderedDict'
trying to work on collab and panda failed to import  'OrderedDict'

here is the code 
import pandas as pd
data = pd.read_csv('/content/causalnex/fulcase_causal.csv', delimiter=';', index_col=0)
data.head(5)

 here is the message 
/usr/local/lib/python3.6/dist-packages/pandas/io/formats/html.py in <module>()
      8 from textwrap import dedent
      9 
---> 10 from pandas.compat import OrderedDict, lzip, map, range, u, unichr, zip
     11 
     12 from pandas.core.dtypes.generic import ABCMultiIndex

ImportError: cannot import name 'OrderedDict'

any suggestion "
549918557,31029,Disallow bare pytest.raises,DylanBrdt,closed,2020-01-15T01:53:12Z,2020-02-07T17:08:02Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

https://github.com/pandas-dev/pandas/issues/30999"
561683368,31778,Error: tuple index out of range,aman-thind,closed,2020-02-07T14:50:49Z,2020-02-07T18:29:50Z,"I/P
dims = np.shape(df)
print(dims)
O/P
(142, 3)

I/P
pixel_matrix = np.reshape(df, (dims[0]* dims[1], dims[2]))
print(np.shape(pixel_matrix))
O/P
IndexError                                Traceback (most recent call last)
<ipython-input-71-0f88d37eb802> in <module>
----> 1 pixel_matrix = np.reshape(df, (dims[0]* dims[1], dims[2]))
      2 print(np.shape(pixel_matrix))

IndexError: tuple index out of range


How to solve this error? 
"
558774393,31589,REF: Move Loc-only methods to Loc,jbrockmendel,closed,2020-02-02T23:15:21Z,2020-02-08T01:41:43Z,
559087516,31615,pd.NA is converted to {} instead of null with pd.DataFrame.to_json,pvieito,closed,2020-02-03T13:34:48Z,2020-02-08T08:11:59Z,"#### Code Sample

```python
import numpy as np
import pandas as pd

pd.DataFrame([[np.nan]], columns=[""NA""]).to_json(orient=""records"")
# '[{""NA"":null}]'
pd.DataFrame([[None]], columns=[""NA""]).to_json(orient=""records"")
# '[{""NA"":null}]'
pd.DataFrame([[pd.NA]], columns=[""NA""]).to_json(orient=""records"")
# '[{""NA"":{}}]'
```
#### Problem Description

`pd.NA` should behave as `np.nan` and `None` when converting a DataFrame to JSON.

#### Expected Output

In the resulting JSON the `pd.NA` instances should be converted to JSON's `null`.
"
561029573,31748,BUG: Fixed encoding of pd.NA with to_json,ArtificialQualia,closed,2020-02-06T13:57:26Z,2020-02-08T08:17:10Z,"- [x] closes #31615
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
561498926,31769,ERR: improve error message for invalid indexer,jorisvandenbossche,closed,2020-02-07T08:47:08Z,2020-02-08T10:27:06Z,"Convert the long repr of the Index into just the name. So something like

```
cannot do slice indexing on <class 'pandas.core.indexes.datetimes.DatetimeIndex'> with these indexers [key] of <class 'float'>
```

becomes

```
cannot do slice indexing on DatetimeIndex with these indexers [key] of type float""
```"
561988099,31804,Backport PR #31748: BUG: Fixed encoding of pd.NA with to_json,jorisvandenbossche,closed,2020-02-08T08:15:36Z,2020-02-08T10:31:41Z,Backport PR #31748
561862287,31791,CI: fix feather test,jbrockmendel,closed,2020-02-07T20:44:58Z,2020-03-12T13:31:17Z,
562003808,31807,Not contains Question,Ayvytr,closed,2020-02-08T10:58:47Z,2020-02-08T13:07:49Z,"#### Not contains Question

```python
    # df is a DataFrame contains column ""title"", then I filter df by column ""title"" below, it works.
    useless = df[df[""title""].str.contains(r""[\d]+[册|本]"", regex=True)]
    # but I want a not contains value below, it raise error: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
    useless = df[not df[""title""].str.contains(r""[\d]+[册|本]"", regex=True)]
  
```
#### Problem description

How can I get a not contains value?
"
562003701,31806,Series.str.contains Question,Ayvytr,closed,2020-02-08T10:57:34Z,2020-02-08T13:16:08Z,"#### Not contains Question

```python
    # df is a DataFrame contains column ""title"", then I filter df by column ""title"" below, it works.
    useless = df[df[""title""].str.contains(r""[\d]+[册|本]"", regex=True)]
    # but I want a not contains value below, it raise error: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
    useless = df[not df[""title""].str.contains(r""[\d]+[册|本]"", regex=True)]
  
```
#### Problem description

How can I get a not contains value?
"
561219013,31755,CLN: misplaced TimedeltaIndex tests,jbrockmendel,closed,2020-02-06T19:24:38Z,2020-02-09T15:39:46Z,
555148134,31315,REF: tighten what we accept in TimedeltaIndex._simple_new,jbrockmendel,closed,2020-01-25T22:37:05Z,2020-02-09T15:40:34Z,
562094967,31812,CLN: tests.indexing.common,jbrockmendel,closed,2020-02-08T23:43:54Z,2020-02-09T15:41:21Z,"A lot of code in there not doing anything, AFAICT leftover from ix cross-comparisons."
561371939,31765,REF: Remove CategoricalIndex.get_value,jbrockmendel,closed,2020-02-07T01:47:17Z,2020-02-09T15:47:49Z,It is no longer needed following #31724.
513035393,29249,CLN: remove algorithms.match,jbrockmendel,closed,2019-10-27T23:02:56Z,2020-02-09T17:05:17Z,"Appears to be dead code, allows for simplification of _get_hashtable_algo."
555027618,31299,DEPR: disallow series[[slice(...)]],jbrockmendel,closed,2020-01-25T01:52:15Z,2020-02-09T17:08:54Z,"One of the more convoluted parts of `Series.__getitem__` is for handling single-entry lists that contain a slice, where we end up just returning `self[key[0]]`.  We have two tests that hit this path, but it isn't clear to me why this should be supported.
"
562187747,31822,DOC: Fix style guide typos,dsaxton,closed,2020-02-09T14:49:47Z,2020-02-09T17:12:14Z,A few minor spelling / grammar edits
561754012,31785,Crash when using align between a Series and a DataFrame and ffill (or bfill) method,m-lal,closed,2020-02-07T16:49:59Z,2020-02-09T17:12:59Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
import pandas as pd

x = pd.Series(range(10), index=range(10))
y = pd.DataFrame(0.0, index=range(20), columns=range(12))

# works fine (no fill method)
x.align(y)

# crash
x.align(y, method='ffill')

# works fine if x is converted to a DataFrame
x.to_frame().align(y, method='ffill')



```
#### Problem description

This problem only occurs on pandas version >= 1.0.0
Pretty simple bug to reproduce... Quite a serious issue! It should probably shows elsewhere also.

**When using align between a Series and a DataFrame with a fill method the code hits this 

assert isinstance(obj, type(self)), type(obj)
AssertionError: <class 'pandas.core.frame.DataFrame'>**

My guess is that isinstance(obj, type(self)) try to assert that a Series is a DataFrame which is not the case.



#### Expected Output
Similar to x.align(y), with forward filled values.

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 85 Stepping 4, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.0.3
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : None
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0
</details>
"
509248255,29079,DOC: mad description incorrectly includes 'numeric_only',stackedsax,closed,2019-10-18T18:37:33Z,2020-02-09T17:19:45Z,"#### Problem description

I noticed a small mismatch between the function signature of `mad` and its description here:

* https://pandas-docs.github.io/pandas-docs-travis/reference/api/pandas.DataFrame.mad.html?highlight=mad#pandas.DataFrame.mad

The function signature is correct:

> DataFrame.mad(self, axis=None, skipna=None, level=None)

The description, meanwhile, includes 'numeric_only':

> numeric_only : bool, default None
> Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.

Passing the `numeric_only` parameter to `mad` will throw an error, as one would expect.

It looks like both `mad` and `compound` are using the `@Appender(_num_doc)` decorator, which points to a description that includes the `numeric_only` parameter even though neither of those methods use `numeric_only`.  

The `compound` method is being deprecated, so that may be less of an issue.

There are a couple of different ways I could think to fix it, but I just wanted to confirm that I had this right before I did anything."
561242304,31758,REF/TST: misplaced tests in tests.indexes.period,jbrockmendel,closed,2020-02-06T20:10:22Z,2020-02-09T17:23:01Z,
561399314,31767,TST: parametrize some indexing tests,jbrockmendel,closed,2020-02-07T03:29:37Z,2020-02-09T17:23:42Z,2/many
558461002,31526,DOC: 10 minutes - <TAB> completion section showing inconsistent suggestions,Cardosaum,closed,2020-02-01T02:05:43Z,2020-02-09T17:23:54Z,"#### Code Sample, a copy-pastable example if possible

From the very end of the section ""[Object Creation](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html#object-creation)""
```python

In [12]: df2.<TAB>  # noqa: E225, E999
df2.A                  df2.bool
df2.abs                df2.boxplot
df2.add                df2.C
df2.add_prefix         df2.clip
df2.add_suffix         df2.clip_lower
df2.align              df2.clip_upper
df2.all                df2.columns
df2.any                df2.combine
df2.append             df2.combine_first
df2.apply              df2.consolidate
df2.applymap
df2.D
```
And the following text:
```rst
As you can see, the columns ``A``, ``B``, ``C``, and ``D`` are automatically
tab completed. ``E`` is there as well; the rest of the attributes have been
truncated for brevity.
```

#### Problem description
In the text we read that columns `A`, `B`, `C` and  `D` are shown. However, `df2.B` isn't shown at all

#### Expected Output

I think we could just add some more <TAB> completions, along side `df2.B` to make the documentation more consistent to what is being said.

Something similar to:

```python

In [12]: df2.<TAB>  # noqa: E225, E999
df2.A                  df2.bool
df2.abs                df2.boxplot
df2.add                df2.C
df2.add_prefix         df2.clip
df2.add_suffix         df2.columns
df2.align              df2.combine
df2.all                df2.combine_first
df2.any                df2.D
df2.append             df2.describe
df2.apply              df2.diff
df2.applymap           df2.div
df2.B                  df2.divide
```

##### Note

I'd like to know what does `# noqa: E225, E999` means. I didn't see this reference early in this guide"
562120225,31818,DOC: '10 minutes to pandas' - <TAB> completion section now show a con…,Cardosaum,closed,2020-02-09T04:30:31Z,2020-02-09T17:23:57Z,"…sistent block of suggestions. Solve issue #31526

- [x] closes #31526
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I didn't run those listed tests above, however I doubt it will break something.

This PR only correct a *Typo* I found on documentation. I hope I'll be useful to improve this project."
562100985,31813,BUG: iloc setitem with 3d indexer not raising,jbrockmendel,closed,2020-02-09T00:44:16Z,2020-02-09T17:24:29Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

also get the exceptions/messages in test_setitem_ndarray_3d to be maximally-specific"
562117859,31816,BUG: Series[dim3array] failing to raise ValueError for some Index subclasses,jbrockmendel,closed,2020-02-09T04:00:15Z,2020-02-09T17:24:54Z,
560785803,31736,REF: turn _try_mi into MultiIndex._get_values_for_loc,jbrockmendel,closed,2020-02-06T05:18:42Z,2020-02-09T17:26:37Z,Discussed in #31640.
562088983,31811,groupby returns non-pertinent combinations in result,RS574,closed,2020-02-08T22:43:04Z,2020-02-09T17:29:06Z,"#### Code Sample, a copy-pastable example if possible

```python

import pandas as pd

d1 = ['foo', 'bar'] * 50
d2 = list('ABCD' * 25)
d3 = pd.date_range('2019-01-01', periods=100, freq='D')
d4 = list(range(1, 101, 1))
d5 = list(range(1000, 100001, 1000))

df = pd.DataFrame(dict(col1=d1, col2=d2, col3=d3, col4=d4, col5=d5))

bins = [0, 30, float('inf')]

grouped = df.groupby(['col1', 'col2', pd.Grouper(key='col3', freq='MS'), 
                      pd.cut(df.col4, bins)])
summary = grouped['col5'].sum()
```
#### Problem description
The groupby() function returns a result that has every possible combination of the grouping criteria, and shows 'NaN' for non-significant combinations (see screenshot of the results below). This behavior was not observed in pandas version 0.24.2. Earlier version only returned combinations with values (ie, combinations with NaN values were not included in the result.)


#### Expected Output
![image](https://user-images.githubusercontent.com/58710879/74092960-2b932b80-4a91-11ea-928c-b2c51a865cda.png)

#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en
LOCALE           : None.None

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : 0.29.14
pytest           : 5.3.5
hypothesis       : 5.4.1
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.48.0
"
561373083,31766,REF: share _partial_date_slice between PeriodIndex/DatetimeIndex,jbrockmendel,closed,2020-02-07T01:51:47Z,2020-02-09T17:29:46Z,
555352592,31335,REF: share code for set-like ops in DTI/TDI/PI,jbrockmendel,closed,2020-01-27T04:41:04Z,2020-02-09T17:30:46Z,"The DatetimeIndexOpsMixin._concat_same_type this has can probably become the ExtensionIndex._concat_same_type default.  I expect IntervalIndex can be updated to look the same pretty easily, not so sure about CategoricalIndex based on a quick look."
562112496,31814,REF: use public indexers in groupby.ops,jbrockmendel,closed,2020-02-09T02:53:12Z,2020-02-09T17:48:33Z,
560731467,31730,fixed mypy errors in mypy-pandas.tests.extension.decimal.test_decimal,SaturnFromTitan,closed,2020-02-06T01:54:12Z,2020-02-09T17:48:39Z,"Part of #28926
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
507118771,28994,CLN: tests/extension/json/test_json.py typefix,lukasbk,closed,2019-10-15T09:24:51Z,2020-02-09T18:08:30Z,"The error is due to the fact that the builtins.staticmethod type defined in extension.base.base.BaseExtensionTests does not match the function type in extension.json.test_json.BaseJSON. I don't see a possibility for a proper type hint fix without changing the structure of the code. One way would be to replace the call of staticmethod by function definitions that call the corresponding methods.
Here I propose an easier suppression of the error using type: Any.
What do you think?

- [ ] xref #28926
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
562208525,31825,Backport PR #31788 on branch 1.0.x (BUG: Too aggressive typing in NDFrame.align),meeseeksmachine,closed,2020-02-09T17:13:34Z,2020-02-09T18:23:44Z,Backport PR #31788: BUG: Too aggressive typing in NDFrame.align
562209070,31826,Backport PR #31820 on branch 1.0.x (correct redirections in doc/redirect.csv for rolling),meeseeksmachine,closed,2020-02-09T17:17:34Z,2020-02-09T18:43:52Z,Backport PR #31820: correct redirections in doc/redirect.csv for rolling
562205889,31824,Test messages test period,raisadz,closed,2020-02-09T16:53:58Z,2020-02-09T18:53:08Z,"- [ ] xref #30999
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
556817380,31415,Bug in AbstractHolidayCalendar.holidays,dhirschfeld,closed,2020-01-29T12:01:14Z,2020-02-09T20:54:38Z,"...when there are no rules.

```python
from pandas.tseries.holiday import AbstractHolidayCalendar

class ExampleCalendar(AbstractHolidayCalendar):
    pass

cal = ExampleCalendar()
```
```python
In [58]: cal.holidays(pd.Timestamp('01-Jan-2020'), pd.Timestamp('01-Jan-2021'))
Traceback (most recent call last):

  File ""<ipython-input-58-022244d4e794>"", line 1, in <module>
    cal.holidays(pd.Timestamp('01-Jan-2020'), pd.Timestamp('01-Jan-2021'))

  File ""C:\Users\dhirschf\envs\dev\lib\site-packages\pandas\tseries\holiday.py"", line 422, in holidays
    self._cache = (start, end, holidays.sort_index())

AttributeError: 'NoneType' object has no attribute 'sort_index'

In [59]: pd.__version__
Out[59]: '0.25.3'
```"
561409491,31768,CLN: trim unreachable indexing code,jbrockmendel,closed,2020-02-07T04:11:11Z,2020-02-09T20:58:48Z,make _set_value not return anything (the DataFrame docstring in particular is misleading)
561161417,31750,Decoding issue when reading in py3 a datetime64 hdf data that was created in py2,pedroreys,closed,2020-02-06T17:30:08Z,2020-02-09T21:34:20Z,"#### Code Sample, a copy-pastable example if possible

### In Python 2, create the `.hdf` file with a `datetime64[ns]` column

```python
#python 2
>>> d = {""data"": pd.Timestamp(""2020-02-06T18:00"")}
>>> df = pd.DataFrame(d, index=[1])
>>> df
                 data
1 2020-02-06 18:00:00
>>> df.dtypes
data    datetime64[ns]
dtype: object
>>> df.to_hdf('py2_data.hdf', 'py2_data')
```
### Reading the hdf in python 2 works just fine

```python
# python2
>>> df = pd.read_hdf('py2_data.hdf', 'py2_data')
>>> df
                 data
1 2020-02-06 18:00:00
>>> df.dtypes
data    datetime64[ns]
dtype: object
```

## Reading the hdf in **Python 3** the dtype of the data column is set to `int64` instead of `datatime64`

```python
# python 3
>>> df = pd.read_hdf('py2_data.hdf', 'py2_data')
>>> df
                  data
1  1581012000000000000
>>> df.dtypes
data    int64
dtype: object
```
#### Problem description

The reason why the _dtype_ is being loaded as a `int64` in python 3 is because pandas is not reading correctly the `value_type` attribute from the hdf file. The current implementation does *not* explicitly decodes to ""UTF-8"" the attribute value when its type is `np.bytes_`, which causes it to read the dtype as `b'datetime64` instead of the correct `datetime64` value. That, in turn, causes it to treat the raw value `1581012000000000000` with an inferred dtype of `int64` instead of coercing it to a `M8[ns]` dtype that would give the correct `2020-02-06 18:00:00` datetime value.

This is a similar problem to the one from issue #15725, but that one was when reading the _index_ from the _hdf_ but this one is when reading the data.

The fix for this seems to be really straight forward, you just need to ensure the value of the`value_type` attribute is decoded to _UTF-8_ when reading it from the hdf file. More specifically, [in this line of pytables.py](https://github.com/pandas-dev/pandas/blob/v1.0.1/pandas/io/pytables.py#L2727)

```diff
diff --git a/pytables.py b/pytables.py
index 06c9aa1..9d074d5 100644
--- a/pytables.py
+++ b/pytables.py
@@ -2724,7 +2724,7 @@ class GenericFixed(Fixed):
         if isinstance(node, tables.VLArray):
             ret = node[0][start:stop]
         else:
-            dtype = getattr(attrs, ""value_type"", None)
+            dtype = _ensure_decoded(getattr(attrs, ""value_type"", None))
             shape = getattr(attrs, ""shape"", None)
```

I can make the change and open a PR, as it seems to be really straight forward, just let me know what you prefer.

Note: this is an issue only when the "".hdf"" file is created in python 2 using the _fixed_ format. If it is created using the _table_ format it works as expected.

Thanks!

#### Expected Output
The expected output is that in python 3 it will parse the dtype of the columns correctly, so in this case it should be:

```python
>>> df = pd.read_hdf('py2_data.hdf', 'py2_data')
>>> df
                 data
1 2020-02-06 18:00:00
>>> df.dtypes
data    datetime64[ns]
dtype: object
```

#### Output of ``pd.show_versions()``

<details>
```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.10.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```
</details>
"
562239904,31836,Fixed mypy errors in pandas/tests/extension/json/test_json.py,SaturnFromTitan,closed,2020-02-09T21:11:26Z,2020-02-09T21:54:01Z,"Part of #28926
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
331237701,21425,BUG: DataFrame.pivot fails on multiple columns to set as index,jorisvandenbossche,closed,2018-06-11T15:26:44Z,2020-02-09T21:57:29Z,"I am not sure anymore if `DataFrame.pivot` actually supports multiple columns to set as `index`/`columns` (it should), but in any case this error is very confusing:

```
In [1]: df = pd.DataFrame({'lev1': [1, 1, 1, 1,2, 2, 2,2], 'lev2': [1, 1, 2, 2, 1, 1, 2, 2], 'lev3': [1, 2, 1, 2, 1, 2, 1, 2], 'values': range(8)})

In [2]: df
Out[2]: 
   lev1  lev2  lev3  values
0     1     1     1       0
1     1     1     2       1
2     1     2     1       2
3     1     2     2       3
4     2     1     1       4
5     2     1     2       5
6     2     2     1       6
7     2     2     2       7

In [3]: df.pivot(index=['lev1', 'lev2'], columns='lev3', values='values')
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-2fef29f9fd39> in <module>()
----> 1 df.pivot(index=['lev1', 'lev2'], columns='lev3', values='values')

~/scipy/pandas/pandas/core/frame.py in pivot(self, index, columns, values)
   5191         """"""
   5192         from pandas.core.reshape.reshape import pivot
-> 5193         return pivot(self, index=index, columns=columns, values=values)
   5194 
   5195     _shared_docs['pivot_table'] = """"""

~/scipy/pandas/pandas/core/reshape/reshape.py in pivot(self, index, columns, values)
    406         else:
    407             indexed = self._constructor_sliced(self[values].values,
--> 408                                                index=index)
    409     return indexed.unstack(columns)
    410 

~/scipy/pandas/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath)
    260                             'Length of passed values is {val}, '
    261                             'index implies {ind}'
--> 262                             .format(val=len(data), ind=len(index)))
    263                 except TypeError:
    264                     pass

ValueError: Length of passed values is 8, index implies 2

```"
548479771,30928,ENH: Allow multi values for index and columns in df.pivot,charlesdong1991,closed,2020-01-11T21:01:38Z,2020-02-09T21:57:34Z,"- [x] closes #21425
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
554306806,31253,Update algorithms.py,jrm5100,closed,2020-01-23T17:43:04Z,2020-02-09T22:18:34Z,"Pass the dtype to `_from_sequence` in `_reconstruct_data` in order to support ExtensionArrays with ExtensionDtypes that have parameters.

- [ ] closes #31252 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
562212205,31829,DOC: Use consistent casing in headers,dsaxton,closed,2020-02-09T17:41:36Z,2020-02-09T22:28:15Z,"This is super nitty, but in the left side bar of the user guide the nullable Boolean data type section is the only one that uses all caps (other than Frequently Asked Questions, which possibly makes sense), so this might look a bit better.

https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html#user-guide"
562221855,31831,Test messages test integer,raisadz,closed,2020-02-09T18:55:32Z,2020-02-09T22:28:31Z,"- [ ] xref #30999
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
556826194,31416,Fix bug in calculation of holidays,dhirschfeld,closed,2020-01-29T12:19:32Z,2020-02-09T23:25:28Z,"Closes #31415

- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
562227080,31834,REF: move loc-only validate_read_indexer to Loc,jbrockmendel,closed,2020-02-09T19:36:02Z,2020-02-09T23:51:52Z,straight cut/paste
562223108,31832,CLN: disallow kind=None in _convert_slice_indexer,jbrockmendel,closed,2020-02-09T19:05:30Z,2020-02-09T23:55:35Z,"The one place where _convert_slice_indexer is currently called without kind is within _setitem_with_indexer, which I've determined should be iloc-only so it is a no-op.  With that usage removed, we can be stricter about what gets passed."
562221469,31830,CLN: Use self.loc for Series __getitem__ with IntervalIndex,jbrockmendel,closed,2020-02-09T18:52:18Z,2020-02-09T23:58:56Z,"Clarify that by the time we get to the affected code, the relevant task is determing whether to use loc or iloc."
562069995,31810,"BUG/API: df.loc[a, b] = c allows positional slice for a",jbrockmendel,closed,2020-02-08T19:59:33Z,2020-02-10T01:54:02Z,"```
dti = pd.date_range(""2016-01-01"", periods=5)
df = pd.DataFrame(np.random.randn(5, 2), index=dti, columns=[""A"", ""B""])

>>> df.loc[4:6, ""A""]
TypeError: cannot do slice indexing on DatetimeIndex with these indexers [4] of type int

>>> df.loc[4:6, ""A""] = 0
# works as if we did df.iloc[4:6, 0] = 0
```

In this case there are plenty of alternatives that amount to doing `df[""A""].iloc[4:6] = 0`, but this isnt robust to mixed-dtype frames, so I'm not sure if we have a supported way of doing this correctly."
457293709,26917,Tabular numbers should be used on tables,dansondergaard,closed,2019-06-18T06:55:39Z,2020-02-10T07:45:35Z,"**Describe the bug**
Tables rendered in Jupyter Lab (likely also Jupyter Notebook), e.g. from pandas, do not use tabular numbers. This causes sorted columns to look unsorted and makes numbers harder to read and compare.

**To Reproduce**

Create a table like this:

<img width=""299"" alt=""Screenshot 2019-06-14 at 08 38 42"" src=""https://user-images.githubusercontent.com/204911/59488447-dc541700-8e7f-11e9-8aff-50274888859c.png"">

**Expected behavior**

With tabular numbers applied through CSS:

<img width=""302"" alt=""Screenshot 2019-06-14 at 08 41 07"" src=""https://user-images.githubusercontent.com/204911/59488557-3228bf00-8e80-11e9-917d-62e6024233bf.png"">

CSS used to apply tabular numbers:

```
table { font-variant-numeric: tabular-nums; }
```

**Screenshots**

Screenshots attached above.

**Desktop (please complete the following information):**

- OS: Any
- Browser: Any
- JupyterLab: Tested on 0.35.6, but probably applies to any version of Jupyter Lab

"
561213510,31754,`raw=True` no longer applies to groupby().rolling() in 1.0.0,icexelloss,closed,2020-02-06T19:13:09Z,2020-02-10T12:50:32Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame({'id': [1, 1, 1], 'value': [1, 2, 3]})

def foo(x):
    print(type(x))
    return 0.0
```
When setting `raw=True`
```
>>> df.groupby(""id"").value.rolling(1).apply(foo, raw=True, engine='numba')
<class 'pandas.core.series.Series'>
<class 'pandas.core.series.Series'>
<class 'pandas.core.series.Series'>
id
1   0    0.0
    1    0.0
    2    0.0
Name: value, dtype: float64

>>> df.groupby(""id"").value.rolling(1).apply(foo, raw=True, engine='cython')
<class 'pandas.core.series.Series'>
<class 'pandas.core.series.Series'>
<class 'pandas.core.series.Series'>
id
1   0    0.0
    1    0.0
    2    0.0
Name: value, dtype: float64

>>> df.groupby(""id"").value.rolling(1).apply(foo, raw=True)
<class 'pandas.core.series.Series'>
<class 'pandas.core.series.Series'>
<class 'pandas.core.series.Series'>
id
1   0    0.0
    1    0.0
    2    0.0
Name: value, dtype: float64
```

#### Problem description

This changes the behavior of `raw=True`, it seems it no long allows user to pass numpy array to a rolling udf.

"
561238439,31756,BUG Decode to UTF-8 the dtype string read from a hdf file,pedroreys,closed,2020-02-06T20:02:14Z,2020-02-10T13:49:36Z,"Fixes GH31750

The dtype value wasn't being decoded to `UTF-8` when reading a DataFrame
from a hdf file. This was a problem when reading a hdf that was
created from python 2 with a fixed format as the dtype was being read as `b'datetime'`
instead of `datetime`, which caused `HDFStore` to read the data as
`int64` instead of coercing it to the correct `datetime64` dtype.

- [x] closes #31750
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
561903650,31797,CLN: assorted indexing-related cleanups,jbrockmendel,closed,2020-02-07T22:26:40Z,2020-02-10T16:20:43Z,"- _setitem_with_indexer isnt consistent about whether or not it returns anything, make it always-None
- avoid using private loc method from DataFrame
"
559385964,31635,"REF: implement ExtensionIndex._concat_same_dtype, use for IntervalIndex",jbrockmendel,closed,2020-02-03T23:00:25Z,2020-02-10T16:40:26Z,
561824998,31789,MemoryError when using series.rolling().corr(other) with >1.0,mdering,closed,2020-02-07T19:24:00Z,2020-02-10T17:51:32Z,"#### Code Sample, a copy-pastable example if possible

```python
srs1 = pd.Series(np.random.rand(11521),pd.date_range('2019-08-15', '2019-08-23',freq='1T'))
srs2 = pd.Series(np.random.rand(11521),pd.date_range('2019-08-15', '2019-08-23',freq='1T'))
srs1.rolling(pd.to_timedelta(""12H"")).corr(srs2)

```
#### Problem description


Running the code above results in the following error `Unable to allocate 314. TiB for an array with shape (43200000000000,) and data type int64` on pandas 1.0.1. Confirmed that this used to work on pandas 0.25.3.

#### Expected Output
The correct calculations

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.2.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : 0.29.14
pytest           : 5.3.5
hypothesis       : 5.4.1
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : 0.13.0
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.48.0

</details>"
562240706,31837,REF: make _setitem_with_indexer iloc-only,jbrockmendel,closed,2020-02-09T21:16:26Z,2020-02-10T19:45:35Z,"some overlap with #31797.  

_setitem_with_indexer is used for positional indexers, so it is misleading for it to be in loc anyway.

_setitem_with_indexer is an absolute beast, will be easier to sort out when we dont have to worry about what class we're in.

The diff looks misleading.  All this does is
- move _setitem_with_indexer, _setitem_with_indexer_missing, _align_series, _align_frame from LocationIndexer to _iLocIndexer
- make `LocationIndexer.__setitem__` call `_has_valid_setitem_indexer` before calling _setitem_with_indexer instead of inside of it, so we get the right class's _has_valid_setitem_indexer
- dispatch to iloc if we are not already iloc
- update usages in core.frame"
561793364,31786,REF: remove iloc case from _convert_slice_indexer,jbrockmendel,closed,2020-02-07T18:12:27Z,2020-02-10T20:10:01Z,"_convert_slice_indexer is turning out to be one of the stickier methods to figure out, xref #31658.

The case with kind=None is only called from one place in core.indexing, and that is not reached in the tests.  I'd like to either a) figure out a non-None kind to pass to it, or b) determine that it can never be reached so can be removed.  Any thoughts on this are very welcome."
555348720,31333,DEPR: indexing Series with single-entry list,jbrockmendel,closed,2020-01-27T04:20:55Z,2020-02-10T20:55:40Z,"- [x] closes #31299 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Could use a better warning message, suggestions welcome."
558872629,31598,REF: simplify PeriodIndex.get_loc,jbrockmendel,closed,2020-02-03T06:29:49Z,2020-02-10T23:32:30Z,"This makes PeriodEngine follow patterns in DatetimeEngine, so soon we can have it subclass DatetimeEngine instead of Int64Engine, which will lead to more simplifications.

This also puts us within striking distance of sharing get_loc code"
562250600,31838,Docstring fixes for PR06 errors,jchen2186,closed,2020-02-09T22:30:02Z,2020-02-11T00:24:47Z,"This contains some small changes that resolve some of the PR06 errors from running `./scripts/validate_docstrings.py --errors=PR06`
- [x] xref #28724
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
562225259,31833,DOC Update documentation DataFrame.nsmallest,Pearcekieser,closed,2020-02-09T19:21:26Z,2020-02-11T04:48:43Z,"Modified the example, to better demonstrate the use of the nsmallest function.
"
562968848,31868,CLN: f-string formatting,monicaw218,closed,2020-02-11T04:16:20Z,2020-02-11T05:15:59Z,"Ref to [#29547](https://github.com/pandas-dev/pandas/issues/29547)

- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Replacing strings interpolated with .format() with strings interpolated with f-strings"
562917798,31862,Pandas qcut - Allow right argument to make left bins inclusive/exclusive,mdinesh9,closed,2020-02-11T00:55:57Z,2020-02-11T08:36:26Z,"
```python
def qcut(
    x,
    q,
    right=True,
    labels=None,
    retbins: bool = False,
    precision: int = 3,
    duplicates: str = ""raise"",
):



fac, bins = _bins_to_cuts(
    x,
    bins,
    right=True,
    labels=labels,
    precision=precision,
    include_lowest=True,
    dtype=dtype,
    duplicates=duplicates,
)

```
#### Similar to R HMISC cut2 Equiwidth Binning

[Having right=False makes left bins inclusive and right bins exclusive whereas right=True makes left bins exclusive and right bins inclusive. When right=False, it replicates what cut2 function from HMISC R package is doing.]


#### Expected Output
<details>
When right=False, index of the dataframe looks like below:

```
[1.0, 3.0)
[4.0, 7.0)
```

When right=True, index of the dataframe looks like below:

```
(1.0, 3.0]
(1.0, 3.0]
```
</details>"
562886986,31860,DOC: Specify use of google cloud storage for CSVs,iamshwin,closed,2020-02-10T23:18:12Z,2020-02-11T09:59:01Z,"- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
563186831,31873,Left merging a 6628 row DF with a 127286 row DF yields a 48939191 row DF,tsoernes,closed,2020-02-11T12:52:41Z,2020-02-11T12:55:33Z,"#### Code Sample, a copy-pastable example if possible

```python
[ins] In [10]: df_e2.info()                                                                                                                                                          
<class 'pandas.core.frame.DataFrame'>
Int64Index: 127286 entries, 0 to 6327
Data columns (total 24 columns):
 #   Column          Non-Null Count   Dtype         
---  ------          --------------   -----         
 0   revenue         18258 non-null   string        
 1   phone           49570 non-null   string        
 2   status          127286 non-null  string        
 3   notes           66101 non-null   string        
 4   facebook_url    43003 non-null   string        
 5   linkedin_url    52043 non-null   string        
 6   email           50212 non-null   string        
 7   description     119431 non-null  string        
 8   crunchbase_url  76787 non-null   string        
 9   name            127286 non-null  string        
 10  url             113731 non-null  string        
 11  twitter_handle  56307 non-null   string        
 12  hq_location     72719 non-null   string        
 13  latitude        72719 non-null   float64       
 14  longitude       72719 non-null   float64       
 15  country_code    72697 non-null   string        
 16  url_redir       47957 non-null   string        
 17  founding_year   127286 non-null  string        
 18  created_at      127286 non-null  datetime64[ns]
 19  updated_at      127286 non-null  datetime64[ns]
 20  type            127286 non-null  string        
 21  raw_tags        127286 non-null  object        
 22  database_id     127286 non-null  Int64         
 23  dupe_group_ix   127286 non-null  Int64         
dtypes: Int64(2), datetime64[ns](2), float64(2), object(1), string(17)
memory usage: 24.5+ MB

[ins] In [11]: df.info()                                                                                                                                                             
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 6629 entries, 0 to 6628
Data columns (total 37 columns):
 #   Column                         Non-Null Count  Dtype         
---  ------                         --------------  -----         
 0   id                             6629 non-null   Int64         
 1   name                           6629 non-null   string        
 2   url                            6370 non-null   string        
 3   description                    6593 non-null   string        
 4   created_at                     6629 non-null   datetime64[ns]
 5   updated_at                     6629 non-null   datetime64[ns]
 6   email                          6481 non-null   string        
 7   slug                           6021 non-null   string        
 8   raw_tags                       6629 non-null   object        
 9   short_description              157 non-null    string        
 10  twitter_handle                 5069 non-null   string        
 11  hq_location                    6428 non-null   string        
 12  latitude                       6416 non-null   float64       
 13  longitude                      6416 non-null   float64       
 14  company_registration_number    61 non-null     string        
 15  company_registrar_url          163 non-null    string        
 16  tax_registration_number        52 non-null     string        
 17  tax_registrar_url              51 non-null     string        
 18  financial_registration_number  56 non-null     string        
 19  financial_registrar_url        56 non-null     string        
 20  instagram_handle               80 non-null     string        
 21  crunchbase_url                 5660 non-null   string        
 22  facebook_url                   4319 non-null   string        
 23  linkedin_url                   5097 non-null   string        
 24  bloomberg_url                  81 non-null     string        
 25  founding_year                  5842 non-null   Int64         
 26  revenue                        2020 non-null   string        
 27  type                           6472 non-null   string        
 28  blog_url                       113 non-null    string        
 29  rss_url                        51 non-null     string        
 30  country_code                   6397 non-null   string        
 31  domain_name                    203 non-null    string        
 32  phone                          4009 non-null   string        
 33  database_id                    6629 non-null   Int64         
 34  status                         6629 non-null   string        
 35  notes                          6458 non-null   string        
 36  complete                       214 non-null    boolean       
dtypes: Int64(3), boolean(1), datetime64[ns](2), float64(2), object(1), string(28)
memory usage: 1.9+ MB

[nav] In [13]: df2 = df.merge(df_e2, on='crunchbase_url', how='left', copy=False)                                                                                                    

[ins] In [14]: df2.info()                                                                                                                                                            
<class 'pandas.core.frame.DataFrame'>
Int64Index: 48939191 entries, 0 to 48939190
Data columns (total 60 columns):
 #   Column                         Dtype         
---  ------                         -----         
 0   id                             Int64         
 1   name_x                         string        
 2   url_x                          string        
 3   description_x                  string        
 4   created_at_x                   datetime64[ns]
 5   updated_at_x                   datetime64[ns]
 6   email_x                        string        
 7   slug                           string        
 8   raw_tags_x                     object        
 9   short_description              string        
 10  twitter_handle_x               string        
 11  hq_location_x                  string        
 12  latitude_x                     float64       
 13  longitude_x                    float64       
 14  company_registration_number    string        
 15  company_registrar_url          string        
 16  tax_registration_number        string        
 17  tax_registrar_url              string        
 18  financial_registration_number  string        
 19  financial_registrar_url        string        
 20  instagram_handle               string        
 21  crunchbase_url                 string        
 22  facebook_url_x                 string        
 23  linkedin_url_x                 string        
 24  bloomberg_url                  string        
 25  founding_year_x                Int64         
 26  revenue_x                      string        
 27  type_x                         string        
 28  blog_url                       string        
 29  rss_url                        string        
 30  country_code_x                 string        
 31  domain_name                    string        
 32  phone_x                        string        
 33  database_id_x                  Int64         
 34  status_x                       string        
 35  notes_x                        string        
 36  complete                       boolean       
 37  revenue_y                      string        
 38  phone_y                        string        
 39  status_y                       string        
 40  notes_y                        string        
 41  facebook_url_y                 string        
 42  linkedin_url_y                 string        
 43  email_y                        string        
 44  description_y                  string        
 45  name_y                         string        
 46  url_y                          string        
 47  twitter_handle_y               string        
 48  hq_location_y                  string        
 49  latitude_y                     float64       
 50  longitude_y                    float64       
 51  country_code_y                 string        
 52  url_redir                      string        
 53  founding_year_y                string        
 54  created_at_y                   datetime64[ns]
 55  updated_at_y                   datetime64[ns]
 56  type_y                         string        
 57  raw_tags_y                     object        
 58  database_id_y                  Int64         
 59  dupe_group_ix                  Int64         
dtypes: Int64(5), boolean(1), datetime64[ns](4), float64(4), object(2), string(44)
memory usage: 22.2+ GB

[ins] In [15]: pd.__version__                                                                                                                                                        
Out[15]: '1.0.1'

[ins] In [16]: len(df)                                                                                                                                                               
Out[16]: 6629

[ins] In [17]: len(df_e2)                                                                                                                                                            
Out[17]: 127286

[ins] In [18]: len(df2)                                                                                                                                                              
Out[18]: 48939191


```

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-1101-aws
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.16.4
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : 0.29.12
pytest           : 5.0.1
hypothesis       : None
sphinx           : 2.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.3.4
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.6.1
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.1.0
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : 0.12.1
pytables         : None
pytest           : 5.0.1
pyxlsb           : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : 1.3.5
tables           : 3.5.2
tabulate         : 0.8.5
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8
numba            : 0.44.1

</details>
"
562163406,31820,correct redirections in doc/redirect.csv for rolling,rushabh-v,closed,2020-02-09T11:40:01Z,2020-02-11T12:59:56Z,"- [x] closes #31762 
"
563254072,31878,CLN-29547 replace old string formatting,3vts,closed,2020-02-11T14:43:22Z,2020-02-11T15:05:19Z,"I splitted PR #31844 in batches, this is the first one
For this PR I ran the command `grep -l -R -e '%s' -e '%d' -e '\.format(' --include=*.{py,pyx} pandas/` and checked all the files that were returned for `.format(` and changed the old string format for the corresponding `fstrings` to attempt a full clean of, [#29547](https://github.com/pandas-dev/pandas/issues/29547). I may have missed something so is a good idea to double check just in case

- [ x  ] tests added / passed
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
562925037,31865,CLN: tests.generic,jbrockmendel,closed,2020-02-11T01:22:55Z,2020-02-11T15:43:47Z,"Many of the tests here really only test either Series or DataFrame, not both."
562734019,31854,REF: make Series/DataFrame _slice always positional,jbrockmendel,closed,2020-02-10T18:08:21Z,2020-02-11T15:44:34Z,ATM we have a loc/iloc kwarg that NDFrame._slice (which is in effect DataFrame._slice) ignores and always slices positionally.  This makes Series._slice always slice positionally too.
562881868,31858,API: df.loc(axis=0)[key] works differently from df.loc[key],jbrockmendel,closed,2020-02-10T23:04:31Z,2020-02-11T15:45:15Z,"I would expect these to behave identically, but, from tests.indexing.multiindex.test_slice.TestMultiIndexSlicers.test_loc_axis_arguments:


```
def _mklbl(prefix, n):
    return [""{prefix}{i}"".format(prefix=prefix, i=i) for i in range(n)]

index = MultiIndex.from_product(
            [_mklbl(""A"", 4), _mklbl(""B"", 2), _mklbl(""C"", 4), _mklbl(""D"", 2)]
        )
columns = MultiIndex.from_tuples(
            [(""a"", ""foo""), (""a"", ""bar""), (""b"", ""foo""), (""b"", ""bah"")],
            names=[""lvl0"", ""lvl1""],
        )
df = (
            DataFrame(
                np.arange(len(index) * len(columns), dtype=""int64"").reshape(
                    (len(index), len(columns))
                ),
                index=index,
                columns=columns,
            )
            .sort_index()
            .sort_index(axis=1)
        )

>>> result = df.loc(axis=0)[""A1"":""A3"", :, [""C1"", ""C3""]]   # <-- currently in the test, works

>>> result2 = df.loc[""A1"":""A3"", :, [""C1"", ""C3""]]
pandas.core.indexing.IndexingError: Too many indexers
```"
563372405,31885,columns keyword has no effect in DataFrame.to_excel() in 1.0.1,ehowatt,closed,2020-02-11T17:45:34Z,2020-02-11T21:09:20Z,"The `columns` keyword argument to the `DataFrame.to_excel()` method in version 1.0.1 of pandas does not restrict the columns in the Excel sheet to the specified column list. This is different from the behavior in 0.25.2 and previous versions, and also differs from the `to_csv()` method.

This can be replicated using:
```
import pandas as pd

df = pd.DataFrame([[1,2,3],[4,5,6],[7,8,9]],columns=['a','b','c'])
df.to_excel('columns_output.xlsx',columns=['a','c'])
w.save()
```
(I'm using the `xlsxwriter` engine)

In previous versions of pandas the output from that code would be an Excel file that only contains the index (no header), column a, and column c:
<table>
<tr>
<td></td>
<td>a</td>
<td>c</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>1</td>
<td>4</td>
<td>6</td>
</tr>
<tr>
<td>2</td>
<td>7</td>
<td>9</td>
</tr>
</table>

In the new version, all three columns are outputted, yielding an Excel file with all three columns:
<table>
<tr>
<td></td>
<td>a</td>
<td><em>b</em></td>
<td>c</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td><em>2</em></td>
<td>3</td>
</tr>
<tr>
<td>1</td>
<td>4</td>
<td><em>5</em></td>
<td>6</td>
</tr>
<tr>
<td>2</td>
<td>7</td>
<td><em>8</em></td>
<td>9</td>
</tr>
</table>
(emphasis mine)

Not all columns in the dataframe need to be shared with the end user (internal values, processing/filtering/sorting columns, etc.) and some end users prefer columns in a specific order. Restoring the previous functionality would make that possible without having to change the dataframe itself.

Also, if the `headers` keyword is specified, matching the number of columns listed, like so:
```
df.to_excel('columns_output_headers.xlsx',columns=['a','c'],header=['column A','column C'])
```
A `ValueError` is returned as the column count does not match the header list length:
```
ValueError: Writing 3 cols but got 2 aliases
```

#### Output of ``pd.show_versions()``
1.0.1
<details>
Replicated in a local version, an aws lambda python 3.7 docker image, and in aws.

Local version:
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.2.2
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 2.6.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : 1.2.7
numba            : None

Docker image
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.9.184-linuxkit
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : 1.2.7
numba            : None

If I force the version on pandas (and change nothing else) when building and running on the local Docker image, the proper column set appears.

</details>
"
540217606,30346,color attribute of medianprops is not correctly understand in a boxplot,gVallverdu,closed,2019-12-19T10:20:28Z,2020-02-11T21:27:07Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame({k: np.random.random(100) for k in ""ABC""})

# wrong output
df.boxplot(medianprops=dict(color=""C1"", lw=2))

# right output
plt.boxplot(df.transpose(), medianprops=dict(color=""C1"", lw=2))
```
#### Problem description

The problem comes from the boxplot function in a dataframe. This function is linked to the matplotlib function. I got an issue when I try to change the color of the medians using the `medianprops` argument of the function (which is a matplotlib argument). If I use the same instruction directly with the boxplot function in matplotlib I got the right behavior. But, I can change the `linewidth` or `linestyle` for example.

Here I put a screenshot of what I obtain if I use the `boxplot` method from pandas or directly from matplotlib. You can see that using the matplotlib function, the color of the median line is ""correct"" (the one I asked) while you cannot change the color of the line from the pandas function.

![Capture d’écran 2019-12-19 à 11 04 32](https://user-images.githubusercontent.com/5417018/71164774-fe27bf80-224f-11ea-87ff-5714faece96d.png)

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.0.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : fr_FR.UTF-8
LOCALE           : fr_FR.UTF-8

pandas           : 0.25.1
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.2.1
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.1
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.9
tables           : 3.5.2
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.1
</details>
"
563417953,31888,"CLN: D300 Use """"""triple double quotes""""""",simonjayhawkins,closed,2020-02-11T19:14:28Z,2020-02-11T21:35:57Z,
563418776,31889,CLN: D201 No blank lines allowed before function docstring,simonjayhawkins,closed,2020-02-11T19:15:53Z,2020-02-11T21:37:31Z,
563419675,31890,CLN: D208 Docstring is over-indented,simonjayhawkins,closed,2020-02-11T19:17:35Z,2020-02-11T21:41:08Z,
210224086,15502,Allow Enums in place of certain parameters values,jimmycallin,closed,2017-02-25T09:38:03Z,2020-02-11T22:28:01Z,"I have a suggestion for how to use Enums to codify less transparent parameter values. Since 3.4, Python comes with built-in support for Enums, which could be useful in certain instances for Pandas.

One example is allowing enums in place of integers for axis values, which frankly to this day I cannot seem to remember which is which. These could for instance be stored in an attribute in each function. This could potentially work like this:

    import pandas as pd

    df = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4]})
    df.sum(axis=df.sum.axis.COLUMNWISE)

    a    6
    b    9
    dtype: int64


Another example is having support for enums in place of string values. This would be useful as a way to codify all supported string values outside of documentation, and also enabling autocomplete support in REPLs. Another example:

    import pandas as pd
    from pandas import axis

    df = pd.DataFrame({'a': [1,2,3,5], 'b': [2,3,4,None]})

    df.sort_values('b', na_position=df.sort_values.na_position.FIRST)

       a    b
    3  5  NaN
    0  1  2.0
    1  2  3.0
    2  3  4.0
"
563421839,31892,CLN: D204 1 blank line required after class docstring,simonjayhawkins,closed,2020-02-11T19:21:23Z,2020-02-11T22:37:33Z,
563420847,31891,CLN: D209 Multi-line docstring closing quotes should be on a separate line,simonjayhawkins,closed,2020-02-11T19:19:42Z,2020-02-11T23:04:14Z,
523981370,29670,BUG: extra leading space in to_string when index=False,charlesdong1991,closed,2019-11-17T12:51:05Z,2020-02-12T00:34:23Z,"- [x] closes #24980 , xref #28538
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I still see people posting issue for the change of behaviour in `to_string(index=False)` on pandas such as #28538 , so I think it might be worth it to reopen my previous stalled PR #25000 and see if could get it solved this time.

All the changes have been made based on the reviews in PR #25000 and for detailed summary and explanation, please check https://github.com/pandas-dev/pandas/pull/25000#issuecomment-458714951
Feel free to take a look and any review and comments are very welcomed!"
497648544,28595,BUG: Timedelta not formatted correctly in to_json,cbertinato,closed,2019-09-24T12:12:30Z,2020-02-12T00:41:12Z,"- [x] closes #28256
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
550021293,31031,TST: Split test_offsets.py - Test_offsets moved to test_date_offsets,Raalsky,closed,2020-01-15T07:50:07Z,2020-02-12T00:49:01Z,"In context of [https://github.com/pandas-dev/pandas/pull/30194](30194)
First part with move test_offsets to test_date_offsets
- [ ] closes #27085
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
548437810,30920,BUG30787 fixed unexpected behaviour by removing nullable values,alagappan97,closed,2020-01-11T14:43:31Z,2020-02-12T00:51:03Z,"- [done ] closes #30787 
Added a simple check not to allow nan values for calculation. Numpy seems to throw error when nan values are passed to searchsorted function.
"
563560140,31901,TST: parametrize eval tests,jbrockmendel,closed,2020-02-11T22:49:55Z,2020-02-12T01:15:54Z,
548406229,30912,DOC: Added documentation for ImportError's,gonemad97,closed,2020-01-11T09:26:48Z,2020-02-12T01:16:37Z,"…stall.rst

- [x] closes #29399 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
563671611,31904,"pandas read_excel error:TypeError: 'values' is not ordered, please explicitly specify the categories order by passing in a categories argument.",rosefun,closed,2020-02-12T01:18:54Z,2020-02-12T01:32:11Z,"#### My code 

```python
import pandas as pd 
f = 'F:/example.xlsx'
ex_file = pd.ExcelFile(f)
sheet_names = ex_file.sheet_names
for name in sheet_names:
    print(name)
    dc = pd.read_excel(ex_file, sheet_name=name)
```
#### Problem details

```
Traceback (most recent call last):
  File ""D:\pandas\core\categorical.py"", line 330, in __init__
    codes, categories = factorize(values, sort=True)
  File ""D:\pandas\core\algorithms.py"", line 479, in factorize
    assume_unique=True)
  File ""D:\pandas\core\sorting.py"", line 443, in safe_sort
    ordered = sort_mixed(values)
  File ""D:\pandas\core\sorting.py"", line 436, in sort_mixed
    nums = np.sort(values[~str_pos])
  File ""D:\numpy\core\fromnumeric.py"", line 847, in sort
    a.sort(axis=axis, kind=kind, order=order)
TypeError: '<' not supported between instances of 'datetime.datetime' and 'int'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""F:\testReadexcel.py"", line 8, in <module>
    dc = pd.read_excel(ex_file, sheet_name=name)
  File ""D:\pandas\util\_decorators.py"", line 118, in wrapper
    return func(*args, **kwargs)
  File ""D:\pandas\io\excel.py"", line 238, in read_excel
    false_values=false_values, squeeze=squeeze, **kwds)
  File ""D:\pandas\io\excel.py"", line 543, in _parse_excel
    output[asheetname] = parser.read()
  File ""D:\pandas\io\parsers.py"", line 1069, in read
    ret = self._engine.read(nrows)
  File ""D:\pandas\io\parsers.py"", line 2273, in read
    index, columns = self._make_index(data, alldata, columns, indexnamerow)
  File ""D:\pandas\io\parsers.py"", line 1412, in _make_index
    index = self._agg_index(index)
  File ""D:\pandas\io\parsers.py"", line 1512, in _agg_index
    index = _ensure_index_from_sequences(arrays, names)
  File ""D:\pandas\core\indexes\base.py"", line 4152, in _ensure_index_from_sequences
    return MultiIndex.from_arrays(sequences, names=names)
  File ""D:\pandas\core\indexes\multi.py"", line 1150, in from_arrays
    labels, levels = _factorize_from_iterables(arrays)
  File ""D:\pandas\core\categorical.py"", line 2352, in _factorize_from_iterables
    return map(list, lzip(*[_factorize_from_iterable(it) for it in iterables]))
  File ""D:\pandas\core\categorical.py"", line 2352, in <listcomp>
    return map(list, lzip(*[_factorize_from_iterable(it) for it in iterables]))
  File ""D:\pandas\core\categorical.py"", line 2324, in _factorize_from_iterable
    cat = Categorical(values, ordered=True)
  File ""D:\pandas\core\categorical.py"", line 336, in __init__
    raise TypeError(""'values' is not ordered, please ""
TypeError: 'values' is not ordered, please explicitly specify the categories order by passing in a categories argument.
```

#### Expected Output
It can print the sheet name, but pandas can't read the first sheet correctly.

#### Output of ``pd.show_versions()``
0.22.0
"
563423852,31894,DOC: Fix divmod return values,naterarmstrong,closed,2020-02-11T19:25:09Z,2020-02-12T01:39:14Z,"- [x] closes #31663
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
561591791,31772,BUG: setitem on string dtype with slices fails,jorisvandenbossche,closed,2020-02-07T11:50:24Z,2020-02-12T12:38:27Z,"
```
In [33]: df = pd.DataFrame({'A': ['a', 'b', 'c']}, dtype='string')   

In [34]: df.loc[0:, 'A'] = ""test""  
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-34-bc65087be9d7> in <module>
----> 1 df.loc[0:, 'A'] = ""test""

~/scipy/pandas/pandas/core/indexing.py in __setitem__(self, key, value)
    629             key = com.apply_if_callable(key, self.obj)
    630         indexer = self._get_setitem_indexer(key)
--> 631         self._setitem_with_indexer(indexer, value)
    632 
    633     def _validate_key(self, key, axis: int):

~/scipy/pandas/pandas/core/indexing.py in _setitem_with_indexer(self, indexer, value)
   1014             # actually do the set
   1015             self.obj._consolidate_inplace()
-> 1016             self.obj._data = self.obj._data.setitem(indexer=indexer, value=value)
   1017             self.obj._maybe_update_cacher(clear=True)
   1018 

~/scipy/pandas/pandas/core/internals/managers.py in setitem(self, **kwargs)
    538 
    539     def setitem(self, **kwargs):
--> 540         return self.apply(""setitem"", **kwargs)
    541 
    542     def putmask(self, **kwargs):

~/scipy/pandas/pandas/core/internals/managers.py in apply(self, f, filter, **kwargs)
    417                 applied = b.apply(f, **kwargs)
    418             else:
--> 419                 applied = getattr(b, f)(**kwargs)
    420             result_blocks = _extend_blocks(applied, result_blocks)
    421 

~/scipy/pandas/pandas/core/internals/blocks.py in setitem(self, indexer, value)
   1801 
   1802         check_setitem_lengths(indexer, value, self.values)
-> 1803         self.values[indexer] = value
   1804         return self
   1805 

~/scipy/pandas/pandas/core/arrays/string_.py in __setitem__(self, key, value)
    260                 raise ValueError(""Must provide strings."")
    261 
--> 262         super().__setitem__(key, value)
    263 
    264     def fillna(self, value=None, method=None, limit=None):

~/scipy/pandas/pandas/core/arrays/numpy_.py in __setitem__(self, key, value)
    273             value = np.asarray(value, dtype=self._ndarray.dtype)
    274 
--> 275         self._ndarray[key] = value
    276 
    277     def __len__(self) -> int:

IndexError: arrays used as indices must be of integer (or boolean) type
```"
561597736,31773,BUG: fix StringArray/PandasArray setitem with slice,jorisvandenbossche,closed,2020-02-07T12:03:45Z,2020-02-12T13:14:26Z,Closes #31772
563974982,31921,Backport PR #31794 on branch 1.0.x (BUG: Avoid casting Int to object in Categorical.from_codes),meeseeksmachine,closed,2020-02-12T12:42:34Z,2020-02-12T13:14:08Z,Backport PR #31794: BUG: Avoid casting Int to object in Categorical.from_codes
563991863,31923,Backport PR #31773 on branch 1.0.x (BUG: fix StringArray/PandasArray setitem with slice),meeseeksmachine,closed,2020-02-12T13:14:06Z,2020-02-12T13:49:09Z,Backport PR #31773: BUG: fix StringArray/PandasArray setitem with slice
561696081,31779,Categorical.from_codes fails for the (new nullable) Int64 dtype,mila,closed,2020-02-07T15:10:35Z,2020-02-12T14:12:51Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import pandas as pd
>>> codes = pd.Series([1, 0], dtype=""Int64"")
>>> pd.Categorical.from_codes(codes, categories=[""foo"", ""bar""])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../lib/python3.7/site-packages/pandas/core/arrays/categorical.py"", line 649, in from_codes
    raise ValueError(""codes need to be array-like integers"")
ValueError: codes need to be array-like integers
```
#### Problem description

`Categories.from_codes` works with Series with the Numpy `""int64""` dtype. 

```python
>>> codes = pd.Series([1, 0])
>>> pd.Categorical.from_codes(codes, categories=[""foo"", ""bar""])
[bar, foo]
Categories (2, object): [foo, bar]
```

I would expect that it will work with the new Pandas `""Int64""` dtype.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
563429315,31896,BUG: read_parquet unexpected output with pyarrow 0.16.0 and nullable usigned int dtype,cat-Yu,closed,2020-02-11T19:35:36Z,2020-02-12T15:04:32Z,"```python
data = [np.nan, 1, 2, 3, 4]
df = pd.DataFrame()
df['col'] = pd.Series(data, dtype = ""UInt32"")
df.to_parquet('example.parquet')
res = pd.read_parquet('example.parquet')
print(res)
```
#### Problem description

The above cope prints:
```
    col
0  <NA>
1     0
2     1
3     0
4     2
```

This happened after I upgraded to `pyarrow 0.16.0`, and `UInt32` is the only dtype that I had trouble with.
The problem should be with `read_parquet`, I tried reading parquet with another package, and got the expected output.

#### Expected Output
```
    col
0  <NA>
1     1
2     2
3     3
4     4
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : 5.3.2
hypothesis       : None
sphinx           : 2.3.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.3.2
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0
</details>
"
560750052,31731,"BUG: DataFrame.convert_dtypes fails on column that is already ""string"" dtype",ghost,closed,2020-02-06T03:06:19Z,2020-02-12T15:05:29Z,"#### Code Sample, a copy-pastable example if possible

```python
In [1]: df = pd.DataFrame({'A': ['a', 'b', 'c'], 'B': ['d', 'e', 'f']})
In [2]: df.dtypes
Out[2]:
A    object
B    object
dtype: object

In [3]: df1 = df.convert_dtypes()

In [4]: df1.dtypes
Out[4]:
A    string                 # <------  expected
B    string                 # <------  expected
dtype: object

In [5]: df2 = df1.convert_dtypes()

In [6]: df2.dtypes
Out[6]:
A    object                 # <------ NOT expected
B    object                 # <------ NOT expected
dtype: object

In [7]: df3['A'] = 'test'

In [8]: df3
Out[8]:
      A  B
0  test  d                 # <------  expected
1  test  e                 # <------  expected
2  test  f                 # <------  expected

In [9]: df3.dtypes
Out[9]:
A    object                 # <------ NOT expected (but understandable)
B    string                 # <------  expected
dtype: object

In [10]: df4 = df3.convert_dtypes()

In [11]: df4
Out[11]:
      A     B
0  test  b'd'                 # <------ NOT expected (Why are they bytes now???)
1  test  b'e'                 # <------ NOT expected
2  test  b'f'                 # <------ NOT expected

In [12]: df4.dtypes
Out[12]:
A    string                 # <------  expected
B    object                 # <------ NOT expected
dtype: object
```
#### Problem description

The documentation for [DataFrame.convert_dtypes()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.convert_dtypes.html) *claims* that it will 'Convert columns to best possible dtypes using dtypes supporting pd.NA'. However, this does not appear to be the case. As you can see in `Out[6]` above, if the dtypes are already optimal, they will be converted back to objects.

Even worse, if some of the dtypes are optimal, but other dtypes are objects, they will be switched -- the optimal dtypes will become objects, and the objects will become optimal.

Some mysterious additional details:

- Assigning a string to an 'optimal' column changes the dtype to object. (`Out[9]`)
- If one column is a string, and the other an object `df.convert_dtypes()` will not only cause the string column to become object and the object column to become string, it will also change the strings in the newly created object column into bytes (`Out[11]`)

#### Expected Output

Anything which can be converted to one of the new nullable datatypes would be converted to one of the new nullable datatypes. Anything which is already a nullable datatype would remain as it is.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : 0.29.14
pytest           : 5.3.4
hypothesis       : 4.54.2
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.4
pyxlsb           : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.48.0

</details>
"
563232192,31877,BUG: fix infer_dtype for StringDtype,jorisvandenbossche,closed,2020-02-11T14:10:21Z,2020-02-12T15:14:15Z,Closes #31731
561885127,31794,BUG: Avoid casting Int to object in Categorical.from_codes,dsaxton,closed,2020-02-07T21:37:33Z,2020-02-12T15:34:46Z,"- [x] closes #31779
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
563999881,31924,Pandas negative timedelta is 1 day off,tuxcell,closed,2020-02-12T13:28:13Z,2020-02-12T15:46:48Z,"#### Code Sample:

```python
import datetime
import pandas as pd
a=pd.to_datetime('2017-04-03T14:06:00+01:00')
b=pd.to_datetime('2017-04-03T14:00:00+01:00')
print(a-b)
print(b-a)

#Timedelta('0 days 00:06:00')
#Timedelta('-1 days +23:54:00')

```
#### Problem description

Two timestamps obtained from Pandas ""to_datetime"" and having a time delta of 6 minutes, will have a strange result when the difference is negative.

#### Expected Output

#### Timedelta('0 days -00:06:00')

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-76-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 4.3.1
pip: 19.0.3
setuptools: 40.8.0
Cython: 0.29.6
numpy: 1.16.1
scipy: 1.2.1
pyarrow: 0.14.0
xarray: None
IPython: 7.3.0
sphinx: 1.8.5
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: 1.2.1
tables: 3.5.1
numexpr: 2.6.9
feather: None
matplotlib: 3.0.3
openpyxl: 2.6.1
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.5
lxml.etree: 4.3.2
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.3.1
pymysql: None
psycopg2: 2.7.6.1 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
564061802,31926,Backport PR #31877 on branch 1.0.x (BUG: fix infer_dtype for StringDtype),meeseeksmachine,closed,2020-02-12T15:05:41Z,2020-02-12T15:54:22Z,Backport PR #31877: BUG: fix infer_dtype for StringDtype
564067006,31928,Backport PR #31918 on branch 1.0.x (BUG: fix parquet roundtrip with unsigned integer dtypes),meeseeksmachine,closed,2020-02-12T15:13:37Z,2020-02-12T15:57:04Z,Backport PR #31918: BUG: fix parquet roundtrip with unsigned integer dtypes
563685201,31907,CLN: remove odious kludge,jbrockmendel,closed,2020-02-12T01:57:37Z,2020-02-12T15:57:58Z,"This is a legacy of Panel/ix AFAICT, was introduced in 2011."
547300372,30843,pre-commit check for flake8 doesn't check Cython code,xhochy,closed,2020-01-09T07:28:35Z,2020-02-12T16:00:42Z,Currently the check only matches `.py` files. Cython files aren't checked.
547477037,30847,BLD: Run flake8 check on Cython files in pre-commit,xhochy,closed,2020-01-09T13:27:14Z,2020-02-12T16:00:49Z,"- [x] closes #30843
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
548492695,30933,DOC: Replace old string formatting syntax in calling of Appender decorators,HH-MWB,closed,2020-01-11T23:13:30Z,2020-02-12T16:01:46Z,"I found calling of `@Appender()` are using `%` to format string. I think it might be better to replace with `.format` based on [PEP 3101](https://www.python.org/dev/peps/pep-3101/) and [a discussion on Stack Overflow](https://stackoverflow.com/questions/5082452/string-formatting-vs-format). 

#29547 is working on replacing `%` with f-strings. This change would also help to keep the code more consistent in string template. Be more specific, the template using `%` to formate will be something like `%(XXX)s`, but using `.format` and f-strings will be the same as `{XXX}`. "
555206312,31325,BUG: MultiIndex intersection with sort=False does not preserve order,jeffzi,closed,2020-01-26T10:05:17Z,2020-02-12T16:04:23Z,"```python
import pandas as pd

arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],
          ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]
tuples = list(zip(*arrays))
idx = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])

left = idx[2::-1]
print(left)
#> MultiIndex([('baz', 'one'),
#>             ('bar', 'two'),
#>             ('bar', 'one')],
#>            names=['first', 'second'])
right = idx[:5]
print(right)
#> MultiIndex([('bar', 'one'),
#>             ('bar', 'two'),
#>             ('baz', 'one'),
#>             ('baz', 'two'),
#>             ('foo', 'one')],
#>            names=['first', 'second'])

# expected same order as left
intersect = left.intersection(right, sort=False)
print(intersect)
#> MultiIndex([('bar', 'two'),
#>             ('baz', 'one'),
#>             ('bar', 'one')],
#>            names=['first', 'second'])
```
<sup>Created on 2020-01-25 by the [reprexpy package](https://github.com/crew102/reprexpy)</sup>

#### Problem description

The intersection of 2 `MultiIndex` with `sort=False` does not preserve the order, whereas other logical operators on Index and MultiIndex preserve order when `sort=False`.

#### Expected Output

The intersection of 2 `MultiIndex` with `sort=False` should return the common elements in the order of the left side.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : ca3bfcc54676b04ac5150c913328d26920bb2591
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.2.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.0rc0+212.gca3bfcc54
numpy            : 1.17.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.1
setuptools       : 45.1.0.post20200119
Cython           : 0.29.14
pytest           : 5.3.4
hypothesis       : 5.3.0
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.3.4
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.47.0

</details>
"
555118834,31312,BUG: MultiIndex intersection with sort=False  does not preserve order,jeffzi,closed,2020-01-25T18:25:24Z,2020-02-12T16:04:32Z,"- [x] closes #31325
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The intersection of 2 `MultiIndex` with `sort=False` does not preserve the order, whereas `Index. intersection()` does. This behavior does not seem to be intentional since the tests for `difference` and `symmetric_difference` are testing for order preservation.

For example:
```python
import pandas as pd

arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],
          ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]
tuples = list(zip(*arrays))
idx = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])

left = idx[2::-1]
print(left)
#> MultiIndex([('baz', 'one'),
#>             ('bar', 'two'),
#>             ('bar', 'one')],
#>            names=['first', 'second'])
right = idx[:5]
print(right)
#> MultiIndex([('bar', 'one'),
#>             ('bar', 'two'),
#>             ('baz', 'one'),
#>             ('baz', 'two'),
#>             ('foo', 'one')],
#>            names=['first', 'second'])

# expected same order as left
intersect = left.intersection(right, sort=False)
print(intersect)
#> MultiIndex([('bar', 'two'),
#>             ('baz', 'one'),
#>             ('bar', 'one')],
#>            names=['first', 'second'])
```
<sup>Created on 2020-01-25 by the [reprexpy package](https://github.com/crew102/reprexpy)</sup>

I verified that my PR does not decrease performances. 

I also modified the test for `union`. The implementation was preserving the order with `sort=False` but the tests were not verifying it."
530641693,29941,"ENH: support datetime64, datetime64tz in nanops.mean, nanops.median",jbrockmendel,closed,2019-11-30T23:16:15Z,2020-02-12T16:28:16Z,"Tracking down relevant issues and figuring out test coverage is the ""WIP"" part of this."
562357790,31844,CLN-29547 replace old string formatting,3vts,closed,2020-02-10T06:45:14Z,2020-02-12T17:25:10Z,
563730392,31911,CLN: implement _getitem_tuple_same_dim,jbrockmendel,closed,2020-02-12T03:42:23Z,2020-02-12T17:49:41Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
560694506,31728,TYP: partial typing of masked array,simonjayhawkins,closed,2020-02-05T23:42:50Z,2020-02-12T20:56:43Z,
555666874,31352,TYP: pandas/core/dtypes/base.py,simonjayhawkins,closed,2020-01-27T15:54:05Z,2020-02-12T20:57:42Z,
563424650,31895,CLN: D202 No blank lines allowed after function docstring,simonjayhawkins,closed,2020-02-11T19:26:39Z,2020-02-12T20:59:38Z,This one could conflict which the unchecked rule blank line before comment
560027458,31669,CLN: remove kwargs from signature of (Index|MultiIndex).copy,topper-123,closed,2020-02-04T22:52:42Z,2020-02-12T21:48:50Z,"Removes ``**kwargs`` from ``Index.copy`` and ``MultiIndex.copy``.
"
560039200,31675,CLN: Remove CategoricalAccessor._deprecations,topper-123,closed,2020-02-04T23:23:05Z,2020-02-12T21:49:03Z,"These deprecated attributes have all been removed, so ``._deprecated`` is no longer needed."
561810111,31788,BUG: Too aggressive typing in NDFrame.align,topper-123,closed,2020-02-07T18:50:27Z,2020-02-12T21:49:41Z,"- [x] closes #31785

The type checking was too aggressive. ``right`` has type ``Any``, so the wrapping in ``_ensure_type`` should not be done."
563528907,31900,TST: parametrize generic/internals tests,jbrockmendel,closed,2020-02-11T22:21:50Z,2020-02-12T21:53:46Z,
563267926,31879,Monthly Dev Meeting,TomAugspurger,closed,2020-02-11T15:04:22Z,2020-02-12T22:09:02Z,"The next monthly dev call is tomorrow (Wednesday, February 12th) at 18:00 UTC. We invite all to come. https://dev.pandas.io/docs/development/meeting.html

We'll use this Zoom link: https://zoom.us/j/942410248

The agenda is at https://docs.google.com/document/d/1tGbTiYORHiSPgVMXawiweGJlBw5dOkVJLY-licoBmBU/edit?usp=sharing if you have topics to discuss."
558040890,31486,DOC: Parameter doc strings for Groupby.(sum|prod|min|max|first|last),topper-123,closed,2020-01-31T10:20:17Z,2020-02-12T22:12:38Z,Follow-up to #31473.
557796964,31473,CLN: clean signature in Groupby.add_numeric_operations,topper-123,closed,2020-01-30T22:33:41Z,2020-02-12T22:13:34Z,"Avoid **kwars, replace with named parameters."
544157086,30578,ENH: Add ignore_index to sort_index,charlesdong1991,closed,2019-12-31T10:46:18Z,2020-02-13T00:54:28Z,"- [x] closes #30114
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry


@jreback 
I just looked back at #30114 and found out that i overlooked to add `ignore_index` to `sort_index`, and this might be the reason this issue is still left open. I added this to `sort_index` to close this issue."
550532265,31060,CLN: Replace Appender and Substitution with simpler doc decorator,HH-MWB,closed,2020-01-16T02:13:33Z,2020-02-13T01:39:28Z,"- [X] closes #30933 
- [x] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

A new decorator to handle docstring formatting. There is also an update for an existing case to show how it works."
564384513,31943,sort_index is different from before version,gao4263,closed,2020-02-13T01:37:48Z,2020-02-13T01:47:09Z,"```python
dates = pd.date_range('20170101', periods=6)
df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))
print(df.sort_index())
```
####  version 1 output
D         C         B         A
2017-01-01 -1.418768  0.441179  0.668193 -0.883575
2017-01-02 -0.642718  0.406244 -1.558845 -0.050165
2017-01-03 -0.492745 -1.578937  0.829258 -1.092268
2017-01-04  1.113140 -2.376751  0.608188 -0.080518
2017-01-05  1.003710  1.969951 -1.583741 -0.610659
2017-01-06 -0.154932 -0.375000  0.179116 -0.294583

#### Expected Output
```
                 D        C      B       A
2017-01-01  0.426359  2.542352 -0.324047  0.418973
2017-01-02 -0.834625 -1.356709  0.150744 -1.690500
2017-01-03 -0.018274  0.900801  1.072851  0.149830
2017-01-04 -1.075027 -0.889379 -0.663223 -1.404002
2017-01-05 -1.273966 -1.335761 -1.356561 -1.135199
2017-01-06 -1.590793  0.693430 -0.504164  0.143386
```

"
550379054,31050,HDF5 API docs missing methods,kylekeppler,open,2020-01-15T19:18:50Z,2020-02-13T02:02:53Z,"The [user guide IO Tools HDF5 section](https://pandas.io/docs/user_guide/io.html#hdf5-pytables) references a number of methods that are not documented in the [API docs for HDF5](https://pandas.io/docs/reference/io.html#hdfstore-pytables-hdf5).

The following `HDFStore` methods/attributes are mentioned only in the user guide:

Methods:
- `__init__`
- `remove`
- `create_table_index`
- `get_storer`
- `select_as_coordinates`
- `select_as_multiple`
- `append_to_multiple`
- `select_column`
- `flush`


Attributes:
- `is_open`
- `root`

The following are ""public"" (ie no leading underscore) methods/attributes in 0.25.3 not listed in either location:

Methods:
- `open`
- `close`
- `copy`
- `get_node`
- `items`
- `iteritems`

Attributes:
- `filename`


The ""storer object"" that is the result of `get_storer` doesn't seem to be documented. This can be a `pandas.io.pytables.FrameFixed` or `pandas.io.pytables.AppendableFrameTable` (and maybe others?). The `nrows` and `table` attributes are referenced in the user guide. There are a number of other attributes that appear public but are not documented. Seems this is starting to get into the implementation weeds, but since a method mentioned in the user guide can return this object, it would be nice to have this explicitly documented.

It would be useful to document the public methods/attributes of `HDFStore` in the API docs. is there any reason these were omitted originally? I think all the would need to be done would be to add to [io.rst](https://github.com/pandas-dev/pandas/blob/master/doc/source/reference/io.rst).

There should probably be a discussion of what should be documented for the storer objects. Perhaps just a reference from the `get_storer` API doc page and a listing of methods/attributes of the abstract base class that are implemented by all subclasses?
"
563300115,31881,assert_numpy_array_equal raises TypeError for pd.NA,WillAyd,closed,2020-02-11T15:51:26Z,2020-02-13T08:16:34Z,"Discovered in #31799

```python
>>> arr1 = np.array([True, False])
>>> arr2 = np.array([True, pd.NA])
>>> tm.assert_numpy_array_equal(arr1, arr2)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/williamayd/clones/pandas/pandas/_testing.py"", line 1001, in assert_numpy_array_equal
    if not array_equivalent(left, right, strict_nan=strict_nan):
  File ""/Users/williamayd/clones/pandas/pandas/core/dtypes/missing.py"", line 447, in array_equivalent
    ensure_object(left.ravel()), ensure_object(right.ravel())
  File ""pandas/_libs/lib.pyx"", line 583, in pandas._libs.lib.array_equivalent_object
    raise
  File ""pandas/_libs/lib.pyx"", line 574, in pandas._libs.lib.array_equivalent_object
    elif not (PyObject_RichCompareBool(x, y, Py_EQ) or
  File ""pandas/_libs/missing.pyx"", line 360, in pandas._libs.missing.NAType.__bool__
    raise TypeError(""boolean value of NA is ambiguous"")
TypeError: boolean value of NA is ambiguous
```

Should yield an AssertionError instead of a TypeError"
564512481,31947,Backport PR #31910 on branch 1.0.x (BUG: Handle NA in assert_numpy_array_equal),meeseeksmachine,closed,2020-02-13T08:16:45Z,2020-02-13T12:41:05Z,Backport PR #31910: BUG: Handle NA in assert_numpy_array_equal
560914571,31741,TST: expand tests for ExtensionArray setitem with nullable arrays,jorisvandenbossche,closed,2020-02-06T10:22:46Z,2020-02-13T13:41:55Z,"Follow-up on https://github.com/pandas-dev/pandas/pull/31484 to add some more test cases (eg also test the validation done in the `check_array_indexer` (wrong length, missing values), as well for nullable integer arrays), similarly to the tests I added for `__getitem__`.

cc @charlesdong1991 "
563725767,31910,BUG: Handle NA in assert_numpy_array_equal,dsaxton,closed,2020-02-12T03:29:38Z,2020-02-13T14:14:33Z,"- [x] closes #31881
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

I think in the the case where we have NA we want to use identity rather than equality for checking when two arrays are equal."
564721268,31952,Multiline pd.eval() seems to be broken,resposit,open,2020-02-13T14:29:44Z,2020-02-13T16:59:47Z,"#### Code Sample:

```python
In [93]: df
Out[93]:
   A  B
0  1  2
1  2  3
2  3  4
3  4  5

In [102]: pd.eval(""""""A = df.A - df.B
     ...: B = df.A + df.B
     ...: """""",target=pd.DataFrame())

```
#### Problem description

```python
....
/usr/local/lib64/python3.6/site-packages/pandas/core/computation/scope.py in resolve(self, key, is_local)
    201                 from pandas.core.computation.ops import UndefinedVariableError
    202
--> 203                 raise UndefinedVariableError(key, is_local)
    204
    205     def swapkey(self, old_key: str, new_key: str, new_value=None):

UndefinedVariableError: name 'df' is not defined

```

#### Expected Output
```python
Out[106]:
   A  B
0 -1  1
1 -1  2
2 -1  3
3 -1  4
```

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1062.9.1.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 9.0.3
setuptools       : 45.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None



</details>
"
564273061,31938,CLN: assorted cleanups,jbrockmendel,closed,2020-02-12T21:09:05Z,2020-02-13T17:35:33Z,broken off of other local branches
508713964,29062,REF: Store metadata in an attrs dict,TomAugspurger,closed,2019-10-17T20:50:41Z,2020-02-13T19:11:27Z,"This aids in the implementation of
https://github.com/pandas-dev/pandas/pull/28394. Over there, I'm having
issues with using `NDFrame.__finalize__` to copy attributes, in part
because getattribute on NDFrame is so complicated.

This simplifies that PR because we only need to look in NDFrame.attrs (name borrowed from xarray), 
which is just a plain dictionary.

Aside from the addition of a public NDFrame.attrs dictionary, there
aren't any user-facing API changes."
510295970,29140,Convert DataFrame.rename to keyword only; simplify axis validation,WillAyd,closed,2019-10-21T21:11:20Z,2020-02-13T20:00:23Z,"- [X] closes #29136 and ref #17963
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

I think we can greatly simplify core if we update our assumptions around the axes of dimensions we deal with. I came to this change trying to get rid of `NDFrame._construct_axes_from_arguments` which appears to do some argument mutation that isn't very clear. This was one of the few methods that called that, while also calling `validate_axis_style_args` which is guilty of the same kind of mutation

By being more explicit about the signature and converting this to keyword only arguments (which a FutureWarning detailed anyway) we can simplify the code and I think make more readable

cc @TomAugspurger "
564854175,31956,CLN: D412: No blank lines allowed between a section header and its content,simonjayhawkins,closed,2020-02-13T17:56:02Z,2020-02-13T20:15:35Z,
564863633,31958,CLN: D409: Section underline should match the length of its name,simonjayhawkins,closed,2020-02-13T18:14:27Z,2020-02-13T20:19:08Z,
564874148,31959,CLN: D411: Missing blank line before section,simonjayhawkins,closed,2020-02-13T18:35:14Z,2020-02-13T20:20:48Z,
564884916,31961,CLN: D414: Section has no content,simonjayhawkins,closed,2020-02-13T18:54:56Z,2020-02-13T20:31:15Z,
558770165,31588,BUG: Handle object arrays with NaN in cut,dsaxton,closed,2020-02-02T22:41:04Z,2020-02-13T22:57:03Z,"- [x] closes #31586
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
347767893,22210,pd.io.sql.to_sql  can't find the existing table,yanzhaojun7,closed,2018-08-06T03:37:34Z,2020-02-13T23:25:31Z,"this is my code:
pd.io.sql.to_sql(df, ""TMP_ACCOUNT"", con, if_exists='append', index=False)
this is my error:
(teradata.api.DatabaseError) (3803, ""[42S01] [Teradata][ODBC Teradata Driver][Teradata Database] Table 'TMP_ACCOUNT' already exists. "") 

My database is teradata   ,and table ""TMP_ACCOUNT"" is a VOLATILE MULTISET  TABLE。Before i use pd.io.sql.to_sql i have created the table ""TMP_ACCOUNT"".  Anyone can fix my issue???thanks.
"
563737705,31912,Obtain the category codes from `CategoricalDtype`,ssche,closed,2020-02-12T04:05:53Z,2020-02-14T01:49:48Z,"```python
cat = pd.Categorical.from_codes([-1, 1, 2, 3, 4], ['-', 'a', 'b', 'c', 'd'])
print(cat.codes)
>>> array([-1,  1,  2,  3,  4], dtype=int8)
dtype = cat.dtype

dtype.codes
>>> Traceback (most recent call last):
>>>  File ""<stdin>"", line 1, in <module>
>>> AttributeError: 'CategoricalDtype' object has no attribute 'codes'

```
#### Problem description

Once a `pd.CategoricalDtype` is created, the codes (which are attached to `pd.Categorical`) are essentially gone. I am using another dataframe as data holder for categorical data and need to switch back and forth between code and label representation of categorical. Once a series is converted to categorical the `dtype` does not hold the code information any more.

This is where the codes go missing (`categories` was series with indices representing the codes, but only values are used to construct the `categories`).

```python
@staticmethod
    def validate_categories(categories, fastpath: bool = False):
        ...
        elif not isinstance(categories, ABCIndexClass):
            categories = Index(categories, tupleize_cols=False)
```

#### Expected Output

`dtype.codes` returns something, or even better don't differentiate between pd.Categorical and `pd.CategoricalDtype`.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.17-200.fc31.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_AU.UTF-8
LOCALE           : en_AU.UTF-8

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.0.1
Cython           : 0.29.11
pytest           : 5.1.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 0.9.6
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.9.6
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 2.2.4
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 1.8.6
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : 1.3.12
tables           : 3.5.1
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 0.9.6

</details>
"
564127507,31932,CLN: remove unreachable in Series._reduce,jbrockmendel,closed,2020-02-12T16:45:49Z,2020-02-14T01:54:05Z,"these are made unreachable now that Series._values returns DTA/TDA for datetime64/timedelta64 dtypes.
"
565057148,31969,Update doc decorator for pandas/core/base.py,HH-MWB,closed,2020-02-14T02:01:22Z,2020-02-14T02:02:05Z,"- [ ] working on #31942 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
553910668,31239,COMPAT: tzawareness behavior to be same as datetime,ShaharNaveh,closed,2020-01-23T02:59:44Z,2020-02-14T09:00:54Z,"- [x] closes #28507
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
551857288,31130,TST: Implement external error raised helper function.,ShaharNaveh,closed,2020-01-19T02:55:19Z,2020-02-14T09:05:38Z,"- [x] ref #30999
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Opened PR for what @gfyoung wrote [here](https://github.com/pandas-dev/pandas/issues/30999#issuecomment-575426086)"
167930557,13822,KeyError shows incorrect column name when DataFrame has duplicate columns,Wilfred,closed,2016-07-27T19:02:37Z,2020-02-14T10:34:14Z,"``` python
>>> import pandas as pd
>>> df = pd.DataFrame({'x': [1.], 'y': [2.], 'z': [3.]})
>>> df.columns = ['x', 'x', 'z']
>>> df[['x', 'y', 'z']]
KeyError: ""['z'] not in index""
```

I expected to see `KeyError: ""['y'] not in index""`.

I've tested this on the latest code in master (and on 0.16):

```
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.3.final.0
python-bits: 64
OS: Linux
OS-release: 2.6.18-400.1.1.el5
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_GB
LOCALE: None.None

pandas: 0.18.1+279.g31f8e4d
nose: None
pip: 1.3.1
setuptools: 0.6
Cython: 0.22
numpy: 1.9.2
scipy: None
statsmodels: None
xarray: None
IPython: 3.2.0-1
sphinx: None
patsy: None
dateutil: 2.4.2
pytz: 2015.6
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
boto: None
pandas_datareader: None
```
"
565255148,31973, BUG: groupby-nunique modifies null values,MarcoGorelli,closed,2020-02-14T11:02:02Z,2020-02-14T11:37:53Z,"- [ ] closes #31950
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565449366,31979,CI: macOS py36_macos build fails,AlexKirko,closed,2020-02-14T17:03:48Z,2020-02-14T17:53:15Z,"#### Problem description

Currently macOS py36_macos Azure pipeline fails with this build error:

```
    ImportError: dlopen(/Users/runner/miniconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-darwin.so, 2): Library not loaded: @rpath/libffi.6.dylib
      Referenced from: /Users/runner/miniconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-darwin.so
      Reason: image not found

```

Seen on #31563 and #31977"
564262689,31937,Why does Series.transform() exist?,UchuuStranger,closed,2020-02-12T20:50:16Z,2020-02-14T18:24:13Z,"This is my first issue on GitHub, so apologies in advance if there's something wrong with the format.

My issue does not have any expected output, I just really want to understand if and why the `Series.transform() ` method is not redundant. Overall, the `transform()` methods are very similar to `apply()` methods, and as I was trying to figure out what the difference between them is ([this](https://stackoverflow.com/questions/27517425/apply-vs-transform-on-a-group-object) Stack Overflow topic was helpful), I managed to pinpoint 3 primary differences:

1)	When the DataFrame is grouped on several categories, `apply()` sends the entire sub-DataFrames within the function, while `transform()` sends each column of each sub-DataFrame separately. That's why columns can't access values in other columns within `transform()`;
2)	When the input passed to the function is an iterable of a certain length, `apply()` can still have the output of any length, while `transform()` has a limitation of having to output an iterable of the same length as the input;
3)	When the function outputs a scalar, `apply()` returns that scalar, while `transform()` propagates that scalar to the iterable of the input length.

I conducted a series of experiments that test these three differences on each applicable pandas object type: Series, DataFrame, SeriesGroupBy, and DataFrameGroupBy. I can send my ipynb with the code and the results if necessary, but it would be sufficient to just look at the conclusion for the Series type:

1 – not applicable. In both cases the function has a scalar input.
2 – not applicable. No matter what the function returns, in both cases the result is assigned to the single cell, even if it means entire DataFrames within cells of a Seires.
3 – not applicable. The input length is always ""1"" (it's considered ""1"" even when it's an iterable), so there's no need to propagate.

Inapplicability of 1 is self-explanatory. But 2 was a surprise. Below is the code I tried:
```
import pandas as pd

df = pd.DataFrame({'State':['Texas', 'Texas', 'Florida', 'Florida'], 
                   'a':[4,5,1,3], 'b':[6,10,3,11]})

def return_df(x):
    return pd.DataFrame([[4, 5], [3, 2]])

def return_series(x):
    return pd.Series([1, 2])

df['a'].transform(return_df)
df['a'].transform(return_series)
```

If you try this code, you'll see that it doesn't matter what the function returns. Whatever it is, it will be put inside the single Series cell in its entirety. Is this behavior intentional? It results in the output size being predetermined by the input size, so all the size checks that `Series.transform()` has within itself become redundant. I can't imagine any situation where `Series.transform()` could behave in a different way from `Series.apply()`. And that raises the question I posed: why does `Series.transform()` exist?"
564732304,31953,CLN: index related attributes on Series/DataFrame,topper-123,closed,2020-02-13T14:47:06Z,2020-02-14T21:50:48Z,"Followup to #31126. IMO the current approach to adding the index related class attributes is too indirect and therefore unnecessary difficult to follow. Just adding the class attributes directly on the relevant class makes reading the code easier, IMO.

Notice that the types are already defined in pandas/core/generic.py:304-316."
560410621,31701,started to fixturize pandas/tests/base,SaturnFromTitan,closed,2020-02-05T14:32:07Z,2020-02-15T01:22:29Z,"Part of #23877
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

-------------------------------------------
I didn't apply the fixtures to the rest of `test_ops.py` yet since @jreback voiced some [concerns in this comment](https://github.com/pandas-dev/pandas/pull/30147#discussion_r356587642)."
565048289,31968,DOC: Use use recommended library over deprecated library,martinbjeldbak,closed,2020-02-14T01:28:20Z,2020-02-15T01:52:04Z,"Importing `pandas.io.json.json_normalize` in pandas 1.0.1 gives the following warning

```
FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead
```

This PR updates the documentation here https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html to use the recommended approach."
562817426,31855,TST: parametrize tests.indexing.test_float,jbrockmendel,closed,2020-02-10T20:45:41Z,2020-02-15T01:53:31Z,"Working on making our exception-raising more consistent, getting these tests cleaned up will make that easier."
565464527,31982,CLN 29547 Replace old string formatting syntax with f-strings,alysbrooks,closed,2020-02-14T17:36:30Z,2020-02-15T02:19:42Z,"Addresses #29547 for pandas/io/sas/sas_xport.py. I searched through for instances of the `%` operator and `.format`.

- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

I noticed that `_read_sas_doc` doesn't seem to be used anymore. I could delete that as well, or we could save that for a separate PR.
"
561800459,31787,plotting backends doesn't plot for the first time after importing pandas,rushabh-v,closed,2020-02-07T18:28:42Z,2020-02-15T05:07:15Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
pd.set_option('plotting.backend', 'matplotlib')
pd.Series([1, 2, 3]).plot()
```
```python
import pandas as pd
pd.options.plotting.backend = 'matplotlib'
pd.Series([1, 2, 3]).plot()
```
```python
import pandas as pd
pd.Series([1, 2, 3]).plot(backend='matplotlib')
```
#### Problem description
If you use any of the above three options to plot. for the first time, it will just return something like `<matplotlib.axes._subplots.AxesSubplot at 0x7f6b28a75ac8>
`. And then, from the next time onwards it will plot the graph. Even if you use one option after another then also from the second time onwards it works fine.
 
#### Expected Output
It should plot the graph for the first time also.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.20.5-042005-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_IN
LOCALE           : en_IN.ISO8859-1

pandas           : 1.0.0rc0
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 20.0.2
setuptools       : 41.0.1
Cython           : 0.29.12
pytest           : 5.0.1
hypothesis       : None
sphinx           : 2.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.3.4
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.6.1
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.1.0
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.0.1
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.5
tables           : 3.5.2
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8
numba            : 0.44.1
</details>
"
565520682,31985,Error running pd.show_versions(),langhimebaugh,closed,2020-02-14T19:40:29Z,2020-02-15T05:48:04Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
pd.show_versions()
```
#### Problem description
Was trying to import xlsb file...
pd.read_excel('my_file.xlsb', engine='pyxlsb')
when i received this error:
ImportError: Can't determine version for pyxlsb

Next, I tried pd.show_versions() and received the same error after restarting the kernel and running these two lines...
import pandas as pd
pd.show_versions()
ImportError: Can't determine version for pyxlsb

FYI:
Python 3.6.9 (default, Nov  7 2019, 10:44:02)
IPython 5.5.0 -- An enhanced Interactive Python.

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
In [1]: import pandas as pd

In [2]: pd.show_versions()
Traceback (most recent call last):

  File ""<ipython-input-2-3d232a07e144>"", line 1, in <module>
    pd.show_versions()

  File ""/home/lang/.local/lib/python3.6/site-packages/pandas/util/_print_versions.py"", line 97, in show_versions
    modname, raise_on_missing=False, on_version=""ignore""

  File ""/home/lang/.local/lib/python3.6/site-packages/pandas/compat/_optional.py"", line 98, in import_optional_dependency
    version = _get_version(module)

  File ""/home/lang/.local/lib/python3.6/site-packages/pandas/compat/_optional.py"", line 43, in _get_version
    raise ImportError(f""Can't determine version for {module.__name__}"")

ImportError: Can't determine version for pyxlsb

pd.__version__
Out[3]: '1.0.1'
</details>
"
565371708,31975,CI: Removed pattern check for specific modules,ShaharNaveh,closed,2020-02-14T14:47:35Z,2020-02-15T08:23:06Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565332283,31974,STY: Fixed wrong placement of whitespace,ShaharNaveh,closed,2020-02-14T13:39:16Z,2020-02-15T08:37:15Z,"- [x] ref #30755
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565719341,31996,DOC PR09 Add missing dots at con parameter on io/sql.py file,za,closed,2020-02-15T08:54:18Z,2020-02-15T09:04:32Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565720255,31997,DOC PR09 on freq parameter,za,closed,2020-02-15T09:03:43Z,2020-02-15T09:05:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565720681,31998,DOC PR09 Add missing . on freq parameter on groupby.py,za,closed,2020-02-15T09:07:44Z,2020-02-15T10:07:58Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565716191,31993,"Backport PR: CI:Testing whole doctest, and not specific module",ShaharNaveh,closed,2020-02-15T08:21:19Z,2020-02-15T10:15:40Z,"xref #31975

Doing as mentioned [here](https://github.com/pandas-dev/pandas/pull/31975#issuecomment-586443425)"
565723184,32001,DOC PR09 Add . in the description parameter,za,closed,2020-02-15T09:31:01Z,2020-02-15T10:29:08Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565723153,32000,DOC: PR09 Add missing . on Parameter con description,asyarif93,closed,2020-02-15T09:30:45Z,2020-02-15T10:58:37Z,"- [ ] closes https://github.com/pandanistas/pandanistas_sprint_jakarta2020/issues/22
- [ ] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Output of `python scripts/validate_docstrings.py pandas.DataFrame.to_sql`:
```
################################################################################
##################### Docstring (pandas.DataFrame.to_sql)  #####################
################################################################################

Write records stored in a DataFrame to a SQL database.

Databases supported by SQLAlchemy [1]_ are supported. Tables can be
newly created, appended to, or overwritten.

Parameters
----------
name : str
    Name of SQL table.
con : sqlalchemy.engine.Engine or sqlite3.Connection
    Using SQLAlchemy makes it possible to use any DB supported by that
    library. Legacy support is provided for sqlite3.Connection objects. The user
    is responsible for engine disposal and connection closure for the SQLAlchemy
    connectable See `here                 <https://docs.sqlalchemy.org/en/13/core/connections.html>`_.

schema : str, optional
    Specify the schema (if database flavor supports this). If None, use
    default schema.
if_exists : {'fail', 'replace', 'append'}, default 'fail'
    How to behave if the table already exists.

    * fail: Raise a ValueError.
    * replace: Drop the table before inserting new values.
    * append: Insert new values to the existing table.

index : bool, default True
    Write DataFrame index as a column. Uses `index_label` as the column
    name in the table.
index_label : str or sequence, default None
    Column label for index column(s). If None is given (default) and
    `index` is True, then the index names are used.
    A sequence should be given if the DataFrame uses MultiIndex.
chunksize : int, optional
    Specify the number of rows in each batch to be written at a time.
    By default, all rows will be written at once.
dtype : dict or scalar, optional
    Specifying the datatype for columns. If a dictionary is used, the
    keys should be the column names and the values should be the
    SQLAlchemy types or strings for the sqlite3 legacy mode. If a
    scalar is provided, it will be applied to all columns.
method : {None, 'multi', callable}, optional
    Controls the SQL insertion clause used:

    * None : Uses standard SQL ``INSERT`` clause (one per row).
    * 'multi': Pass multiple values in a single ``INSERT`` clause.
    * callable with signature ``(pd_table, conn, keys, data_iter)``.

    Details and a sample callable implementation can be found in the
    section :ref:`insert method <io.sql.method>`.

    .. versionadded:: 0.24.0

Raises
------
ValueError
    When the table already exists and `if_exists` is 'fail' (the
    default).

See Also
--------
read_sql : Read a DataFrame from a table.

Notes
-----
Timezone aware datetime columns will be written as
``Timestamp with timezone`` type with SQLAlchemy if supported by the
database. Otherwise, the datetimes will be stored as timezone unaware
timestamps local to the original timezone.

.. versionadded:: 0.24.0

References
----------
.. [1] https://docs.sqlalchemy.org
.. [2] https://www.python.org/dev/peps/pep-0249/

Examples
--------
Create an in-memory SQLite database.

>>> from sqlalchemy import create_engine
>>> engine = create_engine('sqlite://', echo=False)

Create a table from scratch with 3 rows.

>>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})
>>> df
     name
0  User 1
1  User 2
2  User 3

>>> df.to_sql('users', con=engine)
>>> engine.execute(""SELECT * FROM users"").fetchall()
[(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]

>>> df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})
>>> df1.to_sql('users', con=engine, if_exists='append')
>>> engine.execute(""SELECT * FROM users"").fetchall()
[(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),
 (0, 'User 4'), (1, 'User 5')]

Overwrite the table with just ``df1``.

>>> df1.to_sql('users', con=engine, if_exists='replace',
...            index_label='id')
>>> engine.execute(""SELECT * FROM users"").fetchall()
[(0, 'User 4'), (1, 'User 5')]

Specify the dtype (especially useful for integers with missing values).
Notice that while pandas is forced to store the data as floating point,
the database supports nullable integers. When fetching the data with
Python, we get back integer scalars.

>>> df = pd.DataFrame({""A"": [1, None, 2]})
>>> df
     A
0  1.0
1  NaN
2  2.0

>>> from sqlalchemy.types import Integer
>>> df.to_sql('integers', con=engine, index=False,
...           dtype={""A"": Integer()})

>>> engine.execute(""SELECT * FROM integers"").fetchall()
[(1,), (None,), (2,)]

################################################################################
################################## Validation ##################################
################################################################################
```"
565731760,32010,DOC: Fix errors in pandas.Series.argmax,farhanreynaldo,closed,2020-02-15T10:55:10Z,2020-02-15T11:33:09Z,"- [x] closes https://github.com/pandanistas/pandanistas_sprint_jakarta2020/issues/18
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

output of `python scripts/validate_docstrings.py pandas.Series.argmax`:
```
################################################################################
################################## Validation ##################################
################################################################################
```"
565723046,31999,DOC PR07: Add description to parameter skipna,farhanreynaldo,closed,2020-02-15T09:29:50Z,2020-02-15T11:33:47Z,"- [ ] closes https://github.com/pandanistas/pandanistas_sprint_jakarta2020/issues/18
- [ ] tests added / passed
- [ ] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

output of `python scripts/validate_docstrings.py pandas.Series.argmax`:
```
################################################################################
################################## Validation ##################################
################################################################################

4 Errors found:
        No extended summary found
        Parameters {'*args', '**kwargs'} not documented
        Missing description for See Also ""numpy.ndarray.argmax"" reference
        No examples section found
```
"
565506365,31983,DOC: pivot_table example,raybellwaves,closed,2020-02-14T19:11:45Z,2020-02-15T13:09:26Z,"The documentation for pivot table seems limited https://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.pivot_table.html

It doesn't show how `df` was generated. In addition, should pivot_table be `pd.pivot_table`."
564364309,31941,DOC: update ohlc docstring so that it reflects the real use #31919,dequadras,closed,2020-02-13T00:30:02Z,2020-02-15T18:07:40Z,"- [x] closes #31919
- [x] tests added / passed

The test python scripts/validate_docstrings.py pandas.core.groupby.GroupBy.ohlc returns errors, but that is something that happens with many functions at the moment

```################################################################################
################# Docstring (pandas.core.groupby.GroupBy.ohlc) #################
################################################################################

Compute open, high, low and close values of a group, excluding missing values.

For multiple groupings, the result index will be a MultiIndex

Returns
-------
DataFrame
    Open, high, low and close values within each group.

See Also
--------
Series.groupby
DataFrame.groupby

################################################################################
################################## Validation ##################################
################################################################################

3 Errors found:
	Missing description for See Also ""Series.groupby"" reference
	Missing description for See Also ""DataFrame.groupby"" reference
	No examples section found```"
565760664,32026,What's the difference between Timedelta and DateOffset on Earth?,GYHHAHA,closed,2020-02-15T15:11:57Z,2020-02-15T19:03:14Z,"After reading the docs, still I feel confused about the difference between the Timedelta and DataOffset.
```python
>>>ts = pd.Timestamp('2020-1-29 01:00:00')
>>>ts+pd.DateOffset(days=-1)
Timestamp('2020-01-28 01:00:00')
>>>ts+pd.Timedelta(days=-1)
Timestamp('2020-01-28 01:00:00')
```
It seems to be completely the same.
And I also find the difference appears when it involves a certain time zone operation.
```python
>>>ts = pd.Timestamp('2020-3-29 01:00:00', tz='Europe/Helsinki')
>>>ts + pd.Timedelta(days=1)
Timestamp('2020-03-30 02:00:00+0300', tz='Europe/Helsinki')
>>>ts + pd.DateOffset(days=1)
Timestamp('2020-03-30 01:00:00+0300', tz='Europe/Helsinki')
```
Therefore, how to tell the difference from these two concepts in a more explicit way?
Thanks for your answer!!!"
565729685,32005,doc: Add period to parameter description,DavaIlhamHaeruzaman,closed,2020-02-15T10:34:47Z,2020-02-15T19:06:35Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

```

there are a few errors left:
################################################################################
################################## Validation ##################################
################################################################################

11 Errors found:
        Parameters {'**fields', 'name', 'ordinal'} not documented
        Unknown parameters {'second', 'minute', 'year', 'month', 'quarter', 'hour', 'day'}
        Parameter ""year"" has no description
        Parameter ""month"" has no description
        Parameter ""quarter"" has no description
        Parameter ""day"" has no description
        Parameter ""hour"" has no description
        Parameter ""minute"" has no description
        Parameter ""second"" has no description
        Parameter ""dtype"" has no description
        Examples do not pass tests
```"
565738669,32021,DOC SS06 Make the summery in one line on offsets.py,za,closed,2020-02-15T12:01:20Z,2020-02-15T19:07:25Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565732966,32014,DOC: Update pandas.Series.between_time docstring params,adamwdb,closed,2020-02-15T11:05:41Z,2020-02-15T19:10:34Z,"- [x] closes [https://github.com/pandanistas/pandanistas_sprint_jakarta2020/issues/20](https://github.com/pandanistas/pandanistas_sprint_jakarta2020/issues/20)
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

output from python scripts/validate_docstrings.py pandas.Series.between_time:
################################################################################
#################### Docstring (pandas.Series.between_time) ####################
################################################################################

Select values between particular times of the day (e.g., 9:00-9:30 AM).

By setting ``start_time`` to be later than ``end_time``,
you can get the times that are *not* between the two times.

Parameters
----------
start_time : datetime.time or str
        The first time value.
end_time : datetime.time or str
        The second time value.
include_start : bool, default True
        Adding start_time value in the result.
include_end : bool, default True
        Adding end_time value in the result.
axis : {0 or 'index', 1 or 'columns'}, default 0
        Determine range time on index or columns value.
    .. versionadded:: 0.24.0

Returns
-------
Series or DataFrame
    If axis set on columns, it will return DataFrame format, vice versa.

Raises
------
TypeError
    If the index is not  a :class:`DatetimeIndex`

See Also
--------
at_time : Select values at a particular time of the day.
first : Select initial periods of time series based on a date offset.
last : Select final periods of time series based on a date offset.
DatetimeIndex.indexer_between_time : Get just the index locations for
    values between particular times of the day.

Examples
--------
>>> i = pd.date_range('2018-04-09', periods=4, freq='1D20min')
>>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)
>>> ts
                     A
2018-04-09 00:00:00  1
2018-04-10 00:20:00  2
2018-04-11 00:40:00  3
2018-04-12 01:00:00  4

>>> ts.between_time('0:15', '0:45')
                     A
2018-04-10 00:20:00  2
2018-04-11 00:40:00  3

You get the times that are *not* between two times by setting
``start_time`` later than ``end_time``:

>>> ts.between_time('0:45', '0:15')
                     A
2018-04-09 00:00:00  1
2018-04-12 01:00:00  4

################################################################################
################################## Validation ##################################
################################################################################"
565790174,32029,CLN: GH29547 replace old string formatting,pcandoalmeida,closed,2020-02-15T18:27:21Z,2020-02-15T20:10:57Z,"Hi there! This is my first contribution. Please let me know if there are any issues, thank you.
- [x] tests passed
- [x] passes `black pandas`
"
565757061,32025,CI: silence numpy-dev failures,simonjayhawkins,closed,2020-02-15T14:43:34Z,2020-02-18T10:54:50Z,xref #31992
563423021,31893,CLN: D213: Multi-line docstring summary should start at the second line,simonjayhawkins,closed,2020-02-11T19:23:29Z,2020-02-15T20:15:30Z,
565739064,32022,DOC: Add missing period to parameter description,DavaIlhamHaeruzaman,closed,2020-02-15T12:05:33Z,2020-02-15T21:50:54Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565730160,32007,CLN 29574 Replace old string formating,panjacek,closed,2020-02-15T10:39:50Z,2020-02-16T06:31:37Z,"Its my first contribution, modified the str formating to use fstrings
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ]  pandas/tests/frame/test_to_csv.py
"
283244757,18842,OverflowError for string of large numbers (int64) while reading json,MaxBer,closed,2017-12-19T14:17:28Z,2020-02-16T06:57:01Z,"

```python
import json
import pandas as pd
json_data = {""articleId"":""1404366058080022500245""}
pd.read_json(json.dumps(json_data), typ='series')

Python int too large to convert to C long: OverflowError
...
File ""/var/task/pandas/io/json/json.py"", line 366, in read_json
return json_reader.read()
File ""/var/task/pandas/io/json/json.py"", line 467, in read
obj = self._get_object_parser(self.data)
File ""/var/task/pandas/io/json/json.py"", line 489, in _get_object_parser
obj = SeriesParser(json, **kwargs).parse()
File ""/var/task/pandas/io/json/json.py"", line 582, in parse
self._try_convert_types()
File ""/var/task/pandas/io/json/json.py"", line 752, in _try_convert_types
'data', self.obj, convert_dates=self.convert_dates)
File ""/var/task/pandas/io/json/json.py"", line 621, in _try_convert_data
new_data, result = self._try_convert_to_date(data)
File ""/var/task/pandas/io/json/json.py"", line 684, in _try_convert_to_date
new_data = data.astype('int64')
File ""/var/task/pandas/util/_decorators.py"", line 118, in wrapper
return func(*args, **kwargs)
File ""/var/task/pandas/core/generic.py"", line 4004, in astype
**kwargs)
File ""/var/task/pandas/core/internals.py"", line 3462, in astype
return self.apply('astype', dtype=dtype, **kwargs)
File ""/var/task/pandas/core/internals.py"", line 3329, in apply
applied = getattr(b, f)(**kwargs)
File ""/var/task/pandas/core/internals.py"", line 544, in astype
**kwargs)
File ""/var/task/pandas/core/internals.py"", line 625, in _astype
values = astype_nansafe(values.ravel(), dtype, copy=True)
File ""/var/task/pandas/core/dtypes/cast.py"", line 692, in astype_nansafe
return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape)
File ""pandas/_libs/lib.pyx"", line 854, in pandas._libs.lib.astype_intsafe
File ""pandas/_libs/src/util.pxd"", line 91, in util.set_value_at_unsafe
OverflowError: Python int too large to convert to C long
```

Reading json containing stringified numbers larger than int64 causes OverflowError in pandas 0.21.1. Appears to be working fine in 0.21.0

Max


"
549248716,30990,Added clang inline helper,WillAyd,closed,2020-01-13T23:48:43Z,2020-02-16T18:41:27Z,"gcc on macOS I think executes clang more often than not, and it appears we didn't have a inline helper defined for that. This is ported from `CYTHON_INLINE` which essentially does the same in cythonized files, save the static declaration

This might add a bit of build time to macOS (I think partially explains why it's a few minutes faster than the rest) but makes it more similar in performance to the others

Running the full benchmark suite to see where this may help"
484984100,28143,Use clang-format for Extension Modules,WillAyd,closed,2019-08-25T22:34:32Z,2020-02-16T21:49:36Z,"Right now we use cpplint for header files only. Unfortunately this leaves some of our extension module implementations rather sloppy.

We could extend cpplint to those but from what I have seen it doesn't have an opinion on indentation, which coming from a Python perspective could be useful. It also not surprisingly focuses more on C++ which isn't applicable to our extensions

As an alternate I'd propose taking a look at the `clang-format` tool. I'll push up a PR for review to give an idea of diff. gnu indent is also an option though I'm not sure if that's available to Windows devs that may want to work on these

In either case a CI code check could be added simply using a rule like this:

https://stackoverflow.com/questions/22866609/can-clang-format-tell-me-if-formatting-changes-are-necessary#comment63404587_22866610"
317736064,20820,Established Standards for Tests,WillAyd,closed,2018-04-25T18:06:56Z,2020-02-16T21:50:39Z,"xref https://github.com/pandas-dev/pandas/pull/20813#discussion_r183850580 there is some inconsistency with how tests are built which we could standardize and cleanup. For example, all of the following may appear in any given test:

```python
# Short variable name notation
res = 'foo'
exp = 'bar'
assert res == exp

# Evaluation as operand
exp = 'bar'
assert some_func() == exp

# Full variable name notation
result = 'foo'
expected = 'bar'
assert result == expected
```

There are of course many more combinations in between but the above should cover the concept. With the assumption that the last is preferable, the proposal here is to explicitly document that somewhere (perhaps TESTING_README.md?) and beyond that perhaps set up LINTing rules to enforce the standard.

Thoughts?"
563795364,31915,CLN: Some groupby internals,mroeschke,closed,2020-02-12T07:03:11Z,2020-02-17T00:23:58Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

1. Avoids calling `_get_sorted_data()` twice
2. Eliminates `*args, **kwargs` from an internal function"
565717910,31995,DOC: Add See Also section on series.py,za,closed,2020-02-15T08:39:06Z,2020-02-17T08:23:50Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
455697784,26831,DOC: Discussion on new getting started page (Numfocus Grant),stijnvanhoey,closed,2019-06-13T11:55:27Z,2020-02-17T11:49:43Z,"As currently prepared together with @jorisvandenbossche, the following document is a proposal on how to adjust the getting started page and section of the documentation: https://docs.google.com/document/d/1Rc_eql5KLrdf0c582KyWfs2ADVNxbJy4jfosnqdrVak/edit?usp=sharing

The general idea is to split the current 10 minutes into 10 x 1' topics of Pandas. Input is very welcome on:
- the topics to pick and how to introduce them
- the content of the introduction tutorial linked to each of the topics"
566276353,32054,error that converts nan to non-nan float value via groupby and nunique,mnky9800n,closed,2020-02-17T12:46:10Z,2020-02-17T12:51:31Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np
print(pd.__version__)
>> '1.0.1'
df = pd.DataFrame({'a':[1,2,3], 'b':[np.nan, np.nan, np.nan], 'c':['d', 'e', 'f']}, index=[0, 1, 2])
print(df)
print(df.loc[1].b)
# >> nan
df.groupby('a').nunique()
print(df)
#    a             b  c
# 0  1 -9.223372e+18  d
# 1  2 -9.223372e+18  e
# 2  3 -9.223372e+18  f
print(df.c.unique())
print(df)
# print(df.groupby('a').nunique().c.unique())
# >> array([1])
print(df.loc[1].b)
# >> -9.223372036854776e+18
```
#### Problem description

When a `groupby` operation followed by an `nunique` operation, nans are converted to non-nan float values. According to the documentation, the [`nunique`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.nunique.html) function should return a series but should not update the original dataframe. Also according to the documentation you can ignore nans, however this seems to still change the value nans to a non-nan number.

This does not happen if `nunique` is applied without the `groupby` function.

#### Expected Output
The expected output is that nan values should not be converted to any other value based on an aggregrate operation on the dataframe.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.10.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-45-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : 0.29.15
pytest           : 5.3.5
hypothesis       : 5.4.1
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.48.0
</details>
"
565200984,31972,"Boolean diff is slower than float, int diff",JulianWgs,closed,2020-02-14T09:20:50Z,2020-02-17T13:05:58Z,"#### Code Sample

This code in Google Colab (Jupyter Notebook/Lab):
```python
import numpy as np
import pandas as pd


size = int(1e7)
value = np.sin(np.linspace(0, 100, size)) < 0.5

print(""Numpy Boolean"")
%timeit np.diff(value)

value = value.astype(int)
print(""Numpy Integer"")
%timeit np.diff(value)
df = pd.Series(np.sin(np.linspace(0, 100, size))) < 0.5

print(""Pandas Boolean"")
%timeit df.diff()

print(""Pandas Integer"")
%timeit df.astype(int).diff()

print(""Pandas Float"")
%timeit df.astype(float).diff()
```
Creates the following output:
```
Numpy Boolean
100 loops, best of 3: 11.8 ms per loop
Numpy Integer
100 loops, best of 3: 19 ms per loop
Pandas Boolean
1 loop, best of 3: 208 ms per loop
Pandas Integer
10 loops, best of 3: 51.7 ms per loop
Pandas Float
10 loops, best of 3: 43.8 ms per loop
```
#### Problem description

The diff function on a boolean series is about four times slower than on an integer or float series. This behavior is not reproducable in numpy, where diff on a boolean series is the fastest. Right now it is even faster to convert the boolean series to integer, do the diff and then convert back to boolean.

#### Expected Output

Boolean diff should be the fastest.

#### Output of ``pd.show_versions()``

Google Colab with pandas 1.0.1 installed.

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.137+
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.17.5
pytz             : 2018.9
dateutil         : 2.6.1
pip              : 19.3.1
setuptools       : 45.1.0
Cython           : 0.29.15
pytest           : 3.6.4
hypothesis       : None
sphinx           : 1.8.5
blosc            : None
feather          : 0.4.0
xlsxwriter       : None
lxml.etree       : 4.2.6
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.6.1 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 5.5.0
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.6
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 2.5.9
pandas_gbq       : 0.11.0
pyarrow          : 0.14.1
pytables         : None
pytest           : 3.6.4
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.4.4
tabulate         : 0.8.6
xarray           : 0.14.1
xlrd             : 1.1.0
xlwt             : 1.3.0
xlsxwriter       : None
numba            : 0.47.0

</details>

#### Next steps
- Find out why it is slower
- Quick fix: `df.astype(int).diff().dropna().astype(bool)` which benchmarks to `10 loops, best of 3: 114 ms per loop`"
500216973,28683,Read long text from a cell,LukaszKrolicki,closed,2019-09-30T11:31:16Z,2020-02-17T14:53:36Z,"So I have very long text in excel cell, when I want to read this it only shows a few lines. How to get all text from a cell?

        tekst=self.alco.loc[self.alco['Name']=='Vodka', 'description']    
        tekst=tekst.to_string(index=False)
        print(tekst)

Output:
Vodka is a distilled beverage composed primari...
"
526290658,29759,TypeError: data type not understood,tusharsp60,closed,2019-11-21T00:19:49Z,2020-02-17T15:23:49Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here

Reducer()
#!/usr/bin/env python3
# coding: utf-8

# In[7]:


import pandas as pd
import sys
import re

data=pd.read_csv(sys.stdin,names=[""LOCATION"",""Value""],dtype=['str','float64'])
data[""Value""]=data[""Value""].str.strip()
#data1=pd.DataFrame(data,columns=['LOCATION','Value'],dtype=['str','float'])
#data[""Value""]=data.Value.astype(""float64"")
#data[""Value""]=data[""Value""].str.astype(""float64"")
#data[""Value""]=float(data[""Value""])
#data[""Value""]=data[""Value""].astype(float)
#s=pd.Series(creditsa['Value'])
#pd.to_numeric(s)
#df1=pd.DataFrame(data=s,columns=['Value'])
#df1=data.groupby([""LOCATION""])[""Value""].mean()
print(data.info())
```
#### Problem description
unable to convert datatype of column Value to float. I have tried several possible ways of doing it(commented in above code) but I am getting below error. Can you please let me know what is the problem here.
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/common.py"", line 2050, in pandas_dtype
    npdtype = np.dtype(dtype)
TypeError: data type not understood

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/hadoop/reducer.py"", line 13, in <module>
    data=pd.read_csv(sys.stdin,names=[""LOCATION"",""Value""],dtype=['str','float64'])
  File ""/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py"", line 685, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py"", line 457, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py"", line 895, in __init__
    self._make_engine(self.engine)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py"", line 1135, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py"", line 1917, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File ""pandas/_libs/parsers.pyx"", line 493, in pandas._libs.parsers.TextReader.__cinit__
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/common.py"", line 2054, in pandas_dtype
    raise TypeError(""data type not understood"")
TypeError: data type not understood

#### Expected Output
dtype of Value column should be float
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
505471489,28905,Warning when plotting a DataFrame with a single row,madphysicist,closed,2019-10-10T19:41:05Z,2020-02-17T15:27:36Z,"Closely related to #18306, item 9 in the listing, but probably not an exact duplicate. Based on StackOverflow question https://stackoverflow.com/q/58322744/2988730.

Simple code to reproduce:

```
import pandas as pd

df = pd.DataFrame([[1, 2, 3]], columns=['A', 'B', 'C'])
df.set_index('A', inplace=True)
df.plot()
```

This generates

```
UserWarning: Attempting to set identical left == right == 1.0 results in singular transformations; automatically expanding.
  ax.set_xlim(left, right)
```

Expected behavior is that the default xlims of `[None, None]` will let matplotlib compute its own limits. Instead, it appears that pandas is computing the limits based on the min and max of the data. This seems redundant at best, and incorrect in this particular case."
560599263,31720,`Series.replace` fails on categoricals with list,bnaul,closed,2020-02-05T20:05:27Z,2020-02-17T16:59:53Z,"Looks like this was introduced in #27026: not sure of the details of what was incorrect in the old version but I do know that this at least didn't error:
```python
cat = pd.CategoricalIndex(['a', 'b']).to_series()
cat.replace('a', 'A')  # works
cat.replace(['a'], 'A')
~/model/.venv/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in replace(self, to_replace, value, inplace)
   2440         inplace = validate_bool_kwarg(inplace, ""inplace"")
   2441         cat = self if inplace else self.copy()
-> 2442         if to_replace in cat.categories:
   2443             if isna(value):
   2444                 cat.remove_categories(to_replace, inplace=True)

~/model/.venv/lib/python3.7/site-packages/pandas/core/indexes/base.py in __contains__(self, key)
   3898     @Appender(_index_shared_docs[""contains""] % _index_doc_kwargs)
   3899     def __contains__(self, key) -> bool:
-> 3900         hash(key)
   3901         try:
   3902             return key in self._engine

TypeError: unhashable type: 'list'
```
Seems like most of the methods used in this implementation already handle list-like inputs so it should be a pretty easy fix..?

#### Output of ``pd.show_versions()``

<details>
pandas           : 1.0.0
</details>
"
560775119,31734,BUG: list-like to_replace on Categorical.replace is ignored or crash,JustinZhengBC,closed,2020-02-06T04:41:51Z,2020-02-17T17:00:54Z,"- [X] closes #31720
- [X] tests added / passed
- [x] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

Covers the case where `to_replace` is a list-like and `value` is a string. Other cases, like ""`to_replace` is dict and `value` is None"", or ""`to_replace` and `value` are both lists"" are handled earlier in generic.py"
560969543,31743,"pd.to_datetime() Error in MacOS - TypeError(f""dtype {data.dtype} cannot be converted to datetime64[ns]"") TypeError: dtype timedelta64[ns] cannot be converted to datetime64[ns]",CaioEuzebio,closed,2020-02-06T12:05:07Z,2020-02-17T17:06:16Z,"#### Function Error in  MacOS

```python
df = pd.read_csv('filemane.csv', encoding='latin-1')

df1['ProcessFinishTime'] = pd.to_datetime(df1['ProcessFinishTime'],errors='ignore')
df1['ProcessStartTime'] = pd.to_datetime(df1['ProcessStartTime'],errors='ignore')
```


#### Problem description

Hi Guys,

This is a problem that a faced in MacOS only about DateTime, I trying to convert format (12:54:58)
in to DateTime.

It run correctly in Windows(10) and Linux(ubuntu) as well, however, in MacOS i faced this problem that returns an error message below:

```
raise TypeError(f""dtype {data.dtype} cannot be converted to datetime64[ns]"")
TypeError: dtype timedelta64[ns] cannot be converted to datetime64[ns]
``` 

I search for and for while it's not cleary to me the reason for this error happens.

And my questions is:

**1**- This issue is particular for MAcOS?

**2**2 - I try to replace pd.to_datetime() for pd.to_timedelta(), however pd.to_datetime is too better.

**3**- I try to replace pd.to_datetime() for pd.to_timedelta(), however pd.to_datetime is too better.


Below is the documentation that i follow.:

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html

Using latest version in both systems.

"
566048455,32050,map(lambda x: x.encode()) on a dataframe column produces address of result instead of the actual result,bwanaaa,closed,2020-02-17T04:27:49Z,2020-02-17T17:24:10Z,"#### Code Sample, a copy-pastable example if possible
Using pandas 1.0.1 and python 3.7
Using a text file consisting of words separated by carriage returns named sampleTEXT.txt
```
import base64
import numpy as np
import pandas as pd
words = pd.read_table(""sampleTEXT.txt"",names=['word'],header=None)
words.head()
---      word
---------------
---0    difference
---1    where
---2    mc
---3    is
---4    the
---
---
words['words_encoded'] = map(lambda x: x.encode('base64','strict'), words['word'])
print (words)
```
#### Problem description
output is this
```
               word                   words_encoded
0         difference              <map object at 0x7fac57615310>
1              where               <map object at 0x7fac57615310>
2                 mc                 <map object at 0x7fac57615310>
3                 is                   <map object at 0x7fac57615310>
4                the                  <map object at 0x7fac57615310>
...              ...                             ...
999995  distribution          <map object at 0x7fac57615310>
999996            in               <map object at 0x7fac57615310>
999997      scenario           <map object at 0x7fac57615310>
999998          less              <map object at 0x7fac57615310>
999999          land              <map object at 0x7fac57615310>

[1000000 rows x 2 columns]
```

#### Expected Output

I expect the ACTUAL base64 encoded words in the second column


#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-28-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200209
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
552376337,31156,DOC: Update of the 'getting started' pages in the sphinx section of the documentation,stijnvanhoey,closed,2020-01-20T15:42:12Z,2020-02-17T18:54:40Z,"- [ ] closes #26831
 
This PR provides an update of the getting started pages of the Pandas documentation, following from the discussion in #26831 and the [proposal](https://docs.google.com/document/d/1Rc_eql5KLrdf0c582KyWfs2ADVNxbJy4jfosnqdrVak/edit#heading=h.mqz2f6gbl3sd). The update went together with the creation of the new theme and tries to integrate as much as possible with the new layout (using bootstrap elements).  This PR focuses on the getting started section and adds new sections to the documentation:

 - An update of the 'getting started' intro page of the sphinx documentation to provide new users some more guidance on the 'first steps' with Pandas: Directions for installation, a short intro on main Pandas features each linking to a dedicated introduction tutorial (see next point), specific info targeted to people with another background and reference to more tutorials.
- A set of tutorials which introduce some key features of the Pandas package. Each tutorial is setup as a series of 'tasks/questions' so users get a first idea of the type of problems they can solve with Pandas for that topic. All tutorials provide a set of [returning elements](https://stijnvanhoey.github.io/pandas-getting-started-tutorials/tutorial-elements/). Both the intros as well as the tutorials itself try to guide people as much as possible to related sections of the user guide. 
- A set of schemas to illustrate certain concepts, with according to the new theme. These schemas can be used in other locations in the user guide as well.

The aim is to move the [dsintro](https://pandas.io/docs/getting_started/dsintro.html) and the [basics](https://pandas.io/docs/getting_started/basics.html) to the general user guide, as both are too extensive for a getting started section. The 10 intro tutorials try to provide an alternative that is more fit to people starting with Pandas. 

There is still some work to do (typo's in the tutorials, additional schemas...), but input is certainly welcome and appreciated, I'll continue improving it. See https://stijnvanhoey.github.io/example-pandas-docs/getting-started/getting_started/index.html for a live preview.

Note: (1) a separate PR is prepared for the general sphinx intro page with @jorisvandenbossche, see #31148  (2) some of the layout issues are more general and related to the [new sphinx theme](https://github.com/pandas-dev/pydata-bootstrap-sphinx-theme) and will be tackled there (e.g. spacing around titles and subtitles)

Some pictures:
- The intro to the 10 tutorials:
![image](https://user-images.githubusercontent.com/754862/72739475-fbe4b800-3ba3-11ea-9256-1617ed726b44.png)

- the coming from section :
![image](https://user-images.githubusercontent.com/754862/72739547-1c147700-3ba4-11ea-9cda-e518af641df7.png)

- example of 'remember' section in each tutorial:
![image](https://user-images.githubusercontent.com/754862/72739624-4403da80-3ba4-11ea-81b8-0df26fc129f7.png)


 "
565834545,32035,CLN: Clean reductions/test_reductions.py,dsaxton,closed,2020-02-16T01:27:07Z,2020-02-17T20:05:00Z,Some more parameterizing / splitting up of tests
562882782,31859,REF: move loc-only methods to loc,jbrockmendel,closed,2020-02-10T23:07:04Z,2020-02-17T20:06:04Z,
566465897,32064,DOC: pin gitdb2,TomAugspurger,closed,2020-02-17T18:44:41Z,2020-02-17T21:35:25Z,ref #32060
563684508,31906,REF: implement unpack_1tuple to clean up Series.__getitem__,jbrockmendel,closed,2020-02-12T01:55:21Z,2020-02-17T22:16:06Z,"Doing this up-front instead of in two places makes for a nice cleanup.  There will be a small perf penalty for the cases that wouldn't otherwise need to do these checks.
"
566527122,32066,Backport PR #32064 on branch 1.0.x (DOC: pin gitdb2),meeseeksmachine,closed,2020-02-17T21:35:32Z,2020-02-17T22:57:28Z,Backport PR #32064: DOC: pin gitdb2
422599893,25774,length_of_indexer method is not support boolean array indexer.,fx-kirin,closed,2019-03-19T08:28:11Z,2020-02-18T00:15:46Z,"#### Code Sample

```python
import numpy as np
import pandas as pd
import random

# Nothing Raises
df = pd.DataFrame(dict(A=[1, 1, 1, 1, 1], B=[1., 1., 1., 1., 1.], C=[1, 1, 1, 1, 1]))
df.index = [0, 0, 1, 3, 5]
df.index.name = 'test'
df.loc[0, 'A'] = df.loc[0, 'A']

# ValueError raises
length = 20000
df = pd.DataFrame(dict(A=[1 for _ in range(length)], B=[1. for _ in range(length)], C=[1 for _ in range(length)]))
index = []
for _ in range(length):
    index.append(random.randint(0, 2000))
df.index = index
df.loc[0, 'A'] = df.loc[0, 'A']
```
#### Problem description

https://github.com/pandas-dev/pandas/blob/db6993cdd970eb13790493dba38bb637e341948b/pandas/core/indexing.py#L517

https://github.com/pandas-dev/pandas/blob/db6993cdd970eb13790493dba38bb637e341948b/pandas/core/indexing.py#L546-L562

When the `DataFrame.loc` method uses boolean array selector in the backgroud, the variable `lplane_indexer` is not the same value as the number of True value in the array but the size of DataFrame. It didn't happen in a former version.

#### Expected Output

No error raises.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.7.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-46-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.3
pytest: 3.7.3
pip: 18.1
setuptools: 36.5.0.post20170921
Cython: 0.28.4
numpy: 1.15.4
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 7.2.0
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2017.3
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 3.0.2
openpyxl: None
xlrd: 1.2.0
xlwt: None
xlsxwriter: 1.1.5
lxml: None
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.10
pymysql: 0.9.3
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None


</details>
"
563474387,31897,BUG: fix length_of_indexer with boolean mask,jbrockmendel,closed,2020-02-11T21:01:49Z,2020-02-18T00:46:10Z,"- [x] closes #25774
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Allows for a nice cleanup in _setitem_with_indexer.  Note this will conflict with #31887; the order doesn't matter."
451137239,26613,to_csv does not fail with specified columns not in dataframe ,amcpherson,closed,2019-06-02T00:09:24Z,2020-02-18T03:44:15Z,"#### Code Sample, a copy-pastable example if possible

```python
In [4]: data = pd.DataFrame({'a': [1, 2, 3]})

In [5]: data.to_csv('test.csv', columns=['a', 'x'])
/Users/amcphers/Projects/scgenome/venv/lib/python3.7/site-packages/pandas/core/indexing.py:1494: FutureWarning:
Passing list-likes to .loc or [] with any missing label will raise
KeyError in the future, you can use .reindex() as an alternative.

See the documentation here:
https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike
  return self._getitem_tuple(key)
...
$ head test.csv
,a,x
0,1,
1,2,
2,3,
```
#### Problem description

The documentation specified that the columns kwarg is ""Columns to write.""  If there is no matching column to write this would seem to warent an error of some kind.  Instead an unrelated warning is printed.

#### Expected Output

An exception should be thrown.

#### Output of ``pd.show_versions()``

<details>

In [6]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Darwin
OS-release: 16.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_CA.UTF-8
LOCALE: en_CA.UTF-8

pandas: 0.24.2
pytest: None
pip: 19.1
setuptools: 41.0.1
Cython: 0.29
numpy: 1.16.2
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 5.8.0
sphinx: None
patsy: None
dateutil: 2.7.5
pytz: 2018.7
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.2.3
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
566210986,32051,WEB: update blog link to only include my pandas blog posts,jorisvandenbossche,closed,2020-02-17T10:42:48Z,2020-02-18T08:16:38Z,
565968699,32044,CLN: 29547 replace .format() with f-strings,smartvinnetou,closed,2020-02-16T20:58:54Z,2020-02-18T09:19:42Z,"This PR replaces .format() with f-strings as requested in https://github.com/pandas-dev/pandas/issues/29547.

It did this replacement in scripts/find_commits_touching_func.py

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
566294545,32057,Backport PR #32025 and #32031 silence numpy-dev failures,simonjayhawkins,closed,2020-02-17T13:20:18Z,2020-02-18T10:54:15Z,"
"
565800287,32031,CI: change np-dev xfails to not strict,simonjayhawkins,closed,2020-02-15T19:52:07Z,2020-02-18T10:56:10Z,follow-up to #32025
566620530,32070,Hope to optimize pd.ExcelWriter mode,qianxuanyon,closed,2020-02-18T03:19:49Z,2020-02-18T12:30:00Z,"The following code can be implemented to override the sheet and update the specified sheet separately

```python
import pandas as pd
import openpyxl

def to_sheet(df,file,sheet_name):
    wb = openpyxl.load_workbook(file)
    if sheet_name in wb.sheetnames:
        sheet = wb[sheet_name]
        wb.remove(sheet)
    wb.save(file)
    
    with pd.ExcelWriter(file,mode='a',engine='openpyxl') as excel:
        df.to_excel(excel,sheet_name=sheet_name,index=False) 
```  

Hope to optimize pd.ExcelWriter mode"
566297304,32058,Unable to see currency when loading excel file,ShayHa,closed,2020-02-17T13:25:22Z,2020-02-18T12:31:59Z,"
```python
df = pd.read_excel(file)

```
So I have a few columns with numbers and currency e.g 155 USD.
When I load this table to python all I see is the numbers without the currency.
My solution was to convert the file to csv before loading it but I just hate using powershell script to convert each excel file to csv before using it.

Thanks.
"
566953919,32076,FutureWarning: pandas.util.testing is deprecated,ghost,closed,2020-02-18T15:03:50Z,2020-02-18T15:05:19Z,"FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.

It appears everytime I run:
`import pandas as pd`
`from pandas_datareader import data as wb`"
566425959,32063,CLN: GH29547 replace old string formatting,pcandoalmeida,closed,2020-02-17T17:06:14Z,2020-02-18T15:18:19Z,"Hi there! I've added some missed f-strings to:
* tests/tslibs
* tests/tseries

- [x] tests passed
- [x] passes `black pandas`

Thank you.
"
565798796,32030,CLN: remove unused from MultiIndex,jbrockmendel,closed,2020-02-15T19:39:51Z,2020-02-18T15:30:59Z,
566248218,32052,Backport PR #31156 on branch 1.0.x (DOC: Update of the 'getting started' pages in the sphinx section of the documentation),meeseeksmachine,closed,2020-02-17T11:51:00Z,2020-02-18T15:42:02Z,Backport PR #31156: DOC: Update of the 'getting started' pages in the sphinx section of the documentation
567071038,32080,"REGR: Series.str.encode(""base64"")",simonjayhawkins,closed,2020-02-18T18:15:58Z,2020-02-18T19:32:06Z,"- [ ] closes #32048
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
567095130,32081,Repeated `convert_dtypes()` fails for unicode string,tsoernes,closed,2020-02-18T19:01:55Z,2020-02-18T20:15:15Z,"#### Code Sample, a copy-pastable example if possible
Converting object to 'string' succeeds once, but not twice.
```python
In [635]: pd.Series([name], dtype='object')
Out[643]: 
0    OSCAR KANG’ORO
dtype: object

In [644]: pd.Series([name], dtype='object').convert_dtypes()
Out[644]: 
0    OSCAR KANG’ORO
dtype: string

In [645]: pd.Series([name], dtype='object').convert_dtypes().convert_dtypes()
---- (full traceback above) ----
File /home/torstein/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py, line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
File <ipython-input-645-6c1facea723b>, line 1, in <module>
    pd.Series([name], dtype='object').convert_dtypes().convert_dtypes()
File /home/torstein/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py, line 6046, in convert_dtypes
    infer_objects, convert_string, convert_integer, convert_boolean
File /home/torstein/anaconda3/lib/python3.7/site-packages/pandas/core/series.py, line 4393, in _convert_dtypes
    result = input_series.astype(inferred_dtype)
File /home/torstein/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py, line 5698, in astype
    new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors)
File /home/torstein/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py, line 582, in astype
    return self.apply(""astype"", dtype=dtype, copy=copy, errors=errors)
File /home/torstein/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py, line 442, in apply
    applied = getattr(b, f)(**kwargs)
File /home/torstein/anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py, line 607, in astype
    values = self.values.astype(dtype)
File /home/torstein/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/string_.py, line 260, in astype
    return super().astype(dtype, copy)
File /home/torstein/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/base.py, line 443, in astype
    return np.array(self, dtype=dtype, copy=copy)
File /home/torstein/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/numpy_.py, line 184, in __array__
    return np.asarray(self._ndarray, dtype=dtype)
File /home/torstein/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py, line 85, in asarray
    return array(a, dtype, copy=False, order=order)

UnicodeEncodeError: 'ascii' codec can't encode character '\u2019' in position 10: ordinal not in range(128)

```


<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.18-100.fc30.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : nb_NO.UTF-8
LOCALE           : nb_NO.UTF-8



pandas           : 1.0.1
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.6.0.post20191030
Cython           : 0.29.13
pytest           : 5.2.2
hypothesis       : None
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.2
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 2.2.3
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.2.2
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.10
tables           : 3.5.2
tabulate         : 0.8.5
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.2
numba            : 0.46.0

</details>
"
337070837,21684,ENH: set_index for Series,h-vetinari,open,2018-06-29T17:01:17Z,2020-02-18T20:41:08Z,"I was surprised to find that `DataFrame.set_index` has no analogue for `Series`. This is especially surprising because I remember reading in several comments here that it is strongly preferred (i.a. by @jreback) that users do not set attributes like `.name` or `.index` directly -- but currently, the only options to change the index on a `Series` is either that, or reconstructing with `pd.Series(s, index=desired_index)`.

This is relevant in many scenarios, but in case someone would like a more concrete example -- I'm currently working on having `.duplicated` be able to return an inverse for `DataFrame / Series / Index`, see #21645. This inverse needs to link two different indexes -- the one of the original object, and the index of the deduplicated one. To reconstruct from the unique values, one needs exactly such a `.set_index` operation, because `.reindex` in itself cannot read a set of indexes and assign them to a different set of indexes in one go (xref #21685).

```
s = pd.Series(['a', 'b', 'a', 'c', 'a', 'b'])
isdup, inv = s.duplicated(keep='last', return_inverse=True)
isdup
# 0     True
# 1     True
# 2     True
# 3    False
# 4    False
# 5    False
# dtype: bool

inv
# 0    4
# 1    5
# 2    4
# 3    3
# 4    4
# 5    5
# dtype: int64

unique = s.loc[~isdup]
unique
# 3    c
# 4    a
# 5    b
# dtype: object

reconstruct = unique.reindex(inv)
reconstruct 
# 4    a
# 5    b
# 4    a
# 3    c
# 4    a
# 5    b
# dtype: object
```

This object obviously still has the wrong index to be equal to the original. For `DataFrames`, the reconstruction would work as `unique.reindex(inv.values).set_index(inv.index)`, and consequently, this should be available for `Series` as well:

**Desired**:
```
reconstruct = unique.reindex(inv.values).set_index(inv.index)
reconstruct
# 0    a
# 1    b
# 2    a
# 3    c
# 4    a
# 5    b
# dtype: object
```"
559145485,31623,REGR: AssertionError when subtracting Timestamp-valued DataFrames with non-indentical column index,DomKennedy,closed,2020-02-03T15:10:53Z,2020-02-19T00:26:28Z,"```python
import pandas as pd

df = pd.DataFrame(
    {
        ""foo"": [pd.Timestamp(""2019""), pd.Timestamp(""2020"")],
        ""bar"": [pd.Timestamp(""2018""), pd.Timestamp(""2021"")],
    }
)

df2 = df[[""foo""]]

print(df - df2)
```
#### Problem description

The above snippet raises the following exception:

```
Traceback (most recent call last):
  File "".venv/lib/python3.6/site-packages/pandas/core/ops/array_ops.py"", line 149, in na_arithmetic_op
    result = expressions.evaluate(op, str_rep, left, right)
  File "".v
env/lib/python3.6/site-packages/pandas/core/computation/expressions.py"", line 208, in evaluate
    return _evaluate(op, op_str, a, b)
  File "".venv/lib/python3.6/site-packages/pandas/core/computation/expressions.py"", line 70, in _evaluate_standard
    return op(a, b)
  File "".venv/lib/python3.6/site-packages/pandas/core/ops/common.py"", line 64, in new_method
    return method(self, other)
  File "".venv/lib/python3.6/site-packages/pandas/core/ops/__init__.py"", line 500, in wrapper
    result = arithmetic_op(lvalues, rvalues, op, str_rep)
  File "".venv/lib/python3.6/site-packages/pandas/core/ops/array_ops.py"", line 192, in arithmetic_op
    res_values = dispatch_to_extension_op(op, lvalues, rvalues)
  File "".venv/lib/python3.6/site-packages/pandas/core/ops/dispatch.py"", line 125, in dispatch_to_extension_op
    res_values = op(left, right)
  File "".venv/lib/python3.6/site-packages/pandas/core/arrays/datetimelike.py"", line 1390, in __rsub__
    f""cannot subtract {type(self).__name__} from {type(other).__name__}""
TypeError: cannot subtract DatetimeArray from ndarray

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""pandas_bug.py"", line 36, in <module>
    print(df2 - df)
  File "".venv/lib/python3.6/site-packages/pandas/core/ops/__init__.py"", line 703, in f
    new_data = left._combine_frame(right, pass_op, fill_value)
  File "".venv/lib/python3.6/site-packages/pandas/core/frame.py"", line 5297, in _combine_frame
    new_data = ops.dispatch_to_series(self, other, _arith_op)
  File "".venv/lib/python3.6/site-packages/pandas/core/ops/__init__.py"", line 416, in dispatch_to_series
    new_data = expressions.evaluate(column_op, str_rep, left, right)
  File "".venv/lib/python3.6/site-packages/pandas/core/computation/expressions.py"", line 208, in evaluate
    return _evaluate(op, op_str, a, b)
  File "".venv/lib/python3.6/site-packages/pandas/core/computation/expressions.py"", line 70, in _evaluate_standard
    return op(a, b)
  File "".venv/lib/python3.6/site-packages/pandas/core/ops/__init__.py"", line 385, in column_op
    return {i: func(a.iloc[:, i], b.iloc[:, i]) for i in range(len(a.columns))}
  File "".venv/lib/python3.6/site-packages/pandas/core/ops/__init__.py"", line 385, in <dictcomp>
    return {i: func(a.iloc[:, i], b.iloc[:, i]) for i in range(len(a.columns))}
  File "".venv/lib/python3.6/site-packages/pandas/core/ops/array_ops.py"", line 121, in na_op
    return na_arithmetic_op(x, y, op, str_rep)
  File "".venv/lib/python3.6/site-packages/pandas/core/ops/array_ops.py"", line 151, in na_arithmetic_op
    result = masked_arith_op(left, right, op)
  File "".venv/lib/python3.6/site-packages/pandas/core/ops/array_ops.py"", line 75, in masked_arith_op
    assert isinstance(x, np.ndarray), type(x)
```
This is a 1.0.0 regression; in 0.25.3, the operation succeeds and the unmatched `bar` column is filled with `NaN` in the output.

The same error occurs with:
* Any combination of incompatible columns (strict subset, strict superset, overlapping, disjoint)
* Calling the `subtract` method instead of using the subtraction operator
* Timezone-aware `Timestamp`s as well as timezone-naive

It does *not* seem to occur with:
* Mismatches on the row index; transposing the dataframes in the above example prevents the errors occuring.
* `pd.Series` objects with mismatched indexes (e.g. calling the above on the first row of each dataframe works fine)
* Other dtypes; `bool`, `float`, and `int` seem to work fine. Similarly, if the dataframes are explicitly cast to dtype `object`, the operation succeeds.

#### Expected Output

```
   bar    foo
0  NaN 0 days
1  NaN 0 days
```

#### Output of ``pd.show_versions()``

<details>
```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-74-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.6.0
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```

</details>
"
560076609,31679,"REGR: fix op(frame, frame2) with reindex",jbrockmendel,closed,2020-02-05T00:55:52Z,2020-02-19T10:00:44Z,"- [x] closes #31623
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @TomAugspurger this is pretty ugly, and I'm not sure how well it will behave if either frame has MultiIndex colums.

On the plus side, it could improve perf in the many-columns-but-small-intersection case.

The ugliness might be improved by moving this check to before the _align_method_FRAME call"
563285712,31880,MultiLevelIndex sort_index already does support ascending as a list of booleans,petrbel,closed,2020-02-11T15:30:22Z,2020-02-19T01:30:09Z,"#### Code Sample, a copy-pastable example if possible

```python
df.sort_index(level=[1, 0], ascending=[True, False])
```
#### Problem description

The [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_index.html) states that `pandas.DataFrame.sort_index` parameter `accepts` is a boolean. However, in the case of multi level index, it can (already) be a list of booleans indicating which level should ascending and which descending.

Both [lexsort_indexer](https://github.com/pandas-dev/pandas/blob/master/pandas/core/sorting.py#L192) and [sort_levels](https://github.com/pandas-dev/pandas/blob/4181042aad88224c9c8be8e7c7a8525090c8aa07/pandas/core/frame.py#L5031) support this behavior.

I suggest updating both tests and documentation.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.0.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.14-arch1-1
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF8
LOCALE           : en_US.UTF-8

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0
Cython           : 0.29.14
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
563497787,31898,DOC: Update sort_index docs,dsaxton,closed,2020-02-11T21:44:43Z,2020-02-19T01:36:21Z,- [x] closes #31880
562210087,31828,DOC: Mention black and PEP8 in pandas style guide,dsaxton,closed,2020-02-09T17:25:09Z,2020-02-19T01:46:26Z,"`black` and PEP8 are pretty important elements of code style in `pandas` but they aren't mentioned in the style guide, so it may make sense to include them.

https://pandas.pydata.org/pandas-docs/stable/development/code_style.html"
565950582,32043,DOC: Mention black and PEP8 in pandas style guide,raisadz,closed,2020-02-16T18:51:03Z,2020-02-19T01:46:33Z,"- [ ] closes #31828
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
551188293,31097,DOC: Replace ggpy with plotnine in ecosystem,hvardhan20,closed,2020-01-17T04:14:18Z,2020-02-19T03:52:49Z,"- [ ] closes #31087 
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Changed ggpy reference to plotnine.
"
563842291,31916, EHN: fix unsigned int type problem of the result diff(),zzapzzap,closed,2020-02-12T08:45:37Z,2020-02-19T07:05:17Z,"An unexpected output type(float64) occurs to diff() when using uint
type. So fix it by restoring original dtype after storing it

Fixes #28909

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry


What do you think about this problem with the code? @jreback "
566423023,32062,Backport PR #31734 on branch 1.0.x (BUG: list-like to_replace on Categorical.replace is ignored or crash),meeseeksmachine,closed,2020-02-17T17:00:05Z,2020-02-19T07:32:30Z,Backport PR #31734: BUG: list-like to_replace on Categorical.replace is ignored or crash
562316257,31841,BUG: Fix rolling.corr with time frequency,mroeschke,closed,2020-02-10T04:25:10Z,2020-02-19T07:40:50Z,"- [x] closes #31789
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
567372781,32093,Backport PR #31842 on branch 1.0.x (BUG: Fix raw parameter not being respected in groupby.rolling.apply),meeseeksmachine,closed,2020-02-19T07:35:30Z,2020-02-19T08:15:47Z,Backport PR #31842: BUG: Fix raw parameter not being respected in groupby.rolling.apply
565892811,32041,REGR: show_versions,simonjayhawkins,closed,2020-02-16T11:49:56Z,2020-02-19T09:39:44Z,"regression in #31660 (no need to backport)

master
```
INSTALLED VERSIONS
------------------
{k:<17}: {stat}
{k:<17}: {stat}
{k:<17}: {stat}
...
```

this PR
```
INSTALLED VERSIONS
------------------
commit           : b11e0647d42a27c279f3c46d5ce26d79bb5f5dec
python           : 3.7.4.final.0
python-bits      : 64
...
```
"
567374997,32094,Backport PR #31841: BUG: Fix rolling.corr with time frequency,jorisvandenbossche,closed,2020-02-19T07:40:22Z,2020-02-19T10:01:25Z,https://github.com/pandas-dev/pandas/pull/31841
564884063,31960,CLN: remove blocking return,MarcoGorelli,closed,2020-02-13T18:53:29Z,2020-02-19T10:49:57Z,just noticed a `return` statement in the middle of a test that prevents it from being executed. This was added in ##23752
567431729,32095,Unpickled StringArray with pd.NA raises ValueError,bjonen,closed,2020-02-19T09:29:38Z,2020-02-19T13:22:42Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import pickle
df = pd.Series(['abc'] +  [pd.NA]*60, dtype='string')
serialized = pickle.dumps(df)
unserialized = pickle.loads(serialized)
print(unserialized)
```
#### Problem description
The unserialized df cannot be printed. Fails with
ValueError: StringArray requires a sequence of strings or pandas.NA

Note that things work fine with less than 60 pd.NA
```python
import pandas as pd
import pickle
df = pd.Series(['abc'] +  [pd.NA]*59, dtype='string')
serialized = pickle.dumps(df)
unserialized = pickle.loads(serialized)
print(unserialized)
```


#### Expected Output
Print out simple df 


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1062.9.1.el7.x86_64
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : C
LANG             : en_US.UTF-8A
LOCALE           : None.None

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200209
Cython           : 0.29.15
pytest           : 5.3.5
hypothesis       : None
sphinx           : 2.4.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : 0.15.0
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.48.0

</details>
"
560563037,31716,fix mypy errors in pandas/tests/arithmetic/test_datetime64.py,SaturnFromTitan,closed,2020-02-05T18:52:59Z,2020-02-19T14:09:38Z,"Part of #28926
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
567259665,32088,"""Backport PR #31679 on branch 1.0.x""",jbrockmendel,closed,2020-02-19T01:33:22Z,2020-02-19T15:55:41Z,#31679
567706045,32114,Installation of pandas fails due to compilation error,mhwombat,closed,2020-02-19T17:09:13Z,2020-02-19T17:26:11Z,"#### Code Sample, a copy-pastable example if possible

```
#  python --version
Python 3.9.0a3
# pip3 install pandas --log amy.log
```
results in the error shown below. (The full installation log: [amy.log](https://gist.github.com/mhwombat/a10d22dc166035201e0e6981c53db33c))
```
numpy/random/mtrand/mtrand.c:43635:34: error: 'PyTypeObject' {aka 'struct _typeobject'} has no member named 'tp_print'; did you mean 'tp_dict'?
```

#### Problem description

The installation fails. I have searched the issues for ""tp_print"" and ""mtrand.c"", but I haven't found any mention of this particular error.

#### Expected Output

The installation should succeed.

#### Output of ``pd.show_versions()``

Not applicable because pandas won't install, but here are the packages I do have installed.

```
#  pip3 list
Package    Version
---------- -------
numpy      1.18.1 
pip        20.0.2 
scipy      1.3.2  
setuptools 45.1.0 
wheel      0.34.2
```

Note: I am doing all this from a docker image using alpine linux, but I don't think that's the cause of this problem."
562334880,31842,BUG: Fix raw parameter not being respected in groupby.rolling.apply,mroeschke,closed,2020-02-10T05:31:20Z,2020-02-19T17:36:05Z,"- [x] closes #31754
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
567696089,32113,ambiguous pandas filtering doesn't give warnings,mdf-github,closed,2020-02-19T16:53:57Z,2020-02-19T18:17:26Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here

df = pd.DataFrame({'a': [True,True,False,False], 'b': [1,0,1,0]})

fltr1 = (
         (df['a']) &
         df['b'] == 0
        )

fltr2 = (
         (df['a']) &
         (df['b'] == 0)
        )

# fltr1 and fltr2 give very different results
```
#### Problem description


I finally found the bug in my code, but was surprised pandas didn't raise even a warning. Wouldn't it be better to issue a warning or an error about ambiguous combinations like this (which like mine had missing parentheses)?

#### Expected Output

A warning that the filter is ambiguous.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-18362-Microsoft
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.13
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
285599206,19047,CLN: ASV sparse,mroeschke,closed,2018-01-03T04:54:05Z,2020-02-19T18:46:25Z,"Simplified and created a top level function that made sparse arrays (which was repeated throughout the file). Otherwise, simplified benchmarks with `param` where possible. 

```
$ asv dev -b ^sparse
· Discovering benchmarks
· Running 15 total benchmarks (1 commits * 1 environments * 15 benchmarks)
[  0.00%] ·· Building for existing-py_home_matt_anaconda_envs_pandas_dev_bin_python
[  0.00%] ·· Benchmarking existing-py_home_matt_anaconda_envs_pandas_dev_bin_python
[  6.67%] ··· Running sparse.Arithmetic.time_add                             ok
[  6.67%] ···· 
               ================== ======== ========
               --                     fill_value   
               ------------------ -----------------
                dense_proportion     0       nan   
               ================== ======== ========
                      0.1          85.6ms   68.0ms 
                      0.01         8.46ms   67.5ms 
               ================== ======== ========

[ 13.33%] ··· Running sparse.Arithmetic.time_divide                          ok
[ 13.33%] ···· 
               ================== ======== ========
               --                     fill_value   
               ------------------ -----------------
                dense_proportion     0       nan   
               ================== ======== ========
                      0.1          82.7ms   67.6ms 
                      0.01         8.94ms   67.6ms 
               ================== ======== ========

[ 20.00%] ··· Running sparse.Arithmetic.time_intersect                       ok
[ 20.00%] ···· 
               ================== ======== =======
               --                    fill_value   
               ------------------ ----------------
                dense_proportion     0       nan  
               ================== ======== =======
                      0.1          5.72ms   344ms 
                      0.01         356μs    343ms 
               ================== ======== =======

[ 26.67%] ··· Running sparse.Arithmetic.time_make_union                      ok
[ 26.67%] ···· 
               ================== ======== =======
               --                    fill_value   
               ------------------ ----------------
                dense_proportion     0       nan  
               ================== ======== =======
                      0.1          79.4ms   715ms 
                      0.01         8.63ms   742ms 
               ================== ======== =======

[ 33.33%] ··· Running sparse.ArithmeticBlock.time_addition                   ok
[ 33.33%] ···· 
               ============ ========
                fill_value          
               ------------ --------
                   nan       8.20ms 
                    0        8.03ms 
               ============ ========

[ 40.00%] ··· Running sparse.ArithmeticBlock.time_division                   ok
[ 40.00%] ···· 
               ============ ========
                fill_value          
               ------------ --------
                   nan       8.39ms 
                    0        8.04ms 
               ============ ========

[ 46.67%] ··· Running sparse.ArithmeticBlock.time_intersect                  ok
[ 46.67%] ···· 
               ============ ========
                fill_value          
               ------------ --------
                   nan       3.51ms 
                    0        3.69ms 
               ============ ========

[ 53.33%] ··· Running sparse.ArithmeticBlock.time_make_union                 ok
[ 53.33%] ···· 
               ============ ========
                fill_value          
               ------------ --------
                   nan       7.86ms 
                    0        8.19ms 
               ============ ========

[ 60.00%] ··· Running sparse.FromCoo.time_sparse_series_from_coo         3.30ms
[ 66.67%] ··· Running ...se.SparseArrayConstructor.time_sparse_array         ok
[ 66.67%] ···· 
               ================== ============ ======================== ========
                dense_proportion   fill_value           dtype                   
               ------------------ ------------ ------------------------ --------
                      0.1              0         <type 'numpy.int64'>    46.7ms 
                      0.1              0        <type 'numpy.float64'>   47.1ms 
                      0.1              0           <type 'object'>       104ms  
                      0.1             nan        <type 'numpy.int64'>    364ms  
                      0.1             nan       <type 'numpy.float64'>   47.7ms 
                      0.1             nan          <type 'object'>       116ms  
                      0.01             0         <type 'numpy.int64'>    8.80ms 
                      0.01             0        <type 'numpy.float64'>   8.71ms 
                      0.01             0           <type 'object'>       67.5ms 
                      0.01            nan        <type 'numpy.int64'>    380ms  
                      0.01            nan       <type 'numpy.float64'>   9.57ms 
                      0.01            nan          <type 'object'>       64.7ms 
               ================== ============ ======================== ========

[ 73.33%] ··· Running ...SparseDataFrameConstructor.time_constructor      6.39s
[ 80.00%] ··· Running ...e.SparseDataFrameConstructor.time_from_dict      245ms
[ 86.67%] ··· Running ....SparseDataFrameConstructor.time_from_scipy      563ms
[ 93.33%] ··· Running ...se.SparseSeriesToFrame.time_series_to_frame      280ms
[100.00%] ··· Running sparse.ToCoo.time_sparse_series_to_coo             43.9ms
```"
562669816,31852,add messages to tests,raisadz,closed,2020-02-10T16:17:21Z,2020-02-19T23:19:56Z,"- [ ] xref #30999
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
566751358,32071,CLN: Remove unused script find_commits_touching_func.py,datapythonista,closed,2020-02-18T09:17:07Z,2020-02-20T01:04:20Z,"- [X] xref #31039

I've been trying the script `find_commits_touching_func.py` and doesn't seem to be working. Tried for many existing functions, and never get any result. I guess it doesn't work with newer git versions, or with the latest dependencies. And if nobody realized of this before, I assume nobody is using this script (and in any case, a separate project would be more appropriate for it, since I don't think it's pandas specific).

The script was being refactored in #32044, that's why I was having a look.
"
567878223,32122,CLN: Replace old string formatting syntax with f-strings for scripts/validate_docstrings.py,raisadz,closed,2020-02-19T22:08:21Z,2020-02-20T01:06:05Z,"
- [ ] xref #29547
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
558524230,31534,CI: Reverted changes related to 'jedi' warnings,ShaharNaveh,closed,2020-02-01T11:04:09Z,2020-02-20T03:17:09Z,"- [x] closes #31407
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
566042557,32048,Pandas 1.0.1 gives 1953 TypeError: Cannot use .str.encode with values of inferred dtype 'bytes'.,bwanaaa,closed,2020-02-17T04:02:13Z,2020-02-20T05:02:37Z,"#### Code Sample, a copy-pastable example if possible
This code is used in jupyter lab using python 3.7 and pandas 1.0.1
It accesses a text file of words delimited by carriage returns
```
import base64
import numpy as np
import pandas as pd
words = pd.read_table(""sampleTEXT.txt"",names=['word'],header=None)
words.head()
---   word
--0  difference
--1  where
--2  mc
--3  is
--4  the
words['word_encoded'] = words.word.str.encode('utf-8', 'strict').str.encode('base64')

```


#### Problem description
the following error appears:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-4-0f7040aa4d4e> in <module>
----> 1 words['word_encoded'] = words.word.str.encode('utf-8', 'strict').str.encode('base64')

~/miniconda3/envs/p37cu10.2PyTo/lib/python3.7/site-packages/pandas/core/strings.py in wrapper(self, *args, **kwargs)
   1949                     f""inferred dtype '{self._inferred_dtype}'.""
   1950                 )
-> 1951                 raise TypeError(msg)
   1952             return func(self, *args, **kwargs)
   1953 

TypeError: Cannot use .str.encode with values of inferred dtype 'bytes'.
```

#### Expected Output
If I downgrade pandas to '0.24.2' then I get the desired output
```
       word                  word_encoded
-----------------------------------------------------
0      difference           b'ZGlmZmVyZW5jZQ==\n'
1      where                 b'd2hlcmU=\n'
2      mc                      b'bWM=\n'
3      is                        b'aXM=\n'
4      the                      b'dGhl\n'
```
I discussed this problem on stackoverflow here

https://stackoverflow.com/questions/60254107/how-to-base64-encode-and-decode-a-column-in-python-pandas/60254651?noredirect=1#comment106581486_6025465

I read here
https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.25.0.html
that The .str-accessor performs stricter type checks. This was new since v.25
But I still dont know how to resolve this issue I am having with 'inferred bytes'


#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

the working version

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.7.final.0
python-bits: 64
OS: Linux
OS-release: 5.3.0-28-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: None
pip: 20.0.2
setuptools: 45.2.0.post20200209
Cython: None
numpy: 1.18.1
scipy: None
pyarrow: None
xarray: None
IPython: 7.12.0
sphinx: None
patsy: None
dateutil: 2.8.1
pytz: 2019.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.1.3
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.11.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None


the crashing version

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-28-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200209
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
567500306,32097,TYP: check_untyped_defs core.arrays.categorical,simonjayhawkins,closed,2020-02-19T11:22:04Z,2020-02-20T09:38:34Z,"pandas\core\arrays\categorical.py:347: error: Incompatible types in assignment (expression has type ""None"", variable has type ""str"")
pandas\core\arrays\categorical.py:1550: error: ""argsort"" of ""ExtensionArray"" gets multiple values for keyword argument ""ascending""
pandas\core\arrays\categorical.py:1550: error: ""argsort"" of ""ExtensionArray"" gets multiple values for keyword argument ""kind"""
567538460,32101,TYP: check_untyped_defs core.tools.datetimes,simonjayhawkins,closed,2020-02-19T12:33:10Z,2020-02-20T09:40:16Z,"pandas\core\tools\datetimes.py:299: error: Item ""DatetimeIndex"" of ""Union[DatetimeArray, DatetimeIndex]"" has no attribute ""tz_convert""
pandas\core\tools\datetimes.py:310: error: Item ""DatetimeIndex"" of ""Union[DatetimeArray, DatetimeIndex]"" has no attribute ""tz_localize""
pandas\core\tools\datetimes.py:829: error: Incompatible types in assignment (expression has type ""str"", variable has type ""List[str]"")
pandas\core\tools\datetimes.py:838: error: Incompatible types in assignment (expression has type ""str"", variable has type ""List[Any]"")
pandas\core\tools\datetimes.py:1010: error: Argument 1 to ""append"" of ""list"" has incompatible type ""None""; expected ""time""
pandas\core\tools\datetimes.py:1035: error: Argument 1 to ""append"" of ""list"" has incompatible type ""None""; expected ""time"""
567506030,32098,TYP: check_untyped_defs core.arrays.interval,simonjayhawkins,closed,2020-02-19T11:32:43Z,2020-02-20T11:32:37Z,"pandas\core\arrays\interval.py:438: error: Need type annotation for 'left' (hint: ""left: List[<type>] = ..."")
pandas\core\arrays\interval.py:438: error: Need type annotation for 'right' (hint: ""right: List[<type>] = ..."")"
565460706,31981,"Segfault in version 1.0.1, read_parquet after creating a clickhouse odbc connection",mvcalder-xbk,closed,2020-02-14T17:28:30Z,2020-02-20T13:38:04Z,"We started getting segfaults in upgraded versions of pandas and tracked it down to an interaction with making odbc connections. The specific odbc driver is for Clickhouse built in Dec19 from:

```
https://github.com/ClickHouse/clickhouse-odbc
commit 08b252b93fc771fab607fb973443543479b3d972
```

The core dump is reproducable in the example below: 

#### Code Sample, a copy-pastable example if possible

```python
import pyodbc
import pandas as pd

con_str = f""Driver=libclickhouseodbc.so;url=http://clickhouse/query;timeout=600""
with pyodbc.connect(con_str, autocommit=True) as con:
    pass

df = pd.DataFrame({'A': [1,1,1], 'B': ['a', 'b', 'c']})
df.to_parquet('/tmp/foo.pq')
# This line core dumps:
pd.read_parquet('/tmp/foo.pq')
```
#### Problem description

In the code above, creating the clickhouse odbc connection (not using it) leads to a segfault when reading the parquet file:

```
terminate called after throwing an instance of 'std::bad_cast'
  what():  std::bad_cast
Aborted (core dumped)
```

This does not happen in pandas 0.25.3 but does in pandas 1.0.1. 

#### Expected Output

No core dump.

#### Odbc driver dependencies

```xbk@499e30e4f63f:$ ldd libclickhouseodbc.so
        linux-vdso.so.1 (0x00007ffe02bee000)
        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f246c269000)
        librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f246c061000)
        libltdl.so.7 => /usr/lib/x86_64-linux-gnu/libltdl.so.7 (0x00007f246be57000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f246ba66000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f246cd89000)
        libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f246b862000)
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1062.4.3.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 9.0.1
setuptools       : 45.2.0
Cython           : 0.29.14
pytest           : 5.3.2
hypothesis       : 5.1.2
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 0.999999999
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.2
pandas_gbq       : None
pyarrow          : 0.15.1.dev539+g8cf0c8e0a
pytables         : None
pytest           : 5.3.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.11
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
565769296,32027,CLN: Clean groupby/test_function.py,dsaxton,closed,2020-02-15T15:56:52Z,2020-02-20T14:21:20Z,Some small cleanups (removing unnecessary for loops / adding parameterization)
567671720,32111,REF: misplaced Series.combine_first tests,jbrockmendel,closed,2020-02-19T16:18:36Z,2020-02-20T15:00:13Z,"
"
567549320,32102,TST: Fix bare pytest.raises in test_parsing.py,mabroor,closed,2020-02-19T12:52:56Z,2020-02-20T16:39:53Z,"- [x] ref #30999 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Fixes bare pytest.raises issue for `test_parsing.py`."
534968792,30159,Json Normalize failed,darbelley,closed,2019-12-09T14:29:52Z,2020-02-20T17:08:23Z,"#### Code Sample, a copy-pastable example if possible

{
		""data"": [
			{
				""attributes"": {
					....
					}
				}
		],
		""included"": [
			{
				""attributes"": {					
			}
		]
	},
	{
		""data"": [
			{
				""attributes"": {
					....
					}
				}
		],
		""included"": [
			{
				""attributes"": {					
			}
		]
	},
	....
	.......
	..........
	{
		""data"": [
			{
				""attributes"": {
					....
					}
				}
		]
	}



OrdersResponsesIncluded = json_normalize(data=my_list_loaded, record_path=['included'] , errors='ignore')

	
#### Problem description

Hi All,

since some week I am using Pandas to normalize data extracted by api responses
I would ask your help about a particular case that does not fit with the method json_normalize of pandas.
Basically I have a structure of my json like above (please note that last object contain ONLY the ""data"" array and not the ""included"" array.
Within detail there is the error received.

#### Expected Output

json normalized as usual about ""included"" 


<details>

---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-54-e4812a026c1f> in <module>
      1 # farlo con quanto ricaricato
      2 
----> 3 OrdersResponsesIncluded = json_normalize(data=my_list_loaded, record_path=['included'] , errors='ignore')

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\io\json\_normalize.py in json_normalize(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)
    323                 records.extend(recs)
    324 
--> 325     _recursive_extract(data, record_path, {}, level=0)
    326 
    327     result = DataFrame(records)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\io\json\_normalize.py in _recursive_extract(data, path, seen_meta, level)
    295         else:
    296             for obj in data:
--> 297                 recs = _pull_field(obj, path[0])
    298                 recs = [
    299                     nested_to_record(r, sep=sep, max_level=max_level)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\io\json\_normalize.py in _pull_field(js, spec)
    244                 result = result[field]
    245         else:
--> 246             result = result[spec]
    247 
    248         return result

KeyError: 'included'

</details>
"
563412073,31887,CLN: simplify _setitem_with_indexer,jbrockmendel,closed,2020-02-11T19:02:55Z,2020-02-20T23:27:36Z,"The MultiIndex check it is doing is unnecessary for positional indexing.

Other things being cleaned up here appear to be remnants from Panel"
565829923,32034,CLN: 29547 replace old string formatting,3vts,closed,2020-02-16T00:34:51Z,2020-02-21T01:29:53Z,"I splitted PR #31844 in batches, this is the **last**
For this PR I ran the command `grep -l -R -e '%s' -e '%d' -e '\.format(' --include=*.{py,pyx} pandas/` and checked all the files that were returned for `.format(` and changed the old string format for the corresponding `fstrings` to attempt a full clean of, [#29547](https://github.com/pandas-dev/pandas/issues/29547). I may have missed something so is a good idea to double check just in case

- [ x  ] tests added / passed
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ x ] Ref #29547
"
565800349,32032,CLN: 29547 replace old string formatting 8,3vts,closed,2020-02-15T19:52:37Z,2020-02-21T01:29:55Z,"I splitted PR #31844 in batches, this is the eighth
For this PR I ran the command `grep -l -R -e '%s' -e '%d' -e '\.format(' --include=*.{py,pyx} pandas/` and checked all the files that were returned for `.format(` and changed the old string format for the corresponding `fstrings` to attempt a full clean of, [#29547](https://github.com/pandas-dev/pandas/issues/29547). I may have missed something so is a good idea to double check just in case

- [ x  ] tests added / passed
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
565522964,31986,CLN: 29547 replace old string formatting 7,3vts,closed,2020-02-14T19:45:33Z,2020-02-21T01:29:56Z,"I splitted PR #31844 in batches, this is the seventh
For this PR I ran the command `grep -l -R -e '%s' -e '%d' -e '\.format(' --include=*.{py,pyx} pandas/` and checked all the files that were returned for `.format(` and changed the old string format for the corresponding `fstrings` to attempt a full clean of, [#29547](https://github.com/pandas-dev/pandas/issues/29547). I may have missed something so is a good idea to double check just in case

- [ x  ] tests added / passed
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
565046702,31967,CLN: 29547 replace old string formatting 5,3vts,closed,2020-02-14T01:22:43Z,2020-02-21T01:29:58Z,"I splitted PR #31844 in batches, this is the fifth
For this PR I ran the command `grep -l -R -e '%s' -e '%d' -e '\.format(' --include=*.{py,pyx} pandas/` and checked all the files that were returned for `.format(` and changed the old string format for the corresponding `fstrings` to attempt a full clean of, [#29547](https://github.com/pandas-dev/pandas/issues/29547). I may have missed something so is a good idea to double check just in case

- [ x  ] tests added / passed
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
564925523,31963,CLN: 29547 replace old string formatting 4,3vts,closed,2020-02-13T20:15:24Z,2020-02-21T01:30:00Z,"I splitted PR #31844 in batches, this is the fourth
For this PR I ran the command `grep -l -R -e '%s' -e '%d' -e '\.format(' --include=*.{py,pyx} pandas/` and checked all the files that were returned for `.format(` and changed the old string format for the corresponding `fstrings` to attempt a full clean of, [#29547](https://github.com/pandas-dev/pandas/issues/29547). I may have missed something so is a good idea to double check just in case

- [ x  ] tests added / passed
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
564420745,31945,CLN: 29547 replace old string formatting 3,3vts,closed,2020-02-13T03:45:56Z,2020-02-21T01:30:01Z,"I splitted PR #31844 in batches, this is the third
For this PR I ran the command `grep -l -R -e '%s' -e '%d' -e '\.format(' --include=*.{py,pyx} pandas/` and checked all the files that were returned for `.format(` and changed the old string format for the corresponding `fstrings` to attempt a full clean of, [#29547](https://github.com/pandas-dev/pandas/issues/29547). I may have missed something so is a good idea to double check just in case

- [ x  ] tests added / passed
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
564165638,31933,CLN: 29547 replace old string formatting 2,3vts,closed,2020-02-12T17:51:56Z,2020-02-21T01:30:01Z,"I splitted PR #31844 in batches, this is the second
For this PR I ran the command `grep -l -R -e '%s' -e '%d' -e '\.format(' --include=*.{py,pyx} pandas/` and checked all the files that were returned for `.format(` and changed the old string format for the corresponding `fstrings` to attempt a full clean of, [#29547](https://github.com/pandas-dev/pandas/issues/29547). I may have missed something so is a good idea to double check just in case

- [ x  ] tests added / passed
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
563752926,31914,CLN: 29547 replace old string formatting 1,3vts,closed,2020-02-12T04:56:23Z,2020-02-21T01:30:03Z,"I splitted PR #31844 in batches, this is the first one
For this PR I ran the command `grep -l -R -e '%s' -e '%d' -e '\.format(' --include=*.{py,pyx} pandas/` and checked all the files that were returned for `.format(` and changed the old string format for the corresponding `fstrings` to attempt a full clean of, [#29547](https://github.com/pandas-dev/pandas/issues/29547). I may have missed something so is a good idea to double check just in case

- [ x  ] tests added / passed
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
565456698,31980,CLN: 29547 replace old string formatting 6,3vts,closed,2020-02-14T17:19:28Z,2020-02-21T01:30:05Z,"I splitted PR #31844 in batches, this is the sixth
For this PR I ran the command `grep -l -R -e '%s' -e '%d' -e '\.format(' --include=*.{py,pyx} pandas/` and checked all the files that were returned for `.format(` and changed the old string format for the corresponding `fstrings` to attempt a full clean of, [#29547](https://github.com/pandas-dev/pandas/issues/29547). I may have missed something so is a good idea to double check just in case

- [ x  ] tests added / passed
- [ x ] passes `black pandas`
- [ x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
568577454,32137,CLN: remove unused tm.isiterable,jbrockmendel,closed,2020-02-20T21:16:20Z,2020-02-21T02:15:08Z,
558643671,31560,Add TODOs Badge to the README,patrickdevivo,closed,2020-02-02T05:02:48Z,2020-02-21T03:29:13Z,"Hi there! I wanted to propose adding the following badge to the README to indicate how many `// TODO` comments are in this codebase:

[![TODOs](https://badgen.net/https/api.tickgit.com/badgen/github.com/pandas-dev/pandas)](https://www.tickgit.com/browse?repo=github.com/pandas-dev/pandas)

The badge links to `tickgit.com` which is a free service that indexes and displays TODO comments in public github repos. It can help surface latent work and be a way for new contributors to find areas of code to improve.

The markdown is:

```
[![TODOs](https://badgen.net/https/api.tickgit.com/badgen/github.com/pandas-dev/pandas)](https://www.tickgit.com/browse?repo=github.com/pandas-dev/pandas)
```

Thanks for considering, feel free to close this issue if it's not appropriate or you prefer not to!"
567516010,32099,TYP: check_untyped_defs arrays.sparse.array,simonjayhawkins,closed,2020-02-19T11:51:01Z,2020-02-21T08:16:55Z,"pandas\core\arrays\sparse\array.py:807: error: Need type annotation for 'result' (hint: ""result: List[<type>] = ..."")"
568834152,32145,AttributeError: module 'hypothesis' has no attribute 'unlimited',mguttman,closed,2020-02-21T09:22:38Z,2020-02-21T13:02:19Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> pd.test()
running: pytest --skip-slow --skip-network --skip-db /usr/local/lib/python3.6/dist-packages/pandas
ImportError while loading conftest '/usr/local/lib/python3.6/dist-packages/pandas/conftest.py'.
/usr/local/lib/python3.6/dist-packages/pandas/conftest.py:25: in <module>
    timeout=hypothesis.unlimited,
E   AttributeError: module 'hypothesis' has no attribute 'unlimited'
```
#### Problem description
Running ``pd.test()`` results in the above error


#### Expected Output

#### Output of ``pd.show_versions()``

<details>
Python 3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas as pd
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.9.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-76-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 5.3.5
pip: 20.0.2
setuptools: 41.2.0
Cython: None
numpy: 1.17.2
scipy: 1.3.0
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2018.3
blosc: None
bottleneck: None
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 3.1.1
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: 4.2.1
bs4: 4.6.0
html5lib: 0.999999999
sqlalchemy: 1.3.5
pymysql: 0.9.3
psycopg2: None
jinja2: 2.10.3
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
>>> 
</details>
"
568923332,32148,CI: skip geopandas downstream test (Anaconda installation issue),jorisvandenbossche,closed,2020-02-21T12:20:08Z,2020-02-21T13:12:11Z,xref #32144
568946899,32149,Backport PR #32148 on branch 1.0.x (CI: skip geopandas downstream test (Anaconda installation issue)),meeseeksmachine,closed,2020-02-21T13:12:05Z,2020-02-21T13:47:28Z,Backport PR #32148: CI: skip geopandas downstream test (Anaconda installation issue)
565886667,32039,"""Querying a table"" within Sector HDF5 in User Guide",xushengun,closed,2020-02-16T10:57:08Z,2020-02-21T13:57:16Z,"#### Code Sample, a copy-pastable example if possible

string = ""HolyMoly'""
store.select('df','index == string')


```
#### Problem description
string = ""HolyMoly'"" do not work 

pandas 1.0.1"
567829488,32117,convert_dtypes fails with int and str,pjadzinsky,closed,2020-02-19T20:34:26Z,2020-02-21T14:48:51Z,"#### Code Sample, a copy-pastable example if possible
Thanks for an amazing package and maintaining it

```python
# Your code here
s = pd.Series([""h"", ""i"", 1])
s.convert_dtypes()
```
#### Problem description

Example raises an error, when it shouldn't. I think the case is missing from the tests

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output
series with object dtype ?

#### Output of ``pd.show_versions()``

[paste the output of ``pd.show_versions()`` here below this line]
In [6]: pd.show_versions()                                                                                                                 

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.2.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : None
tabulate         : 0.8.6
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
<details>

</details>
"
565585869,31988,Error in `read_pickle` when loading a DataFrame with MultiIndex columns from a pickle created in py27,pedroreys,closed,2020-02-14T22:23:16Z,2020-02-21T14:52:24Z,"Calling `read_pickle` to load a DataFrame with MultiIndex columns from a pickle file created in py27 throws an error saying:
> UnicodeDecodeError: 'ascii' codec can't decode byte 0xd9 in position 0: ordinal not in range(128)

This issue was introduced by #28645, more specifically by [this change](https://github.com/pandas-dev/pandas/pull/28645/files#diff-7ecbc1597b1273ca207747232d64edad):

```diff
diff --git a/pandas/io/pickle.py b/pandas/io/pickle.py
index adf0aa961..8f9bae0f7 100644
--- a/pandas/io/pickle.py
+++ b/pandas/io/pickle.py
@@ -142,18 +142,24 @@ def read_pickle(path, compression=""infer""):

     # 1) try standard library Pickle
     # 2) try pickle_compat (older pandas version) to handle subclass changes
-    # 3) try pickle_compat with latin1 encoding
+
+    excs_to_catch = (AttributeError, ImportError)
+    if PY36:
+        excs_to_catch += (ModuleNotFoundError,)

     try:
         with warnings.catch_warnings(record=True):
             # We want to silence any warnings about, e.g. moved modules.
             warnings.simplefilter(""ignore"", Warning)
             return pickle.load(f)
-    except Exception:
-        try:
-            return pc.load(f, encoding=None)
-        except Exception:
-            return pc.load(f, encoding=""latin1"")
+    except excs_to_catch:
+        # e.g.
+        #  ""No module named 'pandas.core.sparse.series'""
+        #  ""Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib""
+        return pc.load(f, encoding=None)
+    except UnicodeDecodeError:
+        # e.g. can occur for files written in py27; see GH#28645
+        return pc.load(f, encoding=""latin-1"")
     finally:
         f.close()
         for _f in fh:
```

Note how before when there was an Exception trying to load the file with the builtin `pickle.load` function, it would try to load them using `pickle_compat` with `enconding=None` and if that call also threw an Exception, it would then fallback to try to load with `encoding=""latin-1""`. With the change from #28645 the fallback to use `encoding=""latin-1""` is only in the catch block of the initial `pickle_load` call, not the second one that tries to use pickle_compat with `encoding=None`.

This become an issue for py27 pickles with MultiIndex columns after `FrozenNDArray` was removed by #29840 as `pickle.load(f)` throws an `AttributeError` for  `FrozenNDArray` and then `pc.load(f, encoding=None)` throws an `UnicodeDecodeError`.

Here is a full stack trace:
<details>

```python
In [1]: import pandas as pd
   ...: df = pd.read_pickle('test_mi_py27.pkl')
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~/anaconda3/envs/pandas-pickle/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression)
    180             warnings.simplefilter(""ignore"", Warning)
--> 181             return pickle.load(f)
    182     except excs_to_catch:

AttributeError: Can't get attribute 'FrozenNDArray' on <module 'pandas.core.indexes.frozen' from '~/anaconda3/envs/pandas-pickle/lib/python3.7/site-packages/pandas/core/indexes/frozen.py'>

During handling of the above exception, another exception occurred:

UnicodeDecodeError                        Traceback (most recent call last)
<ipython-input-1-8c3800ea96cb> in <module>
      1 import pandas as pd
----> 2 df = pd.read_pickle('test_mi_py27.pkl')

~/anaconda3/envs/pandas-pickle/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression)
    184         #  ""No module named 'pandas.core.sparse.series'""
    185         #  ""Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib""
--> 186         return pc.load(f, encoding=None)
    187     except UnicodeDecodeError:
    188         # e.g. can occur for files written in py27; see GH#28645

~/anaconda3/envs/pandas-pickle/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in load(fh, encoding, is_verbose)
    239         up.is_verbose = is_verbose
    240
--> 241         return up.load()
    242     except (ValueError, TypeError):
    243         raise

~/anaconda3/envs/pandas-pickle/lib/python3.7/pickle.py in load(self)
   1086                     raise EOFError
   1087                 assert isinstance(key, bytes_types)
-> 1088                 dispatch[key[0]](self)
   1089         except _Stop as stopinst:
   1090             return stopinst.value

~/anaconda3/envs/pandas-pickle/lib/python3.7/pickle.py in load_short_binstring(self)
   1262         len = self.read(1)[0]
   1263         data = self.read(len)
-> 1264         self.append(self._decode_string(data))
   1265     dispatch[SHORT_BINSTRING[0]] = load_short_binstring
   1266

~/anaconda3/envs/pandas-pickle/lib/python3.7/pickle.py in _decode_string(self, value)
   1202             return value
   1203         else:
-> 1204             return value.decode(self.encoding, self.errors)
   1205
   1206     def load_string(self):

UnicodeDecodeError: 'ascii' codec can't decode byte 0xd9 in position 0: ordinal not in range(128)
```

</details>

I believe the fix for this specific issue can be as simple as:

```diff
diff --git a/pandas/io/pickle.py b/pandas/io/pickle.py
index e51f24b55..5c4f2d8c4 100644
--- a/pandas/io/pickle.py
+++ b/pandas/io/pickle.py
@@ -183,7 +183,11 @@ def read_pickle(
         # e.g.
         #  ""No module named 'pandas.core.sparse.series'""
         #  ""Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib""
-        return pc.load(f, encoding=None)
+        try:
+            return pc.load(f, encoding=None)
+        except UnicodeDecodeError:
+            # e.g. can occur for files written in py27;
+            return pc.load(f, encoding=""latin-1"")
     except UnicodeDecodeError:
         # e.g. can occur for files written in py27; see GH#28645
         return pc.load(f, encoding=""latin-1"")
```

I will open a PR with the fix above over the weekend.

Thanks!"
566281691,32055,REGR: read_pickle fallback to encoding=latin_1 upon a UnicodeDecodeError,pedroreys,closed,2020-02-17T12:56:42Z,2020-02-21T14:52:39Z,"When a reading a pickle with MultiIndex columns generated in py27
`pickle_compat.load()` with `enconding=None` would throw an UnicodeDecodeError
when reading a pickle created in py27. Now, `read_pickle` catches that exception and
fallback to use `latin-1` explicitly.

- [x] closes #31988
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
568999926,32153,Backport PR #32126 on branch 1.0.x (BUG: Fix for convert_dtypes with mix of int and string),meeseeksmachine,closed,2020-02-21T14:49:02Z,2020-02-21T16:07:01Z,Backport PR #32126: BUG: Fix for convert_dtypes with mix of int and string
569001985,32154,Backport PR #32055 on branch 1.0.x (REGR: read_pickle fallback to encoding=latin_1 upon a UnicodeDecodeError),meeseeksmachine,closed,2020-02-21T14:52:34Z,2020-02-21T16:07:24Z,Backport PR #32055: REGR: read_pickle fallback to encoding=latin_1 upon a UnicodeDecodeError
568592698,32138,"CLN: make tm.N, tm.K private",jbrockmendel,closed,2020-02-20T21:47:02Z,2020-02-21T18:34:34Z,"AFAICT this gets rid of the last two places where we set `tm.N` externally.  With those gone, this privatizes these variables in the hopes of not falling back into that pattern."
568693343,32140,Pandas 1.0.1 fails get_loc on timeindex (0.25.3 works),AndrewMoscoe,closed,2020-02-21T02:13:55Z,2020-02-21T18:35:15Z,"```python
findtime = pd.Timestamp('2019-12-12 10:19:25', tz='US/Eastern')
start = pd.Timestamp('2019-12-12 0:0:0', tz='US/Eastern')
end = pd.Timestamp('2019-12-13 0:0:0', tz='US/Eastern')
testindex = pd.date_range(start, end, freq='5s')
testindex.get_loc(findtime, method='nearest')

```
#### Problem description
With pandas 1.0.1, python 3.8.1

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Python38\lib\site-packages\pandas\core\indexes\datetimes.py"", line 699, in get_loc
    return Index.get_loc(self, key, method, tolerance)
  File ""C:\Program Files\Python38\lib\site-packages\pandas\core\indexes\base.py"", line 2649, in get_loc
    indexer = self.get_indexer([key], method=method, tolerance=tolerance)
  File ""C:\Program Files\Python38\lib\site-packages\pandas\core\indexes\base.py"", line 2740, in get_indexer
    indexer = self._get_nearest_indexer(target, limit, tolerance)
  File ""C:\Program Files\Python38\lib\site-packages\pandas\core\indexes\base.py"", line 2821, in _get_nearest_indexer
    left_distances = abs(self.values[left_indexer] - target)
numpy.core._exceptions.UFuncTypeError: ufunc 'subtract' cannot use operands with types dtype('<M8[ns]') and dtype('O')
```

With 0.25.3, this returns index location 7433 as expected.

I've tested this on Windows 10 1909, and Ubuntu WSL, and get the same error on both.


#### Output of ``pd.show_versions()``

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : AMD64 Family 23 Model 8 Stepping 2, AuthenticAMD
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_Canada.1252

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```"
561272002,31760,groupby().agg() calls the user function for an extra time with empty inputs in 1.0.0,icexelloss,closed,2020-02-06T21:11:37Z,2020-02-21T18:38:13Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame({""key"": [""a"", ""b"", ""c"", ""c""], ""value"": [1, 2, 3, 4]})                                                                                                                                                                                                                                                                                                                                                                                                                        

def foo(x): 
    print(f""called with {x}"") 
    return len(x) 

df.groupby(""key"")[""value""].agg(foo)
```
#### Problem description

In 1.0.0, this code outputs:
```
called with Series([], Name: value, dtype: int64)
called with 0    1
Name: value, dtype: int64
called with 1    2
Name: value, dtype: int64
called with 2    3
3    4
Name: value, dtype: int64
Out[10]: 
key
a    1
b    1
c    2
Name: value, dtype: int64
```

In 0.25.3, the code outputs:
```
In [5]:  df.groupby(""key"")[""value""].agg(foo)                                                                                                                                                                                                                                                                                                                                                                                                                                                           
called with 0    1
Name: value, dtype: int64
called with 1    2
Name: value, dtype: int64
called with 2    3
3    4
Name: value, dtype: int64
Out[5]: 
key
a    1
b    1
c    2
Name: value, dtype: int64
```

In 1.0.0, `foo` is called one more time with empty input, which can break user code


**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
567862102,32121,REG: dont call func on empty input,jbrockmendel,closed,2020-02-19T21:35:24Z,2020-02-21T18:40:03Z,"- [x] closes #31760
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Could use confirmation from OP that this does in fact close #31760."
569118254,32159,Backport PR #32121 on branch 1.0.x (REG: dont call func on empty input),meeseeksmachine,closed,2020-02-21T18:38:44Z,2020-02-21T19:25:50Z,Backport PR #32121: REG: dont call func on empty input
568977957,32151,DOC: move whatsnew to sync master with Backport PR #31511,simonjayhawkins,closed,2020-02-21T14:10:39Z,2020-02-21T19:27:42Z,xref #32150
568976049,32150,Backport PR #31511 on branch 1.0.x (BUG: fix reindexing with a tz-awre index and method='nearest'),simonjayhawkins,closed,2020-02-21T14:07:13Z,2020-02-21T19:28:49Z,with whatsnew moved to doc/source/whatsnew/v1.0.2.rst
568990767,32152,TST: add test for get_loc on tz-aware DatetimeIndex,simonjayhawkins,closed,2020-02-21T14:32:40Z,2020-02-21T19:39:31Z,"- [ ] closes #32140
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
569004238,32155,TST: add test for DataFrame.reindex on nearest tz-aware DatetimeIndex,simonjayhawkins,closed,2020-02-21T14:56:29Z,2020-02-21T19:40:51Z,"- [ ] closes #31964
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
569263259,32170,DOC: Move testing and pandas_development_faq pages from wiki to doc #30232 & #20501,AdrianMastronardi,closed,2020-02-22T01:56:57Z,2020-02-22T02:30:55Z,"- [ ] closes #20501
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
569240259,32165,Applying groupby and aggregating with nunique over float column with nulls unexpectedly changes dataframe,cuckookernel,closed,2020-02-21T23:50:14Z,2020-02-22T09:57:57Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np
print( pd.__version__ )  # 1.0.1
df = pd.DataFrame( [[1, 0], [2, np.nan], [3, np.nan]], columns=['a', 'b'] )
print( df )
#   a    b
# 0  1  0.0
# 1  2  NaN
# 2  3  NaN
unused = df.groupby('a').agg({'b': 'nunique'})     # this shouldn't change anything in df, right?
print( df ) # prints out
#   a             b
# 0  1  0.000000e+00
# 1  2 -9.223372e+18
# 2  3 -9.223372e+18
# what happened to my nans?
```
#### Problem description

For some reason the agg, groupby operation is changing the orginal nan values in the b column 
to a special float value. 
In the previous version of pandas I was using, this didn't happen.

### Expected Output
The expected output of the  print should the same as the one of the first print as groupby + agg should never change the input df (right?)

#### Output of ``pd.show_versions()``

<details>
[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-70-generic
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : sv_SE.UTF-8
LOCALE           : sv_SE.UTF-8
pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : 1.0.6
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
15405714,3851,"CParserError: Error tokenizing data, when using pd.read_csv()",agconti,closed,2013-06-11T15:44:15Z,2020-02-22T14:52:37Z,"Pandas version : '0.11.0'
NumPy version: '1.7.1'

Replication of Error:

```
import pandas
pandas.read_csv('http://www.netfonds.no/quotes/tradedump.php?date=20130611&paper=AAPL.A&csv_format=csv')
```

Error Message:

```
---------------------------------------------------------------------------
CParserError                              Traceback (most recent call last)
<ipython-input-5-e950a741cc55> in <module>()
----> 1 pandas.read_csv('http://www.netfonds.no/quotes/tradedump.php?date=20130611&paper=AAPL.A&csv_format=csv')

C:\Python27\lib\site-packages\pandas\io\parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze)
    399                     buffer_lines=buffer_lines)
    400 
--> 401         return _read(filepath_or_buffer, kwds)
    402 
    403     parser_f.__name__ = name

C:\Python27\lib\site-packages\pandas\io\parsers.pyc in _read(filepath_or_buffer, kwds)
    214         return parser
    215 
--> 216     return parser.read()
    217 
    218 _parser_defaults = {

C:\Python27\lib\site-packages\pandas\io\parsers.pyc in read(self, nrows)
    631             #     self._engine.set_error_bad_lines(False)
    632 
--> 633         ret = self._engine.read(nrows)
    634 
    635         if self.options.get('as_recarray'):

C:\Python27\lib\site-packages\pandas\io\parsers.pyc in read(self, nrows)
    955 
    956         try:
--> 957             data = self._reader.read(nrows)
    958         except StopIteration:
    959             if nrows is None:

C:\Python27\lib\site-packages\pandas\_parser.pyd in pandas._parser.TextReader.read (pandas\src\parser.c:6014)()

C:\Python27\lib\site-packages\pandas\_parser.pyd in pandas._parser.TextReader._read_low_memory (pandas\src\parser.c:6231)()

C:\Python27\lib\site-packages\pandas\_parser.pyd in pandas._parser.TextReader._read_rows (pandas\src\parser.c:6833)()

C:\Python27\lib\site-packages\pandas\_parser.pyd in pandas._parser.TextReader._tokenize_rows (pandas\src\parser.c:6718)()

C:\Python27\lib\site-packages\pandas\_parser.pyd in pandas._parser.raise_parser_error (pandas\src\parser.c:17131)()

CParserError: Error tokenizing data. C error: Expected 1 fields in line 72, saw 3
```

I am trying to pull stock tick data from netfounds.com as you can see above. The URL is incorrect because the correct ticker symbol is supposed to be `AAPL.O` and not `AAPL.A`  This issue seems to come and go, as some days it just returns a jumbled data frame when `AAPL.A` is queried, (this is what I want), and does not throw the `CParserError`. Having the incorrect query is only a convenience in my program but I'm trying to understand why pandas only throws this error sometimes.  

More Info about the endings:

Typically I will iterate through a list of exchanges in this dict:

```
exchanges = {'Nasdaq': '.O', 'Nyse' : '.N', 'Amex': '.A'}
```

to arrive at the correct ticker 'ending` and get the tick data. 
"
558255250,31503,API: query / boolean selection with nullable dtypes with NAs,tdpetrou,closed,2020-01-31T17:09:33Z,2020-02-22T15:37:38Z,"#### Code Sample

Create a dataframe with nullable integer, string, and float data types.

```python
>>> df = df = pd.DataFrame({'a': [1, 3, np.nan], 'b': ['rain', 'shine', None], 
                                                    'a_float': [1.1, 3.3, np.nan]})
>>> df = df.convert_dtypes()
>>> df
```

|    | a    | b     |   a_float |
|---:|:-----|:------|----------:|
|  0 | 1    | rain  |       1.1 |
|  1 | 3    | shine |       3.3 |
|  2 | `<NA>` | `<NA>`  |     nan   |

Verify data types and attempt to use `query`

```python
>>> df.dtypes
a            Int64
b            string
a_float      float64

>>> df.query('a > 2') # same as df[df['a'] > 2]
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/Documents/Code Practice/pandas-dev/pandas/pandas/core/frame.py in query(self, expr, inplace, **kwargs)
   3227         try:
-> 3228             new_data = self.loc[res]
   3229         except ValueError:

~/Documents/Code Practice/pandas-dev/pandas/pandas/core/indexing.py in __getitem__(self, key)
   1683             maybe_callable = com.apply_if_callable(key, self.obj)
-> 1684             return self._getitem_axis(maybe_callable, axis=axis)
   1685 

~/Documents/Code Practice/pandas-dev/pandas/pandas/core/indexing.py in _getitem_axis(self, key, axis)
   1798             return self._get_slice_axis(key, axis=axis)
-> 1799         elif com.is_bool_indexer(key):
   1800             return self._getbool_axis(key, axis=axis)

~/Documents/Code Practice/pandas-dev/pandas/pandas/core/common.py in is_bool_indexer(key)
    133                 if np.any(key.isna()):
--> 134                     raise ValueError(na_msg)
    135             return True

ValueError: cannot mask with array containing NA / NaN values

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-52-e5d239635d7b> in <module>
----> 1 df.query('a > 2')

~/Documents/Code Practice/pandas-dev/pandas/pandas/core/frame.py in query(self, expr, inplace, **kwargs)
   3230             # when res is multi-dimensional loc raises, but this is sometimes a
   3231             # valid query
-> 3232             new_data = self[res]
   3233 
   3234         if inplace:

~/Documents/Code Practice/pandas-dev/pandas/pandas/core/frame.py in __getitem__(self, key)
   2784 
   2785         # Do we have a (boolean) 1d indexer?
-> 2786         if com.is_bool_indexer(key):
   2787             return self._getitem_bool_array(key)
   2788 

~/Documents/Code Practice/pandas-dev/pandas/pandas/core/common.py in is_bool_indexer(key)
    132             if is_extension_array_dtype(key.dtype):
    133                 if np.any(key.isna()):
--> 134                     raise ValueError(na_msg)
    135             return True
    136     elif isinstance(key, list):

ValueError: cannot mask with array containing NA / NaN values

>>> df.query('a_float > 2')
```
|    |   a | b     |   a_float |
|---:|----:|:------|----------:|
|  1 |   3 | shine |       3.3 |

Using `query` with strings works...

```python
>>> df.query('b == ""rain""')
```
|    |   a | b    |   a_float |
|---:|----:|:-----|----------:|
|  0 |   1 | rain |       1.1 |


...but fails for boolean selection
```python
>>> df[df['b'] == 'rain']
ValueError: cannot mask with array containing NA / NaN values
```

strings also fail for inequalities
```
>>> df.query('b >= ""rain""') # also df.query('b > ""rain""')
ValueError: cannot mask with array containing NA / NaN values
```

#### Problem description

The `query` method  behaves differently for nullable integers, strings, and floats. Here's my summary of how I think they work with the `query method` assuming there are missing values in the columns.

* nullable integers - fails
* strings - works with equality, fails with inequality
* float - works for all

I find it extremely difficult to use if the behavior for all of these types are different for query and boolean selection.

#### Expected Output

I think I would prefer to have both query and boolean selection working like they do with floats, where missing values evaluate as False in a condition. And even if there are missing values in the boolen mask itself, treat those as False. This would harmonize the behavior for all data types. 

This would leave it up to the user to check for missing values. I believe SQL where clauses work in such a manner (missing values in conditions evaluate as False).

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.2.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0+untagged.1.gce8af21.dirty
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : 0.29.14
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.13
tables           : None
tabulate         : 0.8.3
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
569270192,32171,REF/TST: collect Index join tests,jbrockmendel,closed,2020-02-22T02:54:41Z,2020-02-22T15:42:16Z,"we get a little bit of parametrization out of the deal, not much"
562923298,31864,REF: organize base class Index tests,jbrockmendel,closed,2020-02-11T01:15:54Z,2020-02-22T15:51:28Z,implement test_index_new for testing `Index.__new__` inference
567269188,32089,BUG: DataFrame.iat incorrectly wrapping datetime objects,jbrockmendel,closed,2020-02-19T02:08:53Z,2020-02-22T15:58:16Z,
567771837,32116,REF: collect arithmetic benchmarks,jbrockmendel,closed,2020-02-19T18:53:09Z,2020-02-22T16:08:29Z,"ATM these are scattered, making it tough to a) run just arithmetic-relevant benchmarks and b) determine what we have good benchmark coverage for.

This collects the scattered benchmarks in one place, cleaning up and fleshing these out is for a separate pass.

I also intend to do something analogous for indexing benchmarks."
567234620,32085,BUG: Fix incorrect _is_scalar_access check in iloc,jbrockmendel,closed,2020-02-19T00:08:20Z,2020-02-22T16:10:24Z,We only have two test cases that make it to the _take_with_is_copy line in _iget_item_cache
569248582,32168,REF: de-duplicate object-dtype handling,jbrockmendel,closed,2020-02-22T00:26:58Z,2020-02-22T16:32:24Z,We'll also use this for the categorical case following #32167.
569250186,32169,BUG: disallow invalid dtype to CategoricalDtype._from_values_or_dtype,jbrockmendel,closed,2020-02-22T00:34:46Z,2020-02-22T16:33:38Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref #32167.  Together with that, this will allow us to simplify the CategoricalIndex constructor."
568463832,32133,used f-strings in docs,PSY27,closed,2020-02-20T17:39:09Z,2020-02-22T16:33:51Z,"- [x] xref #32039
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
568700856,32141,REF: standardize CategoricalIndex._shallow_copy usage,jbrockmendel,closed,2020-02-21T02:42:26Z,2020-02-22T16:34:04Z,"The dtype keyword is akward, so this removes the one usage.

In turn this will allow us to simplify `_simple_new` and tighten up what it expects.  Saved for separate PR."
562922277,31863,CLN: organize MultiIndex indexing tests,jbrockmendel,closed,2020-02-11T01:12:33Z,2020-02-22T16:38:28Z,
568449299,32131,CLN: NDFrame.__init__ unnecessary code,jbrockmendel,closed,2020-02-20T17:12:27Z,2020-02-22T16:39:02Z,"Make methods into classmethods where feasible, annotate in a couple of places.

Removes nonsense test (constructs an invalid NDFrame)

Preparatory for making a potential fastpath _from_mgr constructor.

`attrs` is never passed.  Is there a plan to do so @TomAugspurger ?"
568422499,32130,"PERF: IntegerIndex._shallow_copy (and by extension, indexing)",jbrockmendel,closed,2020-02-20T16:32:29Z,2020-02-22T16:40:45Z,"```
In [2]: idx = pd.Index(range(1000)).insert(-1, 1)                                                                                                                                                                                                 
In [3]: %timeit idx[:5]                                                                                                                                                                                                                           
2.84 µs ± 97.6 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)  # <-- PR
24.5 µs ± 292 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- master

In [4]: idx2 = idx.astype('uint64')                                                                                                                                                                                                               
In [5]: %timeit idx2[5:]                                                                                                                                                                                                                          
2.99 µs ± 331 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)  # <-- PR
25.8 µs ± 957 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- master
```

Motivated by profiling in #32086 and seeing that _shallow_copy_with_infer was taking over a third of the runtime in the relevant benchmark."
567661314,32110,"TST: method-specific files for DataFrame assign, interpolate ",jbrockmendel,closed,2020-02-19T16:05:16Z,2020-02-22T16:48:55Z,
567683559,32112,REF/TST: implement test_interpolate for Series,jbrockmendel,closed,2020-02-19T16:34:58Z,2020-02-22T16:49:18Z,
292655811,19456,Error when using .loc[integer] with object Index (also) containing floats,toobaz,closed,2018-01-30T05:16:15Z,2020-02-22T17:53:08Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: s = pd.Series(range(2), pd.Index([1, 2.], dtype=object))

In [3]: s.loc[1]
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-3-a7c868f8fb38> in <module>()
----> 1 s.loc[1]

/home/nobackup/repo/pandas/pandas/core/indexing.py in __getitem__(self, key)
   1367 
   1368             maybe_callable = com._apply_if_callable(key, self.obj)
-> 1369             return self._getitem_axis(maybe_callable, axis=axis)
   1370 
   1371     def _is_scalar_access(self, key):

/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_axis(self, key, axis)
   1620 
   1621         # fall thru to straight lookup
-> 1622         self._has_valid_type(key, axis)
   1623         return self._get_label(key, axis=axis)
   1624 

/home/nobackup/repo/pandas/pandas/core/indexing.py in _has_valid_type(self, key, axis)
   1498 
   1499             try:
-> 1500                 key = self._convert_scalar_indexer(key, axis)
   1501                 if not ax.contains(key):
   1502                     error()

/home/nobackup/repo/pandas/pandas/core/indexing.py in _convert_scalar_indexer(self, key, axis)
    247         ax = self.obj._get_axis(min(axis, self.ndim - 1))
    248         # a scalar
--> 249         return ax._convert_scalar_indexer(key, kind=self.name)
    250 
    251     def _convert_slice_indexer(self, key, axis):

/home/nobackup/repo/pandas/pandas/core/indexes/base.py in _convert_scalar_indexer(self, key, kind)
   1373             elif kind in ['loc'] and is_integer(key):
   1374                 if not self.holds_integer():
-> 1375                     return self._invalid_indexer('label', key)
   1376 
   1377         return key

/home/nobackup/repo/pandas/pandas/core/indexes/base.py in _invalid_indexer(self, form, key)
   1557                         ""indexers [{key}] of {kind}"".format(
   1558                             form=form, klass=type(self), key=key,
-> 1559                             kind=type(key)))
   1560 
   1561     def get_duplicates(self):

TypeError: cannot do label indexing on <class 'pandas.core.indexes.base.Index'> with these indexers [1] of <class 'int'>

In [4]: s.loc[1.]
Out[4]: 0
```

#### Problem description

I guess this is closely related to #17286 , which however failed also with float keys (using ``__getitem__`` rather than ``.loc``).

#### Expected Output

```python
In [3]: s.loc[1]
Out[3]: 0
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: d740b65f8a32062d8c299697ff99d3ae30cd84c8
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.0-5-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: it_IT.UTF-8
LOCALE: it_IT.UTF-8

pandas: 0.23.0.dev0+188.gd740b65f8
pytest: 3.2.3
pip: 9.0.1
setuptools: 36.7.0
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.19.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: 1.5.6
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: 1.2.0dev
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.0.0
openpyxl: 2.3.0
xlrd: 1.0.0
xlwt: 1.3.0
xlsxwriter: 0.9.6
lxml: 4.1.1
bs4: 4.5.3
html5lib: 0.999999999
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.2.1

</details>
"
563677327,31905,BUG: using loc[int] with object index,jbrockmendel,closed,2020-02-12T01:32:32Z,2020-02-22T17:53:21Z,"- [x] closes #19456
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This causes us to raise KeyError instead of TypeError in a couple of places, consistent with #31867.

Also note this leaves us with only one non-plotting usage of `holds_integer`, and it wouldnt surprise me if that one is subtly causing problems too."
564256346,31936,dont skip keyerror for IntervalIndex,jbrockmendel,closed,2020-02-12T20:38:17Z,2020-02-22T17:53:56Z,"cc @jschendel this is speculative that this is the desired behavior.

This is our only use of `Index.is_interval()`"
562829603,31857,REF: move Loc method that belongs on Index,jbrockmendel,closed,2020-02-10T21:10:49Z,2020-02-22T19:38:00Z,
526801980,29779,pd.Timestamp.timestamp() doesn't match stdlib,arsturges,closed,2019-11-21T19:41:20Z,2020-02-22T20:27:52Z,"edit: See https://github.com/pandas-dev/pandas/issues/29779#issuecomment-557747641

I searched for this among open issues but couldn't find it. Apologies in advance if I overlooked an existing issue, resolved or otherwise.

```python
Python 3.7.4 (default, Sep  7 2019, 18:29:04)
[Clang 10.0.0 (clang-1000.11.45.5)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import datetime
>>> import pandas as pd
>>> dt = datetime.datetime(1970, 1, 1)
>>> pdt = pd.Timestamp(1970, 1, 1)
>>> dt.timestamp() == pdt.timestamp()  # I expected this to be True
False
>>> dt.timestamp()  # I am in PST, hence the 8-hour offset from 0
28800.0
>>> pdt.timestamp()
0.0
```


[Python3 docs explain](https://docs.python.org/3/library/datetime.html#datetime.datetime.timestamp) that datetime.datetime.timestamp() assume local time if no tzinfo is specified: ""Naive datetime instances are assumed to represent local time and this method relies on the platform C mktime() function to perform the conversion.""


#### Problem description
I expected the behavior of pd.Timestamp.timestamp to match that of the stdlib (datetime.datetime.stdlib). Instead, pd.Timestamp.timestamp appears to treat naive timestamps differently.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.2
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 40.8.0
Cython           : 0.29.13
pytest           : 5.2.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.8
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
569372368,32184,CLN: Removed class in pandas/tests/series/test_validate.py,SaturnFromTitan,closed,2020-02-22T18:16:10Z,2020-02-22T20:59:11Z,"I've seen quite a few test files that only use a class for historic reasons. As functions are easier to comprehend I think it makes sense to move in this direction.

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
569364766,32181,CLN: unnecessary kwargs for take_with_is_copy,jbrockmendel,closed,2020-02-22T17:21:25Z,2020-02-22T21:08:47Z,
569404512,32189,CLN: F-string in pandas/tests/indexes/datetimes/test_to_period.py (#29547),jancervenka,closed,2020-02-22T22:50:16Z,2020-02-23T01:01:03Z,"Issue #29547
- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
558783822,31591,ENH: Enable indexing with nullable Boolean,dsaxton,closed,2020-02-03T00:28:12Z,2020-02-23T14:55:01Z,"closes #31503 

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I think from the discussion in https://github.com/pandas-dev/pandas/issues/31503 that this is something people want to allow."
564065657,31927,Construction of Categorical from array with pd.NA failing,jorisvandenbossche,closed,2020-02-12T15:11:38Z,2020-02-23T14:57:08Z,"So creating a Categorical from an array with pd.NA fails:

```
In [10]: pd.Categorical(np.array([""a"", ""b"", pd.NA], dtype=object))  
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/scipy/pandas/pandas/core/arrays/categorical.py in __init__(self, values, categories, ordered, dtype, fastpath)
    354             try:
--> 355                 codes, categories = factorize(values, sort=True)
    356             except TypeError:

~/scipy/pandas/pandas/core/algorithms.py in factorize(values, sort, na_sentinel, size_hint)
    635         codes, uniques = _factorize_array(
--> 636             values, na_sentinel=na_sentinel, size_hint=size_hint, na_value=na_value
    637         )

~/scipy/pandas/pandas/core/algorithms.py in _factorize_array(values, na_sentinel, size_hint, na_value)
    483     table = hash_klass(size_hint or len(values))
--> 484     uniques, codes = table.factorize(values, na_sentinel=na_sentinel, na_value=na_value)
    485 

~/scipy/pandas/pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.factorize()

~/scipy/pandas/pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable._unique()

~/scipy/pandas/pandas/_libs/missing.pyx in pandas._libs.missing.NAType.__bool__()

TypeError: boolean value of NA is ambiguous
```

while from a list actually ""works"":

```
In [11]: pd.Categorical([""a"", ""b"", pd.NA]) 
Out[11]: 
[a, b, NaN]
Categories (2, object): [a, b]
```

This also means that creating a Categorical from a StringArray with missing values won't work (with the same error as above). 
I am only not sure if it should work (at least before we can have a ""string"" dtype index as the categories of the created Categorical)."
567959165,32123,BUG: GroupBy.first fails with pd.NA on Series with object dtype,jprafael,closed,2020-02-20T00:25:36Z,2020-02-23T14:59:00Z,"#### Code Sample

```python
pd.DataFrame({'x': [1, 1, 2, 2], 'y': [1, 2, 3, pd.NA]}).groupby('x').first()
# *** TypeError: boolean value of NA is ambiguous

pd.DataFrame({'x': [1, 1, 2, 2], 'y': [1, 2, 3, np.nan]}).groupby('x').first()
#     y
# x     
# 1  1.0
# 2  3.0

pd.DataFrame({'x': [1, 1, 2, 2], 'y': [1, 2, 3, pd.NA]}).astype('Int64').groupby('x').first()
#    y
# x   
# 1  1
# 2  3
```
#### Problem description

Applying the `GroupBy.first` aggregation to a `object` dtype column that contains a `pd.NA` causes the method to fail with an exception: `TypeError: boolean value of NA is ambiguous`. Method works fine when using `np.nan` and also works as expected when the column is first converted to an `Int64` dtype column.

#### Expected Output

```
   y
x   
1  1
2  3
```
#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-76-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : pt_PT.UTF-8

pandas           : 1.0.1
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 20.0.2
setuptools       : 42.0.1
Cython           : None
pytest           : 5.2.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.10.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.2.3
pyxlsb           : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : 1.3.10
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.46.0
```
</details>
"
564595416,31950,BUG: groupby-nunique modifies null values,thomas-reineking-by,closed,2020-02-13T10:47:13Z,2020-02-23T15:02:17Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np
df = pd.DataFrame({""GROUP"": 0, ""VALUE"": [1.0, np.nan]})
df.groupby(""GROUP"")[""VALUE""].nunique()
print(df)
```
#### Problem description
Original dataframe is modified:
```
   GROUP         VALUE
0      0  1.000000e+00
1      0 -9.223372e+18
```

Issue seems to have been introduced in version 1.0.0, 0.25.3 works as expected.

#### Expected Output
Original dataframe should not be modified.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.9.87-linuxkit-aufs
machine          : x86_64
processor        :
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : 0.29.13
pytest           : 5.2.1
hypothesis       : 4.23.0
sphinx           : 1.7.9
blosc            : None
feather          : None
xlsxwriter       : 1.2.2
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.2 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : None
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
pytest           : 5.2.1
pyxlsb           : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : 1.3.11
tables           : None
tabulate         : 0.8.5
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : 1.2.2
numba            : 0.45.1
</details>
"
569428930,32192,Backport PR #31591 on branch 1.0.x,dsaxton,closed,2020-02-23T03:09:40Z,2020-02-23T15:06:31Z,xref #31591 
567995128,32124,BUG: Avoid ambiguous condition in GroupBy.first / last,dsaxton,closed,2020-02-20T01:37:52Z,2020-02-23T15:07:44Z,"- [x] closes #32123
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This comparison `val == val` happens in a lot of these groupby operations but only seems to raise here in the presence of `NA`. Are we just always / mostly converting to numpy beforehand in the other cases?"
564277438,31939,BUG: Fix construction of Categorical from pd.NA,dsaxton,closed,2020-02-12T21:17:04Z,2020-02-23T15:10:32Z,"- [x] closes #31927 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
569507970,32199,Backport PR #32124 on branch 1.0.x (BUG: Avoid ambiguous condition in GroupBy.first / last),meeseeksmachine,closed,2020-02-23T15:00:19Z,2020-02-23T15:45:54Z,Backport PR #32124: BUG: Avoid ambiguous condition in GroupBy.first / last
569507994,32200,Backport PR #31939 on branch 1.0.x (BUG: Fix construction of Categorical from pd.NA),meeseeksmachine,closed,2020-02-23T15:00:31Z,2020-02-23T16:02:55Z,Backport PR #31939: BUG: Fix construction of Categorical from pd.NA
569394812,32186,CLN: Removed class in pandas/tests/series/test_timezones.py,SaturnFromTitan,closed,2020-02-22T21:19:02Z,2020-02-23T16:15:29Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
569508259,32201,Backport PR #32175 on branch 1.0.x (BUG: groupby nunique changing values),meeseeksmachine,closed,2020-02-23T15:02:26Z,2020-02-23T16:59:31Z,Backport PR #32175: BUG: groupby nunique changing values
569436732,32193,TYP: annotations,jbrockmendel,closed,2020-02-23T04:41:25Z,2020-02-23T17:04:04Z,
567296443,32091,TST: parametrize and de-duplicate timedelta64 arithmetic tests,jbrockmendel,closed,2020-02-19T03:45:07Z,2020-02-23T17:05:17Z,
567123407,32082,CLN: indexing comments and cleanups,jbrockmendel,closed,2020-02-18T19:57:15Z,2020-02-23T17:10:08Z,
562940264,31866,BUG: catch almost-null-slice in _convert_slice_indexer,jbrockmendel,closed,2020-02-11T02:26:20Z,2020-02-23T17:11:04Z,"its a contrived corner case, but i wanted to use com.is_null_slice rather than duplicating it in _convert_slice_indexer"
569247354,32167,"BUG: Index(categorical, dtype=object) not returning object dtype",jbrockmendel,closed,2020-02-22T00:21:01Z,2020-02-23T17:12:53Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

In master we get a `CategoricalIndex` back.

Fixing this turns out to be pre-requisite to fixing a bug in `CategoricalDtype._from_values_or_dtype`, which in turn is needed to simplify the CategoricalIndex constructors (xref #32141)"
554997177,31297,REF: pass str_rep through arithmetic ops more consistently,jbrockmendel,closed,2020-01-24T23:11:03Z,2020-02-23T17:13:37Z,"This doesn't get all of the places where we fail to pass str_rep, still working out a couple of places where passing it breaks tests."
566002092,32046,Use fixtures in pandas/tests/base,SaturnFromTitan,closed,2020-02-17T00:50:11Z,2020-02-23T17:46:40Z,"part of #23877
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

**Note:** The diff is a bit inflated. Most changes are just indentation because for loops are replaced by a parametrized fixture."
569068828,32158,TST: Fixed bare pytest.raises in test_window.py,mabroor,closed,2020-02-21T16:52:19Z,2020-02-23T19:29:02Z,"- [x] ref #30999 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
566305242,32060,CI: Web and Docs fails,AlexKirko,closed,2020-02-17T13:38:34Z,2020-02-24T12:02:01Z,"#### Problem description

Currently, Web and Docs build seems to fail for everyone during CI, regardless of PR. Saw this in #31563 and #32055. 

This is the error:
```
Extension error:
Could not import extension contributors (exception: No module named 'gitdb.utils.compat')
##[error]Bash exited with code '2'.
```"
565733707,32015,DOC: Improve docstring of Index.delete,andhikayusup,closed,2020-02-15T11:12:36Z,2020-02-24T15:11:04Z,"- [x] closes https://github.com/pandanistas/pandanistas_sprint_jakarta2020/issues/13
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Output from  validate_docstrings.py
################################################################################
####################### Docstring (pandas.Index.delete)  #######################
################################################################################

Make new Index with passed location(-s) deleted.

Use array of integer as loc parameter to delete multiple locations.

Parameters
----------
loc : int
    Location of item(-s) which will be deleted.

Returns
-------
Index
    New Index with passed location(-s) deleted.

See Also
--------
numpy.delete : Delete any rows and column from NumPy array (ndarray).

Examples
--------
>>> idx = pd.Index(['a', 'b', 'c'])
>>> idx.delete(1)  # Deleting 'b'
Index(['a', 'c'], dtype='object')
>>> idx = pd.Index(['a', 'b', 'c'])
>>> idx.delete([0, 2])
Index(['b'], dtype='object')

################################################################################
################################## Validation ##################################
################################################################################
"
569593602,32216,REF: de-duplicate factorize and duplicated code,jbrockmendel,closed,2020-02-24T01:13:36Z,2020-02-24T16:13:10Z,
472059890,27553,`OptionError` raised by top-level function `pandas.set_option` is private,huandzh,closed,2019-07-24T03:22:57Z,2020-02-24T16:32:46Z,"#### Code Sample

```python
import pandas as pd
from pandas.errors import OptionError 
# or from pandas import OptionError
option = ('noexist', 1)
try:
    pd.set_option(*option)
except OptionError as e:
    print('Ignore invaild option: {}'.format(option))
```
#### Problem description

`OptionError` is raised by top-level function `pandas.set_option`, but can only be imported from private module `pandas._config.config`.

Sample code will failed when importing it from `pandas.errors`  :

```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-62b92256ce5f> in <module>
      1 import pandas as pd
----> 2 from pandas.errors import OptionError # or from pandas import OptionError
      3 option = ('noexist', 1)
      4 try:
      5     pd.set_option(*option)

ImportError: cannot import name 'OptionError' from 'pandas.errors' (/home/abc/.local/share/virtualenvs/abc-jIQt2SYy/lib/python3.7/site-packages/pandas/errors/__init__.py)
```

#### Expected Output

```
Ignored invaild option: ('noexist', 1)
```

Expected behavior : `OptionError` can be imported from `pandas.errors` or `pandas`. And other custom exceptions raised by top-level functions are also available in the same public namespace.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-17763-Microsoft
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.0
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : 0.29.12
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.6.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

</details>
"
568777714,32142,Import OptionError in pandas.errors,sumanau7,closed,2020-02-21T07:10:00Z,2020-02-24T16:32:50Z,"- [x] closes #27553
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
553887652,31232,REGR: Fix IntervalIndex.map when result is object dtype,jschendel,closed,2020-01-23T01:22:55Z,2020-02-24T18:13:47Z,"- [X] closes #31202
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

No whatnew since this is a regression.
"
555968333,31378,Backport PR #31238: REGR: Prevent indexes that aren't directly backedby numpy from entering libreduction code paths,jschendel,closed,2020-01-28T03:59:09Z,2020-02-24T18:13:51Z,Backport #31238
569444775,32195,"Tried this and output was true. Again tried to read the file car = pd.read_csv(""/Users/apple/Desktop/usedcars.csv"". But ended with same error",SatyakeerthiS,closed,2020-02-23T06:12:58Z,2020-02-24T19:26:28Z,"Pls provide your directory structure, then try:

```
import os 
os.getcwd()
# your current working directory will be displayed
os.path.exists('Department_of_Recreation_and_Parks__Facility_and_Park_Information.csv')
# Must be True, otherwise it is unrelated to pandas
```

Not sure why you're caring `matplotlib`. If you trying a script taken from somewhere, you can attach the link.

_Originally posted by @sinhrks in https://github.com/pandas-dev/pandas/issues/10457#issuecomment-116166868_"
569190579,32164,CLN: Fix exception causes in datetimelike.py,cool-RR,closed,2020-02-21T21:21:34Z,2020-02-25T01:52:38Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565729223,32004,DOC: Update pandas.DataFrame.droplevel docstring,asyarif93,closed,2020-02-15T10:29:42Z,2020-02-25T01:54:52Z,"- [ ] closes https://github.com/pandanistas/pandanistas_sprint_jakarta2020/issues/23
- [ ] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Output of `python scripts/validate_docstrings.py pandas.DataFrame.droplevel`:
```
################################################################################
#################### Docstring (pandas.DataFrame.droplevel) ####################
################################################################################

Return DataFrame with requested index / column level(s) removed.

.. versionadded:: 0.24.0

Parameters
----------
level : int, str, or list-like
    If a string is given, must be the name of a level
    If list-like, elements must be names or positional indexes
    of levels.

axis : {0 or 'index', 1 or 'columns'}, default 0
    Axis along which the level(s) is removed:

    * 0 or 'index': remove level(s) in column.
    * 1 or 'columns': remove level(s) in row.

Returns
-------
DataFrame
    DataFrame with requested index / column level(s) removed.

Examples
--------
>>> df = pd.DataFrame([
...     [1, 2, 3, 4],
...     [5, 6, 7, 8],
...     [9, 10, 11, 12]
... ]).set_index([0, 1]).rename_axis(['a', 'b'])

>>> df.columns = pd.MultiIndex.from_tuples([
...     ('c', 'e'), ('d', 'f')
... ], names=['level_1', 'level_2'])

>>> df
level_1   c   d
level_2   e   f
a b
1 2      3   4
5 6      7   8
9 10    11  12

>>> df.droplevel('a')
level_1   c   d
level_2   e   f
b
2        3   4
6        7   8
10      11  12

>>> df.droplevel('level_2', axis=1)
level_1   c   d
a b
1 2      3   4
5 6      7   8
9 10    11  12

################################################################################
################################## Validation ##################################
################################################################################

1 Errors found:
	See Also section not found
```"
570287133,32231,DOC: Clean up of DataFrame.ewm,sursu,closed,2020-02-25T03:30:52Z,2020-02-25T04:24:04Z,"- [ ] closes #31647
Though, I don't know what @MarcoGorelli meant in the 5'th issue.

Didn't know how to expand my previous Pull Request: #32212, as I have already deleted the files on my machine.

Here are the screenshots, @WillAyd:

![image](https://user-images.githubusercontent.com/12001304/75212620-caa16e00-5787-11ea-9bd9-532f557d3e33.png)

![image](https://user-images.githubusercontent.com/12001304/75213997-50271d00-578c-11ea-82e0-5f5b986ede6d.png)


In the first image a controversial change might be: log(0.5) -> -ln(2).
The Notes have been expanded with a simple example from the provided link."
564971493,31964,Reindex on nearest datetime gives UFuncTypeError - subtract with dtype('<M8[ns]') and dtype('O'),andrewcooke,closed,2020-02-13T21:51:23Z,2020-02-25T10:13:26Z,"#### Code Sample

```python
import pandas as pd
print(pd.__version__)
i1 = pd.DatetimeIndex(['2016-06-26 14:27:26+00:00'])
i2 = pd.DatetimeIndex(['2016-07-04 14:00:59+00:00'])
f2 = pd.DataFrame(index=i2)
f2.reindex(i1, method='nearest')
```
#### Problem description

Running the above code gives:

```
Python 3.7.4 (default, Jul 27 2019, 21:25:02) 
[GCC 7.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas as pd
>>> print(pd.__version__)
1.0.1
>>> i1 = pd.DatetimeIndex(['2016-06-26 14:27:26+00:00'])
>>> i2 = pd.DatetimeIndex(['2016-07-04 14:00:59+00:00'])
>>> f2 = pd.DataFrame(index=i2)
>>> f2.reindex(i1, method='nearest')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/andrew/project/ch2/choochoo/py/env/lib/python3.7/site-packages/pandas/util/_decorators.py"", line 227, in wrapper
    return func(*args, **kwargs)
  File ""/home/andrew/project/ch2/choochoo/py/env/lib/python3.7/site-packages/pandas/core/frame.py"", line 3856, in reindex
    return self._ensure_type(super().reindex(**kwargs))
  File ""/home/andrew/project/ch2/choochoo/py/env/lib/python3.7/site-packages/pandas/core/generic.py"", line 4544, in reindex
    axes, level, limit, tolerance, method, fill_value, copy
  File ""/home/andrew/project/ch2/choochoo/py/env/lib/python3.7/site-packages/pandas/core/frame.py"", line 3744, in _reindex_axes
    index, method, copy, level, fill_value, limit, tolerance
  File ""/home/andrew/project/ch2/choochoo/py/env/lib/python3.7/site-packages/pandas/core/frame.py"", line 3760, in _reindex_index
    new_index, method=method, level=level, limit=limit, tolerance=tolerance
  File ""/home/andrew/project/ch2/choochoo/py/env/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 3145, in reindex
    target, method=method, limit=limit, tolerance=tolerance
  File ""/home/andrew/project/ch2/choochoo/py/env/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 2740, in get_indexer
    indexer = self._get_nearest_indexer(target, limit, tolerance)
  File ""/home/andrew/project/ch2/choochoo/py/env/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 2821, in _get_nearest_indexer
    left_distances = abs(self.values[left_indexer] - target)
numpy.core._exceptions.UFuncTypeError: ufunc 'subtract' cannot use operands with types dtype('<M8[ns]') and dtype('O')
```

This is a problem because it occurs in a larger dataset that I need to reindex.  It used to work fine with earlier versions of Pandas (although I do not know which, sorry).

#### Expected Output

Expected output would be something like

```
>>> f2.reindex(i1)
Empty DataFrame
Columns: []
Index: [2016-06-26 14:27:26+00:00]
```

#### Output of ``pd.show_versions()``

<details>

```
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.12.14-lp151.28.36-default
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```

</details>"
567549342,32103,added msg to TypeError test_to_boolean_array_error,amy-graham-js,closed,2020-02-19T12:52:58Z,2020-02-25T10:48:57Z,"- [x] ref #30999 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Fixed bare pytest.raises (#30999) for test_to_boolean_array_error in test_boolean.py 
"
570611531,32239,Creating lag windows in pandas is slow for large dataset,ankurrajdev,closed,2020-02-25T14:31:58Z,2020-02-25T15:56:37Z,"#### Code Sample, a copy-pastable example if possible
```python
# Your code here
for lag in [2, 3, 7, 15, 21]:
                df_tmp.set_index(""date"")
                .groupby(""group"")
                .resample(""D"")
                .sum()
                .groupby(""group"")
                .shift()
                .reset_index()
                .fillna(0)
                .groupby(""group"")
                .rolling(f""{lag}D"", on=""date"")
                .agg(self.funcs)[self.cols]
```

I want to add sum_click_last2days, mean_click_last2days, sum_click_last4days, mean_click_last4days and so on depending on columns, functions and lag windows passed.
Here last n days mean rolling sum till last day. Hence, if we are running on 10th, we would want sum for 8th and 9th.
This is working fine for smaller dataset but is not efficient for a large dataset( 19M and there are 700K groups)
#### Problem description
This is working fine for smaller dataset but is not efficient for a large dataset( 19M and there are 700K groups)
"
569321836,32175,BUG: groupby nunique changing values,MarcoGorelli,closed,2020-02-22T11:29:46Z,2020-02-25T17:10:59Z,"- [ ] closes #31950 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

just reopening #31973 for now - this doesn't special-case NaT. Will think if there's a better way to do it, feel free to close / take over if it's wrong"
564829913,31955,add eval examples,MarcoGorelli,closed,2020-02-13T17:11:26Z,2020-02-25T17:11:43Z,"xref #31952 

Screenshot for `pandas.eval`:
![image](https://user-images.githubusercontent.com/33491632/74459941-04b76980-4e84-11ea-9f1b-2b6ad9cb5494.png)

Screenshot for `pandas.DataFrame.eval`:
![image](https://user-images.githubusercontent.com/33491632/74460007-23b5fb80-4e84-11ea-931b-0f73eebb6725.png)
"
563219656,31876,CLN: Move info,MarcoGorelli,closed,2020-02-11T13:50:20Z,2020-02-25T17:11:47Z,"precursor to xref #31796. Have moved `DataFrame.info` code into `pandas/io/formats/info.py`, along with tests"
559892004,31659,check first and last points' labels are correct,MarcoGorelli,closed,2020-02-04T18:11:52Z,2020-02-25T17:12:16Z,"- [ ] closes #31580 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
557556066,31456,BUG: Groupby.apply wasn't allowing for functions which return lists,MarcoGorelli,closed,2020-01-30T15:07:16Z,2020-02-25T17:12:23Z,"- [ ] closes #31441 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
556144674,31388,ENH: truncate output of Groupby.groups,MarcoGorelli,closed,2020-01-28T11:22:57Z,2020-02-25T17:12:40Z,"Resurrection of #24853. Have resolved conflicts and updated according to the last review it received. The original PR fixed some unrelated minor typos, so I've kept them in (and added a fix for another absolutely minor typo I'd noticed)

- [x] closes #1135
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
548882119,30960,[BUG] -1 to the power of pd.NA was returning -1,MarcoGorelli,closed,2020-01-13T11:54:51Z,2020-02-25T17:13:10Z,"- [x] closes #30956 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
570207214,32225,Treatment of Pre-Epoch datetimes in windows,Jacob-Bishop,closed,2020-02-24T23:19:07Z,2020-02-25T18:49:01Z,"#### Code Sample
```python
import datetime
datetime.datetime(1953,1,23,0).astimezone(datetime.timezone(datetime.timedelta(days=-1, seconds=57600), 'Pacific Standard Time'))
```
#### Problem description
```pandas._libs.tslibs.tzconversion._tz_convert_tzlocal_utc``` calls ```dt.astimezone()```, which fails on Windows for pre-Epoch datetimes (see, e.g., code sample above) with ```OSError: [Errno 22] Invalid argument.``` This appears to be related to this issue https://github.com/dateutil/dateutil/issues/197, where time.localtime() can't take negative values on Windows.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

</details>

# Edit
The error appears to be in the datetutil module, not directly in datetime. See the stacktrace below:

<details>
Exception ignored in: 'pandas._libs.tslibs.tzconversion._tz_convert_tzlocal_utc'
Traceback (most recent call last):
  File ""C:\Users\jbishop\AppData\Roaming\Python\Python37\site-packages\dateutil\tz\_common.py"", line 144, in fromutc
    return f(self, dt)
  File ""C:\Users\jbishop\AppData\Roaming\Python\Python37\site-packages\dateutil\tz\_common.py"", line 258, in fromutc
    dt_wall = self._fromutc(dt)
  File ""C:\Users\jbishop\AppData\Roaming\Python\Python37\site-packages\dateutil\tz\_common.py"", line 222, in _fromutc
    dtoff = dt.utcoffset()
  File ""C:\Users\jbishop\AppData\Roaming\Python\Python37\site-packages\dateutil\tz\tz.py"", line 222, in utcoffset
    if self._isdst(dt):
  File ""C:\Users\jbishop\AppData\Roaming\Python\Python37\site-packages\dateutil\tz\tz.py"", line 291, in _isdst
    dstval = self._naive_is_dst(dt)
  File ""C:\Users\jbishop\AppData\Roaming\Python\Python37\site-packages\dateutil\tz\tz.py"", line 260, in _naive_is_dst
    return time.localtime(timestamp + time.timezone).tm_isdst
OSError: [Errno 22] Invalid argument

</details>
"
570126372,32221,CI: Remove float16 fixture value,SaturnFromTitan,closed,2020-02-24T20:24:56Z,2020-02-25T18:51:35Z,"- [x] xref #32220
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Hopefully, this fixes the flaky test
"
538692708,30291,Docs request: Specifying behavior of `dtype` parameter,christina-zhou-96,open,2019-12-16T22:04:19Z,2020-02-25T18:55:06Z,"#### Code Sample, a copy-pastable example if possible

```python
# My data comes in with some empty IDs, but these rows are still usable.
data_with_nan = {'ID':['1000','1001','1002', np.nan],
                 'Price':['900','100','150', '600']}

# I set dtype to str.
df_with_nan = pd.DataFrame(data_with_nan,dtype=str)

# That nan cell is still arriving as a float,
type(df_with_nan.iloc[3,0])

# type(df_with_nan.iloc[3,0])
# <class 'float'>

# and perceived as a nan in pandas.
df_with_nan.notnull()

#       ID  Price
# 0   True   True
# 1   True   True
# 2   True   True
# 3  False   True

```
#### Problem description
`dtype` does not explicitly say that it ignores NaN in the documentation of

-  `pd.DataFrame`
-  `read_excel`
-  `read_csv`

For instance, in `read_csv` docs [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html), this is the description of the `dtype` param:

> dtype : Type name or dict of column -> type, optional
> 
>     Data type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32, ‘c’: ‘Int64’} Use str or object together with suitable na_values settings to preserve and not interpret dtype. If converters are specified, they will be applied INSTEAD of dtype conversion.

For the longest time, I kept reading in my dataframes as `str` in the belief that it would convert `nan` to ""NaN"". This, obviously, drained performance. The docs don't explicitly say that specifying a dtype ignores the null values. I don't know if I'm just oblivious, but that would've helped me several months ago.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 85 Stepping 4, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.0.3
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

</details>
"
570254710,32229,32 bit CI Failures,WillAyd,closed,2020-02-25T01:43:33Z,2020-02-25T18:55:56Z,"Haven't had time to debug but seeing this in the logs when setting up the 32 bit linux environment:

```sh
Traceback (most recent call last):
  File ""/home/vsts/miniconda3/bin/conda"", line 7, in <module>
    from conda.cli import main
ImportError: No module named conda.cli
```"
569398128,32188,BUG/ENH: MI.get_level_values and MI.set_levels get confused if level name happens to be int,charlesdong1991,closed,2020-02-22T21:49:24Z,2020-02-25T19:51:33Z,"While working on a PR, find out a confusion when level names are integer/None, e.g.

```python
>>> mi = pd.MultiIndex.from_frame(pd.DataFrame([[0.5,1],[0.5,0], [0.6, 0], [0.6,1]]), names=[None, 0])
>>> mi
MultiIndex([(0.5, 1),
            (0.5, 0),
            (0.6, 0),
            (0.6, 1)],
           )
>>> mi.get_level_values(0)
Int64Index([1, 0, 0, 1], dtype='int64', name=0)
>>> mi.set_levels([0.2, 0.3], level=0)
MultiIndex([(0.5, 0.3),
            (0.5, 0.2),
            (0.6, 0.2),
            (0.6, 0.3)],
           )
```
Pandas has quite big tolerrance of names setting, however, if names is set by int, and happens to be valid position, then using `set_levels` to change level value could be confusing, and also using level name to only change level values for `None` becomes difficult, because by default, `None` should be used to set values for all levels, the same happens if names is `[0, 1]`, e.g.

```python
>>> mi = pd.MultiIndex.from_frame(pd.DataFrame([[0.5,1],[0.5,0], [0.6, 0], [0.6,1]]), names=[1, 0])
>>> mi
MultiIndex([(0.5, 1),
            (0.5, 0),
            (0.6, 0),
            (0.6, 1)],
           names=[1, 0])
>>> mi.set_levels([0.2,0.3], level=0)
MultiIndex([(0.5, 0.3),
            (0.5, 0.2),
            (0.6, 0.2),
            (0.6, 0.3)],
           )
```

And yes, these are really corner cases, but might be nice to resolve to let users explicitly set/get level values! I am not sure what is the best way to figure it out, I am thinking of something like: `get_level_values_by_name(0)` or `get_level_values_by_loc(0)`, or have an argument in `get_level_values`, e.g. `get_level_values(0, by_name=True)`, default is `False` which uses loc to get values.

All feedbacks are very welcome, if there is kind of consensus, i would like to work on it!
@jreback @WillAyd @TomAugspurger @jorisvandenbossche 
"
570760290,32245,Backport PR #32241 on branch 1.0.x (CI: troubleshoot 32bit build),meeseeksmachine,closed,2020-02-25T18:48:04Z,2020-02-26T12:37:30Z,Backport PR #32241: CI: troubleshoot 32bit build
562501335,31848,"Query abs(column) == [1, 2] fails",klieret,open,2020-02-10T11:32:30Z,2020-02-25T20:17:31Z,"#### Code Sample, a copy-pastable example if possible

```python
df  = pd.DataFrame({""col"": [1, -1, 2, -2]})

# Works:
df.query(""abs(col) == 1"")

# Works:
df.query(""col == [1, -1, 2, -2]"")

# Fails:
df.query(""abs(col) == [1, 2]"")
```

traceback:

<details>

```
ValueError                                Traceback (most recent call last)
<ipython-input-42-1b78845d9a47> in <module>()
----> 1 df.query(""abs(col) == [1, 2]"")

~/.local/lib/python3.6/site-packages/pandas/core/frame.py in query(self, expr, inplace, **kwargs)
   3229         kwargs[""level""] = kwargs.pop(""level"", 0) + 1
   3230         kwargs[""target""] = None
-> 3231         res = self.eval(expr, **kwargs)
   3232 
   3233         try:

~/.local/lib/python3.6/site-packages/pandas/core/frame.py in eval(self, expr, inplace, **kwargs)
   3344         kwargs[""resolvers""] = kwargs.get(""resolvers"", ()) + tuple(resolvers)
   3345 
-> 3346         return _eval(expr, inplace=inplace, **kwargs)
   3347 
   3348     def select_dtypes(self, include=None, exclude=None) -> ""DataFrame"":

~/.local/lib/python3.6/site-packages/pandas/core/computation/eval.py in eval(expr, parser, engine, truediv, local_dict, global_dict, resolvers, level, target, inplace)
    335         eng = _engines[engine]
    336         eng_inst = eng(parsed_expr)
--> 337         ret = eng_inst.evaluate()
    338 
    339         if parsed_expr.assigner is None:

~/.local/lib/python3.6/site-packages/pandas/core/computation/engines.py in evaluate(self)
     71 
     72         # make sure no names in resolvers and locals/globals clash
---> 73         res = self._evaluate()
     74         return reconstruct_object(
     75             self.result_type, res, self.aligned_axes, self.expr.terms.return_type

~/.local/lib/python3.6/site-packages/pandas/core/computation/engines.py in _evaluate(self)
    112         scope = env.full_scope
    113         _check_ne_builtin_clash(self.expr)
--> 114         return ne.evaluate(s, local_dict=scope)
    115 
    116 

~/.local/lib/python3.6/site-packages/numexpr/necompiler.py in evaluate(ex, local_dict, global_dict, out, order, casting, **kwargs)
    832     _numexpr_last = dict(ex=compiled_ex, argnames=names, kwargs=kwargs)
    833     with evaluate_lock:
--> 834         return compiled_ex(*arguments, **kwargs)
    835 
    836 

ValueError: operands could not be broadcast together with shapes (4,) (2,) 
```

</details>

#### Problem description

A value error is raised for the query

#### Expected Output

Same as ``df.query(""col == [1, -1, 2, -2]"")``

#### Output of ``pd.show_versions()``

<details>

```

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 2.6.32-754.23.1.el6.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.17.1
pytz             : 2018.3
dateutil         : 2.6.1
pip              : 19.1.1
setuptools       : 38.5.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 1.7.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.1.1
html5lib         : 0.9999999
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 6.2.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.1.1
matplotlib       : 3.1.1
numexpr          : 2.6.9
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.5.1
tabulate         : 0.8.3
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.45.1
```

</details>
"
570237172,32228,"REF/TST: method-specific files for combine, update; parametrize",jbrockmendel,closed,2020-02-25T00:46:02Z,2020-02-25T20:46:53Z,
570232890,32227,"REF/TST: method-specific files for rename, reset_index",jbrockmendel,closed,2020-02-25T00:32:43Z,2020-02-25T20:47:36Z,
570269497,32230,REF/TST: method-specific files for DataFrame timeseries methods,jbrockmendel,closed,2020-02-25T02:32:51Z,2020-02-25T20:51:01Z,xref #32226.
550796561,31073,The IntervalIndex documentation makes no mention of IntervalIndex.get_loc,dumbledad,open,2020-01-16T13:08:16Z,2020-02-25T20:52:57Z,"#### Code Sample, a copy-pastable example if possible

```python
# No code, documentation issue
```
#### Problem description

Why doesn't the documentation about IntervalIndex ([here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.IntervalIndex.html)) contain any mention of the function [IntervalIndex.get_loc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.IntervalIndex.get_loc.html)?

#### Expected Output

I would expect the list of class methods to include all the methods.
"
542709009,30500,DEPR: Tick comparison with str,jbrockmendel,open,2019-12-27T01:25:05Z,2020-02-25T20:58:16Z,"ATM Tick offset comparisons try casting strings to offsets with to_offset before doing == or !=, but not for inequalities:

```
>>> m = pd.offsets.Minute(4)
>>> m == ""4min""
True
>>> m < ""5min""
TypeError: Invalid comparison between Minute and str
```

In order to make this behavior more consistent, we should deprecate allowing str casting for == and !=.  This would allow us to dispatch to the Timedelta inequalities, which have better handling for corner cases anyway (e.g. `m < np.ndarray([m, m]` which Timedelta gets right but Tick messes up)"
570218446,32226,REF/TST: method-specific files for Series timeseries methods,jbrockmendel,closed,2020-02-24T23:50:30Z,2020-02-25T23:23:59Z,cc @WillAyd @simonjayhawkins got the [go ahead](https://github.com/pandas-dev/pandas/pull/32110#issuecomment-589974610) the other day to take these off Jeff's plate
569572927,32211,CLN/REF: Split up / clean Categorical constructor tests,dsaxton,closed,2020-02-23T22:50:36Z,2020-02-25T23:44:53Z,
567861634,32120,test_show_versions failing locally,jbrockmendel,closed,2020-02-19T21:34:30Z,2020-02-26T01:37:31Z,"```
>       assert re.search(r""commit\s*:\s[0-9a-f]{40}\n"", result)
E       AssertionError: assert None
E        +  where None = <function search at 0x102703560>('commit\\s*:\\s[0-9a-f]{40}\\n', '\nINSTALLED VERSIONS\n------------------\ncommit           : None\npython           : 3.7.6.final.0\npython-bits     ...   : 0.14.1\nxlrd             : 1.2.0\nxlwt             : 1.3.0\nxlsxwriter       : 1.2.1\nnumba            : 0.47.0\n')
E        +    where <function search at 0x102703560> = re.search
```"
568610125,32139,TST: show_versions test with unmerged commits,jbrockmendel,closed,2020-02-20T22:19:56Z,2020-02-26T01:41:32Z,"- [x] closes #32120
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
570926457,32253,TST: method-specific file for combine,jbrockmendel,closed,2020-02-25T23:43:02Z,2020-02-26T02:02:57Z,
570851428,32250,TST: method-specific file for select_dtypes,jbrockmendel,closed,2020-02-25T21:40:07Z,2020-02-26T02:03:42Z,
570847545,32248,REF: method-specific file for to_timestamp,jbrockmendel,closed,2020-02-25T21:33:11Z,2020-02-26T02:04:08Z,"cc @MomIsBestFriend this is the only test we have for DataFrame.to_timestamp, but it was apparently written before the convention of ""small, focused tests"" took hold.  It would be helpful if this could be split/modernized."
570932785,32254,REF: move misplaced Series.append tests,jbrockmendel,closed,2020-02-25T23:53:10Z,2020-02-26T02:04:46Z,
570955380,32256,CLN tests.generic,jbrockmendel,closed,2020-02-26T00:28:31Z,2020-02-26T02:06:31Z,"Some of this is commented out or otherwise unneeded, other parts belong in test_internals."
569592189,32215,REF: include CategoricalIndex in index_cached parametrization,jbrockmendel,closed,2020-02-24T01:05:07Z,2020-02-26T02:14:14Z,"There is a lot of de-duplication to be done, but pytest-based habits dont quite work here.  So baby steps."

id,number,title,user,state,created_at,updated_at,body
597510065,33440,BUG: `weights` is not working for multiple columns in df.plot.hist,charlesdong1991,closed,2020-04-09T19:41:52Z,2020-04-10T17:00:00Z,"- [x] closes #33173 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
596372147,33389,"BUG: when adding multiple plots with different cmap, colorbars legends use first cmap",jorisvandenbossche,closed,2020-04-08T07:36:12Z,2020-04-10T17:00:49Z,"For example, when doing twice a scatter plot with a different colormap, it adds a colorbar for each, but with the wrong colors:

```
df = pd.DataFrame({'x': [1, 2, 3], 'y':  [1, 3, 2], 'c': [1, 2, 3]}
df['x2'] = df['x'] + 1   

import matplotlib.pyplot as plt 

fig, ax = plt.subplots()  
df.plot('x', 'y', c='c', kind='scatter', cmap=""cividis"", ax=ax)
df.plot('x2', 'y', c='c', kind='scatter', cmap=""magma"", ax=ax)   
```

![Figure_1_2](https://user-images.githubusercontent.com/1020496/78757174-59ef9300-797c-11ea-8d2b-a305582efb4b.png)
"
596550870,33392,"BUG/PLT: Multiple plots with different cmap, colorbars legends use first cmap",charlesdong1991,closed,2020-04-08T12:46:16Z,2020-04-10T17:00:52Z,"- [x] closes #33389 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Since this is a plot PR, just post what it looks like after the change

![Screen Shot 2020-04-08 at 3 43 54 PM](https://user-images.githubusercontent.com/9269816/78791204-d94a8a00-79af-11ea-9ebb-e216251c7908.png)
"
597654357,33446,BUG: Fix ValueError when grouping by read-only category (#33410),erik-hasse,closed,2020-04-10T02:12:32Z,2020-04-10T17:10:43Z,"- [x] closes #33410
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
597910244,33456,TYP: F used in decorators to _typing,simonjayhawkins,closed,2020-04-10T14:14:39Z,2020-04-10T17:10:57Z,xref #27404
584605422,32831,BUG: ExtensionBlock.set not setting values inplace,jbrockmendel,closed,2020-03-19T18:14:57Z,2020-04-10T17:11:36Z,"In trying to figure out the difference between Block.set vs Block.setitem I found that ExtensionBlock.set is not inplace like it is supposed to be.  Traced this back to a problem in CategoricalBlock.should_store, which this fixes+tests.

In separate passes I would like to
- rename set and setitem to something like ""setitem_inplace"" and ""setitem_newobj""
- ATM setitem is _sometimes_ inplace; I'd like to make that consistent."
595785812,33363,DEP: Bump min version of dateutil to 2.7.3,ShaharNaveh,closed,2020-04-07T11:08:22Z,2020-04-10T17:16:44Z,"- [x] xref https://github.com/pandas-dev/pandas/pull/32465#discussion_r404449469
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

Not sure where to put the note in the ""whatsnew"""
586504064,32949,CLN: remove unnecessary Series._convert calls,jbrockmendel,closed,2020-03-23T20:39:42Z,2020-04-10T17:23:12Z,"cc @WillAyd my confidence on the correctness here is only ""pretty sure"", so could use an extra set of eyes."
583242883,32782,BUG/API: prohibit dtype-changing IntervalArray.__setitem__,jbrockmendel,closed,2020-03-17T19:21:30Z,2020-04-10T17:25:46Z,"- [ ] <s>closes #27147</s> Not quite
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Still needs a dedicated test, but this is also a non-trivial API change, so I want to get the ball rolling on discussion.  cc @jschendel "
433809682,26109,Constructing PeriodIndex with string data,mfaafm,closed,2019-04-16T14:24:18Z,2020-04-10T17:29:56Z,"#### Code Sample, a copy-pastable example if possible

```python
# quarters as string representation
quarters = pd.Series([""2018Q1"", ""2018Q2"", ""2018Q1"", ""2018Q3""])

# gives TypeError (see details for stack trace)
pi = pd.PeriodIndex(quarters, freq=""Q"")
TypeError: Incorrect dtype

# works as intended
pi = pd.PeriodIndex(quarters.values, freq=""Q"")
```
#### Problem description
Creating a PeriodIndex with string data fails when using a pandas.Series but works if using the underlying np.array.

Is this expected behavior?
#### Output of stack trace
<details>

```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-6-1d88474891ea> in <module>
----> 1 pi = pd.PeriodIndex(quarters, freq=""Q"")

/usr/local/anaconda3/envs/PCS/lib/python3.7/site-packages/pandas/core/indexes/period.py in __new__(cls, data, ordinal, freq, start, end, periods, tz, dtype, copy, name, **fields)
    238             else:
    239                 # don't pass copy here, since we copy later.
--> 240                 data = period_array(data=data, freq=freq)
    241 
    242         if copy:

/usr/local/anaconda3/envs/PCS/lib/python3.7/site-packages/pandas/core/arrays/period.py in period_array(data, freq, copy)
    767         return PeriodArray._from_datetime64(data, freq)
    768     if isinstance(data, (ABCPeriodIndex, ABCSeries, PeriodArray)):
--> 769         return PeriodArray(data, freq)
    770 
    771     # other iterable of some kind

/usr/local/anaconda3/envs/PCS/lib/python3.7/site-packages/pandas/core/arrays/period.py in __init__(self, values, freq, dtype, copy)
    155             values = values._values
    156             if not isinstance(values, type(self)):
--> 157                 raise TypeError(""Incorrect dtype"")
    158 
    159         elif isinstance(values, ABCPeriodIndex):

TypeError: Incorrect dtype

```
</details>


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.2.final.0
python-bits: 64
OS: Darwin
OS-release: 18.2.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: en_US.UTF-8
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.1
pytest: None
pip: 19.0.3
setuptools: 40.8.0
Cython: 0.29.5
numpy: 1.15.4
scipy: 1.2.1
pyarrow: 0.11.1
xarray: None
IPython: 7.3.0
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.3
openpyxl: 2.6.1
xlrd: 1.2.0
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
594550051,33304,REGR: Fix construction of PeriodIndex from strings,dsaxton,closed,2020-04-05T16:45:53Z,2020-04-10T17:36:30Z,"- [ ] closes #26109
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
595454544,33342,BUG: scalar indexing on 2D DTA/TDA/PA,jbrockmendel,closed,2020-04-06T21:57:35Z,2020-04-10T17:40:19Z,"cc @jorisvandenbossche this fixes the issue with `DatetimeLikeBlockMixin.iget` discussed in #33252

Note: this sits on top of #33290."
579243293,32621,BUG: Replace in `string` series with NA,albertotb,closed,2020-03-11T12:54:56Z,2020-04-10T17:40:51Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np

rep = {'one': '1', 'two': '2'}
a = pd.Series(['one', 'two'], dtype='string')
b = pd.Series(['one', 'two', np.nan])
c = pd.Series(['one', 'two', np.nan], dtype='string')


# A: this works
a.replace(to_replace=rep)

# B: this also works
b.replace(to_replace=rep)

# C: this throws exception
c.replace(to_replace=rep)
# TypeError: Cannot compare types 'ndarray(dtype=object)' and 'str'
```
#### Problem description

`pandas.Series.replace` cannot be used in series of type `string` that contain `<NA>`

#### Expected Output

I would expect `C` to output the same as `B` but with `string` dtype

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-112-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : es_ES.UTF-8
LOCALE           : es_ES.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.4.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
597362459,33425,BUG: str.cat produces NaNs when others is an Index,brandon-b-miller,closed,2020-04-09T15:25:23Z,2020-04-10T17:47:25Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
print(pd.__version__)

sr = pd.Series(['a','b','c','d','e'])
others = pd.Index(['a','b','c','d','e'])

result = sr.str.cat(others=others)
print(result)

1.0.3
0    NaN
1    NaN
2    NaN
3    NaN
4    NaN
dtype: object


```

#### Problem description

The result should be the same as when `others` is a list or numpy array with the same values. The result is correct for pandas < 1.0. 

#### Expected Output

```
0    aa
1    bb
2    cc
3    dd
4    ee
dtype: object
```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-134-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.1.post20200322
Cython           : 0.29.15
pytest           : 5.4.1
hypothesis       : 5.7.0
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.0
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
595562010,33356,BUG/REF: unstack with EA dtypes,jbrockmendel,closed,2020-04-07T03:22:44Z,2020-04-10T17:48:07Z,"1) `ser.unstack(args)` should behave like `ser.to_frame().unstack(args).droplevel(level=0, axis=1)`.  This fails on master, is fixed by this PR.
2) `reshape._unstack_extension_series` currently re-implements logic that is in `ExtensionBlock._unstack`.  This de-duplicates by dispatching.
3) We currently transpose in cases where we shouldn't.  This implements `DataFrame._can_fast_transpose` to avoid that.
4) `test_astype_object_series` has what looks like a typo that is making us test not-the-intended-thing; @TomAugspurger can you confirm im reading this right?
5) The dtype comparisons in `test_astype_object_frame` currently raise, are commented out for exposition, should be fixed in a separate PR."
597403628,33429,DOC: Fix heading capitalization in doc/source/whatsnew - part3 (#32550),cleconte987,closed,2020-04-09T16:29:36Z,2020-04-10T17:48:10Z,"- [ ] Modify files v0.8.0.rst, v0.8.1.rst, v0.9.0.rst, v0.9.1.rst, v010.0.rst, v0.10.1.rst, v0.11.0.rst, v0.12.0.rst, v0.13.0.rst, v0.13.1.rst"
589858760,33125,"REF: reshape.concat operate on arrays, not SingleBlockManagers",jbrockmendel,closed,2020-03-29T18:57:11Z,2020-04-10T17:49:18Z,cc @TomAugspurger is there a better way to handle the assert_series_equal patch for the PandasArray tests?
593821979,33283,DOC: Fix EX01 in DataFrame.drop_duplicates,farhanreynaldo,closed,2020-04-04T12:15:30Z,2020-04-10T17:52:03Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. 

```
################################################################################
################################## Validation ##################################
################################################################################"
596770265,33403,DOC: Fix heading capitalization in doc/source/whatsnew - part2 (#32550),cleconte987,closed,2020-04-08T18:24:09Z,2020-04-10T17:54:28Z,"- [ ] Modify files v0.6.1.rst, v0.7.0.rst, v0.7.1.rst, v0.7.2.rst, v0.7.3.rst"
595713059,33359,PERF: improve IntegerArray fast constructor,jorisvandenbossche,closed,2020-04-07T09:07:13Z,2020-04-10T17:57:07Z,"Trying to improve the IntegerArray constructor from its constituents.

Example test case:

```
a = pd.array([1, 2, 3], dtype=""Int64"")
values = a._data
mask = a._mask 
pd.arrays.IntegerArray(values, mask)
```

```
In [5]: %timeit pd.arrays.IntegerArray(values, mask)  
2.9 µs ± 45.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)  # master
775 ns ± 41.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) # PR
```

So what is in the PR is *one possible* solution, to show the impact of using ""general dtype checking functions"" (that eg also need to deal with EA dtypes, getting the dtype of values being passed, etc). It is clear that here we can do a much more specialized check, and that this gives a considerable performance boost (in eg block/column-wise operations on IntegerArray, this is reconstructed many times). Some options:

- do an inline specialized check as done here
- define separate helper functions for those specialized checks (eg restricting to numpy dtypes)
- improve performance of `is_bool_dtype` / `is_integer_dtype` for those cases (maybe putting a check like this as a ""fastpath"" at the beginning of those functions might avoid more costly checks done in those functions for this specific case (eg ""is_bool_np_dtype"")
- simply don't do any validation at all in the IntegerArray constructor, and assume this is the responsibility of the caller
- provide a keyword in the IntegerArray constructor to turn off validation
- add a private constructor (similar as the `_simple_new` we have for others) that uses this fastpath

For optimal performance, having a way to don't do any validation at all (so also don't check the passed values are ndarray, or their dtypes) would actually even be better than what this PR does. But I also like the clean constructor we have now (eg no separate private constructor)"
593190957,33252,INT: provide helpers for accessing the values of DataFrame columns,jorisvandenbossche,closed,2020-04-03T07:53:42Z,2020-04-10T17:57:22Z,"Broken off from https://github.com/pandas-dev/pandas/pull/32867, also mentioned this in https://github.com/pandas-dev/pandas/pull/33229

In general, when doing certain things column-wise, we often just need the arrays (eg to calculate a reduction, to check the dtype, ..), and creating Series gives unnecessary overhead in that case. 

In this PR, I added therefore two helper functions for making it easier to do this (`_ixs_values` as variant on `_ixs` but returning the array instead of Series, and `_iter_arrays` that calls `_ixs_values` for each column iteratively as an additional helper function). 

I also used it in one place as illustration (in `_reduce`, what I was working on in https://github.com/pandas-dev/pandas/pull/32867, but it can of course be used more generally). 
In that example case, it is to replace an apply:

```
In [1]: df = pd.DataFrame(np.random.randint(1000, size=(10000,10))).astype(""Int64"").copy() 

In [2]: from pandas.core.dtypes.common import is_datetime64_any_dtype, is_period_dtype

In [3]: %timeit df.dtypes.apply(lambda x: is_datetime64_any_dtype(x) or is_period_dtype(x)) 
256 µs ± 2.66 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

In [5]: %timeit np.array([is_datetime64_any_dtype(values) or is_period_dtype(values) for values in df._iter_arrays()], dtype=bool) 
56.2 µs ± 282 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```

(and this is only fo 10 columns)

@jbrockmendel 
"
597394302,33427,CLN: avoid accessing private functions,jbrockmendel,closed,2020-04-09T16:13:57Z,2020-04-10T18:02:12Z,"xref #33394 

@simonjayhawkins thoughts on how to handle the mypy complaint?"
597513940,33441,BUG: Timedelta == ndarray[td64],jbrockmendel,closed,2020-04-09T19:49:45Z,2020-04-10T18:06:09Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

xref #33346

Between the two of these, we should be able to get rid of this kludge: https://github.com/pandas-dev/pandas/blob/master/pandas/core/internals/managers.py#L629

IIRC there were some test_coercion xfails related to that kludge, will be worth revisiting."
597460327,33436,BUG: Fix bug when concatenating Index of strings,dsaxton,closed,2020-04-09T18:07:20Z,2020-04-10T18:10:06Z,"- [ ] closes #33425
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
594689866,33308,REF: call _block_shape from EABlock.make_block,jbrockmendel,closed,2020-04-05T22:46:42Z,2020-04-10T18:13:17Z,
587380566,32995,BUG: DataFrame.diff(axis=1) with mixed (or EA) dtypes,jbrockmendel,closed,2020-03-25T01:31:54Z,2020-04-10T18:18:23Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
596798647,33404,BUG: Series.__getitem__ with MultiIndex and leading integer level,jbrockmendel,closed,2020-04-08T19:14:26Z,2020-04-10T18:18:56Z,"- [x] closes #33355
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
592308078,33230,DOC: Fixed examples in `pandas/core/groupby/`,ShaharNaveh,closed,2020-04-02T02:08:28Z,2020-04-10T19:22:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
596092287,33376,DOC: Fix capitalization among headings in doc/source/whatsnew (#32550),themien,closed,2020-04-07T19:15:37Z,2020-04-10T19:23:18Z,"Updated headers for the following files:

```
- [x] doc/source/whatsnew/index.rst
- [x] doc/source/whatsnew/v0.14.0.rst
- [x] doc/source/whatsnew/v0.15.0.rst
- [x] doc/source/whatsnew/v0.18.0.rst
- [x] doc/source/whatsnew/v0.18.1.rst
```"
596103382,33378,DOC: Fix capitalization among headings in doc/source/whatsnew (#32550),themien,closed,2020-04-07T19:35:47Z,2020-04-10T19:23:39Z,"Updated headers for the following files:

```
- [x] doc/source/whatsnew/v0.19.0.rst
- [x] doc/source/whatsnew/v0.20.0.rst
- [x] doc/source/whatsnew/v0.24.0.rst
- [x] doc/source/whatsnew/v0.25.0.rst
- [x] doc/source/whatsnew/v1.0.0.rst
```
"
586877928,32972,DOC: Change doc template to fix SA04 errors in docstrings #28792,dilex42,closed,2020-03-24T11:07:58Z,2020-04-10T19:25:43Z,"- [x] xref #28792

As requested in #32823 creating separate PR.
Also all descriptions for methods starts with upper case and it looks a little out of place. Should it be casted to lowercase perhaps?

"
597962466,33459,asv config to use `build_ext` instead of `build`,ShaharNaveh,closed,2020-04-10T15:59:30Z,2020-04-10T19:27:31Z,"Not 100% sure on this change, feel free to close at any time.

---

I have not seen a single mention in the docs of ```python setup.py build``` there was only mention of ```python setup.py build_ext```, so I thought that asv should follow that as well (I could be wrong here).
"
574604501,32408,Properly handle missing attributes in query/eval strings,alexmojaki,closed,2020-03-03T11:33:08Z,2020-04-10T19:31:01Z,"Consider this script:

```python
import pandas as pd

pd.eval(""pd.thing"")
```

Currently it raises an error like this:

```
  File ""/home/alex/work/pandas/pandas/core/computation/expr.py"", line 640, in visit_Attribute
    raise ValueError(f""Invalid Attribute context {ctx.__name__}"")
AttributeError: 'Load' object has no attribute '__name__'
```

Adding `__class__` to that line changes the error to the more sensible:

    ValueError: Invalid Attribute context Load

Re-raising the original error gives what's really needed:

```
  File ""/home/alex/work/pandas/pandas/core/computation/expr.py"", line 631, in visit_Attribute
    v = getattr(resolved, attr)
  File ""/home/alex/work/pandas/pandas/__init__.py"", line 260, in __getattr__
    raise AttributeError(f""module 'pandas' has no attribute '{name}'"")
AttributeError: module 'pandas' has no attribute 'thing'
```
"
586550555,32957,BUG: iloc on DataFrame[ea],jbrockmendel,closed,2020-03-23T22:12:29Z,2020-04-10T20:51:26Z,"```
from pandas.tests.extension.decimal.test_decimal import *
arr = data.__wrapped__()
ser = pd.Series(arr)
df = pd.DataFrame({""a"": ser})

>>> df.iloc[:, :1]
                                                   a
0  Decimal: 0.32737812402736643502265678762341849...
1                                                   
2                                                   
3                                                   

>>> df.iloc[:, :1].values
array([[Decimal('0.32737812402736643502265678762341849505901336669921875')]],
      dtype=object)
```

Maybe slicing on the wrong axis? "
597646312,33445,REF: remove replace_list kludge,jbrockmendel,closed,2020-04-10T01:43:16Z,2020-04-10T20:57:33Z,This sits on top of #33441
586558739,32959,"BUG: df.iloc[:, :1] with EA column",jbrockmendel,closed,2020-03-23T22:30:23Z,2020-04-10T20:58:16Z,"- [x] closes #32957
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
596859445,33407,REF: call pandas_dtype up-front in Index.__new__,jbrockmendel,closed,2020-04-08T20:59:44Z,2020-04-10T20:58:58Z,This leaves us in position to drop in dtype-only is_foo_dtype checks
592113377,33214,REF: .dot tests,jbrockmendel,closed,2020-04-01T18:21:13Z,2020-04-10T21:13:43Z,Fully parametrized over Series/DataFrame
591559720,33196,to_csv / to_pickle optionally use fast gzip compressionlevel=1,kernc,closed,2020-04-01T01:57:23Z,2020-04-10T22:21:10Z,"#### Code Sample, a copy-pastable example if possible

Enhancement proposal

```python
>>> df.to_csv('data.csv.gz')  # Awfully slow

>>> df.to_pickle('data.pkl.bz2')  # Awfully slow

>>> df.to_csv('data.csv.gz', fast=True)  # Uses fast compressionlevel=1

# or, better:

>>> pd.options.io.compressionlevel = 1

>>> df.to_pickle('data.pkl.bz2')  # Uses fast compressionlevel=1

...
```
#### Problem description

Compression of large objects in pandas is slow.

Popular evidence comparing compression levels for average payloads shows [[1]](https://catchchallenger.first-world.info/wiki/Quick_Benchmark:_Gzip_vs_Bzip2_vs_LZMA_vs_XZ_vs_LZ4_vs_LZO) [[2]](https://tukaani.org/lzma/benchmarks.html) that compressed size is usually far less variable than compression time, which most often spans several folds.
`compressionlevel=1` is _orders of magnitude_ faster, whereas `compressionlevel=9` is only 10% smaller.

One optimizes for size, the other for speed, and much fewer people ever need something in between.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

1.1.0.dev0+786.gec7734169

</details>
"
596665815,33398,ENH: Support passing compression args to gzip and bz2,jessefarnham,closed,2020-04-08T15:28:40Z,2020-04-10T22:21:44Z,"This commit closes #33196 but takes a more generic approach than the suggested
solution. Instead of providing a 'fast' kwarg or global compression level
setting, this commit extends the ability to pass compression settings as a
dict to the gzip and bz2 compression methods. In this way, if the user
wants faster compression, they can pass
compression={'method': 'gzip', 'compresslevel'=1} rather than
just compression='gzip'.

Note: For the API to be consistent when passing  paths vs. filelikes, GZipFile and gzip2.open() must accept the same kwargs.

- [x] closes #33196
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
598009338,33461,CI: fix lint issue,simonjayhawkins,closed,2020-04-10T17:45:51Z,2020-04-11T08:36:45Z,"https://github.com/pandas-dev/pandas/runs/577244968

Linting .py code
##[error]./pandas/core/internals/blocks.py:11:1:F401:'pandas._libs.tslibs.Timedelta' imported but unused
Linting .py code DONE"
592137947,33216,Added option to include/ignore file extensions in `scripts/validate_unwanted/patterns.py`,ShaharNaveh,closed,2020-04-01T19:04:40Z,2020-04-11T12:59:39Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

This is needed for the check in #32942 to ignore cython files."
598276678,33481,CI: Github actions error,ShaharNaveh,closed,2020-04-11T14:26:33Z,2020-04-11T14:34:20Z,"Noticed on #33479 and #33480

Seems like github actions have an internal error.

The error message:

```
This check failed
```"
589904632,33135,BUG: fix ngroups and len(groups) inconsistency when using [Grouper(freq=)] (GH33132),falcaopetri,closed,2020-03-29T22:52:13Z,2020-04-11T15:29:31Z,"- [ ] closes #33132
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
598057238,33464,CLN: Nitpicks,ShaharNaveh,closed,2020-04-10T19:33:41Z,2020-04-11T18:54:14Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
585541594,32890,BUG: Fix replacing in `string` series with NA (pandas-dev#32621),chrispe,closed,2020-03-21T17:21:51Z,2020-04-12T08:04:40Z,"The pd.NA values are replaced with np.nan before comparing the arrays/scalars

- [X] closes #32621 
- [X] tests passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
598137635,33471,REF: Simplify __getitem__ by doing positional-int check first,jbrockmendel,closed,2020-04-10T23:22:34Z,2020-04-12T20:31:02Z,"This gets rid of the last non-test usage of `Index.get_value`, so that can be deprecated, xref #19728

This focuses only on `__getitem__`; `__setitem__` is tougher for a few reasons, including #33469.
"
591933185,33206,"ENH: Add accessor for isocalendar returning iso year, week, and day",mgmarino,closed,2020-04-01T13:50:54Z,2020-04-12T21:34:32Z,"This enhancement request is to add an accessor to DatetimeProperties for the ISO year, which is the year corresponding to the week calculated via ISO 8601.  

Currently, we can access the ISO week via `week` and `weekofyear`, *and* `Timestamp` objects provide `isocalendar`, which returns the correct ISO year, week, and day.


### Examples
For example:
```python
df = pandas.DataFrame()

df[""event_date""] = [""2020-01-01"", ""2019-12-31""]
df[""event_date""] = df.event_date.astype(""datetime64[D]"")

df.event_date.dt.weekofyear.values, df.event_date.dt.year.values
```

outputs:

```python
(array([1, 1]), array([2020, 2019]))
```

and accessing the Timestamp objects

```python
df.event_date[0].isocalendar(), df.event_date[1].isocalendar()
```

yields

```python
((2020, 1, 3), (2020, 1, 2))
```

It would be great to have an accessor to get access this isocalendar year value for a series, e.g.:

```python
df.event_date.dt.year_for_weekofyear.values
```

```python
array([2020, 2020])
```

or *maybe even better* to have an isocalendar accessor where the same iso values are available like in `Timestamp.isocalendar`, e.g.

```python
df.event_date.dt.isocalendar.values
```

```python
array([[2020, 1, 3], [2020, 1, 2]]))
```

I am happy to try a PR here, but some input would be great as to:

- which direction to proceed
- naming, etc."
596668150,33399,DOC: DatetimeTZDtype astype behavior with Series.astype,Loupiol,closed,2020-04-08T15:31:55Z,2020-04-12T21:47:12Z,"#### Code Sample

```python
import pandas as pd

typ = pd.DatetimeTZDtype(tz = 'EST')
s = pd.Series([pd.Timestamp('2020-01-01 15:00')])
print(s.astype(typ))

```

```
0   2020-01-01 10:00:00-05:00
dtype: datetime64[ns, EST] 
```

#### Problem description
Hello,

I noticed a strange behavior using `DatetimeTZDtype` , when specifying a timezone as argument. When the type is applied to a tz-naive datetime column using `astype`, it will first localize the timestamps to `UTC` and then convert the timezone, whereas I was expecting it would just localize my tz-naive timestamps. 

My situation is I am getting data from a database and need to convert the columns to the desired types. Using  `DatetimeTZDtype` and `astype` is a nice way to convert and localize in just one line, but I encountered this unexpected behavior (though it is easy to work around that).

I tracked the issue to these lines [https://github.com/pandas-dev/pandas/blob/master/pandas/core/internals/blocks.py#L2090](url)

Thanks
#### Expected Output
```
0   2020-01-01 15:00:00-05:00
dtype: datetime64[ns, EST]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 13, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : 0.29.16
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0


[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
596640646,33397,CI: pandas warnings in tests about keyword only arguments,TomAugspurger,closed,2020-04-08T14:53:38Z,2020-04-12T21:48:32Z,"e.g. https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=32821&view=logs&j=76104ccd-8dcc-5006-a17c-28bcdd709542&t=cd420693-444f-5c55-14e0-1be238e189a1

```
pandas\tests\io\test_html.py:886
  D:\a\1\s\pandas\tests\io\test_html.py:886: FutureWarning: Starting with Pandas version 2.0 all arguments of read_html except for the argument 'io' will be keyword-only
    result = self.read_html(data, ""Arizona"", index_col=0)[0]
```

We'll need to track down places where we're passing positional arguments like https://github.com/pandas-dev/pandas/blob/def6f6d67c18d6b76ef79defd9e3bd86385ef9de/pandas/tests/io/test_html.py#L886 and update them to use keyword arguments. That may be the only one, not sure."
598277986,33482,TST: Don't use deprecated version of read_html,LTe,closed,2020-04-11T14:33:30Z,2020-04-12T21:48:37Z,"Usage of function read_html is marked as deprecated when used with
positional arguments. In case when we do not explicitly check the
warning message with assert_produces_warning we should use non-deprecated
version.

This will resolve #33397

To identify candidates to change I modified decorator to raise an `Exception`

```diff
--- a/pandas/util/_decorators.py
+++ b/pandas/util/_decorators.py
@@ -294,6 +294,7 @@ def deprecate_nonkeyword_arguments(
                     ""Starting with Pandas version {version} all arguments of {funcname}""
                     ""{except_args} will be keyword-only""
                 ).format(version=version, funcname=func.__name__, except_args=arguments)
+                raise Exception(""keyword argument test"")
                 warnings.warn(msg, FutureWarning, stacklevel=stacklevel)
             return func(*args, **kwargs)
```

Related test build https://dev.azure.com/piotrnielacny/piotrnielacny/_build/results?buildId=2&view=ms.vss-test-web.build-test-results-tab

- [x] closes #33397
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] ~whatsnew entry~"
573523736,32383,pd.read_json() leads to a segmentation fault,gambingo,closed,2020-03-01T10:37:30Z,2020-04-12T21:50:14Z,"#### Code Sample

The following code leads to either a segmentation fault or a bus error:
```python
def load_dataframe(name):
    ...
    df_json = db.dataframes.find_one({""_id"": name})[""df""]
    df = pd.read_json(df_json)
    return df
```

But the following workaround fixes it:
```python
def load_dataframe(name):
    ...
    df_json = db.dataframes.find_one({""_id"": name})[""df""]
    df = pd.DataFrame().from_dict(json.loads(df_json))
    return df
```

#### Problem description
I am storing several jsonified dataframes in MongoDB. Trying to go straight from the json to the dataframe leads to either a segmentation fault or a bus error. I am storying several dataframes, but confusingly it only happens with one dataframe and not the others. The problem dataframe is identical in nature to other dataframes (same column names and data types). Unfortunately, it's not shareable but happy to answer any questions I can.

#### Output of ``pd.show_versions()``
 Note: I am using the latest pandas, 1.0.1. I downgraded to pandas 0.25.3 and had the same issue.

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.0.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 17.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
598583730,33504,WIP: CI: Setup 3.9 Travis Build,alimcmaster1,closed,2020-04-12T22:18:12Z,2020-04-12T22:20:13Z,"- [x] ref https://github.com/pandas-dev/pandas/issues/33395

- In a very similar fashion to @jbrockmendel work for 3.8 - might be useful to have start a 3.9 build so we can start eliminating the issues @TomAugspurger mentioned in https://github.com/pandas-dev/pandas/issues/33395"
595645271,33358,DatetimeIndex.to_period with freq,yohplala,closed,2020-04-07T07:12:43Z,2020-04-12T23:00:01Z,"- [X] I have checked that this issue has not already been reported.
- [X] I have confirmed this bug exists on the latest version of pandas (1.0.3).
---

#### Code Sample

```python
# Input date range:
pi5m = pd.date_range(start='2019-12-22 06:40:00+00:00', end='2019-12-22 08:45:00+00:00', freq='5min')

# Converting to PeriodIndex
pi5m.to_period()

  File ""/home/p/.local/lib/python3.7/site-packages/pandas/core/arrays/period.py"", line 938, in dt64arr_to_periodarr
    freq = Period._maybe_convert_freq(freq)

  File ""pandas/_libs/tslibs/period.pyx"", line 1580, in pandas._libs.tslibs.period._Period._maybe_convert_freq

AttributeError: 'NoneType' object has no attribute 'n'

# Working
pi5m.to_period('5min')

```

#### Problem description

It is specified in documentation that if not provided, `freq` is inferred.
However, with above-provided input, I get the error message indicated.
Is this a bug?

Thanks for your feedback.
Bests,"
597526623,33442,ENH: Add prod to masked_reductions,dsaxton,closed,2020-04-09T20:14:40Z,2020-04-12T23:21:57Z,"Adding prod to /core/array_algos/masked_reductions.py and using them for IntegerArray and BooleanArray. This also seems to offer a decent speedup over nanops like the other reductions:

```python
# Branch

[ins] In [3]: %timeit arr.prod()  # arr = pd.Series([None, 0, 1, 2] * 10_000, dtype=""Int64"")
102 µs ± 379 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)

[ins] In [5]: %timeit arr.prod()  # arr = pd.Series([0, 0, 1, 2] * 10_000, dtype=""Int64"")
82.6 µs ± 752 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)

# Master

[ins] In [4]: %timeit arr.prod()  # arr = pd.Series([None, 0, 1, 2] * 10_000, dtype=""Int64"")
291 µs ± 6.23 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

[ins] In [6]: %timeit arr.prod()  # arr = pd.Series([0, 0, 1, 2] * 10_000, dtype=""Int64"")
78.6 µs ± 2.5 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```"
598130871,33470,DOC: timezone conversion example added to pandas.Series.astype doc #33399,venkateshdatta1993,closed,2020-04-10T23:00:35Z,2020-04-13T01:58:00Z,"…3399

- [x] closes #33399 

Have added an example in the documentation page of pandas.Series.astype for datetime type with timezone."
598521093,33496,BUG: Allow apply of ufunc over Series of list-like,dsaxton,closed,2020-04-12T16:25:50Z,2020-04-13T02:04:25Z,"- [ ] closes #33492
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
598631706,33510,Issue 31175,echozzy629,closed,2020-04-13T02:29:08Z,2020-04-13T04:16:25Z,"- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
292100373,19422,I can't convert 1H OHLC data into monthly data with a specific time by loffset,yomoko,closed,2018-01-27T07:33:27Z,2020-04-13T14:20:47Z,"https://github.com/pandas-dev/pandas/issues/13218
https://github.com/pandas-dev/pandas/issues/13933
With regard to the above issues, I have the same problem. I need to convert 1 hour to monthly data, the requirement is that I need to set the beginning time interval to 17:00.

`ohlc_dict = {'open':'first', 'high':'max', 'low':'min', 'close': 'last'}`
`df = df.resample('1M', loffset='17H',closed='left',label='left').agg(ohlc_dict)`
I specified the time by `loffset='17H`' already, it just changed time label to 17:00, but it still took the Open Price from 00:00 instead of 17:00. What can I do to the get open price at 17:00 instead of 00:00? Thanks."
573920169,32393,DOC: change link to contributing guide in README.md,simonjayhawkins,closed,2020-03-02T11:47:08Z,2020-04-13T14:39:34Z,xref https://github.com/pandas-dev/pandas/pull/32068#issuecomment-593362224
594455141,33301,DOC/CLN: remove versionadded/changed:: 0.21,simonjayhawkins,closed,2020-04-05T13:26:44Z,2020-04-13T14:41:51Z,xref #29126
598877229,33523,CLN: redundant code in IntegerArray._reduce,simonjayhawkins,closed,2020-04-13T13:03:07Z,2020-04-13T15:08:39Z,
598986301,33527,CI: xfail rank tests,jbrockmendel,closed,2020-04-13T16:26:11Z,2020-04-13T16:54:59Z,"With npdev installed, the affected tests arent failing for me on OSX, so this is a shot int he dark."
592187912,33220,ENH: Add isocalendar accessor to DatetimeIndex and Series.dt,mgmarino,closed,2020-04-01T20:35:15Z,2020-04-13T20:01:43Z,"This PR adds the the isocalendar property to `DatetimeIndex` and the corresponding `Series.dt` accessor.    The property returns a DataFrame, e.g.:

```python
>>> idx = pd.date_range(start='2019-12-29', freq='D', periods=4)
>>> idx.isocalendar
   year  week  day
0  2019    52    7
1  2020     1    1
2  2020     1    2
3  2020     1    3
```

and 

```python
>>> pandas.to_datetime(pandas.Series([""2020-01-01""])).dt.isocalendar
   year  week  day
0  2020     1    
>>> pandas.to_datetime(pandas.Series([""2019-12-31""])).dt.isocalendar.week
0    1
Name: week, dtype: int32
```

The behavior is consistent with `Timestamp.isocalendar` and `datetime.date.isocalendar`.

For more information about ISO 8601 calendar, see e.g. https://en.wikipedia.org/wiki/ISO_week_date.

Address GH33206

- [x] closes #33206 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Note that I am very happy to rename the field, but I wanted to go ahead and open up the PR.
"
599164738,33534,CI: pin cython==0.29.16 in npdev build,jbrockmendel,closed,2020-04-13T22:07:32Z,2020-04-13T22:42:28Z,xref #33507
589590667,33099,DOC iris.csv file has moved,benabel,closed,2020-03-28T14:47:04Z,2020-04-14T02:14:36Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
597681288,33447,REF: de-nest Series getitem/setitem checks,jbrockmendel,closed,2020-04-10T03:59:45Z,2020-04-14T03:17:57Z,"ATM the first checks we do in `Series.__getitem__` and `__setitem__` are for `get_loc` with some not-totally-straightforward try/except logic.

This moves the slice case and the com.is_bool_indexer case up, and aligns the ordering of the checks between getitem/setitem.

Getting the com.is_bool_indexer case out from the try/except in setitem is particularly nice.

That said, this _may_ not be what we want to do.  This will very slightly improve performance in the cases moved up, and equally slightly harm performance in the scalar case.  AFAIK the current ordering is not based on optimizing one case over the others."
367474373,23018,CLN: Use consistent  imports in tests,h-vetinari,closed,2018-10-06T16:47:38Z,2020-04-14T05:06:56Z,"A while ago, I got the review from @jreback that:
* Replace instances of pd. with direct imports (from review https://github.com/pandas-dev/pandas/pull/21899#discussion_r202524353):
  > don’t import pd, directly import instead
* Replace direct use of `assert_..._equal` with the `tm.assert_..._equal`
    (from review https://github.com/pandas-dev/pandas/pull/21899#discussion_r202524374):
  > use tm; don’t import assert_series_equal

I kept applying these rules until #22730, where @jreback then said:
> Before we do any more of this
[quote of the above]
I would like a lint rule to see what is what (and basically list the offenders as exclusions for now).
We have been changing things like this for a long time and I think we somtimes change them back (e.g. to use the imports of `assert_*`, rather than `tm.`, and to use the `pd.*` rather than the direct imports.

and further:
> let's see which way is more common, then have a lint rule to enforce it, rather than ad-hoc conversions (which has been the policy up till now).

Opening this issue for someone to write such a lint rule, because I wouldn't know where to start."
591825824,33203,STY/TST: what standard to follow to import top-level objects in tests,jorisvandenbossche,open,2020-04-01T10:59:39Z,2020-04-14T05:07:21Z,"So this discussion regularly comes up (latest one being https://github.com/pandas-dev/pandas/pull/33184), so let's try to decide which one of the two we prefer.

In test code, do we do:

```
from pandas import Series
...
Series(...)
```

or 

```
import pandas as pd
...
pd.Series(..)
```

Personally, I prefer the second, and from recent discussions I have the impression @WillAyd and @datapythonista do as well (is that correct?)

To be clear: consistency within a single file is most important (so not mixing Series and pd.Series in a single file). 
But *when* someone does a PR to fix such inconsistencies in a file, it would be good if there is agreement on whether such a PR should replace `pd.Series` with `Series` or rather replace `Series` with `pd.Series`.
"
99313614,10755,to_csv writes wrong utf-16,jehuelsm,open,2015-08-05T22:31:10Z,2020-04-14T05:11:24Z,"Writing a DataFrame with utf-16 encoding adds garbage characters to the file:

``` python
import codecs
import pandas as pd


#This works
enc = 'utf-8'

print '\n\n',enc,'\n\n'

d = {'one' : pd.Series([1., 2., 3.], index=['a', 'b', 'c']),
     'two' : pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])}


df = pd.DataFrame(d)
print df,'\n\n'

df.to_csv('foo.csv', encoding = enc)

df = pd.read_csv('foo.csv', encoding = enc)
print df,'\n\n'

with codecs.open('foo.csv', encoding=enc, mode='r') as f:
    for line in f:
        print line

#this does not work       
enc = 'utf-16'
print '\n\n',enc,'\n\n'

d = {'one' : pd.Series([1., 2., 3.], index=['a', 'b', 'c']),
     'two' : pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])}


df = pd.DataFrame(d)
print df,'\n\n'

df.to_csv('foo.csv', encoding = enc)

df = pd.read_csv('foo.csv', encoding = enc)
print df,'\n\n'

#prints
#  Unnamed: 0  one  two???????????
#  0          b    2  2.0???????????
#  1          d  NaN            4.0? 

with codecs.open('foo.csv', encoding=enc, mode='r') as f:
    for line in f:
        print line
#prints
#,one,two਍愀Ⰰ㄀⸀　Ⰰ㄀⸀　ഀ
#b,2.0,2.0਍挀Ⰰ㌀⸀　Ⰰ㌀⸀　ഀ
#d,,4.0਍

```

Versions:

```
pandas.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.9.final.0
python-bits: 32
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel
byteorder: little
LC_ALL: None
LANG: de_DE

pandas: 0.16.1
nose: 1.3.4
Cython: 0.21.1
numpy: 1.8.2
scipy: 0.15.1
statsmodels: 0.6.1
IPython: 2.3.1
sphinx: 1.2.3
patsy: 0.3.0
dateutil: 2.3
pytz: 2014.10
bottleneck: 0.8.0
tables: 3.1.1
numexpr: 2.4
matplotlib: 1.4.2
openpyxl: 2.1.3
xlrd: 0.9.3
xlwt: 0.7.5
xlsxwriter: None
lxml: 3.4.1
bs4: 4.3.2
html5lib: 0.999
httplib2: None
apiclient: None
sqlalchemy: 0.9.8
pymysql: None
psycopg2: None
```
"
598590859,33506,Removed xfail for issue #31883,devjeetr,closed,2020-04-12T23:05:53Z,2020-04-14T15:01:00Z,"I removed the xfail in lines 412-414 as discussed in https://github.com/pandas-dev/pandas/issues/31883.

- [x] closes #31883
- [x] 0 tests added / 0 passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
598645582,33512,TST/CLN: Clean categorical min / max tests,dsaxton,closed,2020-04-13T03:21:12Z,2020-04-14T17:58:58Z,"Fixes one small error in a test, and otherwise tries to remove some code duplication"
598401585,33490,Implement pd.NA checking function,dsaxton,closed,2020-04-12T03:30:26Z,2020-04-14T18:04:30Z,"- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This may already exist in some form, but if not I think it could be useful to have a fast function for checking specifically for the presence of pd.NA (e.g. if we're looking at an arbitrary array and don't know if we'll have access to a _mask attribute, or if we're handling some other data type that uses pd.NA for missing values but doesn't have this attribute)."
598831544,33520,TYP: remove assert mask is not None for mypy,simonjayhawkins,closed,2020-04-13T11:16:19Z,2020-04-14T19:31:22Z,
599708778,33546,TST: rename checknull_old -> test_checknull_old,simonjayhawkins,closed,2020-04-14T16:41:32Z,2020-04-14T19:32:42Z,
595480646,33345,TST: add read_json test #GH32383,BenjaminLiuPenrose,closed,2020-04-06T23:03:42Z,2020-04-14T20:06:20Z,"- [x] closes #32383
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
584701580,32838,DataFrame.replace fails to replace value when columns are specified and only non-replacement columns contain pd.NA,tsoernes,closed,2020-03-19T21:09:43Z,2020-04-14T20:33:19Z,"#### Code Sample, a copy-pastable example if possible

```python
In [87]: df2 = pd.DataFrame([['a', 1], ['b', pd.NA]])

In [108]: df2
Out[108]: 
   0     1
0  a     1
1  b  <NA>

In [109]: df2.replace({0: 'a'}, np.nan)
/home/torstein/anaconda3/lib/python3.7/site-packages/pandas/core/missing.py:47: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  mask = arr == x
Out[111]: 
   0     1
0  a     1
1  b  <NA>

In [112]: df2.dtypes
Out[112]: 
0    object
1    object
dtype: object

# This works:
In [121]: df2[0].replace('a', np.nan)
Out[121]: 
0    NaN
1      b
Name: 0, dtype: object

# This also works:
In [126]: pd.DataFrame([['a', 1], ['b', np.nan]]).replace({0: 'a'}, 'b')
Out[126]: 
   0    1
0  b  1.0
1  b  NaN


```
#### Problem description

There are multiple similar issues, but in this case, there are no NaNs in the column specified by the replacement-dictionary. If the dataframe is created without additional column `1` with a `pd.NA`, even though no replacement is performed on that column, then replace works.

#### Expected Output

'a' to be replaced with nan
#### Output of ``pd.show_versions()``

<details>
pandas           : 1.0.2
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.6.0.post20191030
Cython           : 0.29.13
pytest           : 5.2.2
hypothesis       : None
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.2
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 2.2.3
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.2.2
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.10
tables           : 3.5.2
tabulate         : 0.8.5
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.2
numba            : 0.46.0

</details>
"
599801617,33547,"Spelling error in docs, Getting Started, section 03",skregas,closed,2020-04-14T19:19:41Z,2020-04-14T22:05:53Z,"![spelling error in docs](https://user-images.githubusercontent.com/1487974/79264572-1671b900-7e95-11ea-8707-49e4c596a70b.PNG)

In the Getting Started tutorials, section 03 ""How to get subsets from a DataFrame"", the paragraph beginning:

> We now from before...

should be:

> We know from before..."
599857590,33550,DOC: Fix some typos,dsaxton,closed,2020-04-14T21:03:16Z,2020-04-14T22:14:07Z,"Closes #33547, also Titanic should be capitalized"
599926102,33555,CLN: x-array test warnings,alimcmaster1,closed,2020-04-14T23:49:57Z,2020-04-15T00:33:23Z,"Remove these warnings:

```
pandas/tests/generic/test_to_xarray.py:102
  /home/travis/build/pandas-dev/pandas/pandas/tests/generic/test_to_xarray.py:102: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.
    s = Series(range(len(indices)), index=indices)
```

https://travis-ci.org/github/pandas-dev/pandas/jobs/675056013"
599004208,33529,CLN: .values->._values in hashing,jbrockmendel,closed,2020-04-13T16:59:51Z,2020-04-15T01:35:22Z,"This sits on top of #33511 (at first thought it could be orthogonal, but this breaks a couple of hash_scalar tests if we dont rip those out first)"
598641375,33511,CLN: remove unused util.hashing functions,jbrockmendel,closed,2020-04-13T03:06:31Z,2020-04-15T01:35:56Z,The only place where _any_ of util.hashing is used internally is in `CategoricalDtype.__hash__`.  It's pretty ugly dependency-tree wise.  This just prunes a couple of entirely unused functions.
598920881,33526,REF: simplify concat_datetime,jbrockmendel,closed,2020-04-13T14:27:46Z,2020-04-15T01:36:43Z,"By wrapping dt64/td64 ndarrays in DTA/TDA, we can get rid of a bunch of .astype(object) logic"
532526115,30038,API: how to handle NA in conversion to numpy arrays,jorisvandenbossche,closed,2019-12-04T08:31:24Z,2020-04-15T09:57:57Z,"In https://github.com/pandas-dev/pandas/pull/29964 and https://github.com/pandas-dev/pandas/pull/29961(using NA in IntegerArray and BooleanArray), the question comes up how to handle `pd.NA`'s in conversion to numpy arrays.

Such conversion occurs mainly in `__array__` (for `np.(as)array(..)`) and `.astype()`. For example:

```python
In [3]: arr = pd.array([1, 2, pd.NA], dtype=""Int64"")  

In [4]: np.asarray(arr) 
Out[4]: array([1, 2, None/pd.NA/..?], dtype=object)

In [5]: arr.astype(float)  
Out[5]: array([ 1.,  2., nan])  # <--- allow automatic NA to NaN conversion?
```

Questions that come up here:

- By default, when converting to object dtype, what ""NA value"" should be used? Before this was `NaN` or `None`, now it could logically be `pd.NA`. 
  A possible reason to choose None instead of pd.NA is that third party code that needs a numpy array will typically not be able to handle pd.NA while None is much more normal. On the other hand, there is also still time for such third party code to adapt. And it will probably be good to keep `list(arr)` (iteration/getitem) and `np.array(arr, dtype=object)` consisetnt.

- When converting to a float dtype, are we fine to automatically convert `pd.NA` to `np.nan` ? Or do we think the user should explicitly opt in for this?

We will probably want to add a `to_numpy` to those Integer/BooleanArray to be able to make those choices explicit, eg with following signature:

```
def to_numpy(self, dtype=object, na_value=...):
    ... 
```

where you can explicitly say which value to use for the NAs in the final numpy array (and the `Series.numpy` can then forward such keyword). 
That way, a user can do `arr.to_numpy(dtype=object, na_value=None`) to get a numpy array with None instead of pd.NA, or `arr.to_numpy(dtype=float, na_value=np.nan)` to get a float array with NaNs.

But even if we have that function (which I think we should), the above questions about the *defaults* are still to be answered (eg for `__array__` we cannot have such a `na_value` keyword, so we need to make a default choice).

cc @TomAugspurger @Dr-Irv "
600242955,33563,CI: Fix Deprecation warning from jedi,charlesdong1991,closed,2020-04-15T11:58:31Z,2020-04-15T12:24:59Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
599941311,33556,fix,csaoma,closed,2020-04-15T00:36:34Z,2020-04-15T14:38:49Z,"- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
596159687,33382,DOC: Fix capitalisation in doc/source/whatsnew - part1 (issue #32550),cleconte987,closed,2020-04-07T21:21:14Z,2020-04-15T16:21:08Z,"- [ ] modify files index.rst, v0.4.x.rst, v0.5.0.rst, v0.6.0.rst

- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
595802162,33365,PERF: Regression in Series.is_monotonic_increasing for categorical,TomAugspurger,closed,2020-04-07T11:37:18Z,2020-04-15T17:41:38Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

Pandas 1.0.1

```python
In [2]: N = 1000

In [3]: c = pd.CategoricalIndex(list(""a"" * N + ""b"" * N + ""c"" * N))

In [4]: s = pd.Series(c)

In [5]: %timeit s.is_monotonic_increasing
35.3 µs ± 663 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```

Master

```python
In [5]: %timeit s.is_monotonic_increasing
73.6 µs ± 782 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```

#### Problem description

https://pandas.pydata.org/speed/pandas/#categoricals.IsMonotonic.time_categorical_series_is_monotonic_increasing"
588557559,33041,BUG: DataFrame.at implicitly assumes unique axes,jbrockmendel,closed,2020-03-26T16:34:24Z,2020-04-15T18:58:47Z,"Update: `DataFrame.lookup` is also affected

```
arr = np.random.randn(6).reshape(3, 2)
df = pd.DataFrame(arr, columns=[""A"", ""A""])

>>> df.at[0, ""A""]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pandas/core/indexing.py"", line 2069, in __getitem__
    return super().__getitem__(key)
  File ""pandas/core/indexing.py"", line 2034, in __getitem__
    return self.obj._get_value(*key, takeable=self._takeable)
  File ""pandas/core/frame.py"", line 2706, in _get_value
    series = self._get_item_cache(col)
  File ""pandas/core/generic.py"", line 3550, in _get_item_cache
    res = self._box_item_values(item, values)
  File ""pandas/core/frame.py"", line 2880, in _box_item_values
    return self._constructor(values.T, columns=items, index=self.index)
AttributeError: 'BlockManager' object has no attribute 'T'
```

Same problem for `df.at.__setitem__`.  `Series.at` looks OK."
588622707,33047,BUG: DataFrame.at with non-unique axes,jbrockmendel,closed,2020-03-26T18:09:35Z,2020-04-15T19:04:17Z,"- [x] closes #33041
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Closes #33041 in conjunction with #33045."
600364047,33565,REF: simplify broadcasting code,jbrockmendel,closed,2020-04-15T14:58:54Z,2020-04-15T19:29:48Z,
598349623,33486,REF: remove Block.concat_same_type,jbrockmendel,closed,2020-04-11T20:56:32Z,2020-04-15T19:34:21Z,All the relevant logic lives in concat_compat already.
588746046,33052,CLN: remove BlockManager.get,jbrockmendel,closed,2020-03-26T21:39:34Z,2020-04-15T19:37:32Z,"This should not be merged until after #33047 and #33045, since the assertions made here will not always hold until then."
598407038,33491,REF: de-duplicate code in libperiod,jbrockmendel,closed,2020-04-12T04:16:46Z,2020-04-15T19:38:19Z,"discussed a few months ago, this gets rid of `date_info_from_days_and_time` and instead re-uses `pandas_datetime_to_datetimestruct`"
599947568,33557,fstring updates. Changing from .format to fstring,csaoma,closed,2020-04-15T00:58:53Z,2020-04-15T19:40:48Z,"Please accept my updates by converting .format to fstrings.

https://github.com/pandas-dev/pandas/issues/29547
"
563374605,31886,np.ndarray.nonzero() doesn't know how to deal with pd.NA and throws type error,mind6,closed,2020-02-11T17:49:33Z,2020-04-16T00:00:52Z,"#### Code Sample, a copy-pastable example if possible

```python
np.where(pd.Series([True, False,pd.NA], dtype='boolean'))
```
#### Problem description
np.ndarray.nonzero() doesn't know how to deal with pd.NA and throws type error. 

#### Expected Output
(array([0], dtype=int64),)

Since pd.NA is a value that just doesn't exist, it should be ignored and skipped over by nonzero() when returning indices of nonzero elements. 

#### Actual Output
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-55-d3635393ceeb> in <module>
----> 1 np.where(pd.Series([True, False,pd.NA], dtype='boolean'))

<__array_function__ internals> in where(*args, **kwargs)

pandas\_libs\missing.pyx in pandas._libs.missing.NAType.__bool__()

TypeError: boolean value of NA is ambiguous

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United States.1252

pandas           : 1.0.1
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
599269656,33540,PERF: Fix performance regression #33365,rtlee9,closed,2020-04-14T03:28:12Z,2020-04-16T05:28:55Z,"Fix performance regression in Series.is_monotonic_increasing for categorical
by avoiding Categorical construction for categorical series

- [x] closes #33365
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry: perf regression was introduced after previous version
"
600784054,33584,TST: add message check to pytest.raises (tests/arrays/boolean/test_arithmetic.py),Dxin-code,closed,2020-04-16T06:35:01Z,2020-04-16T14:08:59Z,"- [ ] xref #30999
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
600734135,33579,solve bug #32580,echozzy629,closed,2020-04-16T04:12:36Z,2020-04-16T15:13:16Z,"…ns is specified.

- [ ] closes #32580
- [ ] 0 tests added / 0 passed
- [ ] passes `black pandas`

"
600667091,33578,DOC: Added check for standard,willie3838,closed,2020-04-16T00:32:03Z,2020-04-16T15:18:59Z,"- [ ] xref #32316
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This is my first time contributing, if I made any errors please tell me! I tried to continue what @joybhallaa  was working on. I made a check to see which files are not using the standard documentation."
570138111,32222,BUG: the sample skewness is wrong computed,anmyachev,closed,2020-02-24T20:48:21Z,2020-04-16T16:59:58Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Right formula can be found here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skew.html"
578447759,32575,TST: Add tests for duplicated and drop_duplicates,mproszewska,closed,2020-03-10T09:35:01Z,2020-04-16T17:15:04Z,"- [x] refers to  #15752
- [x] tests added / passed
- [x] tests duplicated and drop_duplicates for period, categorical, datetimes, timedelta "
104870279,10987,"convert integer to datetime bug, Python int too large to convert to C long",poc7667,closed,2015-09-04T11:05:18Z,2020-04-16T19:37:48Z,"# pandas version `pandas (0.16.1)`

```
for field in [""release_date"", ""date""]:
    if field in df:
        df[field] = pd.to_datetime(df[field], unit=""ms"")
```

![](https://i.imgur.com/GnNoqY7.png)
"
600773591,33580,Update pytables version,rbenes,closed,2020-04-16T06:10:51Z,2020-04-16T19:45:01Z,"Based on https://github.com/pandas-dev/pandas/pull/32700#pullrequestreview-391684059 I update pytables minimal required version to 3.4.3

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
584840125,32854,ENH: Add numba engine to groupby.transform,mroeschke,closed,2020-03-20T04:39:44Z,2020-04-16T19:57:01Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

In the same spirit of https://github.com/pandas-dev/pandas/issues/31845, adding `engine` and `engine_kwargs` arguments to `groupby.transform` (which was easier to tackle first than `groupby.apply`). This signature is the same as what was added to `rolling.apply`.

Constraints:

- The user defined function's first two arguments must be `def f(values, index, ...)`, explicitly those names, as we will pass in the the values and the pandas index (as a numpy array) into the udf"
546260139,30778,[WIP] style NA in reprs,TomAugspurger,closed,2020-01-07T12:35:03Z,2020-04-16T20:13:23Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
600596031,33573,BUG: DatetimeIndex.insert on empty can preserve freq,jbrockmendel,closed,2020-04-15T21:21:47Z,2020-04-16T20:59:42Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
542844810,30511,tz_localize creates freq inconsistency on time offset change,giuliobeseghi,closed,2019-12-27T11:29:50Z,2020-04-16T21:27:52Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

index = pd.date_range(""2019-3-31"", freq=""30T"", periods=10, tz=""Europe/London"")
print(index)  # this has a freq
print(index.freq)
print(index.tz_localize(None))  # this shouldn't have it anymore, but it does
print(index.tz_localize(None).freq)
```
#### Problem description

`tz_localize(None)` doesn't check if the frequency is still consistent with the new localized index. In the case of Daylight Saving Time switch, the frequency of the index should become None (since duplicated/missing timestamps are created).

#### Expected Output

Achievable with:

```python
new_index = index.tz_localize(None)
new_index.freq = new_index.inferred_freq  # drop frequency if not inferrable
print(new_index)  # this is correct
print(new_index.freq)
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 42.0.2.post20191203
Cython           : 0.29.14
pytest           : 5.3.2
hypothesis       : 4.54.2
sphinx           : 2.3.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.6
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.10.2
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : 0.4.0
scipy            : 1.3.2
sqlalchemy       : 1.3.11
tables           : 3.6.1
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.6

</details>
"
599901044,33553,BUG: tz_localize needs to invalidate freq,jbrockmendel,closed,2020-04-14T22:39:20Z,2020-04-16T21:45:51Z,"- [x] closes #30511
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Identified when trying to add `freq` check to `assert_index_equal` for DatetimeIndex and TimedeltaIndex."
142670575,12689,ERR: invalid error reporting when comparing vs. None,ghost,closed,2016-03-22T14:41:03Z,2020-04-16T21:46:21Z,"The ""in"" function does not work as expected in regard to a data frame. Let's say we have the following data frame :

`df = pd.DataFrame(np.random.randn(6,4), columns=list('ABCD'))`

We can test this data frame :

```
df is None
False
```

However we cannot test its presence in a list : 

```
df in [None]
TypeError: Could not compare [None] with block values
```

We would expect to get just False in this case.

(Pandas version 0.17.1)
"
565711139,31991,BUG: Fix wrong reading sparse matrix,m7142yosuke,closed,2020-02-15T07:29:20Z,2020-04-16T22:16:53Z,"- [x] closes #29814 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
592101028,33213,DOC: Added check for standard pandas reference,joybh98,closed,2020-04-01T18:00:21Z,2020-04-16T23:11:21Z,"- [x] xref #32316
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

As suggested by @datapythonista , I've added checks in `ci/code_checks.sh` to look if the pandas is being referenced in a standardized way i.e pandas, not *pandas* or Pandas"
598522565,33497,"CLN: assorted cleanups, annotations, de-privatizing",jbrockmendel,closed,2020-04-12T16:33:55Z,2020-04-17T02:53:56Z,
598515522,33495,CLN: remove ensure_categorical,jbrockmendel,closed,2020-04-12T15:56:39Z,2020-04-17T02:55:17Z,"only used in two places, its a misnomer because it really only ensures categorical dtype, and it complicates the dependency structure"
597587226,33444,DOC: Fix heading capitalization in doc/source/whatsnew - part4 (#32550),cleconte987,closed,2020-04-09T22:21:38Z,2020-04-17T02:55:52Z,"Add exceptions to the list in 'scripts/validate_rst_title_capitalization.py'

- [ ] Modify files in doc/source/whatsnew: v0.14.0.rst, v0.14.1.rst, v0.15.0.rst, v0.15.1.rst, v0.15.2.rst, v0.16.0.rst, v0.16.1.rst, v0.16.2.rst, v0.17.0.rst, v0.17.1.rst
- [ ] Modify files in scripts: validate_rst_title_capitalization.py"
599013211,33530,REF: matching-dtype case first in concat_compat,jbrockmendel,closed,2020-04-13T17:16:03Z,2020-04-17T02:55:53Z,Moving towards clarifying the EA._concat_same_type behavior with unique vs non-unique dtypes
601629849,33602,.format changed to f string,csaoma,closed,2020-04-17T00:59:56Z,2020-04-17T02:58:45Z,".format changed to f string 
https://github.com/pandas-dev/pandas/issues/29547
- [ ] closes #xxxx
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
601465606,33598,CLN: trim unreachable branches in _compare_or_regex_search,jbrockmendel,closed,2020-04-16T20:57:40Z,2020-04-17T02:58:54Z,
601469493,33599,"CLN: remove BlockManager._get_counts, get_dtype_counts",jbrockmendel,closed,2020-04-16T21:01:28Z,2020-04-17T03:00:30Z,
596712212,33400,PERF: fastpaths in is_foo_dtype checks,jbrockmendel,closed,2020-04-08T16:39:15Z,2020-04-17T03:01:07Z,"xref #33364, partially addresses #33368

For exposition I used the new type checking functions in parsers.pyx.

For the implementation I chose `.type` and `.kind` checks in order to avoid needing the imports from `dtypes.dtypes`, which has dependencies on a bunch of other parts of the code.

```
In [1]: import pandas as pd                                                                                                                                                                                                                                                                                                    
In [2]: from pandas.core.dtypes.common import *                                                                                                                                                                                                                                                                                
In [3]: cat = pd.Categorical([])                                       
In [4]: arr = np.arange(5)
                                                                                                                                                                         
In [5]: %timeit is_categorical_dtype(cat.dtype)                                                                                                                                                                                                                                                                                
1.57 µs ± 34.5 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)

In [6]: %timeit is_cat_dtype(cat.dtype)                                                                                                                                                                                                                                                                                        
316 ns ± 3.78 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)

In [7]: %timeit is_extension_array_dtype(cat.dtype)                                                                                                                                                                                                                                                                            
364 ns ± 5.39 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)

In [8]: %timeit is_ea_dtype(cat.dtype)                                                                                                                                                                                                                                                                                         
270 ns ± 7.31 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)

In [9]: %timeit is_extension_array_dtype(arr.dtype)                                                                                                                                                                                                                                                                           
759 ns ± 5.16 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)

In [10]: %timeit is_ea_dtype(arr.dtype)                                                                                                                                                                                                                                                                                        
199 ns ± 8.77 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
```"
600841308,33585,BUG: Raise a TypeError when record_path doesn't point to an array,LTe,closed,2020-04-16T08:19:35Z,2020-04-17T05:04:34Z,"When `record_path` points to something that is Iterable but is not
a sequence in JSON world we will receive odd results.

```
>>> json_normalize([{'key': 'value'}], record_path='key')
0
0  v
1  a
2  l
3  u
4  e
```

Based on RFC 8259 (https://tools.ietf.org/html/rfc8259) a JSON value MUST be
object, array, number, or string, false, null, true. But only two of them
should be treated as Iterable.

```
An object is an unordered *collection* of zero or more name/value
pairs, where a name is a string and a value is a string, number,
boolean, null, object, or array.

An array is an ordered *sequence* of zero or more values.

--
https://tools.ietf.org/html/rfc8259#page-3
```

Based on that `[{'key': 'value'}]` and `{'key': 'value'}` should not be
treated in the same way. In `json_normalize` documentation `record_path`
is described as `Path in each object to list of records`.

So when we want to translate JSON to Python like an object we need to take
into consideration a list (sequence). Based on that `record_path` should
point out to `list`, not `Iterable`.

In specs I added all possibile values that are allowed in JSON and
should not be treated as a collection. There is a special case for null
value that is already implemented.

|  type  |  value  | Iterable | Should be treated as list |
|--------|---------|----------|---------------------------|
| object | {}      | Yes      | No (unordered list)       |
| array  | []      | Yes      | Yes                       |
| number | 1       | No       | No                        |
| string | ""value"" | Yes      | No                        |
| false  | False   | No       | No                        |
| null   | Null    | No       | No (Check #30148)         |
| true   | True    | No       | No                        |

- [x] closes #26284
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
597740342,33448,Whitelist std and var for use with custom rolling windows,AlexKirko,closed,2020-04-10T07:21:48Z,2020-04-17T06:27:03Z,"- [X] xref #32865
- [X] 0 tests added / 0 passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
 ### The problem
While researching what funcitons are broken for #32865 , I added `std` and `var` to the list since their output didn't match numpy output. I have since discovered that this is because we default to the sample variance formula for all window calculations.
After closer examination, the algorithm itself turned out to be very robust against custom indexers. It is even resilient against non-monothonic window starts and ends.
There is nothing to do there, so we should revert blacklisting `std` and `var`. I don't think tests are necessary, since we aren't changing anything.
### Food for thought
Some background information to make our decision here more informed:

The reason I first believed these functions to be broken is because using the sample variance formula for sliding windows makes no sense to me from a statistical viewpoint. We use sample variance when the dataset is a sample drawn from a larger population. **A window is not a sample**. When we calculate sliding window variance, we aren't interested in getting the correct variance for some underlying general window, we are interested in computing it correctly for each window, and thus each window is the population.

However, as we discussed with @mroeschke:
1) Usually users are interested in calculating variance for large windows, and the difference between formulas for variance is proportional to `1 / (window_size - 1) - 1 / window_size`
2) We use sample variance as a default everywhere else in pandas.
3) The user can specify `rolling.var(ddof=0)` to set degrees of freedom to zero and get population variance, if they know what they want and are aware that pandas uses sample variance by default.

So the default doesn't make much sense, but it is consistent with the rest of our software, and the harm is negligible for most use cases. The harm in changing the default would be that people who know the package well might expect the sample variance default.

Apologies for the long read. It's a part of my job to find mistakes in data science models, so I'm sensitive to stuff like this."
589821911,33121,DOC: Updating capitalization of  doc/source/development,cleconte987,closed,2020-03-29T15:50:25Z,2020-04-17T11:08:20Z,Regarding issue #32550. Changes to documentation folder doc/source/development to capitalise title strings and keep keyword exceptions as is
588868649,33058,Indexing broken inside groupby - apply,diegodlh,closed,2020-03-27T03:53:59Z,2020-04-17T11:16:49Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import pdb

df = pd.DataFrame(
	{
		'col1': ['A', 'A', 'A', 'B', 'B', 'B'],
		'col2': [1, 2, 3, 4, 5, 6],
	}
)

def fn(x):
	pdb.set_trace()
	x.col2[x.index[-1]] = 0
	return x.col2

result = df.groupby(['col1'], as_index=False).apply(fn)
print(result)

```
#### Problem description
The expected output is:
```
0  0    1
   1    2
   2    0
1  3    4
   4    5
   5    0
```
Instead, I get a Series one row longer than expected:
```
0  0    1
   1    2
   2    0
1  3    4
   4    5
   5    6
   5    0
```
The problem seems to come from processing the second group (col1 == 'B'), where indices do not match row numbers. If I stand at the breakpoint (pdb.set_trace()), I can run this with the following results:
```
-> x.col2[x.index[-1]] = 0
(Pdb) x.col2     
3    4
4    5
5    6
Name: col2, dtype: int64
(Pdb) x.col2[5]
*** KeyError: 5
(Pdb) x.col2[5] = 0
(Pdb) x.col2
3    4
4    5
5    6
5    0
Name: col2, dtype: int64
(Pdb) x.col2[5]
5    6
5    0
Name: col2, dtype: int64
(Pdb) x.col2[5] = 0
(Pdb) x.col2
3    4
4    5
5    0
5    0
Name: col2, dtype: int64
```

#### Expected output
```
0  0    1
   1    2
   2    0
1  3    4
   4    5
   5    0
```
This was working before. Unfortunately, I do not know what Pandas version it was.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.5.13-050513-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 40.6.2
Cython           : 0.29.13
pytest           : 5.2.1
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.1
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.2.1
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.9
tables           : 3.5.2
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.1
numba            : 0.45.1

</details>
"
587160294,32984,PERF: Enable %z in parsing datetime,quangngd,closed,2020-03-24T17:48:46Z,2020-04-17T14:23:47Z,"- [ ] closes #32792
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
599203258,33535,REF: put EA concat logic in _concat_arrays,jbrockmendel,closed,2020-04-13T23:54:23Z,2020-04-17T14:59:26Z,"cc @jorisvandenbossche @TomAugspurger per discussion in #32586 (among others) about `_concat_same_type`, this is a proof of concept for a 3-method solution:

- `EA._concat_same_dtype` --> require same type _and_ dtype; DTA/PA do this now
- `EA._concat_same_type` --> require same type but not necessarily same type; we could do without this, but since its already in the API...
- `EA._concat_arrays` --> any ndarray/EAs

For example, `dtypes.concat._concat_sparse` naturally becomes `SparseArray._concat_arrays`.  The middle chunk of `union_categoricals` becomes `Categorical._concat_same_dtype`. 

Everything described above is just a refactor, putting logic in more reasonable places.  The benefit interface-wise is that the dispatching in `concat_compat` looks like

```
if we_have_any_categoricals:
    return Categorical._concat_arrays(to_concat)
elif we_have_any_datetimelike:
    return DatetimeLike._concat_arrays(to_concat)
elif we_have_any_sparse:
    return SparseArray._concat_arrays(to_concat)
[...]
```

ATM the order Categorical -> DTA/TDA/PA -> Sparse is hard-coded, but we could generalize this either with a negotiation logic like Tom [described](https://github.com/pandas-dev/pandas/issues/22994#issuecomment-436279333) or with something simpler like defining `EA.__concat_priority__ = 1, Categorical.__concat_priority__ = 1000, [...]` and the dispatch becomes:

```
eas = {type(x) for x in to_concat if isinstance(x, ExtensionArray)}
eas = list(eas)
eas.sort(lambda x: x.__concat_priority__)

if eas:
    return eas[-1]._concat_arrays(to_concat)
```"
600564153,33572,BUG: set_levels set wrong order levels for MutiIndex,yixinxiao7,closed,2020-04-15T20:21:14Z,2020-04-17T16:07:47Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Added parameter that allows user to reset codes"
58724557,9539,Unclear error when loading unsupported HDF5 file,mangecoeur,closed,2015-02-24T11:16:51Z,2020-04-17T18:08:36Z,"Backstory - I created some H5 files with h5py containing large 3D arrays (not python dataframes). Then I forgot this and tried to load with Pandas. Since they didn't contain real dataframes the load understandably  failed. However the error thrown was:

`TypeError: cannot create a storer if the object is not existing nor a value are passed`

(line 1148 in pandas.io.pytables) which apart from being awkward grammar (rather `cannot create a storer if the object does not exist nor a value given`) doesn't suggest anything about the h5 file not being supported.

Since passing around h5 files is fairly common and sometimes they contain tables and sometimes not, it would be nice if Pandas warned you clearly that the object you are trying to load isn't a pandas-compatible table.
"
598553632,33501,CLN: use _values_for_argsort in fewer places,jbrockmendel,closed,2020-04-12T19:22:19Z,2020-04-17T18:10:38Z,
598166106,33473,BUG: Series.__setitem__[listlike_of_integers] with IntervalIndex,jbrockmendel,closed,2020-04-11T01:54:00Z,2020-04-17T19:44:12Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
371304297,23216,API: Which DatetimeIndex/TimedeltaIndex ops should infer frequency?,jbrockmendel,closed,2018-10-17T23:24:55Z,2020-04-17T21:42:53Z,"In DatetimeIndex/TimedeltaIndex arithmetic operations sometimes we return `type(self)(values, freq=""infer"")` and other times we return `self._shallow_copy(values, freq=None)`.  I don't see any clear logic guiding when to do which.  Anyone else know?"
602053344,33609,TST: make expected DTI/TDI results more specific,jbrockmendel,closed,2020-04-17T15:22:30Z,2020-04-17T22:00:29Z,"assert_index_equal doesnt check for matching freq.  I've got a branch that changes that, but the diff is massive, so i am first going through the tests and updating the `expected`s so that they will pass once that check is in place.

Also in this sequence: xref #33604"
598353515,33487,API: dont infer freq in DTA/TDA arithmetic ops,jbrockmendel,closed,2020-04-11T21:20:39Z,2020-04-17T22:03:53Z,"- [x] closes #23216

freq inference is pure overhead for the Series/DataFrame op because freq gets discarded."
602226746,33614,CLN: assorted cleanups,jbrockmendel,closed,2020-04-17T20:52:26Z,2020-04-17T22:06:38Z,Trying to clear out my local CLN branches
601660281,33604,BUG: DatetimeIndex.intersection losing freq and tz,jbrockmendel,closed,2020-04-17T02:37:56Z,2020-04-17T22:07:16Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
497768752,28597,Missing values in ordered category breaks sorting of unstacked columns,mojones,closed,2019-09-24T15:38:25Z,2020-04-17T22:08:51Z,"#### Code Sample, a copy-pastable example if possible
```python
test = pd.DataFrame(
    {
        'foo' : ['small', 'large', 'large', 'large', 'medium', 'large', 'large', 'medium'],
        'bar' : ['C', 'A', 'A', 'C', 'A', 'C', 'A', 'C']
    })
test['foo'] = test['foo'].astype('category').cat.set_categories(['tiny','small', 'medium', 'large'], ordered=True)
test.groupby(['bar', 'foo']).size().unstack()

# output
foo  medium  large  small
bar                      
A       1.0    3.0    NaN
C       1.0    2.0    1.0

```
#### Problem description

I have a dataframe with an ordered category column `foo`. I want to group by both columns then take the size of the groups and unstack to get a summary table. If all of the values in my ordered category are in the data, then the result is as expected:

```python
test = pd.DataFrame(
    {
        'foo' : ['small', 'large', 'large', 'large', 'medium', 'large', 'large', 'medium'],
        'bar' : ['C', 'A', 'A', 'C', 'A', 'C', 'A', 'C']
    })
test['foo'] = test['foo'].astype('category').cat.set_categories(['small', 'medium', 'large'], ordered=True)
print(test.groupby(['bar', 'foo']).size().unstack())

# output
foo  small  medium  large
bar                      
A      NaN     1.0    3.0
C      1.0     1.0    2.0
```
My columns appear in the specified order. However, if for some reason I have categories that are listed but don't actually appear in the data (in this case, `'tiny'`) the order seems to be determined by the order that the categories appear in the series before stacking:

```python
test = pd.DataFrame(
    {
        'foo' : ['small', 'large', 'large', 'large', 'medium', 'large', 'large', 'medium'],
        'bar' : ['C', 'A', 'A', 'C', 'A', 'C', 'A', 'C']
    })
test['foo'] = test['foo'].astype('category').cat.set_categories(['small', 'medium', 'large'], ordered=True)
print(test.groupby(['bar', 'foo']).size())
print(test.groupby(['bar', 'foo']).size().unstack())

# output
bar  foo   
A    medium    1
     large     3
C    small     1
     medium    1
     large     2
dtype: int64


foo  medium  large  small
bar                      
A       1.0    3.0    NaN
C       1.0    2.0    1.0
```

I originally encountered this when using `pd.cut` to group rows into bins, but an explicitly ordered category I thought made a clearer example. It's also very easy to end up in this situation when filtering a large dataframe.

#### Expected Output
```python
foo  small  medium  large
bar                      
A      NaN     1.0    3.0
C      1.0     1.0    2.0
```
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-64-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.24.2
pytest: None
pip: 19.1.1
setuptools: 41.0.1
Cython: None
numpy: 1.16.4
scipy: 1.3.0
pyarrow: None
xarray: None
IPython: 7.5.0
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.1.0
openpyxl: None
xlrd: 1.2.0
xlwt: None
xlsxwriter: None
lxml.etree: 4.3.3
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
600951842,33588,TST: added test for GH28597,mojones,closed,2020-04-16T11:05:18Z,2020-04-17T22:09:13Z,"Added test to ensure that categories stay ordered when grouping
with missing values.

- [ ] closes #28597
- [X] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
595482952,33347,REF: remove BlockManager.set,jbrockmendel,closed,2020-04-06T23:10:14Z,2020-04-17T22:12:10Z,"xref #33052, #33332.

This touches several DataFrame/NDFrame methods.  Those methods need a refactor in part because the ones defined on NDFrame are only ever called for DataFrame.  Will refactor in follow-up."
579775205,32652,"BUG: process Int64 as ints for preservable ops, not as float64",qwhelan,closed,2020-03-12T08:35:13Z,2020-04-18T02:46:09Z,"Calling `.min()` or `.max()` on `pd.Int64` data returns integers, but uses `float64` as an intermediate representation, leading to data corruption.

This PR attempts to whitelist them to be processed entirely as `Int64`s. Also add a test that verifies this behavior, as existing tests failed to detect the bug.

- [x] closes #32651
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
602352562,33619,CLN: isort core/api.py,alimcmaster1,closed,2020-04-18T01:43:11Z,2020-04-18T03:40:18Z,```isort --quiet --recursive --check-only pandas asv_bench scripts``` runs fine 
601763345,33605,BUG: support count function for custom BaseIndexer rolling windows,AlexKirko,closed,2020-04-17T07:28:21Z,2020-04-18T05:45:10Z,"- [X] xref #32865 
- [X] 1 tests added / 1 passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

## Scope of PR
This PR makes sure that when we call `count` with a `BaseIndexer` subclass, custom `start` and `end` arrays get calculated, and the algorithm in the `aggregations.pyx` gets called instead of the `_Window.create_blocks`.

Turns out that we were sending the calculation into `aggregations.pyx` only for frequency-based windows, and custom `BaseIndexer` subclasses got ignored. Fixed it and cleaned the code up a bit.

## Background on the wider issue
We currently don't support several rolling window functions when building a rolling window object using a custom class descended from `pandas.api.indexers.Baseindexer`. The implementations were written with backward-looking windows in mind, and this led to these functions breaking.
Currently, using these functions returns a `NotImplemented` error thanks to #33057, but ideally we want to update the implementations, so that they will work without a performance hit. This is what I aim to do over a series of PRs.

## Perf notes
No changes to the algorithms necessary.
"
545811398,30743,ENH: Support multi row inserts in to_sql when using the sqlite fallback,simongibbons,closed,2020-01-06T16:17:46Z,2020-04-18T18:00:34Z,"Currently we do not support multi row inserts into sqlite databases
when `to_sql` is passed `method=""multi""` - despite the documentation
suggesting that this is supported.

Adding support for this is straightforward - it only needs us
to implement a single method on the SQLiteTable class and so
this PR does just that.

- [x] closes #29921
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
602059398,33610,TYP: type NDFrame.(_get_axis|_get_axis_name|_get_axis_number),topper-123,closed,2020-04-17T15:31:35Z,2020-04-18T19:07:49Z,Gives return types to ``NDFrame._get_axis`` etc.
600783759,33582,fixup some f-strings,Dxin-code,closed,2020-04-16T06:34:21Z,2020-04-19T00:12:02Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
xref #29547 "
575727677,32447,how to keep nanosecond timestamps?,randomgambit,closed,2020-03-04T20:02:28Z,2020-04-19T01:49:29Z,"Hello there, 

I apologize if this a very simple question but what is the best way to store nanoseconds timestamps with `pandas` and `pyarrow`? 

Right now it seems that using the `flavor = 'spark'` keeps the nanosecond precision while the other flavors do not. 

Is that the right way to see things?

Consider this:
```
import pyarrow as pa
import pandas as pd
import numpy as np

pa.__version__

pa.__version__
Out[131]: '0.13.0'

pd.__version__

pd.__version__
Out[128]: '0.24.2'

mydf = pd.DataFrame({'mytime' : [pd.to_datetime('2020-01-01 10:10:10.123456'),
                                pd.to_datetime('2020-01-01 10:10:10.234567')],
                     'value' : [1,2]})

mydf.to_parquet('testfile_spark.pq', engine = 'pyarrow', flavor = 'spark')

mydf.to_parquet('testfile_regular.pq', engine = 'pyarrow', allow_truncated_timestamps = True)

one = pd.read_parquet('testfile_spark.pq')

one.head()
Out[134]: 
                      mytime  value
0 2020-01-01 10:10:10.123456      1
1 2020-01-01 10:10:10.234567      2

two = pd.read_parquet('testfile_regular.pq')

two.head()
Out[136]: 
                   mytime  value
0 2020-01-01 10:10:10.123      1
1 2020-01-01 10:10:10.234      2

```

EDIT
@wesm asked me to post here from https://github.com/apache/arrow/issues/6536
"
602568339,33640,DOC: Fix: single,kwadrat,closed,2020-04-18T22:42:49Z,2020-04-19T15:02:59Z,One word in documentation was misspelled: single
602653574,33648,DOC: convert_dtypes method is not converted to URL,gosuto-inzasheru,closed,2020-04-19T08:13:30Z,2020-04-19T18:11:41Z,"Somehow the method `convert_dtypes` in the ""See also"" box is not a URL, whereas all the other ones are: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html"
602784606,33657,ENH: favor columns over index levels when groupby-ing over ambiguous label,toobaz,open,2020-04-19T18:15:25Z,2020-04-19T18:15:25Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python3
In [2]: df = pd.DataFrame(2, index=pd.MultiIndex.from_product([range(4), list('abcd')], names=['I', 'II']), columns=['I'])                                                                       

In [3]: df.groupby('I')                                                                                                                                                                          
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-8e63025e89e4> in <module>
----> 1 df.groupby('I')

~/nobackup/repo/pandas/pandas/core/frame.py in groupby(self, by, axis, level, as_index, sort, group_keys, squeeze, observed)
   5881             group_keys=group_keys,
   5882             squeeze=squeeze,
-> 5883             observed=observed,
   5884         )
   5885 

~/nobackup/repo/pandas/pandas/core/groupby/groupby.py in __init__(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated)
    407                 sort=sort,
    408                 observed=observed,
--> 409                 mutated=self.mutated,
    410             )
    411 

~/nobackup/repo/pandas/pandas/core/groupby/grouper.py in get_grouper(obj, key, axis, level, sort, observed, mutated, validate)
    591             if gpr in obj:
    592                 if validate:
--> 593                     obj._check_label_or_level_ambiguity(gpr, axis=axis)
    594                 in_axis, name, gpr = True, gpr, obj[gpr]
    595                 exclusions.append(name)

~/nobackup/repo/pandas/pandas/core/generic.py in _check_label_or_level_ambiguity(self, key, axis)
   1556                 f""{label_article} {label_type} label, which is ambiguous.""
   1557             )
-> 1558             raise ValueError(msg)
   1559 
   1560     def _get_label_or_level_values(self, key: str, axis: int = 0) -> np.ndarray:

ValueError: 'I' is both an index level and a column label, which is ambiguous.
```

#### Problem description

1. The ``groupby`` docs [don't even mention](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) the possibility that a label is interpreted as an index level. They only say ""``A label or list of labels may be passed to group by the columns in self``"".

2. There is an explicit way to resolve the ambiguity in favor of an index level, which is the ``level=`` argument

3. In my experience (and I do use ``MultiIndex``es a lot), grouping on index levels is far less common than grouping on columns

4. ... but there is no easy way to resolve the ambiguity in favor of a column (other than changing the columns/levels names, or explicitly passing the corresponding series)

All this said, I propose to suppress the ``ValueError`` above and just intepret the name as a column name. Given that we suppress an error, this shouldn't break any working code. This is the outcome which was agreed on in #5677 , but then in the PR Jeff [suggested](14432#issuecomment-254802493) to implement the current behavior. I think the rationale made sense but should reconsidered in the context of the typical usage (also reflected in the docs).

Moreover, while Jeff suggested to use a ``pd.Grouper`` in that case, it doesn't allow to group on an ambiguous column name _and_ an index level in the desired order, which was precisely [one of the arguments](5677#issuecomment-30432682) for allowing passing levels to ``by=``.

#### Expected Output

```python3
In [9]: df.reset_index(drop=True).groupby('I')                                                                                                                                                   
Out[9]: <pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f70e60e3828>
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.0-8-amd64
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : it_IT.UTF-8
LOCALE           : it_IT.UTF-8

pandas           : 1.1.0.dev0+276.g2495068ad
numpy            : 1.16.4
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 18.1
setuptools       : 41.0.1
Cython           : 0.29.13
pytest           : 4.6.3
hypothesis       : 3.71.11
sphinx           : 1.8.4
blosc            : 1.7.0
feather          : None
xlsxwriter       : 0.9.3
lxml.etree       : 4.3.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.7 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: 0.8.1
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.2
matplotlib       : 3.0.2
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.4.9
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 4.6.3
pyxlsb           : None
s3fs             : None
scipy            : 1.1.0
sqlalchemy       : 1.2.18
tables           : 3.4.4
tabulate         : 0.8.3
xarray           : 0.11.3
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 0.9.3
numba            : 0.45.0


</details>
"
602576854,33641,CLN: remove unnecessary non-scalar code in maybe_upcast_putmask,jbrockmendel,closed,2020-04-18T23:42:08Z,2020-04-19T18:44:57Z,"A few months ago we got rid of the non-scalar case, so some of this became unnecessary"
602352157,33618,CLN: Remove unrequired imports,alimcmaster1,closed,2020-04-18T01:40:11Z,2020-04-19T18:47:01Z,"Looks like these are no longer needed. 
"
602430629,33625,CLN: simplify info,MarcoGorelli,closed,2020-04-18T10:43:15Z,2020-04-19T19:02:02Z,precursor to #31796 
602578168,33642,API: Series[index_with_no_matches] vs Series[list_with_no_matches],jbrockmendel,closed,2020-04-18T23:51:37Z,2020-04-19T21:14:40Z,"We treat list indexers differently from array-like indexers:

```
ser = pd.Series([""A"", ""B""])
key = pd.Series([""C""])

>>> ser[key]
C    NaN
dtype: object

>>> ser[pd.Index(key)]
C    NaN
dtype: object

>>> ser[np.array(key)]
C    NaN
dtype: object

>>> ser[list(key)]
Traceback (most recent call last):
[...]
  File ""/Users/bmendel/Desktop/pd/pandas/pandas/core/indexing.py"", line 1312, in _validate_read_indexer
    raise KeyError(f""None of [{key}] are in the [{axis_name}]"")
KeyError: ""None of [Index(['C'], dtype='object')] are in the [index]""
```

Also inconsistent because `ser.loc[key]` raises for all 4 cases.

Is there a compelling reason for this?  I tried making all of these behave like the list case and only one test broke (that test being the example above).  The test was added in #5880.
"
602578819,33643,"REF: dispatch Series.__setitem__ to loc/iloc, remove redundant helpers",jbrockmendel,closed,2020-04-18T23:56:25Z,2020-04-19T21:30:23Z,"This will entail some extra overhead, but we get a) de-duplication and b) the structure of `__setitem__` comes to parallel the structure of `__getitem__`, which i find helpful grok-wise."
602600399,33646,BUG/API: getitem behavior with list match ndarray/index/series,jbrockmendel,closed,2020-04-19T02:18:36Z,2020-04-19T21:30:44Z,"- [x] closes #33642
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
339484598,21829,Increasing minimum cython version silently breaks build_ext --inplace,toobaz,closed,2018-07-09T14:55:28Z,2020-04-19T21:36:57Z,"#### Code Sample, a copy-pastable example if possible

```
pietro@ecomp:~/nobackup/repo/pandas_test$ python3 setup.py build_ext --inplace
running build_ext
```

```python
In [1]: import pandas as pd
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
/home/pietro/nobackup/repo/pandas_test/pandas/__init__.py in <module>()
     25 try:
---> 26     from pandas._libs import (hashtable as _hashtable,
     27                              lib as _lib,

/home/pietro/nobackup/repo/pandas_test/pandas/_libs/__init__.py in <module>()
      3
----> 4 from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime
      5

/home/pietro/nobackup/repo/pandas_test/pandas/_libs/tslibs/conversion.pxd in init pandas._libs.tslib (pandas/_libs/tslib.c:19007)()
     10
---> 11 cdef class _TSObject:
     12     cdef:

/home/pietro/nobackup/repo/pandas_test/pandas/_libs/tslibs/__init__.py in <module>()
      3
----> 4 from .conversion import normalize_date, localize_pydatetime, tz_convert_single
      5 from .nattype import NaT, iNaT

ImportError: cannot import name 'normalize_date'

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-af55e7023913> in <module>()
----> 1 import pandas as pd

/home/pietro/nobackup/repo/pandas_test/pandas/__init__.py in <module>()
     33                       ""pandas from the source directory, you may need to run ""
     34                       ""'python setup.py build_ext --inplace --force' to build ""
---> 35                       ""the C extensions first."".format(module))
     36
     37 from datetime import datetime

ImportError: C extension: 'normalize_date' not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace --force' to build the C extensions first.
```

#### Problem description

When the minimum required Cython version is increased (above the installed one), _but_ there is a version of the compiled code available, ``python setup.py build_ext --inplace`` silently skips compilation.

It should be at least emit a warning that the available Cython version is too old.

#### Expected Output

A warning when running ``python setup.py build_ext --inplace``.

The above is ""fixed"" if I remove the check for the ``cython >= '0.28.2'``.

#### Output of ``pd.show_versions()``

(after disabling the check for the minimum Cython version, and re-running ``python setup.py build_ext --inplace`` )

<details>

INSTALLED VERSIONS
------------------
commit: 5cb5880d8f1647e303ba52487dc867db838fd6e0
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.0-6-amd64
machine: x86_64
processor:
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.24.0.dev0+269.g5cb5880d8.dirty
pytest: 3.0.6
pip: 9.0.1
setuptools: 33.1.1
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.18.1
pyarrow: None
xarray: None
IPython: 5.2.2
sphinx: None
patsy: 0.4.1+dev
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: 1.2.0
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.0.0
openpyxl: 2.3.0
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: None
lxml: 3.7.1
bs4: 4.5.3
html5lib: 0.999999999
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.8
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None


</details>
"
516592249,29349,Local test failure: TestMathNumExprPython.test_result_complex128,pv8473h12,closed,2019-11-02T13:24:32Z,2020-04-19T22:09:25Z,"When running the pytest suite, I get the following error locally.

```
_________________ TestMathNumExprPython.test_result_complex128 _________________

self = <pandas.tests.computation.test_eval.TestMathNumExprPython object at 0x1c1e588250>

    @td.skip_if_windows
    def test_result_complex128(self):
        # xref https://github.com/pandas-dev/pandas/issues/12293
        #  this fails on Windows, apparently a floating point precision issue
    
        # Did not test complex64 because DataFrame is converting it to
        # complex128. Due to https://github.com/pandas-dev/pandas/issues/10952
>       self.check_result_type(np.complex128, np.complex128)

pandas/tests/computation/test_eval.py:1811: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pandas/tests/computation/test_eval.py:1796: in check_result_type
    tm.assert_series_equal(got, expect, check_names=False)
pandas/_libs/testing.pyx:65: in pandas._libs.testing.assert_almost_equal
    cpdef assert_almost_equal(a, b,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   raise_assert_detail(obj, msg, lobj, robj)
E   AssertionError: Series are different
E   
E   Series values are different (10.0 %)
E   [left]:  [(-0.9522952601059279-0j), (-0.17178347753563025+0j), (-0.44142785973450865+0j), (0.9805014538002449-0j), (0.33466949854846434+0j), (0.18226199407925794+0j), (0.5086971691486349+0j), (-0.34119630264795875+0j), (0.8144747173149681+0j), (-0.14905691528482845+0j)]
E   [right]: [(-0.9522952601059279+0j), (-0.17178347753563025+0j), (-0.44142785973450865+0j), (0.9805014538002449+0j), (0.33466949854846434+0j), (0.18226199407925794+0j), (0.5086971691486349+0j), (-0.34119630264795875+0j), (0.814474717314968+0j), (-0.14905691528482845+0j)]

pandas/_libs/testing.pyx:176: AssertionError

```

<details>

INSTALLED VERSIONS
------------------
commit           : 84386976a113b9f7e84863f5a8329f129d1b2a92
python           : 3.7.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 17.5.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.26.0.dev0+736.g84386976a
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.6.0.post20191030
Cython           : 0.29.13
pytest           : 5.2.2
hypothesis       : 4.36.2
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.2
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : 0.15.0
pytables         : None
s3fs             : 0.3.4
scipy            : 1.3.1
sqlalchemy       : 1.3.10
tables           : 3.5.1
xarray           : 0.13.0
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.2

</details>
"
590399210,33144,add match message for pytest.raises(),sathyz,closed,2020-03-30T15:51:23Z,2020-04-19T22:22:28Z,"- [x] ref #30999 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
602484383,33631,CLN: Clean missing.py,dsaxton,closed,2020-04-18T15:21:01Z,2020-04-19T22:25:25Z,Random nitpick here
602350274,33616,REF: get .items out of BlockManager.apply,jbrockmendel,closed,2020-04-18T01:28:13Z,2020-04-19T23:53:03Z,
516133647,29321,Pandas dataframe sub-setting returns NaN's when whole column matches fill value,mjobin,closed,2019-11-01T14:06:36Z,2020-04-20T00:36:25Z,"#### Code Sample, a copy-pastable example if possible

```python

import random
import numpy as np
import multiprocessing as mp
import pandas as pd


TEST_LINES = 10


samezeroint = [0 for i in range(TEST_LINES)]
sameoneint = [1 for i in range(TEST_LINES)]
samezerofloat = [0.0 for i in range(TEST_LINES)]
sameonefloat = [1.0 for i in range(TEST_LINES)]
indexone = [i for i in range(TEST_LINES)]

randomint = []
randomfloat = []

for i in range(TEST_LINES):
    randomint.append(random.randint(0,100))
    randomfloat.append(random.random())

testdict = {'indexone': indexone, ""samezeroint"": samezeroint, 'sameoneint': sameoneint, 'samezerofloat': samezerofloat, 'sameonefloat': sameonefloat, 'randomint': randomint, 'randomfloat': randomfloat}
filldict = {'indexone': 0, ""samezeroint"": 0, 'sameoneint': 1, 'samezerofloat': 0.0,
            'sameonefloat': 1.0, 'randomint': random.randint(0,100), 'randomfloat': random.random()}
dtypedict = {'indexone': np.int8, ""samezeroint"": np.int8, 'sameoneint': np.int8, 'samezerofloat': np.float,
            'sameonefloat': np.float, 'randomint': np.int8, 'randomfloat': np.float}


dospar = {}
for l in testdict:
    try:
        fill = filldict[l]
    except KeyError:
        fill = None
    try:
        datatype = dtypedict[l]
    except KeyError:
        datatype = np.str
    if fill is None:
        sparr = pd.Series(pd.array(testdict[l], dtype=datatype))
    else:
        sparr = pd.Series(pd.SparseArray(testdict[l], fill_value=fill, dtype=datatype))
    dospar[l] = sparr
testdf = pd.DataFrame.from_dict(dospar, orient='columns')

# Test a single series

print(""\n\nSeries: All zeroes"")
samezerointseries = pd.Series(pd.SparseArray(testdict['samezeroint'], fill_value=0, dtype=np.int8))
print(""\nOriginal"")
print(samezerointseries)
samezero = samezerointseries.isin([0])
samezerozero = samezerointseries[samezero]
print(""\nFiltered: should be identical to above"")
print(samezerozero)
sameone = samezerointseries.isin([1])
samezeroone = samezerointseries[sameone]
print(""\nFiltered: should be empty"")
print(samezeroone)


print(""\n\nDataframe:"")
with pd.option_context('display.max_rows', None, 'display.max_columns',
                       None):  # more options can be specified also
    print(testdf)
    print(testdf.dtypes)

print(""\n\nDataframe: should be identical to above"")
intone = testdf.loc[:, 'sameoneint'].isin([int(1)])
print(intone)
onedf = testdf[intone]
with pd.option_context('display.max_rows', None, 'display.max_columns',
                       None):  # more options can be specified also
    print(onedf)
    print(onedf.dtypes)

```
#### Problem description

I've run into something odd with pandas data frames composed of sparse series. I am able to make the DF out of dictionaries of values with fills and types, no problem, but when I try to subset that DF, I get some very weird results. What I have been able to reproducibly show is that when subsetting a DF created out of Sparse series, if a column happens to be identical throughout (i.e. all entires match the fill value), the subset DF turns those columns into NaN's, and the dtype convert into float64.

When I run this test, I get the following result:

'''
Series: All zeroes

Original
0    0
1    0
2    0
3    0
4    0
5    0
6    0
7    0
8    0
9    0
dtype: Sparse[int8, 0]

Filtered: should be identical to above
0    0
1    0
2    0
3    0
4    0
5    0
6    0
7    0
8    0
9    0
dtype: Sparse[int8, 0]

Filtered: should be empty
Series([], dtype: Sparse[int8, 0])


Dataframe:
   indexone  samezeroint  sameoneint  samezerofloat  sameonefloat  randomint  \
0         0            0           1            0.0           1.0         95   
1         1            0           1            0.0           1.0         13   
2         2            0           1            0.0           1.0         20   
3         3            0           1            0.0           1.0         29   
4         4            0           1            0.0           1.0         44   
5         5            0           1            0.0           1.0         82   
6         6            0           1            0.0           1.0         22   
7         7            0           1            0.0           1.0         91   
8         8            0           1            0.0           1.0         51   
9         9            0           1            0.0           1.0         13   

   randomfloat  densesamezero  
0     0.989141              0  
1     0.948800              0  
2     0.504600              0  
3     0.014699              0  
4     0.979710              0  
5     0.663198              0  
6     0.773480              0  
7     0.921397              0  
8     0.898101              0  
9     0.445575              0  
indexone                              Sparse[int8, 0]
samezeroint                           Sparse[int8, 0]
sameoneint                            Sparse[int8, 1]
samezerofloat                    Sparse[float64, 0.0]
sameonefloat                     Sparse[float64, 1.0]
randomint                            Sparse[int8, 24]
randomfloat      Sparse[float64, 0.08770512017933063]
densesamezero                                   int64
dtype: object


Dataframe: should be identical to above
0    True
1    True
2    True
3    True
4    True
5    True
6    True
7    True
8    True
9    True
Name: sameoneint, dtype: bool
   indexone  samezeroint  sameoneint  samezerofloat  sameonefloat  randomint  \
0         0          NaN         NaN            NaN           NaN         95   
1         1          NaN         NaN            NaN           NaN         13   
2         2          NaN         NaN            NaN           NaN         20   
3         3          NaN         NaN            NaN           NaN         29   
4         4          NaN         NaN            NaN           NaN         44   
5         5          NaN         NaN            NaN           NaN         82   
6         6          NaN         NaN            NaN           NaN         22   
7         7          NaN         NaN            NaN           NaN         91   
8         8          NaN         NaN            NaN           NaN         51   
9         9          NaN         NaN            NaN           NaN         13   

   randomfloat  densesamezero  
0     0.989141              0  
1     0.948800              0  
2     0.504600              0  
3     0.014699              0  
4     0.979710              0  
5     0.663198              0  
6     0.773480              0  
7     0.921397              0  
8     0.898101              0  
9     0.445575              0  
indexone                             Sparse[int64, 0]
samezeroint                        Sparse[float64, 0]
sameoneint                         Sparse[float64, 1]
samezerofloat                    Sparse[float64, 0.0]
sameonefloat                     Sparse[float64, 1.0]
randomint                            Sparse[int8, 24]
randomfloat      Sparse[float64, 0.08770512017933063]
densesamezero                                   int64
dtype: object

As you can hopefully see, since I am subsetting ""all rows where there is a 0 in the column that is all zeroes"", I SHOULD be creating an identical DF - but instead only the Series where there is some variation in the column are preserved, the rest turn into all-NaNs.

For the subsetting command, I have tried every variant I could find:

newdf = testdf.loc[testdf['sameoneint'] == 1]

newdf =testdf.query('sameoneint == 1')

isone = testdf.loc[:, 'sameoneint'].isin([1])

newdf = testdf[isone]
None of those work any better, and some throw a warning about calling to_dense.

So, am I missing something in how I coded that, or is this something in how pandas works I have not yet figured out? Advice most welcome!

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-693.11.6.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.2
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : 3.5.2
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None



</details>
"
602835371,33661,TST: Dtype constants into pandas._testing to avoid direct imports from conftest,SaturnFromTitan,closed,2020-04-19T22:17:43Z,2020-04-20T00:38:26Z,"part of #30914

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

---------------------------------------------------------------------------------------

Took a naive approach to move these constants into `pandas._testing`. Could also move them to a separate module instead. Let me know what you think is best."
602379762,33621,REF: Make numba function cache globally accessible,mroeschke,closed,2020-04-18T05:03:47Z,2020-04-20T00:45:53Z,"- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

For all the numba accessible operations, we use a cache to store compiled JIT'd functions for performance. 

This `_numba_func_cache` used to be an attribute of `Rolling` and `Groupby` objects before, but while working on https://github.com/pandas-dev/pandas/pull/33388, I cannot easily rely on having access to those objects.

This PR moves `_numba_func_cache` to  `pandas/core/util/numba_.py`. Since this cache is now globally shared, the key has changed from `_numba_func_cache[func] = compiled_func` to `_numba_func_cache[(func, op)] = compiled_func`
"
602676539,33650,TST: add messages to pytest.raises,proost,closed,2020-04-19T10:12:48Z,2020-04-20T01:31:58Z,"- [x] xref #30999
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
602855118,33663,REF: use array_algos shift for Categorical.shift,jbrockmendel,closed,2020-04-19T23:51:41Z,2020-04-20T01:35:37Z,"We have duplicate code in Categorical.shift and Categorical.take that can be refactored out into Categorical._validate_fill_value (which is the name that DTA/TDA/PA use for the same method, which I intend to share eventually, xref #33660).  From there the rest of Categorical.shift can dispatch to array_algos.transforms.shift.

The logic changed here is a TypeError becoming a ValueError (which matches what DTA/TDA/PA do)"
598605540,33509,ENH: clearer error message if read_hdf loads upsupported HDF file (GH9539),a-y-khan,closed,2020-04-13T00:37:04Z,2020-04-20T04:18:38Z,"- [x] closes #9539
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The error message changed since the original bug report but still could be made more clear."
264495108,17845,unstack() on DataFrame of ints with MultiIndex which was sliced casts to float,toobaz,closed,2017-10-11T08:16:51Z,2020-04-20T10:41:49Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: idx = pd.MultiIndex.from_product([['a'], ['A', 'B', 'C', 'D']])[:-1]

In [3]: df = pd.DataFrame([[1, 0]]*3, index=idx)

In [4]: df.unstack()
Out[4]: 
     0              1          
     A    B    C    A    B    C
a  1.0  1.0  1.0  0.0  0.0  0.0
```
#### Problem description

Since there is no missing value, no type casting should occur. Notice that even replacing ``index=idx`` with ``index=idx.copy(deep=True)`` doesn't help (something which would maybe require a fix on its own).

#### Expected Output
```python
In [5]: df.unstack(fill_value=3)
Out[5]: 
   0        1      
   A  B  C  A  B  C
a  1  1  1  0  0  0
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.0-3-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: it_IT.UTF-8
LOCALE: it_IT.UTF-8

pandas: 0.21.0.dev+572.g8e89cb3e1
pytest: 3.0.6
pip: 9.0.1
setuptools: None
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.19.0
pyarrow: None
xarray: None
IPython: 5.1.0.dev
sphinx: 1.5.6
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.0.2
openpyxl: None
xlrd: 1.0.0
xlwt: 1.1.2
xlsxwriter: 0.9.6
lxml: None
bs4: 4.5.3
html5lib: 0.999999999
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.2.1


</details>
"
602905637,33667,CLN: make Categorical.codes a simpler property,jbrockmendel,closed,2020-04-20T03:10:11Z,2020-04-20T15:29:47Z,
603429099,33680,TST: Remove some old xfails,mroeschke,closed,2020-04-20T18:04:52Z,2020-04-20T21:43:52Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
603383372,33679,CLN: Remove redundant mixin to TimedeltaIndex,jbrockmendel,closed,2020-04-20T16:50:25Z,2020-04-20T22:16:42Z,we already pass-through the relevant attributes
548410791,30914,Make a code check which bans imports of fixtures in tests,MarcoGorelli,closed,2020-01-11T10:09:56Z,2020-04-20T22:34:36Z,"xref https://github.com/pandas-dev/pandas/pull/30894#discussion_r365510231

Example: writing
```
from pandas.conftest import ALL_EA_INT_DTYPES
```
in a test file should be disallowed because there exists the fixture `any_nullable_int_dtype`"
602867498,33664,TST: CI checks against direct imports from conftest.py,SaturnFromTitan,closed,2020-04-20T00:47:16Z,2020-04-20T22:34:40Z,"- [x] closes #30914

"
602926051,33669,BUG: Python any() on a dataframe is returning True always,IhorMarkevych,closed,2020-04-20T04:17:17Z,2020-04-21T02:46:02Z,"- [Yes ] I have checked that this issue has not already been reported.

- [ Yes] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame([[False, False],  [False, False]])
print(any(df))
print(df.any().any())
```

#### Problem description

Calling python `any()` on a pandas DataFrame return True even if all elements are False, which is counterintuitive and even misleading. A way that gives correct result is to call `.any().any()`.
  
**Example of use case:**  
I found this bug when I was using `isna()` and it lead me to a conclusion that there are some missing values, while that was not true.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.112+
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : 0.29.16
pytest           : 5.0.1
hypothesis       : 5.5.4
sphinx           : 2.4.4
blosc            : None
feather          : 0.4.0
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: 0.8.1
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.1
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
s3fs             : 0.4.1
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : 3.5.1
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.8
None

</details>
"
603619371,33690,CLN: Split dtype inference tests,dsaxton,closed,2020-04-21T00:28:15Z,2020-04-21T12:51:03Z,Breaking up some very large tests in dtypes/test_inference.py
599412071,33544,BUG:Timezone lost when assigning Datetime via DataFrame.at,torfsen,closed,2020-04-14T09:04:16Z,2020-04-21T13:25:22Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import datetime as dt
import pandas as pd

df = pd.DataFrame({'foo': [dt.datetime(2000, 1, 1)]})
df.at[0, 'foo'] = dt.datetime(2000, 1, 2, tzinfo=dt.timezone.utc)
print(df.at[0, 'foo'])  # Prints 2000-01-02 00:00:00
assert df.at[0, 'foo'].tzinfo is not None  # Fails
```

#### Problem description

When assigning a tz-aware datetime via `at` I expect that the tz-information is kept. If that is not possible due to a datatype problem then I expect a warning or error message. Instead it is lost without a warning or error message.


#### Expected Output

I would have expected that the tz-information is kept, i.e. that

```
assert df.at[0, 'foo'].tzinfo is not None  # Should not fail
```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-46-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
602541421,33637,REF: _AXIS_TO_AXIS_NUMBER to simplify axis access,topper-123,closed,2020-04-18T20:09:29Z,2020-04-21T13:32:25Z,"This adds a dict called ``_AXIS_TO_AXIS_NUMBER `` to NDFrame/DataFrame where the keys are the allowed parameter values for the ``axis`` parameter in various ndframe methods and the dict values are the related axis number. This makes getting to the correct axis more straight forward, see for example the new ``_get_axis_number``, and makes adding type hints to the ``axis`` parameter easier."
603602935,33689,REF: collect validator methods for DTA/TDA/PA,jbrockmendel,closed,2020-04-20T23:36:19Z,2020-04-21T14:29:17Z,"Zero logic changes in this PR.

xref #33685

We have a _lot_ of methods for converting arguments to DTA/TDA/PA/DTI/TDI/PI methods to/from i8 representations.  ATM these have slightly different behavior, which we should try to bring into sync (and then de-duplicate the code).

This PR just collects all those methods in one place to make the upcoming steps easier.

Two more methods that I think/hope can eventually be added to this grouping are the comparisons and get_loc, but those are more complicated, but those are more complicated, so holding off on those for now."
602468271,33628,inspect-safety for DataFrame._constructor_expanddim,jbrockmendel,closed,2020-04-18T14:07:48Z,2020-04-21T14:37:23Z,"- [x] closes #31549, closes #31474
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
600648358,33577,TST: Added test case for DataFrame.at,EdAbati,closed,2020-04-15T23:31:59Z,2020-04-21T14:53:36Z,"- [x] closes #33544
- [x] tests added
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
602229005,33615,"CLN: avoid catching AssertionError, AttributeError in NDFrame methods",jbrockmendel,closed,2020-04-17T20:56:51Z,2020-04-21T14:56:55Z,"@WillAyd when i run `mypy` manually it passes, but when run via the pre-commit hook I get

```
pandas/core/internals/blocks.py:2739: error: unused 'type: ignore' comment
pandas/core/generic.py:4034: error: unused 'type: ignore' comment
pandas/core/generic.py:4093: error: unused 'type: ignore' comment
Found 3 errors in 2 files (checked 2 source files)
```

Something similar is happening with most branches these days.  Have you seen this?"
545233187,30672,CLN: Simplify rolling.py helper functions,mroeschke,closed,2020-01-04T03:06:08Z,2020-04-21T16:29:50Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Cleans helper functions in `rolling.py` and remove dead code paths."
575226008,32428,WIP: ENH: Add engine keyword argument to groupby.apply to leverage Numba,mroeschke,closed,2020-03-04T08:39:03Z,2020-04-21T16:30:48Z,"- [x] closes #31845
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
598916702,33525,TST: Add Categorical Series test,dsaxton,closed,2020-04-13T14:20:13Z,2020-04-21T18:32:12Z,"Follow up to https://github.com/pandas-dev/pandas/pull/33513

cc @jreback "
603379974,33678,REF: remove single-tuple special case for Categorical.__hash__,jbrockmendel,closed,2020-04-20T16:45:14Z,2020-04-21T22:58:49Z,
569567593,32210,REF: share benchmark code,jbrockmendel,closed,2020-02-23T22:10:27Z,2020-04-21T23:02:14Z,Collapse 5 classes down to 1
553901507,31236,WIP/TST: add dt64tz to indices fixture,jbrockmendel,closed,2020-01-23T02:21:51Z,2020-04-21T23:03:13Z,"@TomAugspurger @jorisvandenbossche as you can see, the `__array_ufunc__` that this implements is an unholy mess.  This all came from a much smaller goal: I wanted to add the ""datetime-tz"" key in tests.indexes.conftest.

That broke a test for `np.isfinite`, so I started implementing `array_ufunc`.  Then that broke other ufuncs, and so on and so on.

I guess I could just xfail the one isfinite test.  In retrospect that would have saved a lot of effort.  Woops.

Anyhow, we probably want/need to implement this eventually anyway, so any ideas on how to do it but less awful?

xref #31219."
576530719,32470,Mishandling exception when trying to access inexistent files at private S3 bucket using several pandas.read_ methods ,marlosb,closed,2020-03-05T21:06:26Z,2020-04-21T23:08:35Z,"#### Problem description

When trying to access a non-existing file on a private S3 bucket, using appropriated credentials, pandas will throw a ConnectionTimeoutError instead of a FileNotFoundError.
I tested the following read_ methods and got same behavior in all:  **read_csv, read_excel, read_fwf, read_json, read_msgpack, read_parquet,** 

You got to have right credentials and try to access a private bucket to reproduce it. The s3fs library implicitly reads credentials from environment variables if you have it named properly (AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY).

#### Code to reproduce the issue

```python
import pandas
import s3fs

fs = s3fs.S3FileSystem()
file_name = 's3://<my_bucket>/<my_file>'
pandas.read_csv(file_name)
```

#### Root Cause

I checked source files and found the following excerpt from pandas/io/s3.py

```python
     fs = s3fs.S3FileSystem(anon=False)
    try:
        filepath_or_buffer = fs.open(_strip_schema(filepath_or_buffer), mode)
    except (compat.FileNotFoundError, NoCredentialsError):
        # boto3 has troubles when trying to access a public file
        # when credentialed...
        # An OSError is raised if you have credentials, but they
        # aren't valid for that bucket.
        # A NoCredentialsError is raised if you don't have creds
        # for that bucket.
        fs = s3fs.S3FileSystem(anon=True)
        filepath_or_buffer = fs.open(_strip_schema(filepath_or_buffer), mode)
    return filepath_or_buffer, None, compression, True
```

Pandas is trying to workaround an issue of boto3 and when receiving FileNotFoundError or NoCredentialsError it chances connection credentials to anonymous (see the anon=True argument while creating object fs from class  S3FileSystem) and try it again. 
This help accessing public buckets/files while using S3 credentials, as comment states. However, when trying to access a non-existing file in a private bucket S3 will not return a file not found it will let connection timeout. They probably do that for security reasons, not disclosure private file existence without proper credentials.

#### Why this is an issue

Instead of quiclky throwns a FileNotFoundError it takes a while and then throwns a ConnectionTimeOutError.


#### Double check the cause

Just to make sure this is the cause I tried the code below. However, I don't really understand all implications to check if it is a acceptable solution.

```python
    fs = s3fs.S3FileSystem(anon=False)
    try:
        filepath_or_buffer = fs.open(_strip_schema(filepath_or_buffer), mode)
    except (compat.FileNotFoundError, NoCredentialsError):
        # boto3 has troubles when trying to access a public file
        # when credentialed...
        # An OSError is raised if you have credentials, but they
        # aren't valid for that bucket.
        # A NoCredentialsError is raised if you don't have creds
        # for that bucket. 

        if fs.exists(filepath_or_buffer[0:filepath_or_buffer.rfind('/')]):
            raise FileNotFoundError(filepath_or_buffer, 'not found')
        else: 

            fs = s3fs.S3FileSystem(anon=True)
            filepath_or_buffer = fs.open(_strip_schema(filepath_or_buffer), mode)
    return filepath_or_buffer, None, compression, True
```

I just test if credentials can access up directory. If yes It conclude this is not a credentials issue and thrown a FileNotFoundError.

#### Expected Output

Pandas read methods thrown FileNotFoundError when file doesn't exist on private S3 buckets.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 39.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : 0.4.0
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
603463460,33681,BUG: pd.Series.value_counts with pd.NA implies ValueError,mariusgarbowski,closed,2020-04-20T19:02:52Z,2020-04-22T00:39:33Z,"- [ x] I have checked that this issue has not already been reported.

- [x ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

The new pd.NA seems to have an issues with the pandas.Series method value_counts:
#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np
s1 = pd.Series([np.nan, np.nan], dtype='float')
s2 = pd.Series([None, None], dtype='str')
s3 = pd.Series([pd.NA, pd.NA], dtype='Int64')

print(s1.value_counts()) 
print(s2.value_counts())
print(s3.value_counts()))
```

#### Problem description
The current output (with the new pd.NA values) raises a ValueError (see below). I would expect a blank pandas.Series as output (analogous to np.nan or None values with it's dtypes).
This issues regards pandas/python in my home pc as well as python installed on an ubuntu EC2 instance (see below).

#### Current Output:
Series([], dtype: int64)
Series([], dtype: int64)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-17-a787e255adb5> in <module>
      1 print(s1.value_counts())
      2 print(s2.value_counts())
----> 3 print(s3.value_counts())

~\Miniconda3\lib\site-packages\pandas\core\base.py in value_counts(self, normalize, sort, ascending, bins, dropna)
   1233             normalize=normalize,
   1234             bins=bins,
-> 1235             dropna=dropna,
   1236         )
   1237         return result

~\Miniconda3\lib\site-packages\pandas\core\algorithms.py in value_counts(values, sort, ascending, normalize, bins, dropna)
    726 
    727     if sort:
--> 728         result = result.sort_values(ascending=ascending)
    729 
    730     if normalize:

~\Miniconda3\lib\site-packages\pandas\core\series.py in sort_values(self, axis, ascending, inplace, kind, na_position, ignore_index)
   2960         idx = ibase.default_index(len(self))
   2961 
-> 2962         argsorted = _try_kind_sort(arr[good])
   2963 
   2964         if is_list_like(ascending):

~\Miniconda3\lib\site-packages\pandas\core\series.py in _try_kind_sort(arr)
   2946             try:
   2947                 # if kind==mergesort, it can fail for object dtype
-> 2948                 return arr.argsort(kind=kind)
   2949             except TypeError:
   2950                 # stable sort not available for object dtype

~\Miniconda3\lib\site-packages\pandas\core\arrays\base.py in argsort(self, ascending, kind, *args, **kwargs)
    514         ascending = nv.validate_argsort_with_ascending(ascending, args, kwargs)
    515 
--> 516         result = nargsort(self, kind=kind, ascending=ascending, na_position=""last"")
    517         return result
    518 

~\Miniconda3\lib\site-packages\pandas\core\sorting.py in nargsort(items, kind, ascending, na_position)
    261 
    262     if is_extension_array_dtype(items):
--> 263         items = items._values_for_argsort()
    264     else:
    265         items = np.asanyarray(items)

~\Miniconda3\lib\site-packages\pandas\core\arrays\integer.py in _values_for_argsort(self)
    500         """"""
    501         data = self._data.copy()
--> 502         data[self._mask] = data.min() - 1
    503         return data
    504 

~\Miniconda3\lib\site-packages\numpy\core\_methods.py in _amin(a, axis, out, keepdims, initial, where)
     32 def _amin(a, axis=None, out=None, keepdims=False,
     33           initial=_NoValue, where=True):
---> 34     return umr_minimum(a, axis, None, out, keepdims, initial, where)
     35 
     36 def _sum(a, axis=None, dtype=None, out=None, keepdims=False,

ValueError: zero-size array to reduction operation minimum which has no identity

#### Expected Output
Series([], dtype: int64)
Series([], dtype: int64)
Series([], dtype: int64)

Implemented temporal workaround:

```python
def get_value_counts(s: pd.Series) -> pd.Series:
    try:
        unique_values = s.value_counts()

    except ValueError:
        unique_values = pd.Series(dtype='int64')

    return unique_values
```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.16
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : None
numba            : None

Same issue on Ubutnu Server:

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-1065-aws
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.0.3
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.0.3
setuptools       : 41.0.0
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.3.4
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : 0.4.0
scipy            : None
sqlalchemy       : 1.3.13
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
603593694,33688,Grammatically updated HTML  Footer,mayank98x,closed,2020-04-20T23:16:53Z,2020-04-22T02:34:33Z,"Grammatically updated HTML  Footer, Adding a period in the 89th line.

- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
603647575,33691,CLN: remove shallow_copy_with_infer,jbrockmendel,closed,2020-04-21T01:57:47Z,2020-04-22T02:48:25Z,
604332059,33713,CLN: .values -> ._values,jbrockmendel,closed,2020-04-21T22:46:48Z,2020-04-22T02:49:53Z,
516709167,29375,Deprecate using `xlrd` engine,cruzzoe,closed,2019-11-02T20:20:11Z,2020-04-22T03:56:01Z,"- [x] closes #28547
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry 
"
600783836,33583,Issue 33428 fix,Dxin-code,closed,2020-04-16T06:34:31Z,2020-04-22T04:17:57Z,"- [ ] closes #33428
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
604216859,33705,TYP: remove #type:ignore from core.arrays.datetimelike,simonjayhawkins,closed,2020-04-21T19:08:34Z,2020-04-22T11:27:32Z,"pandas\core\arrays\datetimelike.py:1535: error: Signatures of ""__iadd__"" and ""__add__"" are incompatible
pandas\core\arrays\datetimelike.py:1544: error: Signatures of ""__isub__"" and ""__sub__"" are incompatible"
124662341,11949,Excel encoding issue,cschwem2er,closed,2016-01-03T18:08:48Z,2020-04-22T14:46:00Z,"I want to read in Excel-Data (.xlsx) and have a problem with a wrong encoding of special characters. In the Excel-File, portuguese special characters, for example on ""A"", look like this:
![alt text](https://dl.dropboxusercontent.com/u/22352379/1excel_encoding.png)
**Á**

However, after reading in the data with Pandas Excel IO, they look like this:
![alt text](https://dl.dropboxusercontent.com/u/22352379/pandas_encoding.JPG)
**Ã**

I have no idea why this happens and how to solve this. Any help would be great.
"
603146631,33673,"CLN,TYP: Use subsitutions in info docstrings",MarcoGorelli,closed,2020-04-20T10:59:19Z,2020-04-22T16:11:58Z,"Another precursor to #31796, which should make it easier to review/merge

----

screenshots of docs

![image](https://user-images.githubusercontent.com/33491632/79745028-4f7bb480-82ff-11ea-90d5-8c111a449126.png)
![image](https://user-images.githubusercontent.com/33491632/79744668-aaf97280-82fe-11ea-95e6-7cc34eafbeac.png)
![image](https://user-images.githubusercontent.com/33491632/79744688-b8aef800-82fe-11ea-970f-053825465ef3.png)
![image](https://user-images.githubusercontent.com/33491632/79744720-c5335080-82fe-11ea-88c1-4728488f6221.png)
"
562189225,31823,concat ignore_index option doesnt work,Niroznak,closed,2020-02-09T15:00:00Z,2020-04-22T17:39:36Z,"#### Code Sample, a copy-pastable example if possible

```python
pd.concat([df_general.iloc[:,2:],df],ignore_index=True).fillna(method='ffill')
tried vs.
pd.concat([df_general.iloc[:,2:],df],ignore_index=False).fillna(method='ffill')



```
#### Problem description
concatenating the tables leaves 1 partially empty row that shouldnt be there, on pandas version version (0.23.4) it didn't happen.
this happened after upgrading pandas to below version.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.0.final.0
python-bits      : 64
OS               : Windows
OS-release       : 7
machine          : AMD64
processor        : Intel64 Family 6 Model 30 Stepping 4, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.16.3
pytz             : 2018.5
dateutil         : 2.7.3
pip              : 20.0.2
setuptools       : 41.0.1
Cython           : 0.28.5
pytest           : 3.8.0
hypothesis       : None
sphinx           : 1.7.9
blosc            : None
feather          : None
xlsxwriter       : 1.1.0
lxml.etree       : 4.2.5
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 6.5.0
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.5
matplotlib       : 2.2.3
numexpr          : 2.6.8
odfpy            : None
openpyxl         : 2.5.6
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 3.8.0
pyxlsb           : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : 1.2.11
tables           : 3.4.4
tabulate         : None
xarray           : None
xlrd             : 1.1.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.0
numba            : 0.39.0
</details>
"
604768854,33722,TYP: add types to DataFrame._get_agg_axis,topper-123,closed,2020-04-22T13:43:03Z,2020-04-22T18:31:58Z,Minor followup to #33610.
105955829,11058,pd.concat on two (or more) series produces all-NaN dataframe,timfeirg,closed,2015-09-11T07:04:32Z,2020-04-22T21:26:00Z,"this is from a stackoverflow question [here](http://stackoverflow.com/questions/32441403/pandas-concat-produces-all-nan), you can download serialized object [here](https://www.evernote.com/l/AH4AdfgOJJROuZSfGfDR_jZvA0zEpIHgyq0) and reproduce using **python 2.7** and **pandas 0.16.2**.

I'm trying to concat two series with multiindex using `pd.concat([a, b], axis=1)` like so:

``` python
>>>payed_orders.head()
dt          product_id
2015-01-15  10001          1
            10007          1
            10016         14
            10022          1
            10023          1
Name: payed_orders, dtype: int64

>>>refund_orders.head()
dt          product_id
2015-01-15  10007         1
            10016         4
            10030         1
2015-01-16  10007         3
            10008         1
Name: refund_orders, dtype: int64

>>>pd.concat([payed_orders.head(), refund_orders.head()], axis=1, ignore_index=False)
        payed_orders    refund_orders
dt  product_id      
2015-01-15  10001   NaN NaN
            10007   NaN NaN
            10016   NaN NaN
            10022   NaN NaN
            10023   NaN NaN
            10030   NaN NaN
2015-01-16  10007   NaN NaN
            10008   NaN NaN
```

I've checked the index type and many other stuff to make sure no obvious were made, I've read the docs to learn that concatenating and merging series and dataframes may introduce `NaN`, but I didn't find anything in the docs to explain this behavior.
"
558588137,31549,replace NotImplementedError with AttributeError,RichardBruskiewich,closed,2020-02-01T19:50:25Z,2020-04-22T22:52:33Z,"Fixes Python inspection of members - bug reported in https://github.com/pandas-dev/pandas/issues/31474

- [ ] closes #31474
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
605103684,33733,CLN: Remove unused assignment,dsaxton,closed,2020-04-22T22:03:23Z,2020-04-23T02:51:20Z,
593708746,33279,API/CLN: simplify CategoricalBlock.replace,jbrockmendel,closed,2020-04-04T00:52:22Z,2020-04-06T21:26:20Z,"- [x] closes #33272 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Makes DataFrame.replace on Categorical behave like Series.replace."
595307145,33332,REF: BlockManager.delete -> idelete,jbrockmendel,closed,2020-04-06T17:49:35Z,2020-04-06T21:28:06Z,"Moving towards all-locational inside BlockManager, also makes it easier to grep for where methods are used."
595152114,33322,TST: Don't use 'is' on strings to avoid SyntaxWarning,rebecca-palmer,closed,2020-04-06T14:13:00Z,2020-04-06T21:35:00Z,"This avoids the below warning (in Python 3.8, [user-visible in Debian](https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=956021) because they byte-compile everything on install), and possibly unspecified behaviour (though I haven't seen that in practice).

```
  /usr/lib/python3/dist-packages/pandas/tests/frame/test_alter_axes.py:241: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?
    False if (keys[0] is ""A"" and keys[1] is ""A"") else drop  # noqa: F632
```

"
595063246,33317,BUG: value_counts Int64 zero-size array to reduction,mgsnuno,closed,2020-04-06T12:12:18Z,2020-04-06T21:58:59Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

```python
pd.Series([], dtype='int64').value_counts()  # Series([], dtype: int64)
pd.Series([], dtype='Int64').value_counts()  # ValueError: zero-size array to reduction ...
```

#### Problem description
Found this while using `Int64` types in dask. Error occurred in `_meta.value_counts()`

#### Expected Output
Error not to occur.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.113-1-MANJARO
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200325
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : 1.9.0
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : None
tabulate         : None
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
593608028,33273,Pass method in __finalize__,TomAugspurger,closed,2020-04-03T19:58:08Z,2020-04-06T22:04:50Z,"This passes `method` everywhere we use `__finalize__`. I'm trying to get a better sense for

1. When we call finalize (and when we fail too)
2. When we finalize multiple times
3. The overhead of finalize

I haven't called it anywhere new yet (followup PR with that coming though)."
595425418,33339,BUG: Don't raise on value_counts for empty Int64,dsaxton,closed,2020-04-06T21:02:15Z,2020-04-06T22:17:45Z,"- [ ] closes #33317
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
594013683,33290,BUG: 2D indexing on DTA/TDA/PA,jbrockmendel,closed,2020-04-04T19:23:36Z,2020-04-06T22:33:48Z,"Broken off from #32997.

The fixture this implements in the datetimelike tests can be used in a follow-up to clean up those tests a bit"
594147198,33296,BUG: Timestamp+- ndarray[td64],jbrockmendel,closed,2020-04-04T22:55:57Z,2020-04-06T22:35:25Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Noticed a now-duplicate test that this gets rid of"
594623506,33307,TST: misplaced reduction/indexing tests,jbrockmendel,closed,2020-04-05T20:14:50Z,2020-04-06T22:37:23Z,
595471904,33344,TST: add read_json test for issue #32383,BenjaminLiuPenrose,closed,2020-04-06T22:39:38Z,2020-04-06T22:46:54Z,"- [x] closes #32383 
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
593588152,33270,REF: dispatch TDBlock.to_native_types to TDA._format_native_types,jbrockmendel,closed,2020-04-03T19:18:02Z,2020-04-06T23:20:31Z,
569543306,32207,read_excel loses spaces on ods,joernhees,closed,2020-02-23T19:16:43Z,2020-04-06T23:26:48Z,"#### Code Sample
Create a new spreadsheet with 1 column ""testcol"" in LibreOffice / OpenOffice & Excel, save as `test.ods`/`test.xlsx`:
```
testcol
this is great
4    spaces
1 trailing 
 1 leading
2  spaces  multiple  times
```
For simplicity here as zip: (1 ods, 1 xlsx): [spreadsheets.zip](https://github.com/pandas-dev/pandas/files/4242180/spreadsheets.zip)


```python
import pandas as pd
for i in pd.read_excel('test.ods', engine='odf')['testcol']:
    print(repr(i))

# output:
'this is great'
'4 spaces'
'1 trailing '
'1 leading'
'2 spaces multiple times'

for i in pd.read_excel('test.xlsx')['testcol']:
    print(repr(i))

# output:
'this is great'
'4    spaces'
'1 trailing '
' 1 leading'
'2  spaces  multiple  times'
```

#### Problem description
When reading `.ods` files (OpenOffice or LibreOffice) multiple spaces are collapsed into one, leading ones are lost, trailing ones preserved.

#### Expected Output
see excel output above.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 17.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : 0.29.15
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0
</details>

Not sure why that says `odfpy: None`, `pip list` tells me it's 1.4.1
"
592382734,33233,Ods loses spaces 32207,detrout,closed,2020-04-02T06:10:54Z,2020-04-06T23:26:52Z,"- [X] closes #32207
- [X] tests added / passed tests.io.excel.test_reader_spaces
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff` (I had to use git diff master though)
- [ ] whatsnew entry. It's only a bug fix does it need a whatsnew?

I'm not sure how you make xlsxb files so that test wasn't implement when checking for the different types of space.

There's quite possibly many other ways this parser doesn't handle the odf specification.
"
592330703,33231,REF: put concatenate_block_managers in internals.concat,jbrockmendel,closed,2020-04-02T03:23:23Z,2020-04-06T23:28:14Z,
580110283,32664,Please document expected behavior when default arguments are used.,steve-the-bayesian,closed,2020-03-12T17:45:19Z,2020-04-07T00:23:21Z,"This is a documentation request.

The docs for read_csv currently include the following:

index_col: 
int, str, sequence of int / str, or False, default None
Column(s) to use as the row labels of the DataFrame, either given as string name or column index. If a sequence of int / str is given, a MultiIndex is used.

Note: index_col=False can be used to force pandas to not use the first column as the index, e.g. when you have a malformed file with delimiters at the end of each line.

#### Problem description
The docs explain how read_csv works in every case EXCEPT the default, which is the most common.  Please document what happens when index_col=None is passed.

"
586994782,32977,DOC: Mention default behaviour of index_col in readcsv,bharatr21,closed,2020-03-24T14:19:17Z,2020-04-07T00:23:49Z,"- [x] closes #32664 
- [ ] tests added / passed (Not needed)
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry (Not needed)
"
595177342,33325,CI: Add argument doc/source/development to formatting docstrings in code_checks.sh,cleconte987,closed,2020-04-06T14:45:07Z,2020-04-07T00:59:55Z,Regarding issue #32550
589191315,33069,read_sas fails when passed a file object from GCSFS,tswast,closed,2020-03-27T14:56:17Z,2020-04-07T01:23:29Z,"#### Code Sample, a copy-pastable example if possible

From https://stackoverflow.com/q/60848250/101923

```bash
export BUCKET_NAME=swast-scratch-us
curl -L https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.XPT | gsutil cp - gs://${BUCKET_NAME}/sas_sample/Nchs/Nhanes/2017-2018/DEMO_J.XPT
```

```python
import pandas as pd
import gcsfs


bucket_name = ""swast-scratch-us""
project_id = ""swast-scratch""

fs = gcsfs.GCSFileSystem(project=project_id)
with fs.open(
    ""{}/sas_sample/Nchs/Nhanes/2017-2018/DEMO_J.XPT"".format(bucket_name),
    ""rb""
) as f:
    df = pd.read_sas(f, format=""xport"")
    print(df)
```
#### Problem description

This throws the following exception:

```
Traceback (most recent call last):
  File ""after.py"", line 15, in <module>
    df = pd.read_sas(f, format=""xport"")
  File ""/Users/swast/miniconda3/envs/scratch/lib/python3.7/site-packages/pandas/io/sas/sasreader.py"", line 70, in read_sas
    filepath_or_buffer, index=index, encoding=encoding, chunksize=chunksize
  File ""/Users/swast/miniconda3/envs/scratch/lib/python3.7/site-packages/pandas/io/sas/sas_xport.py"", line 280, in __init__
    contents = contents.encode(self._encoding)
AttributeError: 'bytes' object has no attribute 'encode'
(scratch) 
```

#### Expected Output

```
          SEQN  SDDSRVYR  RIDSTATR  RIAGENDR  ...  SDMVSTRA  INDHHIN2  INDFMIN2  INDFMPIR
0      93703.0      10.0       2.0       2.0  ...     145.0      15.0      15.0      5.00
1      93704.0      10.0       2.0       1.0  ...     143.0      15.0      15.0      5.00
2      93705.0      10.0       2.0       2.0  ...     145.0       3.0       3.0      0.82
3      93706.0      10.0       2.0       1.0  ...     134.0       NaN       NaN       NaN
4      93707.0      10.0       2.0       1.0  ...     138.0      10.0      10.0      1.88
...        ...       ...       ...       ...  ...       ...       ...       ...       ...
9249  102952.0      10.0       2.0       2.0  ...     138.0       4.0       4.0      0.95
9250  102953.0      10.0       2.0       1.0  ...     137.0      12.0      12.0       NaN
9251  102954.0      10.0       2.0       2.0  ...     144.0      10.0      10.0      1.18
9252  102955.0      10.0       2.0       2.0  ...     136.0       9.0       9.0      2.24
9253  102956.0      10.0       2.0       1.0  ...     142.0       7.0       7.0      1.56

[9254 rows x 46 columns]
```

Note: the expected output **is** printed when a local file is read.

#### Output of ``pd.show_versions()``

<details>

Python 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 14:38:56) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas as pd
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.4.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.18.1
pytz             : 2019.2
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0.post20200311
Cython           : None
pytest           : 5.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.7.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : None
fastparquet      : None
gcsfs            : 0.6.0
lxml.etree       : 4.5.0
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : 0.11.0
pyarrow          : 0.15.1
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
xarray           : 0.12.3
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
592827387,33244,CLN: Clean nanops.get_corr_func,dsaxton,closed,2020-04-02T18:06:58Z,2020-04-07T01:25:31Z,"Small cleaning (instead of creating a dictionary of functions and then returning only one, just return the one)"
593446874,33261,PERF: masked ops for reductions (min/max),jorisvandenbossche,closed,2020-04-03T15:04:06Z,2020-04-07T06:57:46Z,"Follow-up on https://github.com/pandas-dev/pandas/pull/30982, adding similar mask reduction but now for min/max."
472417593,27573,API: Make most arguments for read_html and read_json keyword-ony,alexitkes,closed,2019-07-24T17:56:02Z,2020-04-07T08:08:42Z,"As mentioned in #27544 it is better to use keyword-only arguments for functions with too many arguments. A deprecation warning will be displayed if `read_html` get more than 2 positional arguments (`io` and `match`) or `read_json` get more than 1 positional argument (`path_or_buf`).

- [x] tests added / passed
   Three groups of tests are actually needed
   - [x] Tests dof the `deprecate_nonkeyword_arguments` decoratorr
   - [x] Check whether the `read_json` function emits a `FutureWarning` whenever necessary and does not emit it whenever not.
   - [x] Check whether the `read_html` function emits a `FutureWarning` whenever necessary and does not emit it whenever not.
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
593351573,33259,DOC: Added an example for each series.dt field accessor,ShaharNaveh,closed,2020-04-03T12:35:53Z,2020-04-07T10:48:22Z,"- [x] xref https://github.com/pandas-dev/pandas/pull/33208#discussion_r402189839
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

I can also add examples that would cover, ````TimedeltaProperties``` and ```PeriodProperties```, but I think that would be over verbose, wdyt?"
595240883,33328, DOC: Fix examples in `pandas/core/strings.py`,ShaharNaveh,closed,2020-04-06T16:08:05Z,2020-04-07T10:49:02Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
594747653,33310,CLN: remove fill_tuple kludge,jbrockmendel,closed,2020-04-06T01:35:09Z,2020-04-07T11:51:11Z,
495853978,28528,WEB: Deployment of the new website in production,datapythonista,closed,2019-09-19T14:55:52Z,2020-04-07T11:57:42Z,"Until now, we've been deploying the website manually afaik, from the separate repo.

What I would do with the new website is next (open to discussion):
- Have a job running to automatically deploy the website from master, possible frequencies:
  - Every commit
  - Daily
  - Weekly
- Build the website normally, as we do in https://github.com/pandas-dev/pandas/blob/master/azure-pipelines.yml#L122
- Deploy it to the server with rsync, something like: `rsync avz --delete  --exclude=""docs"" ~/web/build pandas_ci@pandas.io:/var/www/pandas`

For the docs, I see two main options:
- Inside `docs/` have a directory per language (currently English), and inside them one per version (with a symlink `stable/ -> 0.25.1`), so the urls would be something like pandas.io/docs/en/stable or pandas.io/docs/en/0.24.0 (and redirect pandas.io/docs/ to pandas.io/docs/en/stable)
- Copy the docs directly in pandas.io/docs/ and keep the old version in directories there pandas.io/docs/0.24.0/

In both cases, we can deploy the master docs into a `dev/` directory together with the versions.

I think the first option is a bit simpler to maintain, and the second makes the url a bit simpler. I don't have a strong preference.

I think this is very simple, but **requires that all pages that we don't want to version are in the web, and not in the docs**. So, if the roadmap is in the docs, the version we will have will be the stable version (0.25.1) and not master.

@jorisvandenbossche you proposed to have the next pages in the docs:
- Roadmap
- Ecosystem
- Contributing to pandas

Do you have a proposal for how to serve or deploy these? Or are you ok having the stable version of those? I thought about it, and I couldn't find any option I liked to keep those in the docs, that's why I'm proposing to move them to the website."
595443820,33341,CI: Sync web and dev docs automatically with prod server,datapythonista,closed,2020-04-06T21:35:09Z,2020-04-07T11:57:43Z,"- [X] closes #28528

This forgets about the OVH server, and after every merge to master will synchronize the repo web and docs in the production server.

**Note** that this will delete any file in the server that is not known, except the cheat sheets, and all the docs versions in `pandas-doc`. I don't think anything else should be kept, but if there is anything else that we upload manually, please let me know. I made a backup in the server, so if we delete anything by accident with this, it can be easily restored.

CC: @TomAugspurger @jorisvandenbossche "
589203680,33070,BUG: AttributeError when read_sas used with GCS,tswast,closed,2020-03-27T15:14:19Z,2020-04-07T14:09:48Z,"With GCSFS (and when a binary file buffer is passed in), the output from
the file read is `bytes`, not a Unicode `str`. The `encode` call is unnecessary
in this case.

- [x] closes #33069 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
558435184,31525,Unpin openpyxl,WillAyd,closed,2020-01-31T23:59:19Z,2020-04-07T14:43:02Z,"I *think* we pinned this originally because of issues with 3.0.2 specifically, but looks like a newer version is available on conda

ref #30009"
595908708,33366,CI: Not deleting docs symlink in prod,datapythonista,closed,2020-04-07T14:25:00Z,2020-04-07T14:43:13Z,"In #33341 I forgot to exclude the `docs` symlink for deletion, meaning that it'll be deleted when a PR is merged. And the docs will be unavailable. Fixing it here. 

@pandas-dev/pandas-core can we prioritize this please."
572277170,32305,BUG: set df.plot.area linewidth to 0,leogermond,closed,2020-02-27T18:39:27Z,2020-04-07T15:47:11Z,"
- [x] closes #32106 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry (`1.1.0`)
"
522211185,29595,fixed issue of empty dataframe combine_first,iamrajhans,closed,2019-11-13T13:06:33Z,2020-04-07T16:06:37Z,"- [ ] closes #29562
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry - fixed issue of combining two empty dataframe. 
"
547768568,30870,BUG: set_index() and reset_index() not preserving object dtypes ,trevorbye,closed,2020-01-09T22:51:42Z,2020-04-07T16:22:43Z,"* changes to `set_index()` and `ensure_index_from_sequences()` to pass current dtype of key through call stack onto Index constructor, rather than letting Index constructor infer the dtype. For now I just aimed to patch a single key param from `set_index()`, as that is the lowest hanging fruit for this issue I think.

* removed `maybe_convert_objects()` call from `_maybe_casted_values()`. This will preserve an `object` dtype on a `set_index().reset_index()` round-trip. I came to the exact same conclusion as the user in [this PR comment](https://github.com/pandas-dev/pandas/pull/27370#issuecomment-511051996), for context. This makes sense conceptually, if your index was an object dtype, it should be an object dtype as a column. Of course, this is somewhat subjective so let me know your opinions, but this is what I interpreted from the discussion on the git issue.

* both of these changes break quite a few existing tests, most of which are testing specifically for the exact opposite conditions that this PR is intended to patch **e.g.** tests that look for object dtypes being coerced to more-specific types. What's the process for handling failed tests for a change like this? Would you attempt to refactor all, or delete the ones where the logic is no longer something to test for? I also ran `pytest pandas` on a fresh pull of master and had 13 fails, with 108 failures on my branch specifically, so I am also curious what to do about those previously-failing tests, or what is acceptable for a merge.

- [x] closes #30517 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
595549429,33353,TST: implement test_getitem for Series,jbrockmendel,closed,2020-04-07T02:41:11Z,2020-04-07T16:43:46Z,"Lime with #33348, lots of scattered stuff is going to end up here"
595532171,33348,TST: implement test_setitem for Series,jbrockmendel,closed,2020-04-07T01:44:55Z,2020-04-07T16:44:26Z,Lots of scattered tests that belong here.  Also lots of scattered local branches.
227785440,16322,PyQt5 and pandas,dibgerge,closed,2017-05-10T19:28:44Z,2020-04-07T18:23:57Z,"I just updated my conda installation to Pandas 0.20.1, and it does not load anymore since it depends on PyQt4 which was updated to PyQt5. 

To make Pandas 0.20.1 load with PyQt5,  I have to modify a function within the ``pandas.io.clipboard.clipboards`` module as follows:

#### Modified function within ``pandas.io.clipboard.clipboards``

```python

def init_qt_clipboard():
    # $DISPLAY should exist
    from PyQt5.QtWidgets import QApplication

    # use the global instance if it exists
    app = QApplication.instance() or QApplication([])

    def copy_qt(text):
        cb = app.clipboard()
        cb.setText(text)

    def paste_qt():
        cb = app.clipboard()
        return text_type(cb.text())

    return copy_qt, paste_qt
```
#### Problem description

The current behavior requires ``PyQt4`` by default, even if it is not needed to be used, which was giving me the error when import pandas:

``ImportError: No module named 'PyQt4.QtGui'``. 

Changing the module as described above solved the problem.

"
592292517,33229,REF: sql insert_data operate column-wise to avoid internals,jbrockmendel,closed,2020-04-02T01:11:32Z,2020-04-07T18:26:37Z,
587031544,32978,DOC: Fix capitalization among headings in documentation files (#32550),themien,closed,2020-03-24T15:09:51Z,2020-04-07T18:29:24Z,"Files updated:

```
- [ ] doc/source/development/policies.rst
- [ ] doc/source/development/roadmap.rst
- [ ] doc/source/ecosystem.rst
- [ ] doc/source/getting_started/comparison/comparison_with_sas.rst
- [ ] doc/source/getting_started/comparison/comparison_with_sql.rst
```

more files updated in different PRs: #32944, #32991"
590475431,33147,DOC: Fix capitalization among headings in documentation files (#32550),themien,closed,2020-03-30T17:48:56Z,2020-04-07T18:29:58Z,"Headers updated for the following files:

```
- [x] doc/source/user_guide/categorical.rst
- [x] doc/source/user_guide/computation.rst
- [x] doc/source/user_guide/cookbook.rst
- [x] doc/source/user_guide/gotchas.rst
- [x] doc/source/user_guide/groupby.rst
```

Other files updated in #32944, #32978 , #32991 and #33149"
586439089,32944,DOC: Fix capitalization among headings in documentation files (#32550) ,themien,closed,2020-03-23T18:53:00Z,2020-04-07T18:35:04Z,"Files updated:

```
doc/source/development/contributing_docstring.rst
doc/source/development/developer.rst
doc/source/development/extending.rst
doc/source/development/maintaining.rst
doc/source/development/meeting.rst
```
"
590499217,33149,DOC: Fix capitalization among headings in documentation files (#32550),themien,closed,2020-03-30T18:27:38Z,2020-04-07T18:38:10Z,"Headers updated for the following files:

```
- [x] doc/source/user_guide/options.rst
- [x] doc/source/user_guide/reshaping.rst
- [x] doc/source/user_guide/text.rst
- [x] doc/source/user_guide/timeseries.rst
- [x] doc/source/user_guide/visualization.rst
```

Other files updated in #32944, #32978, #32991  and #33147
"
593500066,33263,DOC: Fixed examples in `pandas/core/aggregation.py`,ShaharNaveh,closed,2020-04-03T16:28:45Z,2020-04-07T18:49:39Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
595533600,33349,REF: misplaced tests in test_partial_slicing files,jbrockmendel,closed,2020-04-07T01:49:47Z,2020-04-07T19:13:10Z,"Whats left in these after this are all testing Series methods, so belong elsewhere."
595551140,33354,REF: collect Index constructor tests,jbrockmendel,closed,2020-04-07T02:46:52Z,2020-04-07T19:32:02Z,
594269468,33299,BUG: DataFrame._item_cache not cleared on on .copy(),neilkg,closed,2020-04-05T05:26:53Z,2020-04-07T19:52:35Z,"- [x] closes #31784
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
591072238,33178,WEB: Moving pandas blog to the website,datapythonista,closed,2020-03-31T12:35:55Z,2020-04-07T20:04:50Z,"Moving the pandas blog posts (now in a separate repo/project) to the `web/community/blog/` in our website. The static site generator we're using for the web, renders them automatically using our layout:

![pandas_blog_2](https://user-images.githubusercontent.com/10058240/78026717-01a10b80-7354-11ea-830e-65af77ed4838.png)

The posts list doesn't have much visual change, just links to the new path:

![pandas_blog_1](https://user-images.githubusercontent.com/10058240/78026785-1b425300-7354-11ea-8cdd-ab632b62d41e.png)

The main change here, besides adding the posts markdown files (and images) is that the static site generator is now able to not only fetch from rss, but also from the file system.

CC: @TomAugspurger 

Btw @TomAugspurger , I increased the number of posts we show in the list, and see that there are some about dask-ml and other things unrelated to pandas being fetched from your blog. Would be great if you can add a pandas tag to your blog, so we only fetch the relevant content."
595547325,33352,CI: Add argument doc/source/development to formatting docstrings in code_checks.sh,cleconte987,closed,2020-04-07T02:34:42Z,2020-04-07T20:21:12Z,"Issue #32550
"
595536020,33350,"REF: DatetimeIndex test_insert, test_delete",jbrockmendel,closed,2020-04-07T01:58:09Z,2020-04-07T21:09:55Z,
596088232,33375,"CLN: json_table_schema remove unused arg, annotate",jbrockmendel,closed,2020-04-07T19:07:48Z,2020-04-07T23:01:32Z,cc @WillAyd 
589914762,33136,SeriesGroupBy.quantile doesn't work for nullable integers,dsaxton,closed,2020-03-29T23:54:45Z,2020-04-07T23:10:11Z,"```python
import pandas as pd

df = pd.DataFrame(
    {""a"": [""x"", ""x"", ""y"", ""y""], ""b"": pd.array([1, 2, 3, 4], dtype=""Int64"")}
)
df.groupby(""a"")[""b""].quantile(0.5)
```
raises
```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-1-ef27f953b945> in <module>
      4     {""a"": [""x"", ""x"", ""y"", ""y""], ""b"": pd.array([1, 2, 3, 4], dtype=""Int64"")}
      5 )
----> 6 df.groupby(""a"")[""b""].quantile(0.5)

~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in quantile(self, q, interpolation)
   1911                 post_processing=post_processor,
   1912                 q=q,
-> 1913                 interpolation=interpolation,
   1914             )
   1915         else:

~/opt/miniconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in _get_cythonized_result(self, how, cython_dtype, aggregate, needs_values, needs_mask, needs_ngroups, result_is_index, pre_processing, post_processing, **kwargs)
   2289                 func = partial(func, ngroups)
   2290 
-> 2291             func(**kwargs)  # Call func to modify indexer values in place
   2292 
   2293             if result_is_index:

pandas/_libs/groupby.pyx in pandas._libs.groupby.__pyx_fused_cpdef()

TypeError: No matching signature found
```

cc @ghuname xref https://github.com/pandas-dev/pandas/issues/33071"
588080925,33029,DOC: Fixing links to the contributing page,ShaharNaveh,closed,2020-03-26T00:57:50Z,2020-04-07T23:21:43Z,"Not 100% sure on this changes.

---

Feel free to close this at anytime."
594169665,33297,CLN: assorted cleanups,jbrockmendel,closed,2020-04-04T23:59:58Z,2020-04-07T23:30:08Z,Mostly annotations
596234214,33383,FEATURE: pandas.Series.query()(#22347),yaolan4,closed,2020-04-08T00:45:11Z,2020-04-08T00:45:33Z,"
"
596093329,33377,BUG: is_categorical shouldnt recognize Dtype objects,jbrockmendel,closed,2020-04-07T19:17:35Z,2020-04-08T01:12:08Z,"is_categorical(obj) is used in a couple places where we then go on to access `obj.dtype`, so this should not return `True` if we have a `CategoricalDtype` object."
595331079,33334,BLD: Increase minimum version of Cython to 0.29.16,mgmarino,closed,2020-04-06T18:26:27Z,2020-04-08T04:52:18Z,"- In particular, this fixes a bug in code returning ctuples
    - cython/cython#2745 
    - cython/cython#1427
- This is a prereq for #33220

- [x] whatsnew entry"
595969286,33372,BLD: Compatibility issues with master branch cython,ShaharNaveh,closed,2020-04-07T15:47:14Z,2020-04-08T09:12:34Z,"Continuation of issue #33224

---

With the latest version of cython, running ```python setup.py build_ext --inplace -j 0``` gives the following errors:

---

#### ```pandas/_libs/join.pyx```:

<details>

```
building 'pandas._libs.join' extension
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fno-semantic-interposition -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fPIC -DNPY_NO_DEPRECATED_API=0 -Ipandas/_libs/src/klib -I/home/user/Venvs/cythonM-pandas/lib/python3.8/site-packages/numpy/core/include -I/home/user/Venvs/cythonM-pandas/include -I/usr/include/python3.8 -c pandas/_libs/join.c -o build/temp.linux-x86_64-3.8/pandas/_libs/join.o
In file included from /usr/include/python3.8/tupleobject.h:41,
                 from /usr/include/python3.8/Python.h:105,
                 from pandas/_libs/join.c:32:
pandas/_libs/join.c: In function ‘__pyx_pf_6pandas_5_libs_4join_12inner_join_indexer.isra.0’:
/usr/include/python3.8/cpython/tupleobject.h:27:60: warning: array subscript -1 is below array bounds of ‘PyObject *[1]’ {aka ‘struct _object *[1]’} [-Warray-bounds]
   27 | #define PyTuple_GET_ITEM(op, i) (_PyTuple_CAST(op)->ob_item[i])
      |                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~
pandas/_libs/join.c:204503:27: note: in expansion of macro ‘PyTuple_GET_ITEM’
204503 |             PyObject *r = PyTuple_GET_ITEM(o, n);
       |                           ^~~~~~~~~~~~~~~~
pandas/_libs/join.c: In function ‘__pyx_pf_6pandas_5_libs_4join_10left_join_indexer.isra.0’:
/usr/include/python3.8/cpython/tupleobject.h:27:60: warning: array subscript -1 is below array bounds of ‘PyObject *[1]’ {aka ‘struct _object *[1]’} [-Warray-bounds]
   27 | #define PyTuple_GET_ITEM(op, i) (_PyTuple_CAST(op)->ob_item[i])
      |                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~
pandas/_libs/join.c:204503:27: note: in expansion of macro ‘PyTuple_GET_ITEM’
204503 |             PyObject *r = PyTuple_GET_ITEM(o, n);
       |                           ^~~~~~~~~~~~~~~~
pandas/_libs/join.c: In function ‘__pyx_pf_6pandas_5_libs_4join_8left_join_indexer_unique.isra.0’:
/usr/include/python3.8/cpython/tupleobject.h:27:60: warning: array subscript -1 is below array bounds of ‘PyObject *[1]’ {aka ‘struct _object *[1]’} [-Warray-bounds]
   27 | #define PyTuple_GET_ITEM(op, i) (_PyTuple_CAST(op)->ob_item[i])
      |                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~
pandas/_libs/join.c:204503:27: note: in expansion of macro ‘PyTuple_GET_ITEM’
204503 |             PyObject *r = PyTuple_GET_ITEM(o, n);
       |                           ^~~~~~~~~~~~~~~~
pandas/_libs/join.c: In function ‘__pyx_pf_6pandas_5_libs_4join_14outer_join_indexer.isra.0’:
/usr/include/python3.8/cpython/tupleobject.h:27:60: warning: array subscript -1 is below array bounds of ‘PyObject *[1]’ {aka ‘struct _object *[1]’} [-Warray-bounds]
   27 | #define PyTuple_GET_ITEM(op, i) (_PyTuple_CAST(op)->ob_item[i])
      |                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~
pandas/_libs/join.c:204503:27: note: in expansion of macro ‘PyTuple_GET_ITEM’
204503 |             PyObject *r = PyTuple_GET_ITEM(o, n);
       |                           ^~~~~~~~~~~~~~~~
```

</details>

---


#### ```pandas/_libs/reshape.pyx```:

<details>

```
building 'pandas._libs.reshape' extension
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=x86-64 -mtune=generic -O3 
-pipe -fno-plt -fno-semantic-interposition -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -march=x86-64 -mtun
e=generic -O3 -pipe -fno-plt -fPIC -DNPY_NO_DEPRECATED_API=0 -I/home/user/Venvs/cythonM-pandas/lib/python3.8/
site-packages/numpy/core/include -I/home/user/Venvs/cythonM-pandas/include -I/usr/include/python3.8 -c pandas
/_libs/reshape.c -o build/temp.linux-x86_64-3.8/pandas/_libs/reshape.o
In file included from /usr/include/python3.8/tupleobject.h:41,
                 from /usr/include/python3.8/Python.h:105,
                 from pandas/_libs/reshape.c:30:
pandas/_libs/reshape.c: In function ‘__pyx_pf_6pandas_5_libs_7reshape_unstack.isra.0’:
/usr/include/python3.8/cpython/tupleobject.h:27:60: warning: array subscript -1 is below array bounds of ‘PyObject *[1]’ {aka ‘struct _object *[1]’} [-Warray-bounds]
   27 | #define PyTuple_GET_ITEM(op, i) (_PyTuple_CAST(op)->ob_item[i])
      |                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~
pandas/_libs/reshape.c:29728:27: note: in expansion of macro ‘PyTuple_GET_ITEM’
29728 |             PyObject *r = PyTuple_GET_ITEM(o, n);
      |                           ^~~~~~~~~~~~~~~~
```
</details>

---

[join_zip.zip](https://github.com/pandas-dev/pandas/files/4445598/join_zip.zip)

[reshape_zip.zip](https://github.com/pandas-dev/pandas/files/4445599/reshape_zip.zip)

---

Output of ```pd.show_versions()```:

<details>


INSTALLED VERSIONS
------------------
commit           :  bd91f4577c5b54a72a4e092a7d7db43fec1b46bb
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.5.13.b-1-hardened
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0.dev0+1159.gbd91f4577
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3
Cython           : 3.0a0 (https://github.com/cython/cython/tree/30c891e0dc36a553455df9064ecb1f13414a12d2)
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>


"
120236046,11754,DataFrame.iat indexing with duplicate columns,wavexx,closed,2015-12-03T18:18:37Z,2020-04-08T09:55:40Z,"```
# error
In [27]: pd.DataFrame([[1, 1]], columns=['x','x']).iat[0,0]
TypeError: len() of unsized object

# ok
In [26]: pd.DataFrame([[1, 1]], columns=['x','y']).iat[0,0]
Out[26]: 1
```

I have some weird issue in a DataFrame I'm creating from a row-based array.
Using python3 and pandas 0.17.1 (from debian unstable), I get:

```
df = pandas.DataFrame(data=data[1:], columns=data[0])
df.iat[0, 0]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python3/dist-packages/pandas/core/indexing.py"", line 1555, in __getitem__
    return self.obj.get_value(*key, takeable=self._takeable)
  File ""/usr/lib/python3/dist-packages/pandas/core/frame.py"", line 1808, in get_value
    series = self._iget_item_cache(col)
  File ""/usr/lib/python3/dist-packages/pandas/core/generic.py"", line 1116, in _iget_item_cache
    lower = self.take(item, axis=self._info_axis_number, convert=True)
  File ""/usr/lib/python3/dist-packages/pandas/core/generic.py"", line 1371, in take
    convert=True, verify=True)
  File ""/usr/lib/python3/dist-packages/pandas/core/internals.py"", line 3628, in take
    axis=axis, allow_dups=True)
  File ""/usr/lib/python3/dist-packages/pandas/core/internals.py"", line 3510, in reindex_indexer
    indexer, fill_tuple=(fill_value,))
  File ""/usr/lib/python3/dist-packages/pandas/core/internals.py"", line 3536, in _slice_take_blocks_ax0
    slice_or_indexer, self.shape[0], allow_fill=allow_fill)
  File ""/usr/lib/python3/dist-packages/pandas/core/internals.py"", line 4865, in _preprocess_slice_or_indexer
    return 'fancy', indexer, len(indexer)
TypeError: len() of unsized object
```

Interestingly, I can otherwise manage the dataframe just fine.
The same code, running under python2.7 shows no issue.

What could be the cause of such error?
"
595766759,33362,TST: iat with duplicate column names,ShaharNaveh,closed,2020-04-07T10:34:43Z,2020-04-08T10:02:14Z,"- [x] closes #11754
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
591552415,33194,Pandas 1.03: pandas.tseries.offsets.QuarterBegin not showing correct date ,patrickxli,closed,2020-04-01T01:35:06Z,2020-04-08T11:00:59Z,"#### Code Sample, a copy-pastable example if possible

```python
from datetime import datetime
from pandas.tseries.offsets import QuarterBegin

now = datetime.datetime(2020, 1, 12)

now - QuarterBegin()

```
#### Problem description
The code shows the following results:

now - QuarterBegin()
Out[28]: Timestamp('2019-12-01 00:00:00')

which is not expected.

#### Expected Output
Timestamp('2020-01-01 00:00:00')

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
pd.show_versions()
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : 0.29.15
pytest           : 5.3.5
hypothesis       : 5.5.4
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.48.0

</details>
"
592213981,33224,compatibility issues with master branch cython,tacaswell,closed,2020-04-01T21:25:59Z,2020-04-08T13:23:00Z,"On the cython master branch the generated c code has unused  functions which fails the build with:

```

  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fno-semantic-interposition -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fPIC -DNPY_NO_DEPRECATED_API=0 -I./pandas/_libs -Ipandas/_libs/src/klib -I/home/tcaswell/.virtualenvs/sys38/lib/python3.8/site-packages/numpy/core/include -I/home/tcaswell/.virtualenvs/sys38/include -I/usr/include/python3.8 -c pandas/_libs/algos.c -o build/temp.linux-x86_64-3.8/pandas/_libs/algos.o -Werror
    pandas/_libs/algos.c:87270:18: error: ‘__pyx_pf_6pandas_5_libs_5algos_762__defaults__’ defined but not used [-Werror=unused-function]
    87270 | static PyObject *__pyx_pf_6pandas_5_libs_5algos_762__defaults__(CYTHON_UNUSED PyObject *__pyx_self) {
          |                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    pandas/_libs/algos.c:78766:18: error: ‘__pyx_pf_6pandas_5_libs_5algos_744__defaults__’ defined but not used [-Werror=unused-function]
    78766 | static PyObject *__pyx_pf_6pandas_5_libs_5algos_744__defaults__(CYTHON_UNUSED PyObject *__pyx_self) {
          |                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    pandas/_libs/algos.c:71391:18: error: ‘__pyx_pf_6pandas_5_libs_5algos_726__defaults__’ defined but not used [-Werror=unused-function]
    71391 | static PyObject *__pyx_pf_6pandas_5_libs_5algos_726__defaults__(CYTHON_UNUSED PyObject *__pyx_self) {
          |                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    pandas/_libs/algos.c:62479:18: error: ‘__pyx_pf_6pandas_5_libs_5algos_724__defaults__’ defined but not used [-Werror=unused-function]
    62479 | static PyObject *__pyx_pf_6pandas_5_libs_5algos_724__defaults__(CYTHON_UNUSED PyObject *__pyx_self) {
          |                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    pandas/_libs/algos.c:56193:18: error: ‘__pyx_pf_6pandas_5_libs_5algos_678__defaults__’ defined but not used [-Werror=unused-function]
    56193 | static PyObject *__pyx_pf_6pandas_5_libs_5algos_678__defaults__(CYTHON_UNUSED PyObject *__pyx_self) {
          |                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    pandas/_libs/algos.c:50908:18: error: ‘__pyx_pf_6pandas_5_libs_5algos_632__defaults__’ defined but not used [-Werror=unused-function]
    50908 | static PyObject *__pyx_pf_6pandas_5_libs_5algos_632__defaults__(CYTHON_UNUSED PyObject *__pyx_self) {
          |                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    pandas/_libs/algos.c:40806:18: error: ‘__pyx_pf_6pandas_5_libs_5algos_586__defaults__’ defined but not used [-Werror=unused-function]
    40806 | static PyObject *__pyx_pf_6pandas_5_libs_5algos_586__defaults__(CYTHON_UNUSED PyObject *__pyx_self) {
          |                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    pandas/_libs/algos.c:34476:18: error: ‘__pyx_pf_6pandas_5_libs_5algos_540__defaults__’ defined but not used [-Werror=unused-function]
    34476 | static PyObject *__pyx_pf_6pandas_5_libs_5algos_540__defaults__(CYTHON_UNUSED PyObject *__pyx_self) {
          |                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    pandas/_libs/algos.c:29147:18: error: ‘__pyx_pf_6pandas_5_libs_5algos_494__defaults__’ defined but not used [-Werror=unused-function]
    29147 | static PyObject *__pyx_pf_6pandas_5_libs_5algos_494__defaults__(CYTHON_UNUSED PyObject *__pyx_self) {
          |                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    pandas/_libs/algos.c:19248:18: error: ‘__pyx_pf_6pandas_5_libs_5algos_448__defaults__’ defined but not used [-Werror=unused-function]
    19248 | static PyObject *__pyx_pf_6pandas_5_libs_5algos_448__defaults__(CYTHON_UNUSED PyObject *__pyx_self) {
          |                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    pandas/_libs/algos.c:12162:18: error: ‘__pyx_pf_6pandas_5_libs_5algos_446__defaults__’ defined but not used [-Werror=unused-function]
    12162 | static PyObject *__pyx_pf_6pandas_5_libs_5algos_446__defaults__(CYTHON_UNUSED PyObject *__pyx_self) {
          |                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    cc1: all warnings being treated as errors
    error: command 'gcc' failed with exit status 1
```

and similar warnings from other modules (this just happens to be the first one).

If I remove the `""-Werror""` it seems to build ok.

Not sure if this should be reported here at cython.  I will likely try to bisect this to the cython change later today."
596277943,33387,REF: numeric index test_indexing,jbrockmendel,closed,2020-04-08T03:13:05Z,2020-04-08T15:09:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
596241703,33385,DEPR: is_categorical,jbrockmendel,closed,2020-04-08T01:11:15Z,2020-04-08T15:10:30Z,"xref #33377 the current behavior of is_categorical is buggy, besides its redundant with is_categorical_dtype"
594970204,33313,BUG: Bad error message on read_parquet() when wrong version of pyarrow is installed,jfcorbett,closed,2020-04-06T09:53:19Z,2020-04-08T16:45:46Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

With `pyarrow<0.13` installed, 
```python
pd.read_parquet('some_file.parquet')
```

#### Problem description

The error message says that `pyarrow` should be installed, even though `pyarrow` is already installed – which makes the error difficult to diagnose and fix, when in fact, the problem is an incompatible _version_ of pyarrow.

```
  File ""[...]\lib\site-packages\pandas\io\parquet.py"", line 33, in get_engine
    ""Unable to find a usable engine; ""
ImportError: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.
pyarrow or fastparquet is required for parquet support
```

#### Expected Output

The message should state the real cause of the error, namely an incompatible version of pyarrow. Like:

```
  File ""[...]\lib\site-packages\pandas\io\parquet.py"", line 40, in get_engine
    return PyArrowImpl()
  File ""[...]\lib\site-packages\pandas\io\parquet.py"", line 75, in __init__
    ""pyarrow"", extra=""pyarrow is required for parquet support.""
  File ""[...]\lib\site-packages\pandas\compat\_optional.py"", line 109, in import_optional_dependency
    raise ImportError(msg)
ImportError: Pandas requires version '0.13.0' or newer of 'pyarrow' (version '0.11.1' currently installed).
```

This helpful error message does in fact get generated at a lower level in `pandas.compat._optional.import_optional_dependency()`, but then gets swallowed when its client `pandas.io.parquet.get_engine(engine=""auto"")` raises a new `ImportError` from scratch. 

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 61 Stepping 4, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.11.1
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
557627405,31464,"read_json with typ=""series"" of json list of bools results in timestamps/Exception",stewit,closed,2020-01-30T16:59:16Z,2020-04-08T17:13:08Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
pd.read_json('[true, true, false]', typ=""series"")
```
results in the following Pandas Series object in older Pandas versions:
```
0   1970-01-01 00:00:01
1   1970-01-01 00:00:01
2   1970-01-01 00:00:00
dtype: datetime64[ns]
```
Since 1.0.0 it raises TypeError: <class 'bool'> is not convertible to datetime


#### Problem description
The expected output would be a Pandas Series of bools. Note that
* with typ=""frame"" it works and the result is a dataframe with one column with bool values
* with convert_dates set to False correctly outputs a Series of boolean values

This is a problem because
* users would expect a Series of bools (and neither an exception nor a series of timestamps)
* it is inconsistent with the ""frame"" case

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.13-arch1-1
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : de_DE.UTF-8
LOCALE           : de_DE.UTF-8

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 44.0.0
Cython           : 0.29.14
pytest           : 5.2.4
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.2.4
pyxlsb           : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : 1.3.11
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
591671855,33201,ENH: provide standard BaseIndexers in pandas.api.indexers,AlexKirko,closed,2020-04-01T06:51:32Z,2020-04-08T17:20:50Z,"### Problem description

During my work on generalizing our rolling window functions, @mroeschke and @jreback have pointed out that it would be useful to expose a couple `BaseIndexer` subclasses as a part of our api (in pandas.api.indexers, to be specific). For instance, forward-looking windows are fairly common in signal analysis and the like, and it would benefit users if we had a default implementation of those.

This is the implementation I'm currently using for testing:
```python

    class ForwardIndexer(BaseIndexer):
        def get_window_bounds(self, num_values, min_periods, center, closed):
            start = np.empty(num_values, dtype=np.int64)
            end = np.empty(num_values, dtype=np.int64)
            for i in range(num_values):
                if i + min_periods <= num_values:
                    start[i] = i
                    end[i] = min(i + self.window_size, num_values)
                else:
                    start[i] = i
                    end[i] = i + 1
            return start, end
```
This should be rewritten to avoid the for loop, as it slows the function down a lot.

We could also add a backward indexer for completeness or skip it, because it can be done by supplying an integer when creating the rolling window object. Tell me what you think.

I'd be happy to submit a PR with a proposed implementation in a day or two.

#### Implementation details

I think we can put the implementation of the new class/classes in `pandas.core.window.indexers`.

#### Cross-references

xref #32865 
xref #33180
"
545996183,30766,BUG: Fix reindexing with multi-indexed DataFrames,ChrisRobo,closed,2020-01-06T23:35:31Z,2020-04-08T17:27:44Z,"- [x] closes https://github.com/pandas-dev/pandas/issues/29896
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Addresses an issue which appears to have existed since 0.23.0 where bugs in the `get_indexer()` method for the `MultiIndex` class cause incorrect reindexing behavior for multi-indexed DataFrames.

motivating, example, from (the issue)
```python
>>> 
>>> df = pd.DataFrame({
...     'a': [0, 0, 0, 0],
...     'b': [0, 2, 3, 4],
...     'c': ['A', 'B', 'C', 'D']
... }).set_index(['a', 'b'])
>>> 
>>> df
     c
a b   
0 0  A
  2  B
  3  C
  4  D
>>> df.index
MultiIndex([(0, 0),
            (0, 2),
            (0, 3),
            (0, 4)],
           names=['a', 'b'])
>>> mi_2 = pd.MultiIndex.from_product([[0], [-1, 0, 1, 3, 4, 5]])
>>> mi_2
MultiIndex([(0, -1),
            (0,  0),
            (0,  1),
            (0,  3),
            (0,  4),
            (0,  5)],
           )
```

as expected, without a `method` value:
```python
>>> df.reindex(mi_2)
        c
0 -1  NaN
   0    A
   1  NaN
   3    C
   4    D
   5  NaN
```

using `method=""backfill""`, it is:
```python
>>> 
>>> df.reindex(mi_2, method=""backfill"")
      c
0 -1  A
   0  A
   1  D
   3  A
   4  A
   5  C
>>>
```
but should (IMHO) be:
```python
>>> df.reindex(mi_2, method=""backfill"")
        c
0 -1    A
   0    A
   1    B
   3    C
   4    D
   5  NaN
>>>
```

similarly, using `method=""pad""`, it is:
```python
>>> df.reindex(mi_2, method=""pad"")
        c
0 -1  NaN
   0  NaN
   1    D
   3  NaN
   4    A
   5    C
```
but should (IMHO) be:
```python
>>> df.reindex(mi_2, method=""pad"")
        c
0 -1  NaN
   0    A
   1    A
   3    C
   4    D
   5    D
```"
595439576,33340,"REF: replace column-wise, remove BlockManager.apply filter kwarg",jbrockmendel,closed,2020-04-06T21:27:05Z,2020-04-08T17:33:31Z,Follow-up to #33279.
582702234,32767,REF: Implement core._algos,jbrockmendel,closed,2020-03-17T02:06:48Z,2020-04-08T17:54:07Z,"ATM core.algorithms and core.nanops are a mish-mash in terms of what inputs they expect.  This implements core._algos directory intended for guaranteed-ndarray/EA-only implementations.

For the first function to move I de-duplicated a `shift` method.  Need suggestions for what to call this module."
596015029,33373,BUG: #31464 Fix error when parsing JSON list of bool into Series,jessefarnham,closed,2020-04-07T16:56:39Z,2020-04-08T19:04:29Z,"Add a missing exception type to the except clause, to cover
the TypeError that is thrown by Cythonized array_to_datetime
function when trying to convert bool to nonseconds.

- [x] closes #31464
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
595737549,33361,Fix read parquet import error message,jfcorbett,closed,2020-04-07T09:46:52Z,2020-04-08T20:10:58Z,"- [x] closes #33313
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry  N/A

`pandas.io.parquet.get_engine()` uses handling of `ImportError`s for flow control to decide which parquet reader engine is used. In doing so, it quashed lower-level error messages that would have been helpful to the user attempting to diagnose the error, replacing it with a misleading error message. 

I refactored the error handling, allowing for these lower-level error messages to be collected and explicitly ""bubbled up"". Thus fixing the incorrect error message.

No tests added -- this behaviour is not worthy of testing. Presumably not worthy of a whatsnew entry either."
596818807,33405,CLN: follow-ups to #33340,jbrockmendel,closed,2020-04-08T19:49:03Z,2020-04-08T20:49:52Z,
596882600,33408,REF: collect DataFrame.__setitem__ tests,jbrockmendel,closed,2020-04-08T21:44:12Z,2020-04-08T23:23:50Z,
596960839,33413,use https link for Anaconda,partev,closed,2020-04-09T01:35:50Z,2020-04-09T02:22:12Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
486725731,28213,BUG: Fix issue with apply on empty DataFrame,dsaxton,closed,2019-08-29T04:09:10Z,2020-04-09T02:37:34Z,"Fixes a bug where the return value for certain functions was hard-coded as `np.nan` when operating on an empty `DataFrame`.

Before
```python
>>> import numpy as np
>>> import pandas as pd
>>> df = pd.DataFrame(columns=[""a"", ""b"", ""c""])
>>> df.apply(np.sum)
a   NaN
b   NaN
c   NaN
dtype: float64
>>> df.apply(np.prod)
a   NaN
b   NaN
c   NaN
dtype: float64
```

After
```python
>>> import numpy as np
>>> import pandas as pd
>>> df = pd.DataFrame(columns=[""a"", ""b"", ""c""])
>>> df.apply(np.sum)
a    0.0
b    0.0
c    0.0
dtype: float64
>>> df.apply(np.prod)
a    1.0
b    1.0
c    1.0
dtype: float64
```

**Edit**: Closes #28202 and closes #21959 after https://github.com/pandas-dev/pandas/pull/28213/commits/cb68153f4b6fca19440ec6b79a0d1128c002ec11.  The issue was that the arguments of `self.f` were already unpacked here:
https://github.com/pandas-dev/pandas/blob/cb68153f4b6fca19440ec6b79a0d1128c002ec11/pandas/core/apply.py#L112
and then we tried to do this again inside `apply_empty_result` which was raising an error and causing the unusual output."
572846680,32336,CLN: Don't create _join_functions,dsaxton,closed,2020-02-28T16:04:25Z,2020-04-09T02:37:43Z,This seems more clear than defining `_join_functions` far away from where it's actually used
596914456,33409,BUG: DatetimeIndex.astype matches Series.dtype with datetime tz dtype,mroeschke,closed,2020-04-08T23:05:28Z,2020-04-09T02:50:15Z,"- [x] closes #33401
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
592598105,33236,ENH: provide standard BaseIndexers in pandas.api.indexers,AlexKirko,closed,2020-04-02T12:36:58Z,2020-04-09T06:34:57Z,"- [X] closes #33201
- [X] 1 test modified / 1 passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

This PR adds a forward-looking `pandas.api.indexers.BaseIndexer` subclass implementation and exposes it to users as discussed in #33180 .

### Exposing other indexers

It is also possible to expose a backward-looking indexer simply by exposing `pandas.core.window.indexers.FixedWindowIndexer` through the API. Seems redundant to me as this is basically the same as supplying an integer as the window argument when creating a rolling object but it can be done for consistency. I'd be grateful for a discussion.

### Note on tests

The class is incorporated into `test_rolling_forward_window` in `test_base_indexer.py` and this serves as a test that the new indexer is working correctly.

### Notes on performance
I've tested the implementation and it shows comparable performance to `pandas.core.window.indexers.FixedWindowIndexer`. In my tests the new indexer was slightly faster, either by luck, or because I streamlined the backward-looking indexer a bit when making the forward-looking one."
591125065,33180,BUG: support min/max functions for rolling windows with custom BaseIndexer,AlexKirko,closed,2020-03-31T13:48:19Z,2020-04-09T06:35:39Z,"- [X] xref #32865 
- [X] 1 tests added / 1 passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

## The problem
We currently don't support several rolling window functions when building a rolling window object using a custom class descended from `pandas.api.indexers.Baseindexer`. The implementations were written with backward-looking windows in mind, and this led to these functions breaking.
Currently, using these functions returns a `NotImplemented` error thanks to #33057, but ideally we want to update the implementations, so that they will work without a performance hit. This is what I aim to do over a series of PRs.

## Scope of PR
This PR proposes an alternative implementation for the `min` and `max` rolling window functions implemented in the `_roll_min_max_variable` function in the `aggregations.pyx` file.

## Perf note
This implementation is slightly faster than the one on master branch (tested with a `Series` with 10e6 randomly generated values and `timeit`). If necessary, I can add a benchmark to the benchmark suite, although it must be noted that we need to benchmark with large arrays of data to minimize the effect of overhead."
596940306,33412,ENH: More permissable conversion to StringDtype,topper-123,closed,2020-04-09T00:24:37Z,2020-04-09T08:47:32Z,"Currently, converting directly to StringDtype is not really safe unless you know you've got a ``str`` type:

```python
>>> pd.Series([1,2, np.nan], dtype=str)[0]
'1'  # string, works
>>> pd.Series([1,2, np.nan], dtype=""string"")
ValueError  # does not work
>>> pd.Series([1,2, np.nan], dtype=str).astype(""string"")
# works
>>> pd.Series([1,2, np.nan]).astype(""string"")
ValueError  # does not work
>>> pd.Series([1,2, np.nan]).astype(str).astype(""string"")
# works
```
The net result is that we have to wrap conversions, so in general will have to do  ``x.astype(str).astype(""string"")``, which seems unnecessary.

This extends to other conversions to StringDtype, e.g. for ``read_excel`` we have to do:

```python
>>> df = pd.read_excel(..., dtype={""col_x"": str}).astype({""col_x"": ""string""})
```
where directly setting ``dtype={""col_x"": ""string""}`` would seem more natural.

 I think we should be able to convert directly to StringDtype by internally converting first to ``str``, if needed. I might be missing some nuance why this is not allowed, though.

Any comment on loosening up here?"
210278421,15510,PERF: faster unstacking,jreback,closed,2017-02-26T02:25:04Z,2020-04-09T17:23:38Z,"closes #15503

so on a non-masked unstack (IOW, a fully product multi-index for example), this is now just
a simple reshape. On a masked unstack, it now will have a much lower O constant, as its in cython, and with release the GIL.

0.19.2 / master
```
In [2]: m = 100
   ...: n = 1000
   ...: 
   ...: levels = np.arange(m)
   ...: index = pd.MultiIndex.from_product([levels]*2)
   ...: columns = np.arange(n)
   ...: values = np.arange(m*m*n).reshape(m*m, n)
   ...: df = pd.DataFrame(values, index, columns)
   ...: 

In [3]: %timeit df.unstack()
1 loop, best of 3: 285 ms per loop

In [4]: df2 = df.iloc[:-1]

In [5]: %timeit df2.unstack()
1 loop, best of 3: 306 ms per loop
```

PR
```
In [2]: %timeit df.unstack()
10 loops, best of 3: 70 ms per loop

In [3]: df2 = df.iloc[:-1]

# & releasing the GIL here.
In [4]: %timeit df2.unstack()
1 loop, best of 3: 191 ms per loop
```
"
596938182,33411,To numeric,yixinxiao7,closed,2020-04-09T00:18:02Z,2020-04-09T17:26:16Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Addressing GH #33013. Supports downcasting of nullable dtypes by transferring elements in extension array to ndarray to avoid TypeError thrown by np.allclose. Enhancement documented on v1.1.0 in the ""other enhancements"" section. Unit test written in test_to_numeric. 
"
596993171,33416,DOC: Fix EX01 in DataFrame.duplicated,farhanreynaldo,closed,2020-04-09T03:24:13Z,2020-04-09T17:53:40Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. 

```
################################################################################
################################## Validation ##################################
################################################################################"
595568163,33357,PERF: fastpath DataFrame constructor from BlockManager,jbrockmendel,closed,2020-04-07T03:42:11Z,2020-04-09T18:03:02Z,"When trying to make `fast_apply` unnecessary I found that there is non-trivial overhead in the constructors that we can avoid.  Moreover, since users shouldn't be passing BlockManagers around anyway, we _might_ be able to get that case out of the DataFrame constructor entirely.

```
In [2]: ser = pd.Series(range(5))                                                                                                                                        
In [3]: df = ser.to_frame()                                                                                                                                              
In [4]: mgr = df._mgr

In [5]: %timeit pd.DataFrame(mgr)                                                                                                                                       
2.08 µs ± 51.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

In [6]: %timeit pd.DataFrame._from_mgr(mgr)                                                                                                                             
1.26 µs ± 17.3 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
```
"
597407137,33430,DOC: Development guide giving 404,TomAugspurger,closed,2020-04-09T16:35:35Z,2020-04-09T18:18:42Z,https://pandas.pydata.org/docs/development/index.html is giving a 404 right now. Haven't looked into why cc @datapythonista.
482014464,27999,ENH: Add support for dataclasses in the DataFrame constructor,asosnovsky,closed,2019-08-18T16:54:11Z,2020-04-09T23:34:29Z,"Added support for data-classes when used in the construction of a new dataframe.

- [X] closes #21910
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Simply put, added support to use dataclasses in the following way:

```python
from dataclasses import dataclass

@dataclass 
class Person:
    name: str
    age: int

df = DataFrame([Person(""me"", 25), Person(""you"", 35)])
```"
595481331,33346,BUG: Timestamp comparison with ndarray[dt64],jbrockmendel,closed,2020-04-06T23:05:39Z,2020-04-09T23:40:17Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
48659270,8803,DateParseError on bar plot,vfilimonov,open,2014-11-13T16:41:53Z,2020-04-10T02:36:37Z,"`DateParseError` is raised when Series with DateTimeIndex is plotted using bar-plot on axis that already contains another plot:

```
import pandas as pd
df = pd.Series([4,2,1,3,5], index=pd.to_datetime(['2004','2005','2006','2007','2008']))
df.plot()
df.plot(kind='bar')
```

Both of plots work independently, but being called one after another they result in `DateParseError: day is out of range for month`

If called in different order:

```
df.plot(kind='bar')
df.plot()
```

exception is not raised, but x-axis scale is incorrect

pandas version: 0.15.1
numpy verson: 1.9.1
"
335783798,21636,TST: flaky test on 3.7,jreback,closed,2018-06-26T11:54:25Z,2020-04-10T04:26:34Z,"This is occasionally failing the CI.

running this multiple times will cause it to fail:
``pytest pandas//tests/groupby/test_categorical.py  --pdb -v -k apply -s``

not sure where the error is yet. xfailing for now."
344992459,22069,BUG: groupby.plot.kde applies to index as well,javadnoorb,open,2018-07-26T20:24:23Z,2020-04-10T04:26:51Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
%matplotlib inline
import pandas as pd
import numpy as np

np.random.seed(0)
df = pd.DataFrame(np.random.random((10,2)), columns=['x', 'y' ])
df['cat'] = 1

df.groupby('cat').plot.kde()
```
#### Problem description

This results in 4 plots (or 2 plots after #21943).

Although the issue seems similar to #21609 it is coming from a different source (`gaussian_kde` is applied to index which is constant and leads to an exception that is then passed, taking the ~~apply slow path~~ alternate route using `_group_selection_context` afterwards). 

The following , on the other hand produces the desired result:

```python
%matplotlib inline
import pandas as pd
import numpy as np

np.random.seed(0)
df = pd.DataFrame(np.random.random((10,2)), columns=['x', 'y' ])
df['cat'] = 1

df.groupby('cat')[['x', 'y']].plot.kde()
```

#### Expected Output
One plot
#### Output of ``pd.show_versions()``

<details>

No module named 'dask'

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.6.final.0
python-bits: 64
OS: Darwin
OS-release: 13.4.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.0.dev0+483.gd5c909c.dirty
pytest: 3.6.2
pip: 18.0
setuptools: 39.2.0
Cython: 0.28.3
numpy: 1.14.5
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.4.0
sphinx: 1.7.5
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.5
feather: None
matplotlib: 2.2.2
openpyxl: 2.5.4
xlrd: 1.1.0
xlwt: 1.2.0
xlsxwriter: 1.0.5
lxml: 4.2.2
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.8
pymysql: 0.8.1
psycopg2: None
jinja2: 2.10
s3fs: 0.1.5
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: 0.1.0

</details>
"
375606131,23420,Erratic behavior using query() and timedelta,jsevo,open,2018-10-30T17:24:32Z,2020-04-10T04:48:30Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
d = pd.DataFrame([pd.Timedelta(5,'D')], columns=['dt'])
d.query('dt ==""5 days""', engine='numexpr')
# works: 
#      dt
# 0 5 days

d.query('dt ==""4 days""', engine='numexpr')
# works:
#Empty DataFrame
#Columns: [dt]
#Index: []

d.query('dt <""5 days""', engine='numexpr')
# Does not work
# ValueError: unknown type timedelta64[ns]


```
#### Problem description

Using query on timedelta values has erratic behavior, and was working before. I can't see how suddenly, less than, and greater than operations using `query` do not work, but == does.

#### Expected Output
```python
import pandas as pd
d = pd.DataFrame([pd.Timedelta(5,'D')], columns=['dt'])


d.query('dt <""5 days""')
#EXPECTED:
#      dt
# 0 5 days`
```
<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.0.final.0
python-bits: 64
OS: Darwin
OS-release: 17.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: None
pip: 18.1
setuptools: 39.2.0
Cython: 0.28.3
numpy: 1.14.5
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.4.0
sphinx: None
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: 1.2.1
tables: None
numexpr: 2.6.8
feather: None
matplotlib: 2.2.2
openpyxl: 2.5.9
xlrd: 1.1.0
xlwt: None
xlsxwriter: 1.1.2
lxml: 4.2.5
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: 2.7.5 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.7.0

</details>
"
593213498,33253,BUG: BooleanArray.any with all False values and skipna=False is buggy,jorisvandenbossche,closed,2020-04-03T08:31:58Z,2020-04-10T08:37:16Z,"From a report on my blog, this seems buggy:

```
In [10]: a = pd.array([False, False], dtype=""boolean"")  

In [11]: a  
Out[11]: 
<BooleanArray>
[False, False]
Length: 2, dtype: boolean

In [12]: a.any() 
Out[12]: False

In [13]: a.any(skipna=False)   
Out[13]: <NA>
```

I don't think there is a reason to return NA if there are no NAs in the array? (even with `skipna=False`) 

We probably need to do a check for the presence of any NA here in the `else` clause:

https://github.com/pandas-dev/pandas/blob/37dc5dc391333fede424964a3c8ab034318d6ed4/pandas/core/arrays/boolean.py#L520-L526

"
593896897,33284,BUG: fix boolean array skipna=False for .any() and .all(),linxiaow,closed,2020-04-04T14:36:37Z,2020-04-10T08:37:27Z,"- [x] closes #33253
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I added ""not self._mask.any()"" to check if there is no missing value for both .any() and .all() when skipna is False. It looks fine for the example you provided.
![any_all](https://user-images.githubusercontent.com/43714531/78453697-65cc1400-7661-11ea-9666-2fef60f2e81e.png)
"
587983665,33021,CLN: a batch of replacements of @Appender() with @doc(),smartvinnetou,closed,2020-03-25T20:52:09Z,2020-04-10T11:45:15Z,"- [x] xref #31942
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] replaces a number of uses of @Appender() with @doc() in `core/generic.py`
"
596502492,33391,April 2020 Dev Meeting,TomAugspurger,closed,2020-04-08T11:20:23Z,2020-04-10T13:12:05Z,"The monthly dev meeting is today, April 8th, at 18:00 UTC. Note that your local time may be different than last month due to daylight savings time. Our calendar is at https://pandas.pydata.org/docs/development/meeting.html#calendar

Video Call: https://zoom.us/j/942410248
Minutes: https://docs.google.com/document/u/1/d/1tGbTiYORHiSPgVMXawiweGJlBw5dOkVJLY-licoBmBU/edit?ouid=102771015311436394588&usp=docs_home&ths=true

Please add items you'd like to see discussed to the agenda."
597878947,33454,Bump cython asv,TomAugspurger,closed,2020-04-10T13:07:26Z,2020-04-10T13:53:37Z,"We were pulling in an older version, possibly from defaults.

I think we should plan a major update of all these (notably python 3.6 -> 3.8) but that'll require some care to not invalidate the results at pandas.pydata.org/speed/pandas."
590371351,33143,"STY: Using __all__; removed ""noqa"" comment",ShaharNaveh,closed,2020-03-30T15:14:02Z,2020-04-10T15:45:40Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
597862876,33453,CLN/TYP: redundant casts and unused ignores,simonjayhawkins,closed,2020-04-10T12:26:20Z,2020-04-10T15:50:55Z,
595557873,33355,BUG/API: should MultiIndex with leading integer level fallback to positional?,jbrockmendel,closed,2020-04-07T03:10:42Z,2020-04-10T16:00:57Z,"```

idx = pd.Index(range(3))
dti = pd.date_range(""2016-01-01"", periods=2)
mi = pd.MultiIndex.from_product([idx, dti])

ser = pd.Series(range(len(mi))[::-1], index=mi)

>>> ser
0  2016-01-01    5
   2016-01-02    4
1  2016-01-01    3
   2016-01-02    2
2  2016-01-01    1
   2016-01-02    0
dtype: int64
>>> ser[3]
2
```

My intuition is that `ser[3]` should behave like indexing on just the first level of the MultIIndex:

```
ser2 = ser.droplevel(1)

>>> ser2
0    5
0    4
1    3
1    2
2    1
2    0
dtype: int64
>>> ser2[3]
KeyError: 3
```

Should we avoid the fallback-to-positional behavior in this case?"
595251707,33329,CLN: Static types in `pandas/_lib/lib.pyx`,ShaharNaveh,closed,2020-04-06T16:23:39Z,2020-04-10T16:16:25Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
596915403,33410,BUG: ValueError: buffer source array is read-only during groupby,erik-hasse,closed,2020-04-08T23:08:24Z,2020-04-10T16:57:10Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
df = pd.DataFrame(data={'x': [1], 'y': [2]})
df.to_parquet('pq_df', partition_cols='x')
df = pd.read_parquet('pq_df')
df.groupby('x', sort=False)

```

#### Problem description

The above code raises an exception:

```
Traceback (most recent call last):
  File ""mve.py"", line 5, in <module>
    df.groupby('x', sort=False)
  File ""/Users/ehasse/.local/lib/python3.8/site-packages/pandas/core/frame.py"", line 5798, in groupby
    return groupby_generic.DataFrameGroupBy(
  File ""/Users/ehasse/.local/lib/python3.8/site-packages/pandas/core/groupby/groupby.py"", line 402, in __init__
    grouper, exclusions, obj = get_grouper(
  File ""/Users/ehasse/.local/lib/python3.8/site-packages/pandas/core/groupby/grouper.py"", line 615, in get_grouper
    Grouping(
  File ""/Users/ehasse/.local/lib/python3.8/site-packages/pandas/core/groupby/grouper.py"", line 312, in __init__
    self.grouper, self.all_grouper = recode_for_groupby(
  File ""/Users/ehasse/.local/lib/python3.8/site-packages/pandas/core/groupby/categorical.py"", line 72, in recode_for_groupby
    cat = cat.add_categories(c.categories[~c.categories.isin(cat.categories)])
  File ""/Users/ehasse/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 4667, in isin
    return algos.isin(self, values)
  File ""/Users/ehasse/.local/lib/python3.8/site-packages/pandas/core/algorithms.py"", line 447, in isin
    return f(comps, values)
  File ""pandas/_libs/hashtable_func_helper.pxi"", line 555, in pandas._libs.hashtable.ismember_int64
  File ""stringsource"", line 658, in View.MemoryView.memoryview_cwrapper
  File ""stringsource"", line 349, in View.MemoryView.memoryview.__cinit__
ValueError: buffer source array is read-only
```

This specifically requires the following:
* The dataframe is loaded from a parquet file.
* The column being grouped by was used to partition the file.
* sort=False is passed.

In addtion, passing observed=True stops the error from occurring.

I believe this is related to #31710, but they were unable to provide an example for groupby, and the issue remains on 1.0.3.

#### Expected Output

A DataFrameGroupBy object.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0.post20200309
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.47.0

</details>
"
590970884,33173,Setting histogram weights for multiple columns fails,mave240,closed,2020-03-31T10:07:25Z,2020-04-10T16:59:56Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np
df = pd.DataFrame(dict(zip(['A', 'B'], np.random.randn(2, 100))))
df.plot.hist(weights=0.1*np.ones(shape=(100, 2)))  # fail
```
#### Problem description
Trying to plot a histogram of a multi-column data frame with weights fails with `ValueError: weights should have the same shape as a.` There is no error for a single column. It doesn't work with:
```python
df.plot.hist(weights=0.1*np.ones(shape=(100,)))  # fail
```
either.

#### Expected Output
A `matplotlib` figure.
![pandas_hist_weights_bug_01](https://user-images.githubusercontent.com/37543656/78025387-da493f00-7351-11ea-8b5f-a047dad14c9d.png)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.1.post20200323
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
604247725,33708,ENH: to_datetime missing year/other data arg,filipopo,closed,2020-04-21T19:59:29Z,2020-04-23T04:49:48Z,"It seems that to_datetime assumes the year is 1970 if I do
`df['date'] = pd.to_datetime(df['date'])`

To a series in a day/month (no year) format but this bothered me so I did this:
`df['date'] = df['date'].apply(lambda row: pd.to_datetime(row + '/2017', format = '%d/%m/%Y'))`

But an arg would make this neater, I suppose"
605004634,33728,TST: pd.concat on two (or more) series produces all-NaN dataframe,simonjayhawkins,closed,2020-04-22T19:25:46Z,2020-04-23T07:01:36Z,"- [ ] closes #11058
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
604778134,33724,CLN: misc cleanups from LGTM.com,simonjayhawkins,closed,2020-04-22T13:55:34Z,2020-04-23T07:03:00Z,
604911299,33726,BUG: argument 'axis=1' is silently ignored in DataFrame diff() method when dtype is nullable integer (Int64),geppi,closed,2020-04-22T16:53:33Z,2020-04-23T07:08:56Z,"```python
import numpy as np
import pandas as pd
pd.show_versions()
```

    
    INSTALLED VERSIONS
    ------------------
    commit           : None
    python           : 3.7.4.final.0
    python-bits      : 64
    OS               : Windows
    OS-release       : 10
    machine          : AMD64
    processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
    byteorder        : little
    LC_ALL           : None
    LANG             : None
    LOCALE           : None.None
    
    pandas           : 1.0.3
    numpy            : 1.18.1
    pytz             : 2019.3
    dateutil         : 2.8.1
    pip              : 20.0.2
    setuptools       : 40.8.0
    Cython           : None
    pytest           : None
    hypothesis       : None
    sphinx           : None
    blosc            : None
    feather          : None
    xlsxwriter       : 1.2.8
    lxml.etree       : 4.5.0
    html5lib         : None
    pymysql          : None
    psycopg2         : None
    jinja2           : 2.10.3
    IPython          : 7.11.1
    pandas_datareader: None
    bs4              : None
    bottleneck       : None
    fastparquet      : None
    gcsfs            : None
    lxml.etree       : 4.5.0
    matplotlib       : 3.1.3
    numexpr          : None
    odfpy            : None
    openpyxl         : 3.0.3
    pandas_gbq       : None
    pyarrow          : None
    pytables         : None
    pytest           : None
    pyxlsb           : None
    s3fs             : None
    scipy            : None
    sqlalchemy       : None
    tables           : None
    tabulate         : None
    xarray           : None
    xlrd             : None
    xlwt             : None
    xlsxwriter       : 1.2.8
    numba            : None
    


```python
df = pd.DataFrame([[1,2], [3,np.nan]], index= ['a', 'b'], columns=['A', 'B']).convert_dtypes()
df
```




<div>

<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>A</th>
      <th>B</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>a</th>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>b</th>
      <td>3</td>
      <td>&lt;NA&gt;</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.dtypes
```




    A    Int64
    B    Int64
    dtype: object




```python
df.diff(axis=0)
```




<div>

<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>A</th>
      <th>B</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>a</th>
      <td>&lt;NA&gt;</td>
      <td>&lt;NA&gt;</td>
    </tr>
    <tr>
      <th>b</th>
      <td>2</td>
      <td>&lt;NA&gt;</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.diff(axis=1)
```




<div>

<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>A</th>
      <th>B</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>a</th>
      <td>&lt;NA&gt;</td>
      <td>&lt;NA&gt;</td>
    </tr>
    <tr>
      <th>b</th>
      <td>2</td>
      <td>&lt;NA&gt;</td>
    </tr>
  </tbody>
</table>
</div>


Conversion to float fixes the issue

```python
df = df.astype(float)
df
```




<div>

<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>A</th>
      <th>B</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>a</th>
      <td>1.0</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>b</th>
      <td>3.0</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.diff(axis=1)
```




<div>

<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>A</th>
      <th>B</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>a</th>
      <td>NaN</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>b</th>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>




```python

```

"
498602428,28625,Date type output changes when using df.column.unique()[i],jwhendy,closed,2019-09-26T01:35:14Z,2020-04-23T14:05:40Z,"Apologies if this is somehow expected. I admit I don't know the intricacies of all date types and what each of these is designed to return, but I found the result surprising.

#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
dates = [pd.Timestamp(year=2019, month=m, d=1) for m in range(1, 6)]
df = pd.DataFrame({'date': dates})

>>> df.iloc[2].date
Timestamp('2019-03-01 00:00:00')

>>> df.iloc[2].date.year
2019

>>> df.date.unique()[2]
numpy.datetime64('2019-03-01T00:00:00.000000000')

>>> df.date.unique()[2].year
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'numpy.datetime64' object has no attribute 'year'
```
#### Problem description

I was subsetting data based on unique dates in a data frame. Part of my code used a string formatter, assuming I could access `x.year` when I got the `AttributeError` above.

#### Expected Output

Intuitively, I would expect that any incantation of obtaining values from a data frame should give me the type that's there, not a new type I don't expect.

Again, not being familiar with the intricacies of the various types and what's going on behind the curtains, this could easily be a false assumption and somehow `pd.Timestamp` and `np.datetime64` are more closely related than I understand and the call to `.unique()` is expected to cast one to the other?

#### Output of ``pd.show_versions()``

<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.1-arch1-1-ARCH
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.1
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.0.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : 2.6.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
</details>
"
499998023,28676,value_counts with dropna=False gives differing results,anttttti,closed,2019-09-30T00:44:57Z,2020-04-23T14:14:10Z,"#### Code Sample, a copy-pastable example if possible

```python

df[col].value_counts(dropna=False, normalize=True)

```
#### Problem description

Doing df[col].value_counts(dropna=False, normalize=True) gives different results on different x64 machines. Updated both machines to latest version of Python (3.7.4), Numpy (1.17.2), Scipy (1.3.1) and Pandas (0.25.1), and the problem still occurs. Regardless of version updates, the two machines produce slightly different orderings of value counts when null-values are present, the only difference being hardware differences.
"
510756857,29162,.map on category dtype does not respect defaultdict when encountering np.nan values,WesRoach,closed,2019-10-22T16:03:32Z,2020-04-23T14:32:52Z,"Using .map() to x-walk values from a series, and ensure that only expected values are present. 

This scenario works to convert np.nan values to empty strings on object dtypes, but does not replace np.nan on category dtype.

```python
import collections

import pandas as pd
import numpy as np

# ""01"" maps to ""12"", and is a valid value, while everything else should be set to """"
defdic = collections.defaultdict(lambda: """", {""01"": ""12""})

df = pd.DataFrame({""A"": [""01"", ""z7"", """", np.nan]})

df[""A""].map(defdic)
0    12
1
2
3
Name: A, dtype: object

df[""A""].astype('category').map(defdic)
0     12
1
2
3    NaN
Name: A, dtype: object
```

I would have expected the output to be the same for both object/category dtypes.


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1062.4.1.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 9.0.1
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.2.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.10
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
605072313,33731,API: deprecate silently casting np.nan to NaT,jbrockmendel,closed,2020-04-22T21:01:03Z,2020-04-23T15:32:42Z,"When doing a setitem into a dt64/td64 Series, we cast `np.nan` to `NaT`.  The alternative would be to cast to object dtype and set np.nan unchanged.

Similarly anything that ends up calling `is_valid_nat_for_dtype` will allow through np.nan which ends up getting cast to NaT.

We should consider deprecating this behavior and requiring users to pass NaT directly."
598319411,33485,PERF: Cythonize `from_nested_dict`,ShaharNaveh,closed,2020-04-11T18:00:50Z,2020-04-23T16:34:57Z,"I did try to cythonize this function, but the problem is that it also accepting a ```collections.OrderedDict```, which cannot be ```cdef``` as ```dict```, but have to ```cdef``` as ```object```.

---

I don't see much of a performance boost, I have ran the full benchmark suite and ```asv``` says that the ```BENCHMARKS NOT SIGNIFICANTLY CHANGED.```

Maybe just better to remove the ```TODO``` note, thoughts?"
605565692,33747,CI not running linux / windows builds,jreback,closed,2020-04-23T14:03:19Z,2020-04-23T17:09:17Z,"cc @pandas-dev/pandas-core 
"
605199629,33736,REF: use _wrap_joined_index in PeriodIndex.join,jbrockmendel,closed,2020-04-23T02:51:33Z,2020-04-23T17:54:22Z,"PeriodIndex defines _apply_meta, but that is made unnecessary by implementing _wrap_joined_index correctly."
337002592,21681,Allow merging on object / non-object column,jorisvandenbossche,closed,2018-06-29T13:38:42Z,2020-04-23T17:56:57Z,"Follow-up on discussion in https://github.com/pandas-dev/pandas/pull/21310 (cc @jreback )

What this does: allow `merge` on an object and a non-object column (eg object and int, object and categorical, ..), by coercing the non-object column to object dtype.

Reasoning:
- object dtype can be anything, so IMO pandas should not try to be smart and think to know the merge is not valid
- all other, certainly invalid dtype combinations (eg int and datetime64) keep raising an error

The main disadvantage is that we no longer detect the case of merging object strings with eg integer column, but IMO we cannot avoid such cases as long as we don't have a proper string dtype but use object dtype for this (you can also mix string and ints in a column, we also don't raise for that for a similar reason).

Additional option would be to issue a warning in this case.

Closes https://github.com/pandas-dev/pandas/issues/23733"
604961557,33727,REF: avoid passing SingleBlockManager to Series,jbrockmendel,closed,2020-04-22T18:14:00Z,2020-04-23T18:02:57Z,"cc @jorisvandenbossche IIRC you didnt want to disallow SingleBlockManager because geopandas passes it.  In those cases, does it also pass `fastpath=True`?  If so, we can consider deprecating allowing SingleBlockManager in the non-fastpath case"
604342758,33716,REF: implement _validate_comparison_value,jbrockmendel,closed,2020-04-21T23:15:51Z,2020-04-23T18:03:37Z,"Trying to match the patterns we use for all the other DTA/TDA/PA methods that have an `other`-like arg.

No logic should be changed here, just moved around."
605202119,33737,TST: fix tests for asserting matching freq,jbrockmendel,closed,2020-04-23T02:59:53Z,2020-04-23T18:56:34Z,"xref #33711, #33712"
605708741,33750,CLN: Remove is_null_period,dsaxton,closed,2020-04-23T17:18:22Z,2020-04-23T18:58:56Z,is_null_period seems only to be an alias for checknull_with_nat so I think it can be removed?
602981533,33670,Solve missing interpolation method (cubicspline),khyox,closed,2020-04-20T06:35:41Z,2020-04-23T20:10:28Z,"By commit 8bb2cc1 scipy.interpolate.CubicSpline method is
referenced in the pandas documentation (see pandas/core/generic.py)
but it is not wrapped by any interpolation method. This PR solves
this by adding the corresponding wrapper. SciPy's CubicSpline
is a cubic spline data interpolator that allows explicit control
of the boundary conditions for the interval.

Changes to be committed:
	modified:   pandas/core/missing.py
	modified:   pandas/tests/series/methods/test_interpolate.py

- [x] closes #xxxx (no issue opened but PR #20270 commented instead)
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
599350810,33542,ENH: Use self values in Series groupby,vishnu-dev,closed,2020-04-14T07:16:53Z,2020-04-23T20:36:24Z,"#### Is your feature request related to a problem?

I wish I could use pandas to do

```
s = pd.Series([1,2,1,3,1,2])
s.groupby()
```
rather than

```
s.groupby(s)
```

#### Describe the solution you'd like

I would like pandas to interpret the series values as `grouper` when `by` or `level` is not provided.

#### API breaking implications

I can't think of any reason this will affect the API.

#### Describe alternatives you've considered

Grouping using series itself as `grouper`.

`s.groupby(s)`
"
602440049,33627,TST: Groupby first/last/nth nan column test,jamescobonkerr,closed,2020-04-18T11:37:32Z,2020-04-23T21:31:06Z,"- [X] xref https://github.com/pandas-dev/pandas/issues/33591#issuecomment-615365647
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The first two asserts (`first()`/`last()`) fail on `1.0.3`, as expected."
471367450,27526,Groupby Array-Type Quantiles Broken in 0.25.0,sernst,closed,2019-07-22T22:28:47Z,2020-04-23T22:29:47Z,"#### Code Sample

```python
import pandas as pd

df = pd.DataFrame({
    'category': ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B'],
    'value': [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]
})
quantiles = df.groupby('category').quantile([0.25, 0.5, 0.75])
print(quantiles)
```
#### Problem description

In previous versions of Pandas `< 0.25.0`  and in the documentation it is possible to pass an array-type of quantiles into the `DataFrameGroupBy.quantile()` method to return multiple quantile values in a single call. However, upon installation of `0.25.0` the following error results instead:

```
Traceback (most recent call last):
  File ""example.py"", line 8, in <module>
    quantiles = df.groupby('category').quantile([0.25, 0.5, 0.75])
  File ""/usr/local/lib/python3.7/site-packages/pandas/core/groupby/groupby.py"", line 1908, in quantile
    interpolation=interpolation,
  File ""/usr/local/lib/python3.7/site-packages/pandas/core/groupby/groupby.py"", line 2248, in _get_cythonized_result
    func(**kwargs)  # Call func to modify indexer values in place
  File ""pandas/_libs/groupby.pyx"", line 69
```

#### Expected Output

Using Pandas `0.24.2` the output is:

```
               value
category
A        0.25   2.25
         0.50   3.50
         0.75   4.75
B        0.25   2.25
         0.50   3.50
         0.75   4.75
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.9.125-linuxkit
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.0
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : None
pytest           : 5.0.1
hypothesis       : None
sphinx           : 2.1.2
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : None
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : 2.6.9
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : 0.3.0
scipy            : 1.3.0
sqlalchemy       : None
tables           : 3.5.2
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
592142053,33217,BUG: Block.setitem GH#32395,jbrockmendel,closed,2020-04-01T19:12:18Z,2020-04-23T22:38:33Z,"@h-vishal this is based on what you have in #32479.  Can you adapt parts of this to address comments over there?  (that should be merged, not this, as you did the real work)"
605876128,33753,CLN: Parametrize dtype inference tests,dsaxton,closed,2020-04-23T21:11:22Z,2020-04-23T22:39:28Z,
604297984,33712,TST: more accurate expecteds for DTI/TDI,jbrockmendel,closed,2020-04-21T21:27:56Z,2020-04-23T23:11:01Z,xref #33711
492977737,28422,test_cross_engine_pa_fp fails locally.,simonjayhawkins,closed,2019-09-12T19:05:41Z,2020-04-23T23:31:18Z,"```
___________________________ test_cross_engine_pa_fp ___________________________

df_cross_compat =    a  b    d      e          f
0  a  1  4.0   True 2013-01-01
1  b  2  5.0  False 2013-01-02
2  c  3  6.0   True 2013-01-03
pa = 'pyarrow', fp = 'fastparquet'

    def test_cross_engine_pa_fp(df_cross_compat, pa, fp):
        # cross-compat with differing reading/writing engines

        df = df_cross_compat
        with tm.ensure_clean() as path:
            df.to_parquet(path, engine=pa, compression=None)

>           result = read_parquet(path, engine=fp)

pandas\tests\io\test_parquet.py:232:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pandas\io\parquet.py:293: in read_parquet
    return impl.read(path, columns=columns, **kwargs)
pandas\io\parquet.py:198: in read
    return parquet_file.to_pandas(columns=columns, **kwargs)
..\..\..\Anaconda3\envs\pandas-dev\lib\site-packages\fastparquet\api.py:421: in to_pandas
    check_column_names(self.columns + list(self.cats), columns, categories)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

columns = ['a', 'b', 'd', 'e', 'f']
args = (['a', 'b', 'd', 'e', 'f', {'kind': 'range', 'name': None, 'start': 0, 'step': 1, ...}], None)
arg = ['a', 'b', 'd', 'e', 'f', {'kind': 'range', 'name': None, 'start': 0, 'step': 1, ...}]

    def check_column_names(columns, *args):
        """"""Ensure that parameters listing column names have corresponding columns""""""
        for arg in args:
            if isinstance(arg, (tuple, list)):
>               if set(arg) - set(columns):
E               TypeError: unhashable type: 'dict'

..\..\..\Anaconda3\envs\pandas-dev\lib\site-packages\fastparquet\util.py:92: TypeError
```

<details>

INSTALLED VERSIONS
------------------
commit           : 0ab32e88481440bfb4a102bb7731cbde2e5ceafe
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 58 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : None.None

pandas           : 0.25.0+339.g0ab32e884
numpy            : 1.16.4
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 19.2.2
setuptools       : 41.0.1
Cython           : 0.29.13
pytest           : 5.0.1
hypothesis       : 4.32.2
sphinx           : 1.8.5
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : 0.3.0
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
s3fs             : 0.2.1
scipy            : 1.3.1
sqlalchemy       : 1.3.7
tables           : 3.5.2
xarray           : 0.12.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8

</details>
"
514705911,29288,Fix missing tick labels on twinned axes.,ebardie,closed,2019-10-30T14:16:32Z,2020-04-24T03:09:42Z,"## Background:

Multi-row and/or multi-column subplots can utilize shared axes.

An external share happens at axis creation when a sharex or sharey
parameter is specified.

An internal share, or twinning, occurs when an overlayed axis is created
by the `Axes.twinx()` or `Axes.twiny()` calls.

The two types of sharing can be distinguished after the fact in the
following manner. If two axes sharing an axis also have the same
position, they are internally sharing an axis, they are twinned. If their positions are different, they are externally sharing the common axis.

For externally shared axes Pandas automatically removes tick labels for
all but the last row and/or first column in
`./pandas/plotting/_matplotlib/tools.py`'s function `_handle_shared_axes()`.

## The problem:

`_handle_shared_axes()` should be interested in externally shared axes,
whether or not they are also twinned. It should, but **does not**, ignore
axes which are only twinned. 

Which means that twinned-only axes wrongly lose their tick labels.

## The cure:

This commit introduces `_ignore_colocated_axes()` which ignores any
axes in a list which have the same position as the given axis. It uses
this function to envelope calls from `_handle_shared_axes()` to
`Axes.get_shared_{x,y}_axes().get_siblings(ax)`.

## The demonstration test case:

Note especially the axis labels (and associated tick labels).

```python
#!/usr/bin/python3

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Create data
df = pd.DataFrame({'a': np.random.randn(1000),
                   'b': np.random.randn(1000)})

# Create figure
fig = plt.figure()
plots = fig.subplots(2, 3)

# Create *externally* shared axes
plots[0][0] = plt.subplot(231, sharex=plots[1][0])
plots[0][2] = plt.subplot(233, sharex=plots[1][2])

# Create *internally* shared axes
twin_ax1 = plots[0][1].twinx()
twin_ax2 = plots[0][2].twinx()

# Plot data to primary axes
df['a'].plot(ax=plots[0][0], title=""External share only"").set_xlabel(""this label should never be visible"")
df['a'].plot(ax=plots[1][0])

df['a'].plot(ax=plots[0][1], title=""Internal share (twin) only"").set_xlabel(""this label should always be visible"")
df['a'].plot(ax=plots[1][1])

df['a'].plot(ax=plots[0][2], title=""Both"").set_xlabel(""this label should never be visible"")
df['a'].plot(ax=plots[1][2])

# Plot data to twinned axes
df['b'].plot(ax=twin_ax1, color='green')
df['b'].plot(ax=twin_ax2, color='yellow')

# Do it
plt.show()
```
---

- [ ] closes #27812
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
338652998,21747,DOC: update parquet documentation categorical support,jorisvandenbossche,closed,2018-07-05T16:54:23Z,2020-04-24T09:50:57Z,"http://pandas-docs.github.io/pandas-docs-travis/io.html#parquet is a bit outdated, we should investigate a bit the current state of what support there is in pyarrow and fastparquet"
604817843,33725,TYP: construction,jbrockmendel,closed,2020-04-22T14:47:13Z,2020-04-24T14:27:24Z,
575063674,32426,CLN: avoid values_from_object in Series,jbrockmendel,closed,2020-03-04T00:59:24Z,2020-04-24T14:59:13Z,"xref #32422, #32419"
605299476,33742,DOC:,noobcoder486,closed,2020-04-23T07:18:01Z,2020-04-24T15:04:19Z,"I am not able to read a CSV file which is uploaded on Github itself. I want pandas to read the file and provide me the data frame. But I am getting a Parse Error. The Error is as:
df=pd.read_csv(""https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv"")

ParserError                               Traceback (most recent call last)
<ipython-input-8-91ca12bc272f> in <module>
----> 1 df=pd.read_csv(""https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv"")

~\Anaconda3\lib\site-packages\pandas\io\parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)
    700                     skip_blank_lines=skip_blank_lines)
    701 
--> 702         return _read(filepath_or_buffer, kwds)
    703 
    704     parser_f.__name__ = name

~\Anaconda3\lib\site-packages\pandas\io\parsers.py in _read(filepath_or_buffer, kwds)
    433 
    434     try:
--> 435         data = parser.read(nrows)
    436     finally:
    437         parser.close()

~\Anaconda3\lib\site-packages\pandas\io\parsers.py in read(self, nrows)
   1137     def read(self, nrows=None):
   1138         nrows = _validate_integer('nrows', nrows)
-> 1139         ret = self._engine.read(nrows)
   1140 
   1141         # May alter columns / col_dict

~\Anaconda3\lib\site-packages\pandas\io\parsers.py in read(self, nrows)
   1993     def read(self, nrows=None):
   1994         try:
-> 1995             data = self._reader.read(nrows)
   1996         except StopIteration:
   1997             if self._first_chunk:

pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.read()

pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._read_low_memory()

pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._read_rows()

pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows()

pandas/_libs/parsers.pyx in pandas._libs.parsers.raise_parser_error()

ParserError: Error tokenizing data. C error: Expected 1 fields in line 33, saw 2
"
606362990,33763,TYP/CLN: rogue type comment not caught by code checks,simonjayhawkins,closed,2020-04-24T14:38:53Z,2020-04-24T15:38:30Z,
472196636,27558,DOC: Unable to import pandas on python 3.5.2,kszucs,closed,2019-07-24T10:10:47Z,2020-04-24T15:56:14Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas
```
#### Problem description

Although it seems like a typing issue pandas is still affected, error:
```
root@ae9a5374fe6d:/buildbot# python -c ""import pandas""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/pandas/__init__.py"", line 55, in <module>
    from pandas.core.api import (
  File ""/usr/local/lib/python3.5/dist-packages/pandas/core/api.py"", line 5, in <module>
    from pandas.core.arrays.integer import (
  File ""/usr/local/lib/python3.5/dist-packages/pandas/core/arrays/__init__.py"", line 1, in <module>
    from .array_ import array  # noqa: F401
  File ""/usr/local/lib/python3.5/dist-packages/pandas/core/arrays/array_.py"", line 7, in <module>
    from pandas.core.dtypes.common import (
  File ""/usr/local/lib/python3.5/dist-packages/pandas/core/dtypes/common.py"", line 11, in <module>
    from pandas.core.dtypes.dtypes import (
  File ""/usr/local/lib/python3.5/dist-packages/pandas/core/dtypes/dtypes.py"", line 53, in <module>
    class Registry:
  File ""/usr/local/lib/python3.5/dist-packages/pandas/core/dtypes/dtypes.py"", line 84, in Registry
    self, dtype: Union[Type[ExtensionDtype], str]
  File ""/usr/lib/python3.5/typing.py"", line 552, in __getitem__
    dict(self.__dict__), parameters, _root=True)
  File ""/usr/lib/python3.5/typing.py"", line 512, in __new__
    for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):
  File ""/usr/lib/python3.5/typing.py"", line 512, in <genexpr>
    for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):
  File ""/usr/lib/python3.5/typing.py"", line 1077, in __subclasscheck__
    if super().__subclasscheck__(cls):
  File ""/usr/lib/python3.5/abc.py"", line 225, in __subclasscheck__
    for scls in cls.__subclasses__():
TypeError: descriptor '__subclasses__' of 'type' object needs an argument
```

To reproduce:
```
$ docker pull ursalab/amd64-ubuntu-16.04-python-3:worker
$ docker run -it ursalab/amd64-ubuntu-16.04-python-3:worker bash
# python -c ""import pandas""
```

#### Output of ``pip freeze | grep pandas``

```
pandas==0.25.0
```
"
355441482,22542,API: groupby aggregation with apply does not drop groupby-column,h-vetinari,closed,2018-08-30T06:25:41Z,2020-04-24T16:16:25Z,"The docs for groupby say (http://pandas.pydata.org/pandas-docs/stable/groupby.html):
> Note:
Aggregation functions will not return the groups that you are aggregating over if they are named columns, when as_index=True, the default. The grouped columns will be the indices of the returned object.
Passing as_index=False will return the groups that you are aggregating over, if they are named columns.

From the section, it's implied that this is talking about builtins and the `aggregate` functionality, but I very often find myself operating with complicated functions on the groups themselves, so `apply` is my bread and butter (and this is part of a larger issue that `groupby.apply` has some inconsistent behavior).

```
N = 10
df = pd.DataFrame(index=range(N), columns=['id', 'x', 'y', 'z'])
df.loc[:, ['x', 'y', 'z']] = np.arange(N*3).reshape(N, 3)
df.id = np.random.randint(0, int(N/3), (N,)) + 10
df
#    id   x   y   z
# 0  12   0   1   2
# 1  12   3   4   5
# 2  11   6   7   8
# 3  10   9  10  11
# 4  12  12  13  14
# 5  12  15  16  17
# 6  12  18  19  20
# 7  11  21  22  23
# 8  10  24  25  26
# 9  10  27  28  29
```

For something like `sum`, the groupby-column gets dropped, as described:
```
df.groupby('id').sum()
#      x   y   z
# id            
# 10  60  63  66
# 11  27  29  31
# 12  48  53  58
```

But for using the same function in `apply`, the result is different - mainly that the groupby column does not get removed (but also the dtype)
```
df.groupby('id', as_index=True).apply(lambda gr: gr.sum())
#       id     x     y     z
# id                        
# 10  30.0  60.0  63.0  66.0
# 11  22.0  27.0  29.0  31.0
# 12  60.0  48.0  53.0  58.0
```

Ideally, I'd like the make the behaviour of `groupby.apply` more consistent in a number of cases, and this is one of them."
606471321,33772,BUG: DataFrame.truncate() handles decreasing order(#33756),francislan,closed,2020-04-24T17:29:02Z,2020-04-24T17:31:59Z,"- [x] closes #33756 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
606487573,33773,TST: more accurate freq attr in `expected`s,jbrockmendel,closed,2020-04-24T17:58:25Z,2020-04-24T22:17:44Z,preparing to add a freq check to assert_index_equal
606497809,33774,REF: misplaced sort_index tests,jbrockmendel,closed,2020-04-24T18:18:03Z,2020-04-24T22:19:20Z,
604332710,33714,CLN: simplify DTA.__getitem__,jbrockmendel,closed,2020-04-21T22:48:35Z,2020-04-24T22:20:20Z,
606369803,33764,REF: simplify _try_cast,jbrockmendel,closed,2020-04-24T14:48:10Z,2020-04-24T22:21:12Z,Part of a sequence of PRs aimed at sharing code between Series/Index/array
587349251,32992,pd.NA TypeError in drop_duplicates with object dtype,WillAyd,closed,2020-03-24T23:55:10Z,2020-04-24T22:25:24Z,"#### Code Sample, a copy-pastable example if possible

```python-traceback
>>> pd.DataFrame([[1, pd.NA], [2, ""a""]]).drop_duplicates()
Traceback (most recent call last):
   ...
  File ""/Users/williamayd/miniconda3/envs/sitka/lib/python3.8/site-packages/pandas/core/frame.py"", line 4859, in f
    labels, shape = algorithms.factorize(
  File ""/Users/williamayd/miniconda3/envs/sitka/lib/python3.8/site-packages/pandas/core/algorithms.py"", line 629, in factorize
    codes, uniques = _factorize_array(
  File ""/Users/williamayd/miniconda3/envs/sitka/lib/python3.8/site-packages/pandas/core/algorithms.py"", line 478, in _factorize_array
    uniques, codes = table.factorize(values, na_sentinel=na_sentinel, na_value=na_value)
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1806, in pandas._libs.hashtable.PyObjectHashTable.factorize
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1728, in pandas._libs.hashtable.PyObjectHashTable._unique
  File ""pandas/_libs/missing.pyx"", line 360, in pandas._libs.missing.NAType.__bool__
TypeError: boolean value of NA is ambiguous
```
This same failure isn't present when using an extension type:

```python
>>> df = pd.DataFrame([[1, pd.NA], [2, ""a""]], columns=list(""ab""))
>>> df[""b""] = df[""b""].astype(""string"")
>>> df.drop_duplicates()
   a     b
0  1  <NA>
1  2     a
```
"
604291977,33711,TST: prepare for freq-checking in tests.io,jbrockmendel,closed,2020-04-21T21:17:35Z,2020-04-24T23:24:31Z,Working on making assert_index_equal check that `freq` attrs match.  This ports the necessary test changes from tests.io.
606571661,33779,BUG: freq not retained on apply_index,jbrockmendel,closed,2020-04-24T20:41:39Z,2020-04-24T23:28:13Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
605994731,33757,BUG: MonthOffset.name,jbrockmendel,closed,2020-04-24T02:38:17Z,2020-04-25T00:39:36Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

`is_anchored` is being accessed indirectly (its not a property).  As a result we never go down the other path, which would raise AttributeError since there is no `code_rule`."
605818819,33751,TST: pd.NA TypeError in drop_duplicates with object dtype,simonjayhawkins,closed,2020-04-23T19:49:07Z,2020-04-25T09:14:57Z,"- [ ] closes #32992
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
445859335,26460,unhelpful error message with DataFrame constructor and integer MultiIndex DataFrame,simonjayhawkins,open,2019-05-19T20:12:51Z,2020-04-25T11:27:26Z,"similar to #26459, but with integer MultiIndex

#### Code Sample, a copy-pastable example if possible

```python
import numpy as np
import pandas as pd
df = pd.DataFrame(np.random.randn(3, 3),
               columns=[[2, 2, 4], [6, 8, 10]],
               index=[[4, 4, 8], [8, 10, 12]])
pd.DataFrame(df.iloc[[0, 1]], index=[8, 10])
```

#### Problem description

should either work or give similar message to #26459

```python-traceback
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\frame.py"", line 389, in __init__
    dtype=dtype, copy=copy)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\generic.py"", line 147, in _init_mg
r
    copy=False)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\internals\managers.py"", line 1189,
 in reindex_axis
    limit=limit)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexes\multi.py"", line 2254, in r
eindex
    target = MultiIndex.from_tuples(target)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexes\multi.py"", line 400, in fr
om_tuples
    arrays = list(lib.tuples_to_object_array(tuples).T)
  File ""pandas\_libs\lib.pyx"", line 2230, in pandas._libs.lib.tuples_to_object_array
    def tuples_to_object_array(ndarray[object] tuples):
ValueError: Buffer dtype mismatch, expected 'Python object' but got 'long long'
```
#### Expected Output

either

```
           2                   4
          6         8         10
8   0.402010  0.228705  0.154561
10 -0.331031  1.321493 -0.193243
```
or

```
ValueError: cannot handle a non-unique multi-index!
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
606170039,33758,REGR: disallow mean of period column again,jorisvandenbossche,closed,2020-04-24T09:20:59Z,2020-04-25T11:46:15Z,"This reverts parts of https://github.com/pandas-dev/pandas/pull/32426 and https://github.com/pandas-dev/pandas/pull/29941 (those are only on master, not yet released), to disallow taking the mean on a Period dtype column again. The mean for period is not supported on PeriodArray itself or for a Series of period dtype (on purpose, see discussion in https://github.com/pandas-dev/pandas/pull/24757 when mean for datetimelike was added), so for DataFrame columns it should also not work. 

See also comment at https://github.com/pandas-dev/pandas/pull/32426#issuecomment-618872954

cc @jbrockmendel "
552703120,31173,ValueError: can not merge DataFrame with instance of type ,Yitao1024,closed,2020-01-21T08:42:27Z,2020-04-25T12:00:13Z,"#### Code Sample, a copy-pastable example if possible

```python
df_debtor = pd.merge(debtor_marital,debtor_gender, left_on = ""IDENTITY"", right_on = ""IDENTITY"", how = ""inner"")

```
#### Problem description
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
 in 
----> 1 df_debtor = pd.merge(debtor_marital,debtor_gender, left_on = ""IDENTITY"", right_on = ""IDENTITY"", how = ""inner"")

~/opt/anaconda3/lib/python3.7/site-packages/modin/pandas/general.py in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)
     75     if not isinstance(left, DataFrame):
     76         raise ValueError(
---> 77             ""can not merge DataFrame with instance of type {}"".format(type(right))
     78         )
     79 

### ValueError: can not merge DataFrame with instance of type 
"
31712910,6897,ENH: Standard Error of the Mean (sem) aggregation method,toddrjen,closed,2014-04-17T09:27:47Z,2020-04-25T12:29:37Z,"A very common operation when trying to work with data is to find out the error range for the data.  In scientific research, including error ranges is required.

There are two main ways to do this: standard deviation and standard error of the mean.  Pandas has an optimized std aggregation method for both dataframe and groupby.  However, it does not have an optimized standard error method, meaning users who want to compute error ranges have to rely on the unoptimized scipy method.  

Since computing error ranges is such a common operation, I think it would be very useful if there was an optimized `sem` method like there is for `std`.
"
556738512,31414,Cannot replace all occurences of infs and nans to None with a single df.replace,dougvj,open,2020-01-29T09:29:26Z,2020-04-25T12:41:44Z,"#### Example code

```python
>>> df = pd.DataFrame(
...         {'foo':[
...             np.nan,
...             np.inf,
...             1.1,
...             2.2
...             ],
...         'bar':[
...             1,
...             -np.inf,
...             2.1,
...             3.2
...             ]
...         })
>>> df
   foo  bar
0  NaN  1.0
1  inf -inf
2  1.1  2.1
3  2.2  3.2
>>> df.replace({np.nan: None, -np.inf:None, np.inf:None})
   foo  bar
0  NaN  1.0
1  NaN  NaN
2  1.1  2.1
3  2.2  3.2
>>> 
```
#### Problem description
When trying to replace all occurrences of `NaN`s and `Inf`s to `None`s the resulting dataframe simply contains 'NaNs' for every occurrence, retaining the dtype of `float64` rather than the expected `object`.  A work around is to simply issue the replace again with only `np.nan` to `None`. 

This behavior was discovered when trying to convert the contents of a dataframe containing `Inf`s, and `NaN`s to `None`s in preparation for JSON compliant output via a dictionary for compatibility with third party tools rather than directly through panda's to_json() method (which admittedly would make this process unnecessary as it produces the compliant output already)

I strongly suspect this problem is related to inconsistencies outlined here: https://github.com/pandas-dev/pandas/issues/29024

#### Expected Output

I would expect a column type of `object` with `None`s in place as I see when replacing only NaNs.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.13-arch1-1
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.0rc0+233.gec0996c675
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3
setuptools       : 44.0.0
Cython           : 0.29.14
pytest           : None
hypothesis       : None
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.12
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
595537672,33351,ENH: Implement StringArray.min / max,dsaxton,closed,2020-04-07T02:03:12Z,2020-04-25T13:31:46Z,"- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Using the new masked reductions from @jorisvandenbossche to implement these for StringArray. Part of https://github.com/pandas-dev/pandas/issues/31746 but doesn't close because we're not adding sum here."
599256711,33538,ENH: Implement IntegerArray.sum,dsaxton,closed,2020-04-14T02:46:14Z,2020-04-25T13:38:58Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I think this is mostly interesting in that it allows normalize=True for value_counts on an IntegerArray backed Series, which currently doesn't work:

```python
[ins] In [1]: s = pd.Series([1, 2, 3], dtype=""Int64"")

[ins] In [2]: s.value_counts(normalize=True)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-2-2bf1a78353e5> in <module>
----> 1 s.value_counts(normalize=True)

~/pandas/pandas/core/base.py in value_counts(self, normalize, sort, ascending, bins, dropna)
   1252             normalize=normalize,
   1253             bins=bins,
-> 1254             dropna=dropna,
   1255         )
   1256         return result

~/pandas/pandas/core/algorithms.py in value_counts(values, sort, ascending, normalize, bins, dropna)
    725
    726     if normalize:
--> 727         result = result / float(counts.sum())
    728
    729     return result

AttributeError: 'IntegerArray' object has no attribute 'sum'
```"
602120538,33612,DOC: Remove extra backtick in example in documentation.,pkirlin,closed,2020-04-17T17:16:41Z,2020-04-25T17:06:31Z,":meth:`~Series.idxmax``  --> :meth:`~Series.idxmax`

- [x] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
606311360,33762,Stratified sampling,fbossolan,closed,2020-04-24T13:24:32Z,2020-04-25T17:55:35Z,"- [ ] Added new method stratified_sample
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
606691019,33785,Pandas won't drop index after rename using said index,veranscoto,closed,2020-04-25T05:05:51Z,2020-04-25T18:01:10Z,"So, there should probably be a 'drop' option as part of the rename function to drop said index if using it, but like the title says, if you rename using an index, you can't then use the drop function to drop said index (tested with xlsx and csv)

works for whatever index, 1, 2, 3, etc.
[testfile.xlsx](https://github.com/pandas-dev/pandas/files/4532369/testfile.xlsx)


```
    dataframe = pandas.read_excel(ENVT_SETTINGS['working_dir'] + 'testfile.xlsx', engine='openpyxl', header=None)
    
    print(dataframe.head(5))
    dataframe.rename(columns = dataframe.iloc[0], inplace = True)
    print(dataframe.head(5))
    dataframe.drop(dataframe.index[0])
    print(dataframe.head(5))
```
interestingly, you CAN drop it if you then recreate the dataframe like
`    dataframe = dataframe[1:]`
to literally cut out that row from the dataset, but otherwise, it won't go away.
[testfile.xlsx](https://github.com/pandas-dev/pandas/files/4532379/testfile.xlsx)
[testresult.xlsx](https://github.com/pandas-dev/pandas/files/4532380/testresult.xlsx)

"
444331432,26407,SparseArrays backed by other Extension array/dtype ?,jorisvandenbossche,open,2019-05-15T09:29:00Z,2020-04-25T20:53:08Z,"You can create an SparseDtype with a ExtensionDtype as subtype:

```
In [14]: spdt = pd.SparseDtype(pd.DatetimeTZDtype(tz='Europe/Brussels'))

In [15]: spdt
Out[15]: Sparse[datetime64[ns, Europe/Brussels], NaT]
```

but I don't think that you can actually create a SparseArray with that?

At least, there seems to be several places in the sparse code that assumes the subtype is a numpy dtype, and also creating one fails:

```
In [16]: pd.SparseArray([pd.Timestamp('2012-01-01', tz=""Europe/Brussels""), pd.NaT], dtype=spdt)
Out[16]: ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/scipy/repos/ipython/IPython/core/formatters.py in __call__(self, obj)
    700                 type_pprinters=self.type_printers,
    701                 deferred_pprinters=self.deferred_printers)
--> 702             printer.pretty(obj)
    703             printer.flush()
    704             return stream.getvalue()

~/scipy/repos/ipython/IPython/lib/pretty.py in pretty(self, obj)
    400                         if cls is not object \
    401                                 and callable(cls.__dict__.get('__repr__')):
--> 402                             return _repr_pprint(obj, self, cycle)
    403 
    404             return _default_pprint(obj, self, cycle)

~/scipy/repos/ipython/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)
    695     """"""A pprint that just redirects to the normal repr function.""""""
    696     # Find newlines and replace them with p.break_()
--> 697     output = repr(obj)
    698     for idx,output_line in enumerate(output.splitlines()):
    699         if idx:

~/scipy/pandas/pandas/core/base.py in __repr__(self)
     65         Return a string representation for a particular object.
     66         """"""
---> 67         return str(self)
     68 
     69 

~/scipy/pandas/pandas/core/base.py in __str__(self)
     50         Return a string representation for a particular Object
     51         """"""
---> 52         return self.__unicode__()
     53 
     54     def __bytes__(self):

~/scipy/pandas/pandas/core/arrays/sparse.py in __unicode__(self)
   1826     def __unicode__(self):
   1827         return '{self}\nFill: {fill}\n{index}'.format(
-> 1828             self=printing.pprint_thing(self),
   1829             fill=printing.pprint_thing(self.fill_value),
   1830             index=printing.pprint_thing(self.sp_index))

~/scipy/pandas/pandas/io/formats/printing.py in pprint_thing(thing, _nest_lvl, escape_chars, default_escapes, quote_strings, max_seq_items)
    215         result = _pprint_seq(thing, _nest_lvl, escape_chars=escape_chars,
    216                              quote_strings=quote_strings,
--> 217                              max_seq_items=max_seq_items)
    218     elif isinstance(thing, str) and quote_strings:
    219         result = ""'{thing}'"".format(thing=as_escaped_unicode(thing))

~/scipy/pandas/pandas/io/formats/printing.py in _pprint_seq(seq, _nest_lvl, max_seq_items, **kwds)
    111     r = [pprint_thing(next(s),
    112                       _nest_lvl + 1, max_seq_items=max_seq_items, **kwds)
--> 113          for i in range(min(nitems, len(seq)))]
    114     body = "", "".join(r)
    115 

~/scipy/pandas/pandas/io/formats/printing.py in <listcomp>(.0)
    111     r = [pprint_thing(next(s),
    112                       _nest_lvl + 1, max_seq_items=max_seq_items, **kwds)
--> 113          for i in range(min(nitems, len(seq)))]
    114     body = "", "".join(r)
    115 

~/scipy/pandas/pandas/core/arrays/base.py in __iter__(self)
    282         # calls to ``__getitem__``, which may be slower than necessary.
    283         for i in range(len(self)):
--> 284             yield self[i]
    285 
    286     # ------------------------------------------------------------------------

~/scipy/pandas/pandas/core/arrays/sparse.py in __getitem__(self, key)
   1075 
   1076         if is_integer(key):
-> 1077             return self._get_val_at(key)
   1078         elif isinstance(key, tuple):
   1079             data_slice = self.values[key]

~/scipy/pandas/pandas/core/arrays/sparse.py in _get_val_at(self, loc)
   1118             return self.fill_value
   1119         else:
-> 1120             return libindex.get_value_at(self.sp_values, sp_loc)
   1121 
   1122     def take(self, indices, allow_fill=False, fill_value=None):

TypeError: Argument 'arr' has incorrect type (expected numpy.ndarray, got DatetimeArray)
```

(although it is actually the repr that fails)

Is this something we would like to support? (cc @TomAugspurger )"
605162482,33734,REF: implement test_astype,jbrockmendel,closed,2020-04-23T00:50:13Z,2020-04-25T21:02:48Z,Split/parametrized in some places.
606812874,33792,REF: remove need to override get_indexer_non_unique in DatetimeIndexOpsMixin,jbrockmendel,closed,2020-04-25T17:05:31Z,2020-04-25T21:03:17Z,
606511798,33776,"CLN: avoid getattr(obj, ""values"", obj)",jbrockmendel,closed,2020-04-24T18:45:06Z,2020-04-25T21:03:58Z,"xref #27167

@mroeschke can you pls double-check me on the change in window.rolling"
603541774,33685,"BUG: arg validation in DTA/TDA/PA.take, DTI/TDI/PI.where",jbrockmendel,closed,2020-04-20T21:18:24Z,2020-04-25T21:10:13Z,We have a _lot_ of really similar validation/casting methods in these classes.  This is a step in smoothing out the differences between them so we can share them.
599258334,33539,BUG: Series[listlike_of_ints] incorrect on MultiIndex,jbrockmendel,closed,2020-04-14T02:51:16Z,2020-04-25T21:22:00Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This is one of the cases that @jorisvandenbossche identified in #33355."
602819629,33660,REF: Implement NDArrayBackedExtensionArray,jbrockmendel,closed,2020-04-19T21:04:25Z,2020-04-25T21:30:13Z,"Many EAs are thin wrappers around np.ndarray.  We can both de-duplicate a bunch of code and make things easier on downstream authors by implementing `NDArrayBackedExtensionArray` as a base class for such EAs.

This PR only implements `NDArrayBackedExtensionArray.take`, but there is quite a bit more that can be shared in follow-ups:
 - copy, delete, repeat
 - mix in to PandasArray
 - with small changes, `__getitem__`, `__setitem__`, reductions, ...

The only change in logic this PR makes is to make `Categorical.take` raise a ValueError instead of a `TypeError`, matching DTA/TDA/PA."
606854314,33796,CLN: remove unused PeriodEngine methods,jbrockmendel,closed,2020-04-25T21:00:30Z,2020-04-25T21:40:33Z,
601623698,33600,"PERF: op(frame, series) when series is not EA",jbrockmendel,closed,2020-04-17T00:39:34Z,2020-04-25T21:46:55Z,"We have the same optimization in place for the flex ops.

ipython results for the asv this adds:

```
In [3]: arr = np.arange(10 ** 6).reshape(100, -1)                                                                                                                                                                   
In [4]: df = pd.DataFrame(arr)                                                                                                                                                                                         
In [5]: df[""C""] = 1.0                                                                                                                                                                                               
In [6]: row = df.iloc[0]                                                                                                                                                                                            

In [11]: %timeit df + row                                                                                                                                                                                           
1.92 s ± 71.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)   # <-- master
1 ms ± 19.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  # <-- PR
```"
596129018,33380,BUG: `pandas.Series.eq` doc and result mismatch for fill_value,rgsl888prabhu,closed,2020-04-07T20:22:34Z,2020-04-25T21:48:50Z,"- [ ] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample

```
>>> import pandas as pd
>>> import numpy as np
>>> pd.Series([np.nan]).eq(pd.Series([np.nan]), fill_value=5.0)
0    False
dtype: bool
```

#### Problem description

As per the doc, if fill_value is provided and if the element at a position is missing in both series, then result will also be missing, but as per the result, False is being returned rather than `nan`. 

#### Expected Output
So, not sure if doc is wrong or output is wrong. So opening this issue just to clarify this and fix appropriate issue.
#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-26-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : 0.29.12
pytest           : 5.0.1
hypothesis       : None
sphinx           : 2.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.3.4
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.6.1
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.1.0
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.0.1
pyxlsb           : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : 1.3.5
tables           : 3.5.2
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8
numba            : 0.44.1

```
</details>
"
606782234,33789,CLN: Pass numpy args as kwargs,dsaxton,closed,2020-04-25T14:35:04Z,2020-04-25T22:00:36Z,"This is already done in some places for PandasArray, e.g., min / max, and seems cleaner with less boilerplate"
437102724,26209,reindex does not work for groupby series with DateTimeIndex,ptmminh,closed,2019-04-25T09:38:57Z,2020-04-25T22:05:11Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np

# generate a simple df with weekly DateTimeIndex, group and a value
df = pd.DataFrame({
    'group':['Group1','Group2','Group3']*3, 
    'value':np.random.randint(100, 1000, size=9)}, 
    index=pd.date_range('1991-10-2',periods=3, freq='W-MON').repeat(3)
)

# this works as expected
new_value = df.groupby('group')[['value']].apply(
    lambda x: x.reindex(pd.date_range(x.index.min(), x.index.max(), freq='W-MON'), fill_value=0)
)

# this fails
new_value = df.groupby('group').value.apply(
    lambda x: x.reindex(pd.date_range(x.index.min(), x.index.max(), freq='W-MON'), fill_value=0)
)
```
#### Problem description

We have a DF with a group and DateTimeIndex, we want to `reindex` the values per group to make sure all groups have all the appropriate weekly DateTimeIndex. This works as expected when there is a gap to be resolved by the `reindex`ing... However, it fails when there's no gap to be filled by `reindex`.

`ValueError: cannot reindex from a duplicate axis`

This issue is resolved by turning the corresponding Series into a DF by `[[]]` syntax.

#### Expected Output

The original Series.

#### Output of ``pd.show_versions()``

<details>
installed versions
------------------
commit: none
python: 3.6.8.final.0
python-bits: 64
os: linux
os-release: 4.4.0-43-microsoft
machine: x86_64
processor: x86_64
byteorder: little
lc_all: none
lang: en_us.utf-8
locale: en_us.utf-8

pandas: 0.24.2
pytest: 3.5.1
pip: 19.0.3
setuptools: 41.0.0
cython: 0.28.2
numpy: 1.14.3
scipy: 1.1.0
pyarrow: none
xarray: none
ipython: 6.4.0
sphinx: 1.7.4
patsy: 0.5.0
dateutil: 2.8.0
pytz: 2019.1
blosc: none
bottleneck: 1.2.1
tables: 3.4.3
numexpr: 2.6.5
feather: none
matplotlib: 2.2.2
openpyxl: 2.5.3
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.4
lxml.etree: 4.2.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.7
pymysql: none
psycopg2: none
jinja2: 2.10
s3fs: none
fastparquet: none
pandas_gbq: none
pandas_datareader: none
gcsfs: None
</details>
"
602552807,33638,TST: groupby-reindex on DTI,CloseChoice,closed,2020-04-18T21:12:01Z,2020-04-25T22:05:14Z,"Add test to check whether reindexing works correctly.

- [ ] closes #26209
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
604339812,33715,BUG: DTI/TDI/PI.where accepting incorrectly-typed NaTs,jbrockmendel,closed,2020-04-21T23:08:10Z,2020-04-25T22:26:43Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

In addition to the bug in the title, this makes DTI accept datetime/dt64, TDI accept timedelta/td64, and PI accept Period.  This brings these into closer alignment with the other methods, which we ultimately want to make as consistent as possible.

"
599233372,33537,REF: use cached inferred_type when calling lib.infer_dtype(index),jbrockmendel,closed,2020-04-14T01:30:47Z,2020-04-25T22:27:38Z,"Among other things, this will let us avoid a couple of ugly calls in Series

```
        if isinstance(key, Index):
            key_type = key.inferred_type
        else:
            key_type = lib.infer_dtype(key, skipna=False)
```

Cleanup of nearby EA-handling code"
365379889,22921,Merged index is different if left-hand dataframe is empty,carlfischerjba,open,2018-10-01T09:35:34Z,2020-04-25T22:30:53Z,"#### Code Sample

```python
import pandas as pd

pd.__version__
# u'0.23.4'

a = pd.DataFrame(columns=['refA', 'dataA'], data=[[1, 'bla']])
b = pd.DataFrame(columns=['refB', 'dataB'], data=[[2, 'pff']]).set_index('refB')
m = a.merge(b, left_on='refA', right_index=True)

# merged dataframe contains only column from dataframe `a`
m.index
# Index([], dtype='object')
m.columns
# Index([u'refA', u'dataA', u'dataB'], dtype='object')
m
# Empty DataFrame
# Columns: [refA, dataA, dataB]
# Index: []


a = pd.DataFrame(columns=['refA', 'dataA'])
b = pd.DataFrame(columns=['refB', 'dataB'], data=[[1, 'pff']]).set_index('refB')
m = a.merge(b, left_on='refA', right_index=True)

# merged dataframe contains column from dataframe `a` and index from dataframe `b`
m.index
# Index([], dtype='object', name=u'refB')
m.columns
# Index([u'refA', u'dataA', u'dataB'], dtype='object')
m
# Empty DataFrame
# Columns: [refA, dataA, dataB]
# Index: []
```
#### Problem description

When merging an empty dataframe with another dataframe on column and index respectively, the output contains both the index and the column. If the left dataframe is non-empty, then only the column from the left dataframe is carried through while the index from the right dataframe disappears.

#### Expected Output

Consistency between the case where the left dataframe is empty and non-empty, e.g. don't include the index from the righthand dataframe in the merged dataframe. This avoids having to explicitly handle cases where one dataframe is empty in the calling code.


#### Output of ``pd.show_versions()``
<details>
INSTALLED VERSIONS
------------------

commit: None
python: 2.7.13.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.23.4
pytest: 3.0.5
pip: 18.0
setuptools: 39.2.0
Cython: 0.25.2
numpy: 1.14.2
scipy: 0.19.1
pyarrow: None
xarray: None
IPython: 5.1.0
sphinx: 1.7.6
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2018.3
blosc: None
bottleneck: 1.2.1
tables: 3.2.2
numexpr: 2.6.2
feather: None
matplotlib: 2.0.2
openpyxl: 2.4.1
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: 0.9.6
lxml: 3.7.2
bs4: 4.5.3
html5lib: None
sqlalchemy: 1.1.5
pymysql: None
psycopg2: 2.7.4 (dt dec pq3 ext lo64)
jinja2: 2.9.4
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
565742597,32023,Unable to add Timedelta to a Timestamp Interval ,oarcher,closed,2020-02-15T12:37:45Z,2020-04-25T23:58:52Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import pandas as pd
>>> year_2017 = pd.Interval(pd.Timestamp('2017-01-01 00:00:00'),
                                              pd.Timestamp('2018-01-01 00:00:00'))
>>> year_2017
Interval('2017-01-01', '2018-01-01', closed='right')

>>> year_2017 + pd.Timedelta(days=7)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unsupported operand type(s) for +: 'pandas._libs.interval.Interval' and 'Timedelta'
```
#### Problem description

From the [pd.Interval documentation](https://pandas.pydata.org/pandas-docs/version/1.0.1/reference/api/pandas.Interval.html):

> You can operate with + and * over an Interval and the operation is applied to each of its bounds, so the result depends on the type of the bound elements

However , we can apply the + operation manually on each bounds:
```python
>>> (year_2017.left + pd.Timedelta(days=7), year_2017.right + pd.Timedelta(days=7) )
(Timestamp('2017-01-08 00:00:00'), Timestamp('2018-01-08 00:00:00'))
```

#### Expected Output

The same as the manual operation on each bounds:

```python
>>> pd.Interval(year_2017.left + pd.Timedelta(days=7), 
                         year_2017.right + pd.Timedelta(days=7) )
Interval('2017-01-08', '2018-01-08', closed='right')
```
#### Output of ``pd.show_versions()``

pandas 1.0.1
<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.0-146-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : fr_FR.UTF-8
LOCALE           : fr_FR.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.4.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.10
tables           : None
tabulate         : None
xarray           : 0.15.0
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.46.0

</details>
"
567625152,32107,BUG: Allow addition of Timedelta to Timestamp interval,dsaxton,closed,2020-02-19T14:53:31Z,2020-04-26T00:06:54Z,"- [x] closes #32023
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
602504383,33635,ENH: allow passing freq=None to DatetimeIndex/TimedeltaIndex,jbrockmendel,closed,2020-04-18T17:00:30Z,2020-04-26T00:11:17Z,cc @jreback discussed elsewhere
604089202,33703,BUG: DTI/TDI.insert doing invalid casting,jbrockmendel,closed,2020-04-21T15:28:51Z,2020-04-26T00:32:44Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
599898537,33552,BUG: Setting DTI/TDI freq affecting other indexes viewing the same data,jbrockmendel,closed,2020-04-14T22:32:33Z,2020-04-26T02:11:00Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The issue this addresses is related to #31218."
349402209,22268,Rolling with win_type='triang' strange results,WangWenQiang,closed,2018-08-10T07:15:42Z,2020-04-26T03:30:22Z,"in my env(mac, python3.6.1,pandas0.23.4)

```python
In [26]: df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})
In [27]: df
Out[27]:
B
0 0.0
1 1.0
2 2.0
3 NaN
4 4.0

In [28]: df.rolling(2, win_type='triang').sum()
Out[28]:
B
0 NaN
1 0.5
2 1.5
3 NaN
4 NaN

In [29]: df.rolling(2, win_type='triang').mean()
Out[29]:
B
0 NaN
1 0.5
2 1.5
3 NaN
4 NaN
```

but in the docs,

```python
df.rolling(2, win_type='triang').sum()
B
0 NaN
1 1.0
2 2.5
3 NaN
4 NaN
```

q1: docs.sum and myenv.sum is different
q2:sum()'result is the same as means()'result"
596324931,33388,ENH: Add numba engine to groupby.aggregate,mroeschke,closed,2020-04-08T05:48:14Z,2020-04-26T04:21:57Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

In the same spirit of #31845 and #32854, adding `engine` and `engine_kwargs` arguments to `groupby.aggreate`.

This PR has some functionality that is waiting to be merged in #32854"
605397533,33745,BUG: support skew function for custom BaseIndexer rolling windows,AlexKirko,closed,2020-04-23T09:53:20Z,2020-04-26T06:01:34Z,"- [X] xref #32865 
- [X] 1 tests added / 1 passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

## Scope of PR
This PR does a couple things to fix the performance of `skew`, and this also fixes the behavior of all the functions that have `require_min_periods` for small values of `min_periods`:
* clarify docs. Name all sample methods uniformly, add a note to the caveat that users should in general be careful about using sample methods with windows
* fix bug in `_apply` that made us never go into the `BaseIndexer` control flow branch
* fix bug in `_apply`: pass `window_indexer.window_size` into `calc_min_periods` instead of `min_periods or 1`. We want the custom indexer window size there, so it's not clear to me why we were passing `min_periods or 1` .

## Details
The algorithm itself is robust, but it defaults to sample skewness, which is why there was a difference between its output and `numpy`. To prevent misunderstandings, I clarified the docs a bit.
We were also passing a wrong value to `calc_min_periods`, and we weren't going into the proper if branch, because we were checking the type of `window` instead of `self.window`.

## Background on the wider issue
We currently don't support several rolling window functions when building a rolling window object using a custom class descended from `pandas.api.indexers.Baseindexer`. The implementations were written with backward-looking windows in mind, and this led to these functions breaking.
Currently, using these functions returns a `NotImplemented` error thanks to #33057, but ideally we want to update the implementations, so that they will work without a performance hit. This is what I aim to do over a series of PRs.

## Perf notes
No changes to algorithms.
"
606781408,33788,DOC: Remove ambiguity in fill_value documentation,bharatr21,closed,2020-04-25T14:30:52Z,2020-04-26T08:00:14Z,"- [x] closes #33380 
- [ ] tests added / passed (Not needed)
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry (Not needed)
"
602432887,33626,BUG: support median function for custom BaseIndexer rolling windows,AlexKirko,closed,2020-04-18T10:56:16Z,2020-04-26T11:34:35Z,"- [X] xref #32865 
- [X] 1 tests added / 1 passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

## Scope of PR
This PR makes sure that when we calculate the median in `roll_median_c`, we override the maximum window width with the correct value, and the function doesn't shortcut by returning all NaNs.

## Details
The `median` function eventually calls `roll_median_c` which accepts a `win` parameter (maximum window width) to correctly allocate memory and initialize the skiplist data structure which is the backbone of the rolling median algorithm. Currently, `win` is determined by the `_get_window` function which returns `min_periods or 0` for custom `BaseIndexer` subclasses: 
```python
    def _get_window(self, other=None, win_type: Optional[str] = None) -> int:
        """"""
        Return window length.

        Parameters
        ----------
        other :
            ignored, exists for compatibility
        win_type :
            ignored, exists for compatibility

        Returns
        -------
        window : int
        """"""
        if isinstance(self.window, BaseIndexer):
            return self.min_periods or 0
        return self.window
```
Thus, `roll_median_c` either shortcuts or initializes to incorrect depth:
```python
...
    if win == 0 or (end - start).max() == 0:
        output[:] = NaN
        return output
    win = (end - start).max()
    sl = skiplist_init(<int>win)
...
```

<s>I propose we determine max window length directly in the `median` function. This means that `start` and `end` arrays get calculated twice: here and in `_apply`. However, I belive this is better than injecting a median-specific crutch into `_apply` or messing with the shortcut in `roll_median_c` (we could attempt to override `win` if `(end - start).max() > 0`. This other option is explored below.</s>
After discussing with @mroeschke , we decided to implement the bugfix directly in `roll_median_c`. Details below.

Please say if you think another approach would be preferable.

## Background on the wider issue
We currently don't support several rolling window functions when building a rolling window object using a custom class descended from `pandas.api.indexers.Baseindexer`. The implementations were written with backward-looking windows in mind, and this led to these functions breaking.
Currently, using these functions returns a `NotImplemented` error thanks to #33057, but ideally we want to update the implementations, so that they will work without a performance hit. This is what I aim to do over a series of PRs.

## Perf notes
The function currently shortcuts because of the bug or initializes the main datastructure incorrectly. For this reason, benchmarks are meaningless.
Ran benchmarks vs. the shortcut anyway:
```
 asv continuous -f 1.1 master HEAD -b ^rolling.ForwardWindowMethods
...
       before           after         ratio
     [b630cdbc]       [ce82372f]
     <master>         <rolling-median>
+      3.55▒0.2ms       75.2▒0.9ms    21.19  rolling.ForwardWindowMethods.time_rolling('DataFrame', 1000, 'float', 'median')
+     4.25▒0.08ms         76.7▒2ms    18.06  rolling.ForwardWindowMethods.time_rolling('Series', 1000, 'float', 'median')
+      4.02▒0.4ms       60.3▒0.5ms    14.99  rolling.ForwardWindowMethods.time_rolling('DataFrame', 1000, 'int', 'median')
+      3.42▒0.2ms         47.4▒1ms    13.86  rolling.ForwardWindowMethods.time_rolling('DataFrame', 10, 'float', 'median')
+      4.80▒0.1ms       61.2▒0.5ms    12.74  rolling.ForwardWindowMethods.time_rolling('Series', 1000, 'int', 'median')
+     4.58▒0.06ms       49.7▒0.9ms    10.85  rolling.ForwardWindowMethods.time_rolling('Series', 10, 'float', 'median')
+      4.45▒0.5ms       45.0▒0.8ms    10.10  rolling.ForwardWindowMethods.time_rolling('DataFrame', 10, 'int', 'median')
+     5.02▒0.07ms       45.9▒0.6ms     9.14  rolling.ForwardWindowMethods.time_rolling('Series', 10, 'int', 'median')

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE DECREASED.
```
"
606865939,33797,REF: mix NDArrayBackedExtensionArray into PandasArray,jbrockmendel,closed,2020-04-25T22:12:03Z,2020-04-26T14:09:12Z,Share several more methods in NDArrayBackedExtensionArray.
606658595,33781,TST: more specific freq attrs,jbrockmendel,closed,2020-04-25T01:15:24Z,2020-04-26T19:27:20Z,Getting ready to check matching freq in assert_index_equal
607060095,33808,CLN: use unpack_zerodim_and_defer in timedeltas,jbrockmendel,closed,2020-04-26T17:00:34Z,2020-04-26T19:33:14Z,
606898234,33801,BUG: incorrect freq in PeriodIndex-Period,jbrockmendel,closed,2020-04-26T02:11:24Z,2020-04-26T19:34:01Z,introduced in #33552
592114700,33215,[PROPOSAL] Make Series.update() accept a dictionary,raffaem,closed,2020-04-01T18:23:28Z,2020-04-26T19:49:37Z,"#### Code Sample, a copy-pastable example if possible

I propose to re-write the `Series.update() ` method to accept a dictionary object. This would be useful for example when you are iterating over a DataFrame using `DataFrame.iterrows()`, and you want to update the `row` object returned by `iterrows()` in order to build another DataFrame starting from the DataFrame you are iterating over

Here is an example that doesn't work:

```python
import pandas as pd
s = pd.Series({'city':'Rome'})
s.update({'country':'Italy'})
```
#### Problem description

> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""C:\InstalledPrograms\Anaconda3\lib\site-packages\pandas\core\series.py"", line 2807, in update
>     other = other.reindex_like(self)
> AttributeError: 'dict' object has no attribute 'reindex_like'

#### Simple use case

In this use case, an explicit conversion to dict() is needed

```
import random
import pandas as pd

def very_long_dictionary():
    d = dict()
    for c in range(2,10):
        d[c] = random.randint(1,101)
    return d

df = pd.DataFrame([[1,2],[3,4],[5,6]])
print(df)

outdl = list()
for index,row in df.iterrows():
    #Explicit conversion to dict is needed
    row = dict(row)
    row.update(very_long_dictionary())
    outdl.append(row)
newdf = pd.DataFrame(outdl)
print(newdf)
```
#### Expected Output

The series would update to the new value

#### Output of ``pd.show_versions()``

<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : 0.29.15
pytest           : 5.4.1
hypothesis       : 5.5.4
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.8
numba            : 0.48.0

</details>
"
597391177,33426,BUG: core.missing._akima_interpolate will raise AttributeError,jbrockmendel,closed,2020-04-09T16:08:50Z,2020-04-26T19:57:38Z,"```
    from scipy import interpolate

    P = interpolate.Akima1DInterpolator(xi, yi, axis=axis)

    if der == 0:
        return P(x)
    elif interpolate._isscalar(der):
        return P(x, der=der)
    else:
        return [P(x, nu) for nu in der]
```

There is no `interpolate._isscalar` so this would raise if it were ever reached.  AFAICT we have only one test that gets to this function, must always have `der == 0`"
606689589,33784,"BUG, TST, DOC: fix core.missing._akima_interpolate will raise AttributeError",hkennyv,closed,2020-04-25T04:54:33Z,2020-04-26T19:57:42Z,"- [x] closes #33426 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Wasn't sure if this warranted a whatsnew entry since its mostly code cleanup"
600622019,33575,ENH: Add index to output of assert_series_equal on category and datetime values,amilbourne,closed,2020-04-15T22:19:44Z,2020-04-26T20:22:54Z,"- [ ] closes #xxxx
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This is really an extension of #31435, which added index output to the messages produced when series (and DataFrames) are compared.  I failed to notice that categorical values and datetime values were evaluated through a different code path and didn't add the index output for these data types.  Since the original problem (index reordering) could just as easily happen for these types the functionality should be there as well.  Also, it makes the output more consistent.

I added a couple of new tests to surface and check the additional output."
524642594,29697,BUG: merge raises for how='outer'/'right' when duplicate suffixes are specified,jschendel,closed,2019-11-18T21:55:51Z,2020-04-26T20:24:17Z,"#### Code Sample, a copy-pastable example if possible
On `master` the following raises for `how='outer'` and `how='right'` with duplicate `suffixes`:
```python
In [1]: import pandas as pd; pd.__version__
Out[1]: '0.26.0.dev0+958.g545d17529'

In [2]: df1 = pd.DataFrame({'A': list('ab'), 'B': [0, 1]})

In [3]: df2 = pd.DataFrame({'A':list('ac'), 'B': [100, 200]})

In [4]: pd.merge(df1, df2, on=""A"", how=""outer"", suffixes=(""_x"", ""_x""))
---------------------------------------------------------------------------
ValueError: Buffer has wrong number of dimensions (expected 1, got 0)

In [5]: pd.merge(df1, df2, on=""A"", how=""right"", suffixes=(""_x"", ""_x""))
---------------------------------------------------------------------------
ValueError: Buffer has wrong number of dimensions (expected 1, got 0)
```

Note that above works with `how='inner'` and `how='left'`:
```python
In [6]: pd.merge(df1, df2, on=""A"", how=""inner"", suffixes=(""_x"", ""_x""))
Out[6]: 
   A  B_x  B_x
0  a    0  100

In [7]: pd.merge(df1, df2, on=""A"", how=""left"", suffixes=(""_x"", ""_x""))
Out[7]: 
   A  B_x    B_x
0  a    0  100.0
1  b    1    NaN
```

Likewise, if unique `suffixes` are specified then `how='outer'` and `how='right'` work fine:
```python
In [8]: pd.merge(df1, df2, on=""A"", how=""outer"", suffixes=(""_x"", ""_y""))
Out[8]: 
   A  B_x    B_y
0  a  0.0  100.0
1  b  1.0    NaN
2  c  NaN  200.0

In [9]: pd.merge(df1, df2, on=""A"", how=""right"", suffixes=(""_x"", ""_y""))
Out[9]: 
   A  B_x  B_y
0  a  0.0  100
1  c  NaN  200
```

#### Problem description

`pandas.merge` raises for `how='outer'` and `how='right'` with duplicate `suffixes`.

#### Expected Output
I'd expect `In [4]` and `In [5]` not to raise and produce output similar to `Out[8]` and `Out[9]` but with the duplicate suffix names.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 545d1752987f8e325f5ad3d94f0143c453de28cc
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.6.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.26.0.dev0+958.g545d17529
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.6.0.post20191030
Cython           : 0.29.13
pytest           : 4.6.2
hypothesis       : 4.23.6
sphinx           : 1.8.5
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.3.3
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : 4.7.1
bottleneck       : 1.2.1
fastparquet      : 0.3.0
gcsfs            : None
lxml.etree       : 4.3.3
matplotlib       : 3.1.0
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : 0.11.1
pytables         : None
s3fs             : 0.2.1
scipy            : 1.2.1
sqlalchemy       : 1.3.4
tables           : 3.5.2
xarray           : 0.12.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8
</details>
"
598603633,33508,Added two tests for issue #29697,devjeetr,closed,2020-04-13T00:28:11Z,2020-04-26T20:24:21Z,"I added two basic tests for `merge` as requested in #29697, one for `how=""outer""` and one for  `how=""right""`. 

I'm not sure if the way I'm creating duplicate columns for the `expected` dataframe is the recommended way but I could not find anything on this. Please let me know if there's a better way to do this.

- [x] closes #29697
- [x] 2 tests added / 2 passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`



"
607046207,33806,TST: Added message to bare pytest.raises in test_join_multi_levels (#…,Dom-L-G,closed,2020-04-26T16:01:09Z,2020-04-26T20:31:07Z,"…30999)

Updated tests in test_join_multi_levels to include messages for pytest.raises.

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
508575587,29055,URL treated as local file for read_feather,mccarthyryanc,closed,2019-10-17T16:02:49Z,2020-04-26T20:34:07Z,"Not sure if this is a pandas issue or pyarrow, but when I try to read from a URL:
```python
import pandas as pd
pd.read_feather(""https://github.com/wesm/feather/raw/master/R/inst/feather/iris.feather"")
```
I get the following error:
```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/ubuntu/miniconda3/envs/pandas/lib/python3.7/site-packages/pandas/util/_decorators.py"", line 208, in wrapper
    return func(*args, **kwargs)
  File ""/home/ubuntu/miniconda3/envs/pandas/lib/python3.7/site-packages/pandas/io/feather_format.py"", line 119, in read_feather
    return feather.read_feather(path, columns=columns, use_threads=bool(use_threads))
  File ""/home/ubuntu/miniconda3/envs/pandas/lib/python3.7/site-packages/pyarrow/feather.py"", line 214, in read_feather
    reader = FeatherReader(source)
  File ""/home/ubuntu/miniconda3/envs/pandas/lib/python3.7/site-packages/pyarrow/feather.py"", line 40, in __init__
    self.open(source)
  File ""pyarrow/error.pxi"", line 80, in pyarrow.lib.check_status
  File ""pyarrow/io.pxi"", line 1406, in pyarrow.lib.get_reader
  File ""pyarrow/io.pxi"", line 1395, in pyarrow.lib._get_native_file
  File ""pyarrow/io.pxi"", line 788, in pyarrow.lib.memory_map
  File ""pyarrow/io.pxi"", line 751, in pyarrow.lib.MemoryMappedFile._open
  File ""pyarrow/error.pxi"", line 80, in pyarrow.lib.check_status
pyarrow.lib.ArrowIOError: Failed to open local file 'https://github.com/wesm/feather/raw/master/R/inst/feather/iris.feather', error: No such file or directory
```

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-64-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : 0.4.0
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.0
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
```

</details>
"
112076997,11367,BUG: pd.eval handles operator not as operator ~,kawochen,open,2015-10-19T05:50:28Z,2020-04-26T20:54:46Z,"```
>>> import pandas as pd
>>> pd.eval('not True')
-2
```

Not sure if this was intentional, since `'not'` maps to `op.invert` in https://github.com/pydata/pandas/blob/master/pandas/computation/ops.py#L458. After fixing that `numexpr` still `invert`s rather than `not`s the operand.

```
INSTALLED VERSIONS
------------------
commit: 53a0db18104abde098d43c66d77555fad12ed8c1
python: 3.4.3.final.0
python-bits: 64
OS: Linux
OS-release: 3.13.0-63-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.17.0+62.g53a0db1
nose: 1.3.6
pip: 7.1.2
setuptools: 18.4
Cython: 0.22
numpy: 1.9.2
scipy: 0.15.1
statsmodels: None
IPython: 4.0.0
sphinx: 1.3.1
patsy: 0.3.0
dateutil: 2.4.2
pytz: 2015.4
blosc: None
bottleneck: 0.8.0
tables: 3.1.1
numexpr: 2.4.3
matplotlib: 1.4.3
openpyxl: 2.0.3
xlrd: 0.9.4
xlwt: 1.0.0
xlsxwriter: 0.7.6
lxml: 3.4.4
bs4: 4.3.2
html5lib: 1.0b2
httplib2: None
apiclient: None
sqlalchemy: 1.0.4
pymysql: 0.6.6.None
psycopg2: None
```
"
607101638,33812,TST: check freq in assert_equal,jbrockmendel,closed,2020-04-26T20:07:40Z,2020-04-26T20:55:14Z,"Before long we'll move this check into assert_index_equal, for now just want to get validation in place where feasible."
607098439,33811,BUG: pickle after _with_freq,jbrockmendel,closed,2020-04-26T19:51:44Z,2020-04-26T20:55:29Z,introduced in #33552
358378295,22649,BUG: dataframe.eval() throws error when using @function if numexpr package is installed,bogdanCsn,open,2018-09-09T13:23:24Z,2020-04-26T21:01:45Z,"Full details and workaround are available in this [SO question](https://stackoverflow.com/questions/52233633/dataframe-eval-using-function-name-in-expression/52236235)

#### Code Sample, a copy-pastable example if possible

```python
import numpy as np
import pandas as pd
from numpy import around

df = pd.DataFrame({'x':np.array([1.12,2.76])})

# this throws TypeError: 'Series' objects are mutable, thus they cannot be hashed
df['y'] = df.eval('@around(x,1)')
```
#### Problem description
The evaluated expression should work because `around` is imported into the local namespace. There are two solutions:
-  remove `numexpr` from the environment.
-  set the engine for parsing the expression to be python:
```python
df['y'] = df.eval('@around(x,1)', engine = 'python')
```

#### Expected Output
```
          x    y
0  1.133320  1.1
1  2.733336  2.7
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.0.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en
LOCALE: None.None

pandas: 0.23.4
pytest: None
pip: 10.0.1
setuptools: 40.2.0
Cython: None
numpy: 1.15.1
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.5.0
sphinx: 1.7.8
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: None
numexpr: 2.6.8
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
605963140,33756,BUG: DataFrame.truncate() returns incorrect data if the index is sorted in descending order,francislan,closed,2020-04-24T00:52:47Z,2020-04-26T21:04:16Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas (1.0.3)

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame(index=range(0, 5), data={'A': 1})
df.sort_index(ascending=False, inplace=True)
df.truncate(before=3)
```

#### Problem description

When the index is sorted in descending order, the truncate() function behaves incorrectly. In the example above, it should truncate all rows whose index is < 3. Instead it is truncating all rows whose index is > 3.
Similarly when using after.

#### Output

```python
>>> df
   A
4  1
3  1
2  1
1  1
0  1
>>> df.truncate(before=3)
   A
3  1
2  1
1  1
0  1
```

#### Expected Output

```python
>>> df.truncate(before=3)
   A
4  1
3  1
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.4.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
411826895,25369,Extension dtypes do not work with dataframe query method ,techvslife,open,2019-02-19T09:31:44Z,2020-04-26T21:10:26Z,"In the latest pandas version (0.24.1), extension dtypes do not work with numexpr (e.g. the “query” method on dataframes fails when the engine parameter is set to the default ):

https://stackoverflow.com/questions/54759936/extension-dtypes-in-pandas-appear-to-have-a-bug-with-query

Code to reproduce:
df_test = pd.DataFrame(data=[4,5,6], columns=[""col_test""])
df_test = df_test.astype(dtype={""col_test"": pd.Int32Dtype()})
df_test.query(""col_test != 6"")
Last lines of the long error message are:

File ""...\site_packages\numexpr\necompiler.py"", line 822, in evaluate zip(names, arguments)] File ""...\site_packages\numexpr\necompiler.py"", line 821, in signature = [(name, getType(arg)) for (name, arg) in File ""...\site_packages\numexpr\necompiler.py"", line 703, in getType raise ValueError(""unknown type %s"" % a.dtype.name) ValueError: unknown type object"
532136647,30005,Falling back silently to engine='python' whenever numexpr is not installed causes a query call to break more easy,hwalinga,open,2019-12-03T17:12:57Z,2020-04-26T21:11:32Z,"`query` provides an interesting interface to query data from a pandas dataframe. It also opens up possibilities to have these queries be processed by different kind of engines. Today these options are 'python' and 'numexpr', where 'numexpr' is the default here. 

The problematic part here is, is that numexpr is often not installed. As a fallback pandas will use the 'python' engine. This wouldn't be a problem if the difference is only speed, but the 'python' engine provides more features than the 'numexpr' which can cause somebody to write code that works perfectly on his/her machine, but can unexpectedly break on another machine where numexpr happens to be installed. This for example was the case with issue #29027.

I think such a hidden pitfall should not exist, and there should be someway to prevent this from happening or warn the user if it can happen. 

Some possibilities I thought about:

 * Set engine='python' the default. Simple and probably the best solution.

 * Set engine='numexpr' the default and require an explicit engine='python' whenever numexpr is not installed. (So do not silenty fallback.)

 * Alternative, throw an warning whenever numexpr is not installed, which can be suppressed by setting engine='python' explicitly. (So fallback with sound.)

* Instead throw an error whenever numexpr is not installed.

* Have numexpr as a dependency.

 * Figure out if `.query` is called with syntax not allowed for numexpr and throw a warning whenever engine='python' is not explicitly set. I don't know if this is possible. This might be also more work in the future whenever something changes to the parser, but is probably the most ideal solution."
545683471,30729,TypeError when subtracting datetime64 and timestamp but only in eval,alexmojaki,open,2020-01-06T11:37:51Z,2020-04-26T21:12:04Z,"#### Code Sample

```python
import pandas as pd
pd.show_versions()

today = pd.to_datetime(""today"")
df = pd.DataFrame({'date': [today]})

assert isinstance(today, pd.Timestamp)
assert str(df.date.dtype) == 'datetime64[ns]'

delta = df.date - today  # works fine
assert str(delta.dtype) == 'timedelta64[ns]'

df.eval(""date - @today"")  # fails
```

Live demo: https://repl.it/repls/SelfassuredFrighteningNumber

#### Problem description

Subtraction works 'normally' but not when used inside DataFrame.eval or .query. AFAIK the two methods should be equivalent. eval fails with:

```python
Traceback (most recent call last):
  File ""main.py"", line 13, in <module>
    df.eval(""date - @today"")  # fails
  File ""/home/runner/.local/share/virtualenvs/python3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3315, in eval
    return _eval(expr, inplace=inplace, **kwargs)
  File ""/home/runner/.local/share/virtualenvs/python3/lib/python3.7/site-packages/pandas/core/computation/eval.py"", line 322, in eval
    parsed_expr = Expr(expr, engine=engine, parser=parser, env=env, truediv=truediv)
  File ""/home/runner/.local/share/virtualenvs/python3/lib/python3.7/site-packages/pandas/core/computation/expr.py"", line 830, in __init__
    self.terms = self.parse()
  File ""/home/runner/.local/share/virtualenvs/python3/lib/python3.7/site-packages/pandas/core/computation/expr.py"", line 847, in parse
    return self._visitor.visit(self.expr)
  File ""/home/runner/.local/share/virtualenvs/python3/lib/python3.7/site-packages/pandas/core/computation/expr.py"", line 441, in visit
    return visitor(node, **kwargs)
  File ""/home/runner/.local/share/virtualenvs/python3/lib/python3.7/site-packages/pandas/core/computation/expr.py"", line 447, in visit_Module
    return self.visit(expr, **kwargs)
  File ""/home/runner/.local/share/virtualenvs/python3/lib/python3.7/site-packages/pandas/core/computation/expr.py"", line 441, in visit
    return visitor(node, **kwargs)
  File ""/home/runner/.local/share/virtualenvs/python3/lib/python3.7/site-packages/pandas/core/computation/expr.py"", line 450, in visit_Expr
    return self.visit(node.value, **kwargs)
  File ""/home/runner/.local/share/virtualenvs/python3/lib/python3.7/site-packages/pandas/core/computation/expr.py"", line 441, in visit
    return visitor(node, **kwargs)
  File ""/home/runner/.local/share/virtualenvs/python3/lib/python3.7/site-packages/pandas/core/computation/expr.py"", line 565, in visit_BinOp
    return self._maybe_evaluate_binop(op, op_class, left, right)
  File ""/home/runner/.local/share/virtualenvs/python3/lib/python3.7/site-packages/pandas/core/computation/expr.py"", line 536, in _maybe_evaluate_binop
    "" '{lhs}' and '{rhs}'"".format(op=res.op, lhs=lhs.type, rhs=rhs.type)
TypeError: unsupported operand type(s) for -: 'datetime64[ns]' and '<class 'pandas._libs.tslibs.timestamps.Timestamp'>'
```

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-1036-gcp
machine          : x86_64
processor        :
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3numpy            : 1.18.0
pytz             : 2019.3
dateutil         : 2.8.1pip              : 19.0.3
setuptools       : 40.8.0
Cython           : Nonepytest           : None
hypothesis       : None
sphinx           : Noneblosc            : None
feather          : None
xlsxwriter       : Nonelxml.etree       : None
html5lib         : None
pymysql          : Nonepsycopg2         : None
jinja2           : None
IPython          : Nonepandas_datareader: None
bs4              : None
bottleneck       : Nonefastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
</details>
"
547410988,30845,numexpr inconsistent result,misantroop,open,2020-01-09T11:09:49Z,2020-04-26T21:12:46Z,"```python
df = pd.DataFrame([5, 10, 15, 5, 10, 15, 5, 10, 15])

op = '<'
level = 10

t = df[0]
df['t'] = t[numexpr.evaluate(f't {op} {level}')]

trueConds = (df['t'] == np.nan)

```
#### Problem description

Column evaluated with numexpr and successfully creating a new col (t), however the trueConds finds no NaNs, which are present in df.

#### Expected Output

Expecting trueConds as:

0    False
1    True
2    True
3    False
4    True
5    True
6    False
7    True
8    True

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United Kingdom.1252

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 44.0.0
Cython           : 0.29.14
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
606459805,33769,BUG: Adjust truncate for decreasing index,dsaxton,closed,2020-04-24T17:08:07Z,2020-04-26T21:47:37Z,"- [x] closes #33756
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
293273955,19480,Features which Interval / IntervalIndex should probably have,alexlenail,open,2018-01-31T19:00:35Z,2020-04-26T23:59:52Z,"Here's a list of possible features which should be added to the Interval and IntervalIndex types. Some of them might already exist, in which case, please excuse my mistake. **Also note**, I'm not asking for these, just listing some ideas I had which might be useful for others.  

### closest
```
>>> Interval(3, 4).closest(IntervalIndex.from_tuples([(1, 2), (5, 9)]), how=""min|mean|max"")
>>> IntervalIndex.from_tuples([(1, 2), (5, 9)]).closest(IntervalIndex.from_tuples([(1, 4), (2, 5)]))
```
Big question: what if there are two or more identically distant intervals? 

### complement
```
>>> IntervalIndex.from_tuples([(1, 2), (5, 9)]).complement(lower_bound=np.neginf, upper_bound=np.inf)
IntervalIndex.from_tuples([(np.neginf, 1), (2, 5), (9, np.inf)])
```

### intersection
```
>>> IntervalIndex.from_tuples([(1, 2), (5, 9)]).intersect(IntervalIndex.from_tuples([(3, 4), (6, 7)]))
IntervalIndex.from_tuples([(6, 7)])
>>> IntervalIndex.from_tuples([(1, 2), (5, 9)]).intersect(Interval(6, 7))
???
```

### union
```
>>> IntervalIndex.from_tuples([(1, 2), (5, 9)]).intersect(IntervalIndex.from_tuples([(3, 4), (6, 7)]))
IntervalIndex.from_tuples([(1, 2), (3, 4), (5, 9)])
```

### subtract|difference
```
>>> IntervalIndex.from_tuples([(1, 2), (5, 9)]) - IntervalIndex.from_tuples([(3, 4), (6, 7)])
IntervalIndex.from_tuples([(1, 2), (5, 6), (7, 9)])
>>> IntervalIndex.from_tuples([(1, 2), (5, 9)]) - Interval(6, 7)
IntervalIndex.from_tuples([(1, 2), (5, 6), (7, 9)])
>>> Interval(6, 7) - IntervalIndex.from_tuples([(1, 2), (5, 9)])
???
```

### sort
 (I'm guessing this is probably already implicitly implemented. Does it work for multi-index in which one of the levels is Intervalindex?)

### shift
```
>>> IntervalIndex.from_tuples([(1, 2), (5, 9)]).shift(2)
IntervalIndex.from_tuples([(3, 4), (7, 11)])
```
(Could be really useful for datetimes?)

### slop|grow|window|better name
```
>>> IntervalIndex.from_tuples([(1, 2), (5, 9)]).slop(1)
IntervalIndex.from_tuples([(0, 3), (4, 10)])
```
"
456683790,26893,interval.overlaps() mishandles empty intervals,ghost,open,2019-06-16T21:26:20Z,2020-04-27T00:14:50Z,"The special case where `left=right` is not checked properly by the constructor, only `closed='both'` (a point)  or `closed='neither'` (an empty interval) makes sense but '[0,0)' or `(0,0]` are nonsensical. The `overlaps` method returns inconsistent results when encountering such Intervals. 

```python
import pandas as pd
pd.Interval(0,0,'left')
```

**Update:**
Examples (from below):
```python
In [4]:  
   ...: import pandas as pd
   ...: from pandas import Interval
   ...: 
   ...: a=Interval(0,1,'both')
   ...: b=Interval(0,0,'right')
   ...: print(a.overlaps(b))
   ...: b=Interval(0,0,'left')
   ...: print(a.overlaps(b))
True
False
```"
469870854,27456,issues with overlapping multi index intervals,mahdirajabi96,open,2019-07-18T16:33:22Z,2020-04-27T00:16:45Z,"Scenario 1: single-level indexing, which works fine:

    import pandas as pd # pandas version 0.25.0, python version: 3.6.6
    idx = pd.IntervalIndex.from_arrays([1,3,1,2],
                                 [3,4,2,4])
    df = pd.DataFrame({'Value':[1,2,3,4]},index=idx) 

which returns:

    df = 
              Value
    (1,3]   1
    (3,4]   2
    (1,2]   3
    (2,4]   4

query results:

    df.loc[1.5] = 
              Value
    (1,3]   1
    (1,2]   3

Scenario 2: Multi-level indexing:

    idx1 = pd.MultiIndex.from_arrays([
        pd.Index(['label1','label1','label2','label2']),
        pd.IntervalIndex.from_arrays([1,3,1,2],
                                 [3,4,2,4])
    ])
    idx2 = pd.MultiIndex.from_arrays([
        pd.Index(['label1','label1','label2','label2']),
        pd.IntervalIndex.from_arrays([1,2,1,2],
                                 [2,4,2,4])
    ])
    df1 = pd.DataFrame({'Value':[1,2,3,4]},index=idx1) #with overlapping intervals 
    df2 = pd.DataFrame({'Value':[1,2,3,4]},index=idx2) #without overlapping intervals

which returns:

    df1 = 
                        Value
    label1    (1,3]   1
    label1    (3,4]   2
    label2    (1,2]   3
    label2    (2,4]   4
    df2 = 
                        Value
    label1    (1,2]   1
    label1    (2,4]   2
    label2    (1,2]   3
    label2    (2,4]   4

query method 1: works fine on both df1 and df2 but is slow

    df1.Value.loc['label1'].loc[1.5]
    1

query method 2: works only with df2, doesn't work with df1, is 10 times faster than query method 1

    df2.Value.loc[('label1',1.5)]
    1
    df1.Value.loc[('label1',1.5)]

---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
C:\Program Files\ArcGIS\Pro\bin\Python\envs\arcgispro-py3\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
   2889             try:
-> 2890                 return self._engine.get_loc(key)
   2891             except KeyError:

pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 1.5

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
<ipython-input-12-ef83c1160165> in <module>()
     11 display(df)
     12 print(df.loc['label1'].loc[1.5])
---> 13 print(df.loc[('label1',1.5)])

C:\Program Files\ArcGIS\Pro\bin\Python\envs\arcgispro-py3\lib\site-packages\pandas\core\indexing.py in __getitem__(self, key)
   1402                 except (KeyError, IndexError, AttributeError):
   1403                     pass
-> 1404             return self._getitem_tuple(key)
   1405         else:
   1406             # we by definition only have the 0th axis

C:\Program Files\ArcGIS\Pro\bin\Python\envs\arcgispro-py3\lib\site-packages\pandas\core\indexing.py in _getitem_tuple(self, tup)
    789     def _getitem_tuple(self, tup):
    790         try:
--> 791             return self._getitem_lowerdim(tup)
    792         except IndexingError:
    793             pass

C:\Program Files\ArcGIS\Pro\bin\Python\envs\arcgispro-py3\lib\site-packages\pandas\core\indexing.py in _getitem_lowerdim(self, tup)
    945                     return section
    946                 # This is an elided recursive call to iloc/loc/etc'
--> 947                 return getattr(section, self.name)[new_key]
    948 
    949         raise IndexingError(""not applicable"")

C:\Program Files\ArcGIS\Pro\bin\Python\envs\arcgispro-py3\lib\site-packages\pandas\core\indexing.py in __getitem__(self, key)
   1402                 except (KeyError, IndexError, AttributeError):
   1403                     pass
-> 1404             return self._getitem_tuple(key)
   1405         else:
   1406             # we by definition only have the 0th axis

C:\Program Files\ArcGIS\Pro\bin\Python\envs\arcgispro-py3\lib\site-packages\pandas\core\indexing.py in _getitem_tuple(self, tup)
    789     def _getitem_tuple(self, tup):
    790         try:
--> 791             return self._getitem_lowerdim(tup)
    792         except IndexingError:
    793             pass

C:\Program Files\ArcGIS\Pro\bin\Python\envs\arcgispro-py3\lib\site-packages\pandas\core\indexing.py in _getitem_lowerdim(self, tup)
    913         for i, key in enumerate(tup):
    914             if is_label_like(key) or isinstance(key, tuple):
--> 915                 section = self._getitem_axis(key, axis=i)
    916 
    917                 # we have yielded a scalar ?

C:\Program Files\ArcGIS\Pro\bin\Python\envs\arcgispro-py3\lib\site-packages\pandas\core\indexing.py in _getitem_axis(self, key, axis)
   1823         # fall thru to straight lookup
   1824         self._validate_key(key, axis)
-> 1825         return self._get_label(key, axis=axis)
   1826 
   1827 

C:\Program Files\ArcGIS\Pro\bin\Python\envs\arcgispro-py3\lib\site-packages\pandas\core\indexing.py in _get_label(self, label, axis)
    155             raise IndexingError(""no slices here, handle elsewhere"")
    156 
--> 157         return self.obj._xs(label, axis=axis)
    158 
    159     def _get_loc(self, key: int, axis: int):

C:\Program Files\ArcGIS\Pro\bin\Python\envs\arcgispro-py3\lib\site-packages\pandas\core\generic.py in xs(self, key, axis, level, drop_level)
   3728 
   3729         if axis == 1:
-> 3730             return self[key]
   3731 
   3732         self._consolidate_inplace()

C:\Program Files\ArcGIS\Pro\bin\Python\envs\arcgispro-py3\lib\site-packages\pandas\core\frame.py in __getitem__(self, key)
   2973             if self.columns.nlevels > 1:
   2974                 return self._getitem_multilevel(key)
-> 2975             indexer = self.columns.get_loc(key)
   2976             if is_integer(indexer):
   2977                 indexer = [indexer]

C:\Program Files\ArcGIS\Pro\bin\Python\envs\arcgispro-py3\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
   2890                 return self._engine.get_loc(key)
   2891             except KeyError:
-> 2892                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2893         indexer = self.get_indexer([key], method=method, tolerance=tolerance)
   2894         if indexer.ndim > 1 or indexer.size > 1:

pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 1.5
"
504726591,28872,IntervalIndex: outer intervals closed 'both' should be possible,johannes-mueller,open,2019-10-09T15:27:25Z,2020-04-27T00:19:08Z,"#### The following ipython session demonstrates the issue

```python
import numpy as np
import pandas as pd

# a simple population of integer events
population = np.array([1, 2, 2, 3, 3, 3])

# let's make a histogram of the population
hist, edges = np.histogram(population, 4)

# lets turn the histogram into a dataframe using the edges as IntervalIndex
df = pd.DataFrame(data=hist, index=pd.IntervalIndex.from_breaks(edges))

# The 3 events are indexable properly
print(df.loc[3]) # Works

# To see the event at 1 you need to apply some offset
print(df.loc[1+1e-6]) # Works but ugly

print(df.loc[1]) # fails
```
#### Problem description

The current behavior of `IntervalIndex` is that all the intervals have the same closing. It is however desireable to have an `IntervalIndex` like `[[0, 1], (1, 2], (2, 3]]` including the lower and the upper limit. 

`cut()` solves this by adding an arbitrary offset to the lower limit to make it loo like `[(-0.002, 1], (1, 2], (2, 3]]`. Then the lower interval is left open but the value 0.0 is still contained by the interval. 

This however results in the fact that the intervals contains also values like -0.001 which might be undesired as due to semantics negative values might be impossible. Histograms that allow events in the closed range `[0,1]`, forbidding negative values are impossible to model with a `pandas.IntervalIndex`.

#### Expected behavior.

I'd like to be able to do something like
```
idx = pd.IntervalIndex.from_breaks(np.arange(10))
idx[0].set_closed('both')
"
606675761,33783,DOC: fix doc for crosstab with Categorical data input,GYHHAHA,closed,2020-04-25T03:10:39Z,2020-04-27T02:17:12Z,"Crosstab function should set dropna=False to keep the categories which not appear in the data, but in the DOC, it seems to be inconsistent with the description. Two examples for comparison may be better for users to get this point.

https://pandas.pydata.org/docs/dev/user_guide/reshaping.html#cross-tabulations"
125080620,11968,DataFrame.to_csv convert int to float all the time,channingxiao,closed,2016-01-06T00:02:50Z,2020-04-27T03:08:10Z,"When I use 
test_labels.to_csv(""./data/test_label.csv"", headers=True, index = False)
to write dataframe into csv file, it always convert inteders into float number,(I check the test_labels.dtype, it shows <type ""int"">. How can I let it not convert. Thanks

python 2.7.11, pandas 0.17.1, windows 10
"
598558227,33502,ENH: Make Series.update() use objects coercible to Series,dericke,closed,2020-04-12T19:48:56Z,2020-04-27T04:32:35Z,"- [x] closes #33215 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Allows `Series.update()` to accept objects that can be coerced into a `Series`, like a `dict`. This mirrors the existing behavior of `DataFrame.update()`."
556612443,31410,Cross tabulations for categorical data doesn't work as the way expected in the guide,GYHHAHA,closed,2020-01-29T03:20:22Z,2020-04-27T05:33:25Z,"#### Code Sample, a copy-pastable example if possible

```python
In [73]: foo = pd.Categorical(['a', 'b'], categories=['a', 'b', 'c'])
In [74]: bar = pd.Categorical(['d', 'e'], categories=['d', 'e', 'f'])
In [75]: pd.crosstab(foo, bar)
Out[75]: 
col_0  d  e
row_0      
a      1  0
b      0  1
```
#### Problem description

In the latest [user guide](https://dev.pandas.io/docs/user_guide/reshaping.html#cross-tabulations), it says ""Any input passed containing Categorical data will have all of its categories included in the cross-tabulation, even if the actual data does not contain any instances of a particular category."". But why the example doesn't work like this way? (lack of the row 'c' and column 'f')
Thanks !"
604219835,33706,TYP: remove #type: ignore for pd.array constructor,simonjayhawkins,closed,2020-04-21T19:13:24Z,2020-04-27T07:29:24Z,"pandas\core\arrays\categorical.py:481: error: Argument 1 to ""array"" has incompatible type ""Categorical""; expected ""Sequence[object]"""
545153078,30659,TYP: check_untyped_defs io.sas.sasreader,simonjayhawkins,closed,2020-01-03T20:37:01Z,2020-04-27T08:27:24Z,"
pandas\io\sas\sasreader.py:75: error: Incompatible types in assignment (expression has type ""SAS7BDATReader"", variable has type ""XportReader"")"
556104631,31384,TYP: pandas/core/dtypes/dtypes.py,simonjayhawkins,closed,2020-01-28T10:09:36Z,2020-04-27T09:01:00Z,
509670782,29122,Make color validation more forgiving,AllenDowney,closed,2019-10-20T23:48:28Z,2020-04-27T15:34:39Z,"The current version throws a false positive if the style string contains `s` or `o`, which are valid marker styles and not colors.  For example:

```
style='s', color='C3'
```

should be legal, but currently throws an error.

My suggestion is to check for only the letters that are color codes.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
15675143,3942,Add key to sorting functions,hayd,closed,2013-06-18T08:30:02Z,2020-04-27T16:09:26Z,"Many python functions  (sorting, max/min) accept a key argument, perhaps they could in pandas too.

.

_The terrible motivating example was this awful hack from [this question](http://stackoverflow.com/a/17157110/1240268).... for which maybe one could do_

```
df.sort_index(key=lambda t: literal_eval(t[1:-1]))
```

_This would still be an awful awful hack, but a slightly less awful one_.
"
464410360,27237,ENH: Added key option to df/series.sort_values(key=...) and df/series.sort_index(key=...) sorting,jacobaustin123,closed,2019-07-04T23:04:26Z,2020-04-27T18:03:29Z,"- [x] closes #3942
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Added a key parameter to the DataFrame/Series.sort_values and sort_index functions matching Python sorted semantics for allowing custom sorting orders. Address open issue https://github.com/pandas-dev/pandas/issues/3942. 
"
604738086,33721,BUG: Exceptions not propagated from c_is_list_like in lib.pyx,shwina,closed,2020-04-22T12:59:57Z,2020-04-27T18:58:07Z,"Python exceptions originating in `c_is_list_like()` are not propagated as the function doesn't declare an exception value (`except -1`).

https://github.com/pandas-dev/pandas/blob/master/pandas/_libs/lib.pyx#L988"
604770932,33723,BUG: Propagate Python exceptions from c_is_list_like (#33721),shwina,closed,2020-04-22T13:45:47Z,2020-04-27T18:58:11Z,"- [x] closes #33721
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
585080057,32865,BUG: rolling window functions don't support custom indexers,AlexKirko,closed,2020-03-20T13:45:34Z,2020-04-27T18:59:43Z,"#### Code Sample, a copy-pastable example if possible

```python
class ForwardIndexer(BaseIndexer):
    
    def get_window_bounds(self, num_values, min_periods, center, closed):
        start = np.empty(num_values, dtype=np.int64)
        end = np.empty(num_values, dtype=np.int64)
        for i in range(num_values):
            if i + min_periods <= num_values:
                start[i] = i
                end[i] = min(i + self.window_size, num_values)
            else:
                start[i] = i
                end[i] = i + 1
        return start, end

x = pd.DataFrame({""a"": [1,2,3,4,5,6,7,8,9]})

rolling = x[""a""].rolling(ForwardIndexer(window_size=3), min_periods=2)

result = rolling.min()
result

OUT:
0    0.0
1    0.0
2    1.0
3    2.0
4    3.0
5    4.0
6    5.0
7    7.0
8    NaN
Name: a, dtype: float64

IN:
expected = rolling.apply(lambda x: min(x))
expected

OUT:
0    1.0
1    2.0
2    3.0
3    4.0
4    5.0
5    6.0
6    7.0
7    8.0
8    NaN
Name: a, dtype: float64
```
#### Problem description

We state [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html) that we support supplying a custom Indexer when building a `pandas.DataFrame.rolling` object. While the object does get built, and it returns the correct windows, it doesn't support many rolling window functions. The problem is that our implementations of these aggregation functions expect a standard backward-looking window and we support centered windows via a bit of a crutch.

For example, `rolling.min` eventually falls through to `_roll_min_max_variable` in `aggregations.pyx`, which uses [this bit of code](https://github.com/pandas-dev/pandas/blob/6812842d3653e93f9191a59446b1b4b19c77c428/pandas/_libs/window/aggregations.pyx#L1091) to record the output:

```python
        for i in range(endi[0], endi[N-1]):
            if not Q.empty() and curr_win_size > 0:
                output[i-1+close_offset] = calc_mm(
                    minp, nobs, values[Q.front()])
            else:
                output[i-1+close_offset] = NaN
```
This indexing of output means that the window minimum gets written near the end of the window, even if the window is forward-looking. I've investigated a bit, and there is a similar issue in `rolling.std` - it also isn't adapted to more flexible rolling windows.

While it's not possible to make rolling window aggregation functions completely universal without loss of efficiency, it's possible to adapt them to most useful cases: forward-looking, smoothly contracting and expanding. We'd still have to think on how we would check that we support a custom Indexer, and whether we would check at all. It might be possible to just specify the supported kinds in the docs and throw a warning or do something similar.

If we choose this path, I'd be happy to deal with the problem over a series of PRs or share the load with someone. Looks like a fair bit of work, but the pandemic freed up a lot of time.

#### Expected Output

```
OUT:
0    1.0
1    2.0
2    3.0
3    4.0
4    5.0
5    6.0
6    7.0
7    8.0
8    NaN
Name: a, dtype: float64
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d308712c8edef078524b8a65df7cb74e9019218e
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : ru_RU.UTF-8
LOCALE           : None.None

pandas           : 0.26.0.dev0+2635.gd308712c8
numpy            : 1.17.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 44.0.0.post20200106
Cython           : 0.29.14
pytest           : 5.3.4
hypothesis       : 5.2.0
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.3.1
sqlalchemy       : 1.3.12
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.47.0
</details>
"
607643642,33822,REF: collect set_index tests,jbrockmendel,closed,2020-04-27T15:31:46Z,2020-04-27T19:35:36Z,
607242021,33816,ERR: Add NumbaUtilError,mroeschke,closed,2020-04-27T05:18:54Z,2020-04-27T20:46:12Z,"- xref https://github.com/pandas-dev/pandas/pull/33388#discussion_r415172278
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
607352265,33817,TST: Separate expanding_apply and rolling_apply out of consistency,charlesdong1991,closed,2020-04-27T08:40:49Z,2020-04-27T20:56:39Z,"xref #30577 #30486 

This is the second step for breaking up giant consistency test, this PR separates consistency tests about `expanding_apply` and `rolling_apply` out from their big `expanding_consistency` and `rolling_consistency` tests.

The followup after this would be removing `self.data` initiation from `init` and convert to either fixture or funcs with yields so as to gradually move tests out of the whole consistency class and easier for parametrization instead of current looping.

cc @jreback 
"
607710436,33824,"BUG: DataFrame[object] + Series[dt64], test parametrization",jbrockmendel,closed,2020-04-27T17:09:45Z,2020-04-27T21:33:04Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

In the course of parametrizing tests, found a failing case.  The edit in ops.dispatch fixes it."
598601849,33507,CI: Numpy Dev Build Failing,alimcmaster1,closed,2020-04-13T00:17:50Z,2020-04-27T23:10:34Z,"Looks like our numpy dev build also uses a pre-release (alpha) of cython.

Is this the intended behaviour?

Think this is potentially what is causing the test failures:
https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=33245&view=logs&j=3a03f79d-0b41-5610-1aa4-b4a014d0bc70&t=4d05ed0e-1ed3-5bff-dd63-1e957f2766a9 

```
attrs                     19.3.0                     py_0  
ca-certificates           2020.1.1                      0  
certifi                   2020.4.5.1               py37_0  
**cython                    3.0a1                    pypi_0    pypi**
execnet                   1.7.1                      py_0  
hypothesis                5.5.4                      py_0  
importlib_metadata        1.5.0                    py37_0  
ld_impl_linux-64          2.33.1               h53a641e_7  
libedit                   3.1.20181209         hc058e9b_0  
libffi                    3.2.1                hd88cf55_4  
libgcc-ng                 9.1.0                hdf63c60_0  
libstdcxx-ng              9.1.0                hdf63c60_0  
more-itertools            8.2.0                      py_0  
ncurses                   6.2                  he6710b0_0  
numpy                     1.19.0.dev0+8f7adad          pypi_0    pypi
openssl                   1.1.1f               h7b6447c_0  
packaging                 20.3                       py_0  
pandas                    1.1.0.dev0+1243.gd3ff12cff           dev_0    <develop>
pip                       20.0.2                   py37_1  
pluggy                    0.13.1                   py37_0  
py                        1.8.1                      py_0  
pyparsing                 2.4.6                      py_0  
pytest                    5.4.1                    py37_0  
pytest-azurepipelines     0.8.0                      py_0  
pytest-forked             1.1.3                      py_0  
pytest-xdist              1.31.0                     py_0  
python                    3.7.7           hcf32534_0_cpython  
**python-dateutil           2.8.2.dev25+gc175137          pypi_0    pypi**
pytz                      2019.3                     py_0  
readline                  8.0                  h7b6447c_0  
scipy                     1.5.0.dev0+f614064          pypi_0    pypi
setuptools                46.1.3                   py37_0  
six                       1.14.0                   py37_0  
sortedcontainers          2.1.0                    py37_0  
sqlite                    3.31.1               h7b6447c_0  
tk                        8.6.8                hbc83047_0  
wcwidth                   0.1.9                      py_0  
wheel                     0.34.2                   py37_0  
xz                        5.2.4                h14c3975_4  
zipp                      2.2.0                      py_0  
zlib                      1.2.11               h7b6447c_3
```"
607887177,33834,BUG: TimedeltaIndex[:] losing freq,jbrockmendel,closed,2020-04-27T22:08:46Z,2020-04-27T23:46:56Z,Reverts the freq-override I implemented a few days ago.  That was made unnecessary by the more-recent change to _with_freq that ensures that self.freq and self._data.freq stay in sync.
605923041,33755,CI: Revert Cython Pin (take 2),alimcmaster1,closed,2020-04-23T22:49:55Z,2020-04-27T23:47:08Z,"- [x] closes #33507

reverts #33534 

New Cython pre release available today - https://pypi.org/project/Cython/3.0a2/

cc @WillAyd 

Update: 3.0a2 (no such luck - seems there is a regression which is being looked at)

https://mail.python.org/pipermail/cython-devel/2020-April/005340.html

https://github.com/cython/cython/issues/3544

Update now trying 3.0a3 "
606889155,33798,IO: Fix feather s3 and http paths,alimcmaster1,closed,2020-04-26T01:01:29Z,2020-04-27T23:47:23Z,"- [x] closes #29055
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
607877009,33833,DOC/CLN: Fix whatsnew typo,dsaxton,closed,2020-04-27T21:49:32Z,2020-04-27T23:51:43Z,Should something like this cause a CI failure?
606949750,33803,BUG:  .corr() fails when input is Int64Dtype(),akanz1,closed,2020-04-26T08:02:46Z,2020-04-27T23:53:59Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] This bug exists on the master branch of pandas. [see below](https://github.com/pandas-dev/pandas/issues/33803#issuecomment-619574491)

---

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame({'First':[1,2,3,4,np.nan],
                   'Second':[True, True, False, False, True],
                   'Third':[2,3,8,5,6]})
print(df.dtypes)
print(df.corr())
df
```
```
df_conv = df.convert_dtypes()
print(df_conv.dtypes)
print(df_conv)
df_conv.corr() # results in the following TypeError

TypeError: float() argument must be a string or a number, not 'NAType'
```

#### Problem description

When applying convert_dtypes() to a DataFrame, columns containing NaNs (np.nan) and integer values (stored as float64) are converted to ""pd.NA"" and actual integers (Int64). While this is desirable, it results in an error when calling the .corr() method on the DataFrame with the new dtypes.

**Before convert_dtypes():**
![before_conv](https://user-images.githubusercontent.com/51492342/80301427-5c3b5500-87a4-11ea-87fe-e3727c240a90.png)

**After convert_dtypes():**
![after_conv](https://user-images.githubusercontent.com/51492342/80301430-60677280-87a4-11ea-898c-7d0c9352edc9.png)

**Error message:**
![corr_error](https://user-images.githubusercontent.com/51492342/80301434-652c2680-87a4-11ea-94a9-7dd9b1a0c7f5.png)


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200325
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
607955609,33837,teaching someone this,cesarvargas00,closed,2020-04-28T01:23:36Z,2020-04-28T01:24:41Z,"sorry guys, i was teaching my friend... Dont ban me please."
588615828,33046,"PERF/DISC: Cache .loc, .at and reset whenever index/columns change",jbrockmendel,closed,2020-03-26T17:59:46Z,2020-04-28T03:30:40Z,"Series.index, DataFrame.index, and DataFrame.columns are `AxisProperty`s and whenever they are set, a callback updates the underlying BlockManager.

We could have that same callback also invalidate the cached `obj.loc` and `obj.at` objects.

If we did that, then `LocIndexer` and `AtIndexer` could assume that they would only ever see set of axes, so a bunch of checks that are now done at lookup-time could be cached or otherwise de-duplicated.

No idea what the perf impact would look like.  Thoughts?"
242890852,16918,ENH: json_normalize() avoid loss of precision for int64 with missing values,jzwinck,closed,2017-07-14T03:55:42Z,2020-04-28T05:52:00Z,"This code:
```python
x = 1234567890123456789
x - pd.io.json.json_normalize([{'x': x}, {}]).loc[0, 'x'].astype(int)
```
Gives `21`, when reasonable users might expect it to give `0`.

This inaccuracy occurs when one field has an int64 value that is not present in all records, triggering a conversion of the Series dtype to float64.  Of course, Pandas does this conversion so that it can put NAN where no value exists.

One solution could be to add a `fill_value` parameter, as seen in `add()`, `unstack()`, and other Pandas functions.  It would be good for this to support a dict as well as a single value, in case different fill values are required for different columns.

The usage might be like this:

```python
pd.io.json.json_normalize([{'x': x}, {}], fill_value=-1).x
# or
pd.io.json.json_normalize([{'x': x}, {}], fill_value={'x': -1}).x
```
Then instead of the current result:
```
0    1.234568e+18
1             NaN
Name: x, dtype: float64
```
The result would be:
```
0    1234567890123456789
1                     -1
Name: x, dtype: int64
```

I'm using Pandas 0.20.1.
"
554915701,31293,ENH: investigate using a bitarray as the mask in the nullable/masked ExtensionArrays,jorisvandenbossche,open,2020-01-24T19:35:20Z,2020-04-28T06:03:02Z,"Currently, our nullable / masked extension arrays (boolean, integer, for now) are using a numpy boolean array as their `_mask` to keep track of missing values. A potential route for improving memory and performance would be using a bitarray instead of a boolean numpy array (which is a byte per value).

This should require some exploration: what are options how to implement this? (existing libraries, custom implementation) What is the performance impact? (some things like masking will also be slower, since we still rely on numpy for that, which needs boolean arrays) Is this worth it to do a custom implementation rather than using pyarrow for this? etc"
589061346,33066,BUG: pd.NA acts differently when inside/outside a series/dataframe with object dtype,HYChou0515,open,2020-03-27T11:16:13Z,2020-04-28T06:09:32Z,"#### Code Sample, a copy-pastable example if possible

```python
# return pd.NA, cool
pd.NA | pd.NA

# return False, not cool
pd.Series([pd.NA]) | pd.Series([pd.NA])
```
#### Problem description

pd.NA acts differently when inside/outside a series/dataframe may be confusing. It force me to handle each entry of a series/dataframe.


#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-91-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None


</details>
"
608107436,33841,BUG:RecursionError occur when running the script which belongs to other directories,Levr4,closed,2020-04-28T07:59:09Z,2020-04-28T08:22:49Z,"This is my first issue. So, if I'm wrong, please teach me to be kind!

I cannot import pandas when running the script which belongs to other directories.

```python
# workdir/src/sample.py
import numpy as np
import pandas as pd

```
```bash
/workdir$ python src/sample.py
```
```
Traceback (most recent call last):
  File ""src/sample.py"", line 2, in <module>
    import pandas as pd
  File ""/usr/local/share/pip-global/pandas/__init__.py"", line 55, in <module>
    from pandas.core.api import (
  File ""/usr/local/share/pip-global/pandas/core/api.py"", line 5, in <module>
    from pandas.core.arrays.integer import (
  File ""/usr/local/share/pip-global/pandas/core/arrays/__init__.py"", line 7, in <module>
    from .categorical import Categorical  # noqa: F401
  File ""/usr/local/share/pip-global/pandas/core/arrays/categorical.py"", line 54, in <module>
    from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs
  File ""/usr/local/share/pip-global/pandas/core/base.py"", line 36, in <module>
    import pandas.core.nanops as nanops
  File ""/usr/local/share/pip-global/pandas/core/nanops.py"", line 38, in <module>
    bn = import_optional_dependency(""bottleneck"", raise_on_missing=False, on_version=""warn"")
  File ""/usr/local/share/pip-global/pandas/compat/_optional.py"", line 90, in import_optional_dependency
    module = importlib.import_module(name)
  File ""/opt/conda/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/opt/conda/lib/python3.7/site-packages/bottleneck/__init__.py"", line 41, in <module>
    from numpy.testing import Tester
  File ""/usr/local/share/pip-global/numpy/testing/__init__.py"", line 10, in <module>
    from unittest import TestCase
  File ""/opt/conda/lib/python3.7/unittest/__init__.py"", line 59, in <module>
    from .case import (TestCase, FunctionTestCase, SkipTest, skip, skipIf,
  File ""/opt/conda/lib/python3.7/unittest/case.py"", line 6, in <module>
    import logging
  File ""/workdir/src/logging.py"", line 5, in <module>
    from lightgbm.callback import _format_eval_result
  File ""/usr/local/share/pip-global/lightgbm/__init__.py"", line 8, in <module>
    from .basic import Booster, Dataset
  File ""/usr/local/share/pip-global/lightgbm/basic.py"", line 12, in <module>
    import scipy.sparse
  File ""/usr/local/share/pip-global/scipy/sparse/__init__.py"", line 229, in <module>
    from .base import *
  File ""/usr/local/share/pip-global/scipy/sparse/base.py"", line 7, in <module>
    from scipy._lib._numpy_compat import broadcast_to
  File ""/usr/local/share/pip-global/scipy/_lib/_numpy_compat.py"", line 16, in <module>
    _assert_warns = np.testing.assert_warns
  File ""/usr/local/share/pip-global/numpy/__init__.py"", line 213, in __getattr__
    import numpy.testing as testing
  File ""/usr/local/share/pip-global/numpy/__init__.py"", line 213, in __getattr__
    import numpy.testing as testing
  File ""/usr/local/share/pip-global/numpy/__init__.py"", line 213, in __getattr__
    import numpy.testing as testing
  [Previous line repeated 871 more times]
  File ""<frozen importlib._bootstrap>"", line 200, in _lock_unlock_module
  File ""<frozen importlib._bootstrap>"", line 163, in _get_module_lock
RecursionError: maximum recursion depth exceeded while calling a Python object
```


#### Output of ``pd.show_versions()``
'1.0.3'
"
607872956,33831,DOC/CLN: remove outdated warnings in enhancingperf.rst,simonjayhawkins,closed,2020-04-27T21:41:58Z,2020-04-28T08:58:20Z,xref #16732 and SyntaxError
607418494,33818,CLN/TYP: update setup.cfg,simonjayhawkins,closed,2020-04-27T10:11:09Z,2020-04-28T10:52:47Z,"```
@pytest.mark.xfail(	
    StrictVersion(dateutil.__version__.split("".dev"")[0]) < StrictVersion(""2.7.0""),	
    reason=""Bug in dateutil < 2.7.0 when parsing old dates: Period('0001-01-07', 'D')"",	
    strict=False,	
)
```
removed in #33680"
607761610,33827,BUG: pandas.io.common _stringify_path uses isinstance in a wrong way raising an exception,Fjolnir-Dvorak,closed,2020-04-27T18:34:07Z,2020-04-28T11:49:36Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# pandas.io.common._stringify_path (line 150) uses pathlib.Path in an
# isinstance check but misses a wrapping type()
# broken:
isinstance(""dummyvalue"", pahtlib.Path)
# working fix:
isinstance(""dummyvalue"", type(pathlib.Path)
```

#### Problem description

The current implementation raises following exception: 
TypeError: isinstance() arg 2 must be a type or tuple of types

Tested in the latest python 3.7 and 3.8 releases

#### Expected Output

#### Output of ``pd.show_versions()``

<details>
import pandas as pd
pd.show_versions()
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.33-3-MANJARO
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8
pandas           : 1.0.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.11.1
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.3.3
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.16
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0


</details>
"
576380396,32466,Should Whitespaces be placed at the begging of a line,ShaharNaveh,closed,2020-03-05T16:31:41Z,2020-04-28T14:48:42Z,"As of our current [code style guide](https://dev.pandas.io/docs/development/code_style.html) we are placing whitespaces at the begging of the lines, [Black](https://github.com/psf/black) had a discussion at https://github.com/psf/black/issues/182 (https://github.com/psf/black/pull/1132#issuecomment-594062628 sums all of that discussion) where they decided that when ""Black"" wraps long strings it will place the whitespace at the begging of the line.

I think that we should reconsider/open for discussion the placement of whitespaces over long strings."
456591741,26873,STY: rename `filter` kwarg in `replace` methods,jbrockmendel,closed,2019-06-16T01:38:58Z,2020-04-28T16:50:50Z,"Overlaps with builtin `filter`.

Could use e.g. https://pypi.org/project/flake8-builtins/ to check for other cases like this, xref #22122"
586521949,32951,CI: Exception ignored in: 'pandas._libs.tslibs.util.get_c_string_buf_and_size',jbrockmendel,closed,2020-03-23T21:13:15Z,2020-04-28T19:40:32Z,"Seen in the logs lately:

```
Exception ignored in: 'pandas._libs.tslibs.util.get_c_string_buf_and_size'
UnicodeEncodeError: 'utf-8' codec can't encode character '\ud83d' in position 0: surrogates not allowed
```

Possibly related to #32701?
"
459370340,26993,xfail test_missing_required_dependency test,TomAugspurger,closed,2019-06-21T20:58:02Z,2020-04-28T20:42:11Z,See https://github.com/MacPython/pandas-wheels/pull/50. I can't get this test to work reliably.
591997838,33210,MacPython/pandas-wheels wheel builds failing,TomAugspurger,closed,2020-04-01T15:19:08Z,2020-04-28T22:54:21Z,"This affects the linux builds, both py37 and py38, 32 or 64-bit.

https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=30996&view=logs&j=517fe804-fa30-5dc2-1413-330699242c05&t=2e128ad5-2f7f-5333-3f34-c85b8fbc7250&l=1216

```
=================================== FAILURES ===================================
__________________________ test_no_permission[c_high] __________________________
[gw1] linux -- Python 3.7.0 /venv/bin/python

            with pytest.raises(PermissionError, match=msg) as e:
>               parser.read_csv(path)

/venv/lib/python3.7/site-packages/pandas/tests/io/parser/test_common.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/venv/lib/python3.7/site-packages/pandas/tests/io/parser/conftest.py:22: in read_csv
    return read_csv(*args, **kwargs)
/venv/lib/python3.7/site-packages/pandas/io/parsers.py:676: in parser_f
    return _read(filepath_or_buffer, kwds)
/venv/lib/python3.7/site-packages/pandas/io/parsers.py:449: in _read
    parser = TextFileReader(fp_or_buf, **kwds)
/venv/lib/python3.7/site-packages/pandas/io/parsers.py:880: in __init__
    self._make_engine(self.engine)
/venv/lib/python3.7/site-packages/pandas/io/parsers.py:1112: in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
/venv/lib/python3.7/site-packages/pandas/io/parsers.py:1935: in __init__
    self._reader = parsers.TextReader(src, **kwds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   pandas.errors.EmptyDataError: No columns to parse from file

pandas/_libs/parsers.pyx:532: EmptyDataError

```

Looks to be from https://github.com/pandas-dev/pandas/pull/32839

cc @gfyoung @roberthdevries "
424403835,25839,"df.to_hdf5() in table append mode very slow with data_columns=True, append=True, index=False ",dragoljub,open,2019-03-22T21:19:16Z,2020-04-29T02:02:37Z,"#### Code Sample, a copy-pastable example if possible

```python
import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.randint(123,456, size=(300,225)), columns=['COL'+str(i) for i in range(225)])
df.loc[:,:'COL10'] = df.loc[:,:'COL10'].astype(str)
df.loc[:,'COL11':'COL20'] = df.loc[:,'COL11':'COL20'].astype(float)

df.to_hdf(r'test.h5', 'df', mode='w', data_columns=True, append=True, index=False)
# <-- Quite Fast on first run creation of the table.

df.to_hdf(r'test.h5', 'df', mode='a', data_columns=True, append=True, index=False)
# <-- 3.6 Minutes for 2nd run trying to append to the table. Note index=False

```
#### Problem description

``df.to_hdf()``  In Append Table mode used to work very quickly when you set each column as a data column and want _no_ index created. ``data_columns=True, append=True, index=False``. On the new version of Pandas and PyTables it seems to be taking a very long to append additional rows.

#### Expected Output

Appending HDF5 Table rows  with ''data_columns=True, append=True, index=False'' should create all columns as data columns, but not create an index and should be quite fast.


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.1.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.1
pytest: 4.0.0
pip: 19.0.3
setuptools: 40.6.2
Cython: 0.29
numpy: 1.15.4
scipy: 1.1.0
pyarrow: 0.12.0
xarray: 0.11.0
IPython: 7.1.1
sphinx: 1.8.2
patsy: 0.5.1
dateutil: 2.7.5
pytz: 2018.7
blosc: 1.6.2
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.8
feather: None
matplotlib: 3.0.2
openpyxl: 2.5.10
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: None
lxml.etree: 4.2.5
bs4: 4.6.3
html5lib: 1.0.1
sqlalchemy: 1.2.14
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: 0.1.6
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
387310713,24090,TST: Work around statsmodels bug,TomAugspurger,closed,2018-12-04T14:25:25Z,2020-04-29T02:34:41Z,Closes https://github.com/pandas-dev/pandas/issues/24088
525324098,29725,DEPR: remove Index fastpath kwarg,jbrockmendel,closed,2019-11-19T23:17:20Z,2020-04-29T02:55:05Z,
608057209,33840,DOC: Fix groupby.agg/transform API reference and add numba arguments to documentation,mroeschke,closed,2020-04-28T06:22:51Z,2020-04-29T03:42:13Z,"- xref https://github.com/pandas-dev/pandas/pull/33388#issuecomment-618808443
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

<img width=""927"" alt=""Screen Shot 2020-04-27 at 11 19 16 PM"" src=""https://user-images.githubusercontent.com/10647082/80453833-e20eeb80-88dd-11ea-9acc-ef322cb59df5.png"">
<img width=""934"" alt=""Screen Shot 2020-04-27 at 11 19 33 PM"" src=""https://user-images.githubusercontent.com/10647082/80453838-e4714580-88dd-11ea-85b2-735d327ee686.png"">
<img width=""922"" alt=""Screen Shot 2020-04-27 at 11 19 53 PM"" src=""https://user-images.githubusercontent.com/10647082/80453844-e6d39f80-88dd-11ea-8c49-a9d35fd80e0c.png"">
<img width=""927"" alt=""Screen Shot 2020-04-27 at 11 20 16 PM"" src=""https://user-images.githubusercontent.com/10647082/80453851-e935f980-88dd-11ea-8c62-b92043ed26e4.png"">
"
608852104,33861,DOC: Fix typo in inferred_freq docstring,taljaards,closed,2020-04-29T07:52:45Z,2020-04-29T09:32:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
608526501,33852,DOC: Code Standards Broken Link,Stockfoot,closed,2020-04-28T18:32:02Z,2020-04-29T12:15:02Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#code-standards
![image](https://user-images.githubusercontent.com/36280534/80523857-f4277300-894b-11ea-9e9c-0ae0b2b8ed77.png)


#### Documentation problem

There is a broken link on Development -> Contributing To Pandas -> Contributing To The Code Base -> Code Standards -> ""pandas code style guide""

Link location goes to 404 Error at
https://pandas.pydata.org/pandas-docs/stable/development/code_style

#### Suggested fix for documentation

Change link address to proper page/document
"
608975195,33865,DOC: Code Standards Broken Link,simonjayhawkins,closed,2020-04-29T11:18:29Z,2020-04-29T12:16:22Z,"- [ ] closes #33852
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
607929142,33835,REF/CLN: Parametrize _isna,dsaxton,closed,2020-04-27T23:59:50Z,2020-04-29T12:38:28Z,Replaces two nearly identical isna functions with one by parametrizing over the difference
608755787,33859,TST: stop skipping Categorical take tests,jbrockmendel,closed,2020-04-29T03:41:36Z,2020-04-29T14:43:00Z,
528848001,29862,CI: Fix version openpyxl,charlesdong1991,closed,2019-11-26T16:47:04Z,2020-04-29T16:50:45Z,"- [ ] closes #29854 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
608961413,33864,TYP/CLN: remove #type: ignore from pandas\tests\base\test_conversion.py,simonjayhawkins,closed,2020-04-29T10:56:21Z,2020-04-29T17:15:06Z,
607753964,33826,ENH: add support for snake case in pandas.Series.str.title,jeffolsi,closed,2020-04-27T18:20:59Z,2020-04-29T19:13:35Z,"add support for snake case in `pandas.Series.str.title`

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.title.html

so that :
s = pd.Series(['snake', 'CAPITALS', 'IdNumber'])

will be:
```
s
0                 snake
1              capitals
2              id_number
dtype: object
```

This is very much needed when we want to create database columns and they can't have capital letters. lower() isn't good as well because `nextExecutionDateForYearlyBatch` will be impossible to read : `nextexecutiondateforyearlybatch`

Basically I'm looking to do this:
```
import pandas as pd
import janitor
df = pd.DataFrame(...).clean_names()
```
```
Columns before: First Name, Last Name, Employee Status, Subject
Columns after: first_name, last_name, employee_status, subject
```
https://pyjanitor.readthedocs.io/reference/janitor.functions/janitor.clean_names.html
within the `pandas` domain without the need for `janitor` package."
608754618,33858,CLN: address FIXME/TODO/XXX comments,jbrockmendel,closed,2020-04-29T03:37:23Z,2020-04-29T19:14:11Z,"- changed XXX comments to TODO (much more grep-friendly, since we sometimes use XXX for a `name` attribute.
- Removed some now-fixed FIXME/TODO comments
  - tests.arithmetic.test_datetime64, tests.arithmetic.test_period, test_conversion, tests.indexes.common, test_date_range, test_css, scalar.timedelta.test_arithmetic
- Added some TODO(EA2D) comments
- Removed some `TODO(wesm): unused?` comments"
608604057,33854,"TST: de-xfail, remove strict=False",jbrockmendel,closed,2020-04-28T20:45:13Z,2020-04-29T19:52:22Z,
608002609,33839,CLN: freq inference in DTI/TDI set ops,jbrockmendel,closed,2020-04-28T03:46:04Z,2020-04-29T20:52:51Z,"These ops are a PITA for getting the freq check into assert_index_equal.  I'm leaning towards ripping out the freq=""infer"" behavior in all the cases where we can't fast-infer it."
608678191,33855,TST: fix xfailed arithmetic tests,jbrockmendel,closed,2020-04-28T23:27:31Z,2020-04-29T20:56:07Z,
609248205,33874,TST: Fix xfailed offset test,jbrockmendel,closed,2020-04-29T18:00:01Z,2020-04-29T22:32:35Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
609229684,33873,TST: parametrize test_partial_slicing tests,jbrockmendel,closed,2020-04-29T17:29:00Z,2020-04-29T22:37:38Z,
609129938,33870,TST/REF: collect Index tests by method,jbrockmendel,closed,2020-04-29T15:03:43Z,2020-04-29T22:38:22Z,xfail tests in test_coercion rather than silently passing
609103583,33869,DOC: Remove deprecated Y and M from to_timedelta,taljaards,closed,2020-04-29T14:29:15Z,2020-04-29T22:48:17Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

[`to_timedelta()`](https://github.com/pandas-dev/pandas/blob/master/pandas/core/tools/timedeltas.py#L84-L88) blocks those arguments anyway, so no reason to include them in the docstring."
589880617,33130,build_table_schema references non-existent _as_json_table_type,mriswithe,closed,2020-03-29T20:39:53Z,2020-04-29T22:49:08Z,"#### Problem description
Documentation issue: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.io.json.build_table_schema.html
(and master branch contains this as well)

Description contains: 
```
See _as_json_table_type for conversion types. 
```
Which I cannot find anywhere else in the documentation

"
606786989,33791,DOC: Link to table schema and remove reference to internal API,bharatr21,closed,2020-04-25T14:59:44Z,2020-04-29T22:49:11Z,"- [x] closes #33130
- [ ] tests added / passed (Not needed)
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff` (Markdown link which will not be shown)
- [ ] whatsnew entry (Not needed)
"
609199534,33871,DOC/CLN: Fix to_numpy docstrings,dsaxton,closed,2020-04-29T16:40:33Z,2020-04-29T23:13:47Z,
589450711,33082,Accessing DataFrame with DatetimeIndex behaves unexpectedly with reverse chronological ordered data,jsdnrs,closed,2020-03-27T22:52:08Z,2020-04-29T23:31:18Z,"#### Problem description

When a Panda's DataFrame with a DatetimeIndex is in reverse chronological order accessing the data using the .loc method does not work unless the range is _also_ in reverse order.

#### Code Sample

```python
import pandas as pd
 
data = [
  (""2020-03-25 12:00:00"", ""A""),
  (""2020-03-24 18:00:00"", ""B""),
  (""2020-03-24 12:00:00"", ""C""),
  (""2020-03-24 06:00:00"", ""D""),
  (""2020-03-23 12:00:00"", ""E""),
]
 
df = pd.DataFrame(data, columns=[""timestamp"", ""x""])
df[""timestamp""] = pd.to_datetime(df[""timestamp""])
df.index = df.pop(""timestamp"")
```

Accessing the date range 2020-03-24 through 2020-03-25 for this DataFrame results in the following:

```python
df.loc[""2020-03-24"":""2020-03-25""]
>>>
Empty DataFrame
Columns: [x]
Index: []
```

However, reversing the date range returns the expected output.

```python 
df.loc[""2020-03-25"":""2020-03-24""]
>>>
                     x
timestamp            
2020-03-25 12:00:00  A
2020-03-24 18:00:00  B
2020-03-24 12:00:00  C
2020-03-24 06:00:00  D
```

Is this really how the DatetimeIndex should function? Either should work when working with a DatetimeIndex.

This can be fixed by executing the .sort_index() method of the DataFrame. However, when working with a DatetimeIndex, I believe chronological order should be implied.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.8.0.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-88-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: 5.4.1
pip: 20.0.2
setuptools: 41.2.0
Cython: None
numpy: 1.18.1
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: 2.4.4
patsy: None
dateutil: 2.8.1
pytz: 2019.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.2.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: 4.8.2
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.11.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
106489118,11100,Passing colors to barplots with subplot=True,alvinwt,closed,2015-09-15T06:25:42Z,2020-04-30T05:49:21Z,"Is there a way to pass a list or dict of colors to a pandas barplot with subplot=True? The colors are used per plot instead of colors for individual bars. I would like to plot bar plots that have a different color per bar. 
"
110822630,11284,Pandas HDF query syntax is more restrictive than PyTables,BrenBarn,open,2015-10-10T23:23:55Z,2020-04-30T05:51:49Z,"With PyTables, you can do queries like `someTable.where(""X**2 + Y**2 < 1"")` (as described [here](http://www.pytables.org/usersguide/condition_syntax.html)).  It seems, though, that pandas imposes its own more restrictive query syntax which allows only a very limited set of query operations.  This is confusing to users who expect to be able to query a Pandas HDFStore in the same way that they would query the underlying PyTables table.

Comments by @jreback on [this Stackoverflow question](http://stackoverflow.com/questions/33056227/arithmetic-in-pandas-hdf5-queries) suggest the pandas query handling is needed to handle complex queries and/or queries involving datetimes.  However, it's rather extreme to block _all_ queries from using complex expressions, even when the same query would work fine on the Pytable itself.  (The SO question gives a simple example of that.)  I suggest the following:
1. The [documentation](http://pandas.pydata.org/pandas-docs/stable/io.html#querying-a-table) should make it very clear that the examples provided there exhaustively describe the types of possible queries.  (If they don't, then we need to come up with a comprehensive explanation of what is allowed.)  They should also document that this query syntax is a subset of what is allowed with Pytables.
2. It would be nice to provide some sort of override flag (or perhaps a separate function) saying ""Just pass this query through to Pytables"", to stop pandas from messing with the query.  There is still work to be done in terms of wrapping the query result in a DataFrame, but this is on the output side and doesn't require modifying the query on the way in.  This might also need to accept the `condvars` argument to pass ad-hoc variables to PyTables for use in queries.

Any thoughts on this?  I'm curious what kinds of queries motivated the creation of this pandas-specific query syntax initially.
"
152028552,13041,secondary_y sequence doesn't work,ohadle,open,2016-04-30T12:42:50Z,2020-04-30T05:52:56Z,"#### Code Sample, a copy-pastable example if possible

```
import pandas
df = pandas.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9], 'd': [10, 11, 12]})
df.plot(y=['a', 'b'], secondary_y=['c', 'd'])
```

I only get the primary axis results:
![figure_1](https://cloud.githubusercontent.com/assets/2196196/14935868/cd987ce2-0ee9-11e6-9a89-f1459e38630b.png)
Others seem to be using this - am I missing something?
#### output of `pd.show_versions()`

```
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.11.final.0
python-bits: 64
OS: Darwin
OS-release: 15.4.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: en_US.UTF-8
LANG: en_US.UTF-8

pandas: 0.18.0
nose: 1.3.7
pip: 8.1.1
setuptools: 20.7.0
Cython: 0.24
numpy: 1.10.4
scipy: 0.17.0
statsmodels: 0.6.1
xarray: None
IPython: 4.2.0
sphinx: 1.4.1
patsy: 0.4.1
dateutil: 2.5.2
pytz: 2016.3
blosc: None
bottleneck: 1.0.0
tables: 3.2.2
numexpr: 2.5.2
matplotlib: 1.5.1
openpyxl: 2.3.2
xlrd: 0.9.4
xlwt: 1.0.0
xlsxwriter: 0.8.5
lxml: 3.6.0
bs4: 4.4.1
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 1.0.12
pymysql: None
psycopg2: 2.6.1 (dt dec pq3 ext)
jinja2: 2.8
boto: 2.39.0
```
"
209433080,15474,VIS/BUG DatetimeIndex with freq=None plot return rotated xtickslabels,weber-s,closed,2017-02-22T11:57:15Z,2020-04-30T06:00:36Z,"#### Code Sample

```python
file = 'test.csv' #with a ""date"" column
df1  = pd.read_csv(file, index_col=""date"", parse_dates=[""date""])
df1.plot()

idx =pd.date_range('2000-01-01','2003-01-01', freq=""6d"")
df2 = pd.DataFrame(index=idx, data=np.random.randn(len(idx),1))
df2.plot()
```

#### Problem description

When plotting a time indexed series, we expect no rotation in x labels. It does not work when importing data with 
```
df  = pd.read_csv(file, index_col=""date"", parse_dates=[""date""])
```
where the rotation occurs. Moreover, set ```rot=0``` gives a strange behavior with label not centered on ticks.

The only difference is that ```freq=None``` in the first case.

#### Expected Output
No rotation by default.

Here is what we have for ```df1.plot()``` (Problem) : 
![df1](https://cloud.githubusercontent.com/assets/25247745/23210539/4e568686-f8fe-11e6-89b7-a71417d5ac37.png)

Here is what we have for ```df1.plot(rot=0)``` (Problem) : 
![df1rot](https://cloud.githubusercontent.com/assets/25247745/23210548/56dcf8da-f8fe-11e6-81b8-3aca0464ffdb.png)

Here is what we have for ```df2.plot()``` (Ok) : 
![df2](https://cloud.githubusercontent.com/assets/25247745/23210373/8bcb829c-f8fd-11e6-9b1b-ffc99d8d223a.png)


### Details
May be related to : 
 
- https://github.com/pandas-dev/pandas/issues/8150
- https://github.com/pandas-dev/pandas/issues/15025


<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.0.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.9-1-ARCH
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: fr_FR.UTF-8
LOCALE: fr_FR.UTF-8

pandas: 0.19.2
nose: None
pip: None
setuptools: 34.2.0
Cython: None
numpy: 1.12.0
scipy: 0.18.1
statsmodels: None
xarray: None
IPython: 5.1.0
sphinx: None
patsy: None
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: 2.0.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
httplib2: 0.10.3
apiclient: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.9.5
boto: None
pandas_datareader: None

</details>
"
242815565,16911,pd.io.formats.format.DataFrameFormatter issue: columns not truncating properly,bsolomon1124,open,2017-07-13T20:08:01Z,2020-04-30T06:01:55Z,"In several IDEs, DataFrames with long column _names_ (not necessarily a large _number_ of columns) do not seem to truncate properly.  I originally [posted](https://stackoverflow.com/questions/45043968/pandas-display-truncate-column-display-rather-than-wrapping) this in SO thinking it was a `pd.set_option` that I had ignored, but one answer pointed out that it may be an issue of `pd.io.formats.format.DataFrameFormatter` checking `max_cols` against the number of columns, not the total width of the columns, in deciding whether to truncate.


#### Problem description
I would like to _keep_
`pd.set_option('expand_frame_repr', False)`
but still truncate the view of DataFrames as shown in ""Expected Output"" below.  I've noticed that this seems to be dependent on the _length_ of columns rather than number of columns.  For instance, this df displays in a readable way:

```
df = pd.DataFrame(np.random.randn(1000, 1000),
                  columns=['col' + str(i) for i in range(1000)])
```

but this one is unreadable:

`df.add_prefix('really_long_column_name')`

producing the ""Messy output' below.  For users wanting to keep `pd.set_option('expand_frame_repr', False)` but still have a truncated view, shouldn't `pd.io.formats.format.DataFrameFormatter` check the total length of all columns?  (or somehow consider the effect of both column width and number of columns)

Messy output
https://i.stack.imgur.com/yzZUI.png

Expected output
https://i.stack.imgur.com/arvRm.png

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.0.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.20.1
nose: 1.3.7
pip: 9.0.1
setuptools: 27.2.0
Cython: 0.25.2
numpy: 1.11.3
scipy: 0.18.1
statsmodels: 0.6.1
xarray: None
IPython: 5.1.0
sphinx: 1.5.1
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: 1.2.0
tables: 3.2.2
numexpr: 2.6.1
matplotlib: 2.0.0
openpyxl: 2.4.1
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: 0.9.6
lxml: 3.7.2
bs4: 4.5.3
html5lib: 0.9999999
httplib2: None
apiclient: None
sqlalchemy: 1.1.5
pymysql: None
psycopg2: None
jinja2: 2.9.4
boto: 2.45.0
pandas_datareader: 0.4.0

</details>
"
609506897,33887,DOC: Fix Syntax Typo in Indexing,zacharylawrence,closed,2020-04-30T02:08:42Z,2020-04-30T07:02:59Z,Please let me know if anything else needs to accompany this change!
608883052,33862,TYP/CLN: remove #type: ignore from pandas\tests\test_strings.py,simonjayhawkins,closed,2020-04-29T08:46:22Z,2020-04-30T08:48:54Z,
609207348,33872,DOC: Remove TODO from `test_openpyxl`,charlesdong1991,closed,2020-04-29T16:52:53Z,2020-04-30T09:00:59Z,"xref #29862 

Brought up by @jbrockmendel seems the `xfail` has been removed and version has been unpinned, just a tiny PR to remove this `TODO` from tests. probably people who fixed it forgot to remove this.

cc @jbrockmendel 
"
608683700,33856,Requested Follow-Ups,jbrockmendel,closed,2020-04-28T23:43:51Z,2020-04-30T14:43:51Z,
610038741,33896,"FEATURE: Type hints for DataFrame, Series, etc.",fkromer,closed,2020-04-30T14:07:02Z,2020-04-30T16:01:38Z,"#### Is your feature request related to a problem?

Type hinting with `pd.DataFrame` and `pd.Series` (`def data_science_etl_function(input_data_frame: pd.DataFrame) -> pd.DataFrame:`) does not document ETL APIs build on top of `pandas` w.r.t. `DataFrame` and/or `Series` meta-data like shape, indices, etc.

#### Describe the solution you'd like

It would be great to have something like [`nptyping`](https://github.com/ramonhagenaars/nptyping) (type hints for numpy data types) for `pd.DataFrames` and `pd.Series`.

#### API breaking implications

Should not break the API.

#### Describe alternatives you've considered

There is no real workaround. Adding `docstrings` everywhere is anoying and people tend to write them in an incomplete manner or miss them completly. Extensive unit testing of functionality :dizzy_face:"
609251618,33875,TST/REF: Parametrize consistency data and de-private methods,charlesdong1991,closed,2020-04-29T18:04:43Z,2020-04-30T17:48:38Z,"xref #30486 

I am sorry that this PR looks huge again. @jreback 
But this seems a bit unavoidable, and please allow me to briefly explain this, and it is quite simple actually:
1. move `_create_consistency_data` method to `conftest.py` and fixturize it as `consistency_data`.
2. Use `consistency_data` as fixture, and parametrize it to replace the inner loops.
3. Now since consistency_data is fixture, we do not need `self.data = _create_consistency_data()` as part init in `ConsistencyBase`, so I remove it, also a bunch of tests can be moved out of the BIG ConsistencyBase class (which I think is the ultimate goal: avoid using big classes, but simply the functions).
4. I also move some tests in `test_moments_rolling`, `test_moments_ewm`, and `test_moment_expanding` out of their classes to be independent test functions

After this PR, we could start reducing the reliance on the giant Base/ConsistencyBase classes, and next step will be fixturize other data and then reduce more reliance on those classes and make tests look smaller.
"
609402482,33881,CLN: address FIXME comments,jbrockmendel,closed,2020-04-29T22:28:49Z,2020-04-30T19:11:57Z,
607806953,33829,TST: ensure groupby get by index value #33439,sam-cohan,closed,2020-04-27T19:47:14Z,2020-05-01T00:26:49Z,"- [x] closes #33439
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
590283617,33141,Pandas cut method gives an error if labels are non-unique,harmbuisman,closed,2020-03-30T13:19:13Z,2020-05-01T00:31:48Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
data = pd.DataFrame({'morf':[8142, 8153, 8161]})
bins = [8140, 8150, 8160, 8163]
labels = ['Adenocarcinomas', 'Other specific carcinomas', 'Adenocarcinomas']

data['group'] = pd.cut(data['morf'], bins, labels=labels)
```
#### Problem description
I want to use pandas cut to fill in labels for ranges that are scattered. E.g. I want to map the following table onto my data, where the definitions span ranges that are scattered across the morphology continuum:
![image](https://user-images.githubusercontent.com/7702207/77915793-5aab6980-7298-11ea-945b-839b954ddec5.png)

The above code gives the error:
ValueError: Categorical categories must be unique

#### Expected Output
I expect the example above to run and give back the labeled dataframe without an error.

My workaround is to add the index to the labels and then remove that again after the cut.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200209
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : 0.15.0
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0
</details>
"
598273015,33480,BUG: Add unordered option to pandas.cut (#33141),mabelvj,closed,2020-04-11T14:07:04Z,2020-05-01T00:31:53Z,"Added tests and error message when there are no labels and `ordered=False`.

Issue: Pandas cut raises error if labels are non-unique (Closes #33141)

- [x] closes #33141 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
590945307,33172,value_counts not working correctly on (some?) ExtensionArrays,buhrmann,closed,2020-03-31T09:29:46Z,2020-05-01T00:34:43Z,"#### Code Sample

```python
pd.Series(list(""abcde""), dtype=""string"").value_counts(normalize=True)
```

```bash
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-40-718821f804b4> in <module>
      1 # lang.value_counts(normalize=True)
----> 2 pd.Series(list(""abcde""), dtype=""string"").value_counts(normalize=True)

~/anaconda/envs/grapy/lib/python3.7/site-packages/pandas/core/base.py in value_counts(self, normalize, sort, ascending, bins, dropna)
   1233             normalize=normalize,
   1234             bins=bins,
-> 1235             dropna=dropna,
   1236         )
   1237         return result

~/anaconda/envs/grapy/lib/python3.7/site-packages/pandas/core/algorithms.py in value_counts(values, sort, ascending, normalize, bins, dropna)
    729 
    730     if normalize:
--> 731         result = result / float(counts.sum())
    732 
    733     return result

AttributeError: 'IntegerArray' object has no attribute 'sum'
```

#### Problem description
The problem seems to be that `value_counts()` on a string extension dtype returns an Int64 dtype, and `sum` is not implemented for IntegerArrays , although it is for Series with ExtensionArrays:

#### Expected Output
```python
vc = pd.Series(list(""abcde""), dtype=""string"").value_counts(normalize=False)
print(vc)
print(vc / vc.sum())
```

```bash
d    1
e    1
b    1
c    1
a    1
dtype: Int64
d    0.2
e    0.2
b    0.2
c    0.2
a    0.2
dtype: float64
```

May be related to #22843?

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.2.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 20.0.2
setuptools       : 46.1.1.post20200322
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : 2.4.4
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : 0.9.3
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
603226191,33674,BUG: value_counts not working correctly on ExtensionArrays,kotamatsuoka,closed,2020-04-20T13:10:13Z,2020-05-01T00:58:27Z,"- [ ] closes #33172
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
608761710,33860,DOC: Fix groupby.agg/transform rst reference and numba references,mroeschke,closed,2020-04-29T04:03:04Z,2020-05-01T05:41:13Z,"- [x] xref https://github.com/pandas-dev/pandas/pull/33388#issuecomment-618808443
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
297792505,19728,DEPR: Index.get_value ?,jorisvandenbossche,closed,2018-02-16T13:55:43Z,2020-05-01T12:54:54Z,"`Index.get_value` is a very strange (from user perspective), and completely undocumented method, so let's rename it to `_get_value` for internal usage and deprecate the public one?

Example usage to show its 'not-usefulness':

```
In [85]: idx = pd.Index([1, 2, 3])

In [87]: idx.get_value(np.array([0, 1, 2]), 2)
Out[87]: 1
```
(so you can index into another object based on the location of the key in the calling index)

"
610515457,33907,DEPR: get_value,jbrockmendel,closed,2020-05-01T02:20:09Z,2020-05-01T14:30:44Z,"- [x] closes #19728
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

No longer used internally."
558717499,31574,TYP: Arraylike,simonjayhawkins,closed,2020-02-02T16:06:04Z,2020-05-01T14:57:54Z,"reopening #31518 

cc @jbrockmendel @WillAyd "
602736456,33654,BUG: can't concatenate DataFrame with Series with duplicate keys,MarcoGorelli,closed,2020-04-19T14:58:58Z,2020-05-01T16:50:31Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
>>> import pandas as pd
>>> df = pd.DataFrame({'a': [1,2,3], 'b': [1,2,3]})
>>> s1 = pd.Series([1,2,3], name='a')
>>> s2 = pd.Series([1,2,3], name='a')
>>>pd.concat([df, s1, s2], axis=1, keys=['a', 'b', 'b'])
TypeError: int() argument must be a string, a bytes-like object or a number, not 'slice'
```

<details>
<summary>full traceback</summary>

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-f6a5f4790f76> in <module>
      3 s1 = pd.Series([1,2,3], name='a')
      4 s2 = pd.Series([1,2,3], name='a')
----> 5 pd.concat([df, s1, s2], axis=1, keys=['a', 'b', 'b'])

~/pandas-dev/pandas/core/reshape/concat.py in concat(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    269     ValueError: Indexes have overlapping values: ['a']
    270     """"""
--> 271     op = _Concatenator(
    272         objs,
    273         axis=axis,

~/pandas-dev/pandas/core/reshape/concat.py in __init__(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)
    449         self.copy = copy
    450 
--> 451         self.new_axes = self._get_new_axes()
    452 
    453     def get_result(self):

~/pandas-dev/pandas/core/reshape/concat.py in _get_new_axes(self)
    512     def _get_new_axes(self) -> List[Index]:
    513         ndim = self._get_result_dim()
--> 514         return [
    515             self._get_concat_axis() if i == self.bm_axis else self._get_comb_axis(i)
    516             for i in range(ndim)

~/pandas-dev/pandas/core/reshape/concat.py in <listcomp>(.0)
    513         ndim = self._get_result_dim()
    514         return [
--> 515             self._get_concat_axis() if i == self.bm_axis else self._get_comb_axis(i)
    516             for i in range(ndim)
    517         ]

~/pandas-dev/pandas/core/reshape/concat.py in _get_concat_axis(self)
    569             concat_axis = _concat_indexes(indexes)
    570         else:
--> 571             concat_axis = _make_concat_multiindex(
    572                 indexes, self.keys, self.levels, self.names
    573             )

~/pandas-dev/pandas/core/reshape/concat.py in _make_concat_multiindex(indexes, keys, levels, names)
    651             names = names + get_consensus_names(indexes)
    652 
--> 653         return MultiIndex(
    654             levels=levels, codes=codes_list, names=names, verify_integrity=False
    655         )

~/pandas-dev/pandas/core/indexes/multi.py in __new__(cls, levels, codes, sortorder, names, dtype, copy, name, verify_integrity, _set_identity)
    281         # we've already validated levels and codes, so shortcut here
    282         result._set_levels(levels, copy=copy, validate=False)
--> 283         result._set_codes(codes, copy=copy, validate=False)
    284 
    285         result._names = [None] * len(levels)

~/pandas-dev/pandas/core/indexes/multi.py in _set_codes(self, codes, level, copy, validate, verify_integrity)
    880 
    881         if level is None:
--> 882             new_codes = FrozenList(
    883                 _coerce_indexer_frozen(level_codes, lev, copy=copy).view()
    884                 for lev, level_codes in zip(self._levels, codes)

~/pandas-dev/pandas/core/indexes/multi.py in <genexpr>(.0)
    881         if level is None:
    882             new_codes = FrozenList(
--> 883                 _coerce_indexer_frozen(level_codes, lev, copy=copy).view()
    884                 for lev, level_codes in zip(self._levels, codes)
    885             )

~/pandas-dev/pandas/core/indexes/multi.py in _coerce_indexer_frozen(array_like, categories, copy)
   3681         Non-writeable.
   3682     """"""
-> 3683     array_like = coerce_indexer_dtype(array_like, categories)
   3684     if copy:
   3685         array_like = array_like.copy()

~/pandas-dev/pandas/core/dtypes/cast.py in coerce_indexer_dtype(indexer, categories)
    866     length = len(categories)
    867     if length < _int8_max:
--> 868         return ensure_int8(indexer)
    869     elif length < _int16_max:
    870         return ensure_int16(indexer)

~/pandas-dev/pandas/_libs/algos_common_helper.pxi in pandas._libs.algos.ensure_int8()
     59             return arr
     60         else:
---> 61             return arr.astype(np.int8, copy=copy)
     62     else:
     63         return np.array(arr, dtype=np.int8)

TypeError: int() argument must be a string, a bytes-like object or a number, not 'slice'
```
</details>




#### Problem description

Noticed while working on #30858, I think this one needs to be solved first if we want to solve the `ohlc` case

#### Expected Output

```python
   a     b  b
   a  b  a  a
0  1  1  1  1
1  2  2  2  2
2  3  3  3  3
```

#### Output of ``pd.show_versions()``

<details>                                                                                              

INSTALLED VERSIONS
------------------
commit           : e878fdc4170f6a2aee8d7b42aa39a438fdf6c67f
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-46-generic
Version          : #38~18.04.1-Ubuntu SMP Tue Mar 31 04:17:56 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.0.dev0+1302.ge878fdc41
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200325
Cython           : 0.29.16
pytest           : 5.4.1
hypothesis       : 5.8.0
sphinx           : 3.0.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : 1.3.2
fastparquet      : 0.3.3
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.16
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0


</details>
"
606972183,33805,BUG: can't concatenate DataFrame with Series with duplicate keys,MarcoGorelli,closed,2020-04-26T10:01:32Z,2020-05-01T17:23:29Z,"- [x] closes #33654
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
610823379,33925,30999 fix bare pytest raises,andresmcneill,closed,2020-05-01T15:55:15Z,2020-05-01T19:30:35Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
30470937,6745,"CLN: revisit & simplify Block/BlockManager, remove axes",immerrr,closed,2014-03-30T18:16:10Z,2020-05-01T19:33:53Z,"This is a WIP request for Stage 1 deliverable of #6744.

Roadmap:
- [x] make ref_locs primary source of information (leaving items/ref_items in place to back it up and avoid breakage)
- [x] port groupby  to loc-based implementation (there's quite a number of hacks a.t.m. that make this non-trivial)
- [x] ditto for merge/join/concat
- [x] ditto for io code
- [x] ditto for reshape
- [x] clean up Block `items`
- [x] clean up Block `ref_items`
- [x] fix performance issues & integrate with mainline
"
604355244,33718,DEP: bump numpy min version?,jbrockmendel,closed,2020-04-21T23:52:44Z,2020-05-01T19:38:39Z,"We currently require 1.13.3 which was released 2017-09-29.  The last bump was in 0.25.0 (#25554).

I think its time for another bump.  The release dates for the 1.x.0 and 1.x.MAX versions out there:

1.14.0 -> 2018-01-06
1.14.6 -> 2018-09-23
1.15.0 -> 2018-07-23
1.15.4 -> 2018-11-04
1.16.0 -> 2019-01-13
1.16.6 -> 2019-12-29
1.17.0 -> 2019-07-06
1.17.5 -> 2020-01-01

1.15.4 looks not-unreasonable to me."
610458442,33906,REF: implement _unbox to de-duplicate unwrapping,jbrockmendel,closed,2020-04-30T23:35:27Z,2020-05-01T20:16:19Z,Ultimately we want these _validate_foo methods to be whittled down to only one or two methods.
609500646,33886,CLN: address TODOs,jbrockmendel,closed,2020-04-30T01:54:40Z,2020-05-01T20:17:09Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
610386151,33904,BUG: DTI/TDI intersection result names,jbrockmendel,closed,2020-04-30T21:06:56Z,2020-05-01T20:29:54Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry


Also improve _can_fast_intersect to catch the missing case where we can fast-intersect.  Identified that _most_ of the remaining places where we do freq inference are not tested, so may be removeable/deprecateable"
610576843,33910,REF: Create numba helper function for jitting + generating cache key,mroeschke,closed,2020-05-01T05:38:10Z,2020-05-01T20:32:41Z,"- [x] xref https://github.com/pandas-dev/pandas/pull/33388#discussion_r413967963
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
606462681,33770,BUG: ExtensionArrays whose elements are non-numeric numpy arrays crash Series.__repr__(),frreiss,closed,2020-04-24T17:13:15Z,2020-05-01T20:36:03Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

Prerequisites:
```console
$ pip install pandas numpy memoized-property
$ pip install git+https://github.com/frreiss/text-extensions-for-pandas
```

Reproduce the problem, using [`TensorArray`](https://github.com/frreiss/text-extensions-for-pandas/blob/master/text_extensions_for_pandas/array/tensor.py), our extension type for storing tensors in a Pandas series:
```python
>>> import pandas as pd
>>> import numpy as np
>>> import text_extensions_for_pandas as tp
>>> # Integers work
    int_tensors = np.array([[1, 2], [3, 4]])
    int_tensor_series = pd.Series(tp.TensorArray(int_tensors))
    int_tensor_series

0   [1 2]
1   [3 4]
dtype: TensorType

>>> # Boolean values don't work
    bool_tensors = np.array([[True, False], [False, True]])
    bool_tensor_series = pd.Series(tp.TensorArray(bool_tensors))
    bool_tensor_series
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/pd/covid-notebooks/env/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj)
    700                 type_pprinters=self.type_printers,
    701                 deferred_pprinters=self.deferred_printers)
--> 702             printer.pretty(obj)
    703             printer.flush()
    704             return stream.getvalue()

[...many lines of stack trace...]

~/pd/covid-notebooks/env/lib/python3.7/site-packages/pandas/io/formats/format.py in _format_strings(self)
   1255         fmt_values = []
   1256         for i, v in enumerate(vals):
-> 1257             if not is_float_type[i] and leading_space:
   1258                 fmt_values.append("" {v}"".format(v=_format(v)))
   1259             elif is_float_type[i]:

ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()

```

#### Problem description

`Series.__repr__()` invokes `ExtensionArrayFormatter` to render extension types. If the individual elements managed by an ExtensionArray are numpy arrays (or slices of a larger numpy array), then  `ExtensionArrayFormatter` uses Pandas' facilities for rendering numpy arrays. These facilities comprise the base class `GenericArrayFormatter` and subclasses such as `FloatArrayFormatter` for handling specific types (see `pandas/io/formats/format.py`). The formatters for numeric types can render numpy arrays with more than one dimension, but the base class `GenericArrayFormatter` cannot. The limitation appears to be an oversight. I will submit a small pull request with a fix in a few minutes.

#### Expected Output

The above code should output something like this:

```python
0   [True False]
1   [False True]
dtype: TensorType
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.4.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : 0.29.15
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.3.3
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.13.0
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : 0.8.3
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.49.0

</details>
"
606468435,33771,[#33770] bug fix to prevent ExtensionArrays from crashing Series.__repr__(),frreiss,closed,2020-04-24T17:23:45Z,2020-05-01T20:36:04Z,"- [X] closes #33770
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

This pull request fixes a bug in `GenericArrayFormatter` that caused a crash when rendering arrays of arrays. This problem causes `Series.__repr__()` to fail with certain extension types; see #33770 for more information.

The root cause is this line:
``` python
1231       is_float_type = lib.map_infer(vals, is_float) & notna(vals)
```
The call to `notna()` here returns a 2D array if `vals` is a 2D array, while `lib.map_infer(vals, is_float)` returns a 1D array if `vals` is a 2D array.

In addition to fixing that line, this PR also adds tests for the issue."
569422274,32191,BLD: Init PyDateTimeAPI Warnings,alimcmaster1,closed,2020-02-23T01:55:35Z,2020-05-01T22:02:02Z,"- Fixes below warning:
```
In file included from pandas/_libs/src/ujson/python/date_conversions.h:7:0,
                 from pandas/_libs/src/ujson/python/date_conversions.c:4:
/home/vsts/miniconda3/envs/pandas-dev/include/python3.7m/datetime.h:204:25: warning: ‘PyDateTimeAPI’ defined but not used [-Wunused-variable]
 static PyDateTime_CAPI *PyDateTimeAPI = NULL;
```

https://github.com/pandas-dev/pandas/pull/32163#issuecomment-590010308 (will check CI)

Still same warning coming from np_datetime and np_datetime_strings
"
560709731,31729,BUG: Fix to_excel writers handling of cols,alimcmaster1,closed,2020-02-06T00:32:35Z,2020-05-01T22:02:17Z,"- [x] closes #31677
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry - pending https://github.com/pandas-dev/pandas/pull/31723

Ref: https://github.com/pandas-dev/pandas/pull/31723

cc. @WillAyd @jbrockmendel "
602351790,33617,CLN: Cython warnings for type declartions,alimcmaster1,closed,2020-04-18T01:37:29Z,2020-05-01T22:03:19Z,"```
warning: pandas/_libs/internals.pyx:144:29: Strings should no longer be used for type declarations. Use 'cython.int' etc. directly.
warning: pandas/_libs/internals.pyx:147:32: Strings should no longer be used for type declarations. Use 'cython.int' etc. directly.
warning: pandas/_libs/internals.pyx:188:28: Strings should no longer be used for type declarations. Use 'cython.int' etc. directly.
```
(Against cython master)

"

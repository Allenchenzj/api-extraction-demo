id,number,title,user,state,created_at,updated_at,body
574025812,32394,"pd.to_numeric(..., errors=""coerce"") failing silently when strings contain ""uint64""",buhrmann,closed,2020-03-02T14:49:11Z,2020-03-15T00:38:58Z,"#### Problem description
When trying to coerce strings to numeric values using `to_numeric()`, the occurrence of the substring ""uint64"" (but not any other dtype-like substring it seems) leads to silent failure to coerce.

```python
strs = [""32"", ""64"", ""uint32"", ""float64"", ""sdnfonsdf uint32 knsdf"", ""sdnfonsdf uint64 knsdf"", ""uint64""]
print([pd.to_numeric(s, errors=""coerce"") for s in strs])
pd.to_numeric(pd.Series([""32"", ""64"", ""uint64""]), errors=""coerce"")
```

```bash
[32, 64, nan, nan, nan, 'sdnfonsdf uint64 knsdf', 'uint64']

0        32
1        64
2    uint64
dtype: object
```

#### Expected Output
```bash
[32, 64, nan, nan, nan, nan, nan]

0        32.0
1        64.0
2        NaN
dtype: float64
```

Seems to fail equally in 0.25.3 and 1.0...

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None

pandas           : 0.25.3
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200119
Cython           : None
pytest           : 5.3.4
hypothesis       : None
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : 0.9.3
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.14.1
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.12
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None

</details>
"
578693354,32582,Creating a column with a set replicates the set n times,AlexanderNixon,closed,2020-03-10T16:06:41Z,2020-03-15T00:41:34Z,"#### Code Sample

If we try to define a dataframe using a dictionary containing a set, we get:

```python
pd.DataFrame({'a':{1,2,3}})

       a
0  {1, 2, 3}
1  {1, 2, 3}
2  {1, 2, 3}
```
#### Problem description

The set is being replicated `n` times, `n` being the length of the actual set. 
While defining a column with a set directly might not make a lot of sense given that they are by definition unordered collections, the behaviour  in any case seems clearly unexpected. 

#### Expected Output

In the case of a list, in order to obtain a single row containing a list, we would have to define a nested list, such as `pd.DataFrame({'a':[[1,2,3]]})`.
So similarly, with sets I would expect the same behaviour by defining the row with `pd.DataFrame({'a':[{1,2,3}]})`. 

In the case of a single set, even if the order is not guaranteed to be preserved, I'd see more reasonable the same output that we would obtain with:

```python

pd.DataFrame({'a':[1,2,3]})

   a
0  1
1  2
2  3
```
So:

```python
pd.DataFrame({'a':{1,2,3}})

   a
0  1
1  2
2  3
```

Where:
```python
pd.__version__
# '1.0.0'
```
"
578809047,32594,BUG: Don't multiply sets during construction,dsaxton,closed,2020-03-10T19:25:56Z,2020-03-15T00:42:47Z,"- [x] closes #32582
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
576833581,32487,CLN: Remove redundant tests for .duplicated and .drop_duplicates in tests/base,SaturnFromTitan,closed,2020-03-06T10:20:17Z,2020-03-15T00:51:49Z,"part of #23877
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

duplicated and drop_duplicates are already thoroughly tested in `tests/indexes` and `tests/series`. I added comments to highlight the redundancy and extended the existing test cases where needed."
570513038,32237,travis: enable bionic & multi-cpu testing,xnox,closed,2020-02-25T11:46:40Z,2020-03-15T00:56:51Z,"- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
341234108,21910,Dataclass support,aidiss,closed,2018-07-14T12:46:23Z,2020-03-15T01:09:41Z,"#### Proposal  description

Dataclasses were added in Python 3.7.

It would be nice for pandas to support dataclasses. For example could be possible to construct dataframe from by calling `.from_dataclasses` or just `.DataFrame(data=dataclass_list)`. There should be also possibility to do `.to_dataclasses`.

#### Expected Behaviour

```python
from dataclasses import dataclass
import pandas as pd

@dataclass
class SimpleDataObject(object):
  field_a: int
  field_b: str

dataclass_object1 = SimpleDataObject(1, 'a')
dataclass_object2 = SimpleDataObject(2, 'b')
>>> asd

# Dataclasses to DataFrame
df = pd.from_dataclasses([dataclass_object1, dataclass_object2])
df.dtypes == ['field_a', 'field_b']
>>> True
df.dtypes == ['int', 'str']
>>> True

# Dataclasses to DataFrame
df = pd.DataFrame(data=[dataclass_object1, dataclass_object2])
df.dtypes == ['field_a', 'field_b']
>>> True
df.dtypes == ['int', 'str']
>>> True

# DataFrame to Dataclasses
df = pd.DataFrame(columns=['field_a', 'field_b'], data=[[1, 'a'], [2, 'b']])
dataclass_list = df.to_dataclasses()
dataclass_list == [dataclass_object1, dataclass_object2]
>>> True
```"
579600073,32639,DEPR: Categorical.to_dense,jbrockmendel,closed,2020-03-11T23:23:37Z,2020-03-15T01:27:12Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Ideally I'd want to keep the method and change the behavior to (an improved variant of) _internal_get_values, but so it goes."
581427104,32710,REF: avoid runtime import of Index,jbrockmendel,closed,2020-03-15T00:00:24Z,2020-03-15T01:27:43Z,
578880573,32598,BUG: Fix file descriptor leak,roberthdevries,closed,2020-03-10T21:47:38Z,2020-03-15T07:14:03Z,"- [x] closes #31488 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
578125591,32560,TST: Add extra test for pandas.to_numeric() for issue #32394,roberthdevries,closed,2020-03-09T19:01:15Z,2020-03-15T07:16:17Z,"- [x] closes #32394
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry (covered by PR #32541)
"
581554998,32717,1.0.2 release is not on pypi,scarabeusiv,closed,2020-03-15T07:01:27Z,2020-03-15T14:41:22Z,"As per $SUBJ the pypi contains just 1.0.1 release.
In openSUSE we validate all added archives against upstream ones and because of that the https://build.opensuse.org/request/show/784545 is now not acceptable :)

Could you please push the wheels/etc on pypi?"
581604901,32721,CLN: Switch to using `is not None` rather than `bool()`,skasturi,closed,2020-03-15T09:57:10Z,2020-03-15T16:59:57Z,"- [x] closes #32720
- [ ] tests added / passed - This change seemed trivial to *add* tests and should be covered by existing ones. 
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [] whatsnew entry
"
580217609,32668,BUG: DatetimeArray._from_sequence accepting bool dtype,jbrockmendel,closed,2020-03-12T21:04:08Z,2020-03-15T21:00:00Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Identified when making the change this makes in Series."
581627209,32726,CLN: Use keep fixture in more places,SaturnFromTitan,closed,2020-03-15T11:08:15Z,2020-03-16T01:36:59Z,"Follow up of #32487 - for [this comment of jreback](https://github.com/pandas-dev/pandas/pull/32487#issuecomment-599153289) in particular

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
581625689,32725,CLN: Remove Ops Mixin from pandas/tests/base,SaturnFromTitan,closed,2020-03-15T11:03:39Z,2020-03-16T01:38:06Z,"11 PRs later, the moment has finally come: closes #23877 🎉 

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`


"
483600707,28071,Loading CSV files (using `read_csv`) with blank lines between header and data rows quits Python interpreter,plartoo,closed,2019-08-21T19:07:05Z,2020-03-16T02:21:29Z,"#### Code Sample, a copy-pastable example if possible

```
import pandas as pd # tested with pandas 25.0 using Python 3.6.8
pd.read_csv('my_csv.csv', delimiter='|', header=4, nrows=1, skip_blank_lines=False) # this makes interpreter exit without any error message

pd.read_csv('my_csv.csv', delimiter='|', header=4, nrows=2, skip_blank_lines=False) # this is fine producing output below
   int_id_1_1_1  date_2019-01-01_2019-12-31_2  ascii_str_8_8_3  double_-1.0_1.0_4  integer_-1000_1000_5
0           NaN                           NaN              NaN                NaN                   NaN
1           NaN                           NaN              NaN                NaN                   NaN
```



#### Problem description

I have been trying to load a test CSV file (""my_csv.txt"", attached), which is structured in a way that there's an information text on the second row; row header line on the fifth row; and the data starts at the ninth row. As you can see in the Python code above, `read_csv` fails when `nrows=1` , but doesn't when `nrows>1`.

I think there's some uncaught bug in Pandas' `read_csv` when CSV file has blank lines between header and the start of the data rows. Thank you for your hard work maintaining and extending this very useful library.


[my_csv.txt](https://github.com/pandas-dev/pandas/files/3526892/my_csv.txt)


"
581476810,32715,CLN: remove values_from_object,jbrockmendel,closed,2020-03-15T02:46:07Z,2020-03-16T02:48:28Z,
581451059,32713,REF: misplaced tests,jbrockmendel,closed,2020-03-15T01:19:08Z,2020-03-16T03:57:17Z,
581826814,32736,REF: misplaced repr tests,jbrockmendel,closed,2020-03-15T21:08:40Z,2020-03-16T03:57:44Z,
581470855,32714,REF: merge NonConsolidateableMixin into ExtensionArray,jbrockmendel,closed,2020-03-15T02:25:19Z,2020-03-16T03:58:07Z,without this mypy complains when we try to annotate some NonConsolidateableMixin methods
582013933,32742,Remove the observations with 0 entries. ,Boblorde,closed,2020-03-16T05:26:44Z,2020-03-16T06:45:34Z,"import pandas as pd

battle = pd.read_csv('battledeath_1.csv', low_memory = False)


battle_without_duplicate = battle.drop_duplicates()

I have dropped the duplicates in the original csv file but now i am asked to delete the entries that have zero in its values. They are just 2 columns, one being the name of the country and the other being the value in it. I need help in taking out the rows with zero in them... "
264921442,17854,"Adding an integer-location based ""get"" method",hyamanieu,closed,2017-10-12T12:25:10Z,2020-03-16T09:30:53Z,"#### Code Sample, a copy-pastable example if possible

```python
a = pd.Series(data=[0],index=['A'])
print(a.get(1))#expected behaviour: returns default, here None
a['B']=2
print(a.get(1))#expected behaviour: returns 2
a.loc['C']=3
print(a.get(1))#expected behaviour: returns 2
a.loc[1]=5
print(a.get(1))#What is it? 2 or 5? (answer: 5=> Integer location based)
print(a.get(0))#What is it? default or 0? (answer:0=> index-name location based)
```
#### Problem description
Sometimes it is necessary to return a value from a DataFrame or a Series without throwing an error if the index is not present. The ""get"" function does just that.
Unfortunately, the ""get"" method doesn't allow to use an integer-location based index except for Series. 

However, as shown in the example, the ""get"" method can return unexpected values.

I would suggest to add an ""iget"" method for both Series and DataFrames similar to the ""get"" method:
```python
def iget(self, key, default=None):
            """"""
            Get item from object for given index. Returns default value if not found.
            Parameters
            ----------
            key : object
            Returns
            -------
            value : type of items contained in object
            """"""
            try:
                return self.iloc[key]
            except (KeyError, ValueError, IndexError):
                return default
```
While it doesn't solve the unexpected behavior of ""get"" with Series, it allows for an integer-based get methods with expected behavior for both pandas classes.

#### Expected Output
```python
a = pd.Series(data=[0],index=['A'])
print(a.get(1))#returns default, here None
a['B']=2
print(a.get(1))#returns default, here None
a.loc['C']=3
print(a.get(1))#returns default, here None
a.loc[1]=5
print(a.get(1))#returns 5
print(a.iget(1))#returns 2
print(a.iget(0))#returns 0
```
#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit: None
python: 3.5.3.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en
LOCALE: None.None

pandas: 0.20.1
pytest: 3.0.7
pip: 9.0.1
setuptools: 27.2.0
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.19.0
xarray: None
IPython: 6.0.0
sphinx: 1.3.1
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.2.2
numexpr: 2.6.2
feather: None
matplotlib: 2.0.2
openpyxl: 2.4.7
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: 0.9.6
lxml: 3.7.3
bs4: 4.6.0
html5lib: 0.999
sqlalchemy: 1.1.9
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
pandas_gbq: None
pandas_datareader: None

</details>
"
574975743,32423,BUG: df.apply disobeys raw=True,kernc,closed,2020-03-03T21:56:01Z,2020-03-16T10:08:28Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> df = pd.DataFrame(dict(ints=np.arange(3), 
...                        floats=np.arange(3, dtype=float)))                                         

# df.apply callbacks with Series even though raw requested
>>> df.apply(type, raw=True)                                                     
ints      <class 'pandas.core.series.Series'>
floats    <class 'pandas.core.series.Series'>
dtype: object

# Works for single dtype dfs
>>> df.astype(int).apply(type, raw=True)                                         
ints      <class 'numpy.ndarray'>
floats    <class 'numpy.ndarray'>
dtype: object

```
#### Problem description

When `df.apply(..., raw=True)`, the callback should _always_ be passed a numpy array (as documented) for reasons of performance and _convenience_ (arrays index much differently than Series).

This is not a recent regression; 0.25.3 exhibits the same behavior.

#### Expected Output

```py
>>> df.apply(type, raw=True, axis=1)
0    <class 'numpy.ndarray'>
1    <class 'numpy.ndarray'>
2    <class 'numpy.ndarray'>
dtype: object
```

#### Output of ``pd.show_versions()``

pandas 1.1.0.dev0+679.gd33b0025d
pandas 1.0.1
pandas 0.25.3"
582222566,32746,DOC: start 1.0.3,TomAugspurger,closed,2020-03-16T11:38:25Z,2020-03-16T14:37:42Z,
582292037,32750,Backport PR #32746: DOC: start 1.0.3,TomAugspurger,closed,2020-03-16T13:23:59Z,2020-03-16T14:34:53Z,https://github.com/pandas-dev/pandas/pull/32746
581449084,32712,CLN: Prelims for stronger typing in Block methods,jbrockmendel,closed,2020-03-15T01:13:09Z,2020-03-16T14:59:39Z,
577230798,32508,CLN: avoid values_from_object in nanops,jbrockmendel,closed,2020-03-06T23:40:32Z,2020-03-16T15:03:06Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
581749757,32729,DOC: Edit release notes,dsaxton,closed,2020-03-15T17:15:04Z,2020-03-16T15:40:50Z,Small cosmetic changes to rendering of release notes
576927563,32493,"to_numeric fails with empty data and downcast=""unsigned""",pspeter,closed,2020-03-06T13:21:28Z,2020-03-16T16:00:40Z,"#### Code Sample, a copy-pastable example if possible

```python
pd.to_numeric([], downcast=""unsigned"")
```
#### Problem description

The code currently calls `np.min()` on the data which fails when the data is empty. This does not happen for any other downcast.

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../lib/python3.7/site-packages/pandas/core/tools/numeric.py"", line 163, in to_numeric
    elif downcast == ""unsigned"" and np.min(values) >= 0:
  File ""<__array_function__ internals>"", line 6, in amin
  File ""/.../lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 2793, in amin
    keepdims=keepdims, initial=initial, where=where)
  File ""/.../lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 90, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
ValueError: zero-size array to reduction operation minimum which has no identity
```

#### Expected Output

array([], dtype=uint8)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.3.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 41.2.0
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : 0.4.0
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : 0.6.0
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0
</details>
"
581767164,32731,REF: simplify json get_values,jbrockmendel,closed,2020-03-15T18:06:30Z,2020-03-16T16:09:33Z,"cc @WillAyd trying to get away from _internal_get_values in the json code; this is zero-logic-changing to narrow the scope of what gets passed to _internal_get_values.

AFAICT replacing the _internal_get_values call with calling `__array__` doesnt break any tests, but in general the two are not always equivalent and it isnt obvious to me whether there are untested cases where it matters."
581447137,32711,Requested follow-up whatsnews,jbrockmendel,closed,2020-03-15T01:06:58Z,2020-03-16T16:41:13Z,
582315906,32752,TST: re-enable downstream geopandas test,jorisvandenbossche,closed,2020-03-16T13:56:56Z,2020-03-16T16:55:59Z,Closes https://github.com/pandas-dev/pandas/issues/32144
582045262,32743,Fix typo (add missing comma in list),deepyaman,closed,2020-03-16T06:40:20Z,2020-03-16T18:13:50Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
502724780,28793,DOC: Reference level name as Term of HDFStore.select query (#28791),nrebena,closed,2019-10-04T16:14:24Z,2020-03-16T19:31:31Z,"Add documentation for default level name with HDFStore MultiIndex.

- [x] closes #28791
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
506006193,28933,BUG: Preserve key order when using loc on MultiIndex DataFrame ,nrebena,closed,2019-10-11T18:52:53Z,2020-03-16T19:31:39Z,"## Description
closes #22797 
As described in #22797, the key order given to loc for a MultiIndex DataFrame was not respected:
```
import pandas as pd
import numpy as np
df = pd.DataFrame(np.arange(12).reshape((4, 3)),
    index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],
    columns=[['Ohio', 'Ohio', 'Colorado'],
    ['Green', 'Red', 'Green']])

df.loc[(['b','a'],[2, 1]),:]

# Out
     Ohio     Colorado
    Green Red    Green
a 1     0   1        2
  2     3   4        5
b 1     6   7        8
  2     9  10       11
```

## Proposed fix
The culprit was the use of intersection of indexers in the loc function. I tried keeping the indexers sorted during the whole function (in the main loop), but performance were really affected (by a factor 3!!!).
As an other solution, I tried to sort the result after the indexers were computed. It was already way better (worse ""only"" by a factor 1.15 or so, see the asv benchmark result).
So I computed and add a flag testing if the result need to be sorted (the benchmark  seems to always have sorted key in the loc call).

**Update** The sorting function is now a separate private function (_reorder_indexer). It is called at the end of the get_locs function.

## Benchmark

Benchmark with the flag (I run asv compare with -s option):
<details>
Benchmarks that have got worse:

       before           after         ratio
     [39602e7d]       [da8b55af]
     <master>         <multiindex_sort_loc_order_issue_22797>
+      5.62±0.2μs       6.27±0.2μs     1.11  index_cached_properties.IndexCache.time_shape('Float64Index')
+      6.57±0.2μs       7.49±0.2μs     1.14  index_cached_properties.IndexCache.time_shape('TimedeltaIndex')
</details>

Benchmark without flag:
<details>
Benchmarks that have got worse:

       before           after         ratio
     [39602e7d]       [c786822a]
     <master>         <multiindex_sort_loc_order_issue_22797~1>
+     2.49±0.02ms      2.87±0.01ms     1.15  ctors.SeriesConstructors.time_series_constructor(<class 'list'>, False, 'int')
+        2.53±0ms      2.91±0.01ms     1.15  ctors.SeriesConstructors.time_series_constructor(<class 'list'>, True, 'int')
+      29.2±0.7ms      33.1±0.02ms     1.13  frame_ctor.FromLists.time_frame_from_lists
+        87.2±1ms         98.9±1ms     1.13  frame_ctor.FromRecords.time_frame_from_records_generator(None)
+     12.8±0.09ms      14.3±0.09ms     1.11  groupby.MultiColumn.time_col_select_numpy_sum
+      5.62±0.2μs       6.32±0.4μs     1.12  index_cached_properties.IndexCache.time_shape('Float64Index')
+     4.96±0.02ms      5.71±0.01ms     1.15  indexing.MultiIndexing.time_index_slice
+        2.91±0ms      3.29±0.01ms     1.13  inference.ToNumeric.time_from_numeric_str('coerce')
+        2.92±0ms      3.29±0.01ms     1.13  inference.ToNumeric.time_from_numeric_str('ignore')
+     3.45±0.01ms      3.84±0.01ms     1.11  series_methods.Map.time_map('lambda', 'object')
+      29.3±0.2ms      33.2±0.04ms     1.13  strings.Methods.time_len
</details>

## Checklist

- [x] closes #22797
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
511498912,29190,BUG: Preserve key order when using loc on MultiIndex DataFrame,nrebena,closed,2019-10-23T18:31:44Z,2020-03-16T19:31:41Z,"This is another implementation to solve the issue described in #22797.

The other PR is #28933. When the other PR keep the main code identical and resort the indexer at the end, this implementation try to resolve the problem in the main loop of the original code.
The solution also as consequences on the performance when the key order is sorted as the index, so it add a flag to only keep the order if needed.

## Benchmark

<details>

Benchmarks that have improved:

       before           after         ratio
     [e623f0fb]       [b5b7be06]
     <stash~5>        <multiindex_loc_order_issue_22797>
-     2.50±0.01ms         2.35±0ms     0.94  ctors.SeriesConstructors.time_series_constructor(<class 'list'>, False, 'int')
-     2.55±0.01ms         2.40±0ms     0.94  ctors.SeriesConstructors.time_series_constructor(<class 'list'>, True, 'int')
-       767±0.5ms          707±2ms     0.92  groupby.Transform.time_transform_lambda_max
-      2.74±0.2μs       2.49±0.2μs     0.91  index_cached_properties.IndexCache.time_inferred_type('Float64Index')
-      2.64±0.1μs       2.51±0.2μs     0.95  index_cached_properties.IndexCache.time_is_all_dates('Float64Index')
-        517±10μs         491±10μs     0.95  index_cached_properties.IndexCache.time_is_monotonic('MultiIndex')
-         671±6μs          605±6μs     0.90  index_cached_properties.IndexCache.time_is_monotonic_decreasing('MultiIndex')
-        514±10μs         489±10μs     0.95  index_cached_properties.IndexCache.time_is_monotonic_increasing('MultiIndex')
-      6.61±0.3μs         5.68±1μs     0.86  index_cached_properties.IndexCache.time_shape('TimedeltaIndex')
-      3.87±0.7μs       2.84±0.2μs     0.73  index_cached_properties.IndexCache.time_values('TimedeltaIndex')
-     2.87±0.03μs      2.72±0.01μs     0.95  indexing.GetItemSingleColumn.time_frame_getitem_single_column_label
-     1.19±0.02ms         787±20μs     0.66  indexing.MultiIndexing.time_frame_ix
-     1.11±0.02ms         722±20μs     0.65  indexing.MultiIndexing.time_series_ix
-     3.54±0.01ms      3.37±0.01ms     0.95  indexing.NonNumericSeriesIndexing.time_getitem_list_like('string', 'unique_monotonic_inc')
-         644±2μs          568±1μs     0.88  multiindex_object.Duplicates.time_remove_unused_levels
-         746±8μs          189±2μs     0.25  multiindex_object.GetLoc.time_large_get_loc
-       169±0.6ms        102±0.4ms     0.61  multiindex_object.GetLoc.time_large_get_loc_warm
-      94.2±0.2μs       23.8±0.3μs     0.25  multiindex_object.GetLoc.time_med_get_loc
-      90.1±0.3ms       23.1±0.3ms     0.26  multiindex_object.GetLoc.time_med_get_loc_warm
-      87.1±0.2ms       22.8±0.2ms     0.26  multiindex_object.GetLoc.time_small_get_loc_warm
-      88.9±0.2μs       23.1±0.2μs     0.26  multiindex_object.GetLoc.time_string_get_loc
-       700±0.8μs        550±0.6μs     0.79  multiindex_object.Values.time_datetime_level_values_sliced
-     1.65±0.03ms      1.21±0.02ms     0.73  reindex.LevelAlign.time_align_level
-     1.65±0.04ms      1.22±0.01ms     0.74  reindex.LevelAlign.time_reindex_level
-      91.7±0.1ms       86.8±0.1ms     0.95  reshape.Crosstab.time_crosstab_normalize_margins
-      25.5±0.4ms       24.0±0.4ms     0.94  reshape.PivotTable.time_pivot_table
-      51.6±0.6ms       48.0±0.2ms     0.93  reshape.PivotTable.time_pivot_table_agg
-     19.7±0.05ms       18.4±0.2ms     0.93  reshape.PivotTable.time_pivot_table_categorical
-     14.2±0.03ms      13.1±0.04ms     0.92  reshape.PivotTable.time_pivot_table_categorical_observed
-       107±0.8ms       94.0±0.9ms     0.88  reshape.PivotTable.time_pivot_table_margins
-     6.02±0.02ms      5.47±0.02ms     0.91  reshape.SimpleReshape.time_stack
-     2.46±0.02ms      2.20±0.02ms     0.89  reshape.SimpleReshape.time_unstack
-        2.30±0ms      1.73±0.01ms     0.75  reshape.SparseIndex.time_unstack
-      18.9±0.1ms      18.0±0.03ms     0.95  rolling.Pairwise.time_pairwise(10, 'cov', True)
-        2.60±0ms         2.33±0ms     0.90  sparse.FromCoo.time_sparse_series_from_coo
-      41.3±0.2ms       39.2±0.2ms     0.95  sparse.ToCoo.time_sparse_series_to_coo
-         3.98±0s       3.75±0.01s     0.94  stat_ops.Correlation.time_corr_wide_nans('spearman', False)
-      3.99±0.01s       3.74±0.01s     0.94  stat_ops.Correlation.time_corr_wide_nans('spearman', True)
-     40.9±0.07ms       36.8±0.1ms     0.90  stat_ops.FrameMultiIndexOps.time_op(0, 'kurt')
-       195±0.3ms        165±0.3ms     0.85  stat_ops.FrameMultiIndexOps.time_op(1, 'kurt')
-        87.7±1ms         78.1±1ms     0.89  stat_ops.FrameMultiIndexOps.time_op(1, 'skew')
-         710±1ms          628±3ms     0.88  stat_ops.FrameMultiIndexOps.time_op([0, 1], 'skew')
-      10.7±0.1ms       9.66±0.3ms     0.90  stat_ops.SeriesMultiIndexOps.time_op(0, 'kurt')
-     10.6±0.05ms       9.15±0.2ms     0.86  stat_ops.SeriesMultiIndexOps.time_op(0, 'skew')
-      4.55±0.1ms      4.01±0.08ms     0.88  stat_ops.SeriesMultiIndexOps.time_op(0, 'std')
-     4.15±0.06ms       3.86±0.1ms     0.93  stat_ops.SeriesMultiIndexOps.time_op(0, 'var')
-      49.4±0.4ms       43.3±0.2ms     0.88  stat_ops.SeriesMultiIndexOps.time_op(1, 'kurt')
-       128±0.3ms        112±0.3ms     0.88  stat_ops.SeriesMultiIndexOps.time_op(1, 'mad')
-      48.0±0.3ms       40.3±0.4ms     0.84  stat_ops.SeriesMultiIndexOps.time_op(1, 'skew')
-     4.55±0.08ms       3.97±0.1ms     0.87  stat_ops.SeriesMultiIndexOps.time_op(1, 'std')
-      4.14±0.1ms      3.81±0.09ms     0.92  stat_ops.SeriesMultiIndexOps.time_op(1, 'var')
-       434±0.5ms        357±0.4ms     0.82  stat_ops.SeriesMultiIndexOps.time_op([0, 1], 'kurt')
-         1.20±0s          1.04±0s     0.86  stat_ops.SeriesMultiIndexOps.time_op([0, 1], 'mad')
-       422±0.7ms          339±1ms     0.80  stat_ops.SeriesMultiIndexOps.time_op([0, 1], 'skew')
-      48.6±0.2ms      40.5±0.07ms     0.83  timeseries.DatetimeIndex.time_to_pydatetime('repeated')
-       288±0.9ms        271±0.8ms     0.94  timeseries.DatetimeIndex.time_to_pydatetime('tz_aware')
-      48.3±0.1ms       40.6±0.1ms     0.84  timeseries.DatetimeIndex.time_to_pydatetime('tz_naive')

Benchmarks that have got worse:

       before           after         ratio
     [e623f0fb]       [b5b7be06]
     <stash~5>        <multiindex_loc_order_issue_22797>
+       149±0.5ms        170±0.6ms     1.15  categoricals.Rank.time_rank_string
+      7.23±0.2ms       8.38±0.3ms     1.16  categoricals.Rank.time_rank_string_cat
+      9.81±0.1ms       10.4±0.1ms     1.06  categoricals.Rank.time_rank_string_cat_ordered
+     2.45±0.01ms      2.65±0.09ms     1.08  frame_methods.Iteration.time_items_cached
+       227±0.3ms          239±4ms     1.06  groupby.GroupByMethods.time_dtype_as_field('datetime', 'unique', 'transformation')
+       148±0.3ms        157±0.3ms     1.06  groupby.GroupByMethods.time_dtype_as_field('float', 'unique', 'direct')
+       148±0.5ms        157±0.8ms     1.06  groupby.GroupByMethods.time_dtype_as_field('float', 'unique', 'transformation')
+       144±0.5ms          153±3ms     1.06  groupby.GroupByMethods.time_dtype_as_field('int', 'unique', 'direct')
+       143±0.4ms          155±5ms     1.08  groupby.GroupByMethods.time_dtype_as_field('int', 'unique', 'transformation')
+         489±3μs          541±4μs     1.11  groupby.GroupByMethods.time_dtype_as_field('object', 'first', 'direct')
+         488±2μs          546±3μs     1.12  groupby.GroupByMethods.time_dtype_as_field('object', 'first', 'transformation')
+         481±2μs         547±20μs     1.14  groupby.GroupByMethods.time_dtype_as_field('object', 'last', 'direct')
+         480±2μs          536±6μs     1.12  groupby.GroupByMethods.time_dtype_as_field('object', 'last', 'transformation')
+       173±0.2ms        183±0.5ms     1.06  groupby.GroupByMethods.time_dtype_as_field('object', 'unique', 'direct')
+       173±0.5ms        183±0.4ms     1.06  groupby.GroupByMethods.time_dtype_as_field('object', 'unique', 'transformation')
+       334±0.3ms          358±3ms     1.07  groupby.GroupByMethods.time_dtype_as_group('datetime', 'unique', 'direct')
+       334±0.3ms          356±2ms     1.07  groupby.GroupByMethods.time_dtype_as_group('datetime', 'unique', 'transformation')
+       330±0.4ms          352±2ms     1.07  groupby.GroupByMethods.time_dtype_as_group('float', 'unique', 'direct')
+       331±0.7ms        353±0.6ms     1.07  groupby.GroupByMethods.time_dtype_as_group('float', 'unique', 'transformation')
+       212±0.3ms        224±0.8ms     1.06  groupby.GroupByMethods.time_dtype_as_group('int', 'unique', 'direct')
+       211±0.2ms        224±0.7ms     1.06  groupby.GroupByMethods.time_dtype_as_group('int', 'unique', 'transformation')
+         766±3μs         1.67±0ms     2.19  groupby.GroupByMethods.time_dtype_as_group('object', 'unique', 'direct')
+       766±0.7μs         1.68±0ms     2.19  groupby.GroupByMethods.time_dtype_as_group('object', 'unique', 'transformation')
+     4.96±0.05μs       5.42±0.1μs     1.09  index_cached_properties.IndexCache.time_engine('DatetimeIndex')
+     4.25±0.05μs      4.46±0.05μs     1.05  index_cached_properties.IndexCache.time_engine('PeriodIndex')
+      2.92±0.1μs       3.13±0.2μs     1.07  index_cached_properties.IndexCache.time_inferred_type('MultiIndex')
+        988±30ns      1.07±0.05μs     1.08  index_cached_properties.IndexCache.time_inferred_type('RangeIndex')
+      2.38±0.1μs       2.74±0.5μs     1.15  index_cached_properties.IndexCache.time_inferred_type('TimedeltaIndex')
+      2.70±0.2μs       3.05±0.5μs     1.13  index_cached_properties.IndexCache.time_is_all_dates('UInt64Index')
+     1.41±0.02μs      1.51±0.04μs     1.07  index_cached_properties.IndexCache.time_is_monotonic_increasing('Int64Index')
+        941±20ns      1.01±0.02μs     1.07  index_cached_properties.IndexCache.time_is_unique('RangeIndex')
+      3.70±0.1μs       3.93±0.1μs     1.06  index_cached_properties.IndexCache.time_shape('UInt64Index')
+      2.88±0.2μs       3.05±0.2μs     1.06  index_cached_properties.IndexCache.time_values('UInt64Index')
+     5.43±0.08μs      6.39±0.02μs     1.18  index_object.Indexing.time_get_loc('Int')
+     5.44±0.06μs       6.45±0.2μs     1.19  index_object.Indexing.time_get_loc_sorted('Int')
+     2.76±0.06ms         3.01±0ms     1.09  indexing.IntervalIndexing.time_getitem_scalar
+     3.09±0.06ms      3.34±0.02ms     1.08  indexing.IntervalIndexing.time_loc_list
+     2.78±0.05ms         3.03±0ms     1.09  indexing.IntervalIndexing.time_loc_scalar
+     5.83±0.02ms      16.6±0.03ms     2.85  indexing.MultiIndexing.time_index_slice
+     2.45±0.02ms      2.59±0.02ms     1.06  inference.NumericInferOps.time_divide(<class 'numpy.uint8'>)
+     9.78±0.08ms      10.4±0.07ms     1.06  io.csv.ParseDateComparison.time_read_csv_dayfirst(False)
+         789±2ms          849±4ms     1.08  io.json.ReadJSON.time_read_json('index', 'datetime')
+       840±0.6ms          923±3ms     1.10  io.json.ReadJSON.time_read_json('index', 'int')
+     49.2±0.05ms       52.1±0.3ms     1.06  io.parsers.DoesStringLookLikeDatetime.time_check_datetimes('0.0')
+      68.1±0.4ms       72.3±0.5ms     1.06  io.parsers.DoesStringLookLikeDatetime.time_check_datetimes('2Q2005')
+        1.64±0μs       1.86±0.1μs     1.14  period.PeriodProperties.time_property('M', 'daysinmonth')
+     1.68±0.01μs      1.80±0.03μs     1.07  period.PeriodProperties.time_property('M', 'is_leap_year')
+     1.68±0.02μs      1.77±0.02μs     1.05  period.PeriodProperties.time_property('min', 'is_leap_year')
+       739±0.7ms          1.41±0s     1.91  stat_ops.FrameMultiIndexOps.time_op([0, 1], 'kurt')
+        764±10ns          810±2ns     1.06  timestamp.TimestampProperties.time_freqstr(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, None)
+        763±10ns        812±0.9ns     1.07  timestamp.TimestampProperties.time_freqstr(<UTC>, None)
+        763±10ns          812±1ns     1.06  timestamp.TimestampProperties.time_freqstr(None, None)
+        762±10ns          811±2ns     1.06  timestamp.TimestampProperties.time_freqstr(tzutc(), None)

</details>

## Checklist

- [x] closes #22797
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
578218370,32566,BUG: Fix segfault in csv tokenizer,roberthdevries,closed,2020-03-09T22:07:07Z,2020-03-16T20:08:32Z,"- [x] closes #28071
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
581617042,32722,TST: Fix HDFStore leak in tests/io/pytables/test_store.py,roberthdevries,closed,2020-03-15T10:37:29Z,2020-03-16T20:27:32Z,"
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
581556564,32718,"DOC: Fix PR01, PR07 in Index.min and Index.max",farhanreynaldo,closed,2020-03-15T07:06:11Z,2020-03-16T22:26:39Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. 

Output for pandas.Index.min:
```
################################################################################
################################## Validation ##################################
################################################################################

1 Errors found:
        No extended summary found
```

and output for pandas.Index.max:
```
################################################################################
################################## Validation ##################################
################################################################################

1 Errors found:
        No extended summary found
```"
577533459,32543,obj.fillna(fill_value) doesn't shallow copy if obj is an empty Series,SaturnFromTitan,closed,2020-03-08T17:14:11Z,2020-03-16T22:34:14Z,"While working on #32483 I bumped into the following issue:

#### Code Sample, a copy-pastable example if possible

```python
obj = pd.Series()
result = obj.fillna(0)
assert obj is not result  # raises AssertionError
```
#### Problem description

See `test_fillna` in the mentioned PR (or master when it is merged): With any value of `obj`, even if it is an empty `Index` the shallow copy is executed. This can potentially yield bugs related to unintended mutations of obj further down the road.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 343dd6751347b8a7ff67970c762d4b50e8038dfd
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.3.0
Version          : Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : de_DE.utf-8
LANG             : de_DE.utf-8export
LOCALE           : de_DE.UTF-8

pandas           : 1.1.0.dev0+698.g343dd6751.dirty
numpy            : 1.17.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.1
setuptools       : 45.1.0.post20200119
Cython           : 0.29.14
pytest           : 5.3.5
hypothesis       : 5.3.0
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : 0.3.2
gcsfs            : None
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.47.0

</details>
"
581787549,32733,FIX: series.fillna doesn't shallow copy if len(series) == 0,SaturnFromTitan,closed,2020-03-15T19:09:12Z,2020-03-16T22:41:33Z,"- [x] closes #32543
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
580267511,32670,TST: reintroduce check_series_type in assert_series_equal,martinfleis,closed,2020-03-12T22:52:49Z,2020-03-16T22:43:08Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Re-introduced `check_series_type` in `assert_series_equal` which was recently removed in #32513. It was a part of public API which is not used internally, but it is used elsewhere (like geopandas). Discussion in #32513 suggests that it should be put back where it was."
582651578,32763,CLN: simplify MultiIndex._shallow_copy,topper-123,closed,2020-03-16T23:14:10Z,2020-03-16T23:41:43Z,Minor simplification of ``MultiIndex._shallow_copy``.
581818549,32735,start using f-strings,smartvinnetou,closed,2020-03-15T20:42:58Z,2020-03-16T23:52:57Z,"- [ ] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] upgrades old string formatting to f-strings in core/generic.py
"
579793675,32653,DOC: Fix EX02 in pandas.Series.memory_usage,farhanreynaldo,closed,2020-03-12T09:11:33Z,2020-03-16T23:59:44Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. 

```
################################################################################
################################## Validation ##################################
################################################################################"
554274090,31251,"read_csv: if parse_dates dont appear in use_cols, we get a trace",teto,closed,2020-01-23T16:41:05Z,2020-03-17T00:03:12Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
import pandas as pd
import io

content = io.StringIO('''
time,val
212.23, 32
''')

date_cols = ['time']

df = pd.read_csv(
    content,
    sep=',',
    usecols=['val'],
    dtype= { 'val': int },
    parse_dates=date_cols,
)

```
#### Problem description

triggers
```
Traceback (most recent call last):
  File ""test.py"", line 16, in <module>
    parse_dates=date_cols,
  File ""/nix/store/k4fd48jzsyafvcifa6wi6pk4vaprnw36-python3.7-pandas-0.25.3/lib/python3.7/site-packages/pandas/io/parsers.py"",
line 685, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""/nix/store/k4fd48jzsyafvcifa6wi6pk4vaprnw36-python3.7-pandas-0.25.3/lib/python3.7/site-packages/pandas/io/parsers.py"",
line 463, in _read
    data = parser.read(nrows)
  File ""/nix/store/k4fd48jzsyafvcifa6wi6pk4vaprnw36-python3.7-pandas-0.25.3/lib/python3.7/site-packages/pandas/io/parsers.py"",
line 1154, in read
    ret = self._engine.read(nrows)
  File ""/nix/store/k4fd48jzsyafvcifa6wi6pk4vaprnw36-python3.7-pandas-0.25.3/lib/python3.7/site-packages/pandas/io/parsers.py"",
line 2134, in read
    names, data = self._do_date_conversions(names, data)
  File ""/nix/store/k4fd48jzsyafvcifa6wi6pk4vaprnw36-python3.7-pandas-0.25.3/lib/python3.7/site-packages/pandas/io/parsers.py"",
line 1885, in _do_date_conversions
    keep_date_col=self.keep_date_col,
  File ""/nix/store/k4fd48jzsyafvcifa6wi6pk4vaprnw36-python3.7-pandas-0.25.3/lib/python3.7/site-packages/pandas/io/parsers.py"",
line 3335, in _process_date_conversion
    data_dict[colspec] = converter(data_dict[colspec])
KeyError: 'time'
```
i.e., if you use columns in parse_dates that dont appear in use_cols, then you are screwed.

Either parse_dates sould be added to use_cols or the documentation precise it. An assert could make the error more understandable too.
pandas 0.25.3"
577572583,32549,DOC: Added docstring for Series.name and corrected docstring guide,dan1261,closed,2020-03-08T21:54:37Z,2020-03-17T00:07:43Z,"The main contribution is to add a docstring with examples for the ""name"" property of the Series object.  Also corrected some typos and grammatical points in the ""pandas docstring guide"".

The type hint gives the type as ""Label"" and I could not find any other reference to a custom type defined in pandas._typing which was explicitly mentioned in the docs (instead they specify ""str"" or ""int"" or somesuch), so I chose ""Label (int, str or other hashable object)"".
 
Further I chose ""whenever displaying the Series in the interpreter"" as clearer alternative to ""when invoking the __repr__ method"" or a similar precise statement."
579638405,32646,MultiIndex union/intersection with non-object other,jbrockmendel,closed,2020-03-12T01:38:42Z,2020-03-17T01:39:37Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

ATM if we call MultiIndex.union with e.g. a Float64Index, we get `ValueError: Buffer dtype mismatch, expected 'Python object' but got 'double'` from `lib.fast_unique_multiple`.  This PR changes that to raise NotImplementedError with a reasonable message, but we could alternatively return `self.to_flat_index().union(other)`

I prefer making the user do to_flat_index on their own because this seems like something that would often be reached by accident."
582425564,32754,REF: avoid _internal_get_values in json get_values,jbrockmendel,closed,2020-03-16T16:16:39Z,2020-03-17T02:21:49Z,"cc @WillAyd 

As mentioned in #32731, this changes the behavior for CategoricalIndex[dt64tz] and Series with that for its index.  Suggestions for where tests for that should go?

Note: the Categorical[dt64tz] cases still do a trip through object-dtype; I don't want to futz with that in C-space."
577167819,32503,core.common.random_state is too strict for state input,TomAugspurger,closed,2020-03-06T20:48:17Z,2020-03-17T02:38:04Z,"The numpy RandomState seed can be an int, array-like, or BitGenerator

```python
seed : {None, int, array_like, BitGenerator}, optional
    Random seed used to initialize the pseudo-random number generator or
    an instantized BitGenerator.  If an integer or array, used as a seed for
    the MT19937 BitGenerator. Values can be any integer between 0 and
    2**32 - 1 inclusive, an array (or other sequence) of such integers,
    or ``None`` (the default).  If `seed` is ``None``, then the `MT19937`
    BitGenerator is initialized by reading data from ``/dev/urandom``
    (or the Windows analogue) if available or seed from the clock
    otherwise.
```

Right now, we only pass through integers. Arrays (and BitGenerators if possible) should be passed through to RandomState at https://github.com/pandas-dev/pandas/blob/54c5e9e6e0819f848a10045d1cbc8ee936ffd075/pandas/core/common.py#L409-L410. 

## Current Behavior

```pytb
In [1]: import pandas as pd

In [2]: import numpy as np

In [3]: state_data = np.random.randint(0, 2**31, size=624, dtype='uint32')

In [4]: pd.core.common.random_state(state_data)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-4-4f97e41a5899> in <module>
----> 1 pd.core.common.random_state(state_data)

~/sandbox/pandas/pandas/core/common.py in random_state(state)
    415     else:
    416         raise ValueError(
--> 417             ""random_state must be an integer, a numpy RandomState, or None""
    418         )
    419

ValueError: random_state must be an integer, a numpy RandomState, or None
```

## Expected Output

```python
Out[4]: RandomState(MT19937) at 0x115B16050
```"
577266075,32510,[BUG] Loosen random_state input restriction,mikekutzma,closed,2020-03-07T02:57:25Z,2020-03-17T02:38:05Z,"Alllow for array-like as well as BitGenerator inputs

Addresses: GH32503

- [x] closes #32503 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
572496839,32320,BUG: parse_dates may have columns not in dataframe,sathyz,closed,2020-02-28T03:28:41Z,2020-03-17T03:49:49Z,"read_csv will raise ValueError when columnes used for parse_dates are found in the dataframe.

- [x] closes #31251 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
582693209,32765,Backport PR #32758 on branch 1.0.x (BUG: resample.agg with read-only data),meeseeksmachine,closed,2020-03-17T01:34:52Z,2020-03-17T07:57:54Z,Backport PR #32758: BUG: resample.agg with read-only data
582389103,32753,DOC: Partial fix SA04 errors in docstrings #28792,dilex42,closed,2020-03-16T15:29:53Z,2020-03-17T08:12:40Z,"- [x] xref #28792

My first PR. Tell me if I'm doing this incorrectly.

"
581578165,32719,unexpected behavior in pd.to_datetime,yotamgi,closed,2020-03-15T08:17:12Z,2020-03-17T08:23:47Z,"#### Code Sample

```python
>>> pd.to_datetime('2016-01-01 00:00 UTC-5', utc=True)
Timestamp('2015-12-31 19:00:00+0000', tz='UTC')
```
#### Problem description

The time at UTC-x timezone region should correspond to a +x shift at UTC+00 zones. The behavior of `pd.to_datetime` in this case is wrong, since it correspond to -x shift at UTC.

As opposed to the above, the following syntax *is* working as expected:
```python
>>> pd.to_datetime('2016-01-01 00:00 -05:00', utc=True)
Timestamp('2016-01-01 05:00:00+0000', tz='UTC')
```

#### Expected Output
```python
Timestamp('2016-01-01 05:00:00+0000', tz='UTC')
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.7.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-smp-900.331.0.0
machine: x86_64
processor: 
byteorder: little
LC_ALL: en_US.UTF-8
LANG: None
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: None
pip: None
setuptools: unknown
Cython: None
numpy: 1.16.4
scipy: 1.2.1
pyarrow: 0.15.1
xarray: None
IPython: 2.0.0
sphinx: None
patsy: 0.4.1
dateutil: 2.8.1
pytz: 2019.3
blosc: None
bottleneck: None
tables: 3.5.2
numexpr: 2.6.10dev0
feather: None
matplotlib: 3.0.3
openpyxl: None
xlrd: 1.2.0
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.3
s3fs: None
fastparquet: None
pandas_gbq: 0+unknown
pandas_datareader: None
gcsfs: None

</details>
"
580077391,32663,read_sql_query convert TEXT looking like floats to float,hyamanieu,closed,2020-03-12T16:56:46Z,2020-03-17T09:55:51Z,"#### Code Sample, you need to create a database with ""float looking"" strings.

```python
import pandas as pd
query = ""SELECT MYSTR FROM TABLE""
df = pd.read_sql_query(query, engine)
df.info()
```
returns
```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 524 entries, 0 to 523
Data columns (total 1 columns):
MYSTR          524 non-null float64
dtypes:  float64(1)
```
#### Problem description

I have a MySQL table with a column saved as datatype `TEXT`. These strings may be interpreted as floats because they contain only digits and dots. If I have the right filter happening, the query may return ONLY float-looking strings, meaning either looking like normal integers or integers separated by a decimal dot.

read_sql_query converts without asking anything looking like a float to a float. This is unfortunate and breaks the application.

It is not a solution to reconvert back to string with df.astype(str). Indeed, a four-digit string will be returned as a 6-character string, e.g. `4906 `will return as `4906.0`. Also, a string having left trailing zeros will have these zeros disappear.

On another machine, the column stays a string column. It may be a version difference, either python (good: 3.6 vs bad: 3.7), or sqlalchemy (good: 1.3.12 bad: 1.3.5).

I could not update sqlalchemy to a the higher version using `conda update sqlalchemy`, it stays at 1.3.5.
The python and pandas have however a higher version (6.7 & 0.25). I tried reverting to 0.23.4 as on the functional machine, however the bug (or ""feature""?) was still there.

Is python 3.7 the root cause?

#### Expected Output
TEXT data type should stay strings in pandas.

#### Output of ``pd.show_versions()``

Machine converting to float:
<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 7
machine          : AMD64
processor        : Intel64 Family 6 Model 63 Stepping 2, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.0
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.7.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.5
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.1.8
</details>

Machine keeping string format:
<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.10.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.23.4
pytest: 5.0.1
pip: 19.1.1
setuptools: 44.0.0.post20200106
Cython: 0.28.2
numpy: 1.17.4
scipy: 1.3.2
pyarrow: None
xarray: 0.11.3
IPython: 7.11.1
sphinx: 1.7.9
patsy: 0.5.1
dateutil: 2.8.1
pytz: 2019.3
blosc: None
bottleneck: 1.3.1
tables: 3.6.1
numexpr: 2.7.0
feather: None
matplotlib: 3.1.1
openpyxl: 2.5.4
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.2.7
lxml: 4.4.2
bs4: 4.8.2
html5lib: 1.0.1
sqlalchemy: 1.3.12
pymysql: None
psycopg2: None
jinja2: 2.10.3
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
582109878,32744,CLN: Split pandas/tests/base/test_ops.py,SaturnFromTitan,closed,2020-03-16T08:47:01Z,2020-03-17T11:52:44Z,"As suggested by @jreback in [this PR comment](https://github.com/pandas-dev/pandas/pull/32725#issuecomment-599299873)

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
565817569,32033,CI: add pydocstyle to code_checks,simonjayhawkins,closed,2020-02-15T22:21:31Z,2020-03-17T12:13:38Z,http://www.pydocstyle.org/en/5.0.2/error_codes.html
582824460,32771,Backport PR #32708 on branch 1.0.x (skip 32 bit linux),meeseeksmachine,closed,2020-03-17T07:57:50Z,2020-03-17T12:46:24Z,Backport PR #32708: skip 32 bit linux
580735593,32685,Regression in artihmetic ops with alignment and special cases,TomAugspurger,closed,2020-03-13T17:12:42Z,2020-03-17T15:41:09Z,"I think 1.0.2 has a regression in arithmetic when we align but the result of `op(a, NA)` isn't NA. For example 1**np.nan should be 1.

Previous

```python
In [3]: pd.__version__
Out[3]: '1.0.1'

In [4]: pd.DataFrame({""A"": [0, 1, 2]}) ** pd.DataFrame(index=[0, 1, 2])
Out[4]:
     A
0  NaN
1  1.0
2  NaN
```

1.0.2 / master

```python
In [1]: import pandas as pd

In [2]: pd.DataFrame({""A"": [0, 1, 2]}) ** pd.DataFrame(index=[0, 1, 2])
Out[2]:
    A
0 NaN
1 NaN
2 NaN
```

Most likely from https://github.com/pandas-dev/pandas/pull/31679. cc @jbrockmendel if you have a chance to look (I won't today).

Are there any other special cases?"
581602230,32720,Possible code improvement suggestion: Use `is not None` rather than bool() for None check,skasturi,closed,2020-03-15T09:48:00Z,2020-03-17T15:48:13Z,"#### Code Sample, a copy-pastable example if possible

While going through the code I observed at couple of places where `bool(o)` is being used to check whether the object `o` is `None`. 

Example 1: https://github.com/pandas-dev/pandas/blob/6620dc67ded40c76e466c70027abd012898bbbb1/pandas/core/strings.py#L449
Example 2: https://github.com/pandas-dev/pandas/blob/6620dc67ded40c76e466c70027abd012898bbbb1/pandas/core/strings.py#L821

#### Problem description

`bool(o)` is several times slower than `o is not None` (see below). In the above cases however, majority of the time indeed is being taken by the `re.search` and therefore it might not matter for the overall time. But, why not switch to the possibly correct version? 

Moreover, the functionality remains the same as `re.search` and `re.match` return `None` when they cannot find a match; which is fundamentally what is being checked in this code. 

![image](https://user-images.githubusercontent.com/482709/76699465-e7d0aa80-666a-11ea-88db-e54dc91877fb.png)
"
582534185,32758,BUG: resample.agg with read-only data,jbrockmendel,closed,2020-03-16T18:58:31Z,2020-03-17T16:01:08Z,"- [x] closes #31710
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
581802037,32734,BUG: arithmetic with reindex pow,jbrockmendel,closed,2020-03-15T19:52:43Z,2020-03-17T19:15:08Z,"- [x] closes #32685
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
583003695,32774,pd.to_datetime does not work.,filipporemonato,closed,2020-03-17T13:14:05Z,2020-03-17T16:19:52Z,"#### Code Sample, a copy-pastable example if possible

```python
data = pd.read_csv(""Data_export.csv"")
data.drop_duplicates(keep='first',inplace=True) # dropping duplicate rows
df_out = data.pivot(index='timeutc', columns='attributename', values='value') # produce correct columns with values
df_out = df_out.dropna()

print(""BEFORE"")
print(df_out.index)

df_out.index = pd.to_datetime(df_out.index)

print(""AFTER"")
print(df_out.index)

```
#### Problem description

I am importing a .csv file taken from the local meteorological data service.
The file contains data such as ""humidity"", ""temperature"", etc. together with a ""dateutc"" containing the timestamp of the data sample.

Pandas does not recognise the format immediately, so I explicitly call ```to_datetime```, but it does not seem to work.

Output of my file:
```
BEFORE
Index(['2018-06-06 15:10:00+02', '2018-06-06 15:20:00+02',
       '2018-06-06 15:30:00+02', '2018-06-06 16:20:00+02',
       '2018-06-06 16:30:00+02', '2018-06-06 17:10:00+02',
       '2018-06-06 17:20:00+02', '2018-06-06 17:30:00+02',
       '2018-06-06 17:40:00+02', '2018-06-06 17:50:00+02',
       ...
       '2019-07-21 05:50:00+02', '2019-07-21 06:00:00+02',
       '2019-07-21 06:10:00+02', '2019-07-21 06:20:00+02',
       '2019-07-21 06:30:00+02', '2019-07-21 06:40:00+02',
       '2019-07-21 06:50:00+02', '2019-07-21 07:00:00+02',
       '2019-07-21 07:10:00+02', '2019-07-21 07:20:00+02'],
      dtype='object', name='timeutc', length=52029)
```

This is type Object, as expected. But even after calling ```to_datetime``` it still remains Object:

```
AFTER
Index([2018-06-06 15:10:00+02:00, 2018-06-06 15:20:00+02:00,
       2018-06-06 15:30:00+02:00, 2018-06-06 16:20:00+02:00,
       2018-06-06 16:30:00+02:00, 2018-06-06 17:10:00+02:00,
       2018-06-06 17:20:00+02:00, 2018-06-06 17:30:00+02:00,
       2018-06-06 17:40:00+02:00, 2018-06-06 17:50:00+02:00,
       ...
       2019-07-21 05:50:00+02:00, 2019-07-21 06:00:00+02:00,
       2019-07-21 06:10:00+02:00, 2019-07-21 06:20:00+02:00,
       2019-07-21 06:30:00+02:00, 2019-07-21 06:40:00+02:00,
       2019-07-21 06:50:00+02:00, 2019-07-21 07:00:00+02:00,
       2019-07-21 07:10:00+02:00, 2019-07-21 07:20:00+02:00],
      dtype='object', name='timeutc', length=52029)
```

Calling ```print(type(data.index))``` gives
```
<class 'pandas.core.indexes.base.Index'>
```

#### Expected Output

I expected to have a DateTime index.
My code was working as intended before upgrading the Anaconda Interface. Somehow, without me realising it, upgrading the Anaconda Interface also upgraded Pandas to 1.0.1, and my code broke down in several places. I tried downgrading but I do not remember exactly which version of Pandas I was using before, when the code was working, but the above behaviour happens both with 1.0.1 and with 0.25.3.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0.post20200309
Cython           : 0.29.15
pytest           : 5.3.5
hypothesis       : 5.5.4
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7

</details>
"
581523381,32716,CLN: remove SingleBlockManager._values,jbrockmendel,closed,2020-03-15T05:19:02Z,2020-03-17T16:41:42Z,
582274611,32747,BUG: Regression in assert_frame_equal check_dtype for extension dtypes,jorisvandenbossche,closed,2020-03-16T12:57:50Z,2020-03-17T19:33:40Z,"Consider this small example of two DataFrames, one with an Int64 extension dtype, the other with the same values but object dtype:

```
df1 = pd.DataFrame({'a': pd.array([1, 2, 3], dtype=""Int64"")}) 
df2 = df1.astype(object)   
```

With pandas 1.0.1, this passes `assert_frame_equal` with the `check_dtype=False`:

```
In [5]: pd.testing.assert_frame_equal(df1, df2)  
...
Attribute ""dtype"" are different
[left]:  Int64
[right]: object

In [6]: pd.testing.assert_frame_equal(df1, df2, check_dtype=False)  
```

but with master (since https://github.com/pandas-dev/pandas/pull/32570, see my comment there, cc @jbrockmendel), this fails:

```
In [2]: pd.testing.assert_frame_equal(df1, df2, check_dtype=False)   
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-2-d2d792202db1> in <module>
----> 1 pd.testing.assert_frame_equal(df1, df2, check_dtype=False)

~/scipy/pandas/pandas/_testing.py in assert_frame_equal(left, right, check_dtype, check_index_type, check_column_type, check_frame_type, check_less_precise, check_names, by_blocks, check_exact, check_datetimelike_compat, check_categorical, check_like, obj)
   1378                 check_datetimelike_compat=check_datetimelike_compat,
   1379                 check_categorical=check_categorical,
-> 1380                 obj=f'{obj}.iloc[:, {i}] (column name=""{col}"")',
   1381             )
   1382 

~/scipy/pandas/pandas/_testing.py in assert_series_equal(left, right, check_dtype, check_index_type, check_series_type, check_less_precise, check_names, check_exact, check_datetimelike_compat, check_categorical, check_category_order, obj)
   1177         )
   1178     elif is_extension_array_dtype(left.dtype) or is_extension_array_dtype(right.dtype):
-> 1179         assert_extension_array_equal(left._values, right._values)
   1180     elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):
   1181         # DatetimeArray or TimedeltaArray

~/scipy/pandas/pandas/_testing.py in assert_extension_array_equal(left, right, check_dtype, check_less_precise, check_exact)
   1017     """"""
   1018     assert isinstance(left, ExtensionArray), ""left is not an ExtensionArray""
-> 1019     assert isinstance(right, ExtensionArray), ""right is not an ExtensionArray""
   1020     if check_dtype:
   1021         assert_attr_equal(""dtype"", left, right, obj=""ExtensionArray"")

AssertionError: right is not an ExtensionArray
```"
583249088,32783,DOC: 1.0.3 release date,TomAugspurger,closed,2020-03-17T19:33:00Z,2020-03-17T19:34:11Z,
583249639,32784,Backport PR #32783 on branch 1.0.x (DOC: 1.0.3 release date),meeseeksmachine,closed,2020-03-17T19:34:00Z,2020-03-17T20:52:03Z,Backport PR #32783: DOC: 1.0.3 release date
582522836,32757,BUG: Respect check_dtype in assert_series_equal,dsaxton,closed,2020-03-16T18:40:33Z,2020-03-17T20:56:21Z,"- [x] closes #32747
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

I'm not sure if https://github.com/pandas-dev/pandas/blob/master/pandas/_testing.py#L1174 needs to change too?

cc @jorisvandenbossche "
583293066,32786,CLN: remove unnecessary alias,jbrockmendel,closed,2020-03-17T20:54:56Z,2020-03-17T22:13:44Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
582467961,32756,copy license text from: tidyverse/haven,jamin-aws-ospo,closed,2020-03-16T17:13:07Z,2020-03-18T00:01:06Z,"The existing HAVEN_LICENSE file was lacking any form of actual license.
After checking the pandas source, it appears that the reference to HAVEN
is in relation to tidyverse/haven, copying the license from
tidyverse/haven for clarity.

Signed-off-by: Jamin Collins <jamin@amazon.com>
"
583383648,32795,BLD: Suppressing warnings when compiling pandas/_libs/writers,ShaharNaveh,closed,2020-03-18T00:36:35Z,2020-03-18T09:44:28Z,"- [x] ref #32163
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

This is the error that this PR is getting rid of:

```
pandas/_libs/writers.c: In function ‘__pyx_pw_6pandas_5_libs_7writers_1write_csv_rows’:
pandas/_libs/writers.c:3435:15: warning: ‘__pyx_v_j’ may be used uninitialized in this function [-Wmaybe-uninitialized]
 3435 |     __pyx_t_1 = (__pyx_v_j + 1);
      |     ~~~~~~~~~~^~~~~~~~~~~~~~~~~
pandas/_libs/writers.c:2911:14: note: ‘__pyx_v_j’ was declared here
 2911 |   Py_ssize_t __pyx_v_j;
      |              ^~~~~~~~~
```"
568829281,32144,CI Failing: test_geopandas,simonjayhawkins,closed,2020-02-21T09:14:50Z,2020-03-18T11:02:50Z,"https://travis-ci.org/pandas-dev/pandas/jobs/653274362?utm_medium=notification&utm_source=github_status

cc @jorisvandenbossche 


```
=================================== FAILURES ===================================
________________________________ test_geopandas ________________________________
[gw0] linux -- Python 3.6.10 /home/travis/miniconda3/envs/pandas-dev/bin/python
    @pytest.mark.filterwarnings(""ignore:can't resolve:ImportWarning"")
    def test_geopandas():
    
>       geopandas = import_module(""geopandas"")  # noqa
pandas/tests/test_downstream.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pandas/tests/test_downstream.py:20: in import_module
    return importlib.import_module(name)
../../../miniconda3/envs/pandas-dev/lib/python3.6/importlib/__init__.py:126: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:994: in _gcd_import
    ???
<frozen importlib._bootstrap>:971: in _find_and_load
    ???
<frozen importlib._bootstrap>:955: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:665: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:678: in exec_module
    ???
<frozen importlib._bootstrap>:219: in _call_with_frames_removed
    ???
../../../miniconda3/envs/pandas-dev/lib/python3.6/site-packages/geopandas/__init__.py:5: in <module>
    from geopandas.io.file import read_file  # noqa
../../../miniconda3/envs/pandas-dev/lib/python3.6/site-packages/geopandas/io/file.py:6: in <module>
    import fiona
../../../miniconda3/envs/pandas-dev/lib/python3.6/site-packages/fiona/__init__.py:83: in <module>
    from fiona.collection import BytesCollection, Collection
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
    import logging
    import os
    import warnings
    
    from fiona import compat, vfs
>   from fiona.ogrext import Iterator, ItemsIterator, KeysIterator
E   ImportError: libkea.so.1.4.7: cannot open shared object file: No such file or directory
../../../miniconda3/envs/pandas-dev/lib/python3.6/site-packages/fiona/collection.py:9: ImportError

```"
580643578,32681,PERF: Using Numpy C-API arange,ShaharNaveh,closed,2020-03-13T14:39:29Z,2020-03-18T12:47:08Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

This PR was opened as @jbrockmendel suggested (ref https://github.com/pandas-dev/pandas/pull/32177#discussion_r382923663)

---

### Benchmarks:

#### Master:

```
In [1]: import pandas._libs.internals as internals

In [2]: %timeit internals.BlockPlacement(slice(1_000_000)).as_array
1.55 ms ± 143 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

---

#### PR:

```
In [1]: import pandas._libs.internals as internals

In [2]: %timeit internals.BlockPlacement(slice(1_000_000)).as_array
1.46 ms ± 3.55 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```
 "
583903137,32810,"Not actual, please delete",evyasonov,closed,2020-03-18T17:50:53Z,2020-03-18T17:59:09Z,"Not actual, please delete"
582295074,32751,RLS: 1.0.3,TomAugspurger,closed,2020-03-16T13:28:14Z,2020-03-18T18:40:22Z,"I'd like to release 1.0.3 somewhat soon, primarily motivated by https://github.com/pandas-dev/pandas/issues/32685 (which has an open PR with a fix).

Can we target tomorrow, Tuesday the 17th? If there are other regression fixes to include, ping me on them or post their issue numbers here.

---

I have a bit of free time today and tomorrow, so I might merge https://github.com/MacPython/pandas-wheels/pull/74 to use the new wheel building infrastructure.

cc @pandas-dev/pandas-core 

"
583886428,32809,TYP: annotate to_numpy,jbrockmendel,closed,2020-03-18T17:23:27Z,2020-03-18T19:32:30Z,
578119456,32559,date_range() : allowing argument closed to have value 'open',yohplala,closed,2020-03-09T18:50:05Z,2020-03-18T19:50:45Z,"#### Code Sample

```
# My current code to get rid of both extremities of the DateTimeIndex  returned by date_range()
def my_date_range(start: pd.Timestamp, end: pd.Timestamp, freq: str) -> pd.DatetimeIndex:
    """"""
    Remove automatically first and last timestamps from the generated DateTimeIndex.
    """"""

    DTI = pd.date_range(start = start, end = end, freq = freq, closed = 'left')

    # Last timestamp has not been kept with use of 'closed' keyword, 'left' value.
    # With delete, the 1st timestamp is also removed.
    return DTI.delete(0)

# Now, I can use date_range() the way I want with apply() function :)
time_range = df.apply(lambda x: pd.Series(my_date_range(x.name, x.End, x.Freq)), axis=1).stack()

```
#### Problem description

Please, could the keyword 'closed' accept a 4th value (for instance 'open') that allows suppressing both extremities of the DateTimeIndex?

Thanks for your help and this marvelous library! Loving it!
;)

"
583957554,32811,Nested list multi-index fix,harri471,closed,2020-03-18T19:35:29Z,2020-03-18T19:54:54Z,"- [ ] closes #14467 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
583995362,32813,fstring format added in pandas//tests/io/test_common.py:144:,mglasder,closed,2020-03-18T20:51:53Z,2020-03-18T22:09:51Z,"I'm completely new to this. I hope i didn't do anything wrong here

https://github.com/pandas-dev/pandas/issues/29547
"
583724201,32804,PERF: Using Numpy C-API when calling `np.arange`,ShaharNaveh,closed,2020-03-18T13:22:04Z,2020-03-18T23:51:12Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

Somewhat of a follow up to #32681.

---

I could not benchmark this change as this function is ```cdef``` and not ```def``` or ```cpdef``` (I would also love if someone could give me a tip on how to benchmark  ```cdef``` functions from an ```Ipython``` shell for example).

What I did what I ran ```cython -a pandas/_libs/internals.pyx``` and took a screenshot of the before and after.

---

#### Master:

![orig_arange](https://user-images.githubusercontent.com/50263213/76964675-c4885280-692b-11ea-8ddf-b9eaf2ed0cd2.png)

---

#### PR:

![New_arange](https://user-images.githubusercontent.com/50263213/76964722-d7028c00-692b-11ea-970a-454cbb56adcf.png)


---

Also, is there a reason not to replace every call of ```np.arange``` with ```cnp.PyArray_Arange```? (cython files only)"
378716465,23565,API / internals: exact semantics of _ndarray_values,jorisvandenbossche,closed,2018-11-08T12:47:07Z,2020-03-19T00:24:24Z,"We need to better describe the exact semantics of `_ndarray_values`: what is it expected to return and how it is used.

Currenlty it is defined on the ExtensionArray, but mentioned it is not part of the ""official"" interface:

https://github.com/pandas-dev/pandas/blob/712fa945c878eaed18f79d4cf99ed91e464d51b1/pandas/core/arrays/base.py#L687-L697

One Series/Index, the property will either give you what `EA._ndarray_values` gives, or the underlying ndarray:

https://github.com/pandas-dev/pandas/blob/712fa945c878eaed18f79d4cf99ed91e464d51b1/pandas/core/base.py#L768-L780

---

What it currently is for the EAs:

* Categorical: integer codes
* IntegerArray: the integer `_data`, so but losing any information about missing values
* PeriodArray: the integer ordinals
* IntervalIndex: object array of Interval objects

---

For what it is currently used (this needs to be better looked at, copying now from https://github.com/pandas-dev/pandas/issues/19954#issuecomment-436374598, quoting Tom here):

- Index.itemsize (deprecated)
- Index.strides (deprecated)
- Index._engine
- Index set ops
- Index.insert
- DatetimeIndex.unique
- MultiIndex.equals
- pytables._convert_index (shared across integer and period)

There are a few other uses (mostly datetime / timedelta / period) that could maybe uses asi8 instead. I'm not familiar enough with indexing to know whether that can operate on something other than ndarrays. In theory, EAs can implement the buffer protocol, which would get the data to cython. But I don't know what ops would be required when we're down there.

"
488317421,28256,Timedelta in to_json object array and ISO dates not handled properly,WillAyd,closed,2019-09-02T21:04:56Z,2020-03-19T00:57:22Z,"Intertwined with #15137 though a slight different issue.

`date_format=""iso""` has different behavior for Timedeltas depending on whether or not the Timedelta is in a DTA or an object array. To illustrate:

```python
# Wrong format ref 15137, but at least tries to do some formatting
>>> pd.DataFrame([[pd.Timedelta(""1D"")]]).to_json(date_format=""iso"")
'{""0"":{""0"":""1970-01-02T00:00:00.000Z""}}'

# Object array has no formatting
>>> pd.DataFrame([[pd.Timedelta(""1D"")]]).astype(object).to_json(date_format=""iso"")
'{""0"":{""0"":86400000}}'
```

By contrast the same issue does not appear with datetimes
```python
>>> pd.DataFrame([[pd.Timestamp(1)]]).to_json(date_format=""iso"")
'{""0"":{""0"":""1970-01-01T00:00:00.000Z""}}'

# Below still formats as iso in spite of being object array
>>> pd.DataFrame([[pd.Timestamp(1)]]).astype(object).to_json(date_format=""iso"")
'{""0"":{""0"":""1970-01-01T00:00:00.000Z""}}'
```"
582714419,32768,CLN: remove _ndarray_values,jbrockmendel,closed,2020-03-17T02:47:51Z,2020-03-19T01:30:09Z,"- [x] closes #23565
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
580730707,32684,BUG: Series.__getitem__ with downstream scalars,jbrockmendel,closed,2020-03-13T17:02:56Z,2020-03-19T01:38:15Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @spencerclark I think this fixes a subset of the issues reported in https://github.com/pydata/xarray/issues/3751, can you confirm?

@jorisvandenbossche IIRC geopandas scalars not being recognized by lib.is_scalar has caused some issues there; does this address any of those?"
382225175,23784,read_csv throws wrong exception on permissions issue,omri374,closed,2018-11-19T13:54:33Z,2020-03-19T01:40:41Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
train_df = pd.read_csv(""../input/train.csv"")
```
#### Problem description
When trying to open a CSV with no permissions on ubuntu 16.04, pandas returns this error:
OSError: Initializing from file failed

see this SO thread: https://stackoverflow.com/questions/50552404/oserror-initializing-from-file-failed-on-csv-in-pandas for an example.


#### Expected Output
Expected exception or details of exception should specify that this is a permission error, e.g. PermissionError and not OSError.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.7.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-39-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: None
pip: 18.1
setuptools: 40.5.0
Cython: None
numpy: 1.15.4
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 7.1.1
sphinx: None
patsy: None
dateutil: 2.7.5
pytz: 2018.7
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
582813247,32770,CLN: Consolidate numba facilities,mroeschke,closed,2020-03-17T07:35:10Z,2020-03-19T03:49:06Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Precursor to https://github.com/pandas-dev/pandas/issues/31845 and other numba engine additions, creates `pandas/core/numba_.py` (open to move elsewhere) as a shared place for common numba operations like default parameters and jitting functions."
584198874,32820,See also,farhanreynaldo,closed,2020-03-19T06:46:40Z,2020-03-19T11:09:03Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. 

Output for pandas.Series.corr, pandas.Series.cov, pandas.Series.count:
```
################################################################################
################################## Validation ##################################
################################################################################

1 Errors found:
        No extended summary found
```"
584214725,32821,PERF: fix SparseArray._simple_new object initialization,jorisvandenbossche,closed,2020-03-19T07:26:36Z,2020-03-19T12:02:46Z,"Apart from this being more idiomatic, it also avoids creating a SparseArray through the normal machinery (including validation of the input etc) for the empty list.

With this PR:
```
In [1]: data = np.array([1, 2, 3], dtype=float)  

In [2]: index = pd.core.arrays.sparse.IntIndex(5, np.array([0, 2, 4]))  

In [3]: dtype = pd.SparseDtype(""float64"", 0)      

In [4]: pd.arrays.SparseArray._simple_new(data, index, dtype)  
Out[4]: 
[1.0, 0, 2.0, 0, 3.0]
Fill: 0
IntIndex
Indices: array([0, 2, 4], dtype=int32)

In [5]: %timeit pd.arrays.SparseArray._simple_new(data, index, dtype)    
381 ns ± 4.83 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
```

while on released version this gives around 50µs (100x slower)

Noticed while investigating https://github.com/pandas-dev/pandas/issues/32196"
583387686,32797,Avoid bare pytest.raises in indexes/categorical/test_indexing.py,Vlek,closed,2020-03-18T00:49:39Z,2020-03-19T13:41:03Z,"* [x]  ref #30999
 
* [x]  tests added / passed
 
* [x]  passes `black pandas`

* [x]  passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
584190182,32819,xcel into csv,Fdelahoya,closed,2020-03-19T06:22:12Z,2020-03-19T13:42:38Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here

```
#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
584030487,32814,PERF: Using Numpy C-API for left-join calls,ShaharNaveh,closed,2020-03-18T22:03:46Z,2020-03-19T13:43:09Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

#### Benchmarks:

This is the setup:

```python
import pandas as pd

df1 = pd.DataFrame(
    {
        ""Customer_id"": pd.Series([1, 2, 3, 4, 5, 6]),
        ""Product"": pd.Series(
            [""Oven"", ""Oven"", ""Oven"", ""Television"", ""Television"", ""Television""]
        ),
    }
)

df2 = pd.DataFrame(
    {
        ""Customer_id"": pd.Series([2, 4, 6]),
        ""State"": pd.Series([""California"", ""California"", ""Texas""]),
    }
)
```

And the ```%timeit```:

```python
pd.merge(df1, df2, on=""Customer_id"", how=""left"")   
```

---

```
In [4]: %timeit pd.merge(df1, df2, on=""Customer_id"", how=""left"")
1.54 ms ± 102 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) # Master
1.46 ms ± 21.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) # PR

In [5]: %timeit pd.merge(df1, df2, on=""Customer_id"", how=""left"")
1.47 ms ± 790 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each) # Master
1.45 ms ± 2.59 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) # PR

In [6]: %timeit pd.merge(df1, df2, on=""Customer_id"", how=""left"")
1.47 ms ± 1.99 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) # Master
1.48 ms ± 65 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) # PR
```
Since the results were so small I ran the same test several times:"
581755748,32730,TYP: annotate,jbrockmendel,closed,2020-03-15T17:34:08Z,2020-03-19T15:43:49Z,"cc @simonjayhawkins had to revert some annotations because mypy couldn't tell that ArrayLike had ""copy"" or ""view"".  Is a fix for that depending on numpy making annotations/stubs?"
580797110,32687,TST: Parametrize in pandas/tests/internals/test_internals.py,ShaharNaveh,closed,2020-03-13T19:27:07Z,2020-03-19T17:54:09Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
584554791,32829,TYP: update setup.cfg,simonjayhawkins,closed,2020-03-19T16:51:31Z,2020-03-19T18:50:47Z,
584553315,32828,CLN: Update docstring decorator from Appender to doc,raisadz,closed,2020-03-19T16:49:21Z,2020-03-19T18:56:42Z,"- [ ] xref #31942
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
581848896,32737,BUG: read_csv: fix wrong exception on permissions issue ,roberthdevries,closed,2020-03-15T22:22:11Z,2020-03-19T19:07:07Z,"Get rid of all error printf's and produce proper Python exceptions

- [x] closes #23784
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
581893898,32741,Strange behavior on saving a dataframe containing = at the start of a cell value,AMR-KELEG,closed,2020-03-16T00:33:01Z,2020-03-19T19:16:11Z,"```python
import pandas as pd

index = [0, 1, 2]
text = ['====== 2', '======= abd', 'abc']
df = pd.DataFrame({'index':index, 'text': text})
df.to_excel('temp.xlsx', index=False)
print(df.head())

loaded_df = pd.read_excel('temp.xlsx')
print(loaded_df.head())

```
#### Problem description
Excel sheets interpret an `=` sign at the start of the cell as a signal for writing equations. If a column contains text data where one of the values start with `=` then the values are corrupted after saving the dataframe to an excel sheet.

#### Expected Output
```
   index         text
0      0     ====== 2
1      1  ======= abd
2      2          abc
```

#### Output after loading the exported xlsx sheet
```
   index text
0      0    0
1      1    0
2      2  abc
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 059f9bff62d3bfe53eeece409b48b879abd1a3b1
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-88-generic
Version          : #88-Ubuntu SMP Tue Feb 11 20:11:34 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : C.UTF-8
LANG             : en_US.utf8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0.dev0+793.g059f9bff6
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.0.0
Cython           : 0.29.15
pytest           : 5.4.1
hypothesis       : 5.6.0
sphinx           : 2.4.4
blosc            : 1.8.3
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.2
fastparquet      : 0.3.3
gcsfs            : None
matplotlib       : 3.2.0
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0
</details>
"
416209839,25509,Dir fails on dataframes with pathological column names,mrocklin,closed,2019-03-01T17:42:53Z,2020-03-19T19:50:39Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
df = pd.DataFrame({'\ud83d': []})
_ = dir(df)
```

```python-traceback
---------------------------------------------------------------------------
UnicodeEncodeError                        Traceback (most recent call last)
<ipython-input-3-4e949fe17c82> in <module>
----> 1 _ = dir(df)

~/miniconda/envs/dev/lib/python3.7/site-packages/pandas/core/accessor.py in __dir__(self)
     37         """"""
     38         rv = set(dir(type(self)))
---> 39         rv = (rv - self._dir_deletions()) | self._dir_additions()
     40         return sorted(rv)
     41

~/miniconda/envs/dev/lib/python3.7/site-packages/pandas/core/generic.py in _dir_additions(self)
   5110         If info_axis is a MultiIndex, it's first level values are used.
   5111         """"""
-> 5112         additions = {c for c in self._info_axis.unique(level=0)[:100]
   5113                      if isinstance(c, string_types) and isidentifier(c)}
   5114         return super(NDFrame, self)._dir_additions().union(additions)

~/miniconda/envs/dev/lib/python3.7/site-packages/pandas/core/indexes/base.py in unique(self, level)
   1999         if level is not None:
   2000             self._validate_index_level(level)
-> 2001         result = super(Index, self).unique()
   2002         return self._shallow_copy(result)
   2003

~/miniconda/envs/dev/lib/python3.7/site-packages/pandas/core/base.py in unique(self)
   1312         else:
   1313             from pandas.core.algorithms import unique1d
-> 1314             result = unique1d(values)
   1315
   1316         return result

~/miniconda/envs/dev/lib/python3.7/site-packages/pandas/core/algorithms.py in unique(values)
    360
    361     table = htable(len(values))
--> 362     uniques = table.unique(values)
    363     uniques = _reconstruct_data(uniques, dtype, original)
    364     return uniques

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.StringHashTable.unique()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.StringHashTable._unique()

UnicodeEncodeError: 'utf-8' codec can't encode character '\ud83d' in position 0: surrogates not allowed
```

#### Problem description

Dir fails on dataframes with pathalogical column names

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.1.final.0
python-bits: 64
OS: Darwin
OS-release: 18.2.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.1
pytest: 3.10.1
pip: 18.1
setuptools: 40.6.2
Cython: None
numpy: 1.15.4
scipy: 1.1.0
pyarrow: 0.11.1
xarray: 0.11.3
IPython: 7.2.0
sphinx: 1.8.4
patsy: None
dateutil: 2.7.5
pytz: 2018.7
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: 0.2.0
fastparquet: 0.1.6
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
569173123,32163,Error on C Warnings,WillAyd,closed,2020-02-21T20:39:06Z,2020-03-19T21:31:19Z,"Using Clang and latest numpy locally looks like we can use `-Werror`. WIP as I think there may still be things with gcc and older numpy versions to sort out

Note that there is also a warning emitted by cython for unreachable code in groupby, but -Werror wouldn't affect the ""cythonization"" step.
"
579252850,32622,df.index.is_unique inflates index memory ,meizy,closed,2020-03-11T13:11:04Z,2020-03-11T14:55:34Z,"calling is_unique on an Int64Index seems to inflate the index memory.

```python
l = np.arange(9)
df = pd.DataFrame({'a':l, 'b':l, 'c':l})
print('shape', df.shape)

display(df.memory_usage(deep=True, index=True))
print()

print('drop one row')
df.drop(index=[1], inplace=True)
display(df.memory_usage(deep=True, index=True))
print()

print('is_unique: ', df.index.is_unique)
display(df.memory_usage(deep=True, index=True))
print()

print('drop another row')
df.drop(index=[3], inplace=True)
display(df.memory_usage(deep=True, index=True))
print()

print('is_unique: ', df.index.is_unique)
display(df.memory_usage(deep=True, index=True))
```

output:
```
shape (9, 3)
Index    80
a        36
b        36
c        36
dtype: int64

drop one row
Index    64
a        32
b        32
c        32
dtype: int64

is_unique:  True
Index    384
a         32
b         32
c         32
dtype: int64

drop another row
Index    56
a        28
b        28
c        28
dtype: int64

is_unique:  True
Index    376
a         28
b         28
c         28
dtype: int64
```

#### Problem description

any explanation why the index size grows after the call to index.is_unique?

#### Expected Output

no change in index size is expected

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.9.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.24.2
pytest: None
pip: 19.1.1
setuptools: 39.1.0
Cython: None
numpy: 1.17.0
scipy: None
pyarrow: None
xarray: None
IPython: 7.7.0
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.1.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: 4.3.4
bs4: None
html5lib: None
sqlalchemy: 1.3.12
pymysql: 0.9.3
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
558422997,31524,BUG: non-iterable value in meta raise error in json_normalize,charlesdong1991,closed,2020-01-31T23:14:26Z,2020-03-11T15:02:43Z,"- [x] closes #31507 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
578171061,32563,Fix warning in io/excel/test_openpyxl,roberthdevries,closed,2020-03-09T20:28:59Z,2020-03-11T15:47:52Z,as requested in PR #32544
577503065,32541,"Fix failure to convert string ""uint64"" to NaN",roberthdevries,closed,2020-03-08T13:44:42Z,2020-03-11T15:48:23Z,"Including regression test

- [x] closes #32394
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
579344596,32630,TST: revert parts of #32571,jbrockmendel,closed,2020-03-11T15:21:48Z,2020-03-11T16:40:59Z,Should fix the broken CI builds.
579331370,32629,Backport PR #31524 on branch 1.0.x (BUG: non-iterable value in meta raise error in json_normalize),meeseeksmachine,closed,2020-03-11T15:03:18Z,2020-03-11T18:11:17Z,Backport PR #31524: BUG: non-iterable value in meta raise error in json_normalize
561949344,31802,REGR: cumsum regression with groupby call to agg,mattharrison,closed,2020-02-08T01:30:16Z,2020-03-11T18:34:48Z,"#### Code Sample, a copy-pastable example if possible

I want to define a custom function that I can pass to the `agg` method. It uses the `cumsum` method, which appears to be problematic recently.

```python
# Your code here
import pandas as pd

def max_test(s):
    return s.cumsum().max()
    #return s.max()

dummy_data = pd.DataFrame(
    {'AIRLINE': {0: 'WN', 1: 'UA', 2: 'MQ', 3: 'AA', 4: 'WN'},
     'ORG_AIR': {0: 'LAX', 1: 'DEN', 2: 'DFW', 3: 'DFW', 4: 'LAX'},
     'DIST': {0: 590, 1: 1452, 2: 641, 3: 1192, 4: 1363}})

gb = dummy_data.groupby(['AIRLINE', 'ORG_AIR'])

result = gb.agg(
    #'max' 
    max_test
)

print(result)
```
#### Problem description

Prior to Pandas 1.0rc this worked. It now raises an exception:

```
$ python /tmp/regpandas.py
Traceback (most recent call last):
  File ""/tmp/regpandas.py"", line 16, in <module>
    max_test
  File ""/Users/matt/.env/pandas1/lib/python3.7/site-packages/pandas/core/groupby/generic.py"", line 948, in aggregate
    return self._python_agg_general(func, *args, **kwargs)
  File ""/Users/matt/.env/pandas1/lib/python3.7/site-packages/pandas/core/groupby/groupby.py"", line 936, in _python_agg_general
    result, counts = self.grouper.agg_series(obj, f)
  File ""/Users/matt/.env/pandas1/lib/python3.7/site-packages/pandas/core/groupby/ops.py"", line 641, in agg_series
    return self._aggregate_series_fast(obj, func)
  File ""/Users/matt/.env/pandas1/lib/python3.7/site-packages/pandas/core/groupby/ops.py"", line 666, in _aggregate_series_fast
    result, counts = grouper.get_result()
  File ""pandas/_libs/reduction.pyx"", line 376, in pandas._libs.reduction.SeriesGrouper.get_result
  File ""pandas/_libs/reduction.pyx"", line 193, in pandas._libs.reduction._BaseGrouper._apply_to_group
  File ""/Users/matt/.env/pandas1/lib/python3.7/site-packages/pandas/core/groupby/groupby.py"", line 913, in <lambda>
    f = lambda x: func(x, *args, **kwargs)
  File ""/tmp/regpandas.py"", line 4, in max_test
    return s.cumsum().max()
  File ""/Users/matt/.env/pandas1/lib/python3.7/site-packages/pandas/core/generic.py"", line 11331, in cum_func
    result = self._data.apply(na_accum_func)
  File ""/Users/matt/.env/pandas1/lib/python3.7/site-packages/pandas/core/internals/managers.py"", line 440, in apply
    applied = b.apply(f, **kwargs)
  File ""/Users/matt/.env/pandas1/lib/python3.7/site-packages/pandas/core/internals/blocks.py"", line 403, in apply
    result = self.make_block(values=_block_shape(result, ndim=self.ndim))
  File ""/Users/matt/.env/pandas1/lib/python3.7/site-packages/pandas/core/internals/blocks.py"", line 273, in make_block
    return make_block(values, placement=placement, ndim=self.ndim)
  File ""/Users/matt/.env/pandas1/lib/python3.7/site-packages/pandas/core/internals/blocks.py"", line 3041, in make_block
    return klass(values, ndim=ndim, placement=placement)
  File ""/Users/matt/.env/pandas1/lib/python3.7/site-packages/pandas/core/internals/blocks.py"", line 125, in __init__
    f""Wrong number of items passed {len(self.values)}, ""
ValueError: Wrong number of items passed 2, placement implies 1
```


#### Expected Output

```
$ python /tmp/regpandas.py
                 DIST
AIRLINE ORG_AIR
AA      DFW      1192
MQ      DFW       641
UA      DEN      1452
WN      LAX      1953
```



#### Output of ``pd.show_versions()``

```
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.6.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.0.3
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```

"
578133428,32561,Ensure valid Block mutation in SeriesBinGrouper.,TomAugspurger,closed,2020-03-09T19:16:11Z,2020-03-11T18:34:49Z,"Closes https://github.com/pandas-dev/pandas/issues/31802

~This ""fixes"" #31802 by expanding the number of cases where we swallow an
exception in libreduction. Currently, we're creating an invalid Series
in SeriesBinGrouper where the `.mgr_locs` doesn't match the values. See
https://github.com/pandas-dev/pandas/issues/31802#issuecomment-595954511
for more.~

~For now, we simply catch more cases that fall back to Python. I've gone
with a minimal change which addresses only issues hitting this exact
exception. We might want to go broader, but that's not clear.~


cc @jbrockmendel & @WillAyd "
579440728,32634,pandas plotting - matplotlib 3.2.0 deprecation warning for rowNum/colNum attributes,pythonic2020,closed,2020-03-11T17:48:09Z,2020-03-11T19:01:14Z,"With pandas 1.0.1 and matplotlib 3.2.0 in a conda Python 3.7.6 environment, I'm getting warnings about deprecation of rowNum and colNum attributes when using some pandas plotting routines.

Environment:
```
c> conda list ""python|pandas|matplotlib|numpy|conda""
# packages in environment at C:\Users\me\Miniconda3\envs\forge:
#
# Name                    Version                   Build  Channel
conda                     4.8.2                    py37_0
conda-package-handling    1.6.0            py37h62dcd97_0
ipython                   7.13.0           py37h5ca1d4c_0    conda-forge
ipython_genutils          0.2.0                    py37_0
matplotlib                3.2.0                         1    conda-forge
matplotlib-base           3.2.0            py37h2981e6d_1    conda-forge
msys2-conda-epoch         20160418                      1
numpy                     1.18.1           py37h93ca92e_0
numpy-base                1.18.1           py37hc3f5095_1
numpydoc                  0.9.2                      py_0
pandas                    1.0.1            py37h47e9c7a_0
python                    3.7.6                h60c2a47_2
python-dateutil           2.8.1                      py_0
python-jsonrpc-server     0.3.4                      py_0
python-language-server    0.31.7                   py37_0
python_abi                3.7                     1_cp37m    conda-forge
```
Warnings:
```
C:\Users\me\Miniconda3\envs\forge\lib\site-packages\pandas\plotting\_matplotlib\tools.py:298: MatplotlibDeprecationWarning: 
The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead.
  layout[ax.rowNum, ax.colNum] = ax.get_visible()
C:\Users\me\Miniconda3\envs\forge\lib\site-packages\pandas\plotting\_matplotlib\tools.py:298: MatplotlibDeprecationWarning: 
The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead.
  layout[ax.rowNum, ax.colNum] = ax.get_visible()
```

The above warnings were generated when running this code:

```
iris.plot(subplots=True)
plt.suptitle('Iris data by index', x=0.5, y=0.925)

# Display the plot
plt.show()
```

and this code:

```
df = pd.DataFrame(np.random.randn(250, 5), columns=['A','B','C','D', 'class'])
df['class'] = pd.Series(np.random.randint(0,2, 250))
cmap_tab10 = plt.get_cmap(""tab10"")
pd.plotting.scatter_matrix(df.iloc[:,0:4], color=cmap_tab10(df['class']), alpha=0.2)
for ax in plt.gcf().axes:
    plt.sca(ax)
    plt.ylabel(ax.get_ylabel(), rotation=0, labelpad=8, ha=""right"")
    plt.grid(False)
plt.show()
```
In both of the above cases, the plots did draw as expected.  "
578701378,32585,March 2020 Dev Meeting,TomAugspurger,closed,2020-03-10T16:18:40Z,2020-03-11T19:05:11Z,"The monthly dev meeting is tomorrow at 18:00 UTC. Note that your local time may be different than last month. The US had a DST transition last weekend, while Europe didn't. Our calendar is at https://pandas.pydata.org/docs/development/meeting.html#calendar

Video Call: https://meet.google.com/hav-rmax-zjx
Minutes: https://docs.google.com/document/u/1/d/1tGbTiYORHiSPgVMXawiweGJlBw5dOkVJLY-licoBmBU/edit?ouid=102771015311436394588&usp=docs_home&ths=true

Please add items you'd like to see discussed to the agenda."
579465955,32635,Backport PR #32561 on branch 1.0.x (Ensure valid Block mutation in SeriesBinGrouper.),meeseeksmachine,closed,2020-03-11T18:35:00Z,2020-03-11T19:32:13Z,Backport PR #32561: Ensure valid Block mutation in SeriesBinGrouper.
579081299,32615,PARSE ERROR,SubhodeepSinha,closed,2020-03-11T08:12:49Z,2020-03-11T21:46:21Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here

PLEASE HELP!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
CODE::
import pandas as pd
file1 = r'D:\C_pH\Book1 Lat_LongVspH.csv';
df = pd.read_csv(file1)




**#### Output** 

ParserError                               Traceback (most recent call last)
<ipython-input-2-21d97f424a6d> in <module>
----> 1 df = pd.read_csv(file1)

C:\Users\KIIT\AppData\Roaming\Python\Python37\site-packages\pandas\io\parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)
    683         )
    684 
--> 685         return _read(filepath_or_buffer, kwds)
    686 
    687     parser_f._name_ = name

C:\Users\KIIT\AppData\Roaming\Python\Python37\site-packages\pandas\io\parsers.py in _read(filepath_or_buffer, kwds)
    461 
    462     try:
--> 463         data = parser.read(nrows)
    464     finally:
    465         parser.close()

C:\Users\KIIT\AppData\Roaming\Python\Python37\site-packages\pandas\io\parsers.py in read(self, nrows)
   1152     def read(self, nrows=None):
   1153         nrows = _validate_integer(""nrows"", nrows)
-> 1154         ret = self._engine.read(nrows)
   1155 
   1156         # May alter columns / col_dict

C:\Users\KIIT\AppData\Roaming\Python\Python37\site-packages\pandas\io\parsers.py in read(self, nrows)
   2057     def read(self, nrows=None):
   2058         try:
-> 2059             data = self._reader.read(nrows)
   2060         except StopIteration:
   2061             if self._first_chunk:

pandas\_libs\parsers.pyx in pandas._libs.parsers.TextReader.read()

pandas\_libs\parsers.pyx in pandas._libs.parsers.TextReader._read_low_memory()

pandas\_libs\parsers.pyx in pandas._libs.parsers.TextReader._read_rows()

pandas\_libs\parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows()

pandas\_libs\parsers.pyx in pandas._libs.parsers.raise_parser_error()

ParserError: Error tokenizing data. C error: Expected 1 fields in line 4, saw 3



"
564007092,31925,pandas.DataFrame.reindex_like throws if called from a derived class instance,lguyot,closed,2020-02-12T13:40:15Z,2020-03-11T21:51:40Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import pandas.testing as pdt

class MyDataFrame(pd.DataFrame):
    pass

mdf = MyDataFrame()
df = pd.DataFrame()
# In DataFrame.reindex_like, the derived class is compared to
# the base class. The following line will throw
# 'AssertionError: <class 'pandas.core.frame.DataFrame'>'
mdf.reindex_like(df)

# In pdt.assert_frame_equal,
# check_like=True triggers a call to DataFrame.reindex_like.
# The following line will throw the same
# 'AssertionError: <class 'pandas.core.frame.DataFrame'>'
pdt.assert_frame_equal(mdf, df, check_like=True)
```
#### Problem description

The method `reindex_like` throws an `AssertionError` when the caller is an instance of a derived class of `pandas.DataFrame`.

#### Expected Output

No exception thrown.

#### Output of ``pd.show_versions()``
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-65-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : fr_FR.UTF-8
LOCALE           : fr_FR.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 9.0.1
setuptools       : 39.0.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 0.999999999
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
572298680,32308,DOC: Consolidate pages under /docs/getting_started,mroeschke,closed,2020-02-27T19:18:24Z,2020-03-11T22:15:21Z,"Another piece of feedback from the CZI EOSS kickoff meeting that @WillAyd and I attended:

In https://pandas.io/docs/getting_started/index.html, we link to the following pages in the left side:

<img width=""1429"" alt=""Screen Shot 2020-02-27 at 11 09 33 AM"" src=""https://user-images.githubusercontent.com/10647082/75478083-2e779280-5952-11ea-9cf9-d49c308a69f1.png"">

We got feedback to:

1. Have a dedicated link to the pandas cheat sheet: https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf
2. Consolidate `Getting started tutorials`, `Tutorials`, and `10 minutes to pandas` (which is linked already in `Tutorials`) into a singular `Tutorials` page
"
573769857,32389,DOC: Reorganize Getting Started documentation pages,mroeschke,closed,2020-03-02T07:24:43Z,2020-03-11T22:22:19Z,"- [x] closes #32308 

Issue Related:
- `getting_started/index.html` removes `10 minutes to pandas`, `Essential basic functionality` and `Intro to data structures` from the toctree and moves them as links in `user_guide/index.html`
- `getting_started/tutorials.html` now only references community tutorials
- the pandas cheatsheet is now referenced at the bottom `Tutorial` section of `getting_started/index.html`

Misc:
- Lowercases `Pandas` to `pandas`
- Some rephrasing
"
327282896,21244,BUG: `read_stata` always uses 'utf8',adrian-castravete,closed,2018-05-29T11:39:28Z,2020-03-11T23:17:19Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas
data = pandas.read_stata(file_with_latin1_encoding, chunksize=1048576)
for chunk in data:
    pass # do something with chunk (never reached)
```
This raises `UnicodeDecodeError: 'utf8' codec can't decode byte 0x?? in position ?: invalid start byte`.
OK. So the file isn't a **utf8** one. Even though the StataReader doesn't specify any Unicode support; I then try and open it with a **latin-1** encoding:
```python
import pandas
data = pandas.read_stata(file_with_latin1_encoding, chunksize=1048576, encoding='latin-1')
for chunk in data:
    pass # do something with chunk (never reached)
```
This raises the same exception at exactly the same place (still **utf-8**).

#### Problem description

This is a problem because it appears that `read_stata` doesn't honour the `encoding` argument.
I think this line introduced a bug. The `StataReader` doesn't manage any other type of data than **ascii** or **latin-1**.

Changing the line **1338** of the `pandas.io.stata` module:
```python
        return s.decode('utf-8')
```
to:
```python
        return s.decode('latin-1')
```
Seemed to make everything work and I could read the data from the given file.
Even better, changing it to the following:
```python
        return s.decode(self._encoding or self._default_encoding)
```
also seems to have made it work.

I believe though, that if you want to make this work with **Unicode** too you'd have to add the following encodings to `VALID_ENCODINGS`: **utf-8**, **utf8**, **iso10646**.

#### Expected Output
The file should be correctly read and parsed

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.12.final.0
python-bits: 64
OS: Linux
OS-release: 4.10.0-37-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: ro_RO.UTF-8
LANG: ro_RO.UTF-8
LOCALE: None.None

pandas: 0.24.0.dev0+41.gb2eec25
pytest: 3.2.3
pip: 9.0.3
setuptools: 36.6.0
Cython: 0.28.2
numpy: 1.13.3
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 5.1.0
sphinx: 1.6.3
patsy: None
dateutil: 2.7.3
pytz: 2017.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: 2.4.9
xlrd: 1.0.0
xlwt: 1.3.0
xlsxwriter: None
lxml: 3.8.0
bs4: None
html5lib: 0.999999999
sqlalchemy: 1.1.13
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
579567624,32637,Backport PR #32633: TYP: Remove  _ensure_type,TomAugspurger,closed,2020-03-11T21:56:16Z,2020-03-12T02:33:57Z,https://github.com/pandas-dev/pandas/pull/32633
579149111,32618,DOC: Clarify pivot_table fill_value description,taljaards,closed,2020-03-11T10:14:22Z,2020-03-11T23:25:54Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

Clarify that `fill_value` gets used after performing the aggregation (i.e. it fills values in the resulting pivot table).
(Depending on the aggfunc and df, the result will be different if performing the filling before/after aggregation).

---

### Actually, why not remove it, requiring the user to more explicitly do `df.pivot_table(...).fillna(...)`?"
578067865,32555,CLN: avoid _internal_get_values in Categorical.__iter__,jbrockmendel,closed,2020-03-09T17:23:13Z,2020-03-11T23:45:34Z,
565171762,31971,shift() broken for datetime64 when used with fill_value,BayerSe,closed,2020-02-14T08:16:24Z,2020-03-12T02:26:13Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
from pandas.testing import assert_series_equal

dt_series = pd.Series((pd.to_datetime('2020-01-01'), pd.to_datetime('2020-01-02')))

actual_result = dt_series.shift(1, fill_value=0)
expected_result = pd.Series((pd.to_datetime('1970-01-01'), pd.to_datetime('2020-01-01')))

assert_series_equal(actual_result, expected_result)
```
#### Problem description

Above code snippet works on version 0.25.3 but breaks with 1.0.1.

```
print(actual_result)
0                      0
1    1577836800000000000
dtype: object

print(expected_result)
0   1970-01-01
1   2020-01-01
dtype: datetime64[ns]
```

```
AssertionError: Attributes of Series are different
Attribute ""dtype"" are different
[left]:  object
[right]: datetime64[ns]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-72-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : 5.3.2
hypothesis       : None
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.3.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None


</details>
"
569949721,32219,calling `mean` on a `DataFrameGroupBy` with `Int64` dtype results in `TypeError` ,mojones,closed,2020-02-24T15:54:58Z,2020-03-12T02:28:59Z,"```python
import pandas as pd

df = pd.DataFrame({
    'a' : [0,0,1,1,2,2,3,3],
    'b' : [1,2,3,4,5,6,7,8]
},
dtype='Int64')

df.groupby('a').mean()

```
#### Problem description

Using the new nullable integer data type, calling `mean` after grouping results in a `TypeError`. Using `int64` dtype it works:

```python
import pandas as pd

df = pd.DataFrame({
    'a' : [0,0,1,1,2,2,3,3],
    'b' : [1,2,3,4,5,6,7,8]
},
dtype='int64')

print(df.groupby('a').mean())
```

as does keeping `Int64` dtype but taking a single column to give a `SeriesGroupBy`:

```python
import pandas as pd

df = pd.DataFrame({
    'a' : [0,0,1,1,2,2,3,3],
    'b' : [1,2,3,4,5,6,7,8]
},
dtype='Int64')

print(df.groupby('a')['b'].mean())
```

The error does not occur when calling `min`, `max` or `first`, but does also occur with `median` and `std`.

#### Expected Output

```
     b
a     
0  1.5
1  3.5
2  5.5
3  7.5
```


#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-74-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : None
pytest           : 5.3.4
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.3.3
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.3
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.4
pyxlsb           : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
561673055,31777,GroupBy aggregation of DataFrame with MultiIndex columns breaks with custom function,sbitzer,closed,2020-02-07T14:33:09Z,2020-03-12T02:32:55Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame(
    np.random.rand(10, 4),
    columns=pd.MultiIndex.from_product([[1, 2], [3, 4]]))
grp = df.groupby(np.r_[np.ones(5), np.zeros(5)])
grp.agg(lambda s: s.mean())
```
#### Problem description
The above call raises
```python
ValueError: Length mismatch: Expected axis has 4 elements, new values have 2 elements
```
because
https://github.com/pandas-dev/pandas/blob/cf01369d5d82362565d5c58ee4d6094782b0f8e1/pandas/core/groupby/generic.py#L970-L972

assumes that the original columns were only `Index`. Doing
```python
grp.agg('mean')
```
works as expected (result with MultiIndex columns).

#### Expected Output
That of
```python
grp.agg('mean')
```

#### Output of ``pd.show_versions()``

<details>

commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 7
machine          : AMD64
processor        : Intel64 Family 6 Model 42 Stepping 7, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200127
Cython           : None
pytest           : 5.3.4
hypothesis       : None
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.4
pyxlsb           : None
s3fs             : None
scipy            : 1.3.2
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
579610041,32643,CLN: trim unnecessary checks,jbrockmendel,closed,2020-03-11T23:57:12Z,2020-03-12T02:41:44Z,
579650889,32647,Backport PR #32591 on branch 1.0.x (REG: dt64 shift with integer fill_value),meeseeksmachine,closed,2020-03-12T02:26:57Z,2020-03-12T04:26:18Z,Backport PR #32591: REG: dt64 shift with integer fill_value
579652741,32648,Backport PR #32040 on branch 1.0.x (BUG: GroupBy aggregation of DataFrame with MultiIndex columns breaks with custom function),meeseeksmachine,closed,2020-03-12T02:33:39Z,2020-03-12T04:26:40Z,Backport PR #32040: BUG: GroupBy aggregation of DataFrame with MultiIndex columns breaks with custom function
554547536,31270,"ENH: Move ""corrwith"" from transformation_kernels to reduction_kernels in groupby.base",fujiaxiang,closed,2020-01-24T05:08:26Z,2020-03-12T04:47:13Z,"#### Code Sample
```python
>>> import pandas as pd
>>> pd.__version__
'1.0.0rc0+183.g4da554f75'
>>> df = pd.DataFrame(
...     {
...         ""A"": [1, 1, 1, 1],
...         ""B"": [0, 1, 2, 3]
...     }
... )

>>> df
   A  B
0  1  0
1  1  1
2  1  2
3  1  3

>>> df.groupby(""A"").corrwith(df)  # Clearly corrwith is a reduction rather than transformation
     B   A
A
1  1.0 NaN
```

On current master (`1.0.0rc0+183.g4da554f75`) we have ""corrwith"" in the list of ``transformation_kernels`` rather than ``reduction_kernels`` within `groupby.base`.

We probably have to move it and fix any impact it has."
575584938,32439,DOC: Fix capitalization of the word pandas in the docs,joybh98,closed,2020-03-04T17:09:20Z,2020-03-12T04:49:31Z,"- [x] closes #32316 
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

As stated in the referred issue, _pandas_ reference in the docs should be standardised to pandas, not *pandas* or Pandas.

I've only changed the references in `doc/source/development/*.rst` for now, to see if the changes are okay or not"
107048858,11136,Plot uses incorrect colors when columns have the same name,stevenmanton,closed,2015-09-17T18:51:21Z,2020-03-12T04:52:29Z,"It seems like the `plot` function jumbles up the colors when columns in a DataFrame have the same name:

``` python
pd.concat([pd.DataFrame({'b': [0, 1, 0], 'a': [1, 2, 3]}), pd.DataFrame({'a': [2, 4, 6]})], axis=1).plot();
```

![image](https://cloud.githubusercontent.com/assets/3666725/9942860/5770cf74-5d32-11e5-98b9-904ded128f57.png)
"
575868172,32451,TST/VIZ: add test for legend colors for DataFrame with duplicate column labels #11136,gabrielvf1,closed,2020-03-04T23:16:33Z,2020-03-12T04:52:36Z,"- [X] closes #11136
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
403705384,24975,Different behaviour of set_index with single and multiple columns involving datetime,prakhar987,closed,2019-01-28T08:49:53Z,2020-03-12T04:57:17Z,"#### Code Sample, a copy-pastable example if possible

```python
import datetime
import pandas as pd
df = pd.DataFrame({'date':[datetime.date(2011,1,3),datetime.date(2011,1,4)], 'val':[100,200], 'val1':[9,99] })
df1 = df.set_index('date')
df2 = df.set_index(['date', 'val'])

```
#### Problem description
When setting index with only 'date' column, we get an index with no timestamps.( type is : datetime.date object)
When setting index with 'date' and some other column (multindex), 00:00:00 is introduced (type is : pd.Timestamp)

#### Output
```python
df1.index
Out[71]: Index([2011-01-03, 2011-01-04], dtype='object', name='date')

df2.index
Out[72]: 
MultiIndex(levels=[[2011-01-03 00:00:00, 2011-01-04 00:00:00], [100, 200]],
           labels=[[0, 1], [0, 1]],
           names=['date', 'val'])
```

Why does set_index modify the original format in second case ? .... I know one can manually change the type of index but it is a complicated process since multiindexes are involved
#### Output of ``pd.show_versions()``
pd version : 0.23.4
"
579523018,32636,TST: tighten check_categorical=False tests,jbrockmendel,closed,2020-03-11T20:20:36Z,2020-03-12T07:11:47Z,This removes all check_categorical=False usages except for a) those in tests/util and b) a skipped json test test_latin_encoding (cc @WillAyd is that likely to be enabled in the foreseeable future?)
577399493,32529,DOC: Remove absolute urls from the docs,datapythonista,closed,2020-03-07T20:58:40Z,2020-03-12T07:15:30Z,"Looks like we've got some absolute urls (including the domain) in the docs, like https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases (see here: https://github.com/pandas-dev/pandas/blob/master/pandas/core/indexes/datetimes.py#L178 ).

Sphinx (the tool we use to build the docs) provides a way to create those automatically, with the `:ref:` directive.

See for example how to automatically generate a link [here](https://github.com/pandas-dev/pandas/blame/master/doc/source/user_guide/computation.rst#L113):
```
:ref:`caveats <computation.covariance.caveats>`
```
Which will point to the label defined [here](https://github.com/pandas-dev/pandas/blame/master/doc/source/user_guide/computation.rst#L51):
```
.. _computation.covariance.caveats:
```

To avoid having new cases in the future after fixing the existing ones, we should add a check in `ci/code_checks.sh` to validate that the pattern `pandas.pydata.org` doesn't exist in the repository (see other examples of patterns we don't want to find in `ci/code_checks.sh` that use the `invgrep` function). If there is a better patter than `pandas.pydata.org` feel free to use it."
577449236,32539,DOC: Remove absolute urls from the docs,ArkadeepAdhikari,closed,2020-03-08T05:37:10Z,2020-03-12T08:03:38Z,"- [x] closes #32529
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
541706837,30428,"Error initialising DataFrame with array of shape (1, 1) and extensiondtype",MarcoGorelli,closed,2019-12-23T11:10:54Z,2020-03-12T10:20:45Z,"Here's something I noticed while working on  #30277

#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np
import pytz

temp_1 = pd.Series([pd.Timestamp(""2019-07-19 07:04:57"")])
temp_2 = pd.Series([pd.Timestamp(""2019-07-19 07:04:57+0100"", tz=pytz.FixedOffset(60))])
data = np.array([[""2019-06-19 07:04:57""]])
pd.DataFrame(data, dtype=temp_1.dtype)  # works
pd.DataFrame(data, dtype=temp_2.dtype)  # fails
```
#### Problem description

The example that fails throws the error:
```
ValueError: If using all scalar values, you must pass an index
```
If I pass an index:
```
pd.DataFrame(data, dtype=temp_2.dtype, index=[0])  # fails
```
then it fails with
```
ValueError: Buffer has wrong number of dimensions (expected 1, got 2)
```
although this doesn't:
```
pd.DataFrame(data, dtype=temp_1.dtype, index=[0])  # works
```
#### Expected Output
I wouldn't expect this behaviour to change based on the `dtype` passed.

#### Output of ``pd.show_versions()``

<details>

/home/SERILOCAL/m.gorelli/pandas/pandas/core/index.py:29: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.
  FutureWarning,

INSTALLED VERSIONS
------------------
commit           : 3577b5a34d7f9695bc2e99766cc6c49e7bdab6fe
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-72-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.25.0+1383.g3577b5a34
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 42.0.2.post20191201
Cython           : 0.29.14
pytest           : 5.3.2
hypothesis       : 4.55.4
sphinx           : 2.3.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.6
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : 1.3.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.3.2
s3fs             : 0.4.0
scipy            : 1.4.0
sqlalchemy       : 1.3.11
tables           : 3.6.1
xarray           : 0.14.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.6

</details>
"
559647113,31647,DOC: clean up DataFrame.ewm,MarcoGorelli,closed,2020-02-04T11:08:50Z,2020-03-12T10:22:03Z,"A few issues I've noticed:

1) It says

> When adjust is False, weighted averages are calculated recursively as:
> 
>     weighted_average[0] = arg[0]; weighted_average[i] = (1-alpha)*weighted_average[i-1] + alpha*arg[i].
> 

  but `arg` isn't defined

2) The phrase ""When adjust is False, weighted averages are calculated recursively as:"" appears as bold, but I don't think this was the intention

3) 
```
\text{for} halflife > 0
```
should be
```
\text{for } halflife > 0
```

4) In the ""notes"" section, ```alpha``` is used, while in the description of the parameters, it's
```
:math:`\alpha` 
```

5) The respective doc for Series could be updated, as it says
```
axis{0 or ‘index’, 1 or ‘columns’}, default 0
```"
579601919,32640,PERF: copy cached attributes on extension index shallow_copy,topper-123,closed,2020-03-11T23:29:15Z,2020-03-12T10:29:55Z,"Follow-up to #32568. 

Copies ``._cache`` also when copying using ``.shallow_copy`` for:
* CategoricalIndex
* DatetimeIndex
* PeriodIndex
* DateTimeIndex
* IntervalIndex

After this PR, only ``MultiIndex._shallow_copy`` is missing this optimization. ``MultiIndex._shallow_copy`` is a bit special and might require a refactor so I'd like to take that in a seperate PR.

Example performance improvement:
```pythn
>>> idx = pd.CategoricalIndex(np.arange(100_000))
>>> %timeit idx.get_loc(99_999)
4.46 µs ± 62.6 ns per loop  # master and this PR
>>> %timeit idx._shallow_copy().get_loc(99_999)
4.19 ms ± 117 µs per loop  # master
8.58 µs ± 254 ns per loop  # this PR
```"
579426267,32633,TYP: Remove  _ensure_type,simonjayhawkins,closed,2020-03-11T17:22:50Z,2020-03-12T11:21:36Z,"- [ ] closes #31925
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
514345743,29279,DataFrame.replace: TypeError: “Cannot compare types 'ndarray(dtype=object)' and 'str' ”,mlitvinov,closed,2019-10-30T02:04:16Z,2020-03-12T11:36:59Z,"I have a dataset in csv and a dictionary created by a classifier. It is necessary to put the key values in place of the keys . I use this code `df.replace(mapping,inplace = True) `and it works fine on 100 lines, but for 2000 and more, I get the error 

> ""TypeError:“ Cannot compare types 'ndarray (dtype = object)' and 'str' ”.

 What am I doing wrong ??

"
557656729,31467,pd.ExcelFile closes stream on destruction in pandas 1.0.0,johny-b,closed,2020-01-30T17:54:53Z,2020-03-12T12:18:21Z,"#### Code Sample, a copy-pastable example if possible

```python3

import pandas as pd
print(pd.__version__)
a = open('some_file.xlsx', 'rb')
x = pd.ExcelFile(a)
del x
print(a.read())
```


#### Problem description

Above script behaves in different way in pandas 0.25.3 and 1.0.0:

```bash
$ python3 t1.py
0.25.3
b''
$ sudo pip3 install pandas --upgrade --quiet
$ python3 t1.py
1.0.0
Traceback (most recent call last):
  File ""t1.py"", line 6, in <module>
    print(a.read())
ValueError: read of closed file
```

It seems that stream is closed when ExcelFile is destroyed - and I don't see why.

#### Expected Output

I'd expect either notice in release notes, or the same output in 0.25.3 and 1.0.0.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.0.0-1028-gcp
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 9.0.1
setuptools       : 39.0.1
Cython           : None
pytest           : 4.3.0
hypothesis       : None
sphinx           : 1.8.5
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : 0.999999999
pymysql          : None
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : None
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 2.5.14
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 4.3.0
pyxlsb           : None
s3fs             : None
scipy            : 1.2.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
577065366,32499,Better error message for OOB result,TomAugspurger,closed,2020-03-06T17:10:53Z,2020-03-12T12:22:19Z,"Closes #31774
"
579899752,32657,Backport PR #32544 on branch 1.0.x (BUG: pd.ExcelFile closes stream on destruction),meeseeksmachine,closed,2020-03-12T12:18:53Z,2020-03-12T13:15:43Z,Backport PR #32544: BUG: pd.ExcelFile closes stream on destruction
579891853,32656,DOC: fix announce formtting,TomAugspurger,closed,2020-03-12T12:04:27Z,2020-03-12T13:16:04Z,"Master:

```
    Contributors
    ============

    A total of 143 people contributed patches to this release.  People with a
""+"" by their names contributed a patch for the first time.

    * 3vts +
* A Brooks +
* Abbie Popa +
* Achmad Syarif Hidayatullah +
```

PR

```
Contributors
============

A total of 143 people contributed patches to this release.  People with a
""+"" by their names contributed a patch for the first time.

* 3vts +
* A Brooks +
* Abbie Popa +
```

I think the offset computed by textwrap.dedent changed. With fstrings it's calculated after the values are interpolated. We want it to be computed on the templated version."
570141319,32223,BUG: Fix DateFrameGroupBy.mean error for Int64 dtype,dsaxton,closed,2020-02-24T20:55:12Z,2020-03-12T13:19:45Z,"- [x] closes #32219
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

It looks like this was due to the `TypeError` not being caught"
579411845,32632,groupby nunique makes inplace replacement of NaN values to -9.223372036854776e18,nicholasyli,closed,2020-03-11T16:59:21Z,2020-03-12T13:37:12Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np

df = pd.DataFrame({'x': [1, 1, 1, 2, 2, 2], 'y': [1, 2, np.nan, np.nan, np.nan, 6]})
print(df)

# Doesn't replace NaN's
df.groupby('x').sum()
print(df)

# Replaces NaN's
df.groupby('x').nunique()
print(df)

```
#### Problem description

Using DataFrameGroupBy.nunique on a dataframe makes an inplace replacement of NaN values. Simply applying the function (and not assigning it to anything) will replace existing NaN's with -9223372036854775808.

There are two distinct issues:
1. There is some connection to issue #16674 in comparing NaN with -9223372036854775808 (thanks @mroeschke)
2. The inplace replacement, which seems to be more problematic.


#### Expected Output

   x    y
0  1  1.0
1  1  2.0
2  1  NaN
3  2  NaN
4  2  NaN
5  2  6.0
   x    y
0  1  1.0
1  1  2.0
2  1  NaN
3  2  NaN
4  2  NaN
5  2  6.0
   x    y
0  1  1.0
1  1  2.0
2  1  NaN
3  2  NaN
4  2  NaN
5  2  6.0

#### My Output

   x    y
0  1  1.0
1  1  2.0
2  1  NaN
3  2  NaN
4  2  NaN
5  2  6.0
   x    y
0  1  1.0
1  1  2.0
2  1  NaN
3  2  NaN
4  2  NaN
5  2  6.0
   x             y
0  1  1.000000e+00
1  1  2.000000e+00
2  1 -9.223372e+18
3  2 -9.223372e+18
4  2 -9.223372e+18
5  2  6.000000e+00

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 2.6.32-754.27.1.el6.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : None
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
579931337,32659,Backport PR #32656 on branch 1.0.x (DOC: fix announce formtting),meeseeksmachine,closed,2020-03-12T13:16:12Z,2020-03-12T14:07:37Z,Backport PR #32656: DOC: fix announce formtting
579902015,32658,DOC: Organize regressions,TomAugspurger,closed,2020-03-12T12:23:09Z,2020-03-12T14:11:05Z,
579940321,32660,"Backport PR #32490: BUG: Fix bug, where BooleanDtype columns are conv…",TomAugspurger,closed,2020-03-12T13:30:27Z,2020-03-12T14:11:36Z,"…erted to Int64

https://github.com/pandas-dev/pandas/pull/32490"
579967457,32661,Backport PR #32658 on branch 1.0.x (DOC: Organize regressions),meeseeksmachine,closed,2020-03-12T14:11:16Z,2020-03-12T14:39:01Z,Backport PR #32658: DOC: Organize regressions
579656311,32649,Backport PR #32223: BUG: Fix DateFrameGroupBy.mean error for Int64 dtype,dsaxton,closed,2020-03-12T02:46:56Z,2020-03-12T15:04:33Z,cc @jreback 
577436268,32537,CLN: avoid values_from_object in reshape.merge,jbrockmendel,closed,2020-03-08T03:03:41Z,2020-03-12T15:23:44Z,
578977368,32611,REF: implement _get_engine_target,jbrockmendel,closed,2020-03-11T03:01:59Z,2020-03-12T15:30:46Z,Follow-up to #32467.
578795470,32591,REG: dt64 shift with integer fill_value,jbrockmendel,closed,2020-03-10T18:59:28Z,2020-03-12T17:02:55Z,"- [x] closes #31971
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

"
547636411,30857,"DataFrame arithmetic functions with multiindex, NaN index values, and fill_value set returns an invalid result with non-unique index.",twheys,open,2020-01-09T18:07:21Z,2020-03-12T19:35:28Z,"#### Code Sample, a copy-pastable example if possible

```python
from unittest import TestCase

import numpy as np
import pandas as pd


class TestSubtract(TestCase):
    def test_subtract_partially_aligned_multi_index_dataframes_with_nans(self):
        df0 = pd.DataFrame(
            data=[
                [1, 2],
                [3, 4],
                [5, 6],
                [7, 8],
                [9, 10],
                [11, 12],
                [13, 14],
                [15, 16],
                [17, 18],
            ],
            columns=[""happy"", ""sad""],
            index=pd.MultiIndex.from_product(
                [[""a"", ""b"", None], [0, 1, np.nan]], names=[""l0"", ""l1""]
            ),
        )
        df1 = pd.DataFrame(
            data=[
                [1, 2],
                [3, 4],
                [5, 6],
                [7, 8],
                [9, 10],
                [11, 12],
                [13, 14],
                [15, 16],
                [17, 18],
            ],
            columns=[""happy"", ""sad""],
            index=pd.MultiIndex.from_product(
                [[""b"", ""c"", None], [1, 2, np.nan]], names=[""l0"", ""l1""]
            ),
        )

        result = df0.subtract(df1, fill_value=0)
        expected = pd.DataFrame.from_records(
            [
                [""a"", 0, 1 - 0, 2 - 0],
                [""a"", 1, 3 - 0, 4 - 0],
                [""a"", np.nan, 5 - 0, 6 - 0],
                [""b"", 0, 7 - 0, 8 - 0],
                [""b"", 1, 9 - 1, 10 - 2],
                [""b"", 2, 0 - 3, 0 - 4],
                [""b"", np.nan, 11 - 5, 12 - 6],
                [""c"", 1, 0 - 7, 0 - 8],
                [""c"", 2, 0 - 9, 0 - 10],
                [""c"", np.nan, 0 - 11, 0 - 12],
                [np.nan, 0, 13 - 0, 14 - 12],
                [np.nan, 1, 15 - 13, 16 - 14],
                [np.nan, 2, 0 - 15, 0 - 16],
                [np.nan, np.nan, 17 - 17, 18 - 18],
            ],
            columns=[""l0"", ""l1"", ""happy"", ""sad""],
        ).set_index([""l0"", ""l1""])

        print(expected)
        print(result)

        pd.testing.assert_frame_equal(expected, result)
        self.assertTrue(result.index.is_unique)
```
#### Problem description

I have two data frames with multi-level indices that represent the same sets of values, although both data frames may have index values that are not represented in the other (In my case, I am comparing two SQL queries results with a shifted date time index for week-over-week comparison). I'd expect the `DataFrame.subtract` function using fill values to replace a missing value on either side of the data equation with the fill value. Instead, I am seeing a value returned with a mismatched index with non-unique values.

In my expected dataframe in the test case, I populate each value with the expression I would expect the `subtract` function to perform.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
pd.show_versions()
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 0.25.3
numpy            : 1.16.2
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 18.1
setuptools       : 40.6.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 2.0.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : 0.8.0
psycopg2         : 2.7.3.2 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.5.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.0.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
577549836,32544,BUG: pd.ExcelFile closes stream on destruction,roberthdevries,closed,2020-03-08T19:14:23Z,2020-03-12T21:35:56Z,"- [x] closes #31467
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
577570134,32548,BUG: Add extra check for failing UTF-8 conversion,roberthdevries,closed,2020-03-08T21:35:41Z,2020-03-12T21:36:19Z,"- [x] closes #23809
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
580351802,32674,why would you call it pandas?,yiJiangHen,closed,2020-03-13T03:43:01Z,2020-03-13T05:43:26Z,"why would you guys call it pandas, just because you guys love panda?"
579604999,32642,fix infer_freq raises section,wholmgren,closed,2020-03-11T23:39:42Z,2020-03-13T09:43:43Z,"Fixes the raises section of the `infer_freq` doc string. Seems small enough to skip a whats new entry.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

"
576449538,32467,"CLN: use _values_for_argsort for join_non_unique, join_monotonic",jbrockmendel,closed,2020-03-05T18:29:29Z,2020-03-13T15:53:19Z,"With the `.copy()` removed from `Categorical._values_for_argsort`, `ea_backed_index._data._values_for_argsort()` matches `ea_backed_index._ndarray_values` in all extant cases.

cc @jorisvandenbossche @TomAugspurger need to confirm 

a) this is an intended-adjacent use of _values_for_argsort, and not just a coincidence that it matches extant behavior
b) the `.copy()` this removes from `Categorical._values_for_argsort` is not important for some un-tested reason

xref #32452, #32426"
577333398,32517,CLN: Suppres compile warnings of pandas/io/sas/sas.pyx,ShaharNaveh,closed,2020-03-07T13:15:03Z,2020-03-13T16:59:08Z,"- [x] ref #32163
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

This is the warning that get removed:

```
pandas/io/sas/sas.c: In function ‘__pyx_f_6pandas_2io_3sas_4_sas_rdc_decompress’:
pandas/io/sas/sas.c:3732:65: warning: ‘__pyx_v_ctrl_bits’ may be used uninitialized in this function [-Wmaybe-uninitialized]
 3732 |     __pyx_t_8 = (((__pyx_v_ctrl_bits & __pyx_v_ctrl_mask) == 0) != 0);
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~
```"
578295549,32570,CLN: avoid _internal_get_values in pandas._testing,jbrockmendel,closed,2020-03-10T02:26:43Z,2020-03-13T17:28:11Z,7 more usages left after this
574849475,32416,`_ensure_type` should use `issubclass`,ericchansen,closed,2020-03-03T18:12:19Z,2020-03-13T17:37:26Z,"Commit pandas-dev/pandas@6fd326d5a249967f9b6be60fc3c5f7366d914684 in pull request pandas-dev/pandas#30613 added `_ensure_type`, which utilizes `isinstance`. However, it is reasonable to assume that someone may want to create a DataFrame subclass. Therefore, `_ensure_type` should use `issubclass`.

- [x] closes #31925 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

whatsnew entry isn't necessary?
"
580802238,32689,Tests fail on 32bit archs with Python 3.8 and pandas 1.0.2,frispete,closed,2020-03-13T19:39:52Z,2020-03-13T19:45:08Z,"Apart from Test warnings noted [here](https://github.com/pandas-dev/pandas/issues/32628), we face plain test failures, that prevent packaging pandas for ix86 systems with Python 3.8.2 at least:

```
[ 1167s] =================================== FAILURES ===================================
[ 1167s] ______________ test_maybe_promote_int_with_int[int8-32768-int32] _______________
[ 1167s] [gw7] linux -- Python 3.8.2 /usr/bin/python3
[ 1167s] 
[ 1167s] dtype = dtype('int8'), fill_value = 32768, expected_dtype = dtype('int32')
[ 1167s] 
[ 1167s]     @pytest.mark.parametrize(
[ 1167s]         ""dtype, fill_value, expected_dtype"",
[ 1167s]         [
[ 1167s]             # size 8
[ 1167s]             (""int8"", 1, ""int8""),
[ 1167s]             (""int8"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1167s]             (""int8"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1167s]             (""int8"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1167s]             (""int8"", np.iinfo(""int64"").max + 1, ""object""),
[ 1167s]             (""int8"", -1, ""int8""),
[ 1167s]             (""int8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1167s]             (""int8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1167s]             (""int8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""int8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             # keep signed-ness as long as possible
[ 1167s]             (""uint8"", 1, ""uint8""),
[ 1167s]             (""uint8"", np.iinfo(""int8"").max + 1, ""uint8""),
[ 1167s]             (""uint8"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1167s]             (""uint8"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1167s]             (""uint8"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1167s]             (""uint8"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1167s]             (""uint8"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1167s]             (""uint8"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1167s]             (""uint8"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1167s]             # max of uint8 cannot be contained in int8
[ 1167s]             (""uint8"", -1, ""int16""),
[ 1167s]             (""uint8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1167s]             (""uint8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1167s]             (""uint8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""uint8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             # size 16
[ 1167s]             (""int16"", 1, ""int16""),
[ 1167s]             (""int16"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1167s]             (""int16"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1167s]             (""int16"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1167s]             (""int16"", np.iinfo(""int64"").max + 1, ""object""),
[ 1167s]             (""int16"", -1, ""int16""),
[ 1167s]             (""int16"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1167s]             (""int16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1167s]             (""int16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""int16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             (""uint16"", 1, ""uint16""),
[ 1167s]             (""uint16"", np.iinfo(""int8"").max + 1, ""uint16""),
[ 1167s]             (""uint16"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1167s]             (""uint16"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1167s]             (""uint16"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1167s]             (""uint16"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1167s]             (""uint16"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1167s]             (""uint16"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1167s]             (""uint16"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1167s]             (""uint16"", -1, ""int32""),
[ 1167s]             (""uint16"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1167s]             (""uint16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1167s]             (""uint16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""uint16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             # size 32
[ 1167s]             (""int32"", 1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int8"").max + 1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1167s]             (""int32"", np.iinfo(""int64"").max + 1, ""object""),
[ 1167s]             (""int32"", -1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""int32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             (""uint32"", 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""int8"").max + 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""uint8"").max + 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""int16"").max + 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1167s]             (""uint32"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1167s]             (""uint32"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1167s]             (""uint32"", -1, ""int64""),
[ 1167s]             (""uint32"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1167s]             (""uint32"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1167s]             (""uint32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""uint32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             # size 64
[ 1167s]             (""int64"", 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int8"").max + 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int16"").max + 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int64"").max + 1, ""object""),
[ 1167s]             (""int64"", -1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             (""uint64"", 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""int8"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""uint8"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""int16"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""uint16"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""int32"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1167s]             (""uint64"", -1, ""object""),
[ 1167s]             (""uint64"", np.iinfo(""int8"").min - 1, ""object""),
[ 1167s]             (""uint64"", np.iinfo(""int16"").min - 1, ""object""),
[ 1167s]             (""uint64"", np.iinfo(""int32"").min - 1, ""object""),
[ 1167s]             (""uint64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]         ],
[ 1167s]     )
[ 1167s]     def test_maybe_promote_int_with_int(dtype, fill_value, expected_dtype):
[ 1167s]         dtype = np.dtype(dtype)
[ 1167s]         expected_dtype = np.dtype(expected_dtype)
[ 1167s]     
[ 1167s]         # output is not a generic int, but corresponds to expected_dtype
[ 1167s]         exp_val_for_scalar = np.array([fill_value], dtype=expected_dtype)[0]
[ 1167s]     
[ 1167s] >       _check_promote(dtype, fill_value, expected_dtype, exp_val_for_scalar)
[ 1167s] 
[ 1167s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:237: 
[ 1167s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1167s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:94: in _check_promote
[ 1167s]     _assert_match(result_fill_value, expected_fill_value)
[ 1167s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1167s] 
[ 1167s] result_fill_value = 32768, expected_fill_value = 32768
[ 1167s] 
[ 1167s]     def _assert_match(result_fill_value, expected_fill_value):
[ 1167s]         # GH#23982/25425 require the same type in addition to equality/NA-ness
[ 1167s]         res_type = type(result_fill_value)
[ 1167s]         ex_type = type(expected_fill_value)
[ 1167s]         if res_type.__name__ == ""uint64"":
[ 1167s]             # No idea why, but these (sometimes) do not compare as equal
[ 1167s]             assert ex_type.__name__ == ""uint64""
[ 1167s]         elif res_type.__name__ == ""ulonglong"":
[ 1167s]             # On some builds we get this instead of np.uint64
[ 1167s]             # Note: cant check res_type.dtype.itemsize directly on numpy 1.18
[ 1167s]             assert res_type(0).itemsize == 8
[ 1167s]             assert ex_type == res_type or ex_type == np.uint64
[ 1167s]         else:
[ 1167s]             # On some builds, type comparison fails, e.g. np.int32 != np.int32
[ 1167s] >           assert res_type == ex_type or res_type.__name__ == ex_type.__name__
[ 1167s] E           AssertionError: assert (<class 'numpy.intc'> == <class 'numpy.int32'>
[ 1167s] E             -<class 'numpy.intc'>
[ 1167s] E             +<class 'numpy.int32'> or 'intc' == 'int32'
[ 1167s] E             - intc
[ 1167s] E             + int32)
[ 1167s] 
[ 1167s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:111: AssertionError
[ 1167s] _____________ test_maybe_promote_int_with_int[uint8-65536-uint32] ______________
[ 1167s] [gw7] linux -- Python 3.8.2 /usr/bin/python3
[ 1167s] 
[ 1167s] dtype = dtype('uint8'), fill_value = 65536, expected_dtype = dtype('uint32')
[ 1167s] 
[ 1167s]     @pytest.mark.parametrize(
[ 1167s]         ""dtype, fill_value, expected_dtype"",
[ 1167s]         [
[ 1167s]             # size 8
[ 1167s]             (""int8"", 1, ""int8""),
[ 1167s]             (""int8"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1167s]             (""int8"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1167s]             (""int8"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1167s]             (""int8"", np.iinfo(""int64"").max + 1, ""object""),
[ 1167s]             (""int8"", -1, ""int8""),
[ 1167s]             (""int8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1167s]             (""int8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1167s]             (""int8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""int8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             # keep signed-ness as long as possible
[ 1167s]             (""uint8"", 1, ""uint8""),
[ 1167s]             (""uint8"", np.iinfo(""int8"").max + 1, ""uint8""),
[ 1167s]             (""uint8"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1167s]             (""uint8"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1167s]             (""uint8"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1167s]             (""uint8"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1167s]             (""uint8"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1167s]             (""uint8"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1167s]             (""uint8"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1167s]             # max of uint8 cannot be contained in int8
[ 1167s]             (""uint8"", -1, ""int16""),
[ 1167s]             (""uint8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1167s]             (""uint8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1167s]             (""uint8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""uint8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             # size 16
[ 1167s]             (""int16"", 1, ""int16""),
[ 1167s]             (""int16"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1167s]             (""int16"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1167s]             (""int16"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1167s]             (""int16"", np.iinfo(""int64"").max + 1, ""object""),
[ 1167s]             (""int16"", -1, ""int16""),
[ 1167s]             (""int16"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1167s]             (""int16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1167s]             (""int16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""int16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             (""uint16"", 1, ""uint16""),
[ 1167s]             (""uint16"", np.iinfo(""int8"").max + 1, ""uint16""),
[ 1167s]             (""uint16"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1167s]             (""uint16"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1167s]             (""uint16"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1167s]             (""uint16"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1167s]             (""uint16"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1167s]             (""uint16"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1167s]             (""uint16"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1167s]             (""uint16"", -1, ""int32""),
[ 1167s]             (""uint16"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1167s]             (""uint16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1167s]             (""uint16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""uint16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             # size 32
[ 1167s]             (""int32"", 1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int8"").max + 1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1167s]             (""int32"", np.iinfo(""int64"").max + 1, ""object""),
[ 1167s]             (""int32"", -1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""int32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             (""uint32"", 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""int8"").max + 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""uint8"").max + 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""int16"").max + 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1167s]             (""uint32"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1167s]             (""uint32"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1167s]             (""uint32"", -1, ""int64""),
[ 1167s]             (""uint32"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1167s]             (""uint32"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1167s]             (""uint32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""uint32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             # size 64
[ 1167s]             (""int64"", 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int8"").max + 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int16"").max + 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int64"").max + 1, ""object""),
[ 1167s]             (""int64"", -1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             (""uint64"", 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""int8"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""uint8"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""int16"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""uint16"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""int32"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1167s]             (""uint64"", -1, ""object""),
[ 1167s]             (""uint64"", np.iinfo(""int8"").min - 1, ""object""),
[ 1167s]             (""uint64"", np.iinfo(""int16"").min - 1, ""object""),
[ 1167s]             (""uint64"", np.iinfo(""int32"").min - 1, ""object""),
[ 1167s]             (""uint64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]         ],
[ 1167s]     )
[ 1167s]     def test_maybe_promote_int_with_int(dtype, fill_value, expected_dtype):
[ 1167s]         dtype = np.dtype(dtype)
[ 1167s]         expected_dtype = np.dtype(expected_dtype)
[ 1167s]     
[ 1167s]         # output is not a generic int, but corresponds to expected_dtype
[ 1167s]         exp_val_for_scalar = np.array([fill_value], dtype=expected_dtype)[0]
[ 1167s]     
[ 1167s] >       _check_promote(dtype, fill_value, expected_dtype, exp_val_for_scalar)
[ 1167s] 
[ 1167s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:237: 
[ 1167s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1167s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:94: in _check_promote
[ 1167s]     _assert_match(result_fill_value, expected_fill_value)
[ 1167s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1167s] 
[ 1167s] result_fill_value = 65536, expected_fill_value = 65536
[ 1167s] 
[ 1167s]     def _assert_match(result_fill_value, expected_fill_value):
[ 1167s]         # GH#23982/25425 require the same type in addition to equality/NA-ness
[ 1167s]         res_type = type(result_fill_value)
[ 1167s]         ex_type = type(expected_fill_value)
[ 1167s]         if res_type.__name__ == ""uint64"":
[ 1167s]             # No idea why, but these (sometimes) do not compare as equal
[ 1167s]             assert ex_type.__name__ == ""uint64""
[ 1167s]         elif res_type.__name__ == ""ulonglong"":
[ 1167s]             # On some builds we get this instead of np.uint64
[ 1167s]             # Note: cant check res_type.dtype.itemsize directly on numpy 1.18
[ 1167s]             assert res_type(0).itemsize == 8
[ 1167s]             assert ex_type == res_type or ex_type == np.uint64
[ 1167s]         else:
[ 1167s]             # On some builds, type comparison fails, e.g. np.int32 != np.int32
[ 1167s] >           assert res_type == ex_type or res_type.__name__ == ex_type.__name__
[ 1167s] E           AssertionError: assert (<class 'numpy.uintc'> == <class 'numpy.uint32'>
[ 1167s] E             -<class 'numpy.uintc'>
[ 1167s] E             +<class 'numpy.uint32'> or 'uintc' == 'uint32'
[ 1167s] E             - uintc
[ 1167s] E             + uint32)
[ 1167s] 
[ 1167s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:111: AssertionError
[ 1167s] ___________ test_maybe_promote_int_with_int[uint8-2147483648-uint32] ___________
[ 1167s] [gw7] linux -- Python 3.8.2 /usr/bin/python3
[ 1167s] 
[ 1167s] dtype = dtype('uint8'), fill_value = 2147483648
[ 1167s] expected_dtype = dtype('uint32')
[ 1167s] 
[ 1167s]     @pytest.mark.parametrize(
[ 1167s]         ""dtype, fill_value, expected_dtype"",
[ 1167s]         [
[ 1167s]             # size 8
[ 1167s]             (""int8"", 1, ""int8""),
[ 1167s]             (""int8"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1167s]             (""int8"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1167s]             (""int8"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1167s]             (""int8"", np.iinfo(""int64"").max + 1, ""object""),
[ 1167s]             (""int8"", -1, ""int8""),
[ 1167s]             (""int8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1167s]             (""int8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1167s]             (""int8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""int8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             # keep signed-ness as long as possible
[ 1167s]             (""uint8"", 1, ""uint8""),
[ 1167s]             (""uint8"", np.iinfo(""int8"").max + 1, ""uint8""),
[ 1167s]             (""uint8"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1167s]             (""uint8"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1167s]             (""uint8"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1167s]             (""uint8"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1167s]             (""uint8"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1167s]             (""uint8"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1167s]             (""uint8"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1167s]             # max of uint8 cannot be contained in int8
[ 1167s]             (""uint8"", -1, ""int16""),
[ 1167s]             (""uint8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1167s]             (""uint8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1167s]             (""uint8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""uint8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             # size 16
[ 1167s]             (""int16"", 1, ""int16""),
[ 1167s]             (""int16"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1167s]             (""int16"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1167s]             (""int16"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1167s]             (""int16"", np.iinfo(""int64"").max + 1, ""object""),
[ 1167s]             (""int16"", -1, ""int16""),
[ 1167s]             (""int16"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1167s]             (""int16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1167s]             (""int16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""int16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             (""uint16"", 1, ""uint16""),
[ 1167s]             (""uint16"", np.iinfo(""int8"").max + 1, ""uint16""),
[ 1167s]             (""uint16"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1167s]             (""uint16"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1167s]             (""uint16"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1167s]             (""uint16"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1167s]             (""uint16"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1167s]             (""uint16"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1167s]             (""uint16"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1167s]             (""uint16"", -1, ""int32""),
[ 1167s]             (""uint16"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1167s]             (""uint16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1167s]             (""uint16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""uint16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             # size 32
[ 1167s]             (""int32"", 1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int8"").max + 1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1167s]             (""int32"", np.iinfo(""int64"").max + 1, ""object""),
[ 1167s]             (""int32"", -1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1167s]             (""int32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""int32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             (""uint32"", 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""int8"").max + 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""uint8"").max + 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""int16"").max + 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1167s]             (""uint32"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1167s]             (""uint32"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1167s]             (""uint32"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1167s]             (""uint32"", -1, ""int64""),
[ 1167s]             (""uint32"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1167s]             (""uint32"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1167s]             (""uint32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""uint32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             # size 64
[ 1167s]             (""int64"", 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int8"").max + 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int16"").max + 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int64"").max + 1, ""object""),
[ 1167s]             (""int64"", -1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1167s]             (""int64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]             (""uint64"", 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""int8"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""uint8"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""int16"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""uint16"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""int32"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1167s]             (""uint64"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1167s]             (""uint64"", -1, ""object""),
[ 1167s]             (""uint64"", np.iinfo(""int8"").min - 1, ""object""),
[ 1167s]             (""uint64"", np.iinfo(""int16"").min - 1, ""object""),
[ 1167s]             (""uint64"", np.iinfo(""int32"").min - 1, ""object""),
[ 1167s]             (""uint64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1167s]         ],
[ 1167s]     )
[ 1167s]     def test_maybe_promote_int_with_int(dtype, fill_value, expected_dtype):
[ 1167s]         dtype = np.dtype(dtype)
[ 1167s]         expected_dtype = np.dtype(expected_dtype)
[ 1167s]     
[ 1167s]         # output is not a generic int, but corresponds to expected_dtype
[ 1167s]         exp_val_for_scalar = np.array([fill_value], dtype=expected_dtype)[0]
[ 1167s]     
[ 1167s] >       _check_promote(dtype, fill_value, expected_dtype, exp_val_for_scalar)
[ 1167s] 
[ 1167s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:237: 
[ 1167s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1167s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:94: in _check_promote
[ 1167s]     _assert_match(result_fill_value, expected_fill_value)
[ 1167s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1167s] 
[ 1167s] result_fill_value = 2147483648, expected_fill_value = 2147483648
[ 1167s] 
[ 1167s]     def _assert_match(result_fill_value, expected_fill_value):
[ 1167s]         # GH#23982/25425 require the same type in addition to equality/NA-ness
[ 1167s]         res_type = type(result_fill_value)
[ 1167s]         ex_type = type(expected_fill_value)
[ 1167s]         if res_type.__name__ == ""uint64"":
[ 1167s]             # No idea why, but these (sometimes) do not compare as equal
[ 1167s]             assert ex_type.__name__ == ""uint64""
[ 1167s]         elif res_type.__name__ == ""ulonglong"":
[ 1167s]             # On some builds we get this instead of np.uint64
[ 1167s]             # Note: cant check res_type.dtype.itemsize directly on numpy 1.18
[ 1167s]             assert res_type(0).itemsize == 8
[ 1167s]             assert ex_type == res_type or ex_type == np.uint64
[ 1167s]         else:
[ 1167s]             # On some builds, type comparison fails, e.g. np.int32 != np.int32
[ 1167s] >           assert res_type == ex_type or res_type.__name__ == ex_type.__name__
[ 1167s] E           AssertionError: assert (<class 'numpy.uintc'> == <class 'numpy.uint32'>
[ 1167s] E             -<class 'numpy.uintc'>
[ 1167s] E             +<class 'numpy.uint32'> or 'uintc' == 'uint32'
[ 1167s] E             - uintc
[ 1167s] E             + uint32)
[ 1167s] 
[ 1167s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:111: AssertionError
[ 1168s] ______________ test_maybe_promote_int_with_int[int16-32768-int32] ______________
[ 1168s] [gw7] linux -- Python 3.8.2 /usr/bin/python3
[ 1168s] 
[ 1168s] dtype = dtype('int16'), fill_value = 32768, expected_dtype = dtype('int32')
[ 1168s] 
[ 1168s]     @pytest.mark.parametrize(
[ 1168s]         ""dtype, fill_value, expected_dtype"",
[ 1168s]         [
[ 1168s]             # size 8
[ 1168s]             (""int8"", 1, ""int8""),
[ 1168s]             (""int8"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1168s]             (""int8"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1168s]             (""int8"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int8"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int8"", -1, ""int8""),
[ 1168s]             (""int8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1168s]             (""int8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""int8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # keep signed-ness as long as possible
[ 1168s]             (""uint8"", 1, ""uint8""),
[ 1168s]             (""uint8"", np.iinfo(""int8"").max + 1, ""uint8""),
[ 1168s]             (""uint8"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1168s]             (""uint8"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1168s]             (""uint8"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1168s]             (""uint8"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1168s]             (""uint8"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint8"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint8"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             # max of uint8 cannot be contained in int8
[ 1168s]             (""uint8"", -1, ""int16""),
[ 1168s]             (""uint8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1168s]             (""uint8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""uint8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""uint8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # size 16
[ 1168s]             (""int16"", 1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1168s]             (""int16"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int16"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int16"", -1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""int16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             (""uint16"", 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""int8"").max + 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1168s]             (""uint16"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1168s]             (""uint16"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint16"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint16"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             (""uint16"", -1, ""int32""),
[ 1168s]             (""uint16"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1168s]             (""uint16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""uint16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""uint16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # size 32
[ 1168s]             (""int32"", 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int8"").max + 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int32"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int32"", -1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             (""uint32"", 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""int8"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""uint8"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""int16"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint32"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint32"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             (""uint32"", -1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # size 64
[ 1168s]             (""int64"", 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int8"").max + 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int16"").max + 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int64"", -1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             (""uint64"", 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int8"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint8"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int16"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint16"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int32"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             (""uint64"", -1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int8"").min - 1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int16"").min - 1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int32"").min - 1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]         ],
[ 1168s]     )
[ 1168s]     def test_maybe_promote_int_with_int(dtype, fill_value, expected_dtype):
[ 1168s]         dtype = np.dtype(dtype)
[ 1168s]         expected_dtype = np.dtype(expected_dtype)
[ 1168s]     
[ 1168s]         # output is not a generic int, but corresponds to expected_dtype
[ 1168s]         exp_val_for_scalar = np.array([fill_value], dtype=expected_dtype)[0]
[ 1168s]     
[ 1168s] >       _check_promote(dtype, fill_value, expected_dtype, exp_val_for_scalar)
[ 1168s] 
[ 1168s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:237: 
[ 1168s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1168s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:94: in _check_promote
[ 1168s]     _assert_match(result_fill_value, expected_fill_value)
[ 1168s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1168s] 
[ 1168s] result_fill_value = 32768, expected_fill_value = 32768
[ 1168s] 
[ 1168s]     def _assert_match(result_fill_value, expected_fill_value):
[ 1168s]         # GH#23982/25425 require the same type in addition to equality/NA-ness
[ 1168s]         res_type = type(result_fill_value)
[ 1168s]         ex_type = type(expected_fill_value)
[ 1168s]         if res_type.__name__ == ""uint64"":
[ 1168s]             # No idea why, but these (sometimes) do not compare as equal
[ 1168s]             assert ex_type.__name__ == ""uint64""
[ 1168s]         elif res_type.__name__ == ""ulonglong"":
[ 1168s]             # On some builds we get this instead of np.uint64
[ 1168s]             # Note: cant check res_type.dtype.itemsize directly on numpy 1.18
[ 1168s]             assert res_type(0).itemsize == 8
[ 1168s]             assert ex_type == res_type or ex_type == np.uint64
[ 1168s]         else:
[ 1168s]             # On some builds, type comparison fails, e.g. np.int32 != np.int32
[ 1168s] >           assert res_type == ex_type or res_type.__name__ == ex_type.__name__
[ 1168s] E           AssertionError: assert (<class 'numpy.intc'> == <class 'numpy.int32'>
[ 1168s] E             -<class 'numpy.intc'>
[ 1168s] E             +<class 'numpy.int32'> or 'intc' == 'int32'
[ 1168s] E             - intc
[ 1168s] E             + int32)
[ 1168s] 
[ 1168s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:111: AssertionError
[ 1168s] _____________ test_maybe_promote_int_with_int[uint16-65536-uint32] _____________
[ 1168s] [gw7] linux -- Python 3.8.2 /usr/bin/python3
[ 1168s] 
[ 1168s] dtype = dtype('uint16'), fill_value = 65536, expected_dtype = dtype('uint32')
[ 1168s] 
[ 1168s]     @pytest.mark.parametrize(
[ 1168s]         ""dtype, fill_value, expected_dtype"",
[ 1168s]         [
[ 1168s]             # size 8
[ 1168s]             (""int8"", 1, ""int8""),
[ 1168s]             (""int8"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1168s]             (""int8"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1168s]             (""int8"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int8"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int8"", -1, ""int8""),
[ 1168s]             (""int8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1168s]             (""int8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""int8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # keep signed-ness as long as possible
[ 1168s]             (""uint8"", 1, ""uint8""),
[ 1168s]             (""uint8"", np.iinfo(""int8"").max + 1, ""uint8""),
[ 1168s]             (""uint8"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1168s]             (""uint8"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1168s]             (""uint8"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1168s]             (""uint8"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1168s]             (""uint8"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint8"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint8"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             # max of uint8 cannot be contained in int8
[ 1168s]             (""uint8"", -1, ""int16""),
[ 1168s]             (""uint8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1168s]             (""uint8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""uint8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""uint8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # size 16
[ 1168s]             (""int16"", 1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1168s]             (""int16"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int16"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int16"", -1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""int16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             (""uint16"", 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""int8"").max + 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1168s]             (""uint16"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1168s]             (""uint16"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint16"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint16"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             (""uint16"", -1, ""int32""),
[ 1168s]             (""uint16"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1168s]             (""uint16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""uint16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""uint16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # size 32
[ 1168s]             (""int32"", 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int8"").max + 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int32"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int32"", -1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             (""uint32"", 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""int8"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""uint8"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""int16"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint32"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint32"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             (""uint32"", -1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # size 64
[ 1168s]             (""int64"", 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int8"").max + 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int16"").max + 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int64"", -1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             (""uint64"", 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int8"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint8"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int16"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint16"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int32"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             (""uint64"", -1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int8"").min - 1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int16"").min - 1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int32"").min - 1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]         ],
[ 1168s]     )
[ 1168s]     def test_maybe_promote_int_with_int(dtype, fill_value, expected_dtype):
[ 1168s]         dtype = np.dtype(dtype)
[ 1168s]         expected_dtype = np.dtype(expected_dtype)
[ 1168s]     
[ 1168s]         # output is not a generic int, but corresponds to expected_dtype
[ 1168s]         exp_val_for_scalar = np.array([fill_value], dtype=expected_dtype)[0]
[ 1168s]     
[ 1168s] >       _check_promote(dtype, fill_value, expected_dtype, exp_val_for_scalar)
[ 1168s] 
[ 1168s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:237: 
[ 1168s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1168s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:94: in _check_promote
[ 1168s]     _assert_match(result_fill_value, expected_fill_value)
[ 1168s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1168s] 
[ 1168s] result_fill_value = 65536, expected_fill_value = 65536
[ 1168s] 
[ 1168s]     def _assert_match(result_fill_value, expected_fill_value):
[ 1168s]         # GH#23982/25425 require the same type in addition to equality/NA-ness
[ 1168s]         res_type = type(result_fill_value)
[ 1168s]         ex_type = type(expected_fill_value)
[ 1168s]         if res_type.__name__ == ""uint64"":
[ 1168s]             # No idea why, but these (sometimes) do not compare as equal
[ 1168s]             assert ex_type.__name__ == ""uint64""
[ 1168s]         elif res_type.__name__ == ""ulonglong"":
[ 1168s]             # On some builds we get this instead of np.uint64
[ 1168s]             # Note: cant check res_type.dtype.itemsize directly on numpy 1.18
[ 1168s]             assert res_type(0).itemsize == 8
[ 1168s]             assert ex_type == res_type or ex_type == np.uint64
[ 1168s]         else:
[ 1168s]             # On some builds, type comparison fails, e.g. np.int32 != np.int32
[ 1168s] >           assert res_type == ex_type or res_type.__name__ == ex_type.__name__
[ 1168s] E           AssertionError: assert (<class 'numpy.uintc'> == <class 'numpy.uint32'>
[ 1168s] E             -<class 'numpy.uintc'>
[ 1168s] E             +<class 'numpy.uint32'> or 'uintc' == 'uint32'
[ 1168s] E             - uintc
[ 1168s] E             + uint32)
[ 1168s] 
[ 1168s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:111: AssertionError
[ 1168s] __________ test_maybe_promote_int_with_int[uint16-2147483648-uint32] ___________
[ 1168s] [gw7] linux -- Python 3.8.2 /usr/bin/python3
[ 1168s] 
[ 1168s] dtype = dtype('uint16'), fill_value = 2147483648
[ 1168s] expected_dtype = dtype('uint32')
[ 1168s] 
[ 1168s]     @pytest.mark.parametrize(
[ 1168s]         ""dtype, fill_value, expected_dtype"",
[ 1168s]         [
[ 1168s]             # size 8
[ 1168s]             (""int8"", 1, ""int8""),
[ 1168s]             (""int8"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1168s]             (""int8"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1168s]             (""int8"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int8"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int8"", -1, ""int8""),
[ 1168s]             (""int8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1168s]             (""int8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""int8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # keep signed-ness as long as possible
[ 1168s]             (""uint8"", 1, ""uint8""),
[ 1168s]             (""uint8"", np.iinfo(""int8"").max + 1, ""uint8""),
[ 1168s]             (""uint8"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1168s]             (""uint8"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1168s]             (""uint8"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1168s]             (""uint8"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1168s]             (""uint8"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint8"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint8"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             # max of uint8 cannot be contained in int8
[ 1168s]             (""uint8"", -1, ""int16""),
[ 1168s]             (""uint8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1168s]             (""uint8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""uint8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""uint8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # size 16
[ 1168s]             (""int16"", 1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1168s]             (""int16"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int16"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int16"", -1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""int16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             (""uint16"", 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""int8"").max + 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1168s]             (""uint16"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1168s]             (""uint16"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint16"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint16"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             (""uint16"", -1, ""int32""),
[ 1168s]             (""uint16"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1168s]             (""uint16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""uint16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""uint16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # size 32
[ 1168s]             (""int32"", 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int8"").max + 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int32"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int32"", -1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             (""uint32"", 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""int8"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""uint8"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""int16"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint32"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint32"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             (""uint32"", -1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # size 64
[ 1168s]             (""int64"", 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int8"").max + 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int16"").max + 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int64"", -1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             (""uint64"", 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int8"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint8"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int16"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint16"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int32"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             (""uint64"", -1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int8"").min - 1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int16"").min - 1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int32"").min - 1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]         ],
[ 1168s]     )
[ 1168s]     def test_maybe_promote_int_with_int(dtype, fill_value, expected_dtype):
[ 1168s]         dtype = np.dtype(dtype)
[ 1168s]         expected_dtype = np.dtype(expected_dtype)
[ 1168s]     
[ 1168s]         # output is not a generic int, but corresponds to expected_dtype
[ 1168s]         exp_val_for_scalar = np.array([fill_value], dtype=expected_dtype)[0]
[ 1168s]     
[ 1168s] >       _check_promote(dtype, fill_value, expected_dtype, exp_val_for_scalar)
[ 1168s] 
[ 1168s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:237: 
[ 1168s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1168s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:94: in _check_promote
[ 1168s]     _assert_match(result_fill_value, expected_fill_value)
[ 1168s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1168s] 
[ 1168s] result_fill_value = 2147483648, expected_fill_value = 2147483648
[ 1168s] 
[ 1168s]     def _assert_match(result_fill_value, expected_fill_value):
[ 1168s]         # GH#23982/25425 require the same type in addition to equality/NA-ness
[ 1168s]         res_type = type(result_fill_value)
[ 1168s]         ex_type = type(expected_fill_value)
[ 1168s]         if res_type.__name__ == ""uint64"":
[ 1168s]             # No idea why, but these (sometimes) do not compare as equal
[ 1168s]             assert ex_type.__name__ == ""uint64""
[ 1168s]         elif res_type.__name__ == ""ulonglong"":
[ 1168s]             # On some builds we get this instead of np.uint64
[ 1168s]             # Note: cant check res_type.dtype.itemsize directly on numpy 1.18
[ 1168s]             assert res_type(0).itemsize == 8
[ 1168s]             assert ex_type == res_type or ex_type == np.uint64
[ 1168s]         else:
[ 1168s]             # On some builds, type comparison fails, e.g. np.int32 != np.int32
[ 1168s] >           assert res_type == ex_type or res_type.__name__ == ex_type.__name__
[ 1168s] E           AssertionError: assert (<class 'numpy.uintc'> == <class 'numpy.uint32'>
[ 1168s] E             -<class 'numpy.uintc'>
[ 1168s] E             +<class 'numpy.uint32'> or 'uintc' == 'uint32'
[ 1168s] E             - uintc
[ 1168s] E             + uint32)
[ 1168s] 
[ 1168s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:111: AssertionError
[ 1168s] _______________ test_maybe_promote_int_with_int[uint16--1-int32] _______________
[ 1168s] [gw7] linux -- Python 3.8.2 /usr/bin/python3
[ 1168s] 
[ 1168s] dtype = dtype('uint16'), fill_value = -1, expected_dtype = dtype('int32')
[ 1168s] 
[ 1168s]     @pytest.mark.parametrize(
[ 1168s]         ""dtype, fill_value, expected_dtype"",
[ 1168s]         [
[ 1168s]             # size 8
[ 1168s]             (""int8"", 1, ""int8""),
[ 1168s]             (""int8"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1168s]             (""int8"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1168s]             (""int8"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int8"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int8"", -1, ""int8""),
[ 1168s]             (""int8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1168s]             (""int8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""int8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # keep signed-ness as long as possible
[ 1168s]             (""uint8"", 1, ""uint8""),
[ 1168s]             (""uint8"", np.iinfo(""int8"").max + 1, ""uint8""),
[ 1168s]             (""uint8"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1168s]             (""uint8"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1168s]             (""uint8"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1168s]             (""uint8"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1168s]             (""uint8"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint8"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint8"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             # max of uint8 cannot be contained in int8
[ 1168s]             (""uint8"", -1, ""int16""),
[ 1168s]             (""uint8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1168s]             (""uint8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""uint8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""uint8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # size 16
[ 1168s]             (""int16"", 1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1168s]             (""int16"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int16"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int16"", -1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""int16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             (""uint16"", 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""int8"").max + 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1168s]             (""uint16"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1168s]             (""uint16"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint16"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint16"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             (""uint16"", -1, ""int32""),
[ 1168s]             (""uint16"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1168s]             (""uint16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""uint16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""uint16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # size 32
[ 1168s]             (""int32"", 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int8"").max + 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int32"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int32"", -1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             (""uint32"", 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""int8"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""uint8"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""int16"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint32"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint32"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             (""uint32"", -1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # size 64
[ 1168s]             (""int64"", 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int8"").max + 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int16"").max + 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int64"", -1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             (""uint64"", 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int8"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint8"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int16"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint16"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int32"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             (""uint64"", -1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int8"").min - 1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int16"").min - 1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int32"").min - 1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]         ],
[ 1168s]     )
[ 1168s]     def test_maybe_promote_int_with_int(dtype, fill_value, expected_dtype):
[ 1168s]         dtype = np.dtype(dtype)
[ 1168s]         expected_dtype = np.dtype(expected_dtype)
[ 1168s]     
[ 1168s]         # output is not a generic int, but corresponds to expected_dtype
[ 1168s]         exp_val_for_scalar = np.array([fill_value], dtype=expected_dtype)[0]
[ 1168s]     
[ 1168s] >       _check_promote(dtype, fill_value, expected_dtype, exp_val_for_scalar)
[ 1168s] 
[ 1168s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:237: 
[ 1168s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1168s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:94: in _check_promote
[ 1168s]     _assert_match(result_fill_value, expected_fill_value)
[ 1168s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1168s] 
[ 1168s] result_fill_value = -1, expected_fill_value = -1
[ 1168s] 
[ 1168s]     def _assert_match(result_fill_value, expected_fill_value):
[ 1168s]         # GH#23982/25425 require the same type in addition to equality/NA-ness
[ 1168s]         res_type = type(result_fill_value)
[ 1168s]         ex_type = type(expected_fill_value)
[ 1168s]         if res_type.__name__ == ""uint64"":
[ 1168s]             # No idea why, but these (sometimes) do not compare as equal
[ 1168s]             assert ex_type.__name__ == ""uint64""
[ 1168s]         elif res_type.__name__ == ""ulonglong"":
[ 1168s]             # On some builds we get this instead of np.uint64
[ 1168s]             # Note: cant check res_type.dtype.itemsize directly on numpy 1.18
[ 1168s]             assert res_type(0).itemsize == 8
[ 1168s]             assert ex_type == res_type or ex_type == np.uint64
[ 1168s]         else:
[ 1168s]             # On some builds, type comparison fails, e.g. np.int32 != np.int32
[ 1168s] >           assert res_type == ex_type or res_type.__name__ == ex_type.__name__
[ 1168s] E           AssertionError: assert (<class 'numpy.intc'> == <class 'numpy.int32'>
[ 1168s] E             -<class 'numpy.intc'>
[ 1168s] E             +<class 'numpy.int32'> or 'intc' == 'int32'
[ 1168s] E             - intc
[ 1168s] E             + int32)
[ 1168s] 
[ 1168s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:111: AssertionError
[ 1168s] ______________ test_maybe_promote_int_with_int[uint16--129-int32] ______________
[ 1168s] [gw7] linux -- Python 3.8.2 /usr/bin/python3
[ 1168s] 
[ 1168s] dtype = dtype('uint16'), fill_value = -129, expected_dtype = dtype('int32')
[ 1168s] 
[ 1168s]     @pytest.mark.parametrize(
[ 1168s]         ""dtype, fill_value, expected_dtype"",
[ 1168s]         [
[ 1168s]             # size 8
[ 1168s]             (""int8"", 1, ""int8""),
[ 1168s]             (""int8"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1168s]             (""int8"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1168s]             (""int8"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int8"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int8"", -1, ""int8""),
[ 1168s]             (""int8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1168s]             (""int8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""int8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # keep signed-ness as long as possible
[ 1168s]             (""uint8"", 1, ""uint8""),
[ 1168s]             (""uint8"", np.iinfo(""int8"").max + 1, ""uint8""),
[ 1168s]             (""uint8"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1168s]             (""uint8"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1168s]             (""uint8"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1168s]             (""uint8"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1168s]             (""uint8"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint8"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint8"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             # max of uint8 cannot be contained in int8
[ 1168s]             (""uint8"", -1, ""int16""),
[ 1168s]             (""uint8"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1168s]             (""uint8"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""uint8"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""uint8"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # size 16
[ 1168s]             (""int16"", 1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int8"").max + 1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1168s]             (""int16"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int16"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int16"", -1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int8"").min - 1, ""int16""),
[ 1168s]             (""int16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""int16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             (""uint16"", 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""int8"").max + 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""uint8"").max + 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""int16"").max + 1, ""uint16""),
[ 1168s]             (""uint16"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1168s]             (""uint16"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1168s]             (""uint16"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint16"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint16"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             (""uint16"", -1, ""int32""),
[ 1168s]             (""uint16"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1168s]             (""uint16"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""uint16"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""uint16"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # size 32
[ 1168s]             (""int32"", 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int8"").max + 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int16"").max + 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int32"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int32"", -1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int8"").min - 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int16"").min - 1, ""int32""),
[ 1168s]             (""int32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             (""uint32"", 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""int8"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""uint8"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""int16"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""uint16"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""int32"").max + 1, ""uint32""),
[ 1168s]             (""uint32"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint32"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint32"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             (""uint32"", -1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""uint32"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             # size 64
[ 1168s]             (""int64"", 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int8"").max + 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int16"").max + 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int32"").max + 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int64"").max + 1, ""object""),
[ 1168s]             (""int64"", -1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int8"").min - 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int16"").min - 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int32"").min - 1, ""int64""),
[ 1168s]             (""int64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]             (""uint64"", 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int8"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint8"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int16"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint16"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int32"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint32"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""int64"").max + 1, ""uint64""),
[ 1168s]             (""uint64"", np.iinfo(""uint64"").max + 1, ""object""),
[ 1168s]             (""uint64"", -1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int8"").min - 1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int16"").min - 1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int32"").min - 1, ""object""),
[ 1168s]             (""uint64"", np.iinfo(""int64"").min - 1, ""object""),
[ 1168s]         ],
[ 1168s]     )
[ 1168s]     def test_maybe_promote_int_with_int(dtype, fill_value, expected_dtype):
[ 1168s]         dtype = np.dtype(dtype)
[ 1168s]         expected_dtype = np.dtype(expected_dtype)
[ 1168s]     
[ 1168s]         # output is not a generic int, but corresponds to expected_dtype
[ 1168s]         exp_val_for_scalar = np.array([fill_value], dtype=expected_dtype)[0]
[ 1168s]     
[ 1168s] >       _check_promote(dtype, fill_value, expected_dtype, exp_val_for_scalar)
[ 1168s] 
[ 1168s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:237: 
[ 1168s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1168s] ../../BUILDROOT/python-pandas-1.0.2-37.1.i386/usr/lib/python3.8/site-packages/pandas/tests/dtypes/cast/test_promote.py:94: in _check_promote
[ 1168s]     _assert_match(result_fill_value, expected_fill_value)
[ 1168s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1168s] 
[ 1168s] result_fill_value = -129, expected_fill_value = -129
[ 1168s] 
[ 1168s]     def _assert_match(result_fill_value, expected_fill_value):
[ 1168s]         # GH#23982/25425 require the same type in addition to equality/NA-ness
[ 1168s]         res_type = type(result_fill_value)
[ 1168s]         ex_type = type(expected_fill_value)
[ 1168s]         if res_type.__name__ == ""uint64"":
[ 1168s]             # No idea why, but these (sometimes) do not compare as equal
[ 1168s]             assert ex_type.__name__ == ""uint64""
[ 1168s]         elif res_type.__name__ == ""ulonglong"":
[ 1168s]             # On some builds we get this instead of np.uint64
[ 1168s]             # Note: cant check res_type.dtype.itemsize directly on numpy 1.18
[ 1168s]             assert res_type(0).itemsize == 8
[ 1168s]             assert ex_type == res_type or ex_type == np.uint64
[ 1168s]         else:
[ 1168s]             # On some builds, type comparison fails, e.g. np.int32 != np.int32
[ 1168s] >           assert res_type == ex_type or res_type.__name__ == ex_type.__name__
[ 1168s] E           AssertionError: assert (<class 'numpy.intc'> == <class 'numpy.int32'>
[ 1168s] E             -<class 'numpy.intc'>
[ 1168s] E             +<class 'numpy.int32'> or 'intc' == 'int32'
[ 1168s] E             - intc
[ 1168s] E             + int32)
```

You can find the full build log [here](https://build.opensuse.org/build/home:frispete:branches:openSUSE:Factory/openSUSE_Tumbleweed/i586/python-pandas/_log)"
580606689,32679,Fix wrong docstring in qcut,masterpiga,closed,2020-03-13T13:38:18Z,2020-03-13T22:20:32Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
580781153,32686,Unable to store NaT in parquet format.,IlyaOrson,closed,2020-03-13T18:49:13Z,2020-03-13T23:13:47Z,"#### Code Sample

```python
import pandas as pd
df = pd.DataFrame({""date"": ["""", ""2019-05-01""]})
df.date = pd.to_datetime(df.date).dt.date
df.to_parquet(""issue_NaT_parquet"")
```

#### Problem description

The above gives me the following error:
<details>

> ---------------------------------------------------------------------------
> TypeError                                 Traceback (most recent call last)
> <ipython-input-11-432405bef6ac> in <module>
> ----> 1 df.to_parquet(""test.parquet"")
> 
> ~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\util\_decorators.py in wrapper(*args, **kwargs)
>     212                 else:
>     213                     kwargs[new_arg_name] = new_arg_value
> --> 214             return func(*args, **kwargs)
>     215
>     216         return cast(F, wrapper)
> 
> ~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\core\frame.py in to_parquet(self, path, engine, compression, index, partition_cols, **kwargs)
> 
>    2114             index=index,
>    2115             partition_cols=partition_cols,
> -> 2116             **kwargs,
>    2117         )
>    2118
> 
> ~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\io\parquet.py in to_parquet(df, path, engine, compression, index, partition_cols, **kwargs)
>     262         index=index,
>     263         partition_cols=partition_cols,
> --> 264         **kwargs,
>     265     )
>     266
> 
> ~\AppData\Local\Continuum\miniconda3\lib\site-packages\pandas\io\parquet.py in write(self, df, path, compression, coerce_timestamps, index, partition_cols,
>  **kwargs)
>      99             from_pandas_kwargs[""preserve_index""] = index
>     100
> --> 101         table = self.api.Table.from_pandas(df, **from_pandas_kwargs)
>     102         if partition_cols is not None:
>     103             self.api.parquet.write_to_dataset(
> 
> ~\AppData\Local\Continuum\miniconda3\lib\site-packages\pyarrow\table.pxi in pyarrow.lib.Table.from_pandas()
> 
> ~\AppData\Local\Continuum\miniconda3\lib\site-packages\pyarrow\pandas_compat.py in dataframe_to_arrays(df, schema, preserve_index, nthreads, columns, safe)
> 
>     573     if nthreads == 1:
>     574         arrays = [convert_column(c, f)
> --> 575                   for c, f in zip(columns_to_convert, convert_fields)]
>     576     else:
>     577         from concurrent import futures
> 
> ~\AppData\Local\Continuum\miniconda3\lib\site-packages\pyarrow\pandas_compat.py in <listcomp>(.0)
>     573     if nthreads == 1:
>     574         arrays = [convert_column(c, f)
> --> 575                   for c, f in zip(columns_to_convert, convert_fields)]
>     576     else:
>     577         from concurrent import futures
> 
> ~\AppData\Local\Continuum\miniconda3\lib\site-packages\pyarrow\pandas_compat.py in convert_column(col, field)
>     558
>     559         try:
> --> 560             result = pa.array(col, type=type_, from_pandas=True, safe=safe)
>     561         except (pa.ArrowInvalid,
>     562                 pa.ArrowNotImplementedError,
> 
> ~\AppData\Local\Continuum\miniconda3\lib\site-packages\pyarrow\array.pxi in pyarrow.lib.array()
> 
> ~\AppData\Local\Continuum\miniconda3\lib\site-packages\pyarrow\array.pxi in pyarrow.lib._ndarray_to_array()
> 
> TypeError: an integer is required (got type datetime.date)

</details>

I think this is an issue with pyarrow, I post here just for reference.

#### Expected Output

Parquet with null values mixed with date values.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : 0.4.0
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
464497374,27243,BUG: merge_asof with multiple by columns with tz,mroeschke,closed,2019-07-05T07:14:18Z,2020-03-14T02:08:52Z,"- [x] closes #26649
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
571746115,32282,REF: make DatetimeIndex._simple_new actually simple,jbrockmendel,closed,2020-02-27T00:37:35Z,2020-03-14T02:29:29Z,
580982571,32695,CLN: remove unused int_block,jbrockmendel,closed,2020-03-14T01:53:08Z,2020-03-14T03:01:13Z,
580322986,32671,REF: put all post-processing at end of DataFrame._reduce,jbrockmendel,closed,2020-03-13T02:00:15Z,2020-03-14T03:04:51Z,there's some nasty try/except logic in DataFrame._reduce.  This is simplifying the code _around_ that.
580863790,32691,CLN: remove unused kwargs from BlockManager.downcast,jbrockmendel,closed,2020-03-13T21:28:42Z,2020-03-14T03:12:53Z,add some annotations
579998594,32662,REF: values-> _values,jbrockmendel,closed,2020-03-12T14:55:52Z,2020-03-14T03:14:32Z,For the affected Index subclasses .values and ._values are the same
575513036,32435,"IntegerArray.astype(""datetime64[ns]"") fails with NAs",jbrockmendel,closed,2020-03-04T15:39:43Z,2020-03-14T03:37:22Z,"tools.datetimes has to dig into IntegerArray internals in a way that in principle should be handled by IntegerArray.astype

```
ser = pd.Series(range(2), dtype=""Int64"")
arr = ser.array

>>> arr.astype(""datetime64[ns]"")
array(['1970-01-01T00:00:00.000000000', '1970-01-01T00:00:00.000000001'],
      dtype='datetime64[ns]')

arr[-1] = pd.NA

>>> arr.astype(""datetime64[ns]"")
ValueError: cannot convert to 'datetime64[ns]'-dtype NumPy array with missing values. Specify an appropriate 'na_value' for this dtype.

```"
557738337,31472,DOC: Fix examples in documentation,ShaharNaveh,closed,2020-01-30T20:36:31Z,2020-03-14T14:05:25Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
577336630,32518,CLN: removed unused import,ShaharNaveh,closed,2020-03-07T13:43:01Z,2020-03-14T14:06:31Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

Not 100% sure about this, can close anytime if not a good PR"
576180719,32458,CLN: Removed unused variable,ShaharNaveh,closed,2020-03-05T11:20:26Z,2020-03-14T14:07:03Z,"I can't see the reason for ```res_view``` to exist, the CI should tell me if I am wrong.

Also this is removing one warning when compiling ```pandas/_libs/internals.pyx```

This is the warning that is removing:

```
pandas/_libs/internals.c:8952:36: warning: ‘__pyx_t_23’ may be used uninitialized in this function [-Wmaybe-uninitialized]
 8952 |         __pyx_cur_scope->__pyx_t_9 = __pyx_t_23;
```"
580526550,32677,CLN: Remove PY2 compat code,ShaharNaveh,closed,2020-03-13T11:00:16Z,2020-03-14T14:07:29Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

---

Benchmarks:

#### Master:

```
In [1]: import pandas as pd                                                                                   

In [2]: s = pd.Series(list(""ABCA""))                                                                           

In [3]: %timeit pd.get_dummies(s)                                                                             
482 µs ± 752 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

---

#### PR:

```
In [1]: import pandas as pd                                                                                   

In [2]: s = pd.Series(list(""ABCA""))                                                                           

In [3]: %timeit pd.get_dummies(s)                                                                             
479 µs ± 2.43 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```"
576821593,32485,"CLN: Replace isinstance(foo, Class) with isinstance(foo, ABCClass)",ShaharNaveh,closed,2020-03-06T09:59:31Z,2020-03-14T14:08:14Z,"- [x] ref #27353
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
573289871,32365,"CLN: Replaced ""bool_t"" with ""builtins.bool""",ShaharNaveh,closed,2020-02-29T12:58:10Z,2020-03-14T14:08:43Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
580813484,32690,DOC: filter method example is more clear,epizzigoni,closed,2020-03-13T19:59:42Z,2020-03-14T14:28:05Z,"Added the print of the original example DataFrame. I was struggling to understand the example without looking at the starting point of it. I took the chance to do my first contribution to the project. Hopefully it is only the beginning!
"
581105844,32698,to_sql erases timezone information,lzlarryli,closed,2020-03-14T08:27:22Z,2020-03-14T15:36:09Z,"#### Code Sample, a copy-pastable example if possible

```
In [3]: import sqlalchemy 
   ...: import pandas as pd                                                     

In [4]: conn = sqlalchemy.create_engine('postgres://xxx:xxx@xxx/xxx')                                                                   

In [5]: dates = pd.date_range('2018-01-01', periods=5, freq='6H', tz='Europe/Brussels') 
   ...: expected = pd.DataFrame({'nums': range(5)}, index=dates)                                                    

In [6]: expected                                                                                                    
Out[6]: 
                           nums
2018-01-01 00:00:00+01:00     0
2018-01-01 06:00:00+01:00     1
2018-01-01 12:00:00+01:00     2
2018-01-01 18:00:00+01:00     3
2018-01-02 00:00:00+01:00     4

In [7]: expected.to_sql('test', conn)                                                                               

In [8]: pd.read_sql_table('test', conn)                                                                             
Out[8]: 
                      index  nums
0 2017-12-31 23:00:00+00:00     0
1 2018-01-01 05:00:00+00:00     1
2 2018-01-01 11:00:00+00:00     2
3 2018-01-01 17:00:00+00:00     3
4 2018-01-01 23:00:00+00:00     4
```
#### Problem description

This fails the round-trip expectation. In particular, `to_sql` erases the timezone information. The timezone information is useful in some applications. Otherwise the timestamp would probably not have timezone in the first place and UTC would be assumed and used everywhere. Note that this behavior is unexpected because postgres supports `timestamptz` and in the underlying SQL table the column for `index` is the correct type `timestamptz`.

#### Expected Output

```
                      index  nums
2018-01-01 00:00:00+01:00     0
2018-01-01 06:00:00+01:00     1
2018-01-01 12:00:00+01:00     2
2018-01-01 18:00:00+01:00     3
2018-01-02 00:00:00+01:00     4
```

#### Output of ``pd.show_versions()``

```
In [9]: pd.show_versions()                                                                                          

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.2.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.3.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 41.2.0
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.15
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```"
577357344,32519,TST: fix test creating invalid CategoricalBlock,jbrockmendel,closed,2020-03-07T16:24:17Z,2020-03-14T15:50:59Z,The incorrect tests here turned up when trying to replace Block.get_values calls with simpler alternatives.
577391361,32527,TST: make tests stricter,jbrockmendel,closed,2020-03-07T19:52:24Z,2020-03-14T16:09:26Z,"I expect to see some platform-specific failures, will be nice to document those where possible."
579003871,32612,"CLN: remove Block.array_dtype, SingleBlockManager.array_dtype",jbrockmendel,closed,2020-03-11T04:35:40Z,2020-03-14T16:27:26Z,We can get a more accurately-dtyped empty array instead.
576604619,32476,ENH: implement ExtensionIndex.insert,jbrockmendel,closed,2020-03-06T00:03:26Z,2020-03-14T16:46:23Z,Sort of.
577114985,32501,Disallow lossy SparseArray conversion,jbrockmendel,closed,2020-03-06T18:58:19Z,2020-03-14T16:47:12Z,cc @TomAugspurger 
580931733,32693,CLN: avoid _ndarray_values in reshape.merge,jbrockmendel,closed,2020-03-13T23:33:43Z,2020-03-14T16:47:23Z,
574908096,32420,ENH: Categorical.fillna allow Categorical/ndarray,jbrockmendel,closed,2020-03-03T20:03:02Z,2020-03-14T16:52:48Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref #32414.

cc @TomAugspurger ATM the new test implemented in tests.arrays.categorical.test_missing is failing on the ndarray case for reasons that I dont fully grok.  Are you familiar with this?"
579610790,32644,ENH: implement EA.size,jbrockmendel,closed,2020-03-11T23:59:44Z,2020-03-14T17:00:56Z,I was surprised this didnt already exist.
578877726,32597,REF: implement nanops.na_accum_func,jbrockmendel,closed,2020-03-10T21:41:06Z,2020-03-14T17:03:00Z,this is a follow-up that was requested a few months ago
581048752,32697,DOC: Fix EX01 in pandas.DataFrame.idxmin,farhanreynaldo,closed,2020-03-14T05:30:19Z,2020-03-14T17:36:07Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. 

```
################################################################################
################################## Validation ##################################
################################################################################"
581294807,32705,CI: azure pipelines are not in PRS,jreback,closed,2020-03-14T17:23:14Z,2020-03-14T19:30:02Z,cc @pandas-dev/pandas-core 
578972003,32610,CLN: Clean frame/test_constructors.py,dsaxton,closed,2020-03-11T02:41:46Z,2020-03-14T19:44:07Z,I think these classes are all imported directly so don't need to reference the pandas namespace
576815124,32484,CLN: Remove redundant index test from tests/base/test_ops.py,SaturnFromTitan,closed,2020-03-06T09:48:04Z,2020-03-14T20:12:58Z,"part of #23877
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Boolean indexing is already thoroughly tested in `test_series_mask_boolean` in `tests/indexes/test_na_indexing.py` - [see here](https://github.com/pandas-dev/pandas/blob/master/pandas/tests/indexing/test_na_indexing.py#L7-L61)"
575029541,32425,"BUG: Fix DataFrame.apply(..., raw=True) not calling with raw array",kernc,closed,2020-03-03T23:37:50Z,2020-03-16T09:25:35Z,"- [x] closes https://github.com/pandas-dev/pandas/issues/32423
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
577431890,32535,BUG: retain tz in to_records,jbrockmendel,closed,2020-03-08T02:13:42Z,2020-03-14T21:29:43Z,plus the initial motivation: get rid of two of our `_internal_get_values` calls (of which i count 16 left in master)
551728595,31118,ERR: better error message on unparseable datetimes,baevpetr,closed,2020-01-18T06:20:43Z,2020-03-14T21:38:23Z,"- [ ] closes #10720
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
520564219,29524,ENH: Support for column specific col_space argument for frame.to_html…,AakankshaAshok,closed,2019-11-10T07:12:21Z,2020-03-14T21:39:21Z,"…() method (#28917)

- [ ] closes #28917
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] what's new: the col_space argument of to_html() can also take a dict or list to specify different col_space for different columns. 
"
581318383,32707,CLN: unnecessary usages of Block.get_values,jbrockmendel,closed,2020-03-14T18:29:17Z,2020-03-14T21:41:11Z,
496014974,28541,BUG: Groupby selection context not being properly reset,christopherzimmerman,closed,2019-09-19T20:38:54Z,2020-03-14T21:48:38Z,"- [x] closes #28523 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
578117499,32558,BUG: pivot_table losing tz,jbrockmendel,closed,2020-03-09T18:46:52Z,2020-03-14T23:31:12Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This gets rid of the last values_from_object usage (pending other PRs)

`_tile_compat` likely makes more sense as an Index method.  I kept it here as a proof of concept because I think we actually need it in the EA interface too.  Being able to broadcast a length=1 EA to a length=N EA will be necessary for some of the arithmetic perf going on."
558043823,31488,Unclosed file on EmptyDataError,fominok,closed,2020-01-31T10:25:27Z,2020-03-15T00:37:42Z,"
#### Code Sample, a copy-pastable example if possible

```python
import pandas
import psutil

proc = psutil.Process()
try:
    data = pandas.read_csv('kek.csv', engine='python')  # kek.csv is an empty file
except pandas.errors.EmptyDataError:
    pass

print(proc.open_files())

```
#### Problem description

Hi, there might be an unwanted behavior on attempt to read an empty `.csv` file, providing a
filename instead of handler, so it cannot be closed anymore by user on exception.

#### Expected Output

`[]`

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.13-1.el7.elrepo.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.0
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.2
setuptools       : 45.1.0
Cython           : 0.29.14
pytest           : 5.0.1
hypothesis       : 4.32.2
sphinx           : 2.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : 0.9.2
psycopg2         : 2.8.3 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : 0.15.0
pytables         : None
pytest           : 5.0.1
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.7
tables           : 3.5.2
tabulate         : 0.8.3
xarray           : 0.12.1
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8
numba            : 0.45.1
</details>
"
558679113,31568,Description of Plotting Backends added in the end.,mehersaipreetam,closed,2020-02-02T10:57:17Z,2020-03-08T05:20:20Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
573180857,32354,DOC : fix errors docstrings pandas.to_numeric,isoletslicer,closed,2020-02-29T05:14:06Z,2020-03-08T10:55:55Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [x ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

added docstrings information about arg descriptions, downcast type 'int', and Return type
"
563078154,31870,REGR: changed return type for multi-dimensional indexing,h-vetinari,closed,2020-02-11T09:38:05Z,2020-03-08T11:56:39Z,"I kept my code free of deprecation warnings for 0.25.3 but upgrading still broke it.

I can now see on the tracker that multi-dimensional indexing has been deprecated #27837, #30588 #30867, but it seems the introduction of the deprecation warning has changed the behaviour itself.


```python
>>> pd.__version__
'0.25.3'
>>> idx = pd.Index([f'ID_{x}' for x in range(10)])
>>> selector = np.array(np.random.randint(0, 10, (3, 10)))
>>> selector
array([[2, 3, 4, 9, 1, 8, 5, 6, 3, 3],
       [3, 5, 0, 7, 1, 3, 2, 0, 2, 8],
       [8, 4, 6, 8, 0, 4, 3, 4, 5, 7]])
>>> idx[selector]
Index(['ID_2', 'ID_3', 'ID_4', 'ID_9', 'ID_1', 'ID_8', 'ID_5', 'ID_6', 'ID_3',
       'ID_3', 'ID_3', 'ID_5', 'ID_0', 'ID_7', 'ID_1', 'ID_3', 'ID_2', 'ID_0',
       'ID_2', 'ID_8', 'ID_8', 'ID_4', 'ID_6', 'ID_8', 'ID_0', 'ID_4', 'ID_3',
       'ID_4', 'ID_5', 'ID_7'],
      dtype='object')
>>> idx[selector].values
array([['ID_2', 'ID_3', 'ID_4', 'ID_9', 'ID_1', 'ID_8', 'ID_5', 'ID_6',
        'ID_3', 'ID_3'],
       ['ID_3', 'ID_5', 'ID_0', 'ID_7', 'ID_1', 'ID_3', 'ID_2', 'ID_0',
        'ID_2', 'ID_8'],
       ['ID_8', 'ID_4', 'ID_6', 'ID_8', 'ID_0', 'ID_4', 'ID_3', 'ID_4',
        'ID_5', 'ID_7']], dtype=object)
```

On `v1.0.1`, this yields an `np.array` instead of a `pd.Index`, and so the `.values` call fails.
```python
>>> idx[selector]
__main__:1: DeprecationWarning: Support for multi-dimensional indexing (e.g. `index[:, None]`) on an Index is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.
array([['ID_2', 'ID_3', 'ID_4', 'ID_9', 'ID_1', 'ID_8', 'ID_5', 'ID_6',
        'ID_3', 'ID_3'],
       ['ID_3', 'ID_5', 'ID_0', 'ID_7', 'ID_1', 'ID_3', 'ID_2', 'ID_0',
        'ID_2', 'ID_8'],
       ['ID_8', 'ID_4', 'ID_6', 'ID_8', 'ID_0', 'ID_4', 'ID_3', 'ID_4',
        'ID_5', 'ID_7']], dtype=object)
```

Note also that the warning is (at least for me) not raised on python 3.6, only on 3.7.

If desired, I can flesh out the reasons for needing this multi-dimensional indexing. I wonder how I'll be able to replace it."
444451632,26412,Using .loc.__setitem__ with integer slices does not raise,lumbric,closed,2019-05-15T13:51:49Z,2020-03-08T15:45:26Z,"#### Code Sample

```python
>>> d = pd.DataFrame({'a': range(5), 'b': range(2,7), 'c': range(3,8)})
>>> d
   a  b  c
0  0  2  3
1  1  3  4
2  2  4  5
3  3  5  6
4  4  6  7
>>> d.loc[0:2, 0:2] = 42   # note that loc is used with an integer slice! should this raise?
>>> d  # this is somehow the expected result
    a   b  c
0  42  42  3
1  42  42  4
2  42  42  5
3   3   5  6
4   4   6  7
>>> d.iloc[0:2, 0:2]        # expected result
    a   b
0  42  42
1  42  42
>>> d.loc[0:2, 0:2]        # this is the same syntax as above, but raises
...
TypeError: cannot do slice indexing on <class 'pandas.core.indexes.base.Index'> with these indexers [0] of <class 'int'>
```
#### Problem description

I'd expect a symmetry in `d.loc.__getitem__` and `d.loc.__setitem__`. At the moment only __getitem__ raises if integer slices are passed. Usage of `.loc` was probably a mistake, instead `.iloc` should have been used. Even if it works correctly, it is confusing for users.

Probably this is simply a missing exception in the special case of `d.loc.__setitem__` being called with integer slices. 

See #13831 for a very similar problems with `pd.Series`.

#### Expected Output

Assuming `d.loc.__getitem__` should raise for integer slices (as it does), then also `d.loc.__setitem__` should raise:

```
>>> d.loc[0:2, 0:2]
...
SomException: int & loc don't go together
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.8.final.0
python-bits: 64
OS: Linux
OS-release: 4.18.0-20-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 3.6.4
pip: 9.0.1
setuptools: 40.8.0
Cython: None
numpy: 1.16.2
scipy: 1.2.1
pyarrow: None
xarray: 0.11.3
IPython: 5.5.0
sphinx: 1.7.9
patsy: 0.4.1+dev
dateutil: 2.8.0
pytz: 2019.1
blosc: 1.5.1
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.5
feather: None
matplotlib: 2.2.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: 4.2.5
bs4: 4.5.0
html5lib: 0.999999999
sqlalchemy: None
pymysql: None
psycopg2: 2.7.5 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>"
223976757,16121,.loc interprets slice as positional rather than label-based when setting,toobaz,closed,2017-04-24T23:12:02Z,2020-03-08T15:45:26Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: df = pd.DataFrame(-1, index=['i', 'ii', 'iii'], columns=pd.MultiIndex.from_tuples([['A', 'a'], ['B', 'b']]))

In [3]: df.loc[1:, 'A'] = ''

In [4]: df
Out[4]: 
      A  B
      a  b
i    -1 -1
ii      -1
iii     -1

In [5]: df.loc[1:, 'A']
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-5-98b6153a5b09> in <module>()
----> 1 df.loc[1:, 'A']

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in __getitem__(self, key)
   1322             except (KeyError, IndexError):
    948 
    949         # we maybe be using a tuple to represent multiple dimensions here

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_nested_tuple(self, tup)
   1020 
   1021             current_ndim = obj.ndim
-> 1022             obj = getattr(obj, self.name)._getitem_axis(key, axis=axis)
   1023             axis += 1
   1024 

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_axis(self, key, axis)
   1503         if isinstance(key, slice):
   1504             self._has_valid_type(key, axis)
-> 1505             return self._get_slice_axis(key, axis=axis)
   1506         elif is_bool_indexer(key):
   1507             return self._getbool_axis(key, axis=axis)

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in _get_slice_axis(self, slice_obj, axis)
   1353         labels = obj._get_axis(axis)
   1354         indexer = labels.slice_indexer(slice_obj.start, slice_obj.stop,
-> 1355                                        slice_obj.step, kind=self.name)
   1356 
   1357         if isinstance(indexer, slice):

/home/pietro/nobackup/repo/pandas/pandas/indexes/base.py in slice_indexer(self, start, end, step, kind)
   3247         """"""
   3248         start_slice, end_slice = self.slice_locs(start, end, step=step,
-> 3249                                                  kind=kind)
   3250 
   3251         # return a slice

/home/pietro/nobackup/repo/pandas/pandas/indexes/base.py in slice_locs(self, start, end, step, kind)
   3428         start_slice = None
   3429         if start is not None:
-> 3430             start_slice = self.get_slice_bound(start, 'left', kind)
   3431         if start_slice is None:
   3432             start_slice = 0

/home/pietro/nobackup/repo/pandas/pandas/indexes/base.py in get_slice_bound(self, label, side, kind)
   3367         # For datetime indices label may be a string that has to be converted
   3368         # to datetime boundary according to its resolution.
-> 3369         label = self._maybe_cast_slice_bound(label, side, kind)
   3370 
   3371         # we need to look up the label

/home/pietro/nobackup/repo/pandas/pandas/indexes/base.py in _maybe_cast_slice_bound(self, label, side, kind)
   3325         # this is rejected (generally .loc gets you here)
   3326         elif is_integer(label):
-> 3327             self._invalid_indexer('slice', label)
   3328 
   3329         return label

/home/pietro/nobackup/repo/pandas/pandas/indexes/base.py in _invalid_indexer(self, form, key)
   1447                         ""indexers [{key}] of {kind}"".format(
   1448                             form=form, klass=type(self), key=key,
-> 1449                             kind=type(key)))
   1450 
   1451     def get_duplicates(self):

TypeError: cannot do slice indexing on <class 'pandas.indexes.base.Index'> with these indexers [1] of <class 'int'>

```
#### Problem description

``In[3]:`` should already raise an error (unless the label **1** is compared against the labels in the index, in which case it should certainly be smaller or larger than each of them).

#### Expected Output

An error

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 3.16.0-4-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.19.0+783.gcd35d22a0
pytest: 3.0.6
pip: 1.5.6
setuptools: 5.5.1
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.18.1
xarray: None
IPython: 5.2.2
sphinx: None
patsy: 0.4.1+dev
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: None
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.0.0
openpyxl: 2.3.0
xlrd: 0.9.2
xlwt: 1.2.0
xlsxwriter: None
lxml: 3.7.1
bs4: None
html5lib: 0.999
sqlalchemy: 0.9.8
pymysql: None
psycopg2: None
jinja2: 2.7.3
s3fs: None
pandas_gbq: None
pandas_datareader: None


</details>
"
572358293,32310,BUG: PeriodIndex.asof_locs,jbrockmendel,closed,2020-02-27T21:04:06Z,2020-03-08T16:04:31Z,
562292782,31840,BUG/DEPR: loc.__setitem__ incorrectly accepting positional slices,jbrockmendel,closed,2020-02-10T02:42:55Z,2020-03-08T16:08:01Z,"- [x] closes #26412
- [x] closes #16121
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I plan to flesh out the tests before this goes in.

cc @toobaz "
577177144,32504,CLN: avoid values_from_object in construction,jbrockmendel,closed,2020-03-06T21:10:41Z,2020-03-08T16:18:52Z,
554993305,31296,"PERF: do DataFrame.op(series, axis=0) blockwise",jbrockmendel,closed,2020-01-24T22:57:15Z,2020-03-08T16:28:25Z,"Also fixes the same bug as #31271 (with the same test ported), so if this is accepted that will be closeable.

~2000x speedups for very-wide mixed-dtype cases.

```
arr = np.arange(10**6).reshape(100, -1)    
df = pd.DataFrame(arr)
df[""C""] = 1.0
ser = df[0]

In [11]: %timeit df.eq(ser, axis=0)
1.58 s ± 20 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)  # <-- master
595 µs ± 79.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  <-- PR

In [12]: %timeit df.add(ser, axis=0)
2.06 s ± 52.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)  # <-- master                                                                                                                   
1.04 ms ± 5.07 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)  # <-- PR

```"
481735734,27954,Two bugs with datetime and df to and from csv files,thouston,closed,2019-08-16T18:41:30Z,2020-03-08T17:20:36Z,"1- If index in a df is datetime.date format, when you export it to csv files it returns as timestamp with date and 00 for hours, minutes seconds

2- If column type in dateframe is %b-%y when you export it to csv and back it converts the year to day and each value is current year. 
"
349232885,22263,int64 objects returned from pandas.read_sql_query on windows with sqlalchemy engine,jl0000285,closed,2018-08-09T18:06:46Z,2020-03-08T17:24:47Z,"# Example pseudo code

~~~python
>> import pandas 
>> sql = ""some_sql_statement""
>> engine=""some_sqlalchemy_engine""
>> df = pandas.read_sql_query(sql, engine)
>> df.dtypes #Result contains several numpy.int64 objects, even on windows machines 
~~~

On macs (and perhaps other unix systems), pandas.read_sql_query() tends to return numpy.int64, objects. In this context this return result is fine, as numpy.int64 objects sublcass pythons native int object on macs. This same method also returns numpy.int64 objects on windows, which do not sublcass pythons native int object and thus cause errors whenever used with any other library. 

Proposed solution: Is it not possible for pandas to detect the current operating system and adjust the data types of its resulting data frame natively? "
409964393,25307,Wrong result for float32 Series when using bottleneck,lumbric,closed,2019-02-13T19:29:17Z,2020-03-08T18:13:09Z,"#### Minimal example

Requires bottleneck, numpy and pandas to be installed:

```python
>>> import numpy as np
>>> import pandas as pd
>>> import bottleneck as bn
>>> data = np.ones(2**25, dtype=np.float32)
>>> pd.Series(data).mean()  # wrong
0.5
>>> bn.nanmean(data)  # wrong
0.5
>>> data.mean()  # correct
1.0
```

#### Problem description

The `mean()` of large float32 Series is wrong when [bottleneck](https://github.com/kwgoodman/bottleneck/) is used. Uninstalling bottleneck or using float64 is a valid workaround. xarray is or has been affected too, see pydata/xarray#1346. 

<s>Bottleneck's documentation explicitly mentions that [no error is raised in case of an overflow](https://kwgoodman.github.io/bottleneck-doc/reference.html?highlight=overflow#bottleneck.nanmean), not sure if this is still to be considered as bug in bottleneck.</s> Anyhow since it seems quite severe, I want to raise attention here too.

**Update:** This is not an overflow, it's a numerical error (which is very high because bottleneck does not use [pairwise summation](https://en.wikipedia.org/wiki/Pairwise_summation)).

[Bottleneck's implementation of mean()](https://github.com/kwgoodman/bottleneck/blob/master/bottleneck/src/reduce_template.c#L162).

#### Related issues

* same thing in xarray: pydata/xarray#1346
* same thing in aospy: spencerahill/aospy#217
* similar bug in pandas with bottleneck, but related to int not float: pandas-dev/pandas#15453  and kwgoodman/bottleneck#163
* another very similar but older bug in pandas with bottleneck but related to int not float: pandas-dev/pandas#6915 kwgoodman/bottleneck#83
* something different, not to be confused - race conditions when reading netcdf files: dask/dask#2095
* probably not related, but who knows: kwgoodman/bottleneck#164


#### Expected Output

```python
>>> data = np.ones(2**25, dtype=np.float32)
>>> pd.Series(data).mean()
1.0
```

#### Output of ``pd.show_versions()``

<details>

```bash
$ pip3 freeze 
Bottleneck==1.2.1
numpy==1.16.1
pandas==0.24.1
...
```

```python
>>> pd.show_versions()                                                                                           
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.7.final.0
python-bits: 64
OS: Linux
OS-release: 4.18.0-13-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.1
pytest: None
pip: 19.0.2
setuptools: 40.8.0
Cython: None
numpy: 1.16.1
scipy: None
pyarrow: None
xarray: 0.11.3
IPython: 7.2.0
sphinx: None
patsy: None
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: 1.2.1
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
```
</details>
"
433279146,26100,is_categorical_dtype depends on boolness of the object passed.,rsaim,closed,2019-04-15T13:21:41Z,2020-03-08T18:23:38Z,"```python
In [76]: from pandas.core.dtypes.common import is_categorical_dtype

In [77]: class Foo(object):
    ...:     def __bool__(self):
    ...:         raise NotImplementedError(""Foo doesn't support boolness"")
    ...:
    ...:     def __eq__(self, other):
    ...:         return Foo()

In [78]: is_categorical(Foo())
NotImplementedError                       Traceback (most recent call last)
<ipython-input-78-e619c3f62c63> in <module>
----> 1 is_categorical(Foo())

/usr/python3.7/site-packages/pandas/core/dtypes/common.py in is_categorical(arr)
    289     """"""
    290
--> 291     return isinstance(arr, ABCCategorical) or is_categorical_dtype(arr)
    292
    293

/usr/python3.7/site-packages/pandas/core/dtypes/common.py in is_categorical_dtype(arr_or_dtype)
    600     if arr_or_dtype is None:
    601         return False
--> 602     return CategoricalDtype.is_dtype(arr_or_dtype)
    603
    604

/usr/python3.7/site-packages/pandas/core/dtypes/base.py in is_dtype(cls, dtype)
    110             return True
    111         try:
--> 112             return cls.construct_from_string(dtype) is not None
    113         except TypeError:
    114             return False

/usr/python3.7/site-packages/pandas/core/dtypes/dtypes.py in construct_from_string(cls, string)
    456         it's not possible """"""
    457         try:
--> 458             if string == 'category':
    459                 return cls()
    460             else:

<ipython-input-77-645d89edd55f> in __bool__(self)
      1 class Foo(object):
      2     def __bool__(self):
----> 3         raise NotImplementedError(""Foo doesn't support boolness"")
      4
      5     def __eq__(self, other):

NotImplementedError: Foo doesn't support boolness
```
#### Problem description
is_categorical breaks if \_\_boo\_\_ of an object returns another (or same) object for which \_\_bool\_\_ raises an exception. We return the an object of **Foo** itself in the reproducer above to demonstrate this behavior. This affects `pandas.to_datetime` and probably other functionalities as well.

I guess converting an object to datetime should not depend on it's bool'ness.

#### Expected Output
True or False

#### Output of ``pd.show_versions()``
In [79]: pandas.show_versions()
<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Linux
OS-release: 3.10.0-957.saim.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 4.4.0
pip: 10.0.1
setuptools: 39.0.1
Cython: 0.29.5
numpy: 1.14.3
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 7.3.0
sphinx: 2.0.0
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2017.3
blosc: 1.8.1
bottleneck: None
tables: 3.4.4
numexpr: 2.6.9
feather: None
matplotlib: 2.2.3
openpyxl: 2.6.2
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.5
lxml.etree: 3.6.0
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.2.5+4.saim
pymysql: None
psycopg2: 2.7.7 (dt dec pq3 ext)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>"
573215829,32360,DOC: Update the pandas.DatetimeIndex docstring,Hori75,closed,2020-02-29T06:43:41Z,2020-03-09T09:22:50Z,"- [x] closes https://github.com/pandanistas/pandanistas_sprint_ui2020/issues/7
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Output of ```python scripts/validate_docstrings.py pandas.DatetimeIndex```
```
################################################################################
################################## Validation ##################################
################################################################################

1 Errors found:
	No examples section found
```
"
569573673,32212,DOC: DataFrame.ewm clean-up,sursu,closed,2020-02-23T22:56:41Z,2020-03-09T14:25:48Z,"Cosmetic change.
"
568857093,32146,REGR: Series repr of object Index with bools and NaN is wrong,disimone,closed,2020-02-21T10:06:03Z,2020-03-09T15:42:17Z,"#### Code Sample, a copy-pastable example if possible

```
import pandas as pd
# a series with booleans and nan
pd.Series([False,True,True,pd.NA]).value_counts(dropna=False)
True     2
False    1
True     1
dtype: int64

# check the actual index of the value_counts result
pd.Series([False,True,True,pd.NA]).value_counts(dropna=False).index
Index([True, False, nan], dtype='object')

# a similar Serie, with ints instead of nans, seems to work ok
pd.Series([0,1,1,pd.NA]).value_counts(dropna=False)
1.0    2
0.0    1
NaN    1
dtype: int64
```
#### Problem description
As shown in the example, the __repr__ of the value_counts result is apparently wrong when booleans are in the serie. It should report nan as a possible value, instead it maps it to True.
Note that this is limited to series with boolean. A similar example with ints works ok.


#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-76-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : None
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
555164155,31318,REF: make PeriodIndex.get_value wrap PeriodIndex.get_loc,jbrockmendel,closed,2020-01-26T01:34:53Z,2020-03-12T13:31:07Z,"AFAICT the only behavior this changes is in how datetime objects are treated.  After this they are more consistent.

```
dti = pd.date_range(""2016-01-01"", periods=3, freq=""MS"")
pi = dti.to_period(""H"")
ser = pd.Series(range(7, 10), index=pi)

key = dti[0]
```

master
```
>>> pi.get_loc(key)
0
>>> pi.get_value(ser, key)
KeyError: Timestamp('2016-01-01 00:00:00', freq='MS')
>>> ser.loc[key]
7
>>> ser[key]
KeyError: Timestamp('2016-01-01 00:00:00', freq='MS')
```

Under this PR, the get_value and `__getitem__` calls both return 7."
577553700,32545,CLN: to_dense->np.asarray,jbrockmendel,closed,2020-03-08T19:41:28Z,2020-03-09T16:38:51Z,"This is the only non-test use of Categorical.to_dense, which is slightly different from _internal_get_values (for SparseArray the two methods are identical), and so liable to cause confusion."
551656439,31114,CI: Adding script to validate consistent and correct capitalization among headings in documentation (#26941),tonywu1999,closed,2020-01-17T22:12:44Z,2020-03-09T16:49:12Z,"Closes #26941

This script should be running correctly to validate capitalization for various rst files.  Many keywords will need to be added into the script as more and more files are tested on this script. Currently testing on the following rst files through code_checks.sh:

doc/source/development/contributing.rst
doc/source/index.rst doc/source/ecosystem.rst

contributing.rst and ecosystem.rst should produce an exit code of 1, indicating there are incorrect capitalization in their headings."
576603333,32475,"CLN: assorted cleanups, annotations",jbrockmendel,closed,2020-03-05T23:59:41Z,2020-03-09T17:05:56Z,
569393486,32185,BUG: 2D DTA/TDA arithmetic with object-dtype,jbrockmendel,closed,2020-02-22T21:07:32Z,2020-03-09T17:45:22Z,"The main motivation for this is that it is prerequisite for implementing frame-with-frame ops blockwise.  It also fixes a few broken cases:

- dta - dta.astype(object) currently raises TypeError
- ditto dti - dti.astype(object)
- tda + dta.astype(object) returns ndarray of timestamps instead of DatetimeArray

"
559890557,31658,API: allow step!=1 slice with IntervalIndex,jbrockmendel,closed,2020-02-04T18:08:44Z,2020-03-09T18:05:33Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

See discussion https://github.com/pandas-dev/pandas/pull/16386#discussion_r374807689

cc @jorisvandenbossche @jschendel "
570732480,32242,"BUG: Fixed bug, where pandas._libs.lib.maybe_convert_objects function improperly handled arrays with bools and NaNs",AnnaDaglis,closed,2020-02-25T17:42:41Z,2020-03-09T19:17:42Z,"- [x] closes #32146
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
575784694,32449,CI: Fix flaky test_value_counts_null,SaturnFromTitan,closed,2020-03-04T21:21:24Z,2020-03-09T19:32:12Z,"As mentioned by @simonjayhawkins in #32438

Trace of the failing test:
https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=29965&view=logs&j=077026cf-93c0-54aa-45e0-9996ba75f6f7&t=e95cf409-86ae-5b4d-6c5f-79395ef75e8f"
578140708,32562,CI: TravisCI Failing,TomAugspurger,closed,2020-03-09T19:30:42Z,2020-03-09T20:03:06Z,"Good: https://travis-ci.org/pandas-dev/pandas/jobs/660156644#L7
Bad: https://travis-ci.org/pandas-dev/pandas/jobs/660210493#L1

https://travis-ci.org/pandas-dev/pandas/jobs/660210493#L175-L177 looks strange.

```
$ export PATTERN=""""(not slow and not network and not clipboard)""""

/home/travis/.travis/functions: eval: line 109: syntax error near unexpected token `('

/home/travis/.travis/functions: eval: line 109: `export PATTERN=""""(not slow and not network and not clipboard)"""" '
```

Most likely something changed on the TravisCI end. Opening this to track it."
494240661,28468,"DataFrame.min/max with axis=1, datetime64[ns, tz] types and NaT values returns NaNs",jkukul,closed,2019-09-16T19:45:07Z,2020-03-09T20:07:38Z,"#### Code Sample, a copy-pastable example if possible

```python
s1 = pd.Series([pd.Timestamp('2017-01-01T10', tz='UTC'), pd.Timestamp('2017-01-01T11', tz='UTC')])
s2 = pd.Series([pd.Timestamp('2017-01-01T12', tz='UTC'), pd.NaT])

df = pd.DataFrame(dict(s1=s1, s2=s2))
df.min(axis=1)
```

produces:

```
0  NaN
1  NaN
dtype: float64
```
#### Problem description

For tz-aware timestamp columns, as soon as `pd.NaT` value is present in any row in any of the columns, than the column-wise `min()/max()` operations returns all `NaN`s. 

If columns are non-tz-aware, then `min()/max()` work as expected, i.e. the result is similar to the one pasted below (except for the timezone part). 

#### Expected Output

```
0   2017-01-01 10:00:00+00:00
1   2017-01-01 11:00:00+00:00
dtype: datetime64[ns, UTC]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.2
pytz             : 2019.2
dateutil         : 2.8.0
pip              : 18.0
setuptools       : 41.2.0
Cython           : 0.27.3
pytest           : 4.4.0
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.2.6
html5lib         : None
pymysql          : None
psycopg2         : 2.7.3.2 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.7.0
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.6
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : 0.11.0
pyarrow          : 0.13.0
pytables         : None
s3fs             : None
scipy            : 1.0.0
sqlalchemy       : 1.3.8
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
577435716,32536,TST: separate out pd.crosstab tests from test_pivot,jbrockmendel,closed,2020-03-08T02:57:28Z,2020-03-09T20:18:35Z,
576059218,32454,DEPS: Removing snappy from local/docs dependencies,datapythonista,closed,2020-03-05T07:54:10Z,2020-03-09T20:47:23Z,"- [X] xref #32417

The docs build should fail if snappy is required to build the docs, and should point out where it's being used. If it's not failed, I think this can be merged and we can remove snappy from the local and docs dependencies.
"
577278286,32513,CLN: remove check_series_type,jbrockmendel,closed,2020-03-07T04:52:54Z,2020-03-09T21:13:50Z,"Trying to clean up assert_series_equal and avoid usage of _internal_get_values; I'm finding that these functions are _weird_, and in particular categorical dtype handling is opaque"
578063496,32554,CLN: remove Categorical.put,jbrockmendel,closed,2020-03-09T17:16:17Z,2020-03-09T21:28:30Z,
578171537,32564,DOC: Add missing question mark icon,stijnvanhoey,closed,2020-03-09T20:29:51Z,2020-03-10T07:18:29Z,"- [x] closes #32469
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I added the question mark to the `_static` folder. When rebuilding the documentation pages locally using the latest version of the `pydata-bootstrap-sphinx-theme`, the `div.highlight` seems to be styled correctly (`background: white`), removing the _strange green around the code block_.
"
576461565,32469,DOC: fix styling (css) of getting started tutorials,jorisvandenbossche,closed,2020-03-05T18:52:26Z,2020-03-10T07:24:41Z,"The sphinx theme got some styling updates, but this seems to have broken the custom styling of the getting started guide a bit.

See eg https://pandas.io/docs/getting_started/intro_tutorials/03_subset_data.html : the question marks are not visible. 
And there is also some strange green around the code block in the box of installation at https://pandas.io/docs/getting_started/index.html


cc @stijnvanhoey "
578382719,32573,Backport PR #32564 on branch 1.0.x (DOC: Add missing question mark icon),meeseeksmachine,closed,2020-03-10T07:19:02Z,2020-03-10T07:54:42Z,Backport PR #32564: DOC: Add missing question mark icon
561297557,31762,Rolling Apply Docs Broken,WillAyd,closed,2020-02-06T22:05:07Z,2020-03-10T08:35:48Z,"A google of ""rolling apply"" for me yields:

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.window.Rolling.apply.html

Which results in an HTTP 403. Working link now is:

https://pandas.pydata.org/docs/reference/api/pandas.core.window.rolling.Rolling.apply.html

As pointed out by @TomAugspurger seems to be an issue with `window.Rolling.apply` vs `window.rolling.Rolling.apply` (/docs should be a symlink to /pandas-docs/stable)"
563218690,31875,DOC: add redirects from Rolling to rolling.Rolling,rushabh-v,closed,2020-02-11T13:48:37Z,2020-03-10T08:35:57Z,- [x] closes #31762 
578416575,32574,Backport PR #31875 on branch 1.0.x (DOC: add redirects from Rolling to rolling.Rolling),meeseeksmachine,closed,2020-03-10T08:35:58Z,2020-03-10T10:15:57Z,Backport PR #31875: DOC: add redirects from Rolling to rolling.Rolling
578174536,32565,TST: Fix bare pytest raises in generic/test_frame.py,andresmcneill,closed,2020-03-09T20:34:14Z,2020-03-10T10:37:41Z,"- [x] refers https://github.com/pandas-dev/pandas/issues/30999
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Adds match arguments to pytest.raises calls in pandas/tests/generic/test_frame.py
"
578226241,32567,TST: add test.indexes.common.Base.create_index and annotate .create_index,topper-123,closed,2020-03-09T22:26:59Z,2020-03-10T11:09:31Z,Makes ``test.indexes.common.Base`` and ``create_index`` a bit easier to work with.
578466491,32576,"Index.insert(-1, value) should add value in the last position",mhabets,closed,2020-03-10T10:07:25Z,2020-03-10T12:22:42Z,"#### Problem description and Expected Output

``Index.insert(-1, value)`` should add value in the last position instead of one before the last position, shouldn't it?

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1

</details>
"
577498656,32540,pdf documentation is not bookmarked,MarianD,closed,2020-03-08T13:11:24Z,2020-03-10T13:39:51Z,"
The last PDF documentation 1.0.1. (Feb 05, 2020) has no table of contents, no bookmarks.   

I made Contents in the form of bookmarks, meanwhile up to level 2.  
The edited (bookmarked) pdf file is here: [pandas.pdf](https://github.com/pandas-dev/pandas/files/4303229/pandas.pdf).
"
559329360,31632,Bug in Series.str.repeat with string dtype and sequence of repeats,TomAugspurger,closed,2020-02-03T20:57:09Z,2020-03-10T14:08:01Z,"#### Code Sample, a copy-pastable example if possible

```pytb
In [1]: import pandas as pd

In [2]: s = pd.Series(['a', None], dtype=""string"")

In [3]: s.str.repeat([1, 2])
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/sandbox/pandas/pandas/core/strings.py in rep(x, r)
    781             try:
--> 782                 return bytes.__mul__(x, r)
    783             except TypeError:

TypeError: descriptor '__mul__' requires a 'bytes' object but received a 'NAType'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-3-a01827562f7a> in <module>
----> 1 s.str.repeat([1, 2])

~/sandbox/pandas/pandas/core/strings.py in wrapper(self, *args, **kwargs)
   1950                 )
   1951                 raise TypeError(msg)
-> 1952             return func(self, *args, **kwargs)
   1953
   1954         wrapper.__name__ = func_name

~/sandbox/pandas/pandas/core/strings.py in repeat(self, repeats)
   2780     @forbid_nonstring_types([""bytes""])
   2781     def repeat(self, repeats):
-> 2782         result = str_repeat(self._parent, repeats)
   2783         return self._wrap_result(result)
   2784

~/sandbox/pandas/pandas/core/strings.py in str_repeat(arr, repeats)
    785
    786         repeats = np.asarray(repeats, dtype=object)
--> 787         result = libops.vec_binop(com.values_from_object(arr), repeats, rep)
    788         return result
    789

~/sandbox/pandas/pandas/_libs/ops.pyx in pandas._libs.ops.vec_binop()
    239                 result[i] = y
    240             else:
--> 241                 raise
    242
    243     return maybe_convert_bool(result.base)  # `.base` to access np.ndarray

~/sandbox/pandas/pandas/_libs/ops.pyx in pandas._libs.ops.vec_binop()
    232         y = right[i]
    233         try:
--> 234             result[i] = op(x, y)
    235         except TypeError:
    236             if x is None or is_nan(x):

~/sandbox/pandas/pandas/core/strings.py in rep(x, r)
    782                 return bytes.__mul__(x, r)
    783             except TypeError:
--> 784                 return str.__mul__(x, r)
    785
    786         repeats = np.asarray(repeats, dtype=object)

TypeError: descriptor '__mul__' requires a 'str' object but received a 'NAType'
```
#### Problem description

The `str_repeat` method correctly handles NA values when `repeats` is a scalar, but fails when its a sequence.

#### Expected Output

```python
0       a
1    <NA>
dtype: string
```"
560172378,31684,BUG: string methods with NA,prakhar987,closed,2020-02-05T06:41:15Z,2020-03-10T14:09:34Z,"- [x] closes #31632 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

"
573179133,32353,"docs: fix SS06, PR08, RT02, RT03, SA04 in pandas.DatetimeIndex.indexe…",rahmadiantio,closed,2020-02-29T05:09:41Z,2020-03-10T14:37:20Z,"…r_at_time

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

 Fix : https://github.com/pandanistas/pandanistas_sprint_ui2020/issues/8
"
577417651,32533,CLN: remove unnecessary to_dense call,jbrockmendel,closed,2020-03-07T23:40:04Z,2020-03-10T14:54:35Z,"This call is not reached in the tests, and if it were the following line would make it superfluous anyway.

I'm pretty sure the comment there is wrong too."
577257234,32509,CLN: unused code in reshape.merge,jbrockmendel,closed,2020-03-07T01:56:29Z,2020-03-10T15:01:00Z,"check_duplicates is always passed as False, so we can trim some code"
577385132,32525,TST: remove unused kwargs in assert_sp_array_equal,jbrockmendel,closed,2020-03-07T19:02:03Z,2020-03-10T15:01:32Z,
577418744,32534,CLN: remove unused in pd._testing,jbrockmendel,closed,2020-03-07T23:52:18Z,2020-03-10T15:03:19Z,
578606938,32578,Backport PR #31684 on branch 1.0.x (BUG: string methods with NA),meeseeksmachine,closed,2020-03-10T14:09:22Z,2020-03-10T16:21:21Z,Backport PR #31684: BUG: string methods with NA
578002765,32552,"Backport PR #32242 on branch 1.0.x (BUG: Fixed bug, where pandas._libs.lib.maybe_convert_objects function improperly handled arrays with bools and NaNs)",meeseeksmachine,closed,2020-03-09T15:42:45Z,2020-03-10T16:22:32Z,"Backport PR #32242: BUG: Fixed bug, where pandas._libs.lib.maybe_convert_objects function improperly handled arrays with bools and NaNs"
578740610,32587,Arrow discards timezones,vmarkovtsev,closed,2020-03-10T17:21:14Z,2020-03-10T19:27:27Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
from datetime import datetime, timezone

df = pd.DataFrame.from_records([
    (1, datetime.now().replace(tzinfo=timezone.utc)),
    (2, datetime.now().replace(tzinfo=timezone.min))],
    columns=[""1"", ""2""])

print(df[""2""])
print()

df.to_feather(""/tmp/1"") 
df2 = pd.read_feather(""/tmp/1"")

print(df2[""2""])
```

This code will output:

```
0    2020-03-10 18:13:49.405598+00:00
1    2020-03-10 18:13:49.405626-23:59
Name: 2, dtype: object

0   2020-03-10 18:13:49.405598
1   2020-03-10 18:13:49.405626
Name: 2, dtype: datetime64[ns]
```
#### Problem description

The round-trip dtype changed from the correct `object` to incorrect `datetime64`. Thus the timezones were discarded in Arrow and the timestamps became invalid.

#### Expected Output

(identical)

```
0    2020-03-10 18:13:49.405598+00:00
1    2020-03-10 18:13:49.405626-23:59
Name: 2, dtype: object

0    2020-03-10 18:13:49.405598+00:00
1    2020-03-10 18:13:49.405626-23:59
Name: 2, dtype: object
```

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-40-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.17.4
pytz             : 2019.2
dateutil         : 2.7.3
pip              : 19.3.1
setuptools       : 42.0.1
Cython           : 0.29.14
pytest           : 5.3.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.10.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.3.1
pyxlsb           : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : 1.3.12
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
```

</details>
"
576671188,32480,json_normalize errors if meta fields are integer,joetl,closed,2020-03-06T03:22:39Z,2020-03-10T20:24:51Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

data = [{""person_id"": ""test"", 
         ""test_field"": 1,  
         ""access"": [{""step"": 1, 
                     ""phase"":""starting""}, 
                    {""step"":2,
                     ""phase"":""ending""}
                    ]
         }]

df = pd.json_normalize(data,""access"",meta=[""person_id"",""test_field""])

print(df)

```
#### Problem description
If meta fields are integer we get the below error. json_normalize works fine for string datatype
Traceback (most recent call last):
  File ""C:/Users/HOME/PycharmProjects/pandas_test/pandas_test.py"", line 12, in <module>
    df = pd.json_normalize(data,""access"",meta=[""person_id"",""test_field""])
  File ""C:\Users\HOME\Anaconda3\envs\pandas_test\lib\site-packages\pandas\io\json\_normalize.py"", line 327, in _json_normalize
    _recursive_extract(data, record_path, {}, level=0)
  File ""C:\Users\HOME\Anaconda3\envs\pandas_test\lib\site-packages\pandas\io\json\_normalize.py"", line 314, in _recursive_extract
    meta_val = _pull_field(obj, val[level:])
  File ""C:\Users\HOME\Anaconda3\envs\pandas_test\lib\site-packages\pandas\io\json\_normalize.py"", line 246, in _pull_field
    f""{js} has non iterable value {result} for path {spec}. ""
TypeError: {'person_id': 'test', 'test_field': 1, 'access': [{'step': 1, 'phase': 'starting'}, {'step': 2, 'phase': 'ending'}]} has non iterable value 1 for path ['test_field']. Must be iterable or null.


#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : AMD64 Family 16 Model 10 Stepping 0, AuthenticAMD
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
555650519,31345,ENH: `Styler.highlight_null` to accept `subset` argument,immaxchen,closed,2020-01-27T15:28:22Z,2020-03-10T20:30:08Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
df = pd.DataFrame({""A"": [0, None], ""B"": [0, None]})
df.style.highlight_null(null_color=""red"", subset=[""A""])
```
out:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-3-c4119d081571> in <module>
----> 1 df.style.highlight_null(null_color=""red"", subset=[""A""])

TypeError: highlight_null() got an unexpected keyword argument 'subset'
```
#### Problem description

Most of the `Styler` built-in methods support `subset` argument, except `highlight_null`.
I think it would be nice if we make it consistent with `highlight_min` and `highlight_max` etc. 
This also improves the api completeness for missing values styling and formatting. [(as discussed here)](https://github.com/pandas-dev/pandas/pull/29118#discussion_r340733887)

#### Expected Output
![Screenshot from 2020-01-27 23-16-43](https://user-images.githubusercontent.com/26521816/73186715-311c7780-415b-11ea-8535-f04a6bd6b613.png)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-862.14.4.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 44.0.0.post20200106
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
555662054,31350,ENH: `Styler.highlight_null` can accepts `subset` argument,immaxchen,closed,2020-01-27T15:46:34Z,2020-03-10T20:30:16Z,"As proposed in #31345 and as a follow-up PR for [#29118](https://github.com/pandas-dev/pandas/pull/29118#discussion_r340733887)

- [x] closes #31345
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
578851444,32596,Backport PR #32592 on branch 1.0.x (DOC: cleanup 1.0.2 whatsnew),meeseeksmachine,closed,2020-03-10T20:46:56Z,2020-03-10T21:52:46Z,Backport PR #32592: DOC: cleanup 1.0.2 whatsnew
578903528,32600,Pyarrow and custom dtypes,achapkowski,closed,2020-03-10T22:43:21Z,2020-03-11T00:19:29Z,"Hello,

Are there anymore robust examples showing how to convert custom dtypes to and from Apache arrow?  

Essentially my dtype is a dictionary like data structure but I want to use the arrow storage format.

"
577274325,32511,[BUG] Add int-array-like into random state,xcz011,closed,2020-03-07T04:12:27Z,2020-03-11T00:36:34Z,"- [x] closes #32503
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [] whatsnew entry
"
577676109,32551,DOC: Fix EX01 in pandas.DataFrame.idxmax,farhanreynaldo,closed,2020-03-09T05:53:51Z,2020-03-11T01:35:56Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. 

```
################################################################################
################################## Validation ##################################
################################################################################"
561619074,31774,"REGR: Timestamp subtraction raises TypeError, Non informative",sbrugman,closed,2020-02-07T12:51:01Z,2020-03-11T01:45:54Z,"#### Code Sample

```python
import pandas as pd
print(pd.Timestamp('2101-01-01 00:00:00') - pd.Timestamp('1688-01-01 00:00:00'))
```

Outputs:
```
TypeError: unsupported operand type(s) for -: 'Timestamp' and 'Timestamp'
```

#### Problem description

The difference between the timestamps is too large for the Timedelta object. This can be traced back to these lines:

https://github.com/pandas-dev/pandas/blob/2dadd0f48949182dda412eb4032b5234c43c83c1/pandas/_libs/tslibs/c_timestamp.pyx#L303

For the example we can see that
```python
print(pd.Timedelta(pd.Timestamp('2101-01-01 00:00:00').value - pd.Timestamp('1688-01-01 00:00:00').value))
```
results in
```OverflowError: int too big to convert```

#### Expected Output

Ideally, the expected output is:
```
150845 days 00:00:00
```

Note the expected output matches that of pandas 0.25.3.

Alternatively, the error message should be at least somewhat informative, for example by propagating the OverflowError (removing the try/except block from L302-305) or raising a new TypeError (similar to L296). 

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.0
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.2.post20191201
Cython           : None
pytest           : 5.3.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.3.2
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.46.0

</details>
"
571902731,32287,BUG: convert_dtypes changes BooleanDtype to Int64,jiannmeng,closed,2020-02-27T08:22:44Z,2020-03-11T01:48:34Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import pandas as pd
>>> df = pd.DataFrame(data=[[""abc"", 123, True]])
>>> print(df)
     0    1     2
0  abc  123  True
>>> print(df.dtypes)
0    object
1     int64
2      bool
dtype: object
>>> df = df.convert_dtypes()
>>> print(df)
     0    1     2
0  abc  123  True
>>> print(df.dtypes)
0     string
1      Int64
2    boolean
dtype: object
>>> df = df.convert_dtypes()
>>> print(df)
        0    1  2
0  b'abc'  123  1
>>> print(df.dtypes)
0    object
1     Int64
2     Int64
dtype: object

```
#### Problem description

Applying convert_dtypes() to a column with dtype `string` converts it to a column dtype 'object' (and the individual values from `str` type to `bytes` type).

Applying convert_dtypes() to a column with dtype `boolean` converts it to a column dtype 'Int64' (and the individual values from `bool` type to `int` type).

#### Expected Output

convert_dtypes() should keep StringDtype columns as StringDtype and BooleanDtype columns as BooleanDtype.

#### Output of ``pd.show_versions()``

<details>


pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_Malaysia.1252

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None

</details>"
573568794,32385,BUG: Rolling functions doesn't work on decreasing variable index,leftys,closed,2020-03-01T15:48:28Z,2020-03-11T01:51:04Z,"#### Code Sample

```python
    def test_rolling_on_decreasing_index(self):
        # GH-19248
        index = [
            Timestamp(""20190101 09:00:00""),
            Timestamp(""20190101 09:00:02""),
            Timestamp(""20190101 09:00:03""),
            Timestamp(""20190101 09:00:05""),
            Timestamp(""20190101 09:00:06""),
        ]

        df = DataFrame({""column"": [3, 4, 4, 2, 1]}, index=reversed(index))
        result = df.rolling(""2s"").min()
        expected = DataFrame(
            {""column"": [3.0, 4.0, 4.0, 2.0, 1.0]}, index=reversed(index)
        )
        # actual incorrect output is [3, 3, 3, 2, 1]
        tm.assert_frame_equal(result, expected)
```
#### Problem description

Rolling window operations on variable decreasing index don't work even after https://github.com/pandas-dev/pandas/pull/28297. The reason for this is that the `calculate_variable_window_bounds` function doesn't work with decreasing index. Tested with latest master (~version 1.0.1).
"
573569424,32386,BUG: Fix rolling functions with variable windows on decreasing index,leftys,closed,2020-03-01T15:52:32Z,2020-03-11T01:54:20Z,"- [x] Closes https://github.com/pandas-dev/pandas/issues/32385
- [x] tests updated & passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
572840200,32335,ENH: implement fill_value for df.add(other=Series) #13488,jonathanrhughes,closed,2020-02-28T15:53:18Z,2020-03-11T02:17:59Z,"- [ ] closes #13488 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
578475180,32577,REG: Restore read_csv function for some file-likes,gfyoung,closed,2020-03-10T10:22:21Z,2020-03-11T02:22:06Z,"Restore `read_csv` function for some file-likes
    
Restores behavior down to the fact that the Python engine cannot handle NamedTemporaryFile.
    
Closes https://github.com/pandas-dev/pandas/issues/31819
    
Credit to @sasanquaneuf for [originating idea](https://github.com/pandas-dev/pandas/issues/31819#issuecomment-584529415)."
577372379,32521,"CLN: rename get_block_values, simplify",jbrockmendel,closed,2020-03-07T17:23:11Z,2020-03-11T02:27:23Z,"cc @WillAyd name more explicit, implementation more transparent."
578296457,32571,"TST: stricter tests, avoid check_categorical=False, check_less_precise",jbrockmendel,closed,2020-03-10T02:30:06Z,2020-03-11T02:30:00Z,Won't be too surprised if 32bit builds need some troubleshooting.
578957038,32605,Backport PR #32499 on branch 1.0.x (Better error message for OOB result),meeseeksmachine,closed,2020-03-11T01:46:05Z,2020-03-11T02:33:53Z,Backport PR #32499: Better error message for OOB result
578108508,32557,CLN: values_from_object in computation.pytables,jbrockmendel,closed,2020-03-09T18:30:47Z,2020-03-11T02:34:56Z,By my count there is just one more `values_from_object` call to go.
214128942,15686,BUG: .iloc indexing with duplicates,toobaz,closed,2017-03-14T16:27:11Z,2020-03-11T02:35:21Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: df1 = pd.DataFrame([{'A': None, 'B': 1}, {'A': 2, 'B': 2}])
   ...: df2 = pd.DataFrame([{'A': 3, 'B': 3}, {'A': 4, 'B': 4}])
   ...: df = pd.concat([df1, df2], axis=1)
   ...: df.iloc[0, 0] = df.iloc[0, 2]
   ...: df
   ...: 
Out[2]: 
     A  B    A  B
0  3.0  1  3.0  3
1  2.0  2  4.0  4
```
#### Problem description

The third column shouldn't have changed type

#### Expected Output
``` python
Out[2]: 
     A  B  A  B
0  3.0  1  3  3
1  2.0  2  4  4
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: 32df1e6ae452f7ddd31dc41fa613992493eb51c4
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.7.0-1-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: it_IT.utf8
LOCALE: it_IT.UTF-8

pandas: 0.19.0+597.g32df1e6ae
pytest: 3.0.6
pip: 9.0.1
setuptools: 33.1.1
Cython: 0.25.2
numpy: 1.12.0
scipy: 0.18.1
xarray: None
IPython: 5.1.0.dev
sphinx: 1.4.9
patsy: 0.3.0-dev
dateutil: 2.5.3
pytz: 2016.7
blosc: None
bottleneck: 1.2.0
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.0.0
openpyxl: 2.3.0
xlrd: 1.0.0
xlwt: 1.1.2
xlsxwriter: 0.9.6
lxml: 3.7.1
bs4: 4.5.3
html5lib: 0.999999999
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.8
s3fs: None
pandas_gbq: None
pandas_datareader: 0.2.1



</details>
"
343893937,22036,Replacing a column with iloc replaces another column with same name,mitar,closed,2018-07-24T05:52:30Z,2020-03-11T02:35:22Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas

a = pandas.DataFrame({'a': ['0'], 'b': ['str']})

print('---')
print(a)

a.iloc[:, 0] = [int(v) for v in a.iloc[:, 0]]

print('---')
print(a)

b = pandas.concat([a, pandas.DataFrame({'b': ['str2']})], axis=1)

print('---')
print(b)

b.iloc[:, 2] = ['str3']

print('---')
print(b)
```

#### Problem description

The issue seems to be that if there is a DataFrame with duplicate column names and mixed dtypes, if I try to replace one column with another value, using `iloc`, another column with same name is replaced as well.

#### Expected Output

The final print should be:

```
   a    b     b
0  0  str  str3
```

And not:

```
   a     b     b
0  0  str3  str3
```

It is interesting that if I change concat line to (see renaming of column `b` to `c`):

```
b = pandas.concat([a, pandas.DataFrame({'c': ['str2']})], axis=1)
```

Then the output is correctly:

```
   a    b     c
0  0  str  str3
```

Also, if `a.iloc[:, 0] = [int(v) for v in a.iloc[:, 0]]` is commented out, it works also correctly.

Moreover, the following also work correctly (see the change of `0` column index into `[0]` column index, and similar for `2` (this latter change is not really necessary to make it work)):

```python
import pandas

a = pandas.DataFrame({'a': ['0'], 'b': ['str']})

print('---')
print(a)

a.iloc[:, [0]] = [int(v) for v in a.iloc[:, 0]]

print('---')
print(a)

b = pandas.concat([a, pandas.DataFrame({'b': ['str2']})], axis=1)

print('---')
print(b)

b.iloc[:, [2]] = ['str3']

print('---')
print(b)
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.13.0-46-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.3
pytest: None
pip: 18.0
setuptools: 40.0.0
Cython: None
numpy: 1.15.0
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
576615553,32477,BUG: iloc.__setitem__ with duplicate columns,jbrockmendel,closed,2020-03-06T00:33:41Z,2020-03-11T02:38:32Z,"- [x] closes #15686
- [x] closes #22036
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
573142037,32349,REF: Remove BlockManager.rename_axis,jbrockmendel,closed,2020-02-29T03:33:03Z,2020-03-11T02:41:50Z,"Better to do it using NDFrame methods

cc @toobaz I know you're on board for getting index/axis stuff out of BlockManager."
577184275,32505,CLN: avoid values_from_object in Index.equals,jbrockmendel,closed,2020-03-06T21:26:37Z,2020-03-11T02:43:30Z,
578949693,32604,Avoid bare pytest.raises in dtypes/test_dtypes.py,Vlek,closed,2020-03-11T01:17:22Z,2020-03-11T02:48:21Z,"* [x]  ref #30999
 
* [x]  tests added / passed
 
* [x]  passes `black pandas`

* [x]  passes `git diff origin/master -u -- ""*.py"" | flake8 --diff`

Please be aware that I changed dtypes.py as there was a typo in the exception message."
577567934,32547,CLN: remove unnecessary values_from_objects in groupby.ops,jbrockmendel,closed,2020-03-08T21:19:39Z,2020-03-11T02:49:54Z,2 values_from_object calls left to go...
578958918,32606,Backport PR #32386 on branch 1.0.x (BUG: Fix rolling functions with variable windows on decreasing index),meeseeksmachine,closed,2020-03-11T01:53:52Z,2020-03-11T02:57:58Z,Backport PR #32386: BUG: Fix rolling functions with variable windows on decreasing index
497307740,28584,PERF: Index._shallow_copy doesn't copy ._engine,topper-123,closed,2019-09-23T20:20:36Z,2020-03-11T03:06:03Z,"```python
>>> idx = pd.Index(np.arange(100_000))
>>> %timeit idx.get_loc(99_999)
774 ns ± 26 ns per loop
>>> %timeit idx._shallow_copy().get_loc(99_999)
3.57 ms ± 56.8 µs per loop
```
The same performance issue can be seen on other index types, e.g. CategoricalIndex and MultiIndex.

#### Problem description

The reason for the above diferences is that ``_shallow_copy`` does not copy over the ``._engine`` attribute to the new index and the ``_engine`` is expensive to recreate. 

Indexes are immutable, and likewise - to my understanding - are the ``._engine`` attribute of indexes. The ``._engine`` is quite expensive to create and if it has been created on the original index, I think it should be possible to reuse it on the new index, saving ther time needed to create a new and identical ``._engine``.

``_shallow_copy`` is used in a few places internally in pandas, so there seems to be potential for some speedups for several pandas merhods by copying the _engine over to newly-copied indexes.

Possibly I'm missing some finer details here, e.g. don't know what the ``._engine.clear_mappings`` is for and it seems from its name to be destructive, but overall it seems to be possible to make a change to copy this over.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 79663fb669cce436c6379c83c887755780b84770
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.0.dev0+1363.g79663fb66
numpy            : 1.16.4
pytz             : 2019.1
dateutil         : 2.8.0
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : 0.29.13
pytest           : 5.0.1
hypothesis       : 4.28.2
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.6.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : 2.6.9
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
577380703,32524,CLN: avoid Block.get_values in io.sql,jbrockmendel,closed,2020-03-07T18:27:24Z,2020-03-11T03:07:37Z,"This particular usage is especially non-transparent, since it is effectively `blk.get_values().astype(object)` which is apparently _not_ equivalent to `blk.get_values(object)`"
578959562,32607,TST: fixturize skipna in test_nanops,jbrockmendel,closed,2020-03-11T01:56:21Z,2020-03-11T03:09:37Z,
578916081,32602,DOC: Fix link to monthly meeting calendar,datapythonista,closed,2020-03-10T23:20:13Z,2020-03-11T03:10:38Z,"The link we currently have for Google calendar is to embed, and let you see the calendar, but not subscribe to it. Fixing it here."
577379482,32523,CLN: simplify get_values usage in groupby,jbrockmendel,closed,2020-03-07T18:17:49Z,2020-03-11T03:10:49Z,This will avoid an object-casting for e.g. IntervalArray-backed ExtensionBlock.
516650795,29364,26302 add typing to assert star equal funcs,samuelsinayoko,closed,2019-11-02T16:40:07Z,2020-03-11T03:14:08Z,"- [X] closes #26302 
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
577416734,32532,TYP: Add type hint for DataFrame.T and certain array types,qwhelan,closed,2020-03-07T23:31:02Z,2020-03-11T03:19:15Z,"While updating a large pandas codebase with type coverage to 1.0, I noticed that `DataFrame.transpose()` is annotated to return `DataFrame` while `DataFrame.T` has no such hint. This PR also adds hints in a few places other places where they're trivial.

The definition of `pandas.core.base.IndexOpsMixIn.transpose` (and associated `.T`) currently do not have any type hints and fixing this seems more complicated, meaning `Index.T` and `Series.T` are not fixed by this PR.

Happy to amend to cover that case if anyone has suggestions, but I believe `DataFrame.T` to be the vast majority of usage.

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
576801366,32483,CLN: Split and fixturized test_fillna in tests/base/test_ops.py,SaturnFromTitan,closed,2020-03-06T09:23:46Z,2020-03-11T03:54:08Z,"part of #23877
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
573222351,32362,"DOC: Fixed errors in pandas.DataFrame.asfreq PR07, RT02, RT03, SA04",tolhassianipar,closed,2020-02-29T07:00:18Z,2020-03-11T04:03:23Z,"- [X] closes https://github.com/pandanistas/pandanistas_sprint_ui2020/issues/19
- [ ] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Output of `python scripts/validate_docstrings.py pandas.DataFrame.asfreq`:
```
################################################################################
##################### Docstring (pandas.DataFrame.asfreq)  #####################
################################################################################

Convert TimeSeries to specified frequency.

Optionally provide filling method to pad/backfill missing values.

Returns the original data conformed to a new index with the specified
frequency. ``resample`` is more appropriate if an operation, such as
summarization, is necessary to represent the data at the new frequency.

Parameters
----------
freq : DateOffset or str
    Frequency DateOffset or string.
method : {'backfill'/'bfill', 'pad'/'ffill'}, default None
    Method to use for filling holes in reindexed Series (note this
    does not fill NaNs that already were present):

    * 'pad' / 'ffill': propagate last valid observation forward to next
      valid
    * 'backfill' / 'bfill': use NEXT valid observation to fill.
how : {'start', 'end'}, default end
    For PeriodIndex only (see PeriodIndex.asfreq).
normalize : bool, default False
    Whether to reset output index to midnight.
fill_value : scalar, optional
    Value to use for missing values, applied during upsampling (note
    this does not fill NaNs that already were present).

Returns
-------
converted
    Same type as caller.

See Also
--------
reindex : Conform DataFrame to new index with optional filling logic.

Notes
-----
To learn more about the frequency strings, please see `this link
<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.

Examples
--------
Start by creating a series with 4 one minute timestamps.

>>> index = pd.date_range('1/1/2000', periods=4, freq='T')
>>> series = pd.Series([0.0, None, 2.0, 3.0], index=index)
>>> df = pd.DataFrame({'s':series})
>>> df
                       s
2000-01-01 00:00:00    0.0
2000-01-01 00:01:00    NaN
2000-01-01 00:02:00    2.0
2000-01-01 00:03:00    3.0

Upsample the series into 30 second bins.

>>> df.asfreq(freq='30S')
                       s
2000-01-01 00:00:00    0.0
2000-01-01 00:00:30    NaN
2000-01-01 00:01:00    NaN
2000-01-01 00:01:30    NaN
2000-01-01 00:02:00    2.0
2000-01-01 00:02:30    NaN
2000-01-01 00:03:00    3.0

Upsample again, providing a ``fill value``.

>>> df.asfreq(freq='30S', fill_value=9.0)
                       s
2000-01-01 00:00:00    0.0
2000-01-01 00:00:30    9.0
2000-01-01 00:01:00    NaN
2000-01-01 00:01:30    9.0
2000-01-01 00:02:00    2.0
2000-01-01 00:02:30    9.0
2000-01-01 00:03:00    3.0

Upsample again, providing a ``method``.

>>> df.asfreq(freq='30S', method='bfill')
                       s
2000-01-01 00:00:00    0.0
2000-01-01 00:00:30    NaN
2000-01-01 00:01:00    NaN
2000-01-01 00:01:30    2.0
2000-01-01 00:02:00    2.0
2000-01-01 00:02:30    3.0
2000-01-01 00:03:00    3.0

################################################################################
################################## Validation ##################################
################################################################################
```"
578966529,32609,Backport PR #32577 on branch 1.0.x (REG: Restore read_csv function for some file-likes),meeseeksmachine,closed,2020-03-11T02:22:12Z,2020-03-11T04:08:25Z,Backport PR #32577: REG: Restore read_csv function for some file-likes
565735959,32018,"DOC add extended summary, update parameter types desc, update return types desc, and add whitespaces after commas in list declarations to DataFrame.first in core/generic.py",sagungrp,closed,2020-02-15T11:32:50Z,2020-03-11T08:48:54Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
578228630,32568,PERF: copy cached attributes on index shallow_copy,topper-123,closed,2020-03-09T22:33:40Z,2020-03-11T09:02:49Z,"- [x] closes #28584
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The performance of the example in #28584 is:

```python
>>> idx = pd.Index(np.arange(100_000))
>>> %timeit idx.get_loc(99_999)
1.17 µs ± 80.6 ns per loop  # master and this PR
>>> %timeit idx._shallow_copy().get_loc(99_999)
3.57 ms ± 286 ns per loop  # master
3.67 µs ± 286 ns per loop  # this PR
```

The issue is still on the extension indexes, e.g. ``CategoricalIndex._shallow_copy``. I'd like to take them afterwards."
577228571,32507,CLN: Avoid bare pytest.raises in computation/test_eval.py ,Vlek,closed,2020-03-06T23:31:35Z,2020-03-11T09:09:06Z,"* [x]  ref #30999
 
* [x]  tests added / passed
 
* [x]  passes `black pandas`

* [x]  passes `git diff origin/master -u -- ""*.py"" | flake8 --diff`"
570559679,32238,read_excel with xlsb,ShayHa,closed,2020-02-25T13:12:05Z,2020-03-11T10:24:26Z,"
```python
df = pd.read_excel()

```
As it been said in the new docs that read_excel can now read xlsb, I tried to run my xlsb file and still got and error.
```
Supports xls, xlsx, xlsm, xlsb, and odf file extensions read from a local filesystem or URL. Supports an option to read a single sheet or a list of sheets.

```
#### Expected Output
A working df.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.137+
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.17.5
pytz             : 2018.9
dateutil         : 2.6.1
pip              : 19.3.1
setuptools       : 45.1.0
Cython           : 0.29.15
pytest           : 3.6.4
hypothesis       : None
sphinx           : 1.8.5
blosc            : None
feather          : 0.4.0
xlsxwriter       : None
lxml.etree       : 4.2.6
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.6.1 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 5.5.0
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.6
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 2.5.9
pandas_gbq       : 0.11.0
pyarrow          : 0.14.1
pytables         : None
pytest           : 3.6.4
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.4.4
tabulate         : 0.8.6
xarray           : 0.14.1
xlrd             : 1.1.0
xlwt             : 1.3.0
xlsxwriter       : None
numba            : 0.47.0

</details>

![xlsb](https://user-images.githubusercontent.com/44175527/75250433-074d8380-57e1-11ea-8df1-b1108eea48a4.png)
"
578804844,32592,DOC: cleanup 1.0.2 whatsnew,TomAugspurger,closed,2020-03-10T19:17:48Z,2020-03-11T12:20:39Z,"
"
572267591,32303,CI: Publishing development web and docs to GitHub actions fails,datapythonista,closed,2020-02-27T18:21:08Z,2020-03-11T12:58:03Z,"Looks like using the ssh key to publish the development web and docs is not working. The error we get in the CI is that [the key has an invalid format](https://github.com/pandas-dev/pandas/runs/472871471?check_suite_focus=true#step:14:10).

I guess it's not as simple as the key content being incorrect, I guess it can't be obtained from the secrets, and probably the used key is just empty.

The relevant code is in the [yaml of the GitHub action](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/ci.yml#L180).

Besides making sure that the key is correct (that it's the private and not the public, it's complete...), I guess a good test to help identify the problem could be replace the content of the key by some random text, and echo it and see that we're ready getting the full content with no garbage in it.

I think just few people have access to it, @TomAugspurger do you have time for it? Or do you know who else has access to the settings? I think it was said in some thread, but I fail to remember."
579233183,32620,DOC: fix formatting / links of API refs in 1.0.2 whatsnew,jorisvandenbossche,closed,2020-03-11T12:36:46Z,2020-03-11T14:00:18Z,cc @TomAugspurger (did a few other clean-ups directly as well)
577031386,32496,dtype passed into read_csv as an arg not passed to the dataframe when being created,pgospodinov,closed,2020-03-06T16:13:29Z,2020-03-11T14:01:24Z,"https://github.com/pandas-dev/pandas/blob/015c1c154b9bc70d3b1981ba1dc31c09676065bf/pandas/io/parsers.py#L1143

Is this the desired behaviour? Maybe I am missing something but when we pass dtype to read_csv, the dtype is not used anywhere. Passing it to the dataframe when creating it seems like a good place for that."
573429501,32381,Inconsistent behavior for 2D Groupby with Categorical MultiIndex raises TypeError with observed=False,blalterman,closed,2020-02-29T23:21:46Z,2020-03-11T14:06:00Z,"#### Code Sample, a copy-pastable example if possible

```python
x = np.random.normal(size=1000)
y = np.random.normal(size=1000)
z = pd.Series(np.full_like(x, 1))
xc = pd.cut(x, 10)
yc = pd.cut(y, 10)
cut = pd.DataFrame({""x"": xc, ""y"": yc})

mi = pd.MultiIndex.from_frame(cut)
z.index = mi
```

#### Problem description
When I run the following, I get an expected result.

```python
>>> z.groupby([""x"", ""y""], observed=True).count()

x                 y               
(-3.945, -3.223]  (0.795, 1.448]      1
(-3.223, -2.509]  (0.142, 0.795]      1
                  (0.795, 1.448]      1
(-2.509, -1.795]  (-2.47, -1.817]     1
                  (-1.817, -1.164]    4
                                     ..
(1.775, 2.489]    (2.754, 3.407]      1
(2.489, 3.203]    (-0.511, 0.142]     1
                  (0.795, 1.448]      1
                  (1.448, 2.101]      1
                  (2.101, 2.754]      1
Length: 65, dtype: int64
```


However, if I want to preserve the categories that are not observed as in either of the two following, I get a `TypeError` that appears to be due to the inclusion of empty groups (mixing of NaNs and Categoricals?)

```python
z.groupby([""x"", ""y""]).count()
```

```python
z.groupby([""x"", ""y""], observed=False).count()
```

```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-153-f64fa4f78c55> in <module>
----> 1 z.groupby([""x"", ""y""]).count()

~/.conda/envs/test_env/lib/python3.7/site-packages/pandas/core/groupby/generic.py in count(self)
    797             dtype=""int64"",
    798         )
--> 799         return self._reindex_output(result, fill_value=0)
    800 
    801     def _apply_to_column_groupbys(self, func):

~/.conda/envs/test_env/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in _reindex_output(self, output, fill_value)
   2488         levels_list = [ping.group_index for ping in groupings]
   2489         index, _ = MultiIndex.from_product(
-> 2490             levels_list, names=self.grouper.names
   2491         ).sortlevel()
   2492 

~/.conda/envs/test_env/lib/python3.7/site-packages/pandas/core/indexes/multi.py in from_product(cls, iterables, sortorder, names)
    551 
    552         codes = cartesian_product(codes)
--> 553         return MultiIndex(levels, codes, sortorder=sortorder, names=names)
    554 
    555     @classmethod

~/.conda/envs/test_env/lib/python3.7/site-packages/pandas/core/indexes/multi.py in __new__(cls, levels, codes, sortorder, names, dtype, copy, name, verify_integrity, _set_identity)
    278 
    279         if verify_integrity:
--> 280             new_codes = result._verify_integrity()
    281             result._codes = new_codes
    282 

~/.conda/envs/test_env/lib/python3.7/site-packages/pandas/core/indexes/multi.py in _verify_integrity(self, codes, levels)
    366 
    367         codes = [
--> 368             self._validate_codes(level, code) for level, code in zip(levels, codes)
    369         ]
    370         new_codes = FrozenList(codes)

~/.conda/envs/test_env/lib/python3.7/site-packages/pandas/core/indexes/multi.py in <listcomp>(.0)
    366 
    367         codes = [
--> 368             self._validate_codes(level, code) for level, code in zip(levels, codes)
    369         ]
    370         new_codes = FrozenList(codes)

~/.conda/envs/test_env/lib/python3.7/site-packages/pandas/core/indexes/multi.py in _validate_codes(self, level, code)
    302         to a level with missing values (NaN, NaT, None).
    303         """"""
--> 304         null_mask = isna(level)
    305         if np.any(null_mask):
    306             code = np.where(null_mask[code], -1, code)

~/.conda/envs/test_env/lib/python3.7/site-packages/pandas/core/dtypes/missing.py in isna(obj)
    124     Name: 1, dtype: bool
    125     """"""
--> 126     return _isna(obj)
    127 
    128 

~/.conda/envs/test_env/lib/python3.7/site-packages/pandas/core/dtypes/missing.py in _isna_old(obj)
    181         return False
    182     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):
--> 183         return _isna_ndarraylike_old(obj)
    184     elif isinstance(obj, ABCGeneric):
    185         return obj._constructor(obj._data.isna(func=_isna_old))

~/.conda/envs/test_env/lib/python3.7/site-packages/pandas/core/dtypes/missing.py in _isna_ndarraylike_old(obj)
    281         else:
    282             result = np.empty(shape, dtype=bool)
--> 283             vec = libmissing.isnaobj_old(values.ravel())
    284             result[:] = vec.reshape(shape)
    285 

TypeError: Argument 'arr' has incorrect type (expected numpy.ndarray, got Categorical)
```

#### Expected Output

The expected output should preserve non-observed groups and return a result.
```python
x                 y               
(-3.945, -3.223]  (0.795, 1.448]      1
(-3.223, -2.509]  (0.142, 0.795]      1
                  (0.795, 1.448]      1
(-2.509, -1.795]  (-2.47, -1.817]     1
                  (-1.817, -1.164]    4
                                     ..
(1.775, 2.489]    (2.754, 3.407]      1
(2.489, 3.203]    (-0.511, 0.142]     1
                  (0.795, 1.448]      1
                  (1.448, 2.101]      1
                  (2.101, 2.754]      1
Length: 65, dtype: int64
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1062.4.3.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : 5.3.5
hypothesis       : 5.5.4
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
574916373,32421,TYP: enforce annotation on SingleBlockManager.__init__,jbrockmendel,closed,2020-03-03T20:18:58Z,2020-03-11T14:07:31Z,"cc @simonjayhawkins my impression from yesterday's threads was that mypy should be catching the wrong-type being passed in `SingleBlockManager.__init__`, did I misunderstand something?

Also (and I know ive asked before) is there a nice way to annotate the return type as ""same type as self""?  I tried `pandas._typing.T` but that didnt do it.

Parts of this are repeated in several BlockManager methods, should probably be made into a helper function, pending discussion on above."
552912371,31176,Parquet file written with Dask does not load using Pandas,Code4SAFrankie,closed,2020-01-21T14:28:33Z,2020-03-11T14:18:36Z,"#### Code Sample, a copy-pastable example if possible

```python
import numpy as np
import pandas as pd
import dask.dataframe as dd
from dask.diagnostics import ProgressBar
from dask.distributed import Client

client = Client('192.168.1.33:8786', processes=True)

path = ""D:\\Downloads\\Wikipedia\\Yago\\""
names = ['dummy1', 'class1', 'statement', 'class2', 'value']
dtypes = {'dummy1': 'category', 'class1': 'category', 'statement': 'category', 'class2': np.str, 'value': 'category'}
cols = ['class1', 'statement', 'class2', 'value']
df = dd.read_csv(path + ""yagoGeonamesOnlyData.tsv"", sep=""\t"", skiprows=1, header=None, names=names, dtype=dtypes, usecols=cols, blocksize=1e7)

df.to_parquet(path + ""yagoGeonamesOnlyData"", engine='pyarrow', compression='BROTLI', write_index=False)    # Can't specify npartitions.
```
#### Problem description
The above written file cannot be loaded using Pandas:

`df = pd.read_parquet(path + ""yagoGeonamesOnlyData"", engine='pyarrow')`

but does load with Dask:

`df = dd.read_parquet(path + ""yagoGeonamesOnlyData"", engine='pyarrow', index=False)`

The pandas version gives the following error:

```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
 in 
      1 #df = dd.read_parquet(path + ""yagoGeonamesOnlyData"", engine='pyarrow', index=False)
----> 2 df = pd.read_parquet(path + ""yagoGeonamesOnlyData"", engine='pyarrow')

~\AppData\Roaming\Python\Python37\site-packages\pandas\io\parquet.py in read_parquet(path, engine, columns, **kwargs)
    294 
    295     impl = get_engine(engine)
--> 296     return impl.read(path, columns=columns, **kwargs)

~\AppData\Roaming\Python\Python37\site-packages\pandas\io\parquet.py in read(self, path, columns, **kwargs)
    123         kwargs[""use_pandas_metadata""] = True
    124         result = self.api.parquet.read_table(
--> 125             path, columns=columns, **kwargs
    126         ).to_pandas()
    127         if should_close:

E:\WPy-3710\python-3.7.1.amd64\lib\site-packages\pyarrow\parquet.py in read_table(source, columns, use_threads, metadata, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size)
   1272                             read_dictionary=read_dictionary,
   1273                             buffer_size=buffer_size,
-> 1274                             filesystem=filesystem, filters=filters)
   1275     else:
   1276         pf = ParquetFile(source, metadata=metadata,

E:\WPy-3710\python-3.7.1.amd64\lib\site-packages\pyarrow\parquet.py in __init__(self, path_or_paths, filesystem, schema, metadata, split_row_groups, validate_schema, filters, metadata_nthreads, read_dictionary, memory_map, buffer_size)
   1058 
   1059         if validate_schema:
-> 1060             self.validate_schemas()
   1061 
   1062     def equals(self, other):

E:\WPy-3710\python-3.7.1.amd64\lib\site-packages\pyarrow\parquet.py in validate_schemas(self)
   1111                                  '{1!s}\n\nvs\n\n{2!s}'
   1112                                  .format(piece, file_schema,
-> 1113                                          dataset_schema))
   1114 
   1115     def read(self, columns=None, use_threads=True, use_pandas_metadata=False):

ValueError: Schema in D:\Downloads\Wikipedia\Yago\yagoGeonamesOnlyData\part.439.parquet was different. 
class1: dictionary
statement: dictionary
class2: string
value: null
metadata
--------
{b'ARROW:schema': b'/////zAEAAAUAAAAAAAAAAAACgAOAAYABQAIAAoAAAAAAQMAEAAAAAAA'
                  b'CgAMAAAABAAIAAoAAADUAgAABAAAAAEAAAAMAAAACAAMAAQACAAIAAAA'
                  b'CAAAABAAAAAGAAAAcGFuZGFzAACeAgAAeyJpbmRleF9jb2x1bW5zIjog'
                  b'W10sICJjb2x1bW5faW5kZXhlcyI6IFtdLCAiY29sdW1ucyI6IFt7Im5h'
                  b'bWUiOiAiY2xhc3MxIiwgImZpZWxkX25hbWUiOiAiY2xhc3MxIiwgInBh'
                  b'bmRhc190eXBlIjogImNhdGVnb3JpY2FsIiwgIm51bXB5X3R5cGUiOiAi'
                  b'aW50MzIiLCAibWV0YWRhdGEiOiB7Im51bV9jYXRlZ29yaWVzIjogMTEz'
                  b'MjY3LCAib3JkZXJlZCI6IGZhbHNlfX0sIHsibmFtZSI6ICJzdGF0ZW1l'
                  b'bnQiLCAiZmllbGRfbmFtZSI6ICJzdGF0ZW1lbnQiLCAicGFuZGFzX3R5'
                  b'cGUiOiAiY2F0ZWdvcmljYWwiLCAibnVtcHlfdHlwZSI6ICJpbnQ4Iiwg'
                  b'Im1ldGFkYXRhIjogeyJudW1fY2F0ZWdvcmllcyI6IDEsICJvcmRlcmVk'
                  b'IjogZmFsc2V9fSwgeyJuYW1lIjogImNsYXNzMiIsICJmaWVsZF9uYW1l'
                  b'IjogImNsYXNzMiIsICJwYW5kYXNfdHlwZSI6ICJ1bmljb2RlIiwgIm51'
                  b'bXB5X3R5cGUiOiAib2JqZWN0IiwgIm1ldGFkYXRhIjogbnVsbH0sIHsi'
                  b'bmFtZSI6ICJ2YWx1ZSIsICJmaWVsZF9uYW1lIjogInZhbHVlIiwgInBh'
                  b'bmRhc190eXBlIjogImVtcHR5IiwgIm51bXB5X3R5cGUiOiAib2JqZWN0'
                  b'IiwgIm1ldGFkYXRhIjogbnVsbH1dLCAiY3JlYXRvciI6IHsibGlicmFy'
                  b'eSI6ICJweWFycm93IiwgInZlcnNpb24iOiAiMC4xNS4xIn0sICJwYW5k'
                  b'YXNfdmVyc2lvbiI6ICIwLjI1LjMifQAABAAAANQAAABsAAAAQAAAAAQA'
                  b'AADY////AAABARQAAAAMAAAABAAAAAAAAAAU////BQAAAHZhbHVlAAAA'
                  b'EAAUAAgABgAHAAwAAAAQABAAAAAAAAEFFAAAAAwAAAAEAAAAAAAAAEz/'
                  b'//8GAAAAY2xhc3MyAACs////AAABBTwAAAA0AAAAEAAAACgAAAAIABAA'
                  b'CAAEAAgAAAAMAAAAAQAAAAAAAACk////AAAAAQgAAAAAAAAAnP///wkA'
                  b'AABzdGF0ZW1lbnQAAAAQABgACAAGAAcADAAQABQAEAAAAAAAAQVAAAAA'
                  b'OAAAABAAAAAoAAAACAAIAAAABAAIAAAADAAAAAgADAAIAAcACAAAAAAA'
                  b'AAEgAAAAAAAAAAQABAAEAAAABgAAAGNsYXNzMQAA',
 b'pandas': b'{""index_columns"": [], ""column_indexes"": [], ""columns"": [{""name"":'
            b' ""class1"", ""field_name"": ""class1"", ""pandas_type"": ""categorical"",'
            b' ""numpy_type"": ""int32"", ""metadata"": {""num_categories"": 113267, ""'
            b'ordered"": false}}, {""name"": ""statement"", ""field_name"": ""statemen'
            b't"", ""pandas_type"": ""categorical"", ""numpy_type"": ""int8"", ""metadat'
            b'a"": {""num_categories"": 1, ""ordered"": false}}, {""name"": ""class2"",'
            b' ""field_name"": ""class2"", ""pandas_type"": ""unicode"", ""numpy_type"":'
            b' ""object"", ""metadata"": null}, {""name"": ""value"", ""field_name"": ""v'
            b'alue"", ""pandas_type"": ""empty"", ""numpy_type"": ""object"", ""metadata'
            b'"": null}], ""creator"": {""library"": ""pyarrow"", ""version"": ""0.15.1""'
            b'}, ""pandas_version"": ""0.25.3""}'}

vs

class1: dictionary
statement: dictionary
class2: string
value: string
metadata
--------
{b'ARROW:schema': b'/////zAEAAAUAAAAAAAAAAAACgAOAAYABQAIAAoAAAAAAQMAEAAAAAAA'
                  b'CgAMAAAABAAIAAoAAADUAgAABAAAAAEAAAAMAAAACAAMAAQACAAIAAAA'
                  b'CAAAABAAAAAGAAAAcGFuZGFzAACfAgAAeyJpbmRleF9jb2x1bW5zIjog'
                  b'W10sICJjb2x1bW5faW5kZXhlcyI6IFtdLCAiY29sdW1ucyI6IFt7Im5h'
                  b'bWUiOiAiY2xhc3MxIiwgImZpZWxkX25hbWUiOiAiY2xhc3MxIiwgInBh'
                  b'bmRhc190eXBlIjogImNhdGVnb3JpY2FsIiwgIm51bXB5X3R5cGUiOiAi'
                  b'aW50MTYiLCAibWV0YWRhdGEiOiB7Im51bV9jYXRlZ29yaWVzIjogMTY1'
                  b'NzMsICJvcmRlcmVkIjogZmFsc2V9fSwgeyJuYW1lIjogInN0YXRlbWVu'
                  b'dCIsICJmaWVsZF9uYW1lIjogInN0YXRlbWVudCIsICJwYW5kYXNfdHlw'
                  b'ZSI6ICJjYXRlZ29yaWNhbCIsICJudW1weV90eXBlIjogImludDgiLCAi'
                  b'bWV0YWRhdGEiOiB7Im51bV9jYXRlZ29yaWVzIjogNCwgIm9yZGVyZWQi'
                  b'OiBmYWxzZX19LCB7Im5hbWUiOiAiY2xhc3MyIiwgImZpZWxkX25hbWUi'
                  b'OiAiY2xhc3MyIiwgInBhbmRhc190eXBlIjogInVuaWNvZGUiLCAibnVt'
                  b'cHlfdHlwZSI6ICJvYmplY3QiLCAibWV0YWRhdGEiOiBudWxsfSwgeyJu'
                  b'YW1lIjogInZhbHVlIiwgImZpZWxkX25hbWUiOiAidmFsdWUiLCAicGFu'
                  b'ZGFzX3R5cGUiOiAidW5pY29kZSIsICJudW1weV90eXBlIjogIm9iamVj'
                  b'dCIsICJtZXRhZGF0YSI6IG51bGx9XSwgImNyZWF0b3IiOiB7ImxpYnJh'
                  b'cnkiOiAicHlhcnJvdyIsICJ2ZXJzaW9uIjogIjAuMTUuMSJ9LCAicGFu'
                  b'ZGFzX3ZlcnNpb24iOiAiMC4yNS4zIn0ABAAAANQAAABsAAAAQAAAAAQA'
                  b'AADY////AAABBRQAAAAMAAAABAAAAAAAAAAU////BQAAAHZhbHVlAAAA'
                  b'EAAUAAgABgAHAAwAAAAQABAAAAAAAAEFFAAAAAwAAAAEAAAAAAAAAEz/'
                  b'//8GAAAAY2xhc3MyAACs////AAABBTwAAAA0AAAAEAAAACgAAAAIABAA'
                  b'CAAEAAgAAAAMAAAAAQAAAAAAAACk////AAAAAQgAAAAAAAAAnP///wkA'
                  b'AABzdGF0ZW1lbnQAAAAQABgACAAGAAcADAAQABQAEAAAAAAAAQVAAAAA'
                  b'OAAAABAAAAAoAAAACAAIAAAABAAIAAAADAAAAAgADAAIAAcACAAAAAAA'
                  b'AAEQAAAAAAAAAAQABAAEAAAABgAAAGNsYXNzMQAA',
 b'pandas': b'{""index_columns"": [], ""column_indexes"": [], ""columns"": [{""name"":'
            b' ""class1"", ""field_name"": ""class1"", ""pandas_type"": ""categorical"",'
            b' ""numpy_type"": ""int16"", ""metadata"": {""num_categories"": 16573, ""o'
            b'rdered"": false}}, {""name"": ""statement"", ""field_name"": ""statement'
            b'"", ""pandas_type"": ""categorical"", ""numpy_type"": ""int8"", ""metadata'
            b'"": {""num_categories"": 4, ""ordered"": false}}, {""name"": ""class2"", '
            b'""field_name"": ""class2"", ""pandas_type"": ""unicode"", ""numpy_type"": '
            b'""object"", ""metadata"": null}, {""name"": ""value"", ""field_name"": ""va'
            b'lue"", ""pandas_type"": ""unicode"", ""numpy_type"": ""object"", ""metadat'
            b'a"": null}], ""creator"": {""library"": ""pyarrow"", ""version"": ""0.15.1'
            b'""}, ""pandas_version"": ""0.25.3""}'}
```

#### Expected Output

For the file to be loaded without error.

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
E:\WPy-3710\python-3.7.1.amd64\lib\site-packages\xarray\core\merge.py:10: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.3
numpy            : 1.16.4
pytz             : 2018.6
dateutil         : 2.7.5
pip              : 19.3.1
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.3.1
hypothesis       : 3.82.1
sphinx           : 2.2.1
blosc            : 1.6.1
feather          : 0.4.0
xlsxwriter       : 1.1.2
lxml.etree       : 4.3.4
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.6.1
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : 1.2.1
fastparquet      : 0.3.2
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.0.3
numexpr          : 2.6.8
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.12
tables           : 3.4.4
xarray           : 0.11.3
xlrd             : 1.1.0
xlwt             : None
xlsxwriter       : 1.1.2
</details>
"
548687975,30953,Persistent groupby future warning,VelizarVESSELINOV,closed,2020-01-13T03:48:01Z,2020-03-11T14:18:55Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
 dtf = dtf.groupby('xGUID', sort=True).apply(x_apply)
```
#### Problem description
```
2020-01-12 21:32:07,672|root|99634|MainProcess|CRITICAL| Exception Information
2020-01-12 21:32:07,672|root|99634|MainProcess|CRITICAL| Type: <class 'FutureWarning'>
2020-01-12 21:32:07,672|root|99634|MainProcess|CRITICAL| Value: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass 'sort=False'.

  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/x-1.1.1-py3.8.egg/xpack/core/formation_parser.py"", line 91, in formation_parser
    dtf = dtf.groupby('xGUID', sort=True).apply(x_apply)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/groupby/groupby.py"", line 737, in apply
    return self._python_apply_general(f)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/groupby/groupby.py"", line 744, in _python_apply_general
    return self._wrap_applied_output(
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/groupby/generic.py"", line 372, in _wrap_applied_output
    return self._concat_objects(keys, values, not_indexed_same=not_indexed_same)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/groupby/groupby.py"", line 937, in _concat_objects
    result = concat(values, axis=self.axis)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/reshape/concat.py"", line 244, in concat
    op = _Concatenator(
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/reshape/concat.py"", line 428, in __init__
    self.new_axes = self._get_new_axes()
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/reshape/concat.py"", line 497, in _get_new_axes
    new_axes[i] = self._get_comb_axis(i)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/reshape/concat.py"", line 528, in _get_comb_axis
    return _get_objs_combined_axis(
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexes/api.py"", line 93, in _get_objs_combined_axis
    return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexes/api.py"", line 140, in _get_combined_index
    index = _union_indexes(indexes, sort=sort)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexes/api.py"", line 215, in _union_indexes
    warnings.warn(_sort_msg, FutureWarning, stacklevel=8)
```

#### Expected Output
No FutureWarning or better warning message. sort=True or False don't remove the warning in this case.
#### Output of ``pd.show_versions()``

<details>

NSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.2.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : None
pandas_datareader: 0.8.1
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.12
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
None

</details>
"
579292748,32627,Run travis tests on x86 platforms also,toddrme2178,closed,2020-03-11T14:10:05Z,2020-03-11T14:41:15Z,"Enable x86 tests for pandas under travis.
"
577378948,32522,CLN: remove SingleBlockManager.get_values,jbrockmendel,closed,2020-03-07T18:14:02Z,2020-03-11T14:42:41Z,"It is only used in one place, and its behavior doesn't match `self.blocks[0].get_values()`, which is counter-intuitive."
579283002,32626,DOC: fix formatting / links of API refs in 1.0.2 whatsnew (#32620),TomAugspurger,closed,2020-03-11T13:56:09Z,2020-03-11T14:49:06Z,https://github.com/pandas-dev/pandas/pull/32620
569529367,32204,CLN: simplify CategoricalIndex._simple_new,jbrockmendel,closed,2020-02-23T17:30:58Z,2020-02-26T02:16:00Z,
570751816,32244,CLN: simplify+annotate _shallow_copy,jbrockmendel,closed,2020-02-25T18:31:38Z,2020-02-26T02:17:31Z,"IntervalIndex is still an outlier, will be handled separately"
570172852,32224,PERF: lazify consolidation check,jbrockmendel,closed,2020-02-24T21:58:32Z,2020-02-26T02:18:52Z,"The benchmark I'm using for this (and upcoming related PRs) is based on the asv that is most affected by removing `fast_apply` (see #32086).

```
import numpy as np
from pandas import *
%load_ext line_profiler


def get_df():
    N = 10 ** 4
    labels = np.random.randint(0, 2000, size=N)
    labels2 = np.random.randint(0, 3, size=N)
    df = DataFrame(
        {
            ""key"": labels,
            ""key2"": labels2,
            ""value1"": np.random.randn(N),
            ""value2"": [""foo"", ""bar"", ""baz"", ""qux""] * (N // 4),
        }
    )
    return df

df = get_df()

gb = df.groupby(""key"")

%prun -s cumulative gb.apply(lambda x: 1)
```

If we disable `fast_apply` on master, this gives:
```
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.278    0.278 groupby.py:701(apply)
        1    0.000    0.000    0.278    0.278 groupby.py:750(_python_apply_general)
        1    0.009    0.009    0.275    0.275 ops.py:151(apply)
     1987    0.003    0.000    0.257    0.000 ops.py:858(__iter__)
     1986    0.003    0.000    0.251    0.000 ops.py:889(_chop)
     1986    0.003    0.000    0.247    0.000 indexing.py:814(__getitem__)
     1986    0.001    0.000    0.243    0.000 indexing.py:1462(_getitem_axis)
     1986    0.003    0.000    0.242    0.000 indexing.py:1488(_get_slice_axis)
     1986    0.007    0.000    0.230    0.000 generic.py:3470(_slice)
     1986    0.008    0.000    0.203    0.000 managers.py:713(get_slice)
     1987    0.005    0.000    0.129    0.000 managers.py:125(__init__)
     1987    0.003    0.000    0.060    0.000 managers.py:634(_consolidate_check)
     1987    0.026    0.000    0.059    0.000 managers.py:215(_rebuild_blknos_and_blklocs)
     1987    0.003    0.000    0.056    0.000 managers.py:635(<listcomp>)
     5961    0.015    0.000    0.053    0.000 blocks.py:335(ftype)
```

If we disable `fast_apply` on this PR:
```
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.198    0.198 groupby.py:701(apply)
        1    0.000    0.000    0.198    0.198 groupby.py:750(_python_apply_general)
        1    0.008    0.008    0.195    0.195 ops.py:151(apply)
     1979    0.002    0.000    0.176    0.000 ops.py:903(__iter__)
     1978    0.002    0.000    0.172    0.000 ops.py:934(_chop)
     1978    0.003    0.000    0.169    0.000 indexing.py:814(__getitem__)
     1978    0.001    0.000    0.165    0.000 indexing.py:1462(_getitem_axis)
     1978    0.003    0.000    0.164    0.000 indexing.py:1488(_get_slice_axis)
     1978    0.006    0.000    0.153    0.000 generic.py:3470(_slice)
     1978    0.007    0.000    0.129    0.000 managers.py:713(get_slice)
     1980    0.004    0.000    0.061    0.000 managers.py:125(__init__)
     1980    0.021    0.000    0.052    0.000 managers.py:215(_rebuild_blknos_and_blklocs)
     1978    0.002    0.000    0.048    0.000 managers.py:723(<listcomp>)
     5934    0.010    0.000    0.045    0.000 blocks.py:310(getitem_block)
     5942    0.003    0.000    0.031    0.000 blocks.py:275(make_block_same_class)
```

We save almost 30% by lazifying the consolidation check and consolidate on _slice."
553461830,31200,raise more explanatory error when failing initialising pd.Timestamp,giuliobeseghi,closed,2020-01-22T10:54:37Z,2020-02-26T02:19:00Z,"It's not really an issue, just a suggestion. I noticed this behavior when initialising a timestamp:

```python
# this doesn't fail
pd.Timestamp('2020')
# this does
pd.Timestamp(year=2020)
```

I guess that the latter fails because of args/kwargs specification issues (when an integer is passed it is read as a posix timestamp in the first place), and I'm OK with it. However, the error it raises is:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-9-e2faf77534f3> in <module>
----> 1 pd.Timestamp(year=2020)

pandas/_libs/tslibs/timestamps.pyx in pandas._libs.tslibs.timestamps.Timestamp.__new__()

TypeError: an integer is required (got type NoneType)
```

This is a bit misleading for beginners - it doesn't say anything about **why** it failed, but just about **what** failed.

Is there a chance it can be changed to something more explicit in the long term? Something like ""provide at least year, month, day""?
"
21787065,5377,ENH:  DataFrame.value_counts(),jtratner,closed,2013-10-29T20:52:17Z,2020-02-26T02:20:44Z,"Enable value_counts on DataFrame by delegating along axis (ie columns/items) to value_counts as convenience, boiling down to same Series value_counts (Panel is more complicated so not going to attempt for now).

Gets weird with heterogeneous dtypes (b/c union of all unique values) 

@rockg - can you make up some test cases? I already have the implementation. I can put together panel.
"
554199165,31247,ENH: Implement DataFrame.value_counts,dsaxton,closed,2020-01-23T14:39:35Z,2020-02-26T02:24:35Z,"- [x] closes #5377
- [x] tests added / passed
- [x] passes `black pandas`
- [x] whatsnew entry

This is picking up where https://github.com/pandas-dev/pandas/pull/27350 left off because I think it'd be a nice feature to have. At least one thing that still needs to be done is implementing `bins` when we have only a single column in `subset`, in which case maybe we can just delegate to `Series.value_counts`."
570292846,32232,use ExtensionIndex._concat_same_dtype,jbrockmendel,closed,2020-02-25T03:49:29Z,2020-02-26T02:35:19Z,"DatetimelikeIndex._concat_same_dtpye is identical to ExtensionIndex version, so re-use that."
570795665,32247,REF: simplify IntervalIndex/IntervalArray _shallow_copy,jbrockmendel,closed,2020-02-25T19:54:31Z,2020-02-26T04:08:01Z,xref #32244
569395721,32187,CLN/TST: parametrize some tests in tests.indexing.test_float,jbrockmendel,closed,2020-02-22T21:27:38Z,2020-02-26T04:43:27Z,
568804338,32143,Add ecosystem link to pandas-tfrecords project,schipiga,closed,2020-02-21T08:16:09Z,2020-02-26T05:50:43Z,to understand tfrecords format requires quite much time and finally its details are not so needed in daily work with data. [pandas-tfrecords](https://pypi.org/project/pandas-tfrecords/) hides low-level details and provides easy human-usable api to convert pandas to tfrecords and restore back.
570848108,32249,Deserialization with pyarrow fails for certain Timestamp-based data frame,fuglede,closed,2020-02-25T21:34:11Z,2020-02-26T08:37:12Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd                                                                      
import pyarrow as pa                                                                     
df = pd.DataFrame([{'Minutes5UTC': '2020-02-25T21:15:00+00:00', 'Minutes5DK': '2020-02-25T22:15:00'}])                                                        
df['Minutes5DK'] = pd.to_datetime(df.Minutes5DK)                                         
df['Minutes5UTC'] = pd.to_datetime(df.Minutes5UTC)                                       
context = pa.default_serialization_context()                                             
pa.deserialize(pa.serialize(df).to_buffer().to_pybytes())
```
#### Problem description
When following the [procedure outlined here](https://stackoverflow.com/a/57986261/5085211) to use `pyarrow` to serialize/deserialize data frames, the above example fails with the following traceback:

```
--------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-9-6f75cc47c6d5> in <module>
----> 1 pa.deserialize(pa.serialize(df).to_buffer().to_pybytes())

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/serialization.pxi in pyarrow.lib.deserialize()

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/serialization.pxi in pyarrow.lib.deserialize_from()

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/serialization.pxi in pyarrow.lib.SerializedPyObject.deserialize()

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/serialization.pxi in pyarrow.lib.SerializationContext._deserialize_callback()

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/serialization.py in _deserialize_pandas_dataframe(data)
    167 
    168     def _deserialize_pandas_dataframe(data):
--> 169         return pdcompat.serialized_dict_to_dataframe(data)
    170 
    171     def _serialize_pandas_series(obj):

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/pandas_compat.py in serialized_dict_to_dataframe(data)
    661 def serialized_dict_to_dataframe(data):
    662     import pandas.core.internals as _int
--> 663     reconstructed_blocks = [_reconstruct_block(block)
    664                             for block in data['blocks']]
    665 

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/pandas_compat.py in <listcomp>(.0)
    661 def serialized_dict_to_dataframe(data):
    662     import pandas.core.internals as _int
--> 663     reconstructed_blocks = [_reconstruct_block(block)
    664                             for block in data['blocks']]
    665 

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/pandas_compat.py in _reconstruct_block(item, columns, extension_columns)
    707                                 klass=_int.CategoricalBlock)
    708     elif 'timezone' in item:
--> 709         dtype = make_datetimetz(item['timezone'])
    710         block = _int.make_block(block_arr, placement=placement,
    711                                 klass=_int.DatetimeTZBlock,

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/pandas_compat.py in make_datetimetz(tz)
    734 def make_datetimetz(tz):
    735     tz = pa.lib.string_to_tzinfo(tz)
--> 736     return _pandas_api.datetimetz_type('ns', tz=tz)
    737 
    738 

TypeError: 'NoneType' object is not callable
```

Perhaps interestingly, if I comment out the two `pd.to_datetime` lines, the thing works (perhaps unsurprisingly), but if I then include them again, the original reproducing example all of a sudden works. That is, this works:

```python
import pandas as pd                                                                      
import pyarrow as pa                                                                     
df = pd.DataFrame([{'Minutes5UTC': '2020-02-25T21:15:00+00:00', 'Minutes5DK': '2020-02-25T22:15:00'}])
context = pa.default_serialization_context()
pa.deserialize(pa.serialize(df).to_buffer().to_pybytes())

df = pd.DataFrame([{'Minutes5UTC': '2020-02-25T21:15:00+00:00', 'Minutes5DK': '2020-02-25T22:15:00'}])
df['Minutes5DK'] = pd.to_datetime(df.Minutes5DK)
df['Minutes5UTC'] = pd.to_datetime(df.Minutes5UTC)
context = pa.default_serialization_context()
pa.deserialize(pa.serialize(df).to_buffer().to_pybytes())
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.9.0-11-amd64
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
405366519,25057,API/ENH: Support/document/test fold argument in Timestamp,mroeschke,closed,2019-01-31T17:53:39Z,2020-02-26T12:35:03Z,"Python 3.6 added a `fold` argument in `datetime.datetime` to disambiguate DST transition times that occur twice (in wall time).

https://docs.python.org/3/library/datetime.html#datetime-objects.

Technically `Timestamp` will accept the argument in 3.6, but it's not formally documented or tested.
Additionally since we will still be supporting 3.5 after dropping 2.7, we can add/handle a fold argument directly in the `Timestamp` constructor as well.
"
555493178,31338,BUG: Timestamp UTC offset incorrect for dateutil tz in ambiguous DST time,AlexKirko,closed,2020-01-27T10:47:32Z,2020-02-26T12:35:04Z,"#### Code Sample, a copy-pastable example if possible

If we make a Timestamp in an ambiguous DST period while specifying via the offset (or by supplying `Timestamp.value` directly) that the time is before DST switch, the representation then shows that this is after DST switch. This is backed up by calling `Timestamp.tz.utcoffset(Timestamp)`.

```python

IN:
t1 = pd.Timestamp(1382837400000000000, tz='dateutil/Europe/London')
t1

OUT:
Timestamp('2013-10-27 01:30:00+0100', tz='dateutil/GB-Eire')

IN:
t2 = pd.Timestamp(1382837400000000000, tz='Europe/London')
t2

OUT:
Timestamp('2013-10-27 01:30:00+0000', tz='Europe/London')
```


#### Problem description


The reason for this bug looks to be buried deep in the interaction of `pandas` and `dateutil`.

So this is what I've been able to dig up. When we try to determine whether we are in DST or not, we rely on `timezone.utcoffset` of the underlying timezone package. What gets executed in `dateutil` is this:

```python
def utcoffset(self, dt):
	...

	return self._find_ttinfo(dt).delta
	
def _find_ttinfo(self, dt):
	idx = self._resolve_ambiguous_time(dt)
	...

def _resolve_ambiguous_time(self, dt):
	idx = self._find_last_transition(dt)

	# If we have no transitions, return the index
	_fold = self._fold(dt)
	if idx is None or idx == 0:
		return idx

	# If it's ambiguous and we're in a fold, shift to a different index.
	idx_offset = int(not _fold and self.is_ambiguous(dt, idx))

	return idx - idx_offset
```

`dateutil` is expecting an ordinary `datetime.timedelta` object here, so this is what it does:

1. Use `_find_last_transition` to get the index of the last DST transition before `dt`. This is done by computing `timedelta.total_seconds` since epoch time. Our `pandas.Timedelta.total_seconds` is smart, and returns different `total_seconds` for before and after `DST`, since we basically return `Timedelta.value` which is the same as `Timestamp.value` when counting since epoch time (because of how `_Timestamp.__sub__` in `c_timestamp.pyx` is implemented).

This is what we do (doesn't care about `dt.replace(tzinfo=None)`):
```python
def total_seconds(self):
	""""""
	Total duration of timedelta in seconds (to microsecond precision).
	""""""
	# GH 31043
	# Microseconds precision to avoid confusing tzinfo.utcoffset
	return (self.value - self.value % 1000) / 1e9
```
This is what `datetime.timedelta` does (loses DST awareness after `dt.replace(tzinfo=None)`):
```python
def total_seconds(self):
	""""""Total seconds in the duration.""""""
	return ((self.days * 86400 + self.seconds) * 10**6 +
			self.microseconds) / 10**6
```
2. The remainder of `_resolve_ambiguous_time` corrects for ambiguous times, since `datetime.timedelta.total_seconds` after `dt.replace(tzinfo=None)` isn't DST-aware. It checks if we are in an ambiguous period and if this is the first time this time has occured: this is what `self._fold` is for. [fold](https://www.python.org/dev/peps/pep-0495/#the-fold-attribute) is 0 for the first time, and 1 for the second time. If it's the first time, `dateutil` shifts the relevant transition index back by 1, since it thinks that `total_seconds` always returns the number of seconds calculated using the second time.

I'd like to discuss how we are going to approach this. From what I see, there isn't much we can do on our end. ~Making `total_seconds` non-DST-aware by default is bad, because that would be making our implementation less precise unless the user passes a parameter.~ Scratch that. The problem isn't so much the `total_seconds` implementation as it is the `Timestamp.__sub__` implementation which preserves `value` when we subtract epoch time.

Another approach is to go to `dateutil` with this and implement a check there to avoid running the correction if they are dealing with a `pandas.Timedelta`. Might be tricky to do without introducing a dependency on pandas, though.

First came across this while solving #24329 in #30995 

#### Expected Output
```python
IN:
t1
OUT:
Timestamp('2013-10-27 01:30:00+0000', tz='dateutil/GB-Eire')
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : ru_RU.UTF-8
LOCALE           : None.None

pandas           : 0.26.0.dev0+1947.gca3bfcc54.dirty
numpy            : 1.17.5
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 44.0.0.post20200106
Cython           : 0.29.14
pytest           : 5.3.4
hypothesis       : 5.2.0
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.2
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.4
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.3.1
sqlalchemy       : 1.3.12
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.47.0

</details>
"
569049345,32157,API: joining DatetimeIndexes not preserving freq,jbrockmendel,closed,2020-02-21T16:15:57Z,2020-02-26T12:36:36Z,"```
dti = pd.date_range('2016-01-01', periods=10)
left, right = dti[:5], dti[5:]
joined = left.join(right, how=""outer"")
```

In master `joined.freq` comes back as None, while in 0.25.3 it was ""D"".  There is some code in DatetimeIndex._wrap_joined_index that suggests to me this was intentional.

Can someone confirm this suspicion cc @jorisvandenbossche ?  If so, I've got a branch that restores the old behavior, just need to add tests."
569586193,32213,pandas.to_datetime raises when given pd.NA,dsaxton,closed,2020-02-24T00:27:59Z,2020-02-26T12:39:56Z,"`to_datetime` raises when we pass in `pd.NA` but it should probably convert to `pd.NaT`, I think (or at least this would be better than an error)?

```python
In [1]: import pandas as pd                                                                                                                                            

In [2]: pd.to_datetime([pd.NA])                                                                                                                                        
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-1af314749017> in <module>
----> 1 pd.to_datetime([pd.NA])

~/pandas/pandas/core/tools/datetimes.py in to_datetime(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)
    755             result = _convert_and_box_cache(arg, cache_array)
    756         else:
--> 757             result = convert_listlike(arg, format)
    758     else:
    759         result = convert_listlike(np.array([arg]), format)[0]

~/pandas/pandas/core/tools/datetimes.py in _convert_listlike_datetimes(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)
    447             errors=errors,
    448             require_iso8601=require_iso8601,
--> 449             allow_object=True,
    450         )
    451 

~/pandas/pandas/core/arrays/datetimes.py in objects_to_datetime64ns(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)
   1848             dayfirst=dayfirst,
   1849             yearfirst=yearfirst,
-> 1850             require_iso8601=require_iso8601,
   1851         )
   1852     except ValueError as e:

~/pandas/pandas/_libs/tslib.pyx in pandas._libs.tslib.array_to_datetime()
    479 @cython.wraparound(False)
    480 @cython.boundscheck(False)
--> 481 cpdef array_to_datetime(ndarray[object] values, str errors='raise',
    482                         bint dayfirst=False, bint yearfirst=False,
    483                         object utc=None, bint require_iso8601=False):

~/pandas/pandas/_libs/tslib.pyx in pandas._libs.tslib.array_to_datetime()
    701 
    702     except TypeError:
--> 703         return array_to_datetime_object(values, errors,
    704                                         dayfirst, yearfirst)
    705 

~/pandas/pandas/_libs/tslib.pyx in pandas._libs.tslib.array_to_datetime_object()
    839         else:
    840             if is_raise:
--> 841                 raise
    842             return values, None
    843     return oresult, None

~/pandas/pandas/_libs/tslib.pyx in pandas._libs.tslib.array_to_datetime()
    674                         iresult[i] = NPY_NAT
    675                     else:
--> 676                         raise TypeError(f""{type(val)} is not convertible to datetime"")
    677 
    678             except OutOfBoundsDatetime:

TypeError: <class 'pandas._libs.missing.NAType'> is not convertible to datetime

In [3]: pd.__version__                                                                                                                                                 
Out[3]: '1.1.0.dev0+572.gaa6f241f5'
```

#### Expected Output

```
DatetimeIndex(['NaT'], dtype='datetime64[ns]', freq=None)
```"
570501889,32236,Added message to pytest raises for test_constructor_dict,Dom-L-G,closed,2020-02-25T11:26:39Z,2020-02-26T12:43:58Z,"- [x] ref #30999 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x]  added message to pytest.raises for test_constructor_dict
"
570462317,32235,Fix exception causes in 14 modules,cool-RR,closed,2020-02-25T10:21:12Z,2020-02-26T12:45:32Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
571323378,32267,Backport PR #32214 on branch 1.0.x (BUG: Cast pd.NA to pd.NaT in to_datetime),meeseeksmachine,closed,2020-02-26T12:40:34Z,2020-02-26T15:33:16Z,Backport PR #32214: BUG: Cast pd.NA to pd.NaT in to_datetime
566034923,32047,use numexpr for Series comparisons,jbrockmendel,closed,2020-02-17T03:31:33Z,2020-02-26T15:36:19Z,subsumes #31984.
343341304,22012,BUG: pd.DataFrame.equals return False for same values if different numpy type,louis-red,closed,2018-07-21T17:13:02Z,2020-02-26T15:44:13Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np
df_int32 = pd.DataFrame([0, 1], dtype=np.int32)
df_int64 = pd.DataFrame([0, 1], dtype=np.int64)
df_int32.equals(df_int64)  # False
```
#### Problem description

The docstring of `pd.DataFrame.equals` explains : ""Determines if two NDFrame objects contain the same elements"". Or the result here is not consistent with `(df_int32 == df_int64).all()[0]` that tries to do precisely what the docstring is describing.
This issue exists on the master branch. It is experimented whenever the `dtype`s of the two dataframes are different, not just for `int`s.

#### Expected Output
`True`

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.6.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-130-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: fr_FR.UTF-8
LOCALE: fr_FR.UTF-8

pandas: 0.23.3
pytest: 3.6.3
pip: 10.0.1
setuptools: 39.2.0
Cython: 0.28.3
numpy: 1.14.5
scipy: 1.1.0
pyarrow: 0.9.0
xarray: 0.10.7
IPython: 6.4.0
sphinx: 1.7.5
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.5
feather: 0.4.0
matplotlib: 2.2.2
openpyxl: 2.5.4
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.5
lxml: 4.2.2
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.8
pymysql: 0.9.1
psycopg2: None
jinja2: 2.10
s3fs: 0.1.5
fastparquet: 0.1.5
pandas_gbq: None
pandas_datareader: None


</details>
"
569244694,32166,REGR: preserve freq in DTI/TDI outer join,jbrockmendel,closed,2020-02-22T00:09:16Z,2020-02-27T15:34:46Z,"- [x] closes #32157
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

also just a nice cleanup, hoping to get this into ExtensionIndex before long"
571049024,32260,CLN: remove dtype kwarg from _simple_new,jbrockmendel,closed,2020-02-26T03:42:23Z,2020-02-26T16:08:13Z,"DatetimeIndex remains an outlier, upcoming PR"
567653073,32109,DataFrame: Ctor from non abc.Iterable 2-d array like,jschueller,closed,2020-02-19T15:56:14Z,2020-02-26T17:43:30Z,"In case we pass a 2d array like that do not use __iter__ but the
__getitem__ interface, it falls outside the collection.abc.Iterable case
and throw.

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
559200338,31624,DOC: link on nullables in indexing,anant4299,closed,2020-02-03T16:40:44Z,2020-02-26T20:23:11Z,"DOC: Mention that boolean indexing is impossible for new nullable integer/boolean and string data types when they contain missing values

closes #31537 "
565729760,32006,DOC: Fix pandas.index.copy summary documentation,asepwhite,closed,2020-02-15T10:35:39Z,2020-02-26T20:36:11Z,"- [ ] closes https://github.com/pandanistas/pandanistas_sprint_jakarta2020/issues/8
- [ ] tests added / passed
- [ ] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
Output of `python scripts/validate_docstrings.py pandas.Index.copy`

################################################################################
######################## Docstring (pandas.Index.copy)  ########################
################################################################################

Make a copy of this object.

Name and dtype sets those attributes on the new object.

Parameters
----------
name : Label, optional
    Set name for new object.
deep : bool, default False
dtype : numpy dtype or pandas type, optional
    Set dtype for new object.
names : list-like, optional
    Kept for compatibility with MultiIndex. Should not be used.

Returns
-------
Index
    Index refer to new object which is a copy of this object.

Notes
-----
In most cases, there should be no functional difference from using
``deep``, but if ``deep`` is passed it will attempt to deepcopy.

################################################################################
################################## Validation ##################################
################################################################################

3 Errors found:
        Parameter ""deep"" has no description
        See Also section not found
        No examples section found"
565731291,32009,DOC: Improve documentation for Index.where,laymonage,closed,2020-02-15T10:50:46Z,2020-02-26T20:39:02Z,"- [x] closes https://github.com/pandanistas/pandanistas_sprint_jakarta2020/issues/15
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565724803,32002,DOC: Update the pandas.DataFrame.plot docstring,dmaharika,closed,2020-02-15T09:46:58Z,2020-02-26T20:47:44Z,"Give description for 'figsize','xlim', and 'ylim'.

- [x] closes https://github.com/pandanistas/pandanistas_sprint_jakarta2020/issues/11
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Output of `python scripts/validate_docstrings.py pandas.DataFrame.plot`:|

################################################################################
###################### Docstring (pandas.DataFrame.plot)  ######################
################################################################################

Make plots of Series or DataFrame.

Uses the backend specified by the
option ``plotting.backend``. By default, matplotlib is used.

Parameters
----------
data : Series or DataFrame
    The object for which the method is called.
x : label or position, default None
    Only used if data is a DataFrame.
y : label, position or list of label, positions, default None
    Allows plotting of one column versus another. Only used if data is a
    DataFrame.
kind : str
    The kind of plot to produce:

    - 'line' : line plot (default)
    - 'bar' : vertical bar plot
    - 'barh' : horizontal bar plot
    - 'hist' : histogram
    - 'box' : boxplot
    - 'kde' : Kernel Density Estimation plot
    - 'density' : same as 'kde'
    - 'area' : area plot
    - 'pie' : pie plot
    - 'scatter' : scatter plot
    - 'hexbin' : hexbin plot.

figsize : a tuple (width, height) in inches
    The size of the figure to create in matplotlib.
use_index : bool, default True
    Use index as ticks for x axis.
title : str or list
    Title to use for the plot. If a string is passed, print the string
    at the top of the figure. If a list is passed and `subplots` is
    True, print each item in the list above the corresponding subplot.
grid : bool, default None (matlab style default)
    Axis grid lines.
legend : bool or {'reverse'}
    Place legend on axis subplots.
style : list or dict
    The matplotlib line style per column.
logx : bool or 'sym', default False
    Use log scaling or symlog scaling on x axis.
    .. versionchanged:: 0.25.0

logy : bool or 'sym' default False
    Use log scaling or symlog scaling on y axis.
    .. versionchanged:: 0.25.0

loglog : bool or 'sym', default False
    Use log scaling or symlog scaling on both x and y axes.
    .. versionchanged:: 0.25.0

xticks : sequence
    Values to use for the xticks.
yticks : sequence
    Values to use for the yticks.
xlim : 2-tuple/list
    Set or query x-axis limits.
ylim : 2-tuple/list
    Set or query y-axis limits.
rot : int, default None
    Rotation for ticks (xticks for vertical, yticks for horizontal
    plots).
fontsize : int, default None
    Font size for xticks and yticks.
colormap : str or matplotlib colormap object, default None
    Colormap to select colors from. If string, load colormap with that
    name from matplotlib.
colorbar : bool, optional
    If True, plot colorbar (only relevant for 'scatter' and 'hexbin'
    plots).
position : float
    Specify relative alignments for bar plot layout.
    From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5
    (center).
table : bool, Series or DataFrame, default False
    If True, draw a table using the data in the DataFrame and the data
    will be transposed to meet matplotlib's default layout.
    If a Series or DataFrame is passed, use passed data to draw a
    table.
yerr : DataFrame, Series, array-like, dict and str
    See :ref:`Plotting with Error Bars <visualization.errorbars>` for
    detail.
xerr : DataFrame, Series, array-like, dict and str
    Equivalent to yerr.
mark_right : bool, default True
    When using a secondary_y axis, automatically mark the column
    labels with ""(right)"" in the legend.
include_bool : bool, default is False
    If True, boolean values can be plotted.
backend : str, default None
    Backend to use instead of the backend specified in the option
    ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to
    specify the ``plotting.backend`` for the whole session, set
    ``pd.options.plotting.backend``.

    .. versionadded:: 1.0.0

**kwargs
    Options to pass to matplotlib plotting method.

Returns
-------
:class:`matplotlib.axes.Axes` or numpy.ndarray of them
    If the backend is not the default matplotlib one, the return value
    will be the object returned by the backend.

Notes
-----
- See matplotlib documentation online for more on this subject
- If `kind` = 'bar' or 'barh', you can specify relative alignments
  for bar plot layout by `position` keyword.
  From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5
  (center)

################################################################################
################################## Validation ##################################
################################################################################

3 Errors found:
        Unknown parameters {'colorbar', '**kwargs', 'style', 'use_index', 'rot', 'mark_right', 'grid', 'y', 'fontsize', 'position', 'kind', 'include_bool', 'logy', 'x', 'yerr', 'xlim', 'backend', 'table', 'logx', 'ylim', 'yticks', 'xerr', 'loglog', 'legend', 'colormap', 'figsize', 'title', 'xticks'}
        See Also section not found
        No examples section found"
565734379,32016,DOC: Add reference in keywords arguments to Line2D mathplotlib,nsiregar,closed,2020-02-15T11:18:33Z,2020-02-26T20:52:16Z,"Keyword arguments in `pandas.plotting.bootstrap_plot` doesn't clearly specify argument options

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
519496634,29472,CI: Move documentation build to GitHub actions,datapythonista,closed,2019-11-07T20:33:09Z,2020-02-26T21:08:05Z,"GitHub actions is out of beta, and should have much better integration with GitHub than pipelines.

I'm thinking on giving it a try for the documentation build, and see if we can get an improvement on current CI problems:
- Too many steps from the PR page to the logs (#26895)
- Notify with a comment on the PR when there are problems (#26930)
- Publish the documentation for a PR so it can be seen

Any objection? Depending on how things go with the doc build, I think it may make sense to give other builds a try to."
566788311,32074,CI: Remove docs build from pipelines,datapythonista,closed,2020-02-18T10:18:24Z,2020-02-26T21:08:06Z,"- [X] closes #29472

At the moment we're building the web and the docs from both Actions and Pipelines. While is still under discussion where things should be hosted, it's probably worth to just build on Actions for now, and publish from there to both the OVH cloud, and to GitHub pages.

**This needs the GitHub pages ssh key to be added to the secrets of this repo with name ""github_pages_ssh_key"".*** Can someone with permissions please add it? Or if we already got it with a different name, please let me know the name.
"
569586517,32214,BUG: Cast pd.NA to pd.NaT in to_datetime,dsaxton,closed,2020-02-24T00:30:05Z,2020-02-26T21:25:40Z,"- [x] closes #32213
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
569559947,32209,CLN: Use defaultdict for minor optimization,jaketae,closed,2020-02-23T21:18:20Z,2020-02-26T22:02:22Z,"Edit `_from_nested_dict()` by using `defaultdict` for marginal optimization

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
569504449,32198,DOC: add missing links to introduction to pandas,raisadz,closed,2020-02-23T14:35:26Z,2020-02-27T01:15:56Z,I noticed that there were some links missing in `10 minutes to pandas'.
565736773,32019,DOC: Fix errors in pandas.Series.argmax,farhanreynaldo,closed,2020-02-15T11:42:12Z,2020-02-27T07:35:55Z,"- [x] closes https://github.com/pandanistas/pandanistas_sprint_jakarta2020/issues/18
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

output of `python scripts/validate_docstrings.py pandas.Series.argmax`:
```
################################################################################
################################## Validation ##################################
################################################################################
```
"
570782807,32246,web site unwanted message,simonjayhawkins,closed,2020-02-25T19:30:16Z,2020-02-27T10:55:57Z,"![image](https://user-images.githubusercontent.com/13159005/75280356-3118a180-5805-11ea-9315-b9af6124b464.png)
"
571010955,32257,PERF: pass through to numpy validation for iloc setitem,jbrockmendel,closed,2020-02-26T02:00:45Z,2020-02-27T14:55:30Z,"We lose a little bit of ground on the range and slice (not really sure why), pick up a bigger amount of ground on list or ndarray.

```
In [3]: ser = pd.Series(range(10**5))                                                                                      
In [4]: key = range(100, 200) 
In [5]: key2 = list(key)
In [6]: key3 = slice(100, 200)                                                                                             
In [7]: key4 = np.array(key2)                                                                                                 

In [16]: %timeit ser.iloc[key] = 1
56.3 µs ± 1.19 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- master
62.9 µs ± 1.77 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- PR

In [17]: %timeit ser.iloc[key2] = 1
95.6 µs ± 2.69 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- master
55.6 µs ± 922 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- PR

In [20]: %timeit ser.iloc[key3] = 1
49 µs ± 756 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- master
50.6 µs ± 1.01 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- PR

In [21]: %timeit ser.iloc[key4] = 1
71.6 µs ± 1.98 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- master
45.7 µs ± 427 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  # <-- PR
```"
571548359,32272,TST: method-specific files for droplevel,jbrockmendel,closed,2020-02-26T17:37:25Z,2020-02-27T14:55:37Z,
571486271,32269,Backport PR #32166 on branch 1.0.x,jbrockmendel,closed,2020-02-26T15:56:14Z,2020-02-27T14:57:25Z,#32166
572049851,32291,BUG: groupby with sort=False creates buggy MultiIndex ,MarcoGorelli,closed,2020-02-27T12:28:04Z,2020-02-27T15:51:21Z,"- [ ] closes #32259 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

EDIT
----
probably better/less invasive to make sure .groupby(, sort=False) returns an object where the index has codes which reflect the ordering"
571572342,32275,TST: misplaced arithmetic tests,jbrockmendel,closed,2020-02-26T18:25:37Z,2020-02-27T16:16:04Z,
571564526,32274,TST: implement test_first,jbrockmendel,closed,2020-02-26T18:09:49Z,2020-02-27T16:21:28Z,"cc @MomIsBestFriend there are three things about these tests that I think would make for good (separate) follow-ups if you're interested:

1) They are just begging to be parametrized
2) We don't have a systematic way of naming/organizing tests that are for two specific methods rather than one specific method, which is the idea behind tests.(frame|series).methods.  (see also: head/tail, first_valid_index/last_valid_index)
3) These tests could/should also be shared/parametrized over Series vs DataFrame, but we dont have a systematic home or naming convention for these."
572047046,32290,TST/CLN: Follow-up to #32158,simonjayhawkins,closed,2020-02-27T12:23:25Z,2020-02-27T16:32:33Z,"xref #32158
"
482035820,28004,Feature request: DataFrame.mse(),alik604,closed,2019-08-18T20:49:23Z,2020-02-27T17:19:31Z,"#### Code Sample
```python
def sample_MSE_implementation(df):
    from sklearn.metrics import mean_squared_error
    # If the user wish,they can pass the normalized df as a parameter
    # df=(df-df.min())/(df.max()-df.min()) 
    Matrix = [[round(mean_squared_error(df.iloc[:, row],df.iloc[:, col]),6) for row in range(df.columns.size)] for col in range(df.columns.size)] 

    cols = df.columns
    mse_df = pd.DataFrame(Matrix, columns=cols, index=cols)
    return mse_df
```

#### Problem description

I think there is a need for a `df.mse()`, similar to how there is a `df.corr()`. I have attached my own code and output to illustrate output and a very elementary implementation.

_My apologies if this is not the proper way to submit a feature requests. I would have made this a pull, however, I doubt the quility of my code_


#### Expected Output

![image](https://user-images.githubusercontent.com/29245410/63230121-75bfe200-c1bd-11e9-898d-e71cec9ba05d.png)
![image](https://user-images.githubusercontent.com/29245410/63230308-bf113100-c1bf-11e9-93c9-7a781c4e4914.png)

#### Output of ``pd.show_versions()``

<details>
please note that I am using colab.research.google.com

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.8.final.0
python-bits: 64
OS: Linux
OS-release: 4.14.79+
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 3.6.4
pip: 19.2.1
setuptools: 41.0.1
Cython: 0.29.13
numpy: 1.16.4
scipy: 1.3.1
pyarrow: 0.14.1
xarray: 0.11.3
IPython: 5.5.0
sphinx: 1.8.5
patsy: 0.5.1
dateutil: 2.5.3
pytz: 2018.9
blosc: None
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.9
feather: 0.4.0
matplotlib: 3.0.3
openpyxl: 2.5.9
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: None
lxml.etree: 4.2.6
bs4: 4.6.3
html5lib: 1.0.1
sqlalchemy: 1.3.6
pymysql: None
psycopg2: 2.7.6.1 (dt dec pq3 ext lo64)
jinja2: 2.10.1
s3fs: 0.3.3
fastparquet: None
pandas_gbq: 0.4.1
pandas_datareader: 0.7.4
gcsfs: None
</details>
"
571784285,32285,REF: move misplaced to_time tests,jbrockmendel,closed,2020-02-27T02:45:33Z,2020-02-27T17:20:02Z,
571496904,32270,TST: method-specific file for to_period,jbrockmendel,closed,2020-02-26T16:12:01Z,2020-02-27T17:21:40Z,
572207110,32298,fixed minor docstring typo,mankoff,closed,2020-02-27T16:41:15Z,2020-02-27T17:53:49Z,"- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
571691154,32279,CI: Temporary fix to the docs build while we fix the ssh problems,datapythonista,closed,2020-02-26T22:08:46Z,2020-02-27T18:21:38Z,"Looks like the ssh key we have in the settings is not working as expected in #32074.

While we fix it (I don't have access to the settings, so can't check much), this prevents master from failing for the publishing of the dev docs to GitHub pages."
569289488,32172,DOC: Move pandas_development_faq from wiki to doc #30232,AdrianMastronardi,closed,2020-02-22T06:15:31Z,2020-02-27T19:09:01Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
572255436,32300,"REF/TST: misplaced tests in test_timeseries, test_timezones",jbrockmendel,closed,2020-02-27T17:59:40Z,2020-02-27T20:05:29Z,
567590330,32105,Added in a error message,Samira-g-js,closed,2020-02-19T14:01:05Z,2020-02-27T21:00:45Z,"- [x] ref #30999 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] Added in an error message
"
380555059,23687,BUG: 'Unnamed' != unnamed column in CSV,gfyoung,closed,2018-11-14T06:30:39Z,2020-02-27T22:45:47Z,"False criterion was causing errors when specified headers appeared to capture
a seemingly unnamed row, just because they had the string ""Unnamed"" in it.

Setup:

~~~python
from pandas import read_csv
from pandas.compat import StringIO

data = ""Unnamed,NotUnnamed\n0,1\n2,3\n4,5""
read_csv(StringIO(data), header=[0, 1])
~~~

Previously, this would error:

~~~python
...
ValueError : Passed header=[0,1] are too many rows for this multi_index of columns
~~~

Now, it nicely returns a `DataFrame`:

~~~python
  Unnamed NotUnnamed
        0          1
0       2          3
1       4          5
~~~

Leverages the patch used in #23484 of `self.unnamed_cols`."
486328636,28189,"Merge on CategoricalIndex fails if left_index=True & right_index=True, but not if on={index}",OliverHofkens,closed,2019-08-28T11:43:09Z,2020-02-27T23:07:26Z,"#### Code Sample, a copy-pastable example if possible



```python
import pandas as pd
import numpy as np

pdf = pd.DataFrame({
    ""idx"": pd.Categorical([""1""] * 4),
    ""value"": [1, 2, 3, 4]
})
pdf = pdf.set_index(""idx"")
pdf
```




<div>
<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>value</th>
    </tr>
    <tr>
      <th>idx</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>




```python
agg = pdf.groupby(""idx"").agg(np.sum)[""value""]
agg
```




    idx
    1    10
    Name: value, dtype: int64




```python
merged = pd.merge(pdf, agg, how=""left"", left_index=True, right_index=True)
merged
```


    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    <ipython-input-89-5347bee83336> in <module>
    ----> 1 merged = pd.merge(pdf, agg, how=""left"", left_index=True, right_index=True)
          2 merged


    /usr/local/lib/python3.7/site-packages/pandas/core/reshape/merge.py in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)
         46                          copy=copy, indicator=indicator,
         47                          validate=validate)
    ---> 48     return op.get_result()
         49 
         50 


    /usr/local/lib/python3.7/site-packages/pandas/core/reshape/merge.py in get_result(self)
        544                 self.left, self.right)
        545 
    --> 546         join_index, left_indexer, right_indexer = self._get_join_info()
        547 
        548         ldata, rdata = self.left._data, self.right._data


    /usr/local/lib/python3.7/site-packages/pandas/core/reshape/merge.py in _get_join_info(self)
        742             join_index, left_indexer, right_indexer = \
        743                 left_ax.join(right_ax, how=self.how, return_indexers=True,
    --> 744                              sort=self.sort)
        745         elif self.right_index and self.how == 'left':
        746             join_index, left_indexer, right_indexer = \


    /usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py in join(self, other, how, level, return_indexers, sort)
       3291             if self.is_monotonic and other.is_monotonic:
       3292                 return self._join_monotonic(other, how=how,
    -> 3293                                             return_indexers=return_indexers)
       3294             else:
       3295                 return self._join_non_unique(other, how=how,


    /usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py in _join_monotonic(self, other, how, return_indexers)
       3583         else:
       3584             if how == 'left':
    -> 3585                 join_index, lidx, ridx = self._left_indexer(sv, ov)
       3586             elif how == 'right':
       3587                 join_index, ridx, lidx = self._left_indexer(ov, sv)


    /usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py in _left_indexer(self, left, right)
        217 
        218     def _left_indexer(self, left, right):
    --> 219         return libjoin.left_join_indexer(left, right)
        220 
        221     def _inner_indexer(self, left, right):


    pandas/_libs/join.pyx in pandas._libs.join.__pyx_fused_cpdef()


    TypeError: No matching signature found


#### Problem description

The problem is triggered in `libjoin.left_join_indexer(left, right)`, where both `left` and `right` have `dtype(int8)`, which raises the `TypeError: No matching signature found`.

Interestingly enough, if you change  from

`pd.merge(pdf, agg, how=""left"", left_index=True, right_index=True)`

 to 

`pd.merge(pdf, agg, how=""left"", on=""idx"")` 

everything works fine as demonstrated below.

#### Expected Output

```python
merged = pd.merge(pdf, agg, how=""left"", on=""idx"")
merged
```




<div>
<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>value_x</th>
      <th>value_y</th>
    </tr>
    <tr>
      <th>idx</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>10</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>10</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>10</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>10</td>
    </tr>
  </tbody>
</table>
</div>



#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None

pandas           : 0.25.1
numpy            : 1.16.3
pytz             : 2019.1
dateutil         : 2.7.5
pip              : 19.1.1
setuptools       : 41.0.1
Cython           : 0.29.13
pytest           : None
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 7.5.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.0
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
334337701,21567,Inconsistent error where looking for label of wrong type,toobaz,closed,2018-06-21T04:53:13Z,2020-02-27T23:08:52Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: pd.Series(-1, index=list('abc')).loc[1]
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-c0de4e42231c> in <module>()
----> 1 pd.Series(-1, index=list('abc')).loc[1]

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in __getitem__(self, key)
   1476 
   1477             maybe_callable = com._apply_if_callable(key, self.obj)
-> 1478             return self._getitem_axis(maybe_callable, axis=axis)
   1479 
   1480     def _is_scalar_access(self, key):

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_axis(self, key, axis)
   1909 
   1910         # fall thru to straight lookup
-> 1911         self._validate_key(key, axis)
   1912         return self._get_label(key, axis=axis)
   1913 

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in _validate_key(self, key, axis)
   1786 
   1787             try:
-> 1788                 key = self._convert_scalar_indexer(key, axis)
   1789                 if not ax.contains(key):
   1790                     error()

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in _convert_scalar_indexer(self, key, axis)
    259         ax = self.obj._get_axis(min(axis, self.ndim - 1))
    260         # a scalar
--> 261         return ax._convert_scalar_indexer(key, kind=self.name)
    262 
    263     def _convert_slice_indexer(self, key, axis):

/home/pietro/nobackup/repo/pandas/pandas/core/indexes/base.py in _convert_scalar_indexer(self, key, kind)
   1662             elif kind in ['loc'] and is_integer(key):
   1663                 if not self.holds_integer():
-> 1664                     return self._invalid_indexer('label', key)
   1665 
   1666         return key

/home/pietro/nobackup/repo/pandas/pandas/core/indexes/base.py in _invalid_indexer(self, form, key)
   1846                         ""indexers [{key}] of {kind}"".format(
   1847                             form=form, klass=type(self), key=key,
-> 1848                             kind=type(key)))
   1849 
   1850     def get_duplicates(self):

TypeError: cannot do label indexing on <class 'pandas.core.indexes.base.Index'> with these indexers [1] of <class 'int'>

In [3]: pd.Series(-1, index=list('abc')).loc[1.0]
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in _validate_key(self, key, axis)
   1789                 if not ax.contains(key):
-> 1790                     error()
   1791             except TypeError as e:

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in error()
   1784                                .format(key=key,
-> 1785                                        axis=self.obj._get_axis_name(axis)))
   1786 

KeyError: 'the label [1.0] is not in the [index]'

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
<ipython-input-3-2f3185e45839> in <module>()
----> 1 pd.Series(-1, index=list('abc')).loc[1.0]

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in __getitem__(self, key)
   1476 
   1477             maybe_callable = com._apply_if_callable(key, self.obj)
-> 1478             return self._getitem_axis(maybe_callable, axis=axis)
   1479 
   1480     def _is_scalar_access(self, key):

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_axis(self, key, axis)
   1909 
   1910         # fall thru to straight lookup
-> 1911         self._validate_key(key, axis)
   1912         return self._get_label(key, axis=axis)
   1913 

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in _validate_key(self, key, axis)
   1796                 raise
   1797             except:
-> 1798                 error()
   1799 
   1800     def _is_scalar_access(self, key):

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in error()
   1783                 raise KeyError(u""the label [{key}] is not in the [{axis}]""
   1784                                .format(key=key,
-> 1785                                        axis=self.obj._get_axis_name(axis)))
   1786 
   1787             try:

KeyError: 'the label [1.0] is not in the [index]'
```

#### Problem description

``1`` and ``1.0`` are invalid keys in exactly the same way. So they should raise the same error (and the wrong error is the ``ValueError``, because ``pd.Series(-1, index=['a', 1]).loc[1]`` works just fine, which means that it's not a matter of type of key/index).

Closely related to #19456. Also related to #17569.

#### Expected Output

The same ``KeyError``.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.0-6-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.24.0.dev0+26.gbe90d4928
pytest: 3.0.6
pip: 9.0.1
setuptools: 33.1.1
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.18.1
pyarrow: None
xarray: None
IPython: 5.2.2
sphinx: None
patsy: 0.4.1+dev
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: 1.2.0
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.0.0
openpyxl: 2.3.0
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: None
lxml: 3.7.1
bs4: 4.5.3
html5lib: 0.999999999
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.8
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None


</details>
"
572330238,32309,DOC: Reorder 1.0 releases in whatsnew/index.rst,mroeschke,closed,2020-02-27T20:11:29Z,2020-02-28T00:09:20Z,"- Reorders 1.x releases in descending chronological order (consistent with the other versions sections)
- Hyperlinks `commit logs`
"
562954467,31867,API/BUG: raise only KeyError failed on geitem/loc lookups,jbrockmendel,closed,2020-02-11T03:20:07Z,2020-02-28T00:11:25Z,"- [x] closes #21567
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

ATM we raise a mix of KeyError and TypeError.

I'm in the process of going through the Indexing issues will see what else this closes."
555042558,31305,POC: use typing.final,jbrockmendel,closed,2020-01-25T04:49:23Z,2020-02-28T00:18:52Z,"When working on index/indexing code I frequently find myself checking whether a method is overriden in subclasses.  `@typing.final` marks a method as not-overriden in a way that mypy can check (py38+).  This imports typing.final in py38+ and makes a dummy otherwise.

We'd need to make sure the CI runs mypy in py38."
551552313,31110,POC: docstring inheritance,jbrockmendel,closed,2020-01-17T17:54:44Z,2020-02-28T00:19:13Z,"Proof of concept for automatically inheriting docstrings, so we dont have to use `@Appender(parent_class.method.__doc__)` quite so much.

This doesnt handle `@Substitution`.  If we can make that work for the common case where we're just putting in the class name, we could get a _lot_ less verbose.

xref #31095, #31060."
547590714,30853,REF: share searchsorted and insert for DTI/TDI,jbrockmendel,closed,2020-01-09T16:37:45Z,2020-02-28T00:22:19Z,
572502236,32321,Cant Read Panda,ghost,closed,2020-02-28T03:48:49Z,2020-02-28T04:17:33Z,"
```
first_concurrent_all_table = pd.read_html(requests.get(url).text)
first_concurrent_first_table = first_concurrent_all_table[0]
print(first_concurrent_first_table)
```

The error is the following :

    irst_concurrent_all_table = pd.read_html(requests.get(url).text)
  File ""/usr/local/lib/python3.7/dist-packages/pandas/io/html.py"", line 1100, in read_html
    displayed_only=displayed_only,
  File ""/usr/local/lib/python3.7/dist-packages/pandas/io/html.py"", line 915, in _parse
    raise retained
  File ""/usr/local/lib/python3.7/dist-packages/pandas/io/html.py"", line 895, in _parse
    tables = p.parse_tables()
  File ""/usr/local/lib/python3.7/dist-packages/pandas/io/html.py"", line 213, in parse_tables
    tables = self._parse_tables(self._build_doc(), self.match, self.attrs)
  File ""/usr/local/lib/python3.7/dist-packages/pandas/io/html.py"", line 545, in _parse_tables
    raise ValueError(""No tables found"")
ValueError: No tables found

When I tried this yesterday it was fine. The website had not changed anything regarding their code.
Desired output is the table.


"
558589261,31550,BUG: parase_dates column is in dataframe (#31251),sathyz,closed,2020-02-01T19:59:54Z,2020-02-28T09:38:57Z,"- [ ] closes #31251 
- [ ] add test_missing_column in pandas/tests/io/parser/test_parse_dates.py
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
569332167,32178,"DOC: Added recommanded ""msg"" creation format",ShaharNaveh,closed,2020-02-22T13:03:56Z,2020-02-28T09:40:25Z,"- [x] ref https://github.com/pandas-dev/pandas/pull/32158#discussion_r382710851
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
572635266,32323,STY: spaces in wrong place,ShaharNaveh,closed,2020-02-28T09:33:22Z,2020-02-28T10:28:34Z,"- [x] ref #30755
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

"
571183062,32263,"bug: inconsistent Series indexing crash, depending on DataFrame index type??",xuancong84,closed,2020-02-26T09:17:40Z,2020-02-28T11:52:07Z,"Recently, I encountered a very strange bug when indexing pd.Series in Pandas groupby object.
```
if False:
	df = pd.DataFrame({'app':['a', 'b', 'c', 'd', 'e'], 'sessionId':['1', '1', '2', '3', '3']},
		index=['a1', 'a2', 'a3', 'a4', 'a5']) # this will not cause the last line to crash
else:
	df = pd.DataFrame({'app': ['a', 'b', 'c', 'd', 'e'], 'sessionId': ['1', '1', '2', '3', '3']},
		index=[1,2,3,4,5]) # this will cause the last line to crash

df1 = [v.app[0] for k, v in df.groupby('sessionId')] # the crashing line
```
As shown in the above code, if df.index elements are string, pd.Timestamp, etc., the last line will not crash; however, if df.index elements are int, float, etc., the last line will crash. Ridiculously, in both cases `type(v.app)` is pd.Series, yet one is index-able, while the other one is not. It is worth noting that the crashing line has not even touched df.index, so why does the value type of df.index elements matters?

So do we have another general inconsistency due to incompetent programming?"
572065609,32292,TST: Use `sort` fixture in more places,SaturnFromTitan,closed,2020-02-27T12:55:54Z,2020-02-28T14:40:32Z,"- [x] closes #32183
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
569369138,32183,Use `sort` fixture in more places,SaturnFromTitan,closed,2020-02-22T17:54:57Z,2020-02-28T14:40:59Z,"This is a follow-up on [this review comment by jbrockmendel](https://github.com/pandas-dev/pandas/pull/32046#discussion_r382697823).

There's a `sort` fixture in `tests.indexes.interval.test_setops`, which could be used in other places as well. For instance in `test_difference_base` in `pandas/tests/indexes/common.py`.

There are probably many more places where it could be used.

"
572465507,32317,REF: test_first_valid_index,jbrockmendel,closed,2020-02-28T01:33:24Z,2020-02-28T15:47:56Z,"For this one I went ahead and combined the two files, split tests, and parametrized.  We end up covering a few more cases now than in the status quo (Series with index of strings, DataFrame with DTI)"
572200626,32297,TST: remove invalid internals tests,jbrockmendel,closed,2020-02-27T16:31:16Z,2020-02-28T16:16:32Z,Delay validation check in get_slice is also nice.
572408746,32314,REF/TST: misplaced MultiIndex tests,jbrockmendel,closed,2020-02-27T22:45:40Z,2020-02-28T16:23:08Z,AFAICT this gets all of the tests currently in test_multilevel that are only testing MultiIndex.
514824319,29295,Fix SS06 formatting errors ,profwacko,closed,2019-10-30T16:48:32Z,2020-02-28T19:57:10Z,"More SS06 (Summary should fit in a single line) errors fixed (#29254):

The following docstrings were examined and fixed:
```
pandas.DataFrame
pandas.DataFrame.transform
pandas.DataFrame.corrwith
pandas.DataFrame.describe
pandas.DataFrame.kurt
pandas.DataFrame.kurtosis
pandas.DataFrame.skew
pandas.DataFrame.align
pandas.DataFrame.drop_duplicates
pandas.DataFrame.duplicated

```

Validated with python scripts/validate_docstrings.py

Also referencing issue: #27977"
533577903,30091,BUG: preserve EA dtype in transpose,TomAugspurger,closed,2019-12-05T20:21:21Z,2020-02-28T23:53:51Z,
572161807,32295,Incomplete reference to `convert_dtypes` in `to_numeric` docstring,onietosi,closed,2020-02-27T15:31:27Z,2020-02-29T03:07:24Z,"In the `pandas.to_numeric` [documentation page](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html), in the **See also** section, `convert_dtypes` has no link attached to it.

I am not familiar with contributing to the docs but I believe this is because the docstring of `to_numeric` refers to `convert_dtypes` instead of `DataFrame.convert_dtypes`.

https://github.com/pandas-dev/pandas/blob/29d6b0232aab9576afa896ff5bab0b994760495a/pandas/core/tools/numeric.py#L67-L73"
571759000,32283,DOC: Getting started tutorials feedback from EOSS meeting,mroeschke,open,2020-02-27T01:18:58Z,2020-02-29T03:13:15Z,"@WillAyd and I got some good feedback regarding the tutorials @stijnvanhoey [created](https://pandas.io/docs/getting_started/intro_tutorials/index.html) while at the CZI EOSS Kickoff Meeting. 

1. Generally more white space between pictures and texts/headers
(example image above): https://pandas.io/docs/getting_started/intro_tutorials/01_table_oriented.html#each-column-in-a-dataframe-is-a-series

2. The blue boxes that contain `To user guide` should be replaced by a question like `Want more information?`
(example blue box above): https://pandas.io/docs/getting_started/intro_tutorials/03_subset_data.html#how-do-i-filter-specific-rows-from-a-dataframe

3. h1 headings appear too similar in size to an h2 heading when they follow each other
(example) https://pandas.io/docs/getting_started/intro_tutorials/06_calculate_statistics.html#how-to-calculate-summary-statistics

4. Some of the data topics seems to domain specific
(example) https://pandas.io/docs/getting_started/intro_tutorials/09_timeseries.html

5. The different topics within the page (sections that start with a h1 header) could be separated more clearly
(example in the section separation here): https://pandas.io/docs/getting_started/intro_tutorials/07_reshape_table_layout.html#pivot-table

6. The pandas cheatsheet should be linked in the `Getting started` section: https://pandas.io/docs/getting_started/index.html#getting-started

7. Mention `tidyverse` in `Comparisons with R/ R libraries`: https://pandas.io/docs/getting_started/comparison/comparison_with_r.html

8. The pivot table diagram was not terribly intuitive. Recommended to use values instead of colors
 (example) https://pandas.io/docs/getting_started/intro_tutorials/07_reshape_table_layout.html#pivot-table

"
565411370,31977,Avoid importing from pandas at _libs files,ShaharNaveh,closed,2020-02-14T15:54:20Z,2020-02-29T10:23:29Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Something that brought at to me at first [here](https://github.com/pandas-dev/pandas/pull/30395#discussion_r363101229)

---

cc @jbrockmendel "
572696158,32328,CLN: Removed unused variables defenition,ShaharNaveh,closed,2020-02-28T11:24:57Z,2020-02-29T10:24:23Z,I don't see ```found``` and ```count``` being used anywhere in this function.
572726320,32329,CLN: _libs.interval looping with cdef index,ShaharNaveh,closed,2020-02-28T12:28:47Z,2020-02-29T10:25:01Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
562020426,31808,CLN: some code cleanups in pandas/_libs/,ShaharNaveh,closed,2020-02-08T13:40:45Z,2020-02-29T10:27:10Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
569326179,32176,CLN: Some code cleanups,ShaharNaveh,closed,2020-02-22T12:10:07Z,2020-02-29T10:27:38Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
569328800,32177,CLN: some code cleanups,ShaharNaveh,closed,2020-02-22T12:34:09Z,2020-02-29T10:27:57Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
572240869,32299,CLN: Using sum instead of looping,ShaharNaveh,closed,2020-02-27T17:32:41Z,2020-02-29T10:28:28Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565731943,32011,CI: temporary fix to the CI,ShaharNaveh,closed,2020-02-15T10:56:36Z,2020-02-29T10:30:46Z,"ref #31992

---

This is just a temporary fix to the CI.

Until someone smarter than me fix it :)
"
561864472,31792,Some code cleanups,ShaharNaveh,closed,2020-02-07T20:50:04Z,2020-02-29T10:32:22Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
561836994,31790,TST: Making use of tm.external_error_raised,ShaharNaveh,closed,2020-02-07T19:47:34Z,2020-02-29T10:32:49Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

After #31130 got merged, I can do what I couldn't do [here](https://github.com/pandas-dev/pandas/pull/30998#discussion_r368177775)

"
572644379,32324,TST/CLN: Follow-up to #31867,simonjayhawkins,closed,2020-02-28T09:50:32Z,2020-02-29T13:31:31Z,"xref #31867
"
569145230,32160,Backport PR #32152 on branch 1.0.x (TST: add test for get_loc on tz-aware DatetimeIndex),simonjayhawkins,closed,2020-02-21T19:37:32Z,2020-02-29T13:34:06Z,"xref #32152

@jbrockmendel with tests moving around, is this OK for the backport. i.e. in the same place as on master."
569148797,32161,Backport PR #32155 on branch 1.0.x (TST: add test for DataFrame.reindex on nearest tz-aware DatetimeIndex),simonjayhawkins,closed,2020-02-21T19:45:00Z,2020-02-29T13:37:02Z,"xref #32155
"
571773849,32284,CI: nested DataFrames in npdev,jbrockmendel,closed,2020-02-27T02:10:08Z,2020-02-29T13:39:29Z,xref #32289
573297443,32366,Backport PR #32284 on branch 1.0.x (CI: nested DataFrames in npdev),meeseeksmachine,closed,2020-02-29T13:38:41Z,2020-02-29T14:42:26Z,Backport PR #32284: CI: nested DataFrames in npdev
573118021,32345,TYP: Update type naming in formatter,xcz011,closed,2020-02-29T02:33:58Z,2020-02-29T16:16:44Z,"- [x] part of #26792, #28480
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
573219152,32361,"Changed kind parameter from integer to int, Added example",zakybilfagih,closed,2020-02-29T06:52:17Z,2020-02-29T17:43:54Z,"- [x] closes [Fix PR06 error in pandas.arrays.SparseArray](https://github.com/pandanistas/pandanistas_sprint_ui2020/issues/1)
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

```
################################################################################
################################## Validation ##################################
################################################################################

3 Errors found:
        Parameter ""sparse_index"" has no description
        Parameter ""index"" has no description
        See Also section not found

```"
573307931,32370,PR #32068 follow up - Print merged df result in doc,ryankarlos,closed,2020-02-29T14:47:52Z,2020-02-29T19:10:05Z,"- [x] follow up to PR #32068 which closed #12550  - small fix - see https://github.com/pandas-dev/pandas/pull/32068#discussion_r386029773

"
572910827,32338,TST/REF: move tools test files,jbrockmendel,closed,2020-02-28T17:51:35Z,2020-02-29T20:13:52Z,"These files are in the index tests, but they're not really testing the index objects."
573414399,32378,DOC: Minor typo fixes for code style guide,dan1261,closed,2020-02-29T21:27:25Z,2020-02-29T21:34:29Z,
573415805,32379,DOC: Minor typo fixes for code style guide,dan1261,closed,2020-02-29T21:37:38Z,2020-03-01T02:24:51Z,
573547595,32384,`DataFrame.groupby.apply` keeps group-by column in values,MarcoGorelli,closed,2020-03-01T13:32:12Z,2020-03-01T14:58:47Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import pandas as pd
>>> df = pd.DataFrame({'d': [1.0, 1.0, 1.0, 2.0, 2.0, 2.0], 'v': [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]})
>>> df.groupby('d').sum()                                                                                   
        v
d        
1.0   6.0
2.0  15.0

>>> df.groupby('d').apply(sum)                                                                              
       d     v
d             
1.0  3.0   6.0
2.0  6.0  15.0
```
#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : e6ead5587fce2df844bbe568b63a150c47954fdd
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-88-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 0.26.0.dev0+2391.ge6ead5587.dirty
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.1.0.post20200119
Cython           : 0.29.14
pytest           : 5.3.5
hypothesis       : 5.5.1
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.7
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : 0.3.3
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.1
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : 0.8.6
xarray           : 0.15.0
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.7
numba            : 0.48.0

</details>
"
489797340,28297,fix Rolling for multi-index and reversed index.,leftys,closed,2019-09-05T14:26:07Z,2020-03-01T16:01:26Z,"Fix Rolling operation for level of multi-index and descending time index (that is monotonic, but decreasing).

- [x] closes #19248
- [x] closes #15584
- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
573397250,32374,Having Problems with detection of tables,ghost,closed,2020-02-29T19:35:28Z,2020-03-01T16:12:53Z,"#### Code Sample, a copy-pastable example if possible

```python
first_concurrent = requests.get(url, proxies=proxies, headers=headers)
first_concurrent_text = first_concurrent.text
first_concurrent_all_table = pd.read_html(first_concurrent_text)
first_concurrent_first_table = first_concurrent_all_table[0]
```
#### Problem description

The website has a table on it.  Sometiems when I run the code it finds the table other times it does not. This is not an issue with syntax of code, rather I think its about the read_html function. I went over the module, stackoverflow, etc, and did not find a solution to my problem. 

PS - I have the newest version of pandas. 
"
573142021,32348,Getting the wrong version of pandas when installing from requirements.txt,dstromberg,closed,2020-02-29T03:33:00Z,2020-03-01T17:42:02Z,"#### Code Sample, a copy-pastable example if possible

```
$ python3 -m pip install --user -r requirements.txt | grep -i pandas | grep -vi 'Installing collected packages'
below cmd output started 2020 Sat Feb 29 03:20:45 AM UTC
Collecting pandas==0.23.4 (from -r requirements.txt (line 43))
  Using cached https://files.pythonhosted.org/packages/e1/d8/feeb346d41f181e83fba45224ab14a8d8af019b48af742e047f3845d8cff/pandas-0.23.4-cp36-cp36m-manylinux1_x86_64.whl
above cmd output done    2020 Sat Feb 29 03:21:55 AM UTC
dstromberg@TransientTracking-2:~/src/grok/just-master x86_64-pc-linux-gnu 126478

$ python3 -m pip freeze | grep pandas
below cmd output started 2020 Sat Feb 29 03:22:46 AM UTC
pandas==1.0.1
above cmd output done    2020 Sat Feb 29 03:22:47 AM UTC
dstromberg@TransientTracking-2:~/src/grok/just-master x86_64-pc-linux-gnu 126478

$ grep pandas requirements.txt 
below cmd output started 2020 Sat Feb 29 03:22:59 AM UTC
#pandas
pandas==0.23.4
above cmd output done    2020 Sat Feb 29 03:22:59 AM UTC
dstromberg@TransientTracking-2:~/src/grok/just-master x86_64-pc-linux-gnu 126478

```

#### Problem description

I'm looking at a large codebase that needs an old version of pandas - but for some reason pip install isn't giving it to me.

If I delete pandas and install it manually with the following, I get the right version of pandas.  There's something about requirements.txt that isn't quite right.

```
$ python3 -m pip install pandas==0.23.4 | grep -i pandas | grep -v 'Installing collected packages'
below cmd output started 2020 Sat Feb 29 03:30:27 AM UTC
Collecting pandas==0.23.4
  Using cached https://files.pythonhosted.org/packages/e1/d8/feeb346d41f181e83fba45224ab14a8d8af019b48af742e047f3845d8cff/pandas-0.23.4-cp36-cp36m-manylinux1_x86_64.whl
above cmd output done    2020 Sat Feb 29 03:30:35 AM UTC
dstromberg@TransientTracking-2:~/src/grok/just-master x86_64-pc-linux-gnu 126478

$ python3 -m pip freeze | grep pandas
below cmd output started 2020 Sat Feb 29 03:30:47 AM UTC
pandas==0.23.4
above cmd output done    2020 Sat Feb 29 03:30:48 AM UTC
dstromberg@TransientTracking-2:~/src/grok/just-master x86_64-pc-linux-gnu 126478

```

Any suggestions?

Thanks for the cool software :)
"
572897602,32337,BUG: fixes unhandled NAType when plotting (#32073),jeandersonbc,closed,2020-02-28T17:28:14Z,2020-03-01T17:58:51Z,"- [x] closes #32073
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
573832411,32391,> Not sure this is even supported by gcsfs - I don't see `encoding` as an available option there:,EgorBEremeev,closed,2020-03-02T09:23:25Z,2020-03-02T09:25:27Z,"> Not sure this is even supported by gcsfs - I don't see `encoding` as an available option there:
> 
> https://github.com/dask/gcsfs/blob/523eb65b3e7feb05f9c10ce84523d1058716fecf/gcsfs/core.py#L1133
> 
> Might need to start upstream if wanted to make this possible

Hi, @WillAyd 
In the current gcsf master `GCSFileSystem.open()` has been removed and `fsspec.AbstractFileSystem.open()` has works instead:

https://github.com/intake/filesystem_spec/blob/4c66e096d32dafe264e2d6707992ee6935685944/fsspec/spec.py#L717

where applying of passed `encoding` for the text reading\writing is now implemented:

```
        if ""b"" not in mode:
            mode = mode.replace(""t"", """") + ""b""

            text_kwargs = {
                k: kwargs.pop(k)
                for k in [""encoding"", ""errors"", ""newline""]
                if k in kwargs
            }
            return io.TextIOWrapper(
                self.open(path, mode, block_size, **kwargs), **text_kwargs
            )
```
[Note also this issue from gcsfs](https://github.com/dask/gcsfs/issues/142)

So, it looks that in `pandas` ignoring of `encoding` parameter happens because in the `pandas.io.gcs.get_filepath_or_buffer`  the mode = 'rb' is passed  to call of `GCSFileSystem.open(filepath_or_buffer, mode)`

Tracing back to the moment of the first actual setting the `mode` parameter we have stop on this line:

`pandas.io.common.py`
```
def get_filepath_or_buffer(
    filepath_or_buffer, encoding=None, compression=None, mode=None
)
```

It is so, because in the call of `get_filepath_or_buffer()` performed from here 
https://github.com/pandas-dev/pandas/blob/29d6b0232aab9576afa896ff5bab0b994760495a/pandas/io/parsers.py#L430

we do not pass value of `mode` and default `mode=None` works.

As I could suggest for read_csv() we need pass `mode=r` and for to_csv() we need pass `mode=w` in the call of `get_filepath_or_buffer()`. But I'm not sure where it's better to implement this change.

_Originally posted by @EgorBEremeev in https://github.com/pandas-dev/pandas/issues/26124#issuecomment-593121783_"
572795693,32333,DOC: Fixed reference to `convert_dtypes` in `to_numeric` (#32295),jqmviegas,closed,2020-02-28T14:37:28Z,2020-03-02T10:20:40Z,"- closes #32295 
- just changed the reference from convert_dtypes to DataFrame.convert_dtypes
"
566571038,32068,DOC: Add example for multiindex series and dataframe merge,ryankarlos,closed,2020-02-18T00:05:32Z,2020-03-02T11:41:50Z,"- [x] closes #12550

"
565727402,32003,DOC: Fix SA04 errors from DataFrame.melt,giovanism,closed,2020-02-15T10:11:25Z,2020-03-02T12:52:34Z,"- [ ] closes https://github.com/pandanistas/pandanistas_sprint_jakarta2020/issues/10
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
523564223,29638,Type Error Bug Despite Using Multiple Versions of Pandas,DataDoctorNG,closed,2019-11-15T16:21:41Z,2020-03-02T13:55:36Z,"I have a data set of multiple work lists for my job and I am trying to sort values before dropping duplicates, but the code below give me this error:

import pandas as pd
Disputes = Disputes.sort_values(axis=1, by=['Invoice Number', 'Report Date'], inplace = True).drop_duplicates(subset=['Invoice Number', 'Report Date'], keep=['last']) 

TypeError: sort_values() got an unexpected keyword argument 'by'

I have tried to use multiple versions of Pandas to fix the issue as has been suggested by multiple posts of Stack Overflow and on Github. However, I still get the same error. 

Essentially I would like to use the sort values function without getting this error so that I can focus on the rest of my projects.

Here is the output to show my version of Pandas:

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.2
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 41.6.0.post20191030
Cython           : 0.29.12
pytest           : 5.0.1
hypothesis       : None
sphinx           : 2.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.1.8
lxml.etree       : 4.3.4
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.4
matplotlib       : 3.1.0
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.6.2
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : 1.3.5
tables           : 3.5.2
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.8"
514917011,29297,TypeError when setting MultiIndex in DataFrame,SteveDoyle2,closed,2019-10-30T19:11:07Z,2020-03-02T13:59:23Z,"#### Code Sample

```python
from pandas import read_pickle
df = read_pickle('test.txt')
print(df)
df.set_index(['ElementID', 'NodeID', 'Location'])  # fails
#df.set_index(['ElementID'])  # also fails
```

The pickle file (that I renamed to be a .txt file):
[test.txt](https://github.com/pandas-dev/pandas/files/3790733/test.txt)

#### Problem description

I'm trying to set a MultiIndex on a DataFrame.  I'm getting an error saying that the index (iblockno) is not an integer array, except it is.  Looking at the point in the code  Searching for the error message:

    block = self.blocks[iblockno]
    TypeError: only integer scalar arrays can be converted to a scalar index

 brought me to `pandas\core\internals\managers.py"", line ~977`.  I split out iblockno and iblockloc to see understand the error better.
 
```python
    def iget(self, i):
        iblockno = self._blknos[i]
        iblockloc = self._blklocs[i]
        block = self.blocks[iblockno]  # error is here


        values = block.iget(iblockloc)
```

where:

    iblockno
    >>> array([1], dtype=int64)

What I'm confused on is the data is an integer array, so shouldn't I not be getting an error?

#### Expected Output

The dataframe with a MultiIndex of ElementID, NodeID, and Location.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 0.25.1
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3
setuptools       : 41.4.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
</details>
"
573387805,32373,REF: collect+parametrize reorder_levels tests,jbrockmendel,closed,2020-02-29T18:58:41Z,2020-03-02T15:30:15Z,
567711289,32115,"add test for ""Allow definition of `pd.CategoricalDtype` with a specific `categories.dtype`""",rushabh-v,closed,2020-02-19T17:17:23Z,2020-03-02T15:30:34Z,"- [x] closes #32096 
- [x] tests added / passed
"
571685564,32278,CLN: Use defaultdict for minor optimization,jaketae,closed,2020-02-26T21:56:55Z,2020-03-02T15:34:15Z,"Edit `_from_nested_dict()` by using defaultdict for optimization in performance

- [ ] closes #32209 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
571552498,32273,TST: move misplaced to_datetime test,jbrockmendel,closed,2020-02-26T17:45:29Z,2020-03-02T17:06:55Z,
567500046,32096,Allow definition of `pd.CategoricalDtype` with a specific `categories.dtype`,lr4d,closed,2020-02-19T11:21:37Z,2020-03-02T17:14:45Z,"#### Code Sample, a copy-pastable example if possible

```python
from pandas.api.types import union_categoricals
import pandas as pd
import numpy as np

dt1 = pd.Categorical([], categories=[pd.Timestamp(""2020-01-01"")])
dt2 = pd.Categorical([], categories=[pd.Timestamp(""2020-01-02"")])
dt3 = pd.Categorical([])

pd.CategoricalDtype([]).categories.dtype == np.dtype(""O"")   # should probably be `None` or undefined

union_categoricals([dt1, dt2])
Out[113]: [], Categories (2, datetime64[ns]): [2020-01-01, 2020-01-02]
union_categoricals([dt1, dt3])
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-19-2496f54dfb94> in <module>
----> 1 union_categoricals([dt1, dt3])

~/miniconda3/envs/dask/lib/python3.6/site-packages/pandas/core/dtypes/concat.py in union_categoricals(to_union, sort_categories, ignore_order)
    306         for other in to_union[1:]
    307     ):
--> 308         raise TypeError(""dtype of categories must be the same"")
    309 
    310     ordered = False

TypeError: dtype of categories must be the same


```
#### Problem description

At the moment, it is not possible to define the data type of `categories` of a CategoricalDtype. This makes things difficult when trying to merge categorical data types e.g. before performing a concatenation of dataframes, especially when one dataframe has no defined categories.

Related: https://github.com/dask/dask/issues/5756

#### Expected Output
```python
# Add flag `dtype` to `pd.CategoricalDtype`
dt3 = pd.Categorical([], dtype=pd.CategoricalDtype(dtype=np.dtype(""<M8[ns]""))

union_categoricals([dt1, dt3])
Out[113]: [], Categories (1, datetime64[ns]): [2020-01-01]
```
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.5.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200209
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
573407606,32376,"TYP: annotations for internals, set_axis",jbrockmendel,closed,2020-02-29T20:40:04Z,2020-03-02T17:32:28Z,
573404203,32375,REF/TST: misplaced DataFrame.join tests,jbrockmendel,closed,2020-02-29T20:20:16Z,2020-03-02T17:52:13Z,
544432101,30613,TYP: Add TypeVars to NDFrame,topper-123,closed,2020-01-02T03:08:10Z,2020-03-02T22:35:16Z,"Adding TypeVars to NDFrame methods that don't return optional values. This ensures that e.g. ``(DataFrame|Series).astype`` and ``(DataFrame|Series).copy`` have a known return type, which is nice.

Also adds ``PandasObject._ensure_type``, which is a method used to solve the problem with optional return values, but where we actually know the return type. This is a helper method to help with the tediousness of optional return values experienced in #30565."
562499591,31847,BUG: pd.NA doesn't pickle/unpickle faithfully,tsoernes,closed,2020-02-10T11:29:03Z,2020-03-02T23:13:33Z,"#### Code Sample, a copy-pastable example if possible

```python

In [5]: df['Gold Categories'].count()
Out[5]: 135218

In [6]: df['Gold Categories'].isna().sum()
Out[6]: 0

In [7]: df['Gold Categories'].iloc[256]
Out[7]: <NA>

In [8]: pd.isna(df['Gold Categories'].iloc[256])
Out[8]: False

In [9]: type(df['Gold Categories'].iloc[256])
Out[9]: pandas._libs.missing.NAType

In [10]: pd.__version__
Out[10]: '1.0.1'


```

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.16-200.fc30.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : nb_NO.UTF-8
LOCALE           : nb_NO.UTF-8

pandas           : 1.0.1
numpy            : 1.17.3
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.6.0.post20191030
Cython           : 0.29.13
pytest           : 5.2.2
hypothesis       : None
sphinx           : 2.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.2
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.9.0
pandas_datareader: None
bs4              : 4.8.1
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 2.2.3
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 5.2.2
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.10
tables           : 3.5.2
tabulate         : 0.8.5
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.2
numba            : 0.46.0


</details>
"
569369122,32182,DOC: Fix SA04 errors in docstrings #28792,AdrianMastronardi,closed,2020-02-22T17:54:49Z,2020-03-03T01:22:10Z,"- [x] xref #28792 
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
574312528,32399,Backport PR #32104 on branch 1.0.x (BUG: Pickle NA objects),meeseeksmachine,closed,2020-03-02T23:21:19Z,2020-03-03T01:27:02Z,Backport PR #32104: BUG: Pickle NA objects
574210803,32398,CLN: Avoid unnecessary values_from_object,jbrockmendel,closed,2020-03-02T19:57:40Z,2020-03-03T01:51:08Z,xref #27165.
572487090,32319,CLN: remove _igetitem_cache,jbrockmendel,closed,2020-02-28T02:53:44Z,2020-03-03T01:52:52Z,Only used once (recent bugfixes got rid of the user uses IIRC) and its just an extra layer that isnt needed.
571018150,32258,TST: broken off from #32187,jbrockmendel,closed,2020-02-26T02:13:12Z,2020-03-03T01:59:38Z,troubleshooting an apparently-unrelated Travis failure there
573066137,32341,CLN: setitem_with_indexer cleanups,jbrockmendel,closed,2020-02-28T23:37:52Z,2020-03-03T02:07:33Z,"working on fixing some significant bugs in setitem_with_indexer, this breaks off some easier cleanups"
571695601,32280,REF: simplify PeriodIndex._shallow_copy,jbrockmendel,closed,2020-02-26T22:18:02Z,2020-03-03T02:08:07Z,Adds a test for incorrect behavior in DTI.where and TDI.where that is fixed in master.
572402081,32313,TST: Using  more fixtures in of tests/base/test_ops.py,SaturnFromTitan,closed,2020-02-27T22:29:52Z,2020-03-03T02:48:43Z,"part of #23877
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
574312557,32400,CLN: remove unused values from interpolate call,jbrockmendel,closed,2020-03-02T23:21:24Z,2020-03-03T03:01:35Z,
556353957,31396,TypeError when using 'comment=...' in read_csv from a file,cddf,closed,2020-01-28T17:18:33Z,2020-03-03T03:02:23Z,"#### Code Sample

Given a data file `data.csv` with a line that is commented out:
```csv
+1.280000e+002,-4.078996e+001
+2.560000e+002,-5.155923e+001
# +3.840000e+002,-7.221378e+001
+5.120000e+002,-7.918677e+001
+6.400000e+002,-7.919656e+001
```
```python
import pandas as pd
pd.read_csv('data.csv', sep=None, index_col=0, header=None, engine=""python"", comment='#')
```
#### Problem description

It raises a `TypeError` when using the `comment` parameter:

<details>
<summary>TypeError</summary>

```python
TypeError                                 Traceback (most recent call last)
<ipython-input-17-c89b3c3e691f> in <module>
----> 1 pd.read_csv('data.csv', sep=None, comment='#')

~/.local/share/virtualenvs/openqlab/lib/python3.8/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names,
 index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfoote
r, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirs
t, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encodin
g, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)
    683         )
    684 
--> 685         return _read(filepath_or_buffer, kwds)
    686 
    687     parser_f.__name__ = name

~/.local/share/virtualenvs/openqlab/lib/python3.8/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds)
    455 
    456     # Create the parser.
--> 457     parser = TextFileReader(fp_or_buf, **kwds)
    458 
    459     if chunksize or iterator:

~/.local/share/virtualenvs/openqlab/lib/python3.8/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds)
    893             self.options[""has_index_names""] = kwds[""has_index_names""]
    894 
--> 895         self._make_engine(self.engine)
    896 
    897     def close(self):

~/.local/share/virtualenvs/openqlab/lib/python3.8/site-packages/pandas/io/parsers.py in _make_engine(self, engine)
   1145                     ' ""python-fwf"")'.format(engine=engine)
   1146                 )
-> 1147             self._engine = klass(self.f, **self.options)
   1148 
   1149     def _failover_to_python(self):

~/.local/share/virtualenvs/openqlab/lib/python3.8/site-packages/pandas/io/parsers.py in __init__(self, f, **kwds)
   2297         # Set self.data to something that can read lines.
   2298         if hasattr(f, ""readline""):
-> 2299             self._make_reader(f)
   2300         else:
   2301             self.data = f

~/.local/share/virtualenvs/openqlab/lib/python3.8/site-packages/pandas/io/parsers.py in _make_reader(self, f)
   2427                 self.pos += 1
   2428                 self.line_pos += 1
-> 2429                 sniffed = csv.Sniffer().sniff(line)
   2430                 dia.delimiter = sniffed.delimiter
   2431                 if self.encoding is not None:

/usr/lib64/python3.8/csv.py in sniff(self, sample, delimiters)
    179 
    180         quotechar, doublequote, delimiter, skipinitialspace = \
--> 181                    self._guess_quote_and_delimiter(sample, delimiters)
    182         if not delimiter:
    183             delimiter, skipinitialspace = self._guess_delimiter(sample,

/usr/lib64/python3.8/csv.py in _guess_quote_and_delimiter(self, data, delimiters)
    220                       r'(?:^|\n)(?P<quote>[""\']).*?(?P=quote)(?:$|\n)'):                            #  "".*?"" (no delim, no space)
    221             regexp = re.compile(restr, re.DOTALL | re.MULTILINE)
--> 222             matches = regexp.findall(data)
    223             if matches:
    224                 break

TypeError: expected string or bytes-like object
```

</details>

Without the comment in the data file and without the parameter `comment='#'` everything works as expected.

**It seems that `sep=None` ist the problem here.**
When using `sep=','` it works. But in our case, the import is part of a general importer that should accept a variety of different files. Thus, we must use `sep=None`. 

#### Expected Output

I would expect the following output:

```python
Out[18]: 
                 1
0                 
128.0    -40.78996
256.0    -51.55923
512.0    -79.18677
640.0    -79.19656

[4 rows x 1 columns]
```

#### Output of ``pd.show_versions()``

<details>
<summary>Details</summary>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.13-201.fc31.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : de_DE.UTF-8
LOCALE           : de_DE.UTF-8

pandas           : 0.25.3
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 42.0.2
Cython           : None
pytest           : 5.3.2
hypothesis       : None
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
560008286,31667,BUG: fixes bug when using sep=None and comment keyword for read_csv,s-scherrer,closed,2020-02-04T22:07:59Z,2020-03-03T03:02:28Z,"- [x] closes #31396 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry - added
"
279124459,18634,Series.replace() should raise an exception if invalid argument is given,upkarlidder,closed,2017-12-04T19:46:47Z,2020-03-03T03:24:23Z,"Series.replace() should potentially raise an exception if an invalid argument is given? I am new to pandas and this is more of a question. **This is a user error on my part, but possible improvement to replace.**

```python
df = pd.DataFrame(
{
    'one': ['1','1 ','10'],
    'two': ['1 ', '20 ', '30 ']
})

# creates
     one  two
0    1    1
1    1    20
2    10    30

# I intentionally added a space in df.one. So df.one.value_counts() results in
1     1
10    1
1     1

# I want to strip the spaces around all values in df.one 
# so that value_counts() only has 1 and 10 with values 2 and 1 respectively. 
# The following does not work.

df.one.replace(lambda x: x.strip(), inplace=True)

```
#### Problem description

Series.replace() accepts `""str, regex, list, dict, Series, numeric, or None""`. So I understand why the above does not work. But as @toobaz pointed out on gitter, when a lambda is passed, replace should probably raise an error?

#### Expected Output
Raises TypeError

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.1.final.0
python-bits: 64
OS: Darwin
OS-release: 16.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.20.3
pytest: None
pip: 9.0.1
setuptools: 36.3.0
Cython: None
numpy: 1.13.1
scipy: 0.19.1
xarray: None
IPython: 6.1.0
sphinx: 1.6.3
patsy: None
dateutil: 2.6.1
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: 2.6.2
feather: None
matplotlib: 2.0.2
openpyxl: 2.4.8
xlrd: 1.0.0
xlwt: None
xlsxwriter: None
lxml: 3.8.0
bs4: None
html5lib: 0.999999999
sqlalchemy: 1.1.14
pymysql: 0.7.11.None
psycopg2: None
jinja2: 2.8.1
s3fs: None
pandas_gbq: None
pandas_datareader: None
</details>
"
571624794,32276,Categorical NaN behaviour different from a str,sjvdm,closed,2020-02-26T20:03:33Z,2020-03-03T03:28:43Z,"#### Code Sample

### Series as category
df = pd.Series(['a','a','b','c']).astype('category')
print(df.shift(1))
print(df)
print(df.shift(1) != df)

OUTPUT:

0    NaN
1      a
2      a
3      b
dtype: category
Categories (3, object): [a, b, c]
0    a
1    a
2    b
3    c
dtype: category
Categories (3, object): [a, b, c]
0    False
1    False
2     True
3     True
dtype: bool

### Series as str
df = pd.Series(['a','a','b','c']).astype('str')
print(df.shift(1))
print(df)
print(df.shift(1) != df)

OUTPUT:

0    NaN
1      a
2      a
3      b
dtype: object
0    a
1    a
2    b
3    c
dtype: object
0     True
1    False
2     True
3     True
dtype: bool


```
#### Problem description

The behaviour of NaN in comparison operators is different for type category and str. See example code - the first element is NaN in both instances, but the second instance equates to false, and the first equates to true for a != operation. For a == operation for a category, the behavior is as expected.

#### Expected Output

I would expect both to have the same output.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.0.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1062.12.1.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 41.4.0
Cython           : 0.29.15
pytest           : 5.3.5
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
574370862,32404,Add missing newline,ghost,closed,2020-03-03T02:19:04Z,2020-03-03T03:51:41Z,"The line "">>> idx.isin([1, 4]) array([ True, False, False])"" is was not rendered properly, because it was missing a newline above it.

https://pandas.pydata.org/docs/reference/api/pandas.Index.isin.html

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
562112648,31815,check parser_dates names in columns,sathyz,closed,2020-02-09T02:54:54Z,2020-03-03T04:04:54Z,"if column names are passed in parser_dates, make sure those
columns exist in dataframe.

- [x] closes #31251 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
564463929,31946,API: replace() should raise an exception if invalid argument is given,a-y-khan,closed,2020-02-13T06:13:46Z,2020-03-03T04:32:14Z,"- [x] closes #18634
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I used `is_scalar()` instead of `is_numeric()` to allow data structures like pd.Timestamp to be used in `replace()`. See tests in pandas/tests/frame/methods/test_replace.py, pandas/tests/series/methods/test_replace.py for examples."
573589376,32387,Plotting Int64 columns with nulled integers (NAType) fails #32073,jeandersonbc,closed,2020-03-01T18:08:35Z,2020-03-03T10:48:21Z,"- [x] closes #32073
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565935031,32042,CLN: clean-up show_versions and consistently use null for json output,simonjayhawkins,closed,2020-02-16T16:56:07Z,2020-03-03T11:43:14Z,"note changes to LC_ALL and LOCALE, null is already used for dependencies on master, see blosc and feather.

master
```
{
  ""system"": {
    ""commit"": ""a7ecced88a42c426bf61016c0131cab023c0cdff"",
    ""python"": ""3.7.5.final.0"",
    ""python-bits"": 64,
    ""OS"": ""Windows"",
    ""OS-release"": ""10"",
    ""machine"": ""AMD64"",
    ""processor"": ""Intel64 Family 6 Model 58 Stepping 9, GenuineIntel"",
    ""byteorder"": ""little"",
    ""LC_ALL"": ""None"",
    ""LANG"": ""en_GB.UTF-8"",
    ""LOCALE"": ""None.None""
  },
  ""dependencies"": {
    ""pandas"": ""1.1.0.dev0+497.ga7ecced88"",
    ""numpy"": ""1.17.2"",
    ""pytz"": ""2019.3"",
    ""dateutil"": ""2.8.0"",
    ""pip"": ""19.3.1"",
    ""setuptools"": ""41.6.0.post20191030"",
    ""Cython"": ""0.29.13"",
    ""pytest"": ""5.2.2"",
    ""hypothesis"": ""4.36.2"",
    ""sphinx"": ""2.2.1"",
    ""blosc"": null,
    ""feather"": null,
...
```

this pr
```
{
  ""system"": {
    ""commit"": ""2ac9d30f302e59de035cbea89188d5421212b000"",
    ""python"": ""3.7.5.final.0"",
    ""python-bits"": 64,
    ""OS"": ""Windows"",
    ""OS-release"": ""10"",
    ""Version"": ""10.0.18362"",
    ""machine"": ""AMD64"",
    ""processor"": ""Intel64 Family 6 Model 58 Stepping 9, GenuineIntel"",
    ""byteorder"": ""little"",
    ""LC_ALL"": null,
    ""LANG"": ""en_GB.UTF-8"",
    ""LOCALE"": {
      ""language-code"": null,
      ""encoding"": null
    }
  },
  ""dependencies"": {
    ""pandas"": ""1.1.0.dev0+500.g2ac9d30f3"",
    ""numpy"": ""1.17.2"",
    ""pytz"": ""2019.3"",
    ""dateutil"": ""2.8.0"",
    ""pip"": ""19.3.1"",
    ""setuptools"": ""41.6.0.post20191030"",
    ""Cython"": ""0.29.13"",
    ""pytest"": ""5.2.2"",
    ""hypothesis"": ""4.36.2"",
    ""sphinx"": ""2.2.1"",
    ""blosc"": null,
...
```

console output is unchanged
```
INSTALLED VERSIONS
------------------
commit           : 2ac9d30f302e59de035cbea89188d5421212b000
python           : 3.7.5.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 58 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : None.None

pandas           : 1.1.0.dev0+500.g2ac9d30f3
numpy            : 1.17.2
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 41.6.0.post20191030
Cython           : 0.29.13
pytest           : 5.2.2
hypothesis       : 4.36.2
sphinx           : 2.2.1
blosc            : None
feather          : None
...
```"
567581195,32104,BUG: Pickle NA objects,TomAugspurger,closed,2020-02-19T13:46:22Z,2020-03-03T12:44:06Z,"According to
https://docs.python.org/3/library/pickle.html#object.__reduce__,

> If a string is returned, the string should be interpreted as the name
> of a global variable. It should be the object’s local name relative to
> its module; the pickle module searches the module namespace to determine
> the object’s module. This behaviour is typically useful for singletons.

Closes https://github.com/pandas-dev/pandas/issues/31847"
573413993,32377,ENH: infer freq in timedelta_range,jbrockmendel,closed,2020-02-29T21:24:08Z,2020-03-03T12:57:33Z,Step in the direction of #31195.
574120607,32397,TST: add message check to pytest.raises (tests/arrays/test_boolean.py),ShilpaSugan,closed,2020-03-02T17:13:06Z,2020-03-03T12:58:21Z,"- [x] ref #30999 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] Added in an error message
"
569351107,32180,DOC: Fix SA04 errors in docstrings xref #28792,AdrianMastronardi,closed,2020-02-22T15:40:10Z,2020-03-03T14:22:43Z,"- [x] xref #28792 
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
573194430,32356,"DOC: Fixed ES01, PR07, SA04 error in pandas.core.groupby.DataFrameGroupBy.shift",Iqrar99,closed,2020-02-29T05:48:55Z,2020-03-03T14:26:56Z,"- [x] closes: https://github.com/pandanistas/pandanistas_sprint_ui2020/issues/14
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Output of python scripts/validate_docstrings.py pandas.core.groupby.DataFrameGroupBy.shift:
```
################################################################################
################################## Validation ##################################
################################################################################

1 Errors found:
        No examples section found
```"
574320374,32403,TYP: internals,jbrockmendel,closed,2020-03-02T23:43:42Z,2020-03-03T15:43:27Z,"some of these are a real bear, in particular `SingleBlockManager.__init__`"
566597343,32069,TST: corrwith and tshift in groupby/groupby.transform,ryankarlos,closed,2020-02-18T01:54:52Z,2020-03-03T15:45:25Z,"This PR tests `corrwith` and `tshift`, so not sure if it fully closes the original issue or not, but the other functions`backfill`, `pad` and `cumcount` don't seem to be supported in `groupby.transform` as reported in issues  #27472 and #31269.

- [x] closes #27905 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
564923549,31962,REF: remove _convert_scalar_indexer,jbrockmendel,closed,2020-02-13T20:11:32Z,2020-03-03T15:53:13Z,"This sits on top of #31867, so is partially a demonstration of how much complication is caused by our inconsistent error-raising."
573154097,32350,Implement BlockManager.iset,jbrockmendel,closed,2020-02-29T04:04:58Z,2020-03-03T15:55:54Z,"Preliminary to the PR that fixes `setitem_with_indexer` (#22036, #15686)"
574388758,32405,CLN: remove unreachable branch in Index._union,jbrockmendel,closed,2020-03-03T03:20:40Z,2020-03-03T16:12:34Z,
573139400,32347,"REF: avoid using internals methods for to_timestamp, to_period",jbrockmendel,closed,2020-02-29T03:26:26Z,2020-03-04T01:04:39Z,
573174793,32351,DOC: Fix SS06 formatting errors in merge_asof docstrings,Nael-Nathanael,closed,2020-02-29T04:58:31Z,2020-03-04T04:54:52Z,"Errors fixed (from the list in #29254 ):
pandas.merge_asof

Validated the fixes with python scripts/validate_docstrings.py
Refrencing issue: #29254

Fix : pandanistas/pandanistas_sprint_ui2020#11"
573257037,32363,TYP/cln: generic._make_*_function,topper-123,closed,2020-02-29T08:57:49Z,2020-03-04T08:53:31Z,"Give calls to these funcs named parameters for better clarity + type up the make_stats functions.
"
573196724,32358,DOC: Fixed PR09 error in pandas.testing.assert_series_equal,tolhassianipar,closed,2020-02-29T05:54:48Z,2020-03-04T08:55:38Z,"- [X] closes https://github.com/pandanistas/pandanistas_sprint_ui2020/issues/15
- [ ] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Output of `python scripts/validate_docstrings.py pandas.HDFStore.put`:
```
################################################################################
####################### Docstring (pandas.HDFStore.put)  #######################
################################################################################

Store object in HDFStore.

Parameters
----------
key : str
value : {Series, DataFrame}
format : 'fixed(f)|table(t)', default is 'fixed'
    fixed(f) : Fixed format
               Fast writing/reading. Not-appendable, nor searchable.
    table(t) : Table format
               Write as a PyTables Table structure which may perform
               worse but allow more flexible operations like searching
               / selecting subsets of the data.
append   : bool, default False
    This will force Table format, append the input data to the
    existing.
data_columns : list, default None
    List of columns to create as data columns, or True to
    use all columns. See `here
    <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#query-via-data-columns>`__.
encoding : str, default None
    Provide an encoding for strings.
dropna   : bool, default False, do not write an ALL nan row to
    The store settable by the option 'io.hdf.dropna_table'.

################################################################################
################################## Validation ##################################
################################################################################

8 Errors found:
        No extended summary found
        Parameters {'min_itemsize', 'index', 'complevel', 'errors', 'complib', 'append', 'nan_rep'} not documented
        Unknown parameters {'dropna  ', 'append  '}
        Parameter ""key"" has no description
        Parameter ""value"" has no description
        Parameter ""format"" description should start with a capital letter
        See Also section not found
        No examples section found

(pandas-dev) D:\Keluarga\Tolhas\Kuliah\Semester 4\DSC Pandas\pandas-tolhassianipar>python scripts/validate_docstrings.py pandas.HDFStore.put

################################################################################
####################### Docstring (pandas.HDFStore.put)  #######################
################################################################################

Store object in HDFStore.

Parameters
----------
key : str
value : {Series, DataFrame}
format : 'fixed(f)|table(t)', default is 'fixed'
    fixed(f) : Fixed format
               Fast writing/reading. Not-appendable, nor searchable.
    table(t) : Table format
               Write as a PyTables Table structure which may perform
               worse but allow more flexible operations like searching
               / selecting subsets of the data.
append   : bool, default False
    This will force Table format, append the input data to the
    existing.
data_columns : list, default None
    List of columns to create as data columns, or True to
    use all columns. See `here
    <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#query-via-data-columns>`__.
encoding : str, default None
    Provide an encoding for strings.
dropna   : bool, default False, do not write an ALL nan row to
    The store settable by the option 'io.hdf.dropna_table'.

################################################################################
################################## Validation ##################################
################################################################################

8 Errors found:
        No extended summary found
        Parameters {'errors', 'index', 'nan_rep', 'append', 'min_itemsize', 'complevel', 'complib'} not documented
        Unknown parameters {'append  ', 'dropna  '}
        Parameter ""key"" has no description
        Parameter ""value"" has no description
        Parameter ""format"" description should start with a capital letter
        See Also section not found
        No examples section found

```"
519577916,29473,AreaPlot:  add support for step drawstyle,nick-schultz,closed,2019-11-07T23:48:26Z,2020-03-04T11:06:41Z,"- [ ] closes #29451
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
573497535,32382,problem,ghost,closed,2020-03-01T07:05:32Z,2020-03-04T12:53:05Z,"import pandas as pd
json_file = 'sample_data'
list(pd.read_json(json_file, lines=True))
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-11-ee639448d145> in <module>
      1 import pandas as pd
      2 json_file = 'sample_data'
----> 3 list(pd.read_json(json_file, lines=True))

c:\users\nimal\appdata\local\programs\python\python36\lib\site-packages\pandas\util\_decorators.py in wrapper(*args, **kwargs)
    212                 else:
    213                     kwargs[new_arg_name] = new_arg_value
--> 214             return func(*args, **kwargs)
    215 
    216         return cast(F, wrapper)

c:\users\nimal\appdata\local\programs\python\python36\lib\site-packages\pandas\io\json\_json.py in read_json(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)
    606         return json_reader
    607 
--> 608     result = json_reader.read()
    609     if should_close:
    610         filepath_or_buffer.close()

c:\users\nimal\appdata\local\programs\python\python36\lib\site-packages\pandas\io\json\_json.py in read(self)
    727         elif self.lines:
    728             data = ensure_str(self.data)
--> 729             obj = self._get_object_parser(self._combine_lines(data.split(""\n"")))
    730         else:
    731             obj = self._get_object_parser(self.data)

c:\users\nimal\appdata\local\programs\python\python36\lib\site-packages\pandas\io\json\_json.py in _get_object_parser(self, json)
    751         obj = None
    752         if typ == ""frame"":
--> 753             obj = FrameParser(json, **kwargs).parse()
    754 
    755         if typ == ""series"" or obj is None:

c:\users\nimal\appdata\local\programs\python\python36\lib\site-packages\pandas\io\json\_json.py in parse(self)
    855 
    856         else:
--> 857             self._parse_no_numpy()
    858 
    859         if self.obj is None:

c:\users\nimal\appdata\local\programs\python\python36\lib\site-packages\pandas\io\json\_json.py in _parse_no_numpy(self)
   1087         if orient == ""columns"":
   1088             self.obj = DataFrame(
-> 1089                 loads(json, precise_float=self.precise_float), dtype=None
   1090             )
   1091         elif orient == ""split"":

and my file looks like
{""score_hidden"":false,""name"":""t1_cnas8zv"",""link_id"":""t3_2qyr1a"",""body"":""Most of us have some family members like this. *Most* of my family is like this. "",""downs"":0,""created_utc"":""1420070400"",""score"":14,""author"":""YoungModern"",""distinguished"":null,""id"":""cnas8zv"",""archived"":false,""parent_id"":""t3_2qyr1a"",""subreddit"":""exmormon"",""author_flair_css_class"":null,""author_flair_text"":null,""gilded"":0,""retrieved_on"":1425124282,""ups"":14,""controversiality"":0,""subreddit_id"":""t5_2r0gj"",""edited"":false}
{""distinguished"":null,""id"":""cnas8zw"",""archived"":false,""author"":""RedCoatsForever"",""score"":3,""created_utc"":""1420070400"",""downs"":0,""body"":""But Mill's career was way better. Bentham is like, the Joseph Smith to Mill's Brigham Young."",""link_id"":""t3_2qv6c6"",""name"":""t1_cnas8zw"",""score_hidden"":false,""controversiality"":0,""subreddit_id"":""t5_2s4gt"",""edited"":false,""retrieved_on"":1425124282,""ups"":3,""author_flair_css_class"":""on"",""gilded"":0,""author_flair_text"":""Ontario"",""subreddit"":""CanadaPolitics"",""parent_id"":""t1_cnas2b6""}
{""score_hidden"":false,""link_id"":""t3_2qxefp"",""name"":""t1_cnas8zx"",""created_utc"":""1420070400"",""downs"":0,""body"":""Mine uses a strait razor, and as much as i love the clippers i love the razor so much more. Then he follows it up with a warm towel. \nI think i might go get a hair cut this week."",""distinguished"":null,""id"":""cnas8zx"",""archived"":false,""author"":""vhisic"",""score"":1,""subreddit"":""AdviceAnimals"",""parent_id"":""t3_2qxefp"",""retrieved_on"":1425124282,""ups"":1,""author_flair_css_class"":null,""author_flair_text"":null,""gilded"":0,""controversiality"":0,""subreddit_id"":""t5_2s7tt"",""edited"":false}
{""parent_id"":""t3_2qys4x"",""subreddit"":""AdviceAnimals"",""gilded"":0,""author_flair_text"":null,""author_flair_css_class"":null,""retrieved_on"":1425124282,""ups"":1,""subreddit_id"":""t5_2s7tt"",""edited"":false,""controversiality"":0,""score_hidden"":false,""name"":""t1_cnas8zy"",""link_id"":""t3_2qys4x"",""body"":""[deleted]"",""created_utc"":""1420070400"",""downs"":0,""score"":1,""author"":""[deleted]"",""archived"":false,""distinguished"":null,""id"":""cnas8zy""}
{""body"":""Very fast, thank you!"",""created_utc"":""1420070400"",""downs"":0,""author"":""Mastersimpson"",""score"":2,""id"":""cnas8zz"",""distinguished"":null,""archived"":false,""score_hidden"":false,""name"":""t1_cnas8zz"",""link_id"":""t3_2qm5bi"",""author_flair_css_class"":null,""gilded"":0,""author_flair_text"":null,""retrieved_on"":1425124282,""ups"":2,""controversiality"":0,""edited"":false,""subreddit_id"":""t5_2y51u"",""parent_id"":""t3_2qm5bi"",""subreddit"":""freedonuts""}
{""author"":""BigGupp1"",""score"":6,""archived"":false,""id"":""cnas900"",""distinguished"":null,""body"":""The guy is a professional, and very good at what he does. I highly doubt he misses often, if at all."",""created_utc"":""1420070400"",""downs"":0,""name"":""t1_cnas900"",""link_id"":""t3_2qxe1p"",""score_hidden"":false,""edited"":false,""subreddit_id"":""t5_2qh61"",""controversiality"":0,""author_flair_text"":null,""gilded"":0,""author_flair_css_class"":null,""retrieved_on"":1425124282,""ups"":6,""parent_id"":""t1_cnaqg2t"",""subreddit"":""WTF""}
{""score_hidden"":false,""link_id"":""t3_2qxlvm"",""name"":""t1_cnas901"",""created_utc"":""1420070400"",""downs"":0,""body"":""This is a great question, and I want to thank you for asking it. However, I don't have any answers. I'm interested in learning more myself. "",""archived"":false,""id"":""cnas901"",""distinguished"":null,""score"":1,""author"":""PeglegGecko"",""subreddit"":""needadvice"",""parent_id"":""t3_2qxlvm"",""ups"":1,""retrieved_on"":1425124282,""author_flair_text"":null,""gilded"":0,""author_flair_css_class"":null,""subreddit_id"":""t5_2r367"",""edited"":false,""controversiality"":0}
{""controversiality"":0,""edited"":false,""subreddit_id"":""t5_2t9x3"",""ups"":1,""retrieved_on"":1425124282,""author_flair_css_class"":null,""author_flair_text"":null,""gilded"":0,""subreddit"":""summonerschool"",""parent_id"":""t1_cnaiy0s"",""id"":""cnas902"",""distinguished"":null,""archived"":false,""author"":""politevelociraptor"",""score"":1,""created_utc"":""1420070400"",""downs"":0,""body"":""Is the IE-Shiv-Ghostblade-Zerks-LW-BT Still not the main graves build? 60% crit I think massive Attack speed and decent AD plus Pen?"",""link_id"":""t3_2qx3kt"",""name"":""t1_cnas902"",""score_hidden"":false}
{""author_flair_css_class"":""BartkowskiMatt"",""gilded"":0,""author_flair_text"":""LA Kings fan"",""ups"":1,""retrieved_on"":1425124282,""controversiality"":0,""edited"":false,""subreddit_id"":""t5_2rmt9"",""parent_id"":""t1_cnas8bm"",""subreddit"":""BostonBruins"",""body"":"":D."",""created_utc"":""1420070400"",""downs"":0,""score"":1,""author"":""slicked9778"",""distinguished"":null,""id"":""cnas903"",""archived"":false,""score_hidden"":false,""name"":""t1_cnas903"",""link_id"":""t3_2qyizj""}
{""parent_id"":""t1_cnan0ne"",""subreddit"":""sausagetalk"",""author_flair_css_class"":null,""author_flair_text"":null,""gilded"":0,""ups"":2,""retrieved_on"":1425124282,""controversiality"":0,""subreddit_id"":""t5_2t13q"",""edited"":false,""score_hidden"":false,""name"":""t1_cnas905"",""link_id"":""t3_2qxiao"",""body"":""I don't know how to describe it.  Gently pinched two spots weiner length apart and just twisted them about 3or 4 times."",""downs"":0,""created_utc"":""1420070400"",""score"":2,""author"":""jaggazz"",""id"":""cnas905"",""distinguished"":null,""archived"":false}
{""parent_id"":""t1_cnas7u7"",""subreddit"":""hiphopheads"",""controversiality"":0,""subreddit_id"":""t5_2rh4c"",""edited"":false,""author_flair_css_class"":""django"",""author_flair_text"":"""",""gilded"":0,""retrieved_on"":1425124282,""ups"":2,""name"":""t1_cnas906"",""link_id"":""t3_2qyl4w"",""score_hidden"":false,""score"":2,""author"":""thebasedyeezus"",""id"":""cnas906"",""distinguished"":null,""archived"":false,""body"":""says you my g"",""downs"":0,""created_utc"":""1420070400""}
{""subreddit_id"":""t5_2qh33"",""edited"":false,""controversiality"":0,""ups"":10,""retrieved_on"":1425124282,""author_flair_text"":null,""gilded"":0,""author_flair_css_class"":null,""subreddit"":""funny"",""parent_id"":""t1_cnar76e"",""archived"":false,""id"":""cnas907"",""distinguished"":null,""author"":""Meltingteeth"",""score"":10,""downs"":0,""created_utc"":""1420070401"",""body"":""/r/Im14andthisisfunny"",""link_id"":""t3_2qxubc"",""name"":""t1_cnas907"",""score_hidden"":false}
{""score"":1,""author"":""Clomez"",""archived"":false,""id"":""cnas908"",""distinguished"":null,""body"":""i love this music!"",""created_utc"":""1420070401"",""downs"":0,""name"":""t1_cnas908"",""link_id"":""t3_2qx5b6"",""score_hidden"":false,""edited"":false,""subreddit_id"":""t5_2sokd"",""controversiality"":0,""gilded"":0,""author_flair_text"":null,""author_flair_css_class"":null,""ups"":1,""retrieved_on"":1425124282,""parent_id"":""t1_cnap6jp"",""subreddit"":""explainlikeimfive""}
{""created_utc"":""1420070401"",""downs"":0,""body"":""You mean the village hidden in filler complaints, right?"",""archived"":false,""distinguished"":null,""id"":""cnas909"",""score"":2,""author"":""noitnemid"",""score_hidden"":false,""link_id"":""t3_2qup35"",""name"":""t1_cnas909"",""retrieved_on"":1425124282,""ups"":2,""gilded"":0,""author_flair_text"":"""",""author_flair_css_class"":""deihand"",""subreddit_id"":""t5_2quts"",""edited"":false,""controversiality"":0,""subreddit"":""Naruto"",""parent_id"":""t1_cnald9n""}
{""score_hidden"":false,""link_id"":""t3_2qyp3l"",""name"":""t1_cnas90a"",""downs"":0,""created_utc"":""1420070401"",""body"":""I always forget how to bold on mobile. "",""archived"":false,""id"":""cnas90a"",""distinguished"":null,""author"":""highvoltorb"",""score"":2,""subreddit"":""hockey"",""parent_id"":""t1_cnas1fe"",""ups"":2,""retrieved_on"":1425124282,""author_flair_text"":""CBJAlt1NHL"",""gilded"":0,""author_flair_css_class"":""CBJAlt1NHL"",""edited"":false,""subreddit_id"":""t5_2qiel"",""controversiality"":0}
{""controversiality"":0,""edited"":false,""subreddit_id"":""t5_32vyu"",""ups"":1,""retrieved_on"":1425124282,""author_flair_css_class"":null,""author_flair_text"":null,""gilded"":0,""subreddit"":""GCXRep"",""parent_id"":""t1_cnajnze"",""id"":""cnas90b"",""distinguished"":null,""archived"":false,""score"":1,""author"":""[deleted]"",""downs"":0,""created_utc"":""1420070401"",""body"":""[deleted]"",""link_id"":""t3_2qxljk"",""name"":""t1_cnas90b"",""score_hidden"":false}
{""subreddit"":""Games"",""parent_id"":""t1_cnas5ud"",""retrieved_on"":1425124282,""ups"":1,""author_flair_css_class"":null,""gilded"":0,""author_flair_text"":null,""controversiality"":0,""subreddit_id"":""t5_2qhwp"",""edited"":false,""score_hidden"":false,""link_id"":""t3_2qxl90"",""name"":""t1_cnas90c"",""downs"":0,""created_utc"":""1420070401"",""body"":""If you enjoy deep, 100 hour RPGs, then definitely worth the money."",""distinguished"":null,""id"":""cnas90c"",""archived"":false,""score"":1,""author"":""Crodface""}
{""score_hidden"":false,""name"":""t1_cnas90d"",""link_id"":""t3_2qy6z1"",""body"":""[deleted]"",""downs"":0,""created_utc"":""1420070401"",""author"":""[deleted]"",""score"":1,""archived"":false,""distinguished"":null,""id"":""cnas90d"",""parent_id"":""t1_cnaqtxv"",""subreddit"":""BBW"",""gilded"":0,""author_flair_text"":null,""author_flair_css_class"":null,""ups"":1,""retrieved_on"":1425124282,""subreddit_id"":""t5_2qrqw"",""edited"":false,""controversiality"":0}
{""subreddit"":""knives"",""parent_id"":""t1_cnaqs9i"",""ups"":1,""retrieved_on"":1425124282,""gilded"":0,""author_flair_text"":null,""author_flair_css_class"":null,""edited"":false,""subreddit_id"":""t5_2qzyn"",""controversiality"":0,""score_hidden"":false,""link_id"":""t3_2qy4jn"",""name"":""t1_cnas90e"",""downs"":0,""created_utc"":""1420070401"",""body"":""Haha awesome man I got it from my grandpa this Christmas as well."",""archived"":false,""distinguished"":null,""id"":""cnas90e"",""score"":1,""author"":""MadagascarDifficulty""}
{""id"":""cnas90f"",""distinguished"":null,""archived"":false,""author"":""FreeSoul789"",""score"":3,""created_utc"":""1420070401"",""downs"":0,""body"":""I completely agree. I've spent that long staring at it sober.."",""link_id"":""t3_2qy171"",""name"":""t1_cnas90f"",""score_hidden"":false,""controversiality"":0,""subreddit_id"":""t5_2qhvj"",""edited"":false,""retrieved_on"":1425124282,""ups"":3,""author_flair_css_class"":null,""gilded"":0,""author_flair_text"":null,""subreddit"":""LSD"",""parent_id"":""t1_cnaqrew""}
{""score_hidden"":false,""link_id"":""t3_2qy21w"",""name"":""t1_cnas90g"",""created_utc"":""1420070401"",""downs"":0,""body"":""&gt;&gt;If a woman wants to give up a child for adoption with a named father, she is unable to do so without the father's consent.\n\n&gt;That is easy enough to do. Simply don't name the father.\n\nWomen are *only* not financially responsible *if* there is no named father. If the father isn't named, he's not on the hook for child support either, so *it doesn't matter*.\n\nYou're deliberately removing context and responding to sentences that go together to form an overall argument separately in a way that does not make sense. At this point you're being intentionally misleading to twist this into what you want it to be instead of actually participating in the spirit of the argument. "",""archived"":false,""id"":""cnas90g"",""distinguished"":null,""author"":""dewprisms"",""score"":17,""subreddit"":""changemyview"",""parent_id"":""t1_cnarzyw"",""retrieved_on"":1425124282,""ups"":17,""author_flair_text"":""3\u2206"",""gilded"":0,""author_flair_css_class"":"" points"",""edited"":false,""subreddit_id"":""t5_2w2s8"",""controversiality"":0}
{""score_hidden"":false,""link_id"":""t3_2qxty0"",""name"":""t1_cnas90h"",""created_utc"":""1420070401"",""downs"":0,""body"":""I haven't. Still trying to get someone to commit. \n\nWhere are you located?"",""distinguished"":null,""id"":""cnas90h"",""archived"":false,""score"":1,""author"":""TimDisaster"",""subreddit"":""beertrade"",""parent_id"":""t1_cnarxyo"",""retrieved_on"":1425124282,""ups"":1,""author_flair_css_class"":""repLevel4"",""gilded"":0,""author_flair_text"":null,""controversiality"":0,""subreddit_id"":""t5_2rgco"",""edited"":false}
{""parent_id"":""t3_2qy7at"",""subreddit"":""funny"",""gilded"":0,""author_flair_text"":null,""author_flair_css_class"":null,""retrieved_on"":1425124282,""ups"":1,""subreddit_id"":""t5_2qh33"",""edited"":false,""controversiality"":0,""score_hidden"":false,""name"":""t1_cnas90i"",""link_id"":""t3_2qy7at"",""body"":""Wheredugit?"",""downs"":0,""created_utc"":""1420070401"",""score"":1,""author"":""deephaven"",""archived"":false,""distinguished"":null,""id"":""cnas90i""}
{""created_utc"":""1420070401"",""downs"":0,""body"":""It's a religion that doesn't have a set in stone creed. so I'm assuming there's a few more."",""archived"":false,""distinguished"":null,""id"":""cnas90j"",""score"":2,""author"":""Vamking12"",""score_hidden"":false,""link_id"":""t3_2qy44e"",""name"":""t1_cnas90j"",""retrieved_on"":1425124282,""ups"":2,""author_flair_text"":""Pumpkin Pump/Pimp/Pie"",""gilded"":0,""author_flair_css_class"":"""",""edited"":false,""subreddit_id"":""t5_2vizz"",""controversiality"":0,""subreddit"":""TumblrInAction"",""parent_id"":""t1_cnan3hv""}
{""controversiality"":0,""subreddit_id"":""t5_2t0xk"",""edited"":false,""ups"":2,""retrieved_on"":1425124282,""author_flair_css_class"":"""",""author_flair_text"":""Lazlow"",""gilded"":0,""subreddit"":""GrandTheftAutoV"",""parent_id"":""t3_2qx0qa"",""distinguished"":null,""id"":""cnas90k"",""archived"":false,""score"":2,""author"":""marklar7"",""created_utc"":""1420070401"",""downs"":0,""body"":""\""Hey Rocky, Watch me board this train.\"" \n\""Again?\""\n\""Presto!\"""",""link_id"":""t3_2qx0qa"",""name"":""t1_cnas90k"",""score_hidden"":false}
{""created_utc"":""1420070401"",""downs"":0,""body"":""Roofers, the only people on a job site more savage than the rock people."",""archived"":false,""id"":""cnas90l"",""distinguished"":null,""score"":2,""author"":""Movepeck"",""score_hidden"":false,""link_id"":""t3_2qy8r4"",""name"":""t1_cnas90l"",""retrieved_on"":1425124282,""ups"":2,""author_flair_text"":null,""gilded"":0,""author_flair_css_class"":null,""subreddit_id"":""t5_2qh1i"",""edited"":false,""controversiality"":0,""subreddit"":""AskReddit"",""parent_id"":""t1_cnaqrf6""}
{""name"":""t1_cnas90m"",""link_id"":""t3_2qx8kh"",""score_hidden"":false,""author"":""pregnanthollywood"",""score"":-1,""archived"":false,""id"":""cnas90m"",""distinguished"":null,""body"":""You are a gentleman and a scholar...  I don't know why this is such a fucking hard concept to understand for some people. The police are part of society, not above it."",""created_utc"":""1420070401"",""downs"":0,""parent_id"":""t1_cnarwqe"",""subreddit"":""news"",""edited"":false,""subreddit_id"":""t5_2qh3l"",""controversiality"":0,""author_flair_text"":null,""gilded"":0,""author_flair_css_class"":null,""retrieved_on"":1425124282,""ups"":-1}
{""score_hidden"":false,""link_id"":""t3_2qxrkx"",""name"":""t1_cnas90n"",""created_utc"":""1420070401"",""downs"":0,""body"":""My math prof doesn't do rides, he says \""look at the people who put those rides together, theyre all stoned\"""",""distinguished"":null,""id"":""cnas90n"",""archived"":false,""score"":7,""author"":""submaRED"",""subreddit"":""WTF"",""parent_id"":""t1_cnaly8u"",""ups"":7,""retrieved_on"":1425124282,""author_flair_css_class"":null,""author_flair_text"":null,""gilded"":0,""controversiality"":0,""subreddit_id"":""t5_2qh61"",""edited"":false}
{""score"":1,""author"":""CarpeAeonem"",""distinguished"":null,""id"":""cnas90o"",""archived"":false,""body"":""Agreed! You should get it while it's on sale. "",""created_utc"":""1420070401"",""downs"":0,""name"":""t1_cnas90o"",""link_id"":""t3_2q8qx6"",""score_hidden"":false,""controversiality"":0,""subreddit_id"":""t5_2vs7z"",""edited"":false,""author_flair_css_class"":""stagsilhouette"",""author_flair_text"":"""",""gilded"":0,""retrieved_on"":1425124282,""ups"":1,""parent_id"":""t1_cnas1x6"",""subreddit"":""HannibalTV""}
{""controversiality"":0,""edited"":false,""subreddit_id"":""t5_2qh0u"",""ups"":1,""retrieved_on"":1425124282,""author_flair_css_class"":null,""gilded"":0,""author_flair_text"":null,""subreddit"":""pics"",""parent_id"":""t3_2qxggo"",""id"":""cnas90p"",""distinguished"":null,""archived"":false,""score"":1,""author"":""asdjfweaiv"",""created_utc"":""1420070401"",""downs"":0,""body"":""Tumblr is leaking again."",""link_id"":""t3_2qxggo"",""name"":""t1_cnas90p"",""score_hidden"":false}
{""archived"":false,""id"":""cnas90q"",""distinguished"":null,""score"":0,""author"":""uncannylizard"",""created_utc"":""1420070401"",""downs"":0,""body"":""The vast majority of countries in the world recognize the state of Palestine btw, in addition to the state of Israel."",""link_id"":""t3_2qxiaz"",""name"":""t1_cnas90q"",""score_hidden"":false,""edited"":false,""subreddit_id"":""t5_2qh13"",""controversiality"":0,""ups"":0,""retrieved_on"":1425124282,""gilded"":0,""author_flair_text"":null,""author_flair_css_class"":null,""subreddit"":""worldnews"",""parent_id"":""t1_cnahbvw""}
{""link_id"":""t3_2qyn3p"",""name"":""t1_cnas90r"",""score_hidden"":false,""id"":""cnas90r"",""distinguished"":null,""archived"":false,""author"":""[deleted]"",""score"":2,""created_utc"":""1420070401"",""downs"":0,""body"":""[deleted]"",""subreddit"":""Mustang"",""parent_id"":""t3_2qyn3p"",""controversiality"":0,""edited"":false,""subreddit_id"":""t5_2qqel"",""ups"":2,""retrieved_on"":1425124282,""author_flair_css_class"":null,""author_flair_text"":null,""gilded"":0}
{""score_hidden"":false,""link_id"":""t3_2qy0eu"",""name"":""t1_cnas90s"",""created_utc"":""1420070402"",""downs"":0,""body"":""Thank you, I certainly am!"",""distinguished"":null,""id"":""cnas90s"",""archived"":false,""author"":""gingerguitarx92x"",""score"":2,""subreddit"":""guitarpedals"",""parent_id"":""t1_cnaq8ts"",""retrieved_on"":1425124282,""ups"":2,""author_flair_css_class"":null,""gilded"":0,""author_flair_text"":null,""controversiality"":0,""subreddit_id"":""t5_2r0bp"",""edited"":false}
{""name"":""t1_cnas90t"",""link_id"":""t3_2qxm8n"",""score_hidden"":false,""score"":1,""author"":""billj457"",""distinguished"":null,""id"":""cnas90t"",""archived"":false,""body"":""Chappelle: https://www.youtube.com/watch?v=4B93BoC9ylg"",""created_utc"":""1420070402"",""downs"":0,""parent_id"":""t1_cnaiag9"",""subreddit"":""BMW"",""controversiality"":0,""edited"":false,""subreddit_id"":""t5_2qn3a"",""author_flair_css_class"":"""",""author_flair_text"":""2009 E90 335xi 6MT"",""gilded"":0,""ups"":1,""retrieved_on"":1425124282}
{""subreddit"":""movies"",""parent_id"":""t3_2qyjda"",""controversiality"":0,""subreddit_id"":""t5_2qh3s"",""edited"":false,""retrieved_on"":1425124282,""ups"":1,""author_flair_css_class"":null,""author_flair_text"":null,""gilded"":0,""link_id"":""t3_2qyjda"",""name"":""t1_cnas90u"",""score_hidden"":false,""distinguished"":null,""id"":""cnas90u"",""archived"":false,""author"":""kylionsfan"",""score"":1,""created_utc"":""1420070402"",""downs"":0,""body"":""Goonies""}
{""link_id"":""t3_2e7eti"",""name"":""t1_cnas90v"",""score_hidden"":false,""archived"":false,""id"":""cnas90v"",""distinguished"":null,""score"":1,""author"":""[deleted]"",""created_utc"":""1420070402"",""downs"":0,""body"":""[deleted]"",""subreddit"":""Games"",""parent_id"":""t1_cnark1x"",""edited"":false,""subreddit_id"":""t5_2qhwp"",""controversiality"":0,""retrieved_on"":1425124282,""ups"":1,""gilded"":0,""author_flair_text"":null,""author_flair_css_class"":null}
{""subreddit_id"":""t5_2stl8"",""edited"":false,""controversiality"":0,""ups"":1,""retrieved_on"":1425124282,""author_flair_text"":""Developer of the /r/SRGG"",""gilded"":0,""author_flair_css_class"":""ao-kuang"",""subreddit"":""Smite"",""parent_id"":""t3_2qynia"",""archived"":false,""distinguished"":null,""id"":""cnas90x"",""author"":""MrRangerLP"",""score"":1,""created_utc"":""1420070402"",""downs"":0,""body"":""Finish my Masters grind for 1v1 Joust"",""link_id"":""t3_2qynia"",""name"":""t1_cnas90x"",""score_hidden"":false}
{""subreddit"":""BabyBumps"",""parent_id"":""t3_2qxxgp"",""retrieved_on"":1425124282,""ups"":2,""gilded"":0,""author_flair_text"":null,""author_flair_css_class"":null,""edited"":false,""subreddit_id"":""t5_2s7cl"",""controversiality"":0,""score_hidden"":false,""link_id"":""t3_2qxxgp"",""name"":""t1_cnas90y"",""created_utc"":""1420070402"",""downs"":0,""body"":""I thought you wanted to name your baby Primrose. I was like, \""hmm, I don't hate it.\"""",""archived"":false,""distinguished"":null,""id"":""cnas90y"",""author"":""-purple-is-a-fruit-"",""score"":2}
{""subreddit_id"":""t5_2z5u0"",""edited"":false,""controversiality"":0,""ups"":1,""retrieved_on"":1425124282,""gilded"":0,""author_flair_text"":""Dirt | Sergei | Simetra | Sledge"",""author_flair_css_class"":"""",""subreddit"":""randomsuperpowers"",""parent_id"":""t1_cnas75o"",""archived"":false,""distinguished"":null,""id"":""cnas90z"",""author"":""Mace55555"",""score"":1,""downs"":0,""created_utc"":""1420070402"",""body"":""[Slightly stronger than an RPG.]"",""link_id"":""t3_2qs1ie"",""name"":""t1_cnas90z"",""score_hidden"":false}
{""subreddit"":""funny"",""parent_id"":""t3_2qxtg0"",""ups"":1,""retrieved_on"":1425124282,""gilded"":0,""author_flair_text"":null,""author_flair_css_class"":null,""subreddit_id"":""t5_2qh33"",""edited"":false,""controversiality"":0,""score_hidden"":false,""link_id"":""t3_2qxtg0"",""name"":""t1_cnas910"",""created_utc"":""1420070402"",""downs"":0,""body"":""[deleted]"",""archived"":false,""distinguished"":null,""id"":""cnas910"",""score"":1,""author"":""[deleted]""}
{""author_flair_text"":null,""gilded"":0,""author_flair_css_class"":null,""retrieved_on"":1425124282,""ups"":1,""edited"":false,""subreddit_id"":""t5_2qh1i"",""controversiality"":0,""parent_id"":""t3_2qys1t"",""subreddit"":""AskReddit"",""body"":""I don't know 100% why, but, for me, I delete comments if I was wrong about something. Like, if I misread a question and answered what I misread or I misinterpreted something. I'm not going to delete it just because I got 299\u2640489242049t4bit43\u2665 downvotes. "",""created_utc"":""1420070402"",""downs"":0,""score"":1,""author"":""[deleted]"",""archived"":false,""distinguished"":null,""id"":""cnas911"",""score_hidden"":false,""name"":""t1_cnas911"",""link_id"":""t3_2qys1t""}
{""parent_id"":""t1_cnajcvn"",""subreddit"":""devils"",""author_flair_text"":""#11 - Pain Train Gionta"",""gilded"":0,""author_flair_css_class"":""Home11"",""ups"":3,""retrieved_on"":1425124282,""subreddit_id"":""t5_2roo0"",""edited"":false,""controversiality"":0,""score_hidden"":false,""name"":""t1_cnas912"",""link_id"":""t3_2qxx2l"",""body"":""I'll try to find a good pair at my NYE party. Will report back. "",""downs"":0,""created_utc"":""1420070402"",""author"":""GreasyBastard_"",""score"":3,""archived"":false,""distinguished"":null,""id"":""cnas912""}
{""gilded"":0,""author_flair_text"":""RCUS, yo"",""author_flair_css_class"":"""",""retrieved_on"":1425124282,""ups"":2,""subreddit_id"":""t5_2riuy"",""edited"":false,""controversiality"":0,""parent_id"":""t3_2qyqhj"",""subreddit"":""Reformed"",""body"":""I love it.  Okay, based upon point (6), is it right for a Christian to say, \""it is not me doing the sin, but sin in me\"" [Romans 7:17]?  Or is the passage saying something quite different than what Ryle understands it to mean?"",""created_utc"":""1420070402"",""downs"":0,""author"":""BSMason"",""score"":2,""archived"":false,""id"":""cnas913"",""distinguished"":null,""score_hidden"":false,""name"":""t1_cnas913"",""link_id"":""t3_2qyqhj""}
{""created_utc"":""1420070402"",""downs"":0,""body"":""I like this idea, though just to confirm though technically if 6 bosses dropped loot you needed but lost, does that mean you get 24 counter points?\n\nIf you lose even using counters, does that mean its lost for the duration of the raid? Like\n\nKargath drops something you need, lose loot, get +4 on roll, butcher drops weapon, you still lose, does the counter apply or would you get +8 for the next boss as you still lost?"",""distinguished"":null,""id"":""cnas914"",""archived"":false,""author"":""Deftest366"",""score"":1,""score_hidden"":false,""link_id"":""t3_2qylyq"",""name"":""t1_cnas914"",""ups"":1,""retrieved_on"":1425124282,""author_flair_css_class"":""horde-paladin"",""author_flair_text"":"""",""gilded"":0,""controversiality"":0,""subreddit_id"":""t5_2qio8"",""edited"":false,""subreddit"":""wow"",""parent_id"":""t3_2qylyq""}
{""ups"":4,""retrieved_on"":1425124282,""author_flair_text"":null,""gilded"":0,""author_flair_css_class"":null,""edited"":false,""subreddit_id"":""t5_2v4ay"",""controversiality"":0,""subreddit"":""Stacked"",""parent_id"":""t3_2qxyqy"",""created_utc"":""1420070402"",""downs"":0,""body"":""Yes."",""archived"":false,""distinguished"":null,""id"":""cnas915"",""score"":4,""author"":""amolad"",""score_hidden"":false,""link_id"":""t3_2qxyqy"",""name"":""t1_cnas915""}
{""parent_id"":""t3_2qx5c4"",""subreddit"":""Android"",""edited"":false,""subreddit_id"":""t5_2qlqh"",""controversiality"":0,""gilded"":0,""author_flair_text"":""Nexus 5 CM12"",""author_flair_css_class"":""userGray"",""ups"":0,""retrieved_on"":1425124282,""name"":""t1_cnas916"",""link_id"":""t3_2qx5c4"",""score_hidden"":false,""author"":""Tropiux"",""score"":0,""archived"":false,""id"":""cnas916"",""distinguished"":null,""body"":""I just wish Google dropped support for Android &lt; 4.0 already. "",""created_utc"":""1420070402"",""downs"":0}
{""link_id"":""t3_2qxpv2"",""name"":""t1_cnas917"",""score_hidden"":false,""id"":""cnas917"",""distinguished"":null,""archived"":false,""score"":2,""author"":""flufgun"",""created_utc"":""1420070402"",""downs"":0,""body"":""[](/hellohuman)MS paint plagiarism is best plagiarism!"",""subreddit"":""MLPLounge"",""parent_id"":""t1_cnak0cy"",""controversiality"":0,""edited"":false,""subreddit_id"":""t5_2t403"",""ups"":2,""retrieved_on"":1425124282,""author_flair_css_class"":""lyra"",""gilded"":0,""author_flair_text"":""Lyra""}
{""parent_id"":""t1_cnarr0q"",""subreddit"":""falcons"",""subreddit_id"":""t5_2ql17"",""edited"":false,""controversiality"":0,""author_flair_text"":null,""gilded"":0,""author_flair_css_class"":null,""retrieved_on"":1425124282,""ups"":3,""name"":""t1_cnas918"",""link_id"":""t3_2qy1f3"",""score_hidden"":false,""author"":""ATL84"",""score"":3,""archived"":false,""id"":""cnas918"",""distinguished"":null,""body"":""Haha guilty. But I've heard some people say they weren't thrilled about guys like McDaniels and Del Rio as well. Won't pass judgement until I see how this all plays out"",""created_utc"":""1420070402"",""downs"":0}
{""subreddit_id"":""t5_2ty3s"",""edited"":false,""controversiality"":0,""ups"":1,""retrieved_on"":1425124282,""gilded"":0,""author_flair_text"":null,""author_flair_css_class"":null,""subreddit"":""dayz"",""parent_id"":""t3_2qylol"",""archived"":false,""distinguished"":null,""id"":""cnas919"",""score"":1,""author"":""Arnasx"",""created_utc"":""1420070403"",""downs"":0,""body"":""Unfortunately at this time of point you can have the best CPU, GPU available to buy and you still would not get 30 FPS in Cities. The game is only optimized for specific CPU's and GPU's that the developers use."",""link_id"":""t3_2qylol"",""name"":""t1_cnas919"",""score_hidden"":false}
{""body"":""Have You Tried Turning It Off And On Again?"",""downs"":0,""created_utc"":""1420070403"",""author"":""A_french_chinese_man"",""score"":1,""id"":""cnas91a"",""distinguished"":null,""archived"":false,""score_hidden"":false,""name"":""t1_cnas91a"",""link_id"":""t3_2qxfa5"",""author_flair_css_class"":null,""gilded"":0,""author_flair_text"":null,""retrieved_on"":1425124282,""ups"":1,""controversiality"":0,""edited"":false,""subreddit_id"":""t5_2qh1i"",""parent_id"":""t3_2qxfa5"",""subreddit"":""AskReddit""}
{""retrieved_on"":1425124282,""ups"":1,""author_flair_css_class"":null,""gilded"":0,""author_flair_text"":null,""controversiality"":0,""edited"":1420071816,""subreddit_id"":""t5_2qh1i"",""subreddit"":""AskReddit"",""parent_id"":""t3_2qykcw"",""downs"":0,""created_utc"":""1420070403"",""body"":""My Great to the power of 6 ..or 7 grandfather came over here from Germany around 1776. On the ship ride over his mother dies. \n\nTwo weeks off the boat his father dies. He was only around 9 years old. \n\nHe then was sold as an \""indigent servant\"" he earned his independence by fighting in the revolutionary war. \n\nHe moved from ohio to near Pittsburgh, lived to be 101 years old.  "",""id"":""cnas91b"",""distinguished"":null,""archived"":false,""author"":""TomHanksDied"",""score"":1,""score_hidden"":false,""link_id"":""t3_2qykcw"",""name"":""t1_cnas91b""}
{""edited"":false,""subreddit_id"":""t5_2qhr3"",""controversiality"":0,""retrieved_on"":1425124282,""ups"":1,""gilded"":0,""author_flair_text"":""2001 Pontiac Firebird"",""author_flair_css_class"":"""",""subreddit"":""Autos"",""parent_id"":""t3_2qjwg1"",""archived"":false,""id"":""cnas91c"",""distinguished"":null,""author"":""blazefalcon"",""score"":1,""created_utc"":""1420070403"",""downs"":0,""body"":""Dodge Rampage, pretty much any pre-80s Japanese cars, original VW Scirocco, Porsche 928, clean old Beetles, any Nissan Z series but that's more understandable. \n\nOh, and FJ Cruisers. They're just neat. "",""link_id"":""t3_2qjwg1"",""name"":""t1_cnas91c"",""score_hidden"":false}
{""author_flair_text"":""Gustav1985"",""gilded"":0,""author_flair_css_class"":""cmdr"",""ups"":1,""retrieved_on"":1425124282,""edited"":false,""subreddit_id"":""t5_2vi60"",""controversiality"":0,""parent_id"":""t3_2qynaq"",""subreddit"":""EliteDangerous"",""body"":""I had one to kill pirates in an unclaimed system so when I was there all I had to do wall drop in to USS and if I wasn't attacked I would scan the ships there and if they came up with no sub faction I shot them add it counted. "",""created_utc"":""1420070403"",""downs"":0,""score"":1,""author"":""Gustav55"",""archived"":false,""id"":""cnas91d"",""distinguished"":null,""score_hidden"":false,""name"":""t1_cnas91d"",""link_id"":""t3_2qynaq""}


"
569975481,32220,CI: value_counts tests failing sometimes,dsaxton,closed,2020-02-24T16:35:28Z,2020-03-04T13:52:31Z,"It seems like we're getting some failures from `test_value_counts_unique_nunique` in `pandas/tests/base/test_ops.py` (weird thing is that it only fails some of the time; maybe because of randomized test data?). Looks like these tests were updated in https://github.com/pandas-dev/pandas/pull/32046; @SaturnFromTitan any ideas what might be happening there?

Sample failed pipeline: https://github.com/pandas-dev/pandas/runs/464853016"
569531435,32205,Simplify test_value_counts_unique_nunique_null in pandas/tests/base/test_ops.py,SaturnFromTitan,closed,2020-02-23T17:46:29Z,2020-03-04T13:52:31Z,"As stated by @jreback in [his comment](https://github.com/pandas-dev/pandas/pull/32046#discussion_r383021295) on #32046, there are some weird constructs, duplications and unreachable code in this test.

For more info see the mentioned PR and the unresolved comments in the bottom.

The other tests in this file have a similar problem, but should probably be a separate PR."
556657922,31413,Series.append(DataFrame) should throw TypeError,hvardhan20,closed,2020-01-29T06:06:21Z,2020-03-04T14:17:16Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> import pandas as pd
>>> pd.__version__
'1.0.0rc0+233.gec0996c67'
>>> df = pd.DataFrame({'A':[1,2]})
>>> df.A.append(df)
     0    A
0  1.0  NaN
1  2.0  NaN
0  NaN  1.0
1  NaN  2.0

```
#### Problem description

The behavior of append is the same as it is in 0.25.3 when a DataFrame is appended to Series. But it should actually throw a TypeError when passed a DataFrame.
We clearly document that the elements passed to append should be Series or list/tuple of Series.
Also according to [this](https://github.com/pandas-dev/pandas/issues/30975), a TypeError seems appropriate. The change can be implemented for 1.0.1 or 1.1 after [this](https://github.com/pandas-dev/pandas/pull/31036#issuecomment-577248825) discussion.

#### Expected Output
```python
>>> df = pd.DataFrame({'A':[1, 2]})
>>> df.A.append([df])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Work\Git projects\pandas\pandas\core\series.py"", line 2572, in append
    raise TypeError(msg)
TypeError: to_append should be a Series or list/tuple of Series, got DataFrame
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : ec0996c6751326eed17a0bb456fe1c550689a618
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.0rc0+233.gec0996c67.dirty
numpy            : 1.17.0
pytz             : 2018.9
dateutil         : 2.8.0
pip              : 19.3.1
setuptools       : 44.0.0.post20200106
Cython           : 0.29.14
pytest           : 4.3.1
hypothesis       : 5.1.5
sphinx           : 1.8.5
blosc            : None
feather          : 0.4.0
xlsxwriter       : 1.1.5
lxml.etree       : 4.3.2
html5lib         : 1.0.1
pymysql          : 0.9.3
psycopg2         : None
jinja2           : 2.10
IPython          : 7.4.0
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.3.2
matplotlib       : 3.0.3
numexpr          : 2.6.9
odfpy            : None
openpyxl         : 2.6.1
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pytest           : 4.3.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.1
tables           : 3.5.1
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.1.5
numba            : 0.43.1

</details>
"
574317967,32402,BUG: overflow on pd.Timedelta(nanoseconds=) constructor,jreback,closed,2020-03-02T23:36:34Z,2020-03-04T14:22:36Z,"```
In [1]: pd.__version__                                                                                                                                                                                                      
Out[1]: '1.1.0.dev0+570.g9a02c3503
```

It appears we are overflowing when using nanoseconds= keyword for Timedelta. This works just fine when passing smaller values or passing as the first positional arg.

```
In [3]: pd.Timedelta(nanoseconds=1e10)                                                                                                                                                                                      
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-d56d2f0a71ff> in <module>
----> 1 pd.Timedelta(nanoseconds=1e10)

~/pandas/pandas/_libs/tslibs/timedeltas.pyx in pandas._libs.tslibs.timedeltas.Timedelta.__new__()

ValueError: Could not convert object to NumPy timedelta

# ok if by default constructor
In [4]: pd.Timedelta(1e10)                                                                                                                                                                                                  
Out[4]: Timedelta('0 days 00:00:10')
```"
574901801,32419,"CLN: move away from .values, _ndarray_values",jbrockmendel,closed,2020-03-03T19:51:04Z,2020-03-04T15:08:21Z,"Also _values_from_object and _internal_get_values, though none of those made it into this PR."
571070120,32261,PERF: lazify blknos and blklocs,jbrockmendel,closed,2020-02-26T04:58:26Z,2020-03-04T15:19:36Z,"The benchmark I'm using for this is the same as for #32224, based on the asv that is most affected by removing `fast_apply` (see #32086)

```
import numpy as np
from pandas import *
%load_ext line_profiler


def get_df():
    N = 10 ** 4
    labels = np.random.randint(0, 2000, size=N)
    labels2 = np.random.randint(0, 3, size=N)
    df = DataFrame(
        {
            ""key"": labels,
            ""key2"": labels2,
            ""value1"": np.random.randn(N),
            ""value2"": [""foo"", ""bar"", ""baz"", ""qux""] * (N // 4),
        }
    )
    return df

df = get_df()

gb = df.groupby(""key"")

%prun -s cumulative gb.apply(lambda x: 1)
```

If we disable `fast_apply` on master, this gives:
```
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.207    0.207 groupby.py:701(apply)
        1    0.000    0.000    0.207    0.207 groupby.py:750(_python_apply_general)
        1    0.008    0.008    0.204    0.204 ops.py:151(apply)
     1994    0.003    0.000    0.191    0.000 ops.py:858(__iter__)
     1993    0.003    0.000    0.187    0.000 ops.py:889(_chop)
     1993    0.003    0.000    0.184    0.000 indexing.py:814(__getitem__)
     1993    0.001    0.000    0.180    0.000 indexing.py:1462(_getitem_axis)
     1993    0.003    0.000    0.179    0.000 indexing.py:1488(_get_slice_axis)
     1993    0.007    0.000    0.167    0.000 generic.py:3474(_slice)
     1993    0.007    0.000    0.140    0.000 managers.py:713(get_slice)
     1994    0.004    0.000    0.068    0.000 managers.py:125(__init__)
     1994    0.027    0.000    0.059    0.000 managers.py:215(_rebuild_blknos_and_blklocs)
     1993    0.002    0.000    0.050    0.000 managers.py:723(<listcomp>)
     5979    0.010    0.000    0.048    0.000 blocks.py:310(getitem_block)
     5983    0.003    0.000    0.033    0.000 blocks.py:275(make_block_same_class)
```

If we disable `fast_apply` on this PR:
```
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.136    0.136 groupby.py:701(apply)
        1    0.000    0.000    0.136    0.136 groupby.py:750(_python_apply_general)
        1    0.007    0.007    0.134    0.134 ops.py:151(apply)
     1984    0.002    0.000    0.120    0.000 ops.py:903(__iter__)
     1983    0.002    0.000    0.117    0.000 ops.py:934(_chop)
     1983    0.003    0.000    0.114    0.000 indexing.py:814(__getitem__)
     1983    0.001    0.000    0.110    0.000 indexing.py:1462(_getitem_axis)
     1983    0.003    0.000    0.109    0.000 indexing.py:1488(_get_slice_axis)
     1983    0.006    0.000    0.098    0.000 generic.py:3474(_slice)
     1983    0.006    0.000    0.076    0.000 managers.py:742(get_slice)
     1983    0.002    0.000    0.048    0.000 managers.py:752(<listcomp>)
     5949    0.010    0.000    0.046    0.000 blocks.py:310(getitem_block)
     5957    0.003    0.000    0.032    0.000 blocks.py:275(make_block_same_class)
     5960    0.006    0.000    0.029    0.000 blocks.py:3023(make_block)
```"
406060956,25100,plot() Has Visual Error When Combing Bar & Line Graphs,aslobodnik,closed,2019-02-03T07:17:23Z,2020-03-04T15:24:44Z,"#### Code Sample, a copy-pastable example if possible

```
import matplotlib
import pandas as pd

df = pd.DataFrame({'A':[1,2,3],'B':[0.1,.2,.4]})

print(df)

df.A.plot.bar()
df.B.plot(secondary_y=True,style='g')
```
![image](https://user-images.githubusercontent.com/4892978/52173840-9b652c80-2740-11e9-8426-98ee1a27c5b1.png)

#### Problem description

The first & last bar graph of series is on top of the Y axis. 

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.2.final.0
python-bits: 64
OS: Darwin
OS-release: 18.2.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.0
pytest: None
pip: 18.1
setuptools: 40.6.3
Cython: None
numpy: 1.16.1
scipy: None
pyarrow: None
xarray: None
IPython: 7.2.0
sphinx: None
patsy: None
dateutil: 2.7.5
pytz: 2018.9
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
573181467,32355,"Fix PR08, RT02, RT03, and SA01 on pandas.Index.fillna",RafifEL,closed,2020-02-29T05:15:36Z,2020-03-04T15:41:38Z,"- [x] closes #https://github.com/pandanistas/pandanistas_sprint_ui2020/issues/5
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
574993289,32424,Fix BUG: overflow on pd.Timedelta(nanoseconds=) constructor,roberthdevries,closed,2020-03-03T22:24:14Z,2020-03-04T15:43:52Z,"Add regression test

- [x] closes #32402
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
575260369,32429,DOC: Fix EX02 in pandas.Index.get_loc,farhanreynaldo,closed,2020-03-04T09:39:27Z,2020-03-04T16:22:17Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Related to #27977. 

```
################################################################################
################################## Validation ##################################
################################################################################

5 Errors found:
        No extended summary found
        Parameter ""key"" has no description
        The first line of the Returns section should contain only the type, unless multiple values are being returned
        Return value has no description
        See Also section not found
```"
571696054,32281,TST: Split and simplify test_value_counts_unique_nunique,SaturnFromTitan,closed,2020-02-26T22:19:03Z,2020-03-04T17:16:02Z,"closes #32205, closes #32220

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
575590944,32440,CI Failing: Linux py37_locale TestTSPlot.test_matplotlib_scatter_datetime64,simonjayhawkins,closed,2020-03-04T17:16:09Z,2020-03-04T19:11:38Z,"https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=29960&view=logs&j=a3a13ea8-7cf0-5bdb-71bb-6ac8830ae35c&t=add65f64-6c25-5783-8fd6-d9aa1b63d9d4

matplotlib 3.2.0"
575602227,32442,CI: fix test_matplotlib_scatter_datetime64,simonjayhawkins,closed,2020-03-04T17:29:27Z,2020-03-04T19:14:42Z,"- [ ] closes #32440
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
575685842,32445,Backport PR #32442 on branch 1.0.x (CI: fix test_matplotlib_scatter_datetime64),meeseeksmachine,closed,2020-03-04T19:12:19Z,2020-03-04T20:03:13Z,Backport PR #32442: CI: fix test_matplotlib_scatter_datetime64
567272157,32090,Series append raises TypeError,hvardhan20,closed,2020-02-19T02:18:53Z,2020-03-04T21:07:57Z,"- [ ] closes #31413
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Series.append now raises TypeError when a DataFrame or a sequence containing DataFrame is passed.
"
575704937,32446,BLD: Remove snappy from requirements-dev.txt,kendricng,closed,2020-03-04T19:35:36Z,2020-03-04T21:31:59Z,"#### Problem description

As raised in issues #32417 and #32327, `snappy` is causing issues while setting up a dev environment `requirements-dev.txt`. 

#### Proposed change
I suggest removing `snappy` from `requirements-dev.txt` to allow fewer issues when setting up the environment.

#### Acknowledgements

@WillAyd for suggesting to raise this issue.

Related Issues: #32417, #32327"
576072168,32455,VIZ: Fixing Matplotlib depecation warnings for ax.rowNum and ax.colNum,datapythonista,closed,2020-03-05T08:20:17Z,2020-03-05T08:37:01Z,"- [X] xref #32444

Fixing the deprecation warning in the docs (testing if we really need to be compatible with the old syntax)"
573300063,32368,Silence warnings when compiling pandas/_libs/parsers.pyx,ShaharNaveh,closed,2020-02-29T13:59:09Z,2020-03-05T09:42:13Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This is getting rid of this warning:
```
pandas/_libs/parsers.c: In function ‘__pyx_f_6pandas_5_libs_7parsers_10TextReader__get_header’:
pandas/_libs/parsers.c:9313:27: warning: ‘__pyx_v_data_line’ may be used uninitialized in this function [-Wmaybe-uninitialized]
 9313 |   __pyx_t_5numpy_uint64_t __pyx_v_data_line;
```"
575644726,32444,CI: ax.rowNum and ax.colNum attributes deprecated in Matplotlib 3.2,simonjayhawkins,closed,2020-03-04T18:21:01Z,2020-03-05T10:12:00Z,https://github.com/pandas-dev/pandas/pull/32442/checks?check_run_id=485569958
573298134,32367,CLN: some code cleanups to pandas/_libs/missing.pyx,ShaharNaveh,closed,2020-02-29T13:43:43Z,2020-03-05T10:12:19Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
575583262,32438,CI: mypy fixup for #32261,simonjayhawkins,closed,2020-03-04T17:07:29Z,2020-03-05T10:13:15Z,https://github.com/pandas-dev/pandas/runs/485121000
576137336,32456,Backport PR #32444 on branch 1.0.x (CI: ax.rowNum and ax.colNum attributes deprecated in Matplotlib 3.2),meeseeksmachine,closed,2020-03-05T10:11:04Z,2020-03-05T10:45:00Z,Backport PR #32444: CI: ax.rowNum and ax.colNum attributes deprecated in Matplotlib 3.2
575561610,32436,DOC: correct issue number for PR #32424,simonjayhawkins,closed,2020-03-04T16:37:55Z,2020-03-05T11:26:32Z,"xref #32424
"
565717224,31994,WEB: the donate page is not loading correctly,jorisvandenbossche,closed,2020-02-15T08:31:37Z,2020-03-05T12:13:09Z,"For me, the donate page (and specifically the donate widget) is not loading:

![image](https://user-images.githubusercontent.com/1020496/74584748-e06ba200-4fd5-11ea-9dbc-61dbf0c564f8.png)

It just keeps saying ""loading"" for me."
575126709,32427,CLN: avoid _internal_get_values in groupby.generic,jbrockmendel,closed,2020-03-04T04:19:38Z,2020-03-05T15:00:17Z,"xref #27165, #27167"
573813905,32390,hdf5 loading performance issue for non-sorted data in pandas 1.0.1,leo4183,open,2020-03-02T08:53:47Z,2020-03-05T17:15:45Z,"#### Problem Description
compared to the legacy pandas 0.2x.x, the most recent 1.0.1 version doesnt perform well in HDF5 file loading unless the conditioned column was sorted beforehand

```python
import os
import numpy as np
import pandas as pd
row = 6000
col = 2
start_date = '20000101'
freq = 'B'
ind = pd.date_range(start_date,periods=row,freq=freq)
a = pd.DataFrame(np.random.rand(row*col).reshape((row,-1)),index=ind)
a = a.reset_index(drop=False)
a.columns = ['date','x','y']
a = pd.concat([a]*2000)
a.sort_values('y').to_hdf(os.path.join(os.path.expanduser('~'),'hdf_test.h5'),key='df',data_columns=True,format='table')
a.sort_values('date').to_hdf(os.path.join(os.path.expanduser('~'),'hdf_test_sorted.h5'),key='df',data_columns=True,format='table')

# significantly slower in pandas 1.0.1 (this is not an issue in pandas 0.2x.x)
store = pd.HDFStore(os.path.join(os.path.expanduser('~'),'hdf_test.h5'),mode='r')
result = store.select('df',where=""{0}>=20150101 & {0}<=20160730"".format('date'),)
store.close()

# as fast as pandas 0.2x.x if data was sorted on the 'term' column
store = pd.HDFStore(os.path.join(os.path.expanduser('~'),'hdf_test_sorted.h5'),mode='r')
result = store.select('df',where=""{0}>=20150101 & {0}<=20160730"".format('date'),)
store.close()
```"
573342315,32371,TYP/CLN: Optional[Hashable] -> pandas._typing.Label,simonjayhawkins,closed,2020-02-29T17:16:02Z,2020-03-06T07:13:20Z,
575728114,32448,Series.convert_dtypes is not idempotent with small strings,impredicative,closed,2020-03-04T20:02:59Z,2020-03-06T07:17:19Z,"#### Code Sample, a copy-pastable example if possible

```python
s = pd.Series(['x', 'y', 'z'])
print(s.dtype, s.convert_dtypes().dtype, s.convert_dtypes().convert_dtypes().dtype, sep='\n')
```

Output:
```
object
string
|S1
```
#### Problem description

It is reasonably expected that the output of `convert_dtypes` will always be stable such that if it's applied multiple times to the same Series, the subsequent calls after the first one will be redundant and will result in no change. This reasonable assumption doesn't however hold; it's violated in different ways by the current implementation. In the pasted example, it converts `object` to `string` and then to `|S1` which is a clear violation of stability. In other cases I've seen it convert `object` to `string` and then back to `object`, with this making no sense.

Also, the word ""best"" should be removed from the documentation. In what formal way exactly is its output the best? In some cases I've seen this method change the dtype to dramatically increase the deep memory usage of a Series. Given the seemingly whimsical implementation, the term seems to carry no meaning. I think this entire method, as it stands, is a hack job, and has no place in the package.

#### Expected Output

```
object
|S1
|S1
```

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use ""pip install psycopg2-binary"" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.
  """""")
/usr/local/lib/python3.6/dist-packages/pandas_datareader/compat/__init__.py:18: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.
  from pandas.util.testing import assert_frame_equal


INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.137+
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.17.5
pytz             : 2018.9
dateutil         : 2.6.1
pip              : 19.3.1
setuptools       : 45.1.0
Cython           : 0.29.15
pytest           : 3.6.4
hypothesis       : None
sphinx           : 1.8.5
blosc            : None
feather          : 0.4.0
xlsxwriter       : None
lxml.etree       : 4.2.6
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.6.1 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 5.5.0
pandas_datareader: None
bs4              : 4.6.3
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.6
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 2.5.9
pandas_gbq       : 0.11.0
pyarrow          : 0.14.1
pytables         : None
pytest           : 3.6.4
pyxlsb           : None
s3fs             : 0.4.0
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.4.4
tabulate         : 0.8.6
xarray           : 0.14.1
xlrd             : 1.1.0
xlwt             : 1.3.0
xlsxwriter       : None
numba            : 0.47.0
</details>
"
558660329,31563,ENH: add fold support to Timestamp constructor,AlexKirko,closed,2020-02-02T08:10:40Z,2020-03-06T08:24:04Z,"**PERF Note:** Current implementation slows down the `Timestamp` constructor. I've tested this thoroughly and tracked it down to ba7fcd5 where I changed the function signatures at the very beginning of working on the PR. The performance overhead appeared before any of the logic was implemented.

- [X] closes #25057, closes #31338 
- [X] tests added 32 / passed 32
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

### Reasoning for changes
We currently don't support fold from [PEP 495](https://www.python.org/dev/peps/pep-0495/#the-fold-attribute), and this causes us a lot of grief, including, but not limited to, broken Timestamp representations at the edge of DST shift (see #31338). The values can also break.

Support of the fold attribute helps us deal with that. Now, if the wall clock time occurs twice due to a DST shift, we will know exactly at which point in time we are.

This also removes inconsistencies between using `pytz` and `dateutil`: now both the values and representations match for both timezone packages near DST summer/winter shifts.

### Scope of PR
Implementing fold into Timestamp can easily get out of hand. Parent `pydatetime` has it easy, as the object doesn't need to sync its underlying epoch time, fold, and the representation, like we do.

This PR is already large, so I propose we limit its scope to minimal consistent fold support:
- when fold is explicitly supplied, attempt to shift value across the DST boundary according to the value supplied
- infer fold from value during tz-aware Timestamp construction. For example, if Timestamp is built from epoch time, then we can infer fold.
- pass the resulting fold to the underlying `datetime` constructor in `create_timestamp_from_ts` so that it gets stored in the object (can't assign it directly as it's read-only).
- implement local timezone support for fold. Don't have any idea how to do this though.

Things I suggest we leave to discussion and other PRs:
- conflicts resolution. A user can supply `fold=1` for a datetime that is nowhere near a DST shift. We can raise and Error or a Warning in the future. For now, we check whether `fold` can be 1, and if not, we leave it as default `fold=0`.
- consider reintroducing ambiguous time errors. Currently, the implementation assumes `fold == 0` for ambiguous time like Timestamp(""2019-10-27 01:30:00"", tz='Europe/London'). The error was dropped to mirror the `fold=0` default in pydatetime and to allow the fold attribute to be `bint` in Cython functions.

**Note:** `pydatetime` doesn't infer fold from value and doesn't raise errors when you assign `fold=1` nowhere near a fold. Example:

```python
from datetime import datetime
from dateutil.tz import gettz

dt = datetime.datetime(2015, 10, 27, 5, 30, 0, 0, gettz(""Europe/London""), fold=1)
dt.fold
OUT:
1
```

### Performance problems
This implementation behaves as I expect it to, but it slows down scalar constructor benchmarks by 30-70%, even for tz-naive benchmarks.

Update: since the benchmark slowdown appeared before any of the functionality, this has nothing to do with the logic introduced and all to do with adding `bint fold` to function signatures, apparently. I'm afraid I don't know Cython well enough to research further."
576793029,32482,Plot bar in Pandas 0.25.3 not changing the colors for each of the bars,stenpiren,closed,2020-03-06T09:08:21Z,2020-03-06T09:43:24Z,"```python
# Code
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

df = pd.DataFrame({'category': ['A', 'B', 'C'],
                   'value': [-1432.32, 10.43, 30000.00],
                   'db-id': [1234, 2424, 251]
                   })
df.category.value_counts().plot.bar(color = plt.cm.Paired.colors)
```
#### Problem description

When using the above code on pandas '0.25.3', all bars are plotted with the same color. I would like to have them plotted each with its own color. It used to work before but all of a sudden it doesnt. Possibly due to the pandas version. 
Any clue? "
576163842,32457,commit history attributes commit to committer instead of author,simonjayhawkins,closed,2020-03-05T10:51:17Z,2020-03-06T12:29:24Z,https://github.com/pandas-dev/pandas/commits/master since 690e382d2e93198c3629943607cef9fc3012dfd4
571887870,32286,DOC: Fix errors in pandas.Series.argmin,farhanreynaldo,closed,2020-02-27T07:53:20Z,2020-03-06T12:31:08Z,"- [x] closes https://github.com/pandanistas/pandanistas_sprint_jakarta2020/issues/19
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

output of `python scripts/validate_docstrings.py pandas.Series.argmin`:
```
################################################################################
################################## Validation ##################################
################################################################################
```

"
563029484,31869,Dividing None Series with Timedelta fails with pandas 1.0.1,pquentin,closed,2020-02-11T07:48:23Z,2020-03-06T12:37:07Z,"#### Code Sample

```python
import datetime
import pandas as pd

s1 = pd.Series([datetime.date(2020, 1, 1)])
s2 = pd.Series([None])
print((s1 - s2) / pd.Timedelta(days=1))
```

#### Problem description

With pandas 1.0.0, this was working fine and putting `NaN` in the `Series`.

With pandas 1.0.1, I get the following traceback:

```
Traceback (most recent call last):
  File ""t.py"", line 6, in <module>
    print((s1 - s2) / pd.Timedelta(days=1))
  File "".../lib/python3.6/site-packages/pandas/core/ops/common.py"", line 64, in new_method
    return method(self, other)
  File "".../lib/python3.6/site-packages/pandas/core/ops/__init__.py"", line 500, in wrapper
    result = arithmetic_op(lvalues, rvalues, op, str_rep)
  File "".../lib/python3.6/site-packages/pandas/core/ops/array_ops.py"", line 193, in arithmetic_op
    res_values = dispatch_to_extension_op(op, lvalues, rvalues)
  File "".../lib/python3.6/site-packages/pandas/core/ops/dispatch.py"", line 125, in dispatch_to_extension_op
    res_values = op(left, right)
  File ""pandas/_libs/tslibs/timedeltas.pyx"", line 1397, in pandas._libs.tslibs.timedeltas.Timedelta.__rtruediv__
numpy.core._exceptions.UFuncTypeError: ufunc 'true_divide' cannot use operands with types dtype('O') and dtype('<m8[ns]')
```

I run `git bisect` and the new behavior has been introduced in https://github.com/pandas-dev/pandas/commit/a8aff6cdd12b3e3ed2c767b1752fd17a19be6590.

What is the correct behavior?

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.3.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 40.6.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
553842609,31225,Make the documentation for pandas.Series.str.replace() clearer,drkarthi,closed,2020-01-22T22:55:42Z,2020-03-06T15:25:33Z,"#### Problem description

The current documentation of str.replace says ```Replace occurrences of pattern/regex in the Series/Index with some other string. Equivalent to str.replace() or re.sub().```.

For the novice user this suggests that base Python's str.replace() and re.sub() provide equivalent functionality. My suggestion is to explicitly specify that this depends on the value of regex, making it clearer. Something like this:

```Replace occurrences of pattern/regex in the Series/Index with some other string. Equivalent to str.replace() or re.sub() **depending on the _regex_ value**.```"
573206236,32359,Fixing RT02 pandas.Index.dropna and PR08 pandas.Index.fillna,zaki-indra,closed,2020-02-29T06:19:05Z,2020-03-06T17:00:16Z,"- [x] closes https://github.com/pandanistas/pandanistas_sprint_ui2020/issues/17
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
577139626,32502,DOC: `ref` issues in the contributing.rst file,kendricng,closed,2020-03-06T19:46:26Z,2020-03-06T20:38:15Z,"#### Problem description

Anytime I click on an internal reference link on the [Contributing.rst](https://github.com/pandas-dev/pandas/blob/master/doc/source/development/contributing.rst) file, I expect to be directed somewhere in the internal documentation that explains something about the internal link.

While it generates a `#id39` tag at the end of the link, it doesn't direct me to the page.

I suspect that this is not typical behavior. I just want to make sure this is the case before I post a PR on it."
556385260,31398,NTile function,xmnlab,closed,2020-01-28T18:21:05Z,2020-03-06T23:10:47Z,"
#### Problem description

I've investigated on pandas a `ntile` operation alternative. I just found `qcut` but it seems there is a difference between them (at least I could have the same results I expected).

for more details about `NTile`[3] :

> NTILE() function allows you to divide ordered rows in the partition into a specified number of ranked groups as equal size as possible. These ranked groups are called buckets.
> The NTILE() function assigns each group a bucket number starting from 1. For each row in a group, the NTILE() function assigns a bucket number representing the group to which the row belongs.

I have locally a python version of this function, is this something I could open a PR to pandas? if yes, I would be very happy to work on this PR.

any feedback and/or guidance would be very appreciated.

some references about ntile:

[1] https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions101.htm
[2] https://www.sqlservertutorial.net/sql-server-window-functions/sql-server-ntile-function/
[3]https://www.postgresqltutorial.com/postgresql-ntile-function/

thanks!"
576767628,32481,CLN: imports in pandas/io/excel/_base.py,ShaharNaveh,closed,2020-03-06T08:12:48Z,2020-03-07T11:00:43Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
573285591,32364,TST: Removed import of itertools,ShaharNaveh,closed,2020-02-29T12:27:05Z,2020-03-07T12:09:19Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
574401256,32406,"CLN: remove is_period_arraylike, is_datetime_arraylike",jbrockmendel,closed,2020-03-03T04:04:22Z,2020-03-07T15:11:18Z,There are better alternatives
576535592,32472,CLN: remove unreachable _internal_get_values in blocks,jbrockmendel,closed,2020-03-05T21:16:25Z,2020-03-07T15:14:48Z,"_putmask_smart is only called from Block.putmask, which ExtensionBlock overrides."
567049029,32078,REF: de-nest Series.__setitem__,jbrockmendel,closed,2020-02-18T17:32:32Z,2020-03-07T15:16:18Z,"Splitting this into a couple of steps for readability.  The two main things I have in mind here are a) de-nesting, b) try to parallel the structure of `__getitem__`"
570945522,32255,Implement __array__ on ExtensionIndex,jbrockmendel,closed,2020-02-26T00:13:48Z,2020-03-07T15:18:39Z,
577032823,32497,[Regression] DataFrame.replace throws TypeError when to_replace is list and DF has categorical,buhrmann,closed,2020-03-06T16:15:43Z,2020-03-07T16:21:15Z,"#### Code Sample

```python
df = pd.DataFrame({
    'x': pd.Series([-np.inf, np.inf]),
    'y': pd.Series([""a"", ""b""], dtype=""category"")
})

df.replace([-np.inf, np.inf], np.nan)
```
results in
```
...
~/anaconda/envs/grapy/lib/python3.7/site-packages/pandas/core/indexes/base.py in __contains__(self, key)
   3898     @Appender(_index_shared_docs[""contains""] % _index_doc_kwargs)
   3899     def __contains__(self, key) -> bool:
-> 3900         hash(key)
   3901         try:
   3902             return key in self._engine

TypeError: unhashable type: 'list'
```
#### Problem description

When a DF contains a categorical column, and when `to_replace` argument is a list, `replace()` throws `TypeError: unhashable type: 'list'`.

#### Expected Output
When the `to_replace` argument is a tuple rather than a list (which is not a documented option), all seems good:

```python
df.replace((-np.inf, np.inf), np.nan)
```

```
    x  y
0 NaN  a
1 NaN  b
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.2.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 20.0.2
setuptools       : 45.2.0.post20200209
Cython           : None
pytest           : 5.3.5
hypothesis       : None
sphinx           : 2.4.3
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.4.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.16.0
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : 0.48.0

</details>
"
572367902,32311,TST: refactored test_factorize,SaturnFromTitan,closed,2020-02-27T21:22:58Z,2020-03-07T18:35:35Z,"part of #23877
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
576999663,32495,"read_csv option 'skipfooter' crashes with lines starting with """,thomas-haslwanter,closed,2020-03-06T15:21:02Z,2020-03-07T19:17:21Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

# Generate the test-file
data_file = 'test.txt'

txt = """"""yyyy
1991
2002
""a
""""""

with open(data_file, 'w') as fh:
    fh.write(txt)
    
# Parsing working with:
df_ok = pd.read_csv(data_file, nrows=2)
print(df_ok)

# ... and with
df_ok2 = pd.read_csv(data_file, skipfooter=1, delimiter='[,]', engine='python')
print(df_ok2)

# ... but crashes with
df_crash1 = pd.read_csv(data_file, skipfooter=1, delimiter=',', engine='python')
print(df_crash1)

# ... and with 
df_crash2 = pd.read_csv(data_file, skipfooter=1)
print(df_crash2)
# in order to reach those last 2 lines, you have to comment out lines 24-25

# with the error message:
# ------------------------
# pandas.errors.ParserError: unexpected end of data. Error could possibly be
# due to parsing errors in the skipped footer rows (the skipfooter keyword is
# only applied after Python's csv library has parsed all rows).
# Process terminated with an exit code of 1


```
#### Problem description

Lines in txt-files that start with 
""
can lead to crashes when these lines should be skipped with the option 'skipfooter'.
Funnily, this does NOT happen then the option
delimiter='[,]', engine='python'
is set, but DOES crash when the same option is set with
delimiter=',', engine='python'

The same data are read in correctly when the number of rows is limited with the option 'nrows'.

#### Expected Output
   yyyy
0  1991
1  2002
   yyyy
0  1991
1  2002
   yyyy
0  1991
1  2002
   yyyy
0  1991
1  2002
#### Output of above code sample:

<details>

Traceback (most recent call last):
  File ""c:\Programs\WPy64-3740\python-3.7.4.amd64\lib\site-packages\pandas\io\parsers.py"", line 2854, in _next_iter_line
    return next(self.data)
_csv.Error: unexpected end of data

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""d:\Users\thomas\Data\CloudStation\Teaching\Stats\Coding\error_report.py"", line 24, in <module>
    df_crash1 = pd.read_csv(data_file, skipfooter=1, delimiter=',', engine='python')
  File ""c:\Programs\WPy64-3740\python-3.7.4.amd64\lib\site-packages\pandas\io\parsers.py"", line 702, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""c:\Programs\WPy64-3740\python-3.7.4.amd64\lib\site-packages\pandas\io\parsers.py"", line 435, in _read
    data = parser.read(nrows)
  File ""c:\Programs\WPy64-3740\python-3.7.4.amd64\lib\site-packages\pandas\io\parsers.py"", line 1139, in read
    ret = self._engine.read(nrows)
  File ""c:\Programs\WPy64-3740\python-3.7.4.amd64\lib\site-packages\pandas\io\parsers.py"", line 2388, in read
    content = self._get_lines(rows)
  File ""c:\Programs\WPy64-3740\python-3.7.4.amd64\lib\site-packages\pandas\io\parsers.py"", line 3128, in _get_lines
    row_num=self.pos + rows + 1)
  File ""c:\Programs\WPy64-3740\python-3.7.4.amd64\lib\site-packages\pandas\io\parsers.py"", line 2873, in _next_iter_line
    self._alert_malformed(msg, row_num)
  File ""c:\Programs\WPy64-3740\python-3.7.4.amd64\lib\site-packages\pandas\io\parsers.py"", line 2835, in _alert_malformed
    raise ParserError(msg)
pandas.errors.ParserError: unexpected end of data. Error could possibly be due to parsing errors in the skipped footer rows (the skipfooter keyword is only applied after Python's csv library has parsed all rows).
Process terminated with an exit code of 1

</details>
"
549202540,30985,DOC: remove Table of Contents on the contributing page,jorisvandenbossche,closed,2020-01-13T21:50:29Z,2020-03-07T20:03:53Z,"Currently, the contributing page (https://pandas.pydata.org/pandas-docs/version/1.0.0/development/contributing.html) starts with ""Table of Contents"". 
This PR removes that, because 1) we do that on none of the other pages in our docs and 2) with the new theme, there is a page table of contents in the right sidebar as well (although this shows, when landing on the page, only the first level)"
458008069,26941,DOC: Validate consistency of title capitalization,datapythonista,closed,2019-06-19T13:16:37Z,2020-03-07T20:06:26Z,"In #26933, we're making the capitalization of the title sections consisten. We use to have many titles capitalized as `This is the Section Title`, and we changed all them (probably few were forgotten) to `This is the section title`.

To keep this consistency, we should validate that the capitalization is correct in the CI. This can be done by extracting all the titles, and making sure that only the first letter of the sentence is uppercase, or words defined in a short list, like `Series`, `DataFrame`,...

I think this can be done in two ways:
- As a sphinx extension that validates the titles as they are processed, and generates warnings if they are not (this will automatically fail the CI).
- As an independent script

The first option should be simpler if sphinx can implement this as extension, but not sure if that's the case."
393079500,24373,date_range returns duplicated dates when used with dateutil over a range that includes the end of daylight saving time,lovasoa,closed,2018-12-20T14:52:39Z,2020-03-07T20:10:40Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
from dateutil.tz import gettz

TZ = 'Europe/Paris'
start = '2018-10-28T01:30' # Just before the end of daylight saving time in Paris
end = '2018-10-28T03:30' # Just after the end of daylight saving time in Paris

range_str = pd.date_range(
    start=pd.Timestamp(start, tz=TZ),
    end=pd.Timestamp(end, tz=TZ),
    freq='30min'
)


range_dateutil = pd.date_range(
    start=pd.Timestamp(start, tz=gettz(TZ)),
    end=pd.Timestamp(end, tz=gettz(TZ)),
    freq='30min'
)

assert((range_str.astype('str') == range_dateutil.astype('str')).all())
```

#### Problem description

In the example above `range_dateutil` is : 

```python
# range_dateutil
DatetimeIndex([
'2018-10-28 01:30:00+02:00',
'2018-10-28 02:00:00+02:00',
'2018-10-28 02:30:00+02:00',
'2018-10-28 02:00:00+02:00', # should be +01:00
'2018-10-28 02:30:00+02:00', # should be +01:00
'2018-10-28 03:00:00+01:00',
'2018-10-28 03:30:00+01:00'
],
dtype='datetime64[ns, tzfile('/usr/share/zoneinfo/Europe/Paris')]',
freq='30T')
``` 
In the example above, range_str is correct, but range_dateutil contains duplicated datetimes: 
'2018-10-28 02:00:00+02:00' and '2018-10-28 02:30:00+02:00'.

I am not sure whether the problem is in pandas or dateutil.

#### Expected Output

```python
# range_dateutil
DatetimeIndex([
'2018-10-28 01:30:00+02:00',
'2018-10-28 02:00:00+02:00',
'2018-10-28 02:30:00+02:00',
'2018-10-28 02:00:00+01:00',
'2018-10-28 02:30:00+01:00',
'2018-10-28 03:00:00+01:00',
'2018-10-28 03:30:00+01:00'
],
dtype='datetime64[ns, tzfile('/usr/share/zoneinfo/Europe/Paris')]',
freq='30T')
``` 

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.10.final.0
python-bits: 64
OS: Darwin
OS-release: 17.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.23.4
pytest: None
pip: 18.1
setuptools: 18.5
Cython: None
numpy: 1.15.4
scipy: 0.13.0b1
pyarrow: None
xarray: None
IPython: 5.8.0
sphinx: None
patsy: None
dateutil: 2.7.5
pytz: 2018.7
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 1.3.1
openpyxl: None
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
565730270,32008,DOC: Add description for tz parameter in DatetimeIndex,nsiregar,closed,2020-02-15T10:40:58Z,2020-03-07T20:37:13Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
561284341,31761,replaced old format strings,UnforgivenSoul,closed,2020-02-06T21:36:36Z,2020-03-07T21:17:30Z,"Changed the string formatting to f-strings.
ref #29547 
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
573179086,32352,DOC: Update pandas.core.groupby.GroupBy.pipe docstring,hsjsjsj009,closed,2020-02-29T05:09:34Z,2020-03-07T21:47:52Z,"- [x] closes https://github.com/pandanistas/pandanistas_sprint_ui2020/issues/2
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
565531690,31987,Feature request: option to set week starting on Sunday instead of Monday,guidopetri,closed,2020-02-14T20:06:38Z,2020-03-08T03:34:50Z,"#### Problem description

Currently, calling `pd.Series.dt.weekday` returns Monday = 0, Sunday = 6. It would be nice to have a settable option that allows Sunday to be the beginning of the week, e.g.

```python
my_series.dt.weekday  # Monday = 0, Sunday = 6
pd.options.week_start = 'Sunday'
my_series.dt.weekday  # Sunday = 0, Saturday = 6
```

Pandas version 1.0.1."

id,number,title,user,state,created_at,updated_at,body
753875140,38196,BUG: Avoid duplicates in DatetimeIndex.intersection,phofl,closed,2020-11-30T23:21:56Z,2020-12-02T12:19:55Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

@jbrockmendel I think we should call ``_convert_can_do_setop`` here too, but https://github.com/pandas-dev/pandas/blob/56b9a80e1e47e123eb7a7c01448d1506400f8a5b/pandas/tests/indexes/datetimes/test_setops.py#L472 tests the opposite
"
715987726,36927,BUG: Fix duplicates in intersection of multiindexes,phofl,closed,2020-10-06T20:08:06Z,2020-12-02T12:21:57Z,"- [x] closes #36915
- [x] xref #31326 (closes the intersection part)
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Seems like this was not introduced on purpose. Probably introduced in #31312"
754637954,38215,TST: add cases for nearest_workday and before_nearest_workday,ivanovmg,closed,2020-12-01T18:49:07Z,2020-12-02T14:27:49Z,"- [ ] xref #38139
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Add cases for testing observance rules ``nearest_workday`` and ``before_nearest_workday`` for their full coverage."
753884874,38197,BUG: name attr in RangeIndex.intersection,jbrockmendel,closed,2020-11-30T23:44:23Z,2020-12-02T14:40:30Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
754011387,38204,Retain views with listlike indexers setitem,jbrockmendel,closed,2020-12-01T05:03:22Z,2020-12-02T14:47:06Z,"- [x] closes #37954

xref #38148"
754851251,38223,TST: tighten testing assertions,jbrockmendel,closed,2020-12-02T01:21:51Z,2020-12-02T14:48:19Z,
753873909,38195,BUG: DataFrame.idxmin/idxmax with mixed dtypes,jbrockmendel,closed,2020-11-30T23:19:05Z,2020-12-02T14:49:20Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
680158314,35766,"Revert ""REGR: Fix interpolation on empty dataframe""",simonjayhawkins,closed,2020-08-17T11:21:11Z,2020-12-02T15:33:15Z,Reverts pandas-dev/pandas#35543
745886352,37943,BUG: fix sharey overwrite on area plots,lrusnac,closed,2020-11-18T18:00:52Z,2020-12-02T16:32:06Z,"- [x] closes #37942
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
708724966,36628,TST: check whether printing an IntervalIndex works (GH32553),MBrouns,closed,2020-09-25T07:48:04Z,2020-12-02T17:52:30Z,"- [x] closes #32553
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry (not applicable)

I just added a test that checks whether print succeeds, would we also want to check the exact output of the `str` function on an `IntervalIndex`?
"
598175638,33474,REF: unstack,jbrockmendel,closed,2020-04-11T03:00:36Z,2020-12-02T17:58:46Z,"cc @jreback this moves a small amount of the unstack logic up the call stack, which is worthwhile, but the big wins available are if you can help me figure out the questions in inline comments."
751123353,38071,BUG: DataFrame.loc returning empty result with negative stepsize for MultiIndex,phofl,closed,2020-11-25T21:19:03Z,2020-12-02T18:43:00Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This was easier than thought. Should I add a whatsnew note for this?"
660761068,35341,ENH: Select numeric ExtensionDtypes with DataFrame.select_dtypes,andrewgsavage,closed,2020-07-19T10:52:44Z,2020-12-02T18:57:44Z,"- [x] closes #35340
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
726604308,37313,CLN: always rebox_native in DatetimeLikeArray,jbrockmendel,closed,2020-10-21T15:34:22Z,2020-12-02T18:59:59Z,
750291619,38051,BUG: Index.drop raises on non-unique Index,jbrockmendel,closed,2020-11-25T02:21:50Z,2020-12-02T23:28:04Z,"```
index = pd.Index(range(3)).repeat(2)

>>> index.drop(1)
Traceback (most recent call last):
[...]
pandas.errors.InvalidIndexError: Reindexing only valid with uniquely valued Index objects
```

It is using Index.get_indexer, which is only for uniques indexes."
598503268,33494,BUG: MultiIndex drop produces wrong output if not lexicographically sorted,mrupp-citrine,closed,2020-04-12T14:56:04Z,2020-12-02T23:28:04Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.
  Latest Pandas version my system installs (via `anaconda update pandas`), which is 0.25.3.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
import pandas as pd

df1 = pd.DataFrame(
    [
        [4, 5, 6],
        [5, 4, 6],
        [4, 5, 6],
        [3, 7, 5],
        [5, 9, 0],
        [1, 2, 3],
    ],
    columns=[""a"", ""b"", ""c""],
)

df2 = pd.DataFrame(
    [
        [1, 2, 3],
        [4, 5, 6],
        [1, 6, 0],
    ],
    columns=[""a"", ""b"", ""c""],
)

df1mi = pd.MultiIndex.from_frame(df1)
df2mi = pd.MultiIndex.from_frame(df2)
ismi = df1mi.intersection(df2mi)  # intersection of df1 and df2
# df1mi, _ = df1mi.sortlevel(sort_remaining=True)
complement = df1mi.drop(ismi).to_frame().reset_index(drop=True)

print(df1, ""\n"")
print(df2, ""\n"")
print(complement)
```

#### Problem description

Above code computes the multi-set complement A-B for two multi-sets A and B (only difference to sets is that duplicates in A that are not in B are preserved).

Without the commented-out line, pandas issues a warning (`sys:1: PerformanceWarning: indexing past lexsort depth may impact performance.`). However, the result is wrong: Row 1 from df1 is missing in the complement.

With the commented-out line, no warning is issued and the result is correct.

The bug is that without the sorting, the drop command malfunctions. 

#### Expected Output

Correct output (with explicit sort):

```
   a  b  c
0  3  7  5
1  5  4  6
2  5  9  0
```

Wrong output (without explicit sort):

```
   a  b  c
0  3  7  5
1  5  9  0
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : 5.4.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
721486794,37115,REGR: unstack on 'int' dtype prevent fillna to work,ant1j,closed,2020-10-14T13:59:55Z,2020-12-02T23:30:04Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python

import pandas as pd
import numpy as np

df1 = pd.DataFrame({
    'a': ['A',  'A',  'B'], 
    'b': ['ca', 'cb', 'cb'],
    'v': [10] * 3,
})

df1 = df1.set_index(['a', 'b'])

# int column
df1['is_'] = 1

df1 = df1.unstack('b')

# Will not work, keeping NaN in the value
df1[('is_', 'ca')] = df1[('is_', 'ca')].fillna(0)

# Will raise ValueError: Cannot convert non-finite values (NA or inf) to integer
df1[('is_', 'ca')] = df1[('is_', 'ca')].astype('uint8')

```

#### Problem description

`df.unstack` is creating columns from a int column, but adds some NaNs as expected.

Trying to get rid of the NaN is not possible as `fillna` will silently fail and leave the column values as is.


#### Potential Source of the Problem

It looks like with this configuration, the Block for this `df1[('is_', 'ca')]` Series in the BlockManager is actually `IntBlock`, but holds a float dtype `np.ndarray` (!?).

From what I understand, as soon as the BlockManager operates a block consolidation (with the _consolidate function), the Block is converted to a `FloatBlock` and everything is OK.
e.g. a call to `cat2._is_mixed_type` will trigger a `_consolidate` and thus clear the issue.

#### Potential Solution Suggestion

Should the unstack function fires a consolidation of blocks, especially for column types that do not accept NaN values (like 'int' and 'bool' from numpy)?


#### Some potential workarounds identified so far

* Trigger a Block consolidate:
    * Add a column: the bug appeared if I use `fillna` before adding a column, but not after
    * Call  `cat2._is_mixed_type` 
    * Call `df1.infos()`: this was the most surprising one (it is because if triggers `df.count()` and thus consolidation)
* Use a float value: `df1['is_'] = 1.0`

#### Expected Output

```
      v       is_
b    ca    cb  ca   cb
a
A  10.0  10.0   1  1.0
B   NaN  10.0   0  1.0
``` 

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : fr_FR.UTF-8
LOCALE           : fr_FR.cp1252

pandas           : 1.1.3
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : 0.4.1
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
752988027,38157,BUG: assert_frame_equal exception for datetime #37609,ssortman,closed,2020-11-29T19:26:37Z,2020-12-02T23:31:34Z,"closes #37609 

- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Working with @richardwong8 on this issue.

This addresses issue #37609 where a check `needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype)` in assert_series_equal caused an assert_extension_array on left._values and right._values when DatetimeArray or TimedeltaArray were used. This caused an exception to be raised when either the right or left was not an instance of ExtensionArray but was used with DatetimeArray or TimedeltaArray. 

This would have been covered in the earlier check `is_extension_array_dtype(left.dtype) and is_extension_array_dtype(right.dtype)`, however, DatetimeArray and TimedeltaArray are still experimental and their dtypes are not instances of ExtensionDtype subclass."
372016853,23239,Duplicated column name causes inconsistent ValueError during assignment to a unique column,wmatern,closed,2018-10-19T15:53:30Z,2020-12-02T23:37:06Z,"#### Code Sample, a copy-pastable example if possible

```python
df1 = pd.DataFrame([[1,2,3,4]], columns=['C','D','D','a'])
df1['a'] = df1['a']

ValueError: Buffer has wrong number of dimensions (expected 1, got 0)
```
However, the following works without an error message:
```python
df1 = pd.DataFrame([[1,2,3,4]], columns=['C','B','B','a'])
df1['a'] = df1['a']
```
#### Problem description
I believe ValueError should not be thrown in the first example. The issue appears to only occur when there is a duplicated column. Interestingly, there appears to be some link to the alphabetical order of the column names. 

This appears similar to the issue reported in [#21668](https://github.com/pandas-dev/pandas/issues/21668).

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.0.final.0
python-bits: 64
OS: Linux
OS-release: 4.14.76-1-lts
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.4
pytest: None
pip: 18.0
setuptools: 40.4.3
Cython: None
numpy: 1.15.2
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 6.5.0
sphinx: 1.8.0
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.2.3
openpyxl: 2.5.7
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: 4.2.5
bs4: None
html5lib: 1.0.1
sqlalchemy: 1.2.12
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
755378246,38238,TST: revert too-ambitious part of #38223,jbrockmendel,closed,2020-12-02T15:33:20Z,2020-12-02T23:44:51Z,
755363599,38237,REF: consolidate result-casting in _cython_operation,jbrockmendel,closed,2020-12-02T15:17:11Z,2020-12-02T23:48:48Z,"Orthogonal to #38235.  After this and 38235, we'll be down to only two usages of maybe_cast_result, only one of which is sketchy."
755347808,38235,REF: casting in _python_agg_general,jbrockmendel,closed,2020-12-02T14:59:53Z,2020-12-02T23:59:59Z,"Consolidate two for-loops into one.

Instead of using maybe_cast_result in two places, use it only once, immediately after the function call being wrapped.  Replace those two usages with the less-invasive `maybe_downcast_to_dtype`"
755707191,38252,CI: delete unused import in groupby/groupby.py,arw2019,closed,2020-12-03T00:15:45Z,2020-12-03T02:35:19Z,"Fixes a `flake8` bug on current master:
```
(pandas-dev) andrewwieteska@Andrews-MacBook-Pro pandas % flake8 pandas/core/groupby/groupby.py
pandas/core/groupby/groupby.py:53:1: F401 'pandas.core.dtypes.cast.maybe_cast_result' imported but unused
```"
702934132,36401,TYP: alias IndexLabel without Optional ,ivanovmg,closed,2020-09-16T17:12:58Z,2020-12-03T07:52:52Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Previously in ``_typing.py`` we had the alias ``IndexLabel``, which contained ``Optional``.
The present PR eliminates ``Optional`` from the alias, but adds it in the code where necessary.

Related to the issue raised by @simonjayhawkins in https://github.com/pandas-dev/pandas/pull/36046#discussion_r485510442"
743280523,37862,CLN/TYP: pandas/io/formats/excel.py,ivanovmg,closed,2020-11-15T15:30:45Z,2020-12-03T07:54:23Z,"- [ ] xref #37715
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

- Handle two type ignore items in ``pandas/io/formats/excel.py``
- De-duplicate ``has_aliases`` variable by extracting property
- Add typing for iterators yielding ``ExcelCell``
- Use named arguments for ``ExcelCell`` on some occasions (in two places de-duplicate by updating kwargs inplace)"
755484607,38241,TST: setitem in presence of duplicate cols,ivanovmg,closed,2020-12-02T17:50:01Z,2020-12-03T07:55:24Z,"- [ ] closes #23239
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Add tests required to close the issue.
The tests are based on the example in the issue."
753107988,38171,TST: add comment that plotting tests are slow,ivanovmg,closed,2020-11-30T03:20:25Z,2020-12-03T07:56:14Z,"- [ ] xref https://github.com/pandas-dev/pandas/pull/38079#issuecomment-735262800
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Add comment (module docstring) in ``pandas/tests/plotting/common.py``,
mentioning that all plotting tests are marked as slow."
743502353,37887,TST: parametrize test_info,ivanovmg,closed,2020-11-16T06:09:56Z,2020-12-03T07:56:54Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Precursor for #37320
- Parametrize tests
- Rename some of test functions
"
743319901,37868,REF: prepare dataframe info for series info,ivanovmg,closed,2020-11-15T18:53:36Z,2020-12-03T07:57:23Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Precursor for #37320

Refactor ``pandas/io/formats/info.py`` to simplify further introduction of ``Series.info``.
Basically what is done here:
 - Move docstring to ``BaseInfo`` from ``DataFrameInfo`` - supposedly will be shared with ``SeriesInfo``
 - Move ``col_count``, ``ids`` from ``BaseInfo`` to ``DataFrameInfo`` (will not be relevant to ``SeriesInfo``)
 - Move ``memory_usage_bytes`` to ``DataFrameInfo`` (will have different implementation for ``SeriesInfo``)
 - Extract ``InfoPrinterAbstract``. Concrete classes will differ by the ``_create_table_builder``
 - Move common methods and properties from ``DataFrameTableBuilder`` to ``TableBuilderAbstract``
 - Extract ``TableBuilderVerboseMixin`` from ``DataFrameTableBuilderVerbose`` as the generator functions there will be used for verbose series info builders

I suggest moving the substitution decorator for the docstring from ``pandas/core/frame.py`` to ``pandas/io/formats/info.py`` later on in a separate PR."
739805809,37739,TYP: fix mypy ignored err in pandas/io/formats/format.py,ivanovmg,closed,2020-11-10T10:57:12Z,2020-12-03T07:57:38Z,"- [ ] xref #37715
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Handle ignored mypy error in ``pandas/io/formats/format.py``"
753154446,38173,BUG: array-like quantile fails on column groupby,GYHHAHA,closed,2020-11-30T05:22:31Z,2020-12-03T08:46:39Z,"- [x] closes #33795
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
755500742,38244,REGR: unstack on 'int' dtype prevent fillna to work,simonjayhawkins,closed,2020-12-02T18:14:43Z,2020-12-03T11:11:12Z,"- [ ] closes #37115
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
756099311,38259,Backport PR #38244: REGR: unstack on 'int' dtype prevent fillna to work,simonjayhawkins,closed,2020-12-03T11:10:30Z,2020-12-03T12:02:33Z,Backport PR #38244
623195996,34310,Fix: StringArray non-extensible due to inconsisent assertion,sbrugman,closed,2020-05-22T13:12:23Z,2020-12-03T13:34:57Z,"Fix for #34309

- [X] closes #34309
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
751591770,38085,TST : Categorical DataFrame.at overwritting row,ma3da,closed,2020-11-26T13:27:48Z,2020-12-03T13:54:21Z,"- [ ] closes #37763
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
731193131,37462,PERF: performance regression in timeseries/timestamp comparison (ASV),jorisvandenbossche,closed,2020-10-28T07:46:09Z,2020-12-03T14:19:35Z,"See https://pandas.pydata.org/speed/pandas/#arithmetic.Timeseries.time_series_timestamp_compare?python=3.8&Cython=0.29.21&p-tz=None&commits=ac7ca239-038aab96

This seems to be caused by https://github.com/pandas-dev/pandas/pull/37313 cc @jbrockmendel "
578047708,32553,IntervalIndex raises TypeError on PrettyPrint,blalterman,closed,2020-03-09T16:51:52Z,2020-12-03T14:23:34Z,"#### Code Sample, a copy-pastable example if possible

Consider the following series
```python
left = Float64Index([329.973, 345.137, 360.191, 376.313, 393.235, 411.969, 434.968, 464.453], dtype='float64')
right = pd.Float64Index([345.137, 360.191, 376.313, 393.235, 411.969, 434.968, 464.453, 507.679], dtype='float64')
idx = pd.IntervalIndex([pd.Interval(l, r) for l, r in zip(left, right)])
markers = pd.Series(['^', 'p', 'd', 'P', 'X', 'o', 's', 'v'],  index=idx)

```
#### Problem description

the `IntervalIndex` raises a `TypeError` when printing
```python
>>> markers

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/.conda/envs/test-env/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj)
    700                 type_pprinters=self.type_printers,
    701                 deferred_pprinters=self.deferred_printers)
--> 702             printer.pretty(obj)
    703             printer.flush()
    704             return stream.getvalue()

~/.conda/envs/test-env/lib/python3.7/site-packages/IPython/lib/pretty.py in pretty(self, obj)
    403                         if cls is not object \
    404                                 and callable(cls.__dict__.get('__repr__')):
--> 405                             return _repr_pprint(obj, self, cycle)
    406 
    407             return _default_pprint(obj, self, cycle)

~/.conda/envs/test-env/lib/python3.7/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)
    693     """"""A pprint that just redirects to the normal repr function.""""""
    694     # Find newlines and replace them with p.break_()
--> 695     output = repr(obj)
    696     lines = output.splitlines()
    697     with p.group():

~/.conda/envs/test-env/lib/python3.7/site-packages/pandas/core/series.py in __repr__(self)
   1369             min_rows=min_rows,
   1370             max_rows=max_rows,
-> 1371             length=show_dimensions,
   1372         )
   1373         result = buf.getvalue()

~/.conda/envs/test-env/lib/python3.7/site-packages/pandas/core/series.py in to_string(self, buf, na_rep, float_format, header, index, length, dtype, name, max_rows, min_rows)
   1435             max_rows=max_rows,
   1436         )
-> 1437         result = formatter.to_string()
   1438 
   1439         # catch contract violations

~/.conda/envs/test-env/lib/python3.7/site-packages/pandas/io/formats/format.py in to_string(self)
    358             )
    359 
--> 360         fmt_index, have_header = self._get_formatted_index()
    361         fmt_values = self._get_formatted_values()
    362 

~/.conda/envs/test-env/lib/python3.7/site-packages/pandas/io/formats/format.py in _get_formatted_index(self)
    338         else:
    339             have_header = index.name is not None
--> 340             fmt_index = index.format(name=True)
    341         return fmt_index, have_header
    342 

~/.conda/envs/test-env/lib/python3.7/site-packages/pandas/core/indexes/base.py in format(self, name, formatter, **kwargs)
    964             return header + list(self.map(formatter))
    965 
--> 966         return self._format_with_header(header, **kwargs)
    967 
    968     def _format_with_header(self, header, na_rep=""NaN"", **kwargs):

~/.conda/envs/test-env/lib/python3.7/site-packages/pandas/core/indexes/interval.py in _format_with_header(self, header, **kwargs)
   1000 
   1001     def _format_with_header(self, header, **kwargs):
-> 1002         return header + list(self._format_native_types(**kwargs))
   1003 
   1004     def _format_native_types(self, na_rep=""NaN"", quoting=None, **kwargs):

~/.conda/envs/test-env/lib/python3.7/site-packages/pandas/core/indexes/interval.py in _format_native_types(self, na_rep, quoting, **kwargs)
   1004     def _format_native_types(self, na_rep=""NaN"", quoting=None, **kwargs):
   1005         # GH 28210: use base method but with different default na_rep
-> 1006         return super()._format_native_types(na_rep=na_rep, quoting=quoting, **kwargs)
   1007 
   1008     def _format_data(self, name=None):

~/.conda/envs/test-env/lib/python3.7/site-packages/pandas/core/indexes/base.py in _format_native_types(self, na_rep, quoting, **kwargs)
   1026         Actually format specific types of the index.
   1027         """"""
-> 1028         mask = isna(self)
   1029         if not self.is_object() and not quoting:
   1030             values = np.asarray(self).astype(str)

~/.conda/envs/test-env/lib/python3.7/site-packages/pandas/core/dtypes/missing.py in isna(obj)
    124     Name: 1, dtype: bool
    125     """"""
--> 126     return _isna(obj)
    127 
    128 

~/.conda/envs/test-env/lib/python3.7/site-packages/pandas/core/dtypes/missing.py in _isna_old(obj)
    181         return False
    182     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):
--> 183         return _isna_ndarraylike_old(obj)
    184     elif isinstance(obj, ABCGeneric):
    185         return obj._constructor(obj._data.isna(func=_isna_old))

~/.conda/envs/test-env/lib/python3.7/site-packages/pandas/core/dtypes/missing.py in _isna_ndarraylike_old(obj)
    288         result = values.view(""i8"") == iNaT
    289     else:
--> 290         result = ~np.isfinite(values)
    291 
    292     # box

TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```




**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

When we change the index to a `categorical`, the visual representation prints
```python
>>> markers.index = pd.CategoricalIndex(markers.index)
>>> markers
(329.973, 345.137]    ^
(345.137, 360.191]    p
(360.191, 376.313]    d
(376.313, 393.235]    P
(393.235, 411.969]    X
(411.969, 434.968]    o
(434.968, 464.453]    s
(464.453, 507.679]    v
dtype: object
```


#### Output of ``pd.show_versions()``

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1062.4.3.el7.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : 5.3.5
hypothesis       : 5.5.4
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : 1.3.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.5
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
755486085,38242,TST: string representation of IntervalIndex,arw2019,closed,2020-12-02T17:52:12Z,2020-12-03T14:23:38Z,"- [x] closes #32553
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Picking up #36628"
756153753,38261,DOC: pandas.DataFrame.mask,ab-10,closed,2020-12-03T12:30:11Z,2020-12-03T15:38:56Z,"#### Location of the documentation

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mask.html#pandas-dataframe-mask

#### Documentation problem

The first line of docstring states:

>> Replace values where the condition is True.

However, `where` replaces the values for which the `cond` is False.

e.g.
```
zeros = pd.DataFrame(np.zeros(5))
zeros.where(zeros == 0.0, ""REPLACED"")
>>>
0.0
0.0
0.0
0.0
0.0


zeros.where(zeros != 0.0, ""REPLACED"")
>>>
REPLACED
REPLACED
REPLACED
REPLACED
REPLACED



#### Suggested fix for documentation

I think documentation should say ""Replace values where the condition is False.""

The rest of the `where` documentation should also be changed to reflect the actual behaviour of `where`.
"
755772192,38255,TST: split unfocused groupby test,jbrockmendel,closed,2020-12-03T02:57:59Z,2020-12-03T15:48:30Z,
755735610,38253,REF: simplify _cython_operation return ,jbrockmendel,closed,2020-12-03T01:24:09Z,2020-12-03T15:51:02Z,
755545801,38248,PERF: timeseries comparison ops,jbrockmendel,closed,2020-12-02T19:20:27Z,2020-12-03T15:51:22Z,"- [x] closes #37462
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Locally this gets me to within 3-4% of 1.1.4 on the asv mentioned in #37462"
755524418,38245,BUG: CategoricalDtype.__repr__ with RangeIndex categories,jbrockmendel,closed,2020-12-02T18:47:11Z,2020-12-03T16:01:44Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The problem was introduced a couple weeks ago, so shouldn't need a whatsnew."
743302436,37867,API: membership checks on ExtensionArray containing NA values,topper-123,closed,2020-11-15T17:22:18Z,2020-12-03T19:11:39Z,"Membership checks on ExtensionArrays containing `NA` values raises ValueError in some circumstances (but not in other):

```python
>>> arr1 = pd.array([""a"", pd.NA])
>>> arr2 = pd.array([pd.NA, ""a""])
>>> ""a"" in arr1
True  # ok
>>> ""a"" in arr2
TypeError: boolean value of NA is ambiguous  # not ok
>>> pd.NA in arr1
TypeError: boolean value of NA is ambiguous  # not ok
>>> pd.NA in arr2
True  # ok
```

So overall quite random failures. This PR fixes this problem by adding a custom `__contains__` method on `ExtensionArray`.

I assume that we want `pd.NA in arr1` to keep returning True. Note however that `np.nan in np.array([np.nan])` return False, so pandas' behaviour is different."
755586474,38250,Fix dict call for pandas/tests/util/test_assert_series_equal.py,UrielMaD,closed,2020-12-02T20:26:16Z,2020-12-03T21:26:41Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
756373951,38265,Rewrite dict call to literals,mavismonica,closed,2020-12-03T16:53:39Z,2020-12-03T21:39:58Z,"- [ ] xref #38138 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
Identify unnecessary dict call - rewrite as a literal in `pandas\core\indexes\numeric.py`
"
688340457,35964,BUG/CLN: Decouple Series/DataFrame.transform,rhshadrach,closed,2020-08-28T20:33:13Z,2020-12-03T21:44:40Z,"- [x] closes #35811
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

First step toward #35725. Currently `transform` just calls `aggregate`, and so if we are to forbid `aggregate` from transforming, these need to be decoupled. Other than the bugfix (#35811), the only other behavioral change is in the error messages.

Assuming the bugfix #35811 is the correct behavior, docs/whatsnew also needs to be updated.

I wasn't sure if tests should be marked with #35725 or perhaps this PR #. Any guidance here?"
751090409,38070,BUG: Index.drop raising Error when Index has duplicates,phofl,closed,2020-11-25T20:11:11Z,2020-12-03T22:14:42Z,"- [x] closes #38051
- [x] closes #33494
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

@jbrockmendel  This should deal with duplicates. For MultiIndex sometimes a slice with stepzize greater than zero was given, which dropped to many elements"
756311955,38263,C408 rewrite dict call to dict literals,vmdhhh,closed,2020-12-03T15:40:40Z,2020-12-03T22:58:47Z,"- [ ] xref #38138
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
Identify unnecessary dict call - rewrite as a literal in  `./pandas/tests/io/pytables/test_store.py`, `./pandas/core/arrays/interval.py`

"
756599310,38275,CLN: assorted cleanups,jbrockmendel,closed,2020-12-03T21:20:24Z,2020-12-04T00:02:17Z,
753206341,38176,The error message for unsupported index on shift() could be more explicit,ZhihuiChen0903,closed,2020-11-30T06:54:41Z,2020-12-04T02:00:17Z,"When the type of data index is not datetime-like, the following error message will raise:
`NotImplementedError: Not supported for type Index`
 
The error message could be more useful by explicitly stating which types of data that `shift` supports.
 
Expected error message:
` f""This method is only implemented for DatetimeIndex, PeriodIndex and TimedeltaIndex; Got type {type(self).__name__}.""`
"
756421913,38268,BUG: name retention in Index.difference corner cases,jbrockmendel,closed,2020-12-03T17:58:15Z,2020-12-04T02:08:54Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
461268659,27071,BUG: preserve categorical & sparse types when grouping / pivot,jreback,closed,2019-06-27T02:14:22Z,2020-12-04T12:56:36Z,"closes #18502 
replaces #26550 "
752464339,38122,BUG: Index.intersection casting to object instead of numeric,jbrockmendel,closed,2020-11-27T20:39:46Z,2020-12-04T13:40:48Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
756624714,38276,Removing dict calls,joao-zanutto,closed,2020-12-03T22:00:40Z,2020-12-04T14:22:42Z,"- [ ] xref #38138 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Removed the dict calls for the files:

`./pandas/core/arrays/floating.py`
`./pandas/core/arrays/numpy_.py`
`./pandas/core/arrays/interval.py`"
756123595,38260,C408 fix,gerardjorgensen,closed,2020-12-03T11:45:08Z,2020-12-04T14:23:41Z,"- [x] xref #38138 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Rewrote dict calls to dict literals in the following files
./pandas/tests/io/json/test_ujson.py
./pandas/tests/io/json/test_normalize.py

Also changed list calls to literals in line 730 of the following file
pandas/tests/io/json/test_ujson.py"
756583206,38274,BUG: pandas.dataframe.drop_duplicates  ignore_index parameter,dannemarre,closed,2020-12-03T20:53:31Z,2020-12-04T14:24:58Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import unittest
import pandas as pd

class Test(unittest.TestCase):
    def testError(self):
        cars = {'Brand': ['Honda Civic','Toyota Corolla','Ford Focus','Audi A4'],
                'Price': [22000,25000,27000,35000]
                }

        df = pd.DataFrame(cars, columns = ['Brand', 'Price'])
        dataError = df.drop_duplicates(ignore_index='error')
        print(""Should give ValueError: \n""+
            ""   ValueError: For argument ignore_index expected type bool, received type str."")

if __name__ == '__main__' :
    unittest.main()
```

#### Problem description

Hi! So I am doing white testing for school and found a tiny bug in drop_duplicates. When using the boolean parameter ""ignore_index"" it does not check if it is boolean or not. The ""inplace"" parameter is checked using the function validate_bool_kwarg(--). The ""ignore_index"" should be checked the same way. I have never reported a bug and I could fix it myself but I am a bit afraid to mess things up. Thank you!

#### Expected Output

ValueError: For argument ""ignore_index"" expected type bool, received type str.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.104-microsoft-standard
Version          : #1 SMP Wed Feb 19 06:37:35 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.2
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
756774581,38283,REF: de-duplicate intersection methods,jbrockmendel,closed,2020-12-04T03:42:10Z,2020-12-04T15:14:59Z,"De-duplicating the PeriodIndex, IntervalIndex, and MultiIndex methods is blocked by #38251"
756792094,38284,TST: fixturize unfocused test,jbrockmendel,closed,2020-12-04T04:31:04Z,2020-12-04T15:16:05Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
755656436,38251,ENH: implement _should_compare/_is_comparable_dtype for all Index subclasses,jbrockmendel,closed,2020-12-02T22:27:22Z,2020-12-04T15:20:27Z,"Fixes some problems in get_indexer_non_unique that AFAICT are not actually causing any problems.

Categorical kludge is needed because of #38240."
753039517,38162,REF: implement _ea_wrap_cython_operation,jbrockmendel,closed,2020-11-29T23:08:08Z,2020-12-04T15:44:03Z,"I've identified some bugs in _cython_operation.  As a preliminary to fixing them, this splits out the EA-handling from the ndarray-handling.  This will also make it easier to keep track of what we need to generalize if we want this to support arbitrary EAs."
713178127,36785,REGR: Row series are broken after applying to_datetime(),krassowski,closed,2020-10-01T20:56:11Z,2020-12-04T16:21:24Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
from io import StringIO
from pandas import read_csv, to_datetime, options

df = read_csv(StringIO(""""""\
,A,B,C,D,E,F
P0,,2020-10-01 08:00:00+00:00,,,,2020-10-16 00:01:00+00:00
""""""), index_col=0)

# works
options.display.max_rows = 6
df.apply(lambda d: to_datetime(d, utc=True), axis=0).apply(lambda x: str(x), axis=1)

# raises
options.display.max_rows = 5
df.apply(lambda d: to_datetime(d, utc=True), axis=0).apply(lambda x: str(x), axis=1)

# raises
df.apply(lambda d: to_datetime(d, utc=True), axis=0).apply(lambda x: x.dropna().apply(lambda y: getattr(y, 'year')), axis=1)
# in v 1.0.4 would return:
#        B     F
# P0  2020  2020
```

#### Problem description

Assigning the output of `pd.to_datetime` to a column of a dataframe, although not demonstrated in documentation is [a popular use of this helper function](https://stackoverflow.com/a/26763793/6646912). In previous versions of pandas (1.0.x) it was possible to convert multiple columns using `to_datetime` in combination with `DataFrame.apply`. It is still possible in 1.1.2 and master:

```python
>>>df.apply(lambda d: to_datetime(d, utc=True), axis=0).dtypes
A    datetime64[ns, UTC]
B    datetime64[ns, UTC]
            ...         
E    datetime64[ns, UTC]
F    datetime64[ns, UTC]
Length: 6, dtype: object
```

However, the rows in the subsequent apply operations are broken for certain operations. For example, trying to print them out raises: `TypeError: cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid`. This was not the case in pandas 1.0.4.

```python-traceback
X in Y
     10 
     11 # raises
---> 12 df.apply(lambda d: to_datetime(d, utc=True), axis=0).apply(lambda x: str(x), axis=1)

/pandas/core/frame.py in apply(self, func, axis, raw, result_type, args, **kwds)
   7545             kwds=kwds,
   7546         )
-> 7547         return op.get_result()
   7548 
   7549     def applymap(self, func) -> ""DataFrame"":

/pandas/core/apply.py in get_result(self)
    178             return self.apply_raw()
    179 
--> 180         return self.apply_standard()
    181 
    182     def apply_empty_result(self):

/pandas/core/apply.py in apply_standard(self)
    253 
    254     def apply_standard(self):
--> 255         results, res_index = self.apply_series_generator()
    256 
    257         # wrap results

/pandas/core/apply.py in apply_series_generator(self)
    282                 for i, v in enumerate(series_gen):
    283                     # ignore SettingWithCopy here in case the user mutates
--> 284                     results[i] = self.f(v)
    285                     if isinstance(results[i], ABCSeries):
    286                         # If we have a view on v, we need to make a copy because

X in <lambda>(x)
     10 df.apply(lambda x: str(x), axis=1)
     11 # raises
---> 12 df.apply(lambda d: to_datetime(d, utc=True), axis=0).apply(lambda x: str(x), axis=1)

/pandas/core/series.py in __repr__(self)
   1313         show_dimensions = get_option(""display.show_dimensions"")
   1314 
-> 1315         self.to_string(
   1316             buf=buf,
   1317             name=self.name,

/pandas/core/series.py in to_string(self, buf, na_rep, float_format, header, index, length, dtype, name, max_rows, min_rows)
   1372             String representation of Series if ``buf=None``, otherwise None.
   1373         """"""
-> 1374         formatter = fmt.SeriesFormatter(
   1375             self,
   1376             name=name,

/pandas/io/formats/format.py in __init__(self, series, buf, length, header, index, na_rep, name, float_format, dtype, max_rows, min_rows)
    259         self.adj = _get_adjustment()
    260 
--> 261         self._chk_truncate()
    262 
    263     def _chk_truncate(self) -> None:

/pandas/io/formats/format.py in _chk_truncate(self)
    283             else:
    284                 row_num = max_rows // 2
--> 285                 series = concat((series.iloc[:row_num], series.iloc[-row_num:]))
    286             self.tr_row_num = row_num
    287         else:

/pandas/core/reshape/concat.py in concat(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    272     ValueError: Indexes have overlapping values: ['a']
    273     """"""
--> 274     op = _Concatenator(
    275         objs,
    276         axis=axis,

/pandas/core/reshape/concat.py in __init__(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)
    357                     ""only Series and DataFrame objs are valid""
    358                 )
--> 359                 raise TypeError(msg)
    360 
    361             # consolidate

TypeError: cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid
```

#### Expected Output

Should not raise.

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.8.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-48-generic
Version          : #52-Ubuntu SMP Thu Sep 10 10:58:49 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.2
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : 5.3.4
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.4.2
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.11.1
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.49.0
</details>
"
753051978,38164,REF: avoid try/except in wrapping in cython_agg_blocks,jbrockmendel,closed,2020-11-30T00:11:09Z,2020-12-04T16:21:25Z,
756706187,38280,pandas/core/strings/accesory.py Unnecessary dict call - rewrite as a literal #38138,UrielMaD,closed,2020-12-04T00:52:36Z,2020-12-04T17:57:30Z,"xref #38138

File modified: pandas/core/strings/accesory.py

Changed unnecessary dict calls for flake8 (C408) standard:

- Rewrite dict() as {}

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
757333374,38295,Backport PR #38272 on branch 1.1.x (BUG: DataFrame.apply with axis=1 and EA dtype),meeseeksmachine,closed,2020-12-04T19:04:18Z,2020-12-04T20:03:20Z,Backport PR #38272: BUG: DataFrame.apply with axis=1 and EA dtype
754889125,38225,ENH: context-manager for chunksize/iterator-reader,twoertwein,closed,2020-12-02T02:54:26Z,2020-12-04T20:40:23Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Allows (and encourages) the following use:
```py
import pandas as pd

filename = ""pandas/tests/io/data/csv/iris.csv""
chunksize = 2
with pd.read_csv(filename, chunksize=chunksize) as reader:
    for chunk in reader:
        # risky code that might raise
```
Same can be done for `read_json/sas` (I think these are all methods that support `chunksize`). If this PR should make it into 1.2, I can quickly add the changes for json/sas as well.

Are there more places to promote this new context manager?"
757413352,38298,New dict call fixings #38138,UrielMaD,closed,2020-12-04T21:20:04Z,2020-12-04T23:27:19Z,"xref #38138

File modified: pandas/core/strings/accesory.py

Changed unnecessary dict calls for flake8 (C408) standard:

Rewrite dict() as {}


- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
754918524,38227,BUG: values are dropped when grouping on a variables with NaNs,randomgambit,closed,2020-12-02T04:02:55Z,2020-12-04T23:45:03Z,"Hello there,

Consider this simple example

```
pd.__version__
Out[158]: '1.1.3'

df = pd.DataFrame({'col': [1,2,3],
                   'group' : ['a',np.NaN,'b']})

df
Out[160]: 
   col group
0    1     a
1    2   NaN
2    3     b

df.groupby('group').apply(lambda x: x)
Out[161]: 
   col group
0  1.0     a
1  NaN   NaN
2  3.0     b
```

There are a few things that are puzzling.

1. I thought `groupby` would drop the NA groups by default
2. Why is the variable `col` set to missing?

This looks like a bug to me, unless I am missing something (apologies if this is the case)
What do you think?

Thanks!"
756647055,38277,BUG: Validate drop_duplicates ignore_index argument for bool,phofl,closed,2020-12-03T22:36:46Z,2020-12-04T23:46:21Z,"- [x] closes #38274
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Saw this on the tracker. I think we should validate this, some inputs could lead to weird results.
The last drop duplicates entry in whatsnew was also under reshaping, so put it there"
756328114,38264,"REF: implement Groupby idxmin, idxmax without fallback",jbrockmendel,closed,2020-12-03T16:00:17Z,2020-12-05T00:00:49Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
757461164,38301,BUG: Fix typo in melt value_name futurewarning,fmaguire,closed,2020-12-04T22:57:54Z,2020-12-05T01:57:12Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Just removing the duplicated 'i' in ""resultiing"" of the futurewarning in melt about using `value_name` with the same name as a column the in the original dataframe.  Filing an issue or a whatsnew entry for such a minor change seemed excessive."
753947720,38200,TYP: @final in groupby,jbrockmendel,closed,2020-12-01T02:21:11Z,2020-12-05T02:22:34Z,
757146916,38290,REF: move common arithmetic ops from Integer/FloatingArray to NumericArray,jorisvandenbossche,closed,2020-12-04T14:32:41Z,2020-12-05T09:23:06Z,xref https://github.com/pandas-dev/pandas/issues/38110
754909911,38226,Rewrite dict literal for files in pandas/tests/frame/test_indexing and test_where ,Qbiwan,closed,2020-12-02T03:40:20Z,2020-12-05T10:01:46Z,"- [ ] closes #38138
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
identify unnecessary dict call - rewrite as a literal
Rewrites as dictionary literals for the following files in two spots:
- pandas/tests/frame/test_indexing
- pandas/tests/frame/test_where 
based on @jreback comments:
https://github.com/pandas-dev/pandas/pull/38207#issuecomment-736960208"
753167549,38175,ENH: support nanoseconds in Period constructor,arw2019,closed,2020-11-30T05:47:11Z,2020-12-05T13:13:44Z,"- [x] closes #34621
- [x] closes #17053
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Picking up #34720

ASVs:
```
asv continuous -f 1.1 upstream/master HEAD -b ^period

       before           after         ratio
     [3f510600]       [2b2f1d04]
     <GH34621-nanos-for-Period^2>       <GH34621-nanos-for-Period>
+      7.46±0.2ms       11.6±0.1ms     1.55  period.PeriodIndexConstructor.time_from_ints('D', False)
+      7.57±0.3ms      11.4±0.03ms     1.51  period.PeriodIndexConstructor.time_from_ints('D', True)
+      52.8±0.6ms       59.2±0.4ms     1.12  period.PeriodIndexConstructor.time_from_ints_daily('D', True)
+      52.9±0.9ms       59.0±0.1ms     1.11  period.PeriodIndexConstructor.time_from_ints_daily('D', False)
-      3.21±0.2μs      2.90±0.02μs     0.90  period.Indexing.time_get_loc

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE DECREASED. 
```

cc @jreback https://github.com/pandas-dev/pandas/pull/34720#discussion_r491536763 in case we want this for 1.2"
740211174,37748,BUG: ValueError is mistakenly raised if a numpy array is assigned to a pd.Series of dtype=object and both have the same length,nocluebutalotofit,closed,2020-11-10T20:13:59Z,2020-12-05T17:14:04Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

pd.__version__ #  '1.1.3'
pdseries = pd.Series(index=[1,2,3,4], dtype=object)
pdseries.loc[1] = np.zeros(100)  # this works fine
pdseries.loc[3] = np.zeros(4)     # this raises a value error because len(pdseries)==len(np.zeros(4))
```

TypeError: only size-1 arrays can be converted to Python scalars
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""/Users/daniel/.conda/envs/production_system/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 2878, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-40-460230264bf1>"", line 1, in <module>
    pdseries.loc[3] = np.zeros(4)
  File ""/Users/daniel/.conda/envs/production_system/lib/python3.7/site-packages/pandas/core/indexing.py"", line 670, in __setitem__
    iloc._setitem_with_indexer(indexer, value)
  File ""/Users/daniel/.conda/envs/production_system/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1802, in _setitem_with_indexer
    self.obj._mgr = self.obj._mgr.setitem(indexer=indexer, value=value)
  File ""/Users/daniel/.conda/envs/production_system/lib/python3.7/site-packages/pandas/core/internals/managers.py"", line 534, in setitem
    return self.apply(""setitem"", indexer=indexer, value=value)
  File ""/Users/daniel/.conda/envs/production_system/lib/python3.7/site-packages/pandas/core/internals/managers.py"", line 406, in apply
    applied = getattr(b, f)(**kwargs)
  File ""/Users/daniel/.conda/envs/production_system/lib/python3.7/site-packages/pandas/core/internals/blocks.py"", line 887, in setitem
    values = values.astype(arr_value.dtype, copy=False)
ValueError: setting an array element with a sequence.

#### Problem description

It is possible to assign (numpy) arrays to elements of pandas.Series ofd type=object. Unfortunately, in case the array is of the same size as the Series a ValueError is raised.

How can one avoid this error?

#### Expected Output

The interesting thing is that the assignment takes place as expected:
In[42]: pdseries
Out[42]: 
1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...
2                                                  NaN
3                                 [0.0, 0.0, 0.0, 0.0]
4                                                  NaN

One might argue that a warning could be useful but an error is misleading and tricky to debug.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.8.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Mon Aug 31 22:12:52 PDT 2020; root:xnu-6153.141.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8
pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 49.6.0.post20201009
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 5.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : 1.3.20
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
756452596,38271,BUG: setting object dtype Series row with an Extension Array,arw2019,closed,2020-12-03T18:41:21Z,2020-12-05T17:14:05Z,"On 1.2 master:
``` python
In [11]: import numpy as np
    ...: import pandas as pd
    ...: 
    ...: ser = pd.Series(1, index=list(""abcde""), dtype=""object"")
    ...: 
    ...: expected = pd.array([0,0,3,0,0])
    ...: ser.loc[""a""] = expected
    ...: result = ser[0]

In [12]: ser
Out[12]: 
a    0
b    0
c    3
d    0
e    0
dtype: Int64
```
I expect:
``` python
In [14]: ser
Out[14]: 
a    [0, 0, 3, 0, 0]
b               1
c               1
d               1
e               1
dtype: object
```"
732419775,37486,BUG: ValueError when assigning list in a single-row object dataframe  ,paxcema,closed,2020-10-29T15:31:16Z,2020-12-05T17:14:05Z,"- [ X ] I have checked that this issue has not already been reported. #15490 looks kind of similar, but seems like it was fixed. #17256 is also similar but it uses deprecated methods.

- [ X ] I have confirmed this bug exists on the latest version of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame([['a', '1', '2.0']], columns=['a', 'b', 'c'], dtype=object)
df['c'].iloc[0] = [df['c'].iloc[0]]         
```

#### Problem description

The current behaviour raises a ValueError, even though the assignment happened. Stack trace:


<details>

```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-12824bd17b66> in <module>()
      2 
      3 df = pd.DataFrame([['a', '1', '2.0']], columns=['a', 'b', 'c'], dtype=object)
----> 4 df['c'].iloc[0] = [df['c'].iloc[0]]

4 frames
/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py in __setitem__(self, key, value)
    668 
    669         iloc = self if self.name == ""iloc"" else self.obj.iloc
--> 670         iloc._setitem_with_indexer(indexer, value)
    671 
    672     def _validate_key(self, key, axis: int):

/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py in _setitem_with_indexer(self, indexer, value)
   1800             # actually do the set
   1801             self.obj._consolidate_inplace()
-> 1802             self.obj._mgr = self.obj._mgr.setitem(indexer=indexer, value=value)
   1803             self.obj._maybe_update_cacher(clear=True)
   1804 

/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py in setitem(self, indexer, value)
    532 
    533     def setitem(self, indexer, value) -> ""BlockManager"":
--> 534         return self.apply(""setitem"", indexer=indexer, value=value)
    535 
    536     def putmask(

/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py in apply(self, f, align_keys, **kwargs)
    404                 applied = b.apply(f, **kwargs)
    405             else:
--> 406                 applied = getattr(b, f)(**kwargs)
    407             result_blocks = _extend_blocks(applied, result_blocks)
    408 

/usr/local/lib/python3.6/dist-packages/pandas/core/internals/blocks.py in setitem(self, indexer, value)
    885             values[indexer] = value
    886 
--> 887             values = values.astype(arr_value.dtype, copy=False)
    888 
    889         # set

ValueError: setting an array element with a sequence
```

</details>


#### Expected Output

As the `dtype` is `object`, I would have expected this assignment to happen without raising a ValueError. If the dataframe has more than one row:

```python
df = pd.DataFrame([['a', '1', '2.0']]*2, columns=['a', 'b', 'c'])
df['c'].iloc[0] = [df['c'].iloc[0]]
```

Then the exception is not thrown.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.112+
Version          : #1 SMP Thu Jul 23 08:00:38 PDT 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.18.5
pytz             : 2018.9
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 50.3.0
Cython           : 0.29.21
pytest           : 3.6.4
hypothesis       : None
sphinx           : 1.8.5
blosc            : None
feather          : 0.4.1
xlsxwriter       : None
lxml.etree       : 4.2.6
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.6.1 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 5.5.0
pandas_datareader: 0.9.0
bs4              : 4.6.3
bottleneck       : 1.3.2
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 2.5.9
pandas_gbq       : 0.13.3
pyarrow          : 0.14.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.20
tables           : 3.4.4
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.1.0
xlwt             : 1.3.0
numba            : 0.48.0

</details>
"
756727428,38282,BUG: IntervalIndex.union with mismatched dtypes both empty,jbrockmendel,closed,2020-12-04T01:44:54Z,2020-12-05T17:31:01Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Ever so slowly ironing out the wrinkles between the setops."
748360088,38009,CLN: fix E741 ambiguous variable #34150,fangchenli,closed,2020-11-22T21:59:12Z,2020-12-05T18:49:58Z,"xref #34150
"
743090812,37856,DEP: update Versioneer,fangchenli,closed,2020-11-14T22:25:58Z,2020-12-05T18:50:45Z,"bump versioneer from 0.15 to 0.19

```
(pandas-dev) fangchenli@Fangchens-MacBook-Pro pandas-fangchenli % python setup.py version
running version
keywords are unexpanded, not using
got version from VCS {'version': '1.2.0.dev0+1185.g82894a4b6', 'full-revisionid': '82894a4b6329b8af74bb9c593858453a01b91bf6', 'dirty': False, 'error': None, 'date': '2020-11-14T16:24:02-0600'}
Version: 1.2.0.dev0+1185.g82894a4b6
 full-revisionid: 82894a4b6329b8af74bb9c593858453a01b91bf6
 dirty: False
 date: 2020-11-14T16:24:02-0600
```"
747849008,37983,CLN: remove panel compat shim,fangchenli,closed,2020-11-20T23:00:54Z,2020-12-05T18:51:15Z,"remove panel shim
"
718705267,37039,CI: move py39 build to conda #33948,fangchenli,closed,2020-10-10T21:20:55Z,2020-12-05T18:51:43Z,"- [x] closes #33948

"
752378890,38116,CLN: fix flake8 C408 part 2,fangchenli,closed,2020-11-27T16:58:38Z,2020-12-05T18:52:47Z,followup of #38078
751253012,38078,"CLN: fix flake8 C406, C409, and some of C408",fangchenli,closed,2020-11-26T03:29:26Z,2020-12-05T18:53:01Z,"
"
747145308,37973,BLD: set inplace in setup.cfg,fangchenli,closed,2020-11-20T04:42:33Z,2020-12-05T18:53:25Z,"specify inplace for build_ext in setup.cfg file
"
757534433,38309,TST: tighten xfails,jbrockmendel,closed,2020-12-05T04:08:30Z,2020-12-05T19:26:16Z,
753600469,38187,"BUG: groupby.rank for dt64tz, period dtypes",jbrockmendel,closed,2020-11-30T16:01:42Z,2020-12-05T20:35:34Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This sits on top of #38162"
756460021,38272,BUG: DataFrame.apply with axis=1 and EA dtype,jbrockmendel,closed,2020-12-03T18:51:39Z,2020-12-05T21:01:42Z,"- [x] closes #36785
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
757381553,38297,DOC: Pandas Pg 47 label/61 doc merge() link redirects to first page,davidyakobovitch,open,2020-12-04T20:25:00Z,2020-12-05T22:50:20Z,"#### Location of the documentation

Page 47

[this should provide the location of the documentation, e.g. ""pandas.read_csv"" or the URL of the documentation, e.g. ""https://dev.pandas.io/docs/reference/api/pandas.read_csv.html""]

**Note**: You can check the latest versions of the docs on `master` [here](https://pandas.pydata.org/docs/dev/).

#### Documentation problem

merge() link is broken and links back to the first page

[this should provide a description of what documentation you believe needs to be fixed/improved]

#### Suggested fix for documentation

[this should explain the suggested fix and **why** it's better than the existing documentation]
"
756402858,38266,BUG: Inserting array of same size with Series.loc raises ValueError,ma3da,closed,2020-12-03T17:33:05Z,2020-12-06T00:28:19Z,"closes #37748
closes #37486
closes #38271
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
251039024,17272,"BUG: Index._searchsorted_monotonic(..., side='right') returns the left side position for monotonic decreasing indexes",jschendel,closed,2017-08-17T18:55:50Z,2020-12-06T03:59:04Z,"- [X] closes #17271
- [X] closes #16417
- [X] tests added / passed
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry

I didn't add a whatsnew entry for the `Index._searchsorted_monotonic` fix, since it looks like past convention has been to not add whatsnew entries for private methods.  Happy to add a whatsnew entry if I'm mistaken though.  I did add a whatsnew entry for the downstream effect on IntervalIndex."
757650648,38314,BUG: Sliced subclassed Series returns full data result when custom method is applied,guillaume-marion,closed,2020-12-05T10:31:34Z,2020-12-06T08:55:20Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
import pandas as pd

class _Subseries(pd.Series):

   _metadata = ['custom_method']

   @property
   def _constructor(self):
       return _Subseries

   @property
   def _constructor_expanddim(self):
       return _Subseries

   def custom_method(self, value):
       return self + value

s = _Subseries(range(5))
s = s.iloc[-3:]
s.custom_method(1)
```

#### Problem description

The above code returns:
0&nbsp;&nbsp;&nbsp;1
1&nbsp;&nbsp;&nbsp;2
2&nbsp;&nbsp;&nbsp;3
3&nbsp;&nbsp;&nbsp;4
4&nbsp;&nbsp;&nbsp;5

A similar method, such as .add(), on a sliced Series will return the result of that method on the sliced data and not on the original full data. I would expect this to be the same in a subclassed version of a Series.

#### Expected Output
2&nbsp;&nbsp;&nbsp;3
3&nbsp;&nbsp;&nbsp;4
4&nbsp;&nbsp;&nbsp;5


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.9.0.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_Belgium.1252

pandas           : 1.1.4
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.3.1
setuptools       : 49.6.0.post20201009
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
252517734,17323,TestCategoricalIndex.test_reindexing failing on Travis / Mac build,jorisvandenbossche,closed,2017-08-24T07:56:54Z,2020-12-06T12:48:39Z,"https://travis-ci.org/pandas-dev/pandas/jobs/267760269

Master and some PRs (strangely not all) are all failing on the same test, but only on Mac:

```
=================================== FAILURES ===================================
_____________________ TestCategoricalIndex.test_reindexing _____________________
[gw1] darwin -- Python 3.5.4 /Users/travis/miniconda3/envs/pandas/bin/python
self = <pandas.tests.indexes.test_category.TestCategoricalIndex object at 0x111d5e860>
    def test_reindexing(self):
    
        ci = self.create_index()
        oidx = Index(np.array(ci))
    
        for n in [1, 2, 5, len(ci)]:
            finder = oidx[np.random.randint(0, len(ci), size=n)]
            expected = oidx.get_indexer_non_unique(finder)[0]
    
            actual = ci.get_indexer(finder)
>           tm.assert_numpy_array_equal(expected, actual)
pandas/tests/indexes/test_category.py:389: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pandas/util/testing.py:1174: in assert_numpy_array_equal
    _raise(left, right, err_msg)
pandas/util/testing.py:1157: in _raise
    .format(obj=obj), left.shape, right.shape)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
obj = 'numpy array', message = 'numpy array shapes are different', left = (14,)
right = (6,), diff = None
    def raise_assert_detail(obj, message, left, right, diff=None):
        if isinstance(left, np.ndarray):
            left = pprint_thing(left)
        if isinstance(right, np.ndarray):
            right = pprint_thing(right)
    
        msg = """"""{obj} are different
    
    {message}
    [left]:  {left}
    [right]: {right}"""""".format(obj=obj, message=message, left=left, right=right)
    
        if diff is not None:
            msg += ""\n[diff]: {diff}"".format(diff=diff)
    
>       raise AssertionError(msg)
E       AssertionError: numpy array are different
E       
E       numpy array shapes are different
E       [left]:  (14,)
E       [right]: (6,)
pandas/util/testing.py:1105: AssertionError
```
"
749923653,38044,CLN: Remove .values from groupby.sem,rhshadrach,closed,2020-11-24T17:58:15Z,2020-12-06T14:03:54Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Closure of #22046 allows removing the workaround."
749912034,38043,TYP: Add cast to ABC Index-like types,rhshadrach,closed,2020-11-24T17:40:31Z,2020-12-06T14:03:54Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @simonjayhawkins 

For the added type-ignore, the attribute `inferred_freq` is added dynamically onto the index instance."
748291027,38002,DOC: Whatsnew 1.2.0 cleanup,rhshadrach,closed,2020-11-22T17:26:09Z,2020-12-06T14:04:00Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Cleanup of spelling, grammar, and links.

----

cc @jorisvandenbossche, @dsaxton, @phofl: 

Using ``:meth:`.DataFrameGroupBy.quantile` `` (note the `.`) displays the text `DataFrameGroupBy.quantile` and sphinx finds the link to the proper page. Similar remarks apply to `Rolling` and others. From the [sphinx docs](https://www.sphinx-doc.org/en/master/usage/restructuredtext/domains.html):

> Also, if the name is prefixed with a dot, and no exact match is found, the target is taken as a suffix and all object names with that suffix are searched. 

I don't believe this was known about (at least to me!) and is different from what was aligned on in #37145, so I wanted to get thoughts here on which is preferred. While I've added many cross-links using this, `DataFrameGroupBy.quantile` is the sole example where I have replaced a preexisting link.

----

Other notable changes that are worth a mention to make sure they are correct:
 * Numpy is always referred to as NumPy in plain-text (no backticks)
 * When a method exists on only Series and DataFrame, replaced unlinked reference with a reference to each. For example, ``:meth:`read_csv` `` becomes ``:meth:`DataFrame.to_csv` and :meth:`Series.to_csv` ``
 * dtypes and argument values are always double-backticked
 * Moved the following note from Other to Missing

> Bug in :meth:`Series.nunique` with ``dropna=True`` was returning incorrect results when both ``NA`` and ``None`` missing values were present (:issue:`37566`)"
719004855,37066,TYP: Misc groupby typing,rhshadrach,closed,2020-10-12T03:08:20Z,2020-12-06T14:04:47Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
733607983,37531,CLN/TYP: Alias for aggregation dictionary argument,rhshadrach,closed,2020-10-31T02:11:29Z,2020-12-06T14:04:50Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
733610335,37532,TYP: obj in aggregation.py,rhshadrach,closed,2020-10-31T02:25:13Z,2020-12-06T14:04:51Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
738485836,37702,CLN: Simplify groupby head/tail tests,rhshadrach,closed,2020-11-08T15:08:24Z,2020-12-06T14:04:53Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Going to add axis=1 tests as part of resolution of #9772, simplifying test first."
739410203,37727,ENH: Raise when subsetting columns on groupby with axis=1,rhshadrach,closed,2020-11-09T22:29:32Z,2020-12-06T14:04:54Z,"- [x] closes #37725
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
741182974,37778,BUG: Groupby head/tail with axis=1 fails,rhshadrach,closed,2020-11-12T00:53:11Z,2020-12-06T14:04:57Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This potentially closes #9772. The other issue there involves the perhaps unexpected fact that using `axis=1` with `groupby(...).apply` does _not_ transpose the groups that the function is going to be applied to. I've expressed my thoughts on why we should not do so in https://github.com/pandas-dev/pandas/issues/9772#issuecomment-723498819."
757999363,38333,Backport PR #38330: REGR: Groupby first/last/nth treats None as an observation,simonjayhawkins,closed,2020-12-06T19:11:55Z,2020-12-06T20:33:04Z,Backport PR #38330
738420691,37699,Addressed comments for doc update,taytzehao,closed,2020-11-08T08:44:47Z,2020-12-06T23:48:52Z,"-Arranged alignment, and addressed comments for the document enhancement."
595385269,33338,DOC/API: Discussion on whether __finalize__ is public,TomAugspurger,open,2020-04-06T19:53:42Z,2020-12-07T08:10:53Z,"Currently `NDFrame.__finalize__` is not documented. We only have a couple references to it in the whatsnew.

I have a few questions around its API

1. Is it considered public? Can subclasses call `self.__finalize__()`?
2. Are the values passed in `method='...'` considered public?
"
615108511,34089,QST: Distinguishing row and column slices in DataFrame subclass,zhenruiliao,closed,2020-05-09T06:46:53Z,2020-12-07T08:15:27Z,"- [X ] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com/questions/61246252/distinguish-column-and-row-slice-in-pandas-subclass).

---

Hi pandas devs,

What is the best way to distinguish whether a row or column was sliced in a DataFrame subclass? I am designing a subclass where row slices should return my custom Series/DF subclass, but columns are just vanilla pd.Series. In addition, I want non-slice calls to `_constructor` (e.g., `apply`) to simply return vanilla DataFrames. Is there a straightforward way to do this?

Thanks for a great package!"
155364158,13208,Series __finalized__ not correctly called in binary operators,jdfekete,closed,2016-05-17T21:17:29Z,2020-12-07T08:17:00Z,"```
#!/bin/env python
""""""
Example bug in derived Pandas Series.

__finalized__ is not called in arithmetic binary operators, but it is in in some booleans cases.

>>> m = MySeries([1, 2, 3], name='test')
>>> m.x = 42
>>> n=m[:2]
>>> n
0    1
1    2
dtype: int64
>>> n.x
42
>>> o=n+1
>>> o
0    2
1    3
dtype: int64
>>> o.x
Traceback (most recent call last):
        ...
AttributeError: 'MySeries' object has no attribute 'x'
>>> m = MySeries([True, False, True], name='test2')
>>> m.x = 42
>>> n=m[:2]
>>> n
0     True
1    False
dtype: bool
>>> n.x
42
>>> o=n ^ True
>>> o
0    False
1     True
dtype: bool
>>> o.x
42
>>> p = n ^ o
>>> p
0    True
1    True
dtype: bool
>>> p.x
42

""""""

import pandas as pd

class MySeries(pd.Series):
    _metadata = ['x']

    @property
    def _constructor(self):
        return MySeries

if __name__ == ""__main__"":
    import doctest
    doctest.testmod()

```
#### Expected Output

In all cases, the metadata 'x' should be transferred from the passed values when applying binary operators.
When the right-hand value is a constant, the left-hand value metadata should be used in **finalize** for arithmetic operators, just like it is for Boolean binary operators.
When two series are used in binary operators, some resolution should be possible in **finalize**.
I would pass the second (right-hand) value by calling **finalize**(self, other=other), leaving the resolution to the derived class implementer, but there might be a smarter approach.
#### output of `pd.show_versions()`

pd.show_versions()
## INSTALLED VERSIONS

commit: None
python: 2.7.6.final.0
python-bits: 64
OS: Linux
OS-release: 3.19.0-59-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.18.1
nose: 1.3.7
pip: None
setuptools: 20.2.2
Cython: 0.24
numpy: 1.11.0
scipy: 0.17.0
statsmodels: 0.6.1
xarray: None
IPython: 4.0.1
sphinx: 1.3.1
patsy: 0.4.0
dateutil: 2.4.2
pytz: 2015.7
blosc: None
bottleneck: 1.0.0
tables: 3.2.2
numexpr: 2.5.2
matplotlib: 1.5.0
openpyxl: 2.2.6
xlrd: 0.9.4
xlwt: 1.0.0
xlsxwriter: 0.7.7
lxml: 3.4.4
bs4: 4.4.1
html5lib: 0.9999999
httplib2: None
apiclient: None
sqlalchemy: 1.0.9
pymysql: None
psycopg2: None
jinja2: 2.8
boto: 2.38.0
pandas_datareader: None
"
94509278,10553,Subclassed attributes are not serializable,kjordahl,closed,2015-07-11T21:36:54Z,2020-12-07T08:17:00Z,"In a subclass of pandas objects, pickling an object doesn't serialize properties of an instance of that subclass, even if the attribute has been added to `_metadata`. It would be hard to override this behavior entirely in the subclass, because it will require updates to `__getstate__` and `__setstate__`, and probably also an addition or subclass of `BlockManager`. It would be nice if the `_metadata` serialization was handled in the base pandas class.

Example:

```
class SubDataFrame(DataFrame):

    _metadata = ['my_data']

    @property
    def _constructor(self):
        return SubDataFrame

sdf = SubDataFrame()
sdf.my_data = 'foo'
sdf.to_pickle('tmp.pkl')
new_sdf = read_pickle('tmp.pkl')
new_sdf.my_data
```

raises `AttributeError: 'SubDataFrame' object has no attribute 'my_data'`

(edited original example for correctness)
"
211842337,15563,Preserve subclass family on reshape operations,delgadom,closed,2017-03-04T01:14:08Z,2020-12-07T08:17:01Z,"Subclassed Series and DataFrames with custom `_constructor`, `_constructor_sliced`, and `_constructor_expanddim` methods return pandas Series and DataFrame objects on reshape operations:

```python
>>> import numpy as np, pandas as pd
>>> import pandas.util.testing as tm
>>> df = tm.SubclassedDataFrame(np.random.random((4,5)))
>>> df
          0         1         2         3         4
0  0.151944  0.948561  0.639122  0.718071  0.296193
1  0.004606  0.978378  0.827614  0.479320  0.495945
2  0.158874  0.535476  0.173194  0.065292  0.365851
3  0.772392  0.105038  0.671064  0.164165  0.795803
>>> type(df)
<class 'pandas.util.testing.SubclassedDataFrame'>
```

Slicing operations use `_constructor_sliced`:
```python
>>> type(df[0])
<class 'pandas.util.testing.SubclassedSeries'>
```

Reshape operations return regular pandas objects:
```python
>>> type(df.stack())
<class 'pandas.core.series.Series'>
```

It would be great if this returned a `SublcassedSeries`. Same goes for unstack and pivot."
299092443,19822,Subclassing is lost when using DataFrame.apply,jaumebonet,closed,2018-02-21T19:12:30Z,2020-12-07T08:17:03Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

# Define a subclass of Series
class ExtendedSeries( pd.Series ):
    @property
    def _constructor(self):
        return ExtendedSeries

# Define a subclass of DataFrame that slices into the ExtendedSeries class
class ExtendedFrame( pd.DataFrame ):
    @property
    def _constructor(self):
        return ExtendedFrame
    _constructor_sliced = ExtendedSeries

# Declare instance of ExtendedFrame
df = ExtendedFrame({""c1"": [1, 2, 3, 4], ""c2"": [2, 3, 4, 5], ""c3"": [3, 4, 5, 6]})

print(type(df))
# <class '__main__.ExtendedFrame'>

print(type(df.iloc[0]))
# <class '__main__.ExtendedSeries'>

for x in df.apply(lambda x: type(x), axis=1):
    print(x)
#<class 'pandas.core.series.Series'>
#<class 'pandas.core.series.Series'>
#<class 'pandas.core.series.Series'>
#<class 'pandas.core.series.Series'>
```

#### Problem description

The class type of the _applied_ objects should still be `<class '__main__.ExtendedSeries'>` instead of going back to `<class 'pandas.core.series.Series'>`.

The issue here is that in `pandas.core.apply`, whenever the returning objects are generated, they are declared as `DataFrame` and `Series` instead of `self.obj._constructor` and `self.obj._constructor_slice`.

This behaviour makes it difficult to properly work with subclasses from pandas. 

#### Expected Output

Following the initial code example, the expected output should be:
```python
import pandas as pd

# Define a subclass of Series
class ExtendedSeries( pd.Series ):
    @property
    def _constructor(self):
        return ExtendedSeries

# Define a subclass of DataFrame that slices into the ExtendedSeries class
class ExtendedFrame( pd.DataFrame ):
    @property
    def _constructor(self):
        return ExtendedFrame
    _constructor_sliced = ExtendedSeries

# Declare instance of ExtendedFrame
df = ExtendedFrame({""c1"": [1, 2, 3, 4], ""c2"": [2, 3, 4, 5], ""c3"": [3, 4, 5, 6]})

print(type(df))
# <class '__main__.ExtendedFrame'>

print(type(df.iloc[0]))
# <class '__main__.ExtendedSeries'>

for x in df.apply(lambda x: type(x), axis=1):
    print(x)
#<class '__main__.ExtendedSeries'>
#<class '__main__.ExtendedSeries'>
#<class '__main__.ExtendedSeries'>
#<class '__main__.ExtendedSeries'>
```
(note the change in the last printed lines)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.12.final.0
python-bits: 64
OS: Darwin
OS-release: 17.4.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.21.1
pytest: None
pip: 9.0.1
setuptools: 38.4.0
Cython: None
numpy: 1.14.0
scipy: 0.18.1
pyarrow: None
xarray: None
IPython: 5.4.1
sphinx: 1.6.6
patsy: 0.4.1
dateutil: 2.6.1
pytz: 2017.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.1.1
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: 4.5.1
html5lib: 0.999999999
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
683683726,35843,"REGR: Series.__repr__ is broken for SparseDtype(""datetime64[ns]"")",dsaxton,closed,2020-08-21T16:42:32Z,2020-12-07T09:44:49Z,"Regression due to https://github.com/pandas-dev/pandas/commit/1fa063552eef57ecec9c6a867ff2ae4bb3961e3d :

```python
import pandas as pd

print(pd.__version__)

values = [pd.Timestamp('2012-05-01T01:00:00.000000'), pd.Timestamp('2016-05-01T01:00:00.000000')]
arr = pd.arrays.SparseArray(values)
ser = pd.Series(arr)
ser
```

```python
1.2.0.dev0+147.g07983803b
Out[1]: ---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~/opt/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/IPython/core/formatters.py in __call__(self, obj)
    700                 type_pprinters=self.type_printers,
    701                 deferred_pprinters=self.deferred_printers)
--> 702             printer.pretty(obj)
    703             printer.flush()
    704             return stream.getvalue()

~/opt/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/IPython/lib/pretty.py in pretty(self, obj)
    392                         if cls is not object \
    393                                 and callable(cls.__dict__.get('__repr__')):
--> 394                             return _repr_pprint(obj, self, cycle)
    395
    396             return _default_pprint(obj, self, cycle)

~/opt/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)
    698     """"""A pprint that just redirects to the normal repr function.""""""
    699     # Find newlines and replace them with p.break_()
--> 700     output = repr(obj)
    701     lines = output.splitlines()
    702     with p.group():

~/pandas/pandas/core/series.py in __repr__(self)
   1315         show_dimensions = get_option(""display.show_dimensions"")
   1316
-> 1317         self.to_string(
   1318             buf=buf,
   1319             name=self.name,

~/pandas/pandas/core/series.py in to_string(self, buf, na_rep, float_format, header, index, length, dtype, name, max_rows, min_rows)
   1386             max_rows=max_rows,
   1387         )
-> 1388         result = formatter.to_string()
   1389
   1390         # catch contract violations

~/pandas/pandas/io/formats/format.py in to_string(self)
    356
    357         fmt_index, have_header = self._get_formatted_index()
--> 358         fmt_values = self._get_formatted_values()
    359
    360         if self.truncate_v:

~/pandas/pandas/io/formats/format.py in _get_formatted_values(self)
    341
    342     def _get_formatted_values(self) -> List[str]:
--> 343         return format_array(
    344             self.tr_series._values,
    345             None,

~/pandas/pandas/io/formats/format.py in format_array(values, formatter, float_format, na_rep, digits, space, justify, decimal, leading_space, quoting)
   1179     )
   1180
-> 1181     return fmt_obj.get_result()
   1182
   1183

~/pandas/pandas/io/formats/format.py in get_result(self)
   1210
   1211     def get_result(self) -> List[str]:
-> 1212         fmt_values = self._format_strings()
   1213         return _make_fixed_width(fmt_values, self.justify)
   1214

~/pandas/pandas/io/formats/format.py in _format_strings(self)
   1467
   1468         if not isinstance(values, DatetimeIndex):
-> 1469             values = DatetimeIndex(values)
   1470
   1471         if self.formatter is not None and callable(self.formatter):

~/pandas/pandas/core/indexes/datetimes.py in __new__(cls, data, freq, tz, normalize, closed, ambiguous, dayfirst, yearfirst, dtype, copy, name)
    269         name = maybe_extract_name(name, data, cls)
    270
--> 271         dtarr = DatetimeArray._from_sequence(
    272             data,
    273             dtype=dtype,

~/pandas/pandas/core/arrays/datetimes.py in _from_sequence(cls, data, dtype, copy, tz, freq, dayfirst, yearfirst, ambiguous)
    314         freq, freq_infer = dtl.maybe_infer_freq(freq)
    315
--> 316         subarr, tz, inferred_freq = sequence_to_dt64ns(
    317             data,
    318             dtype=dtype,

~/pandas/pandas/core/arrays/datetimes.py in sequence_to_dt64ns(data, dtype, copy, tz, dayfirst, yearfirst, ambiguous)
   1957     if is_datetime64tz_dtype(data_dtype):
   1958         # DatetimeArray -> ndarray
-> 1959         tz = _maybe_infer_tz(tz, data.tz)
   1960         result = data._data
   1961

AttributeError: 'SparseArray' object has no attribute 'tz'
```

xref https://github.com/pandas-dev/pandas/issues/35762"
757987138,38332,"REGR: Fix Index construction from Sparse[""datetime64[ns]""]",simonjayhawkins,closed,2020-12-06T18:10:59Z,2020-12-07T09:44:58Z,"- [ ] closes #35843
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
757747206,38318,DOC: 1.1.5 release date,simonjayhawkins,closed,2020-12-05T17:52:30Z,2020-12-07T09:48:56Z,
758346738,38341,"Backport PR #38332 on branch 1.1.x (REGR: Fix Index construction from Sparse[""datetime64[ns]""])",meeseeksmachine,closed,2020-12-07T09:45:23Z,2020-12-07T10:39:13Z,"Backport PR #38332: REGR: Fix Index construction from Sparse[""datetime64[ns]""]"
758349470,38342,Backport PR #38318 on branch 1.1.x (DOC: 1.1.5 release date),meeseeksmachine,closed,2020-12-07T09:49:02Z,2020-12-07T10:46:09Z,Backport PR #38318: DOC: 1.1.5 release date
727746132,37350,DISC: Use of assertions in user-facing code,dsaxton,closed,2020-10-22T21:54:58Z,2020-12-07T11:16:50Z,"There are a variety of places in the code base where we have some bare assertions that can be somewhat confusing when encountered by end-users (e.g., https://github.com/pandas-dev/pandas/issues/35509). As @WillAyd pointed out these are sometimes useful in convincing mypy of something's type, but is this nonetheless something we want to avoid as a general rule? If indeed the assertion is valid then it shouldn't actually raise, but if it can then probably we could come up with a more appropriate error type / message than an empty assertion."
757764584,38319,Removed unnecessary dict calls,Arun12121,closed,2020-12-05T19:30:38Z,2020-12-07T13:34:34Z,"xref #38138

File modified:
pandas/tests/io/parser/test_quoting.py
pandas/tests/io/parser/test_usecols.py

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
757525822,38306,TYP/BUG: fix passing incorrect type to interpolate_1d,jbrockmendel,closed,2020-12-05T03:31:29Z,2020-12-07T15:30:20Z,
757515576,38304,TST: share transform tests,jbrockmendel,closed,2020-12-05T02:30:40Z,2020-12-07T15:31:04Z,
757534922,38310,TST: avoid skipping in frame arith test,jbrockmendel,closed,2020-12-05T04:11:33Z,2020-12-07T15:34:02Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
757783107,38321,CLN: remove now-unnecesary coerce_to_dtypes,jbrockmendel,closed,2020-12-05T21:23:26Z,2020-12-07T15:37:10Z,"Made unnecessary bc we now operate blockwise, so coercion is handled at a lower level."
757836534,38327,CLN: dtypes.cast,jbrockmendel,closed,2020-12-06T03:33:12Z,2020-12-07T15:39:04Z,
756666881,38279,BUG: isin numeric vs string,jbrockmendel,closed,2020-12-03T23:16:56Z,2020-12-07T17:28:41Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Using the new hashtables!

```
In [3]: arr = np.arange(10**7).astype(np.int32)

In [4]: %timeit isin(arr, arr[:10])
88.8 ms ± 497 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)   # <-- master
42.7 ms ± 385 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)   # <-- PR
```"
756870770,38286,BUG: GroupBy.first does not skip missing values in string-valued columns,amalcgcg,closed,2020-12-04T07:18:02Z,2020-12-07T17:56:54Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Create a DataFrame with numbers and missing entries, first() on groupby works as expected,
# skipping the missing leading entry in column b
>>> df2=pd.DataFrame({""a"": [4,4,4,4,4,4], ""b"": [None,3,None,3,3,3]})
>>> df2.groupby(""a"").first()
     b
a
4  3.0

# Create a similar DataFrame, but with strings and missing entries, first() on groupby 
# does not work as expected, it doesn't skip the missing leading entry
df2=pd.DataFrame({""a"": [4,4,4,4,4,4], ""b"": [None,""foo"",None,""foo"",""foo"",""foo""]})
>>> df2.groupby(""a"").first()
      b
a
4  None
```

#### Problem description

One utility of the GroupBy.first() function is its coalescing behavior--it skips missing values within groups. This behavior does not seem to happen when a column has string values, which creates two problems: 1) type-dependent behavior of an otherwise generic method, and 2) I'm not able to find a workaround to get the behavior I'd like (returning the first non-missing string in a group.

#### Expected Output

I would expect the output of the second example above to be ""foo"", not None. If the behavior above is the expected behavior of Pandas, then please provide the correct code to use in order to extract the first non-missing string in a groupby column.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Oct 29 22:56:45 PDT 2020; root:xnu-6153.141.2.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.4
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.2.0
Cython           : None
pytest           : 6.1.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
758712293,38345,TST: revert too-ambitious,jbrockmendel,closed,2020-12-07T17:43:50Z,2020-12-07T20:49:53Z,
757549140,38313,REF: simplify casting of scalars to array,jbrockmendel,closed,2020-12-05T05:49:53Z,2020-12-07T20:53:24Z,"cast_scalar_to_array is only used in two places, and in each of them there is a better option available."
758047028,38337,REF: avoid accessing private iloc methods,jbrockmendel,closed,2020-12-06T23:08:34Z,2020-12-07T20:54:00Z,"I'm pretty sure this is made possible by @phofl's recent fix to only call `_align_series` in `_setitem_with_indexer` when `name != ""iloc""`"
757960992,38330,REGR: Groupby first/last/nth treats None as an observation,rhshadrach,closed,2020-12-06T16:13:18Z,2020-12-07T21:55:13Z,"- [x] closes #38286
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This is a regression from 1.0.x to 1.1.x, introduced by #33462. Assuming it doesn't get into the 1.2 rc, not sure if it should go into 1.2 during the rc phase or wait until 1.3.

Had to decide on some edge-case behaviors in odd situations with missing values, see https://github.com/pandas-dev/pandas/issues/38286#issuecomment-739521117"
753799186,38192,DOC: Fixed to_pickle compression argument list 'gzip' > 'gz',ooojpeg,closed,2020-11-30T20:59:11Z,2020-12-07T22:04:43Z,"Removed reference to gzip and replaced with correct extension of gz.

- [x] closes #35364


<img width=""842"" alt=""Screenshot 2020-11-30 at 20 43 59"" src=""https://user-images.githubusercontent.com/74271903/100664097-17d1f200-334f-11eb-90e3-e37ef7ce7c66.png"">
"
753825949,38194,DOC: Clarify development environment creation for documentation changes,ooojpeg,closed,2020-11-30T21:44:23Z,2020-12-07T22:05:28Z,"Changed wording on paragraph to make clear advantages/disadvantages of creating development environment.

- [x] closes #38193"
755976023,38257,BUG: groupby.apply on the NaN group drops values with original axes return,GYHHAHA,closed,2020-12-03T08:22:13Z,2020-12-08T00:39:56Z,"- [x] closes #38227
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
494740732,28486,Add support for schema parameter when calling io.sql.get_schema,bbibentyo,closed,2019-09-17T16:57:48Z,2020-12-08T02:44:48Z,"Currently, there is no way to specify the schema when calling pandas.io.sql.get_schema, the schema is a helpful parameter when working with certain databases such as MSSQL.

```python
import pandas.io.sql

# current state
table_name = 'my_new_table'
sql_create_statement = pandas.io.sql.get_schema(df, table_name, con)
# create statement looks like
""""""
CREATE TABLE [my_new_table] (
    [column1] VARCHAR(max) NULL,
    ....
)
""""""
```

this adds an extra step to have to parse the file in order to add the schema, otherwise when I execute the script later, the table is created in user default schema (dbo in most cases). This issue can be easily addressed by adding  `schema` parameter to `_create_sql_schema` since `SQLTable` already supports `schema`

```python

import pandas.io.sql

# future state
table_name = 'my_new_table'
schema = 'pypi'
sql_create_statement = pandas.io.sql.get_schema(df, table_name, schema=schema)

# create statement will look like this for MSSQL if con parameter is passed
""""""
CREATE TABLE pypi.[my_new_table] (
    [Column1],
    ....
)
""""""
```

"
752837294,38146,"ENH: add ""schema"" kwarg to io.sql.get_schema method",arw2019,closed,2020-11-29T04:41:13Z,2020-12-08T02:45:00Z,"- [x] closes  #28486
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Picking up #33278"
748521753,38014,BUG: Fixes plotting with nullable integers (#32073),rkc007,closed,2020-11-23T06:47:28Z,2020-12-08T02:59:26Z,"- [x] closes #32073
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry


"
755995932,38258,TST: mark all excel tests as slow,ivanovmg,closed,2020-12-03T08:52:05Z,2020-12-08T03:49:09Z,"- [ ] xref #38091
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Marked all excel-related tests as slow.
There are a lot of time for setup and teardown for every test.
Overall, moving these tests to ""slow"" category may probably save about 5 minutes of test time for ""not slow"" pipelines."
548446960,30922,ENH:column-wise DataFrame.fillna and duplicated DataFrame.fillna with Series and Dict ,proost,closed,2020-01-11T16:01:47Z,2020-12-08T04:28:34Z,"- [x] closes #4514
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

description:
Access ""DataFrame"" each column, fills up NA values using ""Series.fillna"" 
and what i found is if dataframe is duplicated, "".fillna"" not guarantee filling NA. So i change them can fill NA.

Q. Why access column base not index base?
A. problem of Index base f"".illna"" is not preserve column's dtype. so i use column base.

Q. Why assign in column new values not use inplace=True ?
A. To avoid chained indexing. If column and index are both duplicated, chained indexing happens, i want to avoid this situation.
"
721862269,37125,Add test_masking_duplicate_columns,GabrielSimonetto,closed,2020-10-14T23:34:51Z,2020-12-08T05:47:57Z,"- [x] closes #31954
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I don't know what an whatsnew entry is...

This PR adds a test in order to solve an issue"
449599522,26552,Deprecate xlwt,WillAyd,closed,2019-05-29T05:03:47Z,2020-12-08T10:11:54Z,"Similar to the conversation in #26487 xlwt isn't maintained. If we deprecate this could open up the path to simplify our Excel writers / extensions and supported file types going forward.

I would think this is also much easier than deprecating xlrd since we have other writers already implemented."
750628678,38057,PERF: fix regression in creation of resulting index in RollingGroupby,jorisvandenbossche,closed,2020-11-25T09:03:16Z,2020-12-08T14:36:41Z,"Closes #38038

TODO: fix corner cases, add benchmark, .."
758839152,38346,PERF: isin casting integers to object,jbrockmendel,closed,2020-12-07T20:52:06Z,2020-12-08T15:45:01Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref https://github.com/pandas-dev/pandas/pull/38279#issuecomment-739973166 cc @jorisvandenbossche "
759486714,38361,DOC: minor fixes to 1.2 release notes,simonjayhawkins,closed,2020-12-08T14:22:06Z,2020-12-08T15:51:18Z,
759480864,38360,CI: activate azure pipelines/github actions on 1.2.x,simonjayhawkins,closed,2020-12-08T14:14:33Z,2020-12-08T15:56:23Z,
564806342,31954,"BUG: ""cannot reindex from duplicate axis"" thrown using unique indexes, duplicated column names and a specific numpy array values",igorluppi,closed,2020-02-13T16:33:54Z,2020-12-08T15:58:34Z,"#### Code Sample

```
import pandas 
import numpy as np

a = np.array([[1,2],[3,4]]) 

# DO NOT WORKS
b = np.array([[0.5,6],[7,8]])  
# b = np.array([[.5,6],[7,8]])  # The same problem

# This one works fine:
# b = np.array([[5,6],[7,8]]) 

dfA = pandas.DataFrame(a)
# This works fine EVEN using .5, because the columns name is different
# dfA = pandas.DataFrame(a, columns=['a','b'])
dfB = pandas.DataFrame(b)

df_new = pandas.concat([dfA, dfB], axis = 1)

print(df_new[df_new > 5])
```


#### Problem description

It has a bug that combines numpy specific values and duplicated DataFrame column names when it's used a select operation, such as `df[df > 5]`. A exception is thrown saying ""cannot reindex from duplicate axis"", however It should not be, because:

- The DataFrame has no duplicated indexes ( `df.index.is_unique` is `True`)
- The DataFrame has duplicated column names, but should not be a problem when we apply the selection operation, such as `df_new[df_new > 5]`
- The DataFrame uses `float `or `int` numpy values, so it should not change the behavior of the code

**However** the values in the numpy array DO changes the behavior of the DataFrame selection, if the DataFrame has duplicated column names.

#### Expected Output

```
    0   1    0  1
0 NaN NaN  NaN  6
1 NaN NaN  7.0  8
```

#### Current Output 

```
~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer)
   3097         # trying to reindex on an axis with duplicates
   3098         if not self.is_unique and len(indexer):
-> 3099             raise ValueError(""cannot reindex from a duplicate axis"")
   3100 
   3101     def reindex(self, target, method=None, level=None, limit=None, tolerance=None):

ValueError: cannot reindex from a duplicate axis
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-28-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : pt_BR.UTF-8

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 2.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.1
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
759086242,38354,TST: a weird bug with numpy + DataFrame with duplicate columns,arw2019,closed,2020-12-08T05:47:26Z,2020-12-08T15:58:38Z,"- [x] closes #31954
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Picking up #37125"
757259479,38291,ENH: use correct dtype in groupby cython ops when it is known (without try/except),jbrockmendel,closed,2020-12-04T17:09:05Z,2020-12-08T16:07:35Z,"Addresses part of #37494. And xref https://github.com/pandas-dev/pandas/pull/38162/files#r536191377

cc @jorisvandenbossche "
758935268,38349,REF: call _ensure_valid_index inside _sanitize_column,jbrockmendel,closed,2020-12-07T23:37:43Z,2020-12-08T16:23:13Z,They are only ever called together.
759472791,38359,DOC: Start v1.3.0 release notes,simonjayhawkins,closed,2020-12-08T14:04:47Z,2020-12-08T16:28:48Z,
759557860,38363,Backport PR #38361 on branch 1.2.x (DOC: minor fixes to 1.2 release notes),meeseeksmachine,closed,2020-12-08T15:50:06Z,2020-12-08T17:07:55Z,Backport PR #38361: DOC: minor fixes to 1.2 release notes
759577037,38368,Shruti bug fix,shrutiguptabu,closed,2020-12-08T16:12:19Z,2020-12-08T17:28:39Z,"- [x] closes #line too long
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
759561104,38365,BUG FIX- C408,shrutiguptabu,closed,2020-12-08T15:54:17Z,2020-12-08T17:29:03Z,"- [ ] closes #c408
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
759562955,38366,Backport PR #38360 on branch 1.2.x (CI: activate azure pipelines/github actions on 1.2.x),meeseeksmachine,closed,2020-12-08T15:56:34Z,2020-12-08T18:10:54Z,Backport PR #38360: CI: activate azure pipelines/github actions on 1.2.x
757783732,38322,REF: use lighter-weight casting function,jbrockmendel,closed,2020-12-05T21:27:50Z,2020-12-08T18:14:50Z,
738334686,37690,DOC: add examples to insert and update generally,erictleung,closed,2020-11-07T23:10:53Z,2020-12-08T18:45:16Z,"This commit mainly adds examples on usage. This commit also fills in
information suggested by `validate_docstrings.py` script.

Ran

```
python scripts/validate_docstrings.py pandas.DataFrame.insert
```

to check docs and added in suggested information (e.g., See Also section).

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
750932114,38066,RLS: 1.1.5,simonjayhawkins,closed,2020-11-25T15:51:33Z,2020-12-08T19:16:55Z,"Tracking issue for the 1.1.5 release.

https://github.com/pandas-dev/pandas/milestone/79

"
758976748,38351,BUG: item_cache invalidation,jbrockmendel,closed,2020-12-08T01:14:28Z,2020-12-08T19:46:08Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
638212033,34753,BUG: build from source failed on Ubuntu 20.04,fangchenli,closed,2020-06-13T18:49:51Z,2020-12-08T20:08:26Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Output of the build script

```bash
(pandas-dev) xxx@xxx-home-office:~/Workspace/pandas-VirosaLi$ python setup.py build_ext --inplace -j 4
running build_ext
building 'pandas._libs.interval' extension
gcc -pthread -B /home/xxx/miniconda3/envs/pandas-dev/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DNPY_NO_DEPRECATED_API=0 -I./pandas/_libs/tslibs -Ipandas/_libs/src/klib -I/home/xxx/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/numpy/core/include -I/home/xxx/miniconda3/envs/pandas-dev/include/python3.8 -c pandas/_libs/interval.c -o build/temp.linux-x86_64-3.8/pandas/_libs/interval.o -Werror
building 'pandas._libs.tslibs.nattype' extension
gcc -pthread -B /home/xxx/miniconda3/envs/pandas-dev/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DNPY_NO_DEPRECATED_API=0 -I./pandas/_libs/tslibs -I/home/xxx/miniconda3/envs/pandas-dev/lib/python3.8/site-packages/numpy/core/include -I/home/xxx/miniconda3/envs/pandas-dev/include/python3.8 -c pandas/_libs/tslibs/nattype.c -o build/temp.linux-x86_64-3.8/pandas/_libs/tslibs/nattype.o -Werror
pandas/_libs/tslibs/nattype.c:4998:18: error: ‘__pyx_pw_6pandas_5_libs_6tslibs_7nattype_4_NaT_11__div__’ defined but not used [-Werror=unused-function]
 4998 | static PyObject *__pyx_pw_6pandas_5_libs_6tslibs_7nattype_4_NaT_11__div__(PyObject *__pyx_v_self, PyObject *__pyx_v_other) {
      |                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
pandas/_libs/interval.c:8301:18: error: ‘__pyx_pw_6pandas_5_libs_8interval_8Interval_25__div__’ defined but not used [-Werror=unused-function]
 8301 | static PyObject *__pyx_pw_6pandas_5_libs_8interval_8Interval_25__div__(PyObject *__pyx_v_self, PyObject *__pyx_v_y) {
      |                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cc1: all warnings being treated as errors
cc1: all warnings being treated as errors
error: command 'gcc' failed with exit status 1

```

#### Problem description

Recently I decide to try out Ubuntu 20.04. The build process failed. See the output in the section above. I replaced my user name with xxx.

"
757533707,38308,REF: implement Index._get_indexer,jbrockmendel,closed,2020-12-05T04:03:56Z,2020-12-08T21:17:04Z,De-duplicate all the boilerplate in Index.get_indexer.
758964737,38350,TST: add message matches to pytest.raises in various tests GH30999,JoseNavy,closed,2020-12-08T00:46:57Z,2020-12-08T21:33:42Z,"- [x] ref [#30999 ](https://github.com/pandas-dev/pandas/issues/30999)
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Files changed: 
pandas/tests/io/pytables/test_complex.py
pandas/tests/io/test_clipboard.py
pandas/tests/plotting/frame/test_frame_subplots.py"
759507081,38362,BUG: Groupby column and min aggregation over period column yields NaT for all rows,franzoni315,closed,2020-12-08T14:47:36Z,2020-12-09T00:58:49Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample:
```python
d = {
  'user_id': {
    0: 4628132781164409,
    1: 7075307556966343,
    2: 8013098674514720,
    3: 7690164883842520,
    4: 4985766983843363,
    5: 3038805367505476,
    6: 2541787162631756,
    7: 4985766983843363,
    8: 2541787162631756,
    9: 3558493739463398},
  'date': {
    0: '2020-02-12',
    1: '2020-02-12',
    2: '2020-02-12',
    3: '2020-02-12',
    4: '2020-02-12',
    5: '2020-02-12',
    6: '2020-02-12',
    7: '2020-02-12',
    8: '2020-02-12',
    9: '2020-02-12'
 }
}

df = pd.DataFrame(d).astype({'date': 'datetime64'})
df['week'] = df['date'].dt.to_period('W')
df['cohort'] = df.groupby('user_id')['week'].min()
```

#### Problem description

The above code returns NaT values for the `cohort` column, but it was expected to return the minimum values for `week`, which in this case would be `2020-02-10/2020-02-16` for all rows.

#### Expected Output
If we change the last line to:
```
df['cohort'] = df.groupby('user_id')['week'].transform('min')
```

We get the expected value for the cohort column, i.e., `2020-02-10/2020-02-16`.


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-1029-aws
Version          : #30~18.04.1-Ubuntu SMP Tue Oct 20 11:09:25 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.4
numpy            : 1.19.4
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : 0.29.15
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.12.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.20
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.52.0

</details>
"
759123789,38355,BUG: pylint with merge of pandas==1.1.5 raises a RecursionError,ikedaosushi,closed,2020-12-08T06:49:55Z,2020-12-09T01:42:45Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```sh
pip install pandas==1.1.5 pylint 
```

```python
# pandas_merge_sample.py
import pandas as pd

pd.merge(None, None)
```

```sh
pylint pandas_merge_sample.py
```

#### Problem description

Performing `pylint` on Python codes that include `pandas.merge` makes a RecursionError like below.
I'm not sure the error was caused by pandas itself, but at least I can say `pylint` with `pandas==1.1.4` doesn't raise the error, and `pylint` with `pandas==1.1.5` raises.

For `pylint` side, an issue about it was already opened, so I've opened it just in case for those who will face the same problem.
https://github.com/PyCQA/pylint/issues/3969

I might think the commit can be related to the problem.
https://github.com/pandas-dev/pandas/commit/e99e5ab32c4e831e7bbac0346189f4d6d86a6225#diff-7c4c2c4161d35fe8d1d71cc86b45abcc642271069f05cf860d9195a304d2ed33

```
   ...
    res = next(generator)
  File ""/private/tmp/pandas-pylint/venv/lib/python3.8/site-packages/astroid/inference.py"", line 293, in infer_attribute
    for owner in self.expr.infer(context):
  File ""/private/tmp/pandas-pylint/venv/lib/python3.8/site-packages/astroid/util.py"", line 160, in limit_inference
    yield from islice(iterator, size)
  File ""/private/tmp/pandas-pylint/venv/lib/python3.8/site-packages/astroid/context.py"", line 113, in cache_generator
    for result in generator:
  File ""/private/tmp/pandas-pylint/venv/lib/python3.8/site-packages/astroid/decorators.py"", line 132, in raise_if_nothing_inferred
    yield next(generator)
  File ""/private/tmp/pandas-pylint/venv/lib/python3.8/site-packages/astroid/decorators.py"", line 93, in wrapped
    generator = _func(node, context, **kwargs)
  File ""/private/tmp/pandas-pylint/venv/lib/python3.8/site-packages/astroid/inference.py"", line 203, in infer_name
    context = contextmod.copy_context(context)
  File ""/private/tmp/pandas-pylint/venv/lib/python3.8/site-packages/astroid/context.py"", line 155, in copy_context
    return context.clone()
  File ""/private/tmp/pandas-pylint/venv/lib/python3.8/site-packages/astroid/context.py"", line 102, in clone
    clone = InferenceContext(self.path, inferred=self.inferred)
RecursionError: maximum recursion depth exceeded
```

#### Expected Output

No error will be raised.

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.8.2.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.3.0
Version          : Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.5
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
```
</details>
"
759925195,38381,"CLN: C408 Unnecessary dict call, last files #38138",UrielMaD,closed,2020-12-09T02:00:38Z,2020-12-09T04:25:17Z,"- [x] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
758046780,38336,rewrite dict as literal in tests/io/parser/test_index_col.py,pillemer,closed,2020-12-06T23:07:29Z,2020-12-09T10:13:53Z,"for issue # 38138 
rewrite unnecessary dict call as a literal in pandas/tests/io/parser/test_index_col.py

Please let me know if anything needs to be amended, changed or fixed. I'm new to contributing.

- [ ] issue #38138 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
759983201,38383,CLN: C408 Unnecessary dict call - rewrite as a literal #38138,UrielMaD,closed,2020-12-09T04:32:51Z,2020-12-09T10:14:53Z,"Fix in:
./pandas/tests/io/generate_legacy_storage_files.py

- [x] closes #38138
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
719706688,37088,ENH: read_sas only selected columns (and rows) of a large data file,paris0120,open,2020-10-12T22:22:20Z,2020-12-09T11:00:50Z,"#### Is your feature request related to a problem?

Reading a large data file can exhaust system memory.  However, most of the time, we don't need all data from that file. It will be convenient to be able to read only the data needed.

#### Describe the solution you'd like

Reading only the data needed through iteration. 

#### API breaking implications

One more function for each type of files

#### Describe alternatives you've considered

May add conditions as well, which will be sweet

#### Additional context

[add any other context, code examples, or references to existing implementations about the feature request here]

```python
def read_sas_by_columns(file_path, keys, chunksize=100, charset = 'utf-8'):
    data = pd.DataFrame()
    for df in pd.read_sas(file_path,iterator=True, chunksize=chunksize):
        data = data.append(df[keys])
    for c in data.select_dtypes(include=['object']).columns:
        data[c] = data[c].str.decode(charset)
    return data

```
"
759777102,38371,Backport PR #38369 on branch 1.2.x (DOC: sync xarray version),meeseeksmachine,closed,2020-12-08T21:00:09Z,2020-12-09T13:36:59Z,Backport PR #38369: DOC: sync xarray version
759834729,38374,CLN: typo fixup in 1.2.0 whatsnew,jbrockmendel,closed,2020-12-08T22:30:26Z,2020-12-09T15:57:14Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @simonjayhawkins 
"
757458543,38300,ENH: Make pandas API more consistent,dclong,closed,2020-12-04T22:51:30Z,2020-12-09T16:20:30Z,"#### Is your feature request related to a problem?

The pandas DataFrame uses both arguments of `inplace` and `copy` (`DataFrame.astype` and `Series.astype`) in different methods to indicate change in place. It would be great if it can be unified to use `inplace` in all methods.

#### Describe the solution you'd like

Always use the argument `inplace` (instead of `copy`) to indicate change in place. 

#### API breaking implications

Some methods such as `DataFrame.astype` and `Series.astype` will be affected as they argument `copy` is changed to `inplace`. 

#### Describe alternatives you've considered

#### Additional context
"
760339082,38384,Backport PR #38374 on branch 1.2.x (CLN: typo fixup in 1.2.0 whatsnew),meeseeksmachine,closed,2020-12-09T13:40:00Z,2020-12-09T17:08:30Z,Backport PR #38374: CLN: typo fixup in 1.2.0 whatsnew
740403069,37751,November 2020 Meeting,TomAugspurger,closed,2020-11-11T02:30:15Z,2020-12-09T17:44:07Z,"The monthly dev meeting is Wednesday November 11th (tomorrow), at 18:00 UTC. Our calendar is at https://pandas.pydata.org/docs/development/meeting.html#calendar to check your local time. The US had a DST transition since the last meeting, so your local time may be different.

Video Call: https://zoom.us/j/942410248?pwd=T2l2Qi9vaC82Z294ZEtFczYxMVM2dz09
Minutes: https://docs.google.com/document/u/1/d/1tGbTiYORHiSPgVMXawiweGJlBw5dOkVJLY-licoBmBU/edit?ouid=102771015311436394588&usp=docs_home&ths=true

FYI, I likely won't be able to attend tomorrow."
759894747,38378,BUG: is_numeric_dtype return bool as TRUE,annaymj,closed,2020-12-09T00:42:35Z,2020-12-09T17:55:13Z,"
#### Code Sample, 

```
from pandas.api.types import is_integer_dtype, is_numeric_dtype, is_string_dtype, is_bool_dtype
import numpy as np
is_numeric_dtype(bool)
# True

is_numeric_dtype(np.bool)
# True
```

#### Problem description

is_numeric_dtype is returning true for boolean values

source code below
https://github.com/pandas-dev/pandas/blob/v1.1.5/pandas/core/dtypes/common.py#L1223-L1262

```
def is_numeric_dtype(arr_or_dtype) -> bool:
    """"""
    Check whether the provided array or dtype is of a numeric dtype.
    Parameters
    ----------
    arr_or_dtype : array-like
        The array or dtype to check.
    Returns
    -------
    boolean
        Whether or not the array or dtype is of a numeric dtype.
    Examples
    --------
    >>> is_numeric_dtype(str)
    False
    >>> is_numeric_dtype(int)
    True
    >>> is_numeric_dtype(float)
    True
    >>> is_numeric_dtype(np.uint64)
    True
    >>> is_numeric_dtype(np.datetime64)
    False
    >>> is_numeric_dtype(np.timedelta64)
    False
    >>> is_numeric_dtype(np.array(['a', 'b']))
    False
    >>> is_numeric_dtype(pd.Series([1, 2]))
    True
    >>> is_numeric_dtype(pd.Index([1, 2.]))
    True
    >>> is_numeric_dtype(np.array([], dtype=np.timedelta64))
    False
    """"""
    return _is_dtype_type(
        arr_or_dtype, classes_and_not_datetimelike(np.number, np.bool_)
    )
```

#### Expected Output
is_numeric_dtype(np.bool)
# False

#### 

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.2.9-98_fbk11_hardened_3627_g39307fe48d19
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : 
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.4
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : None
setuptools       : 46.1.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.0
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.0
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.2.14
tables           : None
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
123817107,11896,DataFrame to_sql fails with percentage symbol `%` in column name with SQLAlchemy and PostgreSQL,LeeMendelowitz,open,2015-12-24T16:04:40Z,2020-12-09T22:50:13Z,"The following call to `to_sql` fails due to the percentage (%) symbol in the column names in pandas 0.17.1.

``` python
import pandas as pd
from sqlalchemy import create_engine

connect_str = ""mysql://{USER}:{PASSWORD}@{HOST}/{DBNAME}"".format(
    USER = 'root',
    PASSWORD = '',
    HOST = '127.0.0.1',
    DBNAME = 'test'
)

engine = create_engine(connect_str, echo = False)


# Create a dataframe with '%' in column name
df = pd.DataFrame()
df['A%'] = [0.1, 0.2, 0.3]
df['B%'] = [0.1, 0.2, 0.3]

# Save to database
df.to_sql('test_table', engine, if_exists = 'replace', index = False)

# Throws error:
# OperationalError: (_mysql_exceptions.OperationalError) (1054, ""Unknown column 'A%%' in 'field list'"") [SQL: u'INSERT INTO test_table (`index`, `A%%`, `B%%`) VALUES (%s, %s, %s)'] [parameters: ((0, 0.1, 0.1), (1, 0.2, 0.2), (2, 0.3, 0.3))]
```

Backtrace:

```
Traceback (most recent call last):
  File ""test_pandas_error.py"", line 20, in <module>
    df.to_sql('test_table', engine, if_exists = 'replace', index = False)
  File ""/usr/local/lib/python2.7/site-packages/pandas/core/generic.py"", line 1003, in to_sql
    dtype=dtype)
  File ""/usr/local/lib/python2.7/site-packages/pandas/io/sql.py"", line 569, in to_sql
    chunksize=chunksize, dtype=dtype)
  File ""/usr/local/lib/python2.7/site-packages/pandas/io/sql.py"", line 1241, in to_sql
    table.insert(chunksize)
  File ""/usr/local/lib/python2.7/site-packages/pandas/io/sql.py"", line 765, in insert
    self._execute_insert(conn, keys, chunk_iter)
  File ""/usr/local/lib/python2.7/site-packages/pandas/io/sql.py"", line 740, in _execute_insert
    conn.execute(self.insert_statement(), data)
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 914, in execute
    return meth(self, multiparams, params)
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/sql/elements.py"", line 323, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1010, in _execute_clauseelement
    compiled_sql, distilled_params
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1146, in _execute_context
    context)
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1341, in _handle_dbapi_exception
    exc_info
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/util/compat.py"", line 199, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb)
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1116, in _execute_context
    context)
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/dialects/mysql/mysqldb.py"", line 95, in do_executemany
    rowcount = cursor.executemany(statement, parameters)
  File ""/usr/local/lib/python2.7/site-packages/MySQLdb/cursors.py"", line 262, in executemany
    r = self._query('\n'.join([query[:p], ',\n'.join(q), query[e:]]))
  File ""/usr/local/lib/python2.7/site-packages/MySQLdb/cursors.py"", line 354, in _query
    rowcount = self._do_query(q)
  File ""/usr/local/lib/python2.7/site-packages/MySQLdb/cursors.py"", line 318, in _do_query
    db.query(q)
sqlalchemy.exc.OperationalError: (_mysql_exceptions.OperationalError) (1054, ""Unknown column 'A%%' in 'field list'"") [SQL: u'INSERT INTO test_table (`A%%`, `B
%%`) VALUES (%s, %s)'] [parameters: ((0.1, 0.1), (0.2, 0.2), (0.3, 0.3))]
```

Versions:

```
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.10.final.0
python-bits: 64
OS: Darwin
OS-release: 15.2.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.17.1
nose: 1.3.7
pip: 7.1.2
setuptools: 18.0.1
Cython: None
numpy: 1.10.2
scipy: 0.16.0
statsmodels: None
IPython: 4.0.0
sphinx: None
patsy: None
dateutil: 2.4.2
pytz: 2015.7
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: 1.4.3
openpyxl: 2.2.0-b1
xlrd: 0.9.4
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 1.0.8
pymysql: None
psycopg2: None
Jinja2: None
```
"
760521360,38388,BUG: `DataFrame.__repr__` not showing correct `MultiIndex` values,galipremsagar,closed,2020-12-09T17:24:53Z,2020-12-09T23:32:05Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
>>> import pandas as pd
>>> df = pd.DataFrame()
>>> df['a'] = pd.Series([1, 2, None], dtype='Int64')
>>> df['b'] = pd.Series([10, 11, 12])
>>> df['c'] = pd.Series(['dsf', 'bac', 'abc'])
>>> df
      a   b    c
0     1  10  dsf
1     2  11  bac
2  <NA>  12  abc
>>> df.set_index(['a','b'])
          c
a   b      
1   10  dsf
2   11  bac
NaN 12  abc
>>> df.set_index(['a','b']).index
MultiIndex([(1.0, 10),
            (2.0, 11),
            (nan, 12)],
           names=['a', 'b'])
```

#### Problem description

The actual underlying values in the multiIndex in first level are all `floats` but the `DataFrame.__repr__` outputs the values as integers.

#### Expected Output

The expected output should be inline with `MultiIndex.__repr__`.

```python
>>> df.set_index(['a','b'])
          c
a     b      
1.0   10  dsf
2.0   11  bac
NaN   12  abc
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 20.1.0
Version          : Darwin Kernel Version 20.1.0: Sat Oct 31 00:07:11 PDT 2020; root:xnu-7195.50.7~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_US.UTF-8
pandas           : 1.1.4
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.0
Cython           : None
pytest           : None
hypothesis       : 5.29.0
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
759892461,38377,CLN: lighter-weight casting,jbrockmendel,closed,2020-12-09T00:36:48Z,2020-12-09T23:52:30Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
754661431,38216,Corrected the grammar in Exception,imaskm,closed,2020-12-01T19:26:51Z,2020-12-10T00:03:45Z,"Exception on different arrays' length corrected

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
760544074,38389,"REF: consolidate NDFrame._iset_item, _set_item with DataFrame methods",jbrockmendel,closed,2020-12-09T17:55:26Z,2020-12-10T00:16:36Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
760774248,38395,Backport PR #35100 on branch 1.2.x (Document Tips for Debugging C Extensions),meeseeksmachine,closed,2020-12-10T00:19:53Z,2020-12-10T10:28:54Z,Backport PR #35100: Document Tips for Debugging C Extensions
459258706,26985,Performance drop and MemoryError during insert and _consolidate_inplace,ololobus,closed,2019-06-21T15:39:48Z,2020-12-11T02:35:42Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import numpy as np

n = 2000000
data = pd.DataFrame({'a' : range(n)})
for i in range(1, 100):
    data['col_' + str(i)] = np.random.choice(['a', 'b'], n)

for i in range(1, 600):
    data['test_{}'.format(i)] = i
    print(str(i))

```

#### Problem description

Following [this StackOverflow question](https://stackoverflow.com/questions/56690909/python-is-facing-an-overhead-every-98-executions).

I run this code sample on Ubuntu 18.04 LTS machine with 16 GB of RAM and 2 GB Swap. Execution produces following stacktrace:

```shell
294
295
296
297
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py"", line 2657, in get_loc
    return self._engine.get_loc(key)
  File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/index.pyx"", line 132, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1601, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1608, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'test_298'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py"", line 1053, in set
    loc = self.items.get_loc(item)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py"", line 2659, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/index.pyx"", line 132, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1601, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1608, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'test_298'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""py-memory-test.py"", line 12, in <module>
    data['test_{}'.format(i)] = i
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 3370, in __setitem__
    self._set_item(key, value)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py"", line 3446, in _set_item
    NDFrame._set_item(self, key, value)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py"", line 3172, in _set_item
    self._data.set(key, value)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py"", line 1056, in set
    self.insert(len(self.items), item, value)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py"", line 1184, in insert
    self._consolidate_inplace()
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py"", line 929, in _consolidate_inplace
    self.blocks = tuple(_consolidate(self.blocks))
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py"", line 1899, in _consolidate
    _can_consolidate=_can_consolidate)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/internals/blocks.py"", line 3149, in _merge_blocks
    new_values = new_values[argsort]
MemoryError
```

I have found [following code](https://github.com/pandas-dev/pandas/blob/224362951942e1f4e05fb8948596620aedac26d9/pandas/core/internals/managers.py#L1179) inside `pandas` core:

```python
def insert(self, loc, item, value, allow_duplicates=False):
    ...
    self._known_consolidated = False

    if len(self.blocks) > 100:
        self._consolidate_inplace()
```

It seems that this consolidation process takes place every ~100th iteration and substantially affects performance and memory usage. In order to proof this hypothesis I have tried to modify `100` to `1000000` and it worked just fine, no performance gaps, no `MemoryError`.

It looks quite weird from my perspective, since 'consolidation' sounds like it should reduce memory usage. Probably `pandas` should allocate some private Swap files (e.g. via `mmap`) if it is running out RAM+SystemSwap in order to be able to successfully complete consolidation process. 

#### Expected Output

```shell
1
2
3
4
5
...
599
```

Without substantial freezes every ~100th iteration and `MemoryError`.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.8.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-50-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: None
pip: 9.0.1
setuptools: 39.0.1
Cython: None
numpy: 1.16.4
scipy: None
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2018.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: 0.999999999
sqlalchemy: None
pymysql: None
psycopg2: 2.7.3.1 (dt dec pq3 ext lo64)
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
759918137,38380,BUG: item_cache invalidation on DataFrame.insert,jbrockmendel,closed,2020-12-09T01:42:38Z,2020-12-11T02:50:21Z,"- [x] closes #26985
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Something wonky is going on with tm.assert_produces_warning, will see if it shows up on the CI."
736573305,37639,REF: move get_filepath_buffer into get_handle,twoertwein,closed,2020-11-05T03:10:46Z,2020-12-11T04:34:31Z,"- [x] closes #33987
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

This PR makes `get_filepath_buffer` a private function and is called inside `get_handle` - one function to rule all of IO ;)

This PR will make it easier for future PRs to make the IO-related interface of `read/to_*` more consistent as most of them should support compression/memory mapping (for reading)/(binary) file handles/storage options.

Notes to keep track of future follow-up PRs:

- context manager for `get_handle`
- `storage_options` for `to_excel`"
696246890,36233,BUG: df.to_markdown() with empty frame incorrect output,ylin00,closed,2020-09-08T23:06:11Z,2020-12-11T04:47:21Z,"- [x] closes #31771
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] added test_empty_frame for test_to_markdown.py (#31771)
- [x] Required tabulate >= 0.8.7
"
680719921,35784,DOC: Updated docstring for Series.str.cat (pandas-dev#35556),souris-dev,closed,2020-08-18T06:01:25Z,2020-12-11T05:01:54Z,"- [X] closes #35556
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I have updated the docstring for `Series.str.cat` to make the issue in #35556 more explicit to the user."
742126773,37805,ENH: Add end and end_day options for origin from resample,GYHHAHA,closed,2020-11-13T04:21:35Z,2020-12-11T05:10:03Z,"- [x] closes #37804
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
760803810,38396,BUG: algos.rank with readonly values,jbrockmendel,closed,2020-12-10T01:31:50Z,2020-12-11T11:21:35Z,One of the two current npdev failures is caused by https://github.com/numpy/numpy/pull/17884
762254609,38409,Backport PR #38396 on branch 1.2.x (BUG: algos.rank with readonly values),meeseeksmachine,closed,2020-12-11T11:22:57Z,2020-12-11T12:28:08Z,Backport PR #38396: BUG: algos.rank with readonly values
729847323,37427,BUG: NBC change in reindexing logic of Series.__setitem__ in pandas=1.1.0,kozlov-alexey,closed,2020-10-26T19:33:28Z,2020-12-11T12:51:07Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

Hi, 

The following example of using `Series.__setitem__` shows different results on **pandas=1.0.5** and **pandas=1.1.3**:

```
import pandas as pd
import numpy as np

S1 = pd.Series([0, 1, -12, 3, -10, 5, 6, 7, 8, -11, -13], dtype=np.int32)
idx = pd.Series([4, 9, 2, 10], dtype=np.int64)
value = pd.Series([-10, -11, -12, -13], dtype=np.int32)
S1[idx] = value  
S1
```

It looks like  [#33643](https://github.com/pandas-dev/pandas/pull/33643) changed the semantics of setitem operation when assigned values are `pandas.Series`, i.e. the old-behavior was to first align value with idx and then assign value items to S1 basing on idx.values as index labels, so result was:

```
>>> S1
0      0
1      1
2    -12
3      3
4    -10
5      5
6      6
7      7
8      8
9    -11
10   -13
dtype: int32
```

And now the result is as if `S1[idx]` took precedence before the actual assignment, so that labels in `idx.values` which are absent in `value.index` are assigned NaNs:

```
>>> S1
0      0.0
1      1.0
2    -12.0
3      3.0
4      NaN
5      5.0
6      6.0
7      7.0
8      8.0
9      NaN
10     NaN
dtype: float64
```

Can you please confirm this was intentional change (as I don't see any tests updated to reflect that, nor there are mentions in ""What’s new in 1.1.0"" section of the documentation)? From the user perspective it looks like with new implementation, user will have to ensure that `value.index == idx.values` to achieve the same result, which is not a big deal of course, but still it's some additional code. 

First version where the issue appears:
```
INSTALLED VERSIONS
------------------
commit           : 4071c3b872e4ef98a2a2d7c752dd5d2030c4df6e
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.0.dev0+1307.g4071c3b87
```

Kind Regards,
Alexey."
761479866,38404,REF: simplify casting,jbrockmendel,closed,2020-12-10T18:10:42Z,2020-12-11T22:54:01Z,This gets us down to only 3 usages of maybe_cast_to_datetime
759781109,38372,REF: de-duplicate get_indexer methods,jbrockmendel,closed,2020-12-08T21:05:53Z,2020-12-11T22:58:04Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Sits on top of #38308"
719233418,37073,ENH: Add MultiIndex.dtypes,skvrahul,closed,2020-10-12T09:42:47Z,2020-12-11T23:04:25Z,"- [x] closes #37062 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Adds the .dtypes property to MultiIndex using the suggested implementation"
760834612,38398,REF: implement sanitize_masked_array,jbrockmendel,closed,2020-12-10T02:46:55Z,2020-12-11T23:08:13Z,The recarray handling in internals.construction is inconsistent with the other recarray handling.  Hopefully I can get that sorted out and then we can re-use sanitize_masked_array there too.
760856143,38400,REF: simplify internals.construction,jbrockmendel,closed,2020-12-10T03:35:35Z,2020-12-11T23:10:00Z,
757787433,38323,CLN: Implement multiindex handling for get_op_result_name,phofl,closed,2020-12-05T21:50:19Z,2020-12-11T23:42:58Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

@jbrockmendel 

This would solve the inconsistency in ``get_op_result_name`` for MultiIndexes, but adds a bit more complexity here.
I think this would allow us to remove the now duplicated code in ``intersection``"
757129482,38289,CLN: remove internal usage of integer_array(),jorisvandenbossche,closed,2020-12-04T14:08:21Z,2020-12-12T08:50:42Z,"The `integer_array()` function stems from a time that `pd.array(..)` didn't exist yet. But since now `pd.array` can do this, IMO we shouldn't be using that outside of the integer module (eg I didn't add a `floating_array(..)`).

Only in `tests/arrays/integer/tst_construction.py` I still left the usage of `integer_array()` as is (but probably could also change it there). Could in principle remove the function altogether, since it is just a small wrapper around the private `arrays.integer.coerce_to_array`"
616610326,34132,TST: DataFrame.interpolate axis name for axis argument,simonjayhawkins,closed,2020-05-12T12:18:24Z,2020-12-12T11:37:22Z,"- [ ] closes #25190
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref #31566
"
529538342,29897,TYP: some types for pandas/_config/config.py,simonjayhawkins,closed,2019-11-27T19:35:56Z,2020-12-12T12:51:23Z,"broken off #28339

"
496123767,28547,Deprecate using `xlrd` engine in favor of openpyxl,WillAyd,closed,2019-09-20T03:25:00Z,2020-12-12T14:57:21Z,"xlrd is unmaintained and the previous maintainer has asked us to move towards openpyxl. xlrd works now, but *might* have some issues when Python 3.9 or later gets released and changes some elements of the XML parser, as default usage right now throws a `PendingDeprecationWarning`

Considering that I think we need to deprecate using `xlrd` in favor of `openpyxl`. We might not necessarily need to remove the former and it does offer some functionality the latter doesn't (namely reading `.xls` files) but should at the very least start moving towards the latter"
756441897,38269,BUG: _metadata does not persist through groupby,stuart23,closed,2020-12-03T18:24:26Z,2020-12-12T16:52:36Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
class MyDataFrame(DataFrame):
    _metadata = ['my_metadata']

data = MyDataFrame({'test': [1,1,1,2,2,2]})
data.my_metadata = 2
for group_id, group_data in data.groupby('test'):
    assert group_data.my_metadata == 2
```

#### Problem description
As per documentation [here](https://pandas.pydata.org/pandas-docs/stable/development/extending.html#define-original-properties), it seems that any derivatives of a DataFrame should inherit the metadata properties assigned. This is true with slicing operations, but is not true with groupby(). It would be more consistent if the property persisted for slicing and groupby().

#### Expected Output
Should not raise an assertion error.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-56-generic
Version          : #62-Ubuntu SMP Mon Nov 23 19:20:19 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.4
numpy            : 1.19.4
pytz             : 2019.3
dateutil         : 2.7.3
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : 0.29.21
pytest           : 6.1.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.4
sqlalchemy       : None
tables           : None
tabulate         : 0.8.6
xarray           : 0.14.1
xlrd             : None
xlwt             : None
numba            : 0.51.2

</details>
"
757528550,38307,TYP: datetimelike,jbrockmendel,closed,2020-12-05T03:50:01Z,2020-12-12T17:26:25Z,"use np.putmask in a few places, as it is appreciably more performant than `__setitem__`"
577325741,32515,df.to_dict() accepts orient which are not in the list of options,elmonsomiat,closed,2020-03-07T12:07:45Z,2020-12-12T17:59:19Z,"#### Code Sample, a copy-pastable example if possible

```python

import pandas as pd

df = pd.DataFrame(
    {'name': ['alice', 'bob'],
     'age': [30, 28]})

df.to_dict(orient='racoon')
```
returns:
```python

[{'age': 30, 'name': 'alice'}, {'age': 28, 'name': 'bob'}]
```

instead of `ValueError`
#### Problem description

The current version of pandas accepts orient= any word which starts with r, l, s, sp, i and d instead of just the actual options in the documentation.

The function should be returning `ValueError` if the orient option is not any of the following: `{'dict', 'list', 'series', 'split', 'records', 'index'}`.

Not fixing this might lead to mistakes by using an orient which might point to a wrong output. I would not consider this a bug but this should be consistent with other methods such as  `df.to_json()`. The part of the code which needs to be fixed in master is:

```python

        if orient.lower().startswith(""d""):
            return into_c((k, v.to_dict(into)) for k, v in self.items())
        elif orient.lower().startswith(""l""):
            return into_c((k, v.tolist()) for k, v in self.items())
        elif orient.lower().startswith(""sp""):
            return into_c(
                (
                    (""index"", self.index.tolist()),
                    (""columns"", self.columns.tolist()),
                    (
                        ""data"",
                        [
                            list(map(com.maybe_box_datetimelike, t))
                            for t in self.itertuples(index=False, name=None)
                        ],
                    ),
                )
            )
        elif orient.lower().startswith(""s""):
            return into_c((k, com.maybe_box_datetimelike(v)) for k, v in self.items())
        elif orient.lower().startswith(""r""):
            columns = self.columns.tolist()
            rows = (
                dict(zip(columns, row))
                for row in self.itertuples(index=False, name=None)
            )
            return [
                into_c((k, com.maybe_box_datetimelike(v)) for k, v in row.items())
                for row in rows
            ]
        elif orient.lower().startswith(""i""):
            if not self.index.is_unique:
                raise ValueError(""DataFrame index must be unique for orient='index'."")
            return into_c(
                (t[0], dict(zip(self.columns, t[1:])))
                for t in self.itertuples(name=None)
            )
        else:
            raise ValueError(f""orient '{orient}' not understood"")
```
by replacing the `.lower().startswith()` with ` ==`  and the corresponding string
I with now open a PR  to fix the issue. *THE BUG IS STILL IN MASTER*

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Linux
OS-release: 4.15.0-88-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.24.2
pytest: None
pip: 19.3.1
setuptools: 41.6.0
Cython: None
numpy: 1.17.4
scipy: None
pyarrow: None
xarray: None
IPython: 7.9.0
sphinx: None
patsy: None
dateutil: 2.8.1
pytz: 2019.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: 1.3.11
pymysql: 0.9.3
psycopg2: None
jinja2: 2.10.3
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None


</details>
"
754613171,38213,dict rewritten as literal for files in directory /pandas/core/arrays/,vrsh,closed,2020-12-01T18:08:36Z,2020-12-12T20:04:10Z,"- [x] xref #38138 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry: Identify unnecessary dict call - rewrite as a literal for directory /pandas/core/arrays/floating.py, interval.py, numpy_.py & base.py
"
751685949,38090,CI/TST: suppress http debug from tests outputs,jreback,closed,2020-11-26T15:43:03Z,2020-11-28T17:33:56Z,"examples from https://travis-ci.org/github/pandas-dev/pandas/jobs/746072525

We have this output coming from our fixtures (we startup a http endpoint to test things), but the output is totally unecessary / distracting.

```
127.0.0.1 - - [26/Nov/2020 13:35:25] ""PUT /pandas-test/items.jsonl HTTP/1.1"" 200 -
127.0.0.1 - - [26/Nov/2020 13:35:25] ""PUT /pandas-test/simple_dataset.feather HTTP/1.1"" 200 -
127.0.0.1 - - [26/Nov/2020 13:35:25] ""PUT /cant_get_it/tips%231.csv HTTP/1.1"" 200 -
127.0.0.1 - - [26/Nov/2020 13:35:25] ""PUT /cant_get_it/tips.csv HTTP/1.1"" 200 -
127.0.0.1 - - [26/Nov/2020 13:35:25] ""PUT /cant_get_it/tips.csv.gz HTTP/1.1"" 200 -
127.0.0.1 - - [26/Nov/2020 13:35:25] ""PUT /cant_get_it/tips.csv.bz2 HTTP/1.1"" 200 -
127.0.0.1 - - [26/Nov/2020 13:35:25] ""PUT /cant_get_it/items.jsonl HTTP/1.1"" 200 -
127.0.0.1 - - [26/Nov/2020 13:35:25] ""PUT /cant_get_it/simple_dataset.feather HTTP/1.1"" 200 -
127.0.0.1 - - [26/Nov/2020 13:35:25] ""HEAD /pandas-test/tips.csv HTTP/1.1"" 200 -
127.0.0.1 - - [26/Nov/2020 13:35:25] ""GET /pandas-test/tips.csv HTTP/1.1"" 206 -
.127.0.0.1 - - [26/Nov/2020 13:35:25] ""GET /pandas-test?list-type=2&prefix=&delimiter=%2F&encoding-type=url HTTP/1.1"" 200 -
127.0.0.1 - - [26/Nov/2020 13:35:25] ""GET / HTTP/1.1"" 200 -
127.0.0.1 - - [26/Nov/2020 13:35:25] ""POST /pandas-test?delete HTTP/1.1"" 200 -
127.0.0.1 - - [26/Nov/2020 13:35:25] ""DELETE /pandas-test HTTP/1.1"" 204 -
127.0.0.1 - - [26/Nov/2020 13:35:25] ""GET /cant_get_it?list-type=2&prefix=&delimiter=%2F&encoding-type=url HTTP/1.1"" 200 -
127.0.0.1 - - [26/Nov/2020 13:35:25] ""GET / HTTP/1.1"" 200 -
127.0.0.1 - - [26/Nov/2020 13:35:25] ""POST /cant_get_it?delete HTTP/1.1"" 200 -
```"
723996038,37217,BUG: pytables index expressions fail in Python 3.9,rebecca-palmer,closed,2020-10-18T12:27:16Z,2020-11-28T17:49:46Z,"- [ y] I have checked that this issue has not already been reported. (Here - it [has in Debian](https://bugs.debian.org/972033))

- [ y] I have confirmed this bug exists on the latest version of pandas.

- [ y] (optional) I have confirmed this bug exists on the master branch of pandas.  (Though with a mix of Debian and pip dependencies, as pip doesn't have them all for 3.9 yet.)

---

#### Code Sample, a copy-pastable example

The tests in tests/io/pytables/test_store.py, or

```python
import pandas as pd;from pandas.io.pytables import HDFStore;s1=HDFStore(""tmp1.h5"",""w"");df=pd.DataFrame([[1,2,3],[4,5,6]],columns=['A','B','C']);s1.append(""d1"",df,data_columns=[""B""]);df2=s1.select(""d1"",""index>df.index[0]"");print(type(df2.index[0]))
```

#### Problem description

In Python 3.9, HDFStore.Select fails if it involves an index expression, with this traceback:

```
self = <DatetimeArray>
['2000-01-03 00:00:00', '2000-01-04 00:00:00', '2000-01-05 00:00:00',
 '2000-01-06 00:00:00', '2000-01...2-08 00:00:00',
 '2000-02-09 00:00:00', '2000-02-10 00:00:00', '2000-02-11 00:00:00']
Length: 30, dtype: datetime64[ns]
key = 4

    def __getitem__(self, key):
        if lib.is_integer(key):
            # fast-path
            result = self._ndarray[key]
            if self.ndim == 1:
                return self._box_func(result)
            return self._from_backing_data(result)
    
        key = extract_array(key, extract_numpy=True)
        key = check_array_indexer(self, key)
>       result = self._ndarray[key]
E       IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

pandas/core/arrays/_mixins.py:200: IndexError
```
The debugger says key is a pandas.core.computation.pytables.Constant, while in Python 3.8 (where this works) it is a plain int.  The underlying cause may be [Python replacing ast.Index with bare values](https://docs.python.org/3/library/ast.html#ast.AST).

The CI may have missed this because it [skips optional dependencies on 3.9](https://github.com/pandas-dev/pandas/blob/master/.travis.yml#L97) (to avoid having to build them).

#### Possible fix
Warning: not fully tested.
```
--- a/pandas/core/computation/pytables.py
+++ b/pandas/core/computation/pytables.py

@@ -429,6 +429,10 @@ class PyTablesExprVisitor(BaseExprVisito
             value = value.value
         except AttributeError:
             pass
+        try:
+            slobj = slobj.value
+        except AttributeError:
+            pass
 
         try:
             return self.const_type(value[slobj], self.env)
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.9.0.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.0-11-amd64
Version          : #1 SMP Debian 4.19.146-1 (2020-09-17)
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : C
LANG             : C
LOCALE           : None.None

pandas           : 0+unknown
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 50.3.0
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : 5.32.1
sphinx           : 3.2.1
blosc            : 1.9.2
feather          : None
xlsxwriter       : 1.1.2
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.2.1
fsspec           : 0.8.4
fastparquet      : None
gcsfs            : 0.7.1
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : 0.5.1
scipy            : 1.5.2
sqlalchemy       : 1.3.19
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.1.0
xlwt             : 1.3.0
numba            : None


</details>
"
752584497,38127,CLN: simplify mask_missing,jbrockmendel,closed,2020-11-28T03:06:49Z,2020-11-28T18:21:24Z,"
"
752588785,38128,CLN: remove unreachable in core.dtypes,jbrockmendel,closed,2020-11-28T03:39:58Z,2020-11-28T18:22:09Z,
752344981,38112,REF: IntervalIndex._assert_can_do_setop,jbrockmendel,closed,2020-11-27T16:00:40Z,2020-11-28T18:23:50Z,"Working towards standardizing+deduplicating set ops across Index subclasses, xref #38111"
714186954,36840,PERF: Index._shallow_copy shares _cache with copies of self,topper-123,closed,2020-10-03T22:20:37Z,2020-11-28T18:25:04Z,"#32640 copied the cache when copying indexes. Indexes are immutable, so some refactoring allows us to __share__ the cache instead. This means potentially fewer expensive index ops.

```python
>>> idx = pd.CategoricalIndex(np.arange(100_000))
>>> copied = idx.copy()
>>> copied._cache
{}
>>> idx.get_loc(99_999)
>>> copied._cache
{'_engine': <pandas._libs.index.Int32Engine at 0x1fd3a6934c8>}  # don't need to recreate _engine on copied
```

Performance example:
```python
>>> idx = pd.CategoricalIndex(np.arange(100_000))
>>> %timeit idx._shallow_copy().get_loc(99_999)
4.09 ms ± 65 µs per loop  # master
7.71 µs ± 355 ns per loop  # this PR
```
"
752692593,38132,DOC: add contibutors to 1.2.0 release notes,simonjayhawkins,closed,2020-11-28T14:58:17Z,2020-11-28T18:25:14Z,
751795549,38096,TST: Suppress http logs in tests,mroeschke,closed,2020-11-26T19:32:25Z,2020-11-28T18:33:18Z,"- [x] closes #38090
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Confirming that this does the trick."
723547605,37173,BUG: fix tab completion,topper-123,closed,2020-10-16T21:40:31Z,2020-11-28T18:41:42Z,"Currently tab completion doesn't work on attributes defined on the instance, only ones defined on the class. This fixes that.

Example:
```python
>>> class MyCls(pd.core.accessor.DirNamesMixin):
...     x = 1
... 
...     def __init__(self):
...         self.y = 2
...
>>> x = MyCls()
>>> ""x"" in dir(x)
True  # master and this PR
>>> ""y"" in dir(x)
False  # master
True  # this PR
```

Also some type fixups."
625729424,34407,"REGR: revert ""CLN: _consolidate_inplace less"" / fix regression in fillna()",jorisvandenbossche,closed,2020-05-27T14:09:47Z,2020-11-28T19:55:32Z,"Reverts pandas-dev/pandas#34389

Closes #36495"
752708148,38135,ENH: Categorical.unique can keep same dtype,topper-123,closed,2020-11-28T16:22:31Z,2020-11-28T19:57:10Z,"There are situations where we want to keep the same dtype as in the original after applying `unique` For example

```python
>>> dtype = pd.categoricalDtype(['very good', 'good', 'neutral', 'bad', 'very bad'], ordered=True)
>>> cat = pd.Categorical(['good','good', 'bad', 'bad'], dtype=dtype)
>>> cat
[good, good, bad, bad]
Categories (5, object): [very good < good < neutral < bad < very bad]
>>> cat.unique().dtype == cat.dtype
False  # this is a bug IMO, but others may not agree
```

Even if it's not a bug, there are situations where we want the comparison above to return True. To alleviate the above, I've added a new parameter, so we can do

```
>>> cat.unique(remove_unused_categories=False).dtype == cat.dtype
True
```

Helps #18291, but does not close the issue.
"
671562988,35514,BUG: fix combine_first converting timestamp to int,nixphix,closed,2020-08-02T06:35:41Z,2020-11-29T04:31:40Z,"- [x] closes #28481
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This fix introduced two regression, but it appears like the fix only made the API consistent. Previously the failing regressions where inconsistent say `df1.combine_first(df2)` would not return the same result as `df2.combine_first(df1)` for the failing cases, more on these in the code comments.

Let me know if there is a better way to handle this."
593672543,33278,ENH: Added schema kwarg to get_schema method,gregorylivschitz,closed,2020-04-03T22:25:26Z,2020-11-29T04:42:42Z,"- [x] closes #28486
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
624451213,34368,BUG: new method for join with multiindex and single index (#34292),CuylenE,closed,2020-05-25T19:19:58Z,2020-11-29T04:47:14Z,"- [x] closes #34292
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Tests still to be added.
I replaced the method to join a multiindex & a single index because outer joins didn't include data from the single index. The new method is similar to a join between 2 multiindexes.
I'm not sure what the keep_order parameter was, but it was never used and in other _join-methods it didn't exist either. So I suggest removing it all together."
752354319,38114,REF: implement _should_compare,jbrockmendel,closed,2020-11-27T16:16:58Z,2020-11-29T15:31:44Z,Broken off from #38105 so I can start using it to fix inconsistent set ops.
750318938,38053,"BUG: calling at[] on a Series with a single-level MultiIndex returns a Series, not a scalar",bpteague,closed,2020-11-25T02:51:39Z,2020-11-29T15:59:10Z,"- [YES] I have checked that this issue has not already been reported.

- [YES] I have confirmed this bug exists on the latest version of pandas.

- [NO] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
idx1 = pd.MultiIndex.from_tuples([(False, 1.0), 
                                  (False, 10.0),
                                  (True, 1.0), 
                                  (True, 10.0)])
s1 = pd.Series((0, 1, 2, 3), index = idx1)
s1.at[(False, 1.0)]   # returns '0', expected output

idx2 = pd.MultiIndex.from_tuples([(False,), (True,)])
s2 = pd.Series((0, 1), index = idx2)
s2.at[False]  # returns a pandas.Series with one element

```

#### Problem description

As per the user guide, I expect `at[]` to return a scalar. However, when I index a `Series` with a `MultiIndex` with only one level, `at[]` returns a `Series` with a single element. A `Series` with a multi-level `MultiIndex` works as expected.


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.6.12.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-54-generic
Version          : #60-Ubuntu SMP Fri Nov 6 10:37:59 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.1.post20201107
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : 1.3.2
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
187347339,14586,API: Index.append behaviour with categoricals,jorisvandenbossche,closed,2016-11-04T14:28:37Z,2020-11-29T15:59:57Z,"Follow-up of https://github.com/pandas-dev/pandas/pull/14545. 

We had a long discussion on what the behaviour of `concat` should be when you have categorical data: https://github.com/pandas-dev/pandas/pull/13767. In the end, for 0.19.0, we changed the behaviour of raising an error when categories didn't match to returning object dtyped data (only data with identical categories and ordered attributed gives a categorical as result). The table below is a summary of the changes between 0.18.1 and 0.19.0:

**For categorical Series:**

| left         | right        | append/concat 0.18 | append/concat 0.19.0    |
|---------|-------------------------------------------|---------|------------------------------|
| category | category (identical categories) | category | category |
| category | category (different categories) | error | object |
| category | not category | category | object  |
| category | not category (different categories) | category with NaNs | object |

However, we didn't change behaviour of `append` for Indexes (the above append is for series):

**For `CategoricalIndex`:**

| left         | right        | append 0.18 | append 0.19.0    | append 0.19.1 |
|---------|-------------------------------------------|---------|------------------------------|----|
| category | category (identical categories) | category | category | category |
| category | category (different categories) | error | error  | error  |
| category | not category | category | category |  category | 
| category | not category (with other values) | error | error  | error  |
| not category | category (with other values) | object | error | object

The last line, i.e. the case where the calling Index is not a CategoricalIndex, changed by accident in 0.19.0, and it is this that I corrected for in PR #14545 for 0.19.1.

Questions:

* Do we want the same behaviour for `Index.append` as we now have for `Series.append` with categorical data? This means that the column in the table above becomes 'object' apart from the first row.
* Do we want to make an exception for the case where the values in the 'right' correspond with the categories? (so that `pd.CategoricalIndex(['a', 'b', 'c']).append(pd.Index(['a']))`keeps working)

Changing this to always return object dtype unless for categoricals with indentical categories is easy, but gives a few failures in our test suite. Namely, in some indexing tests (indexing a DataFrame with a CategoricalIndex) there are changes in behaviour because indexing with a non-existing value in the index was performed using `CategoricalIndex.append()`. But this we can workaround in the indexing code of course.

cc @janschulz @sinhrks "
751838637,38101,BUG: Series.at returning Series with one element instead of scalar,phofl,closed,2020-11-26T21:44:00Z,2020-11-29T16:04:01Z,"- [x] closes #38053
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I think if we are at this place and len(values) is 1, we want to return a scalar instead of a Series. This happens only because loc is a slice (0,1) with one MultiIndex level instead of a scalar.

Should I create a file to move all MultiIndex at tests to the MultiIndex folder?"
554081140,31242,ENH: add use_nullable_dtypes option in read_parquet,jorisvandenbossche,closed,2020-01-23T10:51:32Z,2020-11-29T16:15:15Z,"xref #29752, #30929

Using some work I am doing in pyarrow (https://github.com/apache/arrow/pull/6189), we are able to provide an option in `read_parquet` to directly use new nullable dtypes instead of first using the default conversion (eg which gives floats for ints with nulls) and doing the conversion afterwards

"
751833194,38098,API: CategoricalIndex.append fallback to concat_compat,jbrockmendel,closed,2020-11-26T21:26:43Z,2020-11-29T16:17:11Z,"- [x] closes #14586
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

The main thing being changed here is `CategoricalIndex._concat` to better match the behavior of all the other `FooIndex._concat` methods.  Everything else being changed here is just cleanup that this makes possible.

cc @jorisvandenbossche "
715697324,36915,BUG: Intersection of multiindex returns duplicates,egorvavilov,closed,2020-10-06T13:48:54Z,2020-11-29T17:21:53Z,"```python
import pandas as pd

arraysA = [['val1', 'val1', 'val1', 'val1'], ['val2', 'val2', 'val2', 'val2']]
arraysB = [['val1'], ['val2']]

# MultiIndex([('val1', 'val2'),
#             ('val1', 'val2'),
#             ('val1', 'val2'),
#             ('val1', 'val2')],
#            names=['idx1', 'idx2'])
indexA = pd.MultiIndex.from_arrays(arraysA, names=('idx1', 'idx2'))
# MultiIndex([('val1', 'val2')],
#            names=['idx1', 'idx2'])
indexB = pd.MultiIndex.from_arrays(arraysB, names=('idx1', 'idx2'))

res = indexA.intersection(indexB)
```

#### Problem description

Intersection of multiindexes **must** produce result without duplicates. According to definition the intersection of two sets A and B, denoted by A ∩ B, is the set containing all elements of A that also belong to B (or equivalently, all elements of B that also belong to A. 

Version 1.0.3 gave correct output, versions 1.1.2 , 1,1,3  - not.
#### Output: Pandas 1.1.3
MultiIndex([('val1', 'val2'),
            ('val1', 'val2'),
            ('val1', 'val2'),
            ('val1', 'val2')],
           names=['idx1', 'idx2'])

#### Output: Pandas 1.0.3
MultiIndex([('val1', 'val2')],
           names=['idx1', 'idx2'])

#### Expected Output

I assume correct output should look like pandas 1.0.3 version:
MultiIndex([('val1', 'val2')], names=['idx1', 'idx2'])

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.17763
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.1.3
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 46.0.0
Cython           : None
pytest           : 6.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.3.0
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.5 (dt dec pq3 ext lo64)
jinja2           : 2.11.1
IPython          : 7.14.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : 1.0.6
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.17
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : None
None

</details>
"
656333261,35269,BUG: merge_ordered fails when left_by is set to more than one column,Rufflewind,closed,2020-07-14T05:19:32Z,2020-11-29T17:45:40Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import io, pandas
l = pandas.read_csv(io.StringIO('''
G H T 
g h 1   
g h 3  
'''), delim_whitespace=True)
r = pandas.read_csv(io.StringIO('''
T
2
'''), delim_whitespace=True)
pandas.merge_ordered(l, r, on=['T'], left_by=['G', 'H'])
```

#### Problem description

This fails:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python3.8/site-packages/pandas/core/reshape/merge.py"", line 290, in merge_ordered
    result, _ = _groupby_and_merge(
  File ""/usr/lib/python3.8/site-packages/pandas/core/reshape/merge.py"", line 162, in _groupby_and_merge
    merged[k] = key
  File ""/usr/lib/python3.8/site-packages/pandas/core/frame.py"", line 2938, in __setitem__
    self._set_item(key, value)
  File ""/usr/lib/python3.8/site-packages/pandas/core/frame.py"", line 3000, in _set_item
    value = self._sanitize_column(key, value)
  File ""/usr/lib/python3.8/site-packages/pandas/core/frame.py"", line 3636, in _sanitize_column
    value = sanitize_index(value, self.index, copy=False)
  File ""/usr/lib/python3.8/site-packages/pandas/core/internals/construction.py"", line 611, in sanitize_index
    raise ValueError(""Length of values does not match length of index"")
ValueError: Length of values does not match length of index
```

#### Expected Output

Not failing. Should return:

       G  H  T
    0  g  h  1
    1  g  h  3


For comparison, the above works fine if we use `left_by=['G']` and omit the `H` column entirely.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.7.7-arch1-1
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_FYL.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.1.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.5.1
matplotlib       : 3.2.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
714234614,36850,DOC: `.loc` behavior undocumented for Index argument,hickmanw,closed,2020-10-04T05:00:52Z,2020-11-29T18:06:24Z,"#### Location of the documentation

[pandas.Series.loc](https://pandas.pydata.org/docs/dev/reference/api/pandas.Series.loc.html)

[pandas.DataFrame.loc](https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.loc.html)

#### Documentation problem
The documentation does not mention `pandas.Index` as a valid argument for `.loc`. I expected the result to be the same as passing the values of the index, but the result is instead the same as `reindex` (as long as the result has at least one element, otherwise it raises `KeyError`). I discovered the difference when relying on the index name to remain unchanged after `.loc`. Below is a minimal demonstration of the current behavior for `pandas.Series`. Analogous behavior occurs for both axes of `pandas.DataFrame`.

```python
In  [2]: foo = pd.Index(range(2), name='foo')
         bar = pd.Index(range(1), name='bar')
         baz = pd.Index([2], name='baz')

         s = pd.Series(list('ab'), index=foo)
         s
Out [2]: 
         foo
         0    a
         1    b
         dtype: object

In  [3]: s.loc[bar]
Out [3]: 
         bar
         0    a
         dtype: object

In  [4]: s.loc[bar].index is bar
Out [4]: True

In  [5]: s.reindex(bar).index is bar
Out [5]: True

In  [6]: s.loc[baz]
Out [6]: KeyError: ""None of [Int64Index([2], dtype='int64', name='baz')] are in the [index]""
```

Output of `pd.show_versions`
<details>

INSTALLED VERSIONS

------------------

commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.6.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.193-149.317.amzn2.x86_64
Version          : #1 SMP Thu Sep 3 19:04:44 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.2
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.0
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.2.12
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.51.0
</details>

#### Suggested fix for documentation

Add an entry to explain the current/intended behavior when passing an `Index` to `.loc`, especially mentioning that the index will be replaced. Doing so would complement the clarity recently provided by #35506 for alignable boolean series."
752230879,38109,DOC: Add behavior for Index argument in DataFrame.loc,hongshaoyang,closed,2020-11-27T12:48:37Z,2020-11-29T18:06:28Z,"- [x] closes #36850
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
752592236,38129,REF: de-duplicate ndarray[datetimelike] wrapping,jbrockmendel,closed,2020-11-28T04:05:41Z,2020-11-29T18:19:45Z,
752824428,38144,CLN: remove unnecesary cast.maybe_convert_objects,jbrockmendel,closed,2020-11-29T02:57:19Z,2020-11-29T18:26:33Z,
747250868,37974,BUG: fix astype conversion string -> float,mlondschien,closed,2020-11-20T07:36:46Z,2020-11-29T19:04:23Z,"- [x] Is a step towards closing #37626
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Fixes the `string` -> `float` conversion as discussed in #37626 in presence of `pd.NA`. I used the same approach as used in `string` -> `Int64`. Note that differently to the `Int64` case, we return a `numpy.ndarray`, not a `pd.array`.

I added a test that compares `pd.Series`. Replacing this with `pd.array` the `tm.assert_numpy_array_equal` assertion will raise, as `expected` is of type `FloatingArray`. IIUC this is a new feature of pandas 1.2.0. Should we return this in `.astype(""float"")` by default?"
752382842,38117,ENH: include conversion to nullable float in convert_dtypes(),jorisvandenbossche,closed,2020-11-27T17:07:08Z,2020-11-29T19:29:16Z,"xref https://github.com/pandas-dev/pandas/issues/38110

There is one potentially corner case: what with floats that are all ""integer""-like? I think we want to keep returning nullable int for that, at least by default, and that is what I did now in this PR But we might want to add a parameter controlling that behaviour? (but that can also be added later on if there is demand for it)"
752593088,38130,REF: use np.where instead of maybe_upcast_putmask in nanops,jbrockmendel,closed,2020-11-28T04:12:17Z,2020-11-29T19:30:25Z,"This way we make at most one copy.  Also we get rid of the only use of maybe_upcast_putmask that has fill_value of anything other than np.nan, so in a follow-up it can be simplified."
752957710,38152,CLN: remove unreachable in maybe_cast_result,jbrockmendel,closed,2020-11-29T16:50:13Z,2020-11-29T19:41:35Z,
752792591,38142,DEPR: ExtensionOpsMixin -> OpsMixin,jbrockmendel,closed,2020-11-28T23:44:03Z,2020-11-29T20:25:44Z,"- [x] closes #37080
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
752948618,38151,CLN: remove unused coerce arg in NDFrame._convert,jbrockmendel,closed,2020-11-29T16:06:47Z,2020-11-29T22:03:29Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
645275617,34984,BUG: json.decode fails for nums larger than sys.maxsize,arw2019,closed,2020-06-25T07:44:52Z,2020-11-29T22:31:19Z,"- [x] closes #20599 
- [x] tests added
- [ ] tests passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Follow-up to #34473 "
753026991,38160,REF: implement disallow_invalid_ops,jbrockmendel,closed,2020-11-29T22:00:47Z,2020-11-29T23:06:25Z,"I've identified some bugs in _cython_operation, trying to address them in small pieces to make sure we get appropriate coverage.  this is a preliminary refactor."
444300380,26406,BUG: to_numpy on categorical data with tz aware datetime categories returns datetime64,jorisvandenbossche,closed,2019-05-15T08:21:12Z,2020-11-30T00:09:10Z,"```
In [1]: s = pd.Series(pd.Categorical(pd.date_range(""2012"", periods=3, tz='Europe/Brussels')))

In [2]: s.to_numpy() 
Out[2]: 
array(['2011-12-31T23:00:00.000000000', '2012-01-01T23:00:00.000000000',
       '2012-01-02T23:00:00.000000000'], dtype='datetime64[ns]')

In [3]: s = pd.Series(pd.date_range(""2012"", periods=3, tz='Europe/Brussels'))

In [4]: s.to_numpy() 
Out[4]: 
array([Timestamp('2012-01-01 00:00:00+0100', tz='Europe/Brussels', freq='D'),
       Timestamp('2012-01-02 00:00:00+0100', tz='Europe/Brussels', freq='D'),
       Timestamp('2012-01-03 00:00:00+0100', tz='Europe/Brussels', freq='D')],
      dtype=object)
```

We opted for the default behaviour of `to_numpy` to be as much preserving the values, so for normal datetimetz series, this is object dtype. So probably we should do the same for Categorical?

@TomAugspurger since `to_numpy` is still new, I suppose we can see this is as a bug fix and not change in behaviour (to not go through deprecation cycle)?"
752741687,38136,BUG: Categorical[dt64tz].to_numpy() losing tz,jbrockmendel,closed,2020-11-28T18:37:50Z,2020-11-30T00:11:37Z,"- [x] closes #26406
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
751660290,38089,BUG: merge_ordered fails with list-like left_by or right_by,GYHHAHA,closed,2020-11-26T15:05:37Z,2020-11-30T00:41:16Z,"- [x] closes #35269
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
751278508,38079,TST: mark all plotting tests as slow,ivanovmg,closed,2020-11-26T04:45:52Z,2020-11-30T03:11:16Z,"- [ ] xref https://github.com/pandas-dev/pandas/pull/37735#issuecomment-732530791
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Marked all plotting tests as slow globally at the top of each module."
667832696,35456,Add note about limited propagation of attrs,RobertRosca,closed,2020-07-29T13:09:59Z,2020-11-30T05:57:06Z,"Explicitly mention that `attrs` are not retained once a series is placed into a dataframe, related to issue #35425"
752998656,38158,"Revert ""DEPR: ExtensionOpsMixin -> OpsMixin""",jorisvandenbossche,closed,2020-11-29T20:22:52Z,2020-11-30T06:58:07Z,Reverts pandas-dev/pandas#38142 (see discussion over there)
272898934,18212,TypeError: invalid type promotion,kdebrab,closed,2017-11-10T11:12:56Z,2020-11-30T07:14:23Z,"I'm not sure where to post this, as it involves pandas, seaborn, matplotlib and numpy.

As it works with pandas 0.20.3, but throws an error in pandas 0.21.0, it seems to be a pandas issue. Below, an easily reproducible example:

```python
import pandas as pd
import seaborn as sns

index = pd.date_range('2017-06-01', periods=16)
df = pd.DataFrame(index=index, columns=['a', 'b', 'c'])
df.iloc[:2, 0] = '1000'
df.iloc[[2, 3, 6, 8, 10, 12, 14, 15], 1] = '1000'
df.iloc[[4, 5, 9, 13], 2] = '1000'
df.iloc[[7, 11], 1] = '1001'
df.index.name = 'Datetime'
df.columns.name = 'Element'
df = df.stack().rename('Event').reset_index()
df['Event / Element'] = df['Event'] + ' / ' + df['Element']
sns.stripplot(x='Datetime', y='Event / Element', hue='Element', data=df, orient='h')
```
Throws following error in pandas 0.21.0:
```
Traceback (most recent call last):
  File ""C:\Python27\lib\site-packages\IPython\core\interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-a21ce0484562>"", line 14, in <module>
    sns.stripplot(x='Datetime', y='Event / Element', hue='Element', data=df, orient='h')
  File ""C:\Python27\lib\site-packages\seaborn\categorical.py"", line 2603, in stripplot
    plotter.plot(ax, kwargs)
  File ""C:\Python27\lib\site-packages\seaborn\categorical.py"", line 1191, in plot
    self.draw_stripplot(ax, kws)
  File ""C:\Python27\lib\site-packages\seaborn\categorical.py"", line 1171, in draw_stripplot
    ax.scatter(strip_data, cat_pos, **kws)
  File ""C:\Python27\lib\site-packages\matplotlib\__init__.py"", line 1710, in inner
    return func(ax, *args, **kwargs)
  File ""C:\Python27\lib\site-packages\matplotlib\axes\_axes.py"", line 4087, in scatter
    offsets = np.column_stack([x, y])
  File ""C:\Python27\lib\site-packages\numpy\lib\shape_base.py"", line 353, in column_stack
    return _nx.concatenate(arrays, 1)
TypeError: invalid type promotion
```
numpy==1.13.3
seaborn==0.8.1
matplotlib==2.1.0

The above code does work fine in pandas 0.20.3, with same versions of numpy, seaborn and matplotlib."
709866404,36697,DEPR: is_all_dates,jbrockmendel,closed,2020-09-28T01:30:00Z,2020-11-30T09:59:37Z,"- [x] closes #23598
- [x] closes #27744
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
752984807,38155,Backport PR #36927: BUG: Fix duplicates in intersection of multiindexes,simonjayhawkins,closed,2020-11-29T19:10:15Z,2020-11-30T12:13:29Z,Backport PR #36927
753329770,38181,Backport PR #38148: ENH: Improve performance for df.__setitem__ with list-like indexers,simonjayhawkins,closed,2020-11-30T09:55:27Z,2020-11-30T12:36:08Z,Backport PR #38148
753243834,38177,"Revert ""REF: define nanargminmax without values_for_argsort""",jorisvandenbossche,closed,2020-11-30T07:53:14Z,2020-11-30T13:21:28Z,Reverts pandas-dev/pandas#37815
494545251,28481,combine_first returns unexpected results for timestamp dataframes,leo4183,closed,2019-09-17T10:43:18Z,2020-11-30T13:21:49Z,"#### Problem Description
`combine_first` returns weird results in case there exist unmatched columns between two timestamp dataframes.  this issue probably relates to #24357 (not retaining dtypes)

```python
import pandas as pd
x = pd.DataFrame([pd.NaT,pd.NaT],columns=['a']).T
y = pd.DataFrame([[pd.NaT,pd.NaT],pd.to_datetime(['20190101','20190102'])],index=['a','b'],columns=[1,2])
```

<details>

x|0|1
---|---|---
a|NaT|NaT

y|1|2
---|---|---
a|NaT|NaT
b|2019-01-01|2019-01-02

</details>


##### Output of ``x.combine_first(y)``

current|0|1|2
---|---|---|---
a|NaT|NaT|-9.223372e+18
b|NaT|2019-01-01|1.546387e+18

##### Output of ``x.reindex(columns=x.columns|y.columns).combine_first(y)``
expected|0|1|2
---|---|---|---
a|NaT|NaT|NaT
b|NaT|2019-01-01|2019-01-02


#### INSTALLED VERSIONS
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.18.0-80.7.2.el8_0.x86_64
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.1
numpy            : 1.17.2"
753342109,38184,DOC: trim error traceback in allows_duplicate_labels whatsnew entry,jorisvandenbossche,closed,2020-11-30T10:11:45Z,2020-11-30T13:34:28Z,"xref https://github.com/pandas-dev/pandas/issues/37784#issuecomment-733183201

Ensure a more readable section (shorter error traceback) by using a plain code-block instead of the dynamic ipython directive"
673382447,35563,BUG: freq set to NONE when resampling pd.Multiindex (introduced in v1.1.0),andreas-vester,closed,2020-08-05T09:08:42Z,2020-11-30T13:40:56Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
## Update/Summary

When dealing with a pd.multiindex the frequency `df.loc[i].index.freq` has not been set. This behavior has been introduced in v1.1.0

> ```
> idx = pd.Index(range(2), name=""A"")
> dti = pd.date_range(""2020-01-01"", periods=7, freq=""D"", name=""B"")
> mi = pd.MultiIndex.from_product([idx, dti])
> 
> df = pd.DataFrame(np.random.randn(14, 2), index=mi)
> 
> >>> df.loc[0].index
> ```
> 
> `df.loc[0].index` matches `dti`, so it would be nice to get the `freq=""D""` back on that.


> using code sample in [#35563 (comment)](https://github.com/pandas-dev/pandas/issues/35563#issuecomment-670246757) points to #31315
> 
> [c988567](https://github.com/pandas-dev/pandas/commit/c9885673d6a6f9f433c570d65b664b0c2ec8c6f0) is the first bad commit
> commit [c988567](https://github.com/pandas-dev/pandas/commit/c9885673d6a6f9f433c570d65b664b0c2ec8c6f0)
> Author: jbrockmendel [jbrockmendel@gmail.com](mailto:jbrockmendel@gmail.com)
> Date: Sun Feb 9 07:01:20 2020 -0800
> 
> ```
> REF: tighten what we accept in TimedeltaIndex._simple_new (#31315)
> ```

## Original post

#### Code Sample, a copy-pastable example

```python
import numpy as np
import pandas as pd

idx_level_0 = np.repeat([1, 2], 5)
dates = np.tile(
    [""2020-01-01"", ""2020-01-02"", ""2020-01-04"", ""2020-01-06"", ""2020-01-07""], 2
)
values1 = [1, 2, 3, 4, 5]
values2 = [6, 7, 8, 9, 10]

df = pd.DataFrame(
    {""idx_level_0"": idx_level_0, ""dates"": dates, ""values"": [*values1, *values2]}
)
df[""dates""] = pd.to_datetime(df[""dates""])
df = df.set_index([""idx_level_0"", ""dates""], drop=True)

df = df.groupby(""idx_level_0"").resample(""D"", level=""dates"").last()

# The following assertion is working properly in pandas v1.0.5
# It throws an error in pandas v1.1.0
assert df.index.get_level_values(1).freq == ""D""
```

#### Problem description

When resampling a `groupby`-object, the frequency will incorrectly be set to `None`.

#### Expected Output
The frequency should be set according to the resampled frequency.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.2.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : de_DE.cp1252
pandas           : 1.1.0
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 45.3.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>

"
753325823,38180,Rewrite dict literal for files in tests/reshape/,skvrahul,closed,2020-11-30T09:50:15Z,2020-11-30T14:28:29Z,"- [ ] xref #38138 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Rewrites as dictionary literals for the following files:
 - ./pandas/tests/reshape/concat/test_datetimes.py
 -  ./pandas/tests/reshape/merge/test_merge.py
 -  ./pandas/tests/reshape/merge/test_multi.py"
716762676,36955,Implement DataFrame.__array_ufunc__,TomAugspurger,closed,2020-10-07T18:36:37Z,2020-11-30T14:38:18Z,"For some cases, this will preserve extension types of arrays by calling
the ufunc blockwise.

```python
In [1]: import pandas as pd; import numpy as np
In [2]: df = pd.DataFrame({""A"": pd.array([0, 1], dtype=""Sparse"")})

In [3]: np.sin(df).dtypes
Out[3]:
A    Sparse[float64, nan]
dtype: object
```

Implementation-wise, this was done by moving `Series.__array_ufunc__` to `NDFrame` and making it generic for Series / DataFrame. The DataFrame implementation goes through `BlockManager.apply(ufunc)`.

We don't currently handle the multi-input case well for dataframes (aside from ufuncs that are implemented as dunder ops like `np.add`). For these, we fall back to the old implementation of converting to an ndarray and wrapping the result. This loses extension types.

We also don't currently handle multi-output ufuncs (like `np.modf`). This would require a `BlockManager.apply` that returns a `Tuple[BlockManager]`, `nout` per input block. Maybe someday, but that's low priority.

closes https://github.com/pandas-dev/pandas/issues/23743"
752985630,38156,CLN: remove _recast_datetimelike_result,jbrockmendel,closed,2020-11-29T19:14:28Z,2020-11-30T14:51:00Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
753034569,38161,"REF: more distinctive names for BaseGrouper.aggregate, transform",jbrockmendel,closed,2020-11-29T22:40:41Z,2020-11-30T14:52:25Z,ATM it is way too difficult to reason about where BaseGrouper.transform/aggregate is being called from.  This removes two one-liners to make it this easy.
753071777,38169,"fix frame, generic, series.py",liudj2008,closed,2020-11-30T01:23:30Z,2020-11-30T16:43:11Z,"- [x] xref #38138
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
        Identify unnecessary dict call - rewrite as a literal
"
752753760,38137,test backportability of #38120,simonjayhawkins,closed,2020-11-28T19:45:13Z,2020-11-30T17:34:55Z,#38120
753544213,38185,Backport PR #38120: API: preserve freq in DTI/TDI.factorize,simonjayhawkins,closed,2020-11-30T14:53:04Z,2020-11-30T18:27:10Z,Backport PR #38120
753117472,38172,CI: update tests for numpy 1.20 change to floordiv,jbrockmendel,closed,2020-11-30T03:47:40Z,2020-11-30T18:48:03Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Looks like https://github.com/numpy/numpy/pull/16161 changed numpy's floordiv behavior"
753580491,38186,Rewrite dict literal for files in tests/plotting/test_boxplot_method.py,Qbiwan,closed,2020-11-30T15:36:54Z,2020-11-30T18:52:31Z,"- [ ] xref #38138
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Rewrites as dictionary literals for the following files:
- pandas/tests/plotting/test_boxplot_method.py
"
753614256,38188,Rewrite dict literal for files in tests/plotting/frame/test_frame_subplots.py,Qbiwan,closed,2020-11-30T16:19:54Z,2020-11-30T18:53:56Z,"- [ ] xref #38138
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry 
   identify unnecessary dict call - rewrite as a literal
Rewrites as dictionary literals for the following files:

    pandas/tests/plotting/frame/test_frame_subplots.py
"
753716439,38191,Backport PR #38172 on branch 1.1.x (CI: update tests for numpy 1.20 change to floordiv),meeseeksmachine,closed,2020-11-30T18:48:37Z,2020-11-30T21:00:01Z,Backport PR #38172: CI: update tests for numpy 1.20 change to floordiv
672174809,35527,AssertionError in multidimensional indexing of a Series,TomAugspurger,closed,2020-08-03T15:53:45Z,2020-12-01T00:14:52Z,"In pandas 1.0.5, indexing a Series with a multidimensional key raised an IndexError. In 1.1.0 it raises a TypeError:

```python
In [1]: import pandas as pd

In [2]: s = pd.Series(pd.date_range(""2000"", periods=4, tz=""CET""))

In [3]: s[:, None]
```

```pytb
# 1.0.5
In [3]: s[:, None]
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-3-89522aec9762> in <module>
----> 1 s[:, None]

IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
```

```pytb
# 1.1.0
In [3]: s[:, None]
/Users/taugspurger/sandbox/pandas/pandas/core/indexes/range.py:716: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.
  return super().__getitem__(key)
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-3-89522aec9762> in <module>
----> 1 s[:, None]

~/sandbox/pandas/pandas/core/series.py in __getitem__(self, key)
    906             return self._get_values(key)
    907
--> 908         return self._get_with(key)
    909
    910     def _get_with(self, key):

~/sandbox/pandas/pandas/core/series.py in _get_with(self, key)
    921             )
    922         elif isinstance(key, tuple):
--> 923             return self._get_values_tuple(key)
    924
    925         elif not is_list_like(key):

~/sandbox/pandas/pandas/core/series.py in _get_values_tuple(self, key)
    951         # mpl hackaround
    952         if com.any_none(*key):
--> 953             result = self._get_values(key)
    954             deprecate_ndim_indexing(result, stacklevel=5)
    955             return result

~/sandbox/pandas/pandas/core/series.py in _get_values(self, indexer)
    966     def _get_values(self, indexer):
    967         try:
--> 968             return self._constructor(self._mgr.get_slice(indexer)).__finalize__(self,)
    969         except ValueError:
    970             # mpl compat if we look up e.g. ser[:, np.newaxis];

~/sandbox/pandas/pandas/core/internals/managers.py in get_slice(self, slobj, axis)
   1561         array = blk._slice(slobj)
   1562         block = blk.make_block_same_class(array, placement=slice(0, len(array)))
-> 1563         return type(self)(block, self.index[slobj])
   1564
   1565     @property

~/sandbox/pandas/pandas/core/internals/managers.py in __init__(self, block, axis, do_integrity_check, fastpath)
   1505     ):
   1506         assert isinstance(block, Block), type(block)
-> 1507         assert isinstance(axis, Index), type(axis)
   1508
   1509         if fastpath is not lib.no_default:

AssertionError: <class 'numpy.ndarray'>
```


We should continue to raise an IndexError here.

Reported in https://github.com/matplotlib/matplotlib/issues/18158."
753070985,38168,BUG/CLN: Minimize number of ResourceWarnings,twoertwein,closed,2020-11-30T01:20:42Z,2020-12-01T00:34:33Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

I think I got all `ResourceWarning`s when running:
`PYTHONWARNINGS=default pytest pandas/tests/io`"
752836104,38145,BUG: combine_first does not retain dtypes with Timestamp DataFrames,arw2019,closed,2020-11-29T04:31:12Z,2020-12-01T06:04:26Z,"- [x] closes #28481
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Picking up #35514"
742600652,37815,REF: define nanargminmax without values_for_argsort,jbrockmendel,closed,2020-11-13T16:49:20Z,2020-12-01T07:31:33Z,
753979528,38201,"Revert ""REF: define nanargminmax without values_for_argsort""",jbrockmendel,closed,2020-12-01T03:41:19Z,2020-12-01T07:31:50Z,Reverts pandas-dev/pandas#37815
749678978,38038,REGR: Performance regression on RollingGroupby,cpmbailey,closed,2020-11-24T12:49:16Z,2020-12-01T09:12:37Z,"pd.DataFrame({'a': np.random.randn(10000000), 'b': 1}).groupby('b').rolling(3).mean() is approximately 10x slower between 1.0.5 and 1.1.x"
753280418,38178,BUG: preserve nullable dtype for float result in IntegerArray/BooleanArray arithmetic ops,jorisvandenbossche,closed,2020-11-30T08:46:51Z,2020-12-01T09:32:52Z,"xref https://github.com/pandas-dev/pandas/issues/38110

This ensures that an operation on IntegerArray that results in floats (eg division) returns a FloatingArray instead of numpy float array.

This is only a small change in the actual code, the bulk of the changes is updating the tests. "
751836355,38099,BUG: fix wrong error message in deprecated 2D indexing of Series with datetime values,jorisvandenbossche,closed,2020-11-26T21:37:12Z,2020-12-01T10:02:42Z,"Closes #35527

For most Series types, this was properly raising the deprecation warning about 2D being deprecated, but for Series with datetimetz values, this started raising an AssertionError instead of only raising the warning.

"
754188381,38208,"Revert ""ENH: Improve performance for df.__setitem__ with list-like indexers""",simonjayhawkins,closed,2020-12-01T08:55:12Z,2020-12-01T10:54:05Z,Reverts pandas-dev/pandas#38148 xref https://github.com/pandas-dev/pandas/issues/37784#issuecomment-736200698
752925003,38148,ENH: Improve performance for df.__setitem__ with list-like indexers,phofl,closed,2020-11-29T14:07:15Z,2020-12-01T10:54:21Z,"- [x] closes #37954
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Reindexing the Block Manager improves the performance significantly. I hope I have not missed anything, concerning the reindexing of the blocks. 
Time spent in ``_ensure_listlike_indexer`` is pretty low now.
timeit result for the ops methods:

```
In [20]: %timeit setitem(x, x_col, df)
1.09 ms ± 9.65 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

In [21]: %timeit concat(x, x_col, df)
293 µs ± 4.29 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

Should we add tests here? I have added an asv to capture this case.

cc @jbrockmendel "
754245033,38210,Backport PR #38099: BUG: fix wrong error message in deprecated 2D indexing of Series with datetime values,jorisvandenbossche,closed,2020-12-01T09:58:27Z,2020-12-01T12:54:04Z,Backport PR #38099 on branch 1.1.x
754323912,38211,Backport PR #38057: PERF: fix regression in creation of resulting index in RollingGroupby,simonjayhawkins,closed,2020-12-01T11:42:22Z,2020-12-01T12:57:53Z,Backport PR #38057
751596363,38087,BLD: Only enable -Werror in the CI jobs,xhochy,closed,2020-11-26T13:34:40Z,2020-12-01T16:41:28Z,"closes #33315
closes #33314
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
454912920,26796,limit_area and limit_direction do not have an effect when interpolation method is 'pad',cchwala,closed,2019-06-11T21:30:21Z,2020-12-01T22:05:06Z,"#### Code Sample, a copy-pastable example if possible

```python
In [1]: import pandas as pd                                                                                                                                                                                                        

In [2]: import numpy as np                                                                                                                                                                                                         

In [3]: s = pd.Series([ 
   ...:     np.nan, 
   ...:     1., np.nan, 
   ...:     2., np.nan, np.nan, 
   ...:     5., np.nan, np.nan, np.nan, 
   ...:     -1., np.nan, np.nan 
   ...: ])                                                                                                                                                                                                                         

In [4]: s.interpolate(method='pad', limit_area='inside')                                                                                                                                                                           
Out[4]: 
0     NaN
1     1.0
2     1.0
3     2.0
4     2.0
5     2.0
6     5.0
7     5.0
8     5.0
9     5.0
10   -1.0
11   -1.0
12   -1.0
dtype: float64

# For method='linear' the `limit_area` kwarg works as expected

In [5]: s.interpolate(method='linear', limit_area='inside')                                                                                                                                                                        
Out[5]: 
0     NaN
1     1.0
2     1.5
3     2.0
4     3.0
5     4.0
6     5.0
7     3.5
8     2.0
9     0.5
10   -1.0
11    NaN
12    NaN
dtype: float64


```
#### Problem description

The kwargs `limit_area` and `limit_direction` for `interpolate()`, introduce in https://github.com/pandas-dev/pandas/pull/16513 do not seem to have an effect when using the method `pad`. They work with other interpolation methods, e.g. `linear`.

There are two different pathways for interpolate depending on the selected method. 
https://github.com/pandas-dev/pandas/blob/b8ad9da4213be55c587db015c640d927aeb7feaf/pandas/core/internals/blocks.py#L1096-L1116

For pad, ffill and bfill `_interpolate_with_fill()` is used which calls `missing.interpolate_2d` which does not seem to recognize the keywords limit_direction and limit_area. They are silently ignored.

I might be able to fix that during https://github.com/pandas-dev/pandas/pull/25141, but maybe a fix needs more fundamental changes.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: eaacefd0957bfc6735c8782ac2ba571d7131d250
python: 3.7.2.final.0
python-bits: 64
OS: Darwin
OS-release: 16.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: None
LOCALE: de_DE.UTF-8

pandas: 0.25.0.dev0+725.geaacefd09
pytest: 4.1.1
pip: 18.1
setuptools: 40.6.3
Cython: 0.29.2
numpy: 1.15.4
scipy: 1.2.0
pyarrow: 0.11.1
xarray: 0.11.0
IPython: 7.2.0
sphinx: 1.8.2
patsy: 0.5.1
dateutil: 2.7.5
pytz: 2018.9
blosc: None
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.9
feather: None
matplotlib: 3.0.2
openpyxl: 2.5.12
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.2
lxml.etree: 4.3.0
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.2.16
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: 0.2.0
fastparquet: 0.2.1
pandas_gbq: None
pandas_datareader: None
gcsfs: None


</details>
"
754734025,38218,BUG: pd's std function inconsistency between series and values within a series,zwcolin,closed,2020-12-01T21:19:12Z,2020-12-01T22:33:30Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
temp = pd.DataFrame({'col':[1,2]})
display(temp)
temp['col'].std(), temp['col'].values.std(), np.std(temp['col']), np.std(temp['col'].values)
```

#### Problem description

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution]

Results of std is inconsistent between a series and values within a series

#### Expected Output
(0.7071067811865476, 0.7071067811865476, 0.5, 0.5)
or
(0.5, 0.5, 0.5, 0.5)
#### Output of ``pd.show_versions()``
(0.7071067811865476, 0.5, 0.5, 0.5)
<details>

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
751983934,38106,"BUG in Series.interpolate: limit_area/limit_direction kwargs with method=""pad""/""bfill"" have no effect",arw2019,closed,2020-11-27T05:06:06Z,2020-12-01T22:45:17Z,"- [x] closes #26796
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Picking up #31048"
754536666,38212,CI: xfail test_area_lim,jbrockmendel,closed,2020-12-01T16:21:26Z,2020-12-01T23:47:35Z,
753658165,38189,DOC: fix redirects to user guide in README,erictleung,closed,2020-11-30T17:18:53Z,2020-12-02T01:44:25Z,"Documentation pages were moved so this commit updates those links to
open more directly rather than getting a ""This page has been moved
to..."" message.

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
752342716,38111,BUG: name retention in Index.intersection,jbrockmendel,closed,2020-11-27T15:56:23Z,2020-12-02T01:49:46Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

its going to take a few passes to get the behavior consistent across all subclasses.  once done, we can de-duplicate the boilerplate."
753673813,38190,REF: IntervalIndex.intersection match pattern in other intersection methods,jbrockmendel,closed,2020-11-30T17:42:05Z,2020-12-02T01:51:30Z,Looking to share the boilerplate and have subclasses just implement _intersection.
753823313,38193,DOC: Clarify development environment creation for documentation changes,ooojpeg,closed,2020-11-30T21:39:39Z,2020-12-02T01:52:29Z,"#### Location of the documentation

[https://pandas.pydata.org/docs/dev/development/contributing.html#creating-a-development-environment](contributing.html)

#### Documentation problem

The documentation for creating a development environment is not really clear regarding the benefits/limitations of creating a development environment when changing documentation.

#### Suggested fix for documentation

To test out code changes, you’ll need to build pandas from source, which requires a C/C++ compiler and Python environment. If you’re making documentation changes, you can skip to Contributing to the documentation but if you skip creating the development environment you won’t be able to build the documentation locally before pushing your changes.
"
747692661,37979,Should FilePathorBuffer use os.PathLike instead of pathlib.Path?,willfrey,closed,2020-11-20T18:01:26Z,2020-12-02T02:02:32Z,"The [documentation for `pandas.read_csv(...)`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) states that it can support anything that adheres to the `os.PathLike` protocol, but the definition of `FilePathOrBuffer` is too strict.

https://github.com/pandas-dev/pandas/blob/08e4baf13005d33136608bd4e6e15f6b38f50887/pandas/_typing.py#L138

where `Path` is a `pathlib.Path` object.

Could this be changed to use `os.PathLike[str]` or just `os.PathLike`?

```py3
from os import PathLike

FilePathOrBuffer = Union[""PathLike[str]"", FileOrBuffer]
```

`PathLike` is in quotes because `PathLike` only supports `__class_getitem__` for Python 3.9 and later and `PathLike` alone might be too broad, assuming you don't want to support byte-encoded paths.

Thanks!"
748930315,38018,Using os.PathLike instead of pathlib.Path (#37979),Abhi-H,closed,2020-11-23T16:07:29Z,2020-12-02T02:02:37Z,"- [x] closes #37979 
- [x] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Replaces `pathlib.Path` with `os.PathLike` to bring behaviour further in-line with the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)"
753068310,38167,BUG: merge_ordered should raise when elements in by not exist in df,GYHHAHA,closed,2020-11-30T01:10:58Z,2020-12-02T02:31:02Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

```python
>>>l = pd.DataFrame([['g', 'h', 1], ['g', 'h', 3]], columns=list('GHT'))
>>>l
   G  H  T
0  g  h  1
1  g  h  3
>>>r = pd.DataFrame([[2, 1]], columns=list('TE'))
>>>r
   T  E
0  2  1
```
Result

```python
>>>pd.merge_ordered(l, r, on='T', left_by=['G', 'h'])
```
|    | G   | H   |   T |   E | h   |
|---:|:----|:----|----:|----:|:----|
|  0 | G   | h   |   1 | nan | G   |
|  1 | G   | nan |   2 |   1 | G   |
|  2 | h   | nan |   2 |   1 | h   |
|  3 | h   | h   |   3 | nan | h   |

expected

```python
>>>pd.merge_ordered(l, r, on='T', left_by=['G', 'h'])
KeyError: ""['h'] not found in left columns""
```"
753067418,38166,BUG: merge_ordered gets unexpected left join result,GYHHAHA,closed,2020-11-30T01:07:19Z,2020-12-02T02:31:02Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

```python
>>>l = pd.DataFrame([['g', 'h', 1], ['g', 'h', 3]], columns=list('GHT'))
>>>l
   G  H  T
0  g  h  1
1  g  h  3
>>>r = pd.DataFrame([[2, 1]], columns=list('TE'))
>>>r
   T  E
0  2  1
```

results

```python
>>>pd.merge_ordered(l, r, on='T', left_by=['G'])
```
|    | G   | H   |   T |   E |
|---:|:----|:----|----:|----:|
|  0 | g   | h   |   1 | nan |
|  1 | g   | h   |   3 | nan |

expected

```python
>>>pd.merge_ordered(l, r, on='T', left_by=['G'])
```
|    | G   | H   |   T |   E |
|---:|:----|:----|----:|----:|
|  0 | g   | h   |   1 | nan |
|  1 | g   | h   |   2 |   1 |
|  2 | g   | h   |   3 | nan |"
751967653,38105,REF: implement _should_compare,jbrockmendel,closed,2020-11-27T04:11:58Z,2020-12-02T02:35:02Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Index._is_comparable_dtype is limited because it misses a few cases: object dtype, and sub-dtypes within categorical.  This implements Indes._should_compare, which handles those correctly.  It then implements `_get_indexer_non_comparable` for cases in which we short-circuit non-comparable dtypes.

As a proof of concept, this then uses _should_compare and _get_indexer_non_comparable for `PeriodIndex.get_indexer`.

The behavior change, which this tests, is one that IIUC is a bug.  That is, when we do get_indexer with a method and non-comparable dtypes, we should raise instead of return all minus-ones.

If implemented, we'll be able to use _should_compare to simplify all of the get_indexer, get_indexer_non_unique, and set op methods."
753073124,38170,BUG: unexpected merge_ordered results caused by wrongly groupby,GYHHAHA,closed,2020-11-30T01:28:22Z,2020-12-02T02:45:26Z,"- [x] closes #38166
- [x] closes #38167
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
753886579,38198,DOC: Duplicate Entry of Series.replace,vikramaditya91,closed,2020-11-30T23:48:42Z,2020-12-02T03:02:26Z,"#### Location of the 

https://pandas.pydata.org/pandas-docs/stable/reference/series.html

#### Documentation problem

There are two entries of `Series.replace`. One in the [Missing Data Handling](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#missing-data-handling) and another one in [Combining-Comparing-Joining-Merging](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#combining-comparing-joining-merging)

#### Suggested fix for documentation
Remove the entry from Combining, comparing, joining, merging
"
753886793,38199,Remove duplicate entry on series.rst,vikramaditya91,closed,2020-11-30T23:49:14Z,2020-12-02T03:02:30Z,"- [x] closes #38198 
- [ ] tests passed. Nothing new fails. Few tails already were failing on `master`
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry (Intentionally ignored as insignificant)

Removed the duplicate entry of `Series.replace` from doc"
749339692,38033,TST: dataframe constructor should preserve order with standard Python dicts,arw2019,closed,2020-11-24T04:51:48Z,2020-12-02T03:04:37Z,"We have a couple of dataframe tests (xref #13304) related to column ordering when constructed from an `OrderedDict`:
https://github.com/pandas-dev/pandas/blob/e4cf3abe0020aa801950e0383fffc13ee9703456/pandas/tests/frame/test_constructors.py#L1250-L1288

As of [PEP 468](https://www.python.org/dev/peps/pep-0468/) standard dictionaries are ordered so these tests should also work with `OrderedDict` replaced by `dict`"
754105345,38206,TST: Added dataframe constructor tests confirming order is preserved with standard Python dicts,jusexton,closed,2020-12-01T07:21:28Z,2020-12-02T03:04:46Z,"- [ ] closes #38033 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Adds additional test scenarios that confirm dataframes created with standard Python dicts preserve order as expected."
754163926,38207,Rewrite dict literal for files in pandas/tests/frame,Qbiwan,closed,2020-12-01T08:19:29Z,2020-12-02T03:13:02Z,"- [ ] xref #38138
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
identify unnecessary dict call - rewrite as a literal
Rewrites as dictionary literals for the following files:

	modified:   pandas/tests/frame/indexing/test_indexing.py
	modified:   pandas/tests/frame/indexing/test_where.py
	modified:   pandas/tests/frame/methods/test_fillna.py
	modified:   pandas/tests/frame/methods/test_sort_values.py
	modified:   pandas/tests/frame/methods/test_to_csv.py
	modified:   pandas/tests/frame/methods/test_to_records.py
	modified:   pandas/tests/frame/test_arithmetic.py
	modified:   pandas/tests/frame/test_reductions.py

However, I did not change it to literal in the following two places, as they seem like valid dict call:
1. pandas/tests/frame/indexing/test_indexing.py
```
lambda l: dict(zip(l, range(len(l)))),   # line 76
lambda l: dict(zip(l, range(len(l)))).keys(),  # line 77
```
2. pandas/tests/frame/indexing/test_where.py

```
return DataFrame(dict((c, s + 1) if is_ok(s) else (c, s) for c, s in df.items()))  # line 32
```"
754061242,38205,Rewrite dict literal for files in tests/groupby & ./pandas/tests/dtypes ,Qbiwan,closed,2020-12-01T06:30:45Z,2020-12-02T03:14:16Z,"- [ ] xref #38138
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
identify unnecessary dict call - rewrite as a literal
Rewrites as dictionary literals for the following files:

 ./pandas/tests/dtypes/test_inference.py
./pandas/tests/groupby/test_function.py
./pandas/tests/groupby/transform/test_transform.py

"
754006500,38203,Rewrite dict literal for files in ./pandas/tests/tools/ and  ./pandas/tests/resample/ ,AbdulMAbdi,closed,2020-12-01T04:53:03Z,2020-12-02T03:15:00Z,"- [ ] xref #38138
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Rewrote dict literal for the following files: 

 ./pandas/tests/tools/test_to_numeric.py
 ./pandas/tests/resample/test_time_grouper.py
 ./pandas/tests/util/test_assert_series_equal.py
 ./pandas/tests/util/test_assert_interval_array_equal.py
 ./pandas/tests/util/test_assert_index_equal.py
 ./pandas/tests/util/test_assert_extension_array_equal.py
 ./pandas/tests/io/test_parquet.py
"
754238167,38209,CI/TST: fix CI with numpy dev for changed error message / dev version,jorisvandenbossche,closed,2020-12-01T09:50:12Z,2020-12-02T10:15:59Z,
755135805,38230,Backport PR #38228 on branch 1.1.x (CI: pin pip to 20.2 on numpy-dev build),meeseeksmachine,closed,2020-12-02T10:15:59Z,2020-12-02T11:16:48Z,Backport PR #38228: CI: pin pip to 20.2 on numpy-dev build
755135946,38231,Backport PR #38209 on branch 1.1.x (CI/TST: fix CI with numpy dev for changed error message / dev version),meeseeksmachine,closed,2020-12-02T10:16:10Z,2020-12-02T11:38:41Z,Backport PR #38209: CI/TST: fix CI with numpy dev for changed error message / dev version
755069227,38228,CI: pin pip to 20.2 on numpy-dev build,jorisvandenbossche,closed,2020-12-02T08:45:21Z,2020-12-02T12:02:39Z,See https://github.com/pandas-dev/pandas/issues/38221
746464053,37954,"BUG: df.__setitem__ can be 10x slower than pd.concat(..., axis=1)",ivirshup,closed,2020-11-19T10:48:50Z,2020-12-02T12:10:26Z,"- [x] I have checked that this issue has not already been reported.
  * I think so, it's a bit hard to search for though. Nothing under concat and __setindex__

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd, numpy as np
from string import ascii_lowercase

def setitem(x, x_cols, df):
    new = pd.DataFrame(index=df.index)
    new[x_cols] = x
    new[df.columns] = df
    return new


def concat(x, x_cols, df):
    return pd.concat(
        [
            pd.DataFrame(x, columns=x_cols, index=df.index),
            df,
        ],
        axis=1,
    )

x = np.ones((1000, 10))
x_col = list(ascii_lowercase[:10])
df = pd.DataFrame(
    {
        ""str"": np.random.choice(np.array(list(ascii_lowercase)), size=1000),
        ""int"": np.arange(1000, dtype=int),
    }
)


%timeit setitem(x, x_col, df)
# 3.78 ms ± 193 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

%timeit concat(x, x_col, df)
# 306 µs ± 9.29 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

#### Problem description

This seems unintuitive from a performance perspective. I would assume that these would be close to equivalent. The `setitem` implementation might even be expected to have better performance due to less allocation.

While this is a stripped down example, the use case is building a dataframe to return from a function. Making a dataframe, then adding columns seemed like the natural idiom here.

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Oct 29 22:56:45 PDT 2020; root:xnu-6153.141.2.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.4
numpy            : 1.19.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.2
Cython           : 0.29.21
pytest           : 6.1.2
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.4
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 2.0.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.4
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : None
numba            : 0.51.2
```

</details>
"
632583420,34621,BUG: Period constructor does not take into account nanonseconds,OlivierLuG,closed,2020-06-06T16:48:21Z,2020-12-02T12:12:18Z,"- [x] I have checked that this issue has not already been reported.
- [x] I have confirmed this bug exists on the latest version of pandas.
- [x] (optional) I have confirmed this bug exists on the master branch of pandas.
---

#### Code Sample, a copy-pastable example:
```python
print(pd.Period(""2000-12-31 23:59:59.000000999"").to_timestamp().nanosecond)
print(pd.Period(""2000-12-31 23:59:59.000000500"").to_timestamp().nanosecond)
print(pd.Period(""2000-12-31 23:59:59.000000001"").to_timestamp().nanosecond)
print(pd.Period(""2000-12-31 23:59:59.000000001"").to_timestamp(freq='ns').nanosecond)
```
output:
```python
0
0
0
0
```

#### Problem description

Nanoseconds are set to `0` when using `to_timestamp`  function.

#### Expected Output
```python
999
500
1
1
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : c71bfc36211b5e2d860a06d8fbef902b757bd6e4
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-33-generic
Version          : #37-Ubuntu SMP Thu May 21 12:53:59 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : fr_FR.UTF-8
LOCALE           : fr_FR.UTF-8

pandas           : 1.1.0.dev0+1767.gc71bfc362
numpy            : 1.18.4
pytz             : 2020.1
dateutil         : 2.7.3
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : None
pytest           : 5.4.2
hypothesis       : 5.16.0
sphinx           : 3.0.3
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.9.0
bottleneck       : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.17
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
244870527,17053,BUG: Period with nanoseconds (?),ChadFulton,closed,2017-07-22T20:43:04Z,2020-12-02T12:12:18Z,"#### Code Sample, a copy-pastable example if possible

```python
p = pd.Period('2000-01-01 00:00:00.123456789', freq='N')
print(p.freq)
print(p)
print(p + pd.Timedelta(789, 'ns'))
```

This results in:

```
<Nano>
2000-01-01 00:00:00.123456000
2000-01-01 00:00:00.123456789
```
#### Problem description

From my reading of the docs, it appears that Period objects should accept nanosecond input, and they appear to support it, above, using `Timedelta`s.

The problem is that it appears to truncate the input strings.

#### Expected Output

```
<Nano>
2000-01-01 00:00:00.123456789
2000-01-01 00:00:00.123457578
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 2.7.6.final.0
python-bits: 64
OS: Darwin
OS-release: 15.6.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.20.3
pytest: None
pip: 9.0.1
setuptools: 21.0.0
Cython: 0.24
numpy: 1.13.1
scipy: 0.18.0.dev0+46034f7
xarray: None
IPython: 5.0.0b4
sphinx: 1.4.4
patsy: 0.2.1
dateutil: 2.2
pytz: 2016.4
blosc: None
bottleneck: None
tables: None
numexpr: 2.4
feather: None
matplotlib: 2.0.0rc2
openpyxl: 1.8.6
xlrd: 0.9.2
xlwt: None
xlsxwriter: None
lxml: 3.4.0
bs4: 4.3.2
html5lib: None
sqlalchemy: 1.0.14
pymysql: None
psycopg2: 2.6.1 (dt dec pq3 ext lo64)
jinja2: 2.8
s3fs: None
pandas_gbq: None
pandas_datareader: 0.2.1

</details>
"
758416542,38343,"ENH: Functions for creating data frames and series ""like"" an existing one with different values",felix-hilden,closed,2020-12-07T11:17:15Z,2020-12-12T21:09:25Z,"#### Problem
Sometimes I find myself needing a copy of a `DataFrame` or a `Series` with different values to the original. That is to say, with a similar index and column structure and naming. For example, it could occur when doing some numerical transformations which return NumPy arrays. The thing can of course be done by creating new objects by hand, but it becomes quite repetitive after a few lines.

#### Solution
Utilities like `np.zeros_like` are neat shorthands, and looks like Pandas has used them, at least in Indexing ([`reindex_like`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex_like.html)). I would like to see similar ways of constructing frames and series. I've written such shorthands for my personal use, but I think others could benefit from them being included in the library.

#### API breaking implications
This would introduce new functionality without affecting other parts of the system.

#### Describe alternatives you've considered
Doing it by hand is possible and pretty easy. It's only cumbersome, which the shorthand would solve.

#### Additional context
I'm not in the know of Pandas internal functionality, but the shorthands I've written are as follows:
```python
def df_like(data: Union[np.ndarray, pd.DataFrame], like: pd.DataFrame) -> pd.DataFrame:
    if isinstance(data, pd.DataFrame):
        data = data.values
    return pd.DataFrame(data, index=like.index, columns=like.columns)

def series_like(data: Union[np.ndarray, pd.Series], like: pd.Series) -> pd.Series:
    if isinstance(data, pd.Series):
        data = data.values
    return pd.Series(data, index=like.index, name=like.name)
```
If there's something else to account for, I would expect the functions to handle that as well. Maybe it could also be class method.
```python
class DataFrame:
    @classmethod
    def like(cls, data, like: ""DataFrame"") -> ""DataFrame"":
        if isinstance(data, cls):
            data = data.values
        return cls(data, index=like.index, columns=like.columns)
```

Looking forward to hearing your thoughts! I'd also be open to contributing these changes if an agreement is reached."
754619352,38214,TST: test_is_list_like_recursion segfaults sometimes?,twoertwein,closed,2020-12-01T18:18:17Z,2020-12-12T21:19:47Z,"- [ ] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```sh
pytest pandas/tests/dtypes/test_inference.py  # succeeds
pytest pandas/tests  # fails during test_inference 
```

```
pandas/tests/dtypes/test_generic.py ..                                   [ 25%]
pandas/tests/dtypes/test_inference.py .................................. [ 25%]
..............................................Fatal Python error: Segmentation fault

Thread 0x00007f56c7fff700 (most recent call first):
  File ""~/local/lib/python3.8/multiprocessing/pool.py"", line 576 in _handle_results
  File ""~/local/lib/python3.8/threading.py"", line 870 in run
  File ""~/local/lib/python3.8/threading.py"", line 932 in _bootstrap_inner
  File ""~/local/lib/python3.8/threading.py"", line 890 in _bootstrap

Thread 0x00007f56e4ff9700 (most recent call first):
  File ""~/local/lib/python3.8/multiprocessing/pool.py"", line 528 in _handle_tasks
  File ""~/local/lib/python3.8/threading.py"", line 870 in run
  File ""~/local/lib/python3.8/threading.py"", line 932 in _bootstrap_inner
  File ""~/local/lib/python3.8/threading.py"", line 890 in _bootstrap

Thread 0x00007f56e57fa700 (most recent call first):
  File ""~/local/lib/python3.8/selectors.py"", line 415 in select
  File ""~/local/lib/python3.8/multiprocessing/connection.py"", line 931 in wait
  File ""~/local/lib/python3.8/multiprocessing/pool.py"", line 499 in _wait_for_updates
  File ""~/local/lib/python3.8/multiprocessing/pool.py"", line 519 in _handle_workers
  File ""~/local/lib/python3.8/threading.py"", line 870 in run
  File ""~/local/lib/python3.8/threading.py"", line 932 in _bootstrap_inner
  File ""~/local/lib/python3.8/threading.py"", line 890 in _bootstrap

Thread 0x00007f56e5ffb700 (most recent call first):
  File ""~/local/lib/python3.8/multiprocessing/pool.py"", line 114 in worker
  File ""~/local/lib/python3.8/threading.py"", line 870 in run
  File ""~/local/lib/python3.8/threading.py"", line 932 in _bootstrap_inner
  File ""~/local/lib/python3.8/threading.py"", line 890 in _bootstrap

[...]

Thread 0x00007f57d95f5700 (most recent call first):
  File ""~/local/lib/python3.8/multiprocessing/pool.py"", line 114 in worker
  File ""~/local/lib/python3.8/threading.py"", line 870 in run
  File ""~/local/lib/python3.8/threading.py"", line 932 in _bootstrap_inner
  File ""~/local/lib/python3.8/threading.py"", line 890 in _bootstrap

Current thread 0x00007f5883dd3740 (most recent call first):
  File ""~/git/pandas/pandas/tests/dtypes/test_inference.py"", line 131 in foo
  File ""~/git/pandas/pandas/tests/dtypes/test_inference.py"", line 132 in foo
  File ""~/git/pandas/pandas/tests/dtypes/test_inference.py"", line 132 in foo
  File ""~/git/pandas/pandas/tests/dtypes/test_inference.py"", line 132 in foo
  File ""~/git/pandas/pandas/tests/dtypes/test_inference.py"", line 132 in foo
  File ""~/git/pandas/pandas/tests/dtypes/test_inference.py"", line 132 in foo
 [....]


```

That is probably not caused by something recent. I didn't run the entire test-suite in the past.

Maybe I just have a broken setup, ignoring test_inference leads to the next segfault (running test_json.py on its own works again):

```sh
pytest pandas/tests/extension/json/test_json.py  # succeeds
pytest --ignore=pandas/tests/dtypes/test_inference.py pandas/tests  # fails during test_json 
```


```
pandas/tests/extension/json/test_json.py ............................... [ 32%]
...x..........sFatal Python error: Segmentation fault

Thread 0x00007fc0197fa700 (most recent call first):
  File ""~/local/lib/python3.8/multiprocessing/pool.py"", line 576 in _handle_results
  File ""~/local/lib/python3.8/threading.py"", line 870 in run
  File ""~/local/lib/python3.8/threading.py"", line 932 in _bootstrap_inner
  File ""~/local/lib/python3.8/threading.py"", line 890 in _bootstrap

Thread 0x00007fc019ffb700 (most recent call first):
  File ""~/local/lib/python3.8/multiprocessing/pool.py"", line 528 in _handle_tasks
  File ""~/local/lib/python3.8/threading.py"", line 870 in run
  File ""~/local/lib/python3.8/threading.py"", line 932 in _bootstrap_inner
  File ""~/local/lib/python3.8/threading.py"", line 890 in _bootstrap

Thread 0x00007fc01a7fc700 (most recent call first):
  File ""~/local/lib/python3.8/selectors.py"", line 415 in select
  File ""~/local/lib/python3.8/multiprocessing/connection.py"", line 931 in wait
  File ""~/local/lib/python3.8/multiprocessing/pool.py"", line 499 in _wait_for_updates
  File ""~/local/lib/python3.8/multiprocessing/pool.py"", line 519 in _handle_workers
  File ""~/local/lib/python3.8/threading.py"", line 870 in run
  File ""~/local/lib/python3.8/threading.py"", line 932 in _bootstrap_inner
  File ""~/local/lib/python3.8/threading.py"", line 890 in _bootstrap

Thread 0x00007fc01affd700 (most recent call first):
  File ""~/local/lib/python3.8/multiprocessing/pool.py"", line 114 in worker
  File ""~/local/lib/python3.8/threading.py"", line 870 in run
  File ""~/local/lib/python3.8/threading.py"", line 932 in _bootstrap_inner
  File ""~/local/lib/python3.8/threading.py"", line 890 in _bootstrap

[...]

Current thread 0x00007fc1c0977740 (most recent call first):
  File ""~/git/pandas/pandas/core/indexes/base.py"", line 5888 in maybe_extract_name
  File ""~/git/pandas/pandas/core/series.py"", line 237 in __init__
  File ""~/git/pandas/pandas/core/construction.py"", line 666 in create_series_with_explicit_dtype
  File ""~/git/pandas/pandas/core/series.py"", line 382 in _init_dict
  File ""~/git/pandas/pandas/core/series.py"", line 289 in __init__
  File ""~/git/pandas/pandas/core/construction.py"", line 666 in create_series_with_explicit_dtype
  File ""~/git/pandas/pandas/core/series.py"", line 382 in _init_dict
  File ""~/git/pandas/pandas/core/series.py"", line 289 in __init__
  File ""~/git/pandas/pandas/core/construction.py"", line 666 in create_series_with_explicit_dtype
  File ""~/git/pandas/pandas/core/series.py"", line 382 in _init_dict
  File ""~/git/pandas/pandas/core/series.py"", line 289 in __init__
[...]
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : c2018c120021b6032e0bc52beb02b63bf7134c78
python           : 3.8.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-693.5.2.el7.x86_64
Version          : #1 SMP Fri Oct 20 20:32:50 UTC 2017
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+1400.gc2018c1
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 49.2.1
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : 5.37.1
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.3
fastparquet      : 0.4.1
gcsfs            : 0.7.1
matplotlib       : 3.3.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 2.0.0
pyxlsb           : None
s3fs             : 0.5.1
scipy            : 1.5.4
sqlalchemy       : None
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.51.2



</details>
"
764426805,38432,Added test_set_index_dtypes_on_empty_DataFrames to check dtypes after…,IngErnestoAlvarez,closed,2020-12-12T21:33:21Z,2020-12-12T22:11:04Z,"… using set_index on a DataFrame with empty Series that have dtype

- [x] closes #38419
- [x] tests added / passed test_set_index_dtypes_on_empty_DataFrames
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry. Added a test to check that dtypes don't change after using set_index on a DataFrame with empty Series
"
755201841,38232,TST: Assert msg with pytest raises in pandas/tests/extension/base,marktgraham,closed,2020-12-02T11:45:09Z,2020-12-12T23:00:28Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This PR adds messages to the bare pytest raises in reduce.py and getitem.py in pandas/test/extension/base. This PR references #30999."
603992826,33699,BUG: read_csv with both names and parse_date raises 'NoneType' TypeError,quazgar,closed,2020-04-21T13:20:34Z,2020-12-12T23:03:53Z,"This is probably similar to #14792, but still happens with 0.23.3 (a friend even tested it with 1.0.3).

## Testcase ##
```python
import io
s = """"""A,B,\n1,2""""""
pd.read_csv(io.StringIO(s), parse_dates=[""B""], names=[""B""])
```
raises:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-146-082b3d7afa0a> in <module>()
      1 import io
      2 s = """"""A,B,\n1,2""""""
----> 3 pd.read_csv(io.StringIO(s), parse_dates=[""B""], names=[""B""])

/usr/lib/python3/dist-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)
    676                     skip_blank_lines=skip_blank_lines)
    677
--> 678         return _read(filepath_or_buffer, kwds)
    679
    680     parser_f.__name__ = name

/usr/lib/python3/dist-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds)
    444
    445     try:
--> 446         data = parser.read(nrows)
    447     finally:
    448         parser.close()

/usr/lib/python3/dist-packages/pandas/io/parsers.py in read(self, nrows)
   1034                 raise ValueError('skipfooter not supported for iteration')
   1035
-> 1036         ret = self._engine.read(nrows)
   1037
   1038         # May alter columns / col_dict

/usr/lib/python3/dist-packages/pandas/io/parsers.py in read(self, nrows)
   1887
   1888                 values = self._maybe_parse_dates(values, i,
-> 1889                                                  try_parse_dates=True)
   1890                 arrays.append(values)
   1891

/usr/lib/python3/dist-packages/pandas/io/parsers.py in _maybe_parse_dates(self, values, index, try_parse_dates)
   1946
   1947     def _maybe_parse_dates(self, values, index, try_parse_dates=True):
-> 1948         if try_parse_dates and self._should_parse_dates(index):
   1949             values = self._date_conv(values)
   1950         return values

/usr/lib/python3/dist-packages/pandas/io/parsers.py in _should_parse_dates(self, i)
   1319             else:
   1320                 name = None
-> 1321             j = self.index_col[i]
   1322
   1323             if is_scalar(self.parse_dates):

TypeError: 'NoneType' object is not subscriptable
```

## Notes ##
This does not happen with `engine=""python""` (you only get a ParseError there)."
764388461,38431,BUG: read_csv raising TypeError for engine=c with names and parse_dates,phofl,closed,2020-12-12T21:04:30Z,2020-12-12T23:09:31Z,"- [x] closes #33699
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

"
757447677,38299,"BUG: find_common_type([interval[f8], interval[i8]]) returns object",jbrockmendel,closed,2020-12-04T22:23:58Z,2020-12-12T23:21:45Z,"I would expect it to return interval[f8], analogous to what we'd get with find_common_type([f8, i8])"
749330860,38032,BUG: series construction from dict of Timedelta scalar doesn't work,arw2019,closed,2020-11-24T04:28:02Z,2020-12-12T23:23:56Z,"Construction of `Series` from a dictionary of scalars sometimes fails:

#### Code Sample, a copy-pastable example

```python
In [31]: import pandas as pd
    ...: import pandas._testing as tm
    ...: 
    ...: td = pd.Timedelta(nanoseconds=500)
    ...: ser = pd.Series({""a"": td})
    ...: expected = pd.Series(td, index=[""a""], dtype=""timedelta64[ns]"")
    ...: 
    ...: tm.assert_series_equal(ser, expected)
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-31-a9c6a6312101> in <module>
      6 expected = pd.Series(td, index=[""a""], dtype=""timedelta64[ns]"")
      7 
----> 8 tm.assert_series_equal(ser, expected)

    [... skipping hidden 1 frame]

~/repos/pandas/pandas/_testing.py in assert_extension_array_equal(left, right, check_dtype, index_values, check_less_precise, check_exact, rtol, atol)
   1243         # Avoid slow object-dtype comparisons
   1244         # np.asarray for case where we have a np.MaskedArray
-> 1245         assert_numpy_array_equal(
   1246             np.asarray(left.asi8), np.asarray(right.asi8), index_values=index_values
   1247         )

    [... skipping hidden 1 frame]

~/repos/pandas/pandas/_testing.py in _raise(left, right, err_msg)
   1155             diff = diff * 100.0 / left.size
   1156             msg = f""{obj} values are different ({np.round(diff, 5)} %)""
-> 1157             raise_assert_detail(obj, msg, left, right, index_values=index_values)
   1158 
   1159         raise AssertionError(err_msg)

~/repos/pandas/pandas/_testing.py in raise_assert_detail(obj, message, left, right, diff, index_values)
   1085         msg += f""\n[diff]: {diff}""
   1086 
-> 1087     raise AssertionError(msg)
   1088 
   1089 

AssertionError: numpy array are different

numpy array values are different (100.0 %)
[index]: [a]
[left]:  [500]
[right]: [0]
```

Ran on 1.2 master"
561586460,31771,BUG: df.to_markdown() with empty frame incorrect output,ThibTrip,closed,2020-02-07T11:39:00Z,2020-12-12T23:28:05Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

df = pd.DataFrame({'id': [], 'first_name': [], 'last_name': []}).set_index('id')
print(df.to_markdown())
```
```
| id   | first_name   | last_name   |
||
```

#### Problem description

The markdown output of empty DataFrames does not seem right to me (see Expected Output).

#### Expected Output

```
| id   | first_name   | last_name   |
|-----:|-------------:|------------:|
```

#### Render difference

##### Current

| id   | first_name   | last_name   |
||


##### Expected

| id   | first_name   | last_name   |
|-----:|-------------:|------------:|


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.4.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 85 Stepping 4, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.2.1
hypothesis       : None
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.1
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.2.1
pyxlsb           : None
s3fs             : None
scipy            : 1.3.1
sqlalchemy       : 1.3.12
tables           : 3.5.2
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
xlsxwriter       : 1.2.1
numba            : 0.45.1

</details>
"
761893956,38406,BUG: DataFrame.to_markdown with an empty frame,arw2019,closed,2020-12-11T04:41:13Z,2020-12-12T23:28:09Z,"- [x] closes #31771
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Picking up #36233. 

Bumping the minimum version of tabulate as per https://github.com/pandas-dev/pandas/issues/31771#issuecomment-685401053"
747064598,37967,BUG: Limited available color name list when using style and to_excel,GentilsTo,closed,2020-11-20T01:28:12Z,2020-12-12T23:31:26Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd

df = pd.DataFrame( {'A': [1, 2, 3],
                   'B': [4, 5, 6],})

# /1 -- It Fails ---
df_out = (df.style.
          highlight_max(color='orangered').
          highlight_min(color='dodgerblue')
         )
df_out.to_excel('pandas_to_excel_color_issue.xlsx')

# /2 -- It works ----
df_out = (df.style.
          highlight_max(color='#FF4500'). # orangered
          highlight_min(color='#1E90FF') # dodgerblue)
         )
df_out.to_excel('pandas_to_excel_color_issue.xlsx')
```

#### Problem description

When using style, the `to_excel` function is not able to handle some common color name that otherwise works with an interactive python console.

See with code block /1 returns : 

```python
c:\program files\python38\lib\site-packages\pandas\io\formats\excel.py:331: CSSWarning: Unhandled color format: 'dodgerblue'
  warnings.warn(f""Unhandled color format: {repr(val)}"", CSSWarning)
c:\program files\python38\lib\site-packages\pandas\io\formats\excel.py:331: CSSWarning: Unhandled color format: 'orangered'
  warnings.warn(f""Unhandled color format: {repr(val)}"", CSSWarning)
```
And in the Excel file :
![image](https://user-images.githubusercontent.com/62483870/99742962-ed717080-2ad4-11eb-93be-98446dacf721.png)

While the /2 block is fine:
![image](https://user-images.githubusercontent.com/62483870/99743014-0c700280-2ad5-11eb-9f56-b02e3795f02a.png)


#### Expected Output

I understand that the to_excel function only accepts a minimal list of color names (based on the export engine I presume).
It would hence be great if pandas could manage the color Hex conversion under the hood for the user, and hence allow him to use a more large color name list (the one from matplotlib for example with `matplotlib.colors.cnames`), preventing the usage of unreadable color code.
In addition, I don't know if the current Excel output in case of CSS warning is really appropriate (the 'black render'), maybe simply ignoring the formatting would be better? (one could argue that it's more noticeable in the Excel file like that, that's right).


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.8.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en
LOCALE           : fr_FR.cp1252

pandas           : 1.1.4
numpy            : 1.19.3
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.2
Cython           : None
pytest           : 6.1.2
hypothesis       : None
sphinx           : 3.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.3.2
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : 0.16.0
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : None

</details>
"
761818176,38405,BUG: Series/DataFrame construction from scalars,jbrockmendel,closed,2020-12-11T02:44:24Z,2020-12-12T23:51:05Z,"- [x] closes #38032
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
761904911,38407,CLN: remove coerce_float keyword from construction.internals,jbrockmendel,closed,2020-12-11T04:55:45Z,2020-12-12T23:52:23Z,This sits on top of #38400
763181803,38421,REF: simplify maybe_casted_values,jbrockmendel,closed,2020-12-12T02:14:24Z,2020-12-12T23:53:23Z,The meat of this function boils down to a call to algorithms.take.
764097795,38428,REF: simplify maybe_upcast,jbrockmendel,closed,2020-12-12T17:39:07Z,2020-12-13T00:02:50Z,"It reduces to a 2-liner.  It is currently used in 4 places, 2 of which I'm looking to remove in the near future.  At that point it will make sense to inline those 2 lines and remove maybe_upcast entirely."
758032211,38335,BUG: iloc.__setitem__ with dict value and mixed dtypes,jbrockmendel,closed,2020-12-06T21:53:13Z,2020-12-13T00:03:40Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
762371565,38411,REF: move __from_arrow__ to common base class for numeric masked arrays,jorisvandenbossche,closed,2020-12-11T14:04:06Z,2020-12-13T07:30:24Z,xref https://github.com/pandas-dev/pandas/issues/38110
763990899,38425,ENH: Allow numerical values in groupby to be within a certain tolerance,Illviljan,closed,2020-12-12T15:58:30Z,2020-12-13T07:50:32Z,"#### Is your feature request related to a problem?
It is rather simple to group exact values together:
```python
# Example without noise:
import numpy as np
import pandas as pd

# Create input data without noise:
y = np.array([100, 100, 100, 100, 300, 300, 300, 300, 500, 500, 500, 500])
z = np.array([1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2])
time = np.arange(*y.shape)*0.1
df = pd.DataFrame(data=dict(y=y, z=z), index=time)

# Plot the groups:
df.y.groupby(by=df.y).plot(style=""o"")

```
yields
![image](https://user-images.githubusercontent.com/14371165/101986960-7ec9a200-3c91-11eb-900f-f201defda526.png)

But once the values are a little noisy the default groupby function does not work:

```python
# Example with noise where groupby is successful:
import numpy as np
import pandas as pd

# Create input data with noise:
y = np.array([100, 100, 100, 100, 300, 300, 300, 300, 500, 500, 500, 500])
z = np.array([1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2])
noise = 1 + 0.1*(np.random.rand(*y.shape) - 0.5)
y = y * noise
z = z * noise
time = np.arange(*y.shape)*0.1
df = pd.DataFrame(data=dict(y=y, z=z), index=time)

# Plot the groups:
df.y.groupby(by=df.y).plot(style=""o"")

```
yields
![image](https://user-images.githubusercontent.com/14371165/101987249-2eebda80-3c93-11eb-9ed1-dcefc5c3f570.png)


#### Describe the solution you'd like
I want to add the optional arguments `atol=0 `and `rtol=0` (similar to `np.isclose`) to groupby. 
Then `df.y.groupby(by=df.y, rtol=0.1).plot(style=""o"")` would yield the same groups as without any noise.


#### Describe alternatives you've considered
At the moment I have a function that does this. But it is not an elegant 1-liner. And at least I didn't think it was super obvious how to group like this either.

```python
# Example with noise with a workaround:
import numpy as np
import pandas as pd

# Create input data with noise:
y = np.array([100, 100, 100, 100, 300, 300, 300, 300, 500, 500, 500, 500])
z = np.array([1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2])
noise = 1 + 0.1*(np.random.rand(*y.shape) - 0.5)
y = y * noise
z = z * noise
time = np.arange(*y.shape)*0.1
df = pd.DataFrame(data=dict(y=y, z=z), index=time)


def groupby_isclose(series, atol=0, rtol=0):
    # Sort values to make sure values are monotonically increasing:
    s = series.sort_values()

    # Calculate tolerance value:
    tolerance = atol + rtol * s

    # Calculate a monotonically increasing index that increase when the
    # differnce between current and previous value changes:
    by = s.diff().fillna(0).gt(tolerance).cumsum()
    # s_old = s.shift().fillna(s)
    # by = ((s - s_old).abs() > tolerance).cumsum().sort_index()

    return by


# groupby values that are close:
by = groupby_isclose(df.y, rtol=0.1)

# Plot the groups:
df.y.groupby(by=by).plot(style=""o"")
```
yields
![image](https://user-images.githubusercontent.com/14371165/101987624-94d96180-3c95-11eb-9f30-88d33baaa06e.png)


#### API breaking implications
`groupby_isclose` sorts the values to determine when values changes significantly. Unless there's another way of calculating this it might make the `sort` argument unusable."
416000063,25497,regression in error message for DatetimeIndex subtraction,simonjayhawkins,closed,2019-03-01T08:50:26Z,2020-12-13T09:56:52Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
td = pd.Timedelta('1 days')
dti = pd.date_range('20130101', periods=3, name='bar')
td - dti
```
#### Problem description


0.25.0.dev0+179.g0a61ecdf6

```python-traceback
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexes\datetimelike.py"", line 507, in __rsub__
    result = self._data.__rsub__(maybe_unwrap_index(other))
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\arrays\datetimelike.py"", line 1325, in __rsub__
    return -(self - other)
TypeError: bad operand type for unary -: 'DatetimeArray'
```
#### Expected Output

same as 0.23.4

```python-traceback
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-2a4f19a48f3e> in <module>
      3 td = pd.Timedelta('1 days')
      4 dti = pd.date_range('20130101', periods=3, name='bar')
----> 5 td - dti

~\Anaconda3\lib\site-packages\pandas\core\indexes\datetimelike.py in __rsub__(self, other)
    949                 from pandas import DatetimeIndex
    950                 return DatetimeIndex(other) - self
--> 951             return -(self - other)
    952         cls.__rsub__ = __rsub__
    953 

~\Anaconda3\lib\site-packages\pandas\core\ops.py in invalid_op(self, other)
    174     def invalid_op(self, other=None):
    175         raise TypeError(""cannot perform {name} with this index type: ""
--> 176                         ""{typ}"".format(name=name, typ=type(self).__name__))
    177 
    178     invalid_op.__name__ = name

TypeError: cannot perform __neg__ with this index type: DatetimeIndex
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: 0a61ecdf6b4ea61a67afb4e3862df79adc07053a
python: 3.7.2.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 158 Stepping 9, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.25.0.dev0+179.g0a61ecdf6
pytest: 4.2.0
pip: 19.0.1
setuptools: 40.8.0
Cython: 0.29.5
numpy: 1.15.4
scipy: 1.2.1
pyarrow: 0.11.1
xarray: 0.11.3
IPython: 7.2.0
sphinx: 1.8.4
patsy: 0.5.1
dateutil: 2.7.5
pytz: 2018.9
blosc: None
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.9
feather: None
matplotlib: 3.0.2
openpyxl: 2.6.0
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.2
lxml.etree: 4.3.1
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.2.18
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: 0.2.0
fastparquet: 0.2.1
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
764578592,38436,"Backport PR #38331 on branch 1.2.x (BUG: first(""1M"") returning two months when first day is last day of month)",meeseeksmachine,closed,2020-12-12T23:32:42Z,2020-12-13T12:46:12Z,"Backport PR #38331: BUG: first(""1M"") returning two months when first day is last day of month"
757934641,38329,REF/TYP: Rename ABCIndexClass->ABCIndex and use cast,rhshadrach,closed,2020-12-06T14:10:12Z,2020-12-13T14:05:02Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
765426216,38449,BUG: to_parquet() / read_parquet() cannot round-trip datetime.timezone.utc,magic-david,closed,2020-12-13T14:20:07Z,2020-12-13T14:44:42Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import datetime
import pandas as pd

utc = datetime.timezone.utc
now = lambda: datetime.datetime.now(utc)

df = pd.DataFrame([[now(), ""a""], [now(), ""b""]], columns=(""date"", ""val""))
df.set_index('date', inplace=True)

df.to_parquet('/tmp/a.parquet')
d2 = pd.read_parquet('/tmp/a.parquet')

# Kaboom
```

#### Problem description

Current behaviour:

* Silently writes a Parquet file that cannot be re-read
* Failure appears to be a missing check during write, however error confusing appears during read

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.8.0.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-24-generic
Version          : #26-Ubuntu SMP Thu Nov 14 01:33:18 UTC 2019
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_GB.UTF-8
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.5
numpy            : 1.19.0
pytz             : 2020.4
dateutil         : 2.8.0
pip              : 20.3.1
setuptools       : 40.8.0
Cython           : 0.29.20
pytest           : 6.2.0
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.1
html5lib         : 1.1
pymysql          : None
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.20
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.52.0

</details>
"
765418215,38448,"Revert ""BUG: first(""1M"") returning two months when first day is last day of month""",simonjayhawkins,closed,2020-12-13T14:08:12Z,2020-12-13T15:45:08Z,Reverts pandas-dev/pandas#38331
757985230,38331,"BUG: first(""1M"") returning two months when first day is last day of month",phofl,closed,2020-12-06T18:02:05Z,2020-12-13T15:45:27Z,"- [x] closes #29623
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
230892434,16469,API/DOC: Specification for `names` parameter in read_csv,gfyoung,closed,2017-05-24T01:10:54Z,2020-12-13T17:08:29Z,"On `master` (<a href=""https://github.com/pandas-dev/pandas/commit/04356a83c0dc8a749c84e168535e6673f2548ec6"">04356a</a>), the documentation is unclear as to what a valid `names` parameter is for `read_csv`.

* Does it need to have the same length as the number of columns in the file? OR
* Does it need to have the same length as `usecols` if specified?

I find evidence in the test cases to support both (C engine allows both for example, but the Python engine disallows the former).  This point should also be clarified in the documentation.

xref #14671."
765293510,38445,BUG: read_csv usecols and names parameters inconsistent between c and python,phofl,closed,2020-12-13T11:17:41Z,2020-12-13T17:15:09Z,"- [x] closes #16469
- [x] tests added / passed The tests added by @gfyoung which were marked as xfail are passing now.
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
764935707,38440,TYP: pandas/core/frame.py (easy: bool/str) ,arw2019,closed,2020-12-13T04:39:41Z,2020-12-13T17:22:09Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Splitting this away from #38416"
508083756,29042,"BUG: read_csv with trailing comma, specified header names and usecols gives confusing error",jorisvandenbossche,closed,2019-10-16T20:33:37Z,2020-12-13T17:37:12Z,"Encountered this somewhat strange case: in case you have a malformed file (trailing comma's), we typically set the first column as the index. But if you also pass custom names, and use `usecols`, in that case you get a cryptic error message:

```
import io

s = """"""a, b, c, d 
1,2,3,4, 
5,6,7,8,"""""" 
```

```
>>> pd.read_csv(io.StringIO(s), header=0, names=['A', 'B', 'C', 'D'], usecols=[2,3]) 
...
ValueError: Passed header names mismatches usecols
```

I am not fully sure what the behaviour *should* be, but the current error message is not very helpful (since the usecols argument is only using integers, they are positional, and don't need to match the header names)"
763069830,38419,BUG: set_index screws up the dtypes on empty DataFrames,Rufflewind,closed,2020-12-11T23:53:48Z,2020-12-13T17:41:40Z,"- [x] I have checked that this issue has not already been reported.

  #30517 is similar in concept but relates to non-empty DataFrames, whereas the current bug is about empty ones.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas
d1 = pandas.DataFrame({'a': pandas.Series(dtype='datetime64[ns]'), 'b': pandas.Series(dtype='int64'), 'c': []})
d2 = d1.set_index(['a', 'b'])
assert (d1.loc[:, ['a', 'b']].dtypes == d2.index.to_frame().dtypes).all()
```

```
>>> d1.loc[:, ['a', 'b']].dtypes
a    datetime64[ns]
b             int64
dtype: object
>> d2.index.to_frame().dtypes
a    object
b     int64
dtype: object
```

#### Problem description

The dtype of the columns are silently changed when `.set_index` is called. This only happens when the DataFrame is empty, which suggests that this is an edge-case bug.

This is problematic because the behavior of `.set_index()` varies depending on whether the DataFrame has any rows.

#### Expected Output

The assertions should not fail.

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.9.0.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.9.13-arch1-1
Version          : #1 SMP PREEMPT Tue, 08 Dec 2020 12:09:55 +0000
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_FYL.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.5
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 50.3.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.6.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.4
sqlalchemy       : 1.3.20
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
```

</details>
"
764372540,38430,Fixed #38419 - BUG: set_index screws up the dtypes on empty DataFrames,jordi-crespo,closed,2020-12-12T20:53:22Z,2020-12-13T17:41:44Z,"
Fixes #38419 - BUG: set_index screws up the dtypes on empty DataFrames
- [ test_set_index_empty_dataframe] tests added / passed

"
751925409,38102,REF: use _validate_fill_value in Index.insert,jbrockmendel,closed,2020-11-27T01:39:00Z,2020-12-13T18:26:11Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
765506348,38452,ENH: Improve error message when names and usecols do not match for read_csv with engine=c,phofl,closed,2020-12-13T16:21:43Z,2020-12-13T18:29:29Z,"- [x] closes #29042
- [x] tests added / passed This case was already covered with differing error messages. I adjusted the message accordingly
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
764578233,38435,Backport PR #38247 on branch 1.2.x (BUG: Limited available colors),meeseeksmachine,closed,2020-12-12T23:32:26Z,2020-12-13T19:19:45Z,Backport PR #38247: BUG: Limited available colors
734074438,37568,BUG: Fix bug in combine_first with string dtype and only NA,phofl,closed,2020-11-01T22:11:31Z,2020-12-13T19:29:30Z,"- [x] closes #37519
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This caused bugs in ``Index.union()`` and ``Index.join()``  with ``how=outer`` too, but they are called from combine_first. Could add test, if this would be preferable. Hope Adding ExtensionArray to the if condition does not crash anything else. Run most categorical tests already locally."
763671116,38423,CLN: MultiIndex.union align with Index.union(),phofl,closed,2020-12-12T12:04:35Z,2020-12-13T20:01:07Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Similar to MultiIndex.intersection()

Stumbled over a issue in the maybe_match_names function. Can not return plain None, because MultiIndex.rename can not handle this.

cc @jbrockmendel "
757308550,38292,BUG: read_csv raises IndexError when index_col is passed,robertwb,closed,2020-12-04T18:23:59Z,2020-12-13T20:01:38Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import io
import pandas as pd

pd.read_csv(io.StringIO('a0, a1, a2\n    b0, b1, b2\n'), header=[0, 1], index_col=0)
```

#### Problem description

This is a legal, if empty, csv. Note that variations such as

```pd.read_csv(io.StringIO('a0, a1, a2\n'), header=[0], index_col=0)```,
```pd.read_csv(io.StringIO('a0, a1, a2\n    b0, b1, b2\n'), header=[0, 1])```, and
```pd.read_csv(io.StringIO('a0, a1, a2\n    b0, b1, b2\ndata,data'), header=[0, 1], index_col=0)``` work fine.

#### Expected Output

```
Empty DataFrame
Columns: [( a1,  b1), ( a2,  b2)]
Index: []
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.9.0.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.4
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.2.1
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
757806486,38325,"BUG: read_csv raising IndexError with multiple header cols, specified index_col and no data rows",phofl,closed,2020-12-05T23:43:55Z,2020-12-13T20:02:42Z,"- [x] closes #38292
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Parser returned an empty list instead of a list filled with None, which caused the IndexError."
765001866,38441,TYP: pandas/core/frame.py (easy: Axis/Level),arw2019,closed,2020-12-13T05:37:03Z,2020-12-13T22:04:47Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Splitting this away from #38416"
765558866,38455,REF: Separate values-casting from Block.astype,jbrockmendel,closed,2020-12-13T17:44:13Z,2020-12-14T00:36:08Z,"This separates Block._astype from Block.astype.  Block._astype only requires Block.values, the idea being that we can roll it into astype_nansafe before long."
752508418,38125,BUG: to/read_* do not use user-provided file handle if handle implements os.PathLike and also opened the file,twoertwein,closed,2020-11-27T23:55:08Z,2020-12-14T01:07:31Z,"- [x] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas (theoretically affected as well but my example doesn't work for <1.2 (need binary file handle) other examples should trigger this bug in <1.2).

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---



#### Code Sample, a copy-pastable example

```python
import os

import fsspec
import pandas as pd

# create a 'normal' file handle
with open(""abc.test"", mode=""w"") as open_obj:
    assert not isinstance(open_obj, os.PathLike)  # is not converted to a string
    position = open_obj.tell()

    # let to_csv write to the opened file
    pd.DataFrame({""a"": [1, 2, 3]}).to_csv(open_obj)

    # the position of the file buffer should have changed if to_csv used it
    assert open_obj.tell() != position


# create a file handle that also implements os.PathLike/has __fspath__
fsspec_obj = fsspec.open(""file://abc.test"", mode=""wb"").open()
with fsspec_obj:
    assert isinstance(fsspec_obj, os.PathLike)  # is converted to a string
    position = fsspec_obj.tell()

    # let to_csv write to the opened file
    pd.DataFrame({""a"": [1, 2, 3]}).to_csv(fsspec_obj)

    # the position of the file buffer should have changed if to_csv used it
    assert fsspec_obj.tell() != position  # fails
```

#### Problem description

`get_filepath_or_buffer` (<1.2) or `get_handle` (1.2) call `stringify_path` to convert `pathlib.Path` and other `os.PathLike` to a string. This string is then later opened. It seems that there is at least one file object that implements `os.PathLike` but at the same time already opens the file. In this case case, all `to/read_*` that use `get_handle` (or `get_filepath_or_buffer` in <1.2) extract the string and then open the file (even though the user already opened it).

I'm not sure whether there are other examples. I will look into how to fix this."
752770745,38141,BUG: do not stringify file-like objects,twoertwein,closed,2020-11-28T21:23:29Z,2020-12-14T01:09:00Z,"- [x] closes #38125
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I will finish this PR (fix the mypy issues) after #38018 is merged.
"
759832922,38373,REF: share IntervalIndex.intersection with Index.intersection,jbrockmendel,closed,2020-12-08T22:26:56Z,2020-12-14T01:22:44Z,"- [x] closes #38299
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This changes the return dtype of IntervalIndex.intersection when that intersection is empty. "
653519262,35185,"BUG: read_sql parse_dates does not allow for ""errors"" arg",Trollgeir,closed,2020-07-08T18:49:58Z,2020-12-14T01:41:44Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas

query = ...
connection = ...
parse_dates_dict = {'column_name': {'errors': 'coerce'}}
pandas.read_sql(query, parse_dates=parse_dates_dict)
```

#### Problem description

According to the api for [read_sql](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html), the `parse_date` argument should be able to contain a dict of dicts with kwargs per column to be passed along to `pandas.to_datetime()`, as shown in the example above.

However, this will result in the error:
`TypeError: to_datetime() got multiple values for keyword argument 'errors'`

This is because of [this line of code:](https://github.com/pandas-dev/pandas/blob/v1.0.5/pandas/io/sql.py#L81)
`return to_datetime(col, errors=""ignore"", **format)` - where `format` is our kwargs from that specific column.
Specifically, the hardcoded `errors` argument collides with the passed kwarg.

It seems like the `errors` argument cannot be overridden by kwargs in this implementation.

#### Expected Output

Expect `errors` arg in to be overridden by the kwargs argument, or that the documentation explicitly says that it cannot be overridden for some reason.

"
742801082,37823,BUG: Allow custom error values in parse_dates argument of read_sql like functions (GH35185),avinashpancham,closed,2020-11-13T21:31:54Z,2020-12-14T01:41:53Z,"- [x] closes #35185 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
760659156,38392,BUG: read_csv interpreting NA value as comment when NA contains comment string,phofl,closed,2020-12-09T20:46:56Z,2020-12-14T08:21:46Z,"- [x] xref #34002
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I  looked into the ``c`` case, but this is not that trivial as here. Comments are removed way before checking for na values. Would have to move that part way down to be able to do this."
765749314,38458,Backport PR #38141 on branch 1.2.x (BUG: do not stringify file-like objects),meeseeksmachine,closed,2020-12-14T01:10:30Z,2020-12-14T10:24:50Z,Backport PR #38141: BUG: do not stringify file-like objects
760396315,38386,REGR: assigning column with name that is registered as extension dtype errors,jorisvandenbossche,closed,2020-12-09T14:51:18Z,2020-12-14T14:11:03Z,"```
In [34]: df = pd.DataFrame([0])

In [35]: df[""Int64""] = [1]
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/scipy/pandas/pandas/core/generic.py in _set_item(self, key, value)
   3822         try:
-> 3823             loc = self._info_axis.get_loc(key)
   3824         except KeyError:

~/scipy/pandas/pandas/core/indexes/range.py in get_loc(self, key, method, tolerance)
    353                     raise KeyError(key) from err
--> 354             raise KeyError(key)
    355         return super().get_loc(key, method=method, tolerance=tolerance)

KeyError: 'Int64'

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
<ipython-input-35-26205ed43ee5> in <module>
----> 1 df[""Int64""] = [1]

~/scipy/pandas/pandas/core/frame.py in __setitem__(self, key, value)
   3161         else:
   3162             # set column
-> 3163             self._set_item(key, value)
   3164 
   3165     def _setitem_slice(self, key: slice, value):

~/scipy/pandas/pandas/core/frame.py in _set_item(self, key, value)
   3238         self._ensure_valid_index(value)
   3239         value = self._sanitize_column(key, value)
-> 3240         NDFrame._set_item(self, key, value)
   3241 
   3242         # check if we are modifying a copy

~/scipy/pandas/pandas/core/generic.py in _set_item(self, key, value)
   3824         except KeyError:
   3825             # This item wasn't present, just insert at end
-> 3826             self._mgr.insert(len(self._info_axis), key, value)
   3827             return
   3828 

~/scipy/pandas/pandas/core/internals/managers.py in insert(self, loc, item, value, allow_duplicates)
   1195 
   1196         # insert to the axis; this could possibly raise a TypeError
-> 1197         new_axis = self.items.insert(loc, item)
   1198 
   1199         if value.ndim == self.ndim - 1 and not is_extension_array_dtype(value.dtype):

~/scipy/pandas/pandas/core/indexes/numeric.py in insert(self, loc, item)
    172     def insert(self, loc: int, item):
    173         try:
--> 174             item = self._validate_fill_value(item)
    175         except TypeError:
    176             return self.astype(object).insert(loc, item)

~/scipy/pandas/pandas/core/indexes/numeric.py in _validate_fill_value(self, value)
    117         Convert value to be insertable to ndarray.
    118         """"""
--> 119         if is_bool(value) or is_bool_dtype(value):
    120             # force conversion to object
    121             # so we don't lose the bools

~/scipy/pandas/pandas/core/dtypes/common.py in is_bool_dtype(arr_or_dtype)
   1398         return arr_or_dtype.is_object and arr_or_dtype.inferred_type == ""boolean""
   1399     elif is_extension_array_dtype(arr_or_dtype):
-> 1400         return getattr(arr_or_dtype, ""dtype"", arr_or_dtype)._is_boolean
   1401 
   1402     return issubclass(dtype.type, np.bool_)

AttributeError: 'str' object has no attribute '_is_boolean'
```

This is something that started to fail recently. Getting this from the geopandas CI testing with pandas master."
765873656,38461,"TYP: DataFrame.to_gbq, DataFrame.to_html (easy: copy-paste from format module)",arw2019,closed,2020-12-14T04:28:12Z,2020-12-14T15:43:46Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Spinning off from #38416"
759077017,38353,PERF: IntervalIndex.isin,jbrockmendel,closed,2020-12-08T05:28:38Z,2020-12-14T15:52:36Z,"```
In [1]: import pandas as pd

In [2]: ii = pd.IntervalIndex.from_breaks(range(100000))

In [3]: values = ii[:100]

In [4]: from pandas.core.algorithms import isin

In [5]: %timeit isin(ii, values)
91.3 ms ± 5.24 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)  # <-- master
3.84 ms ± 57.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  # <-- PR
```"
766482812,38464,Backport PR #38427 on branch 1.2.x (REGR: Assigning label with registered EA dtype raises),meeseeksmachine,closed,2020-12-14T14:11:32Z,2020-12-14T16:11:17Z,Backport PR #38427: REGR: Assigning label with registered EA dtype raises
759945672,38382,REF: unify Index union methods,jbrockmendel,closed,2020-12-09T02:53:00Z,2020-12-14T16:15:04Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Sits on top of #38373 which focuses on intersection.

@simonjayhawkins this changes corner-case behavior for IntervalIndex.union, which we already changed in #38282 for 1.2.0.  I think this is unambiguously More Correct.  Question for you: should we try to get it in to 1.2.0 to avoid back-to-back behavior changes?"
560267731,31691,Inconsistent kwargs argument 'color' passed to upstream matplotlib plot functions,xuancong84,closed,2020-02-05T10:11:50Z,2020-12-14T17:27:38Z,"In matplotlib.pyplot.bar , there is a keyword argument called 'color' which can control the color of all bars as well as each bar, e.g.,
<img width=""639"" alt=""image"" src=""https://user-images.githubusercontent.com/10172392/73830682-08257200-4840-11ea-9aa0-64bb6bb2b9ab.png"">

However, in Pandas DataFrame.plot.bar, by passing a list into 'color', the color of all bars is controlled only by the 1st element in the list, i.e.,
<img width=""630"" alt=""image"" src=""https://user-images.githubusercontent.com/10172392/73831000-900b7c00-4840-11ea-8121-fe139365fdfc.png"">

Ironically, if we pass in a list of list into 'color', we can control the color of each bar, i.e.,
<img width=""651"" alt=""image"" src=""https://user-images.githubusercontent.com/10172392/73832150-9995e380-4842-11ea-9e35-6345364168b5.png"">

So my question is why the behavior of the 'color' argument different from that in matplotlib? Is this intended inconsistency?"
760482031,38387,Updated docstring for DataFrame.plot.bar graph clearify the use of color list,ankushduacodes,closed,2020-12-09T16:32:58Z,2020-12-14T17:27:45Z,"- [X] closes #31691 
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
660760403,35340,ENH: Select numeric ExtensionDtypes with DataFrame.select_dtypes,andrewgsavage,closed,2020-07-19T10:51:22Z,2020-12-14T17:58:22Z,"#### Is your feature request related to a problem?

I would like to plot ExtensionArrays. DataFrame.select_dtypes(np.number) is called when plotting, which filters out ExtensionArrays. 

#### Describe the solution you'd like

DataFrame.select_dtypes(np.number) should select columns where ExtensionDtype._is_numeric is True, suggested here https://github.com/pandas-dev/pandas/issues/26173#issuecomment-503761441 

#### API breaking implications

Not considered.

#### Describe alternatives you've considered

Not considered.

#### Additional context

N/A"
765860024,38459,REF: simplify _sanitize_column,jbrockmendel,closed,2020-12-14T04:08:07Z,2020-12-14T18:01:21Z,"It gets called indirectly from `iloc._setitem_with_indexer`, which we're trying to simplify"
762572728,38414,TST: add messages to bare pytest raises,marktgraham,closed,2020-12-11T16:24:17Z,2020-12-14T19:11:26Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This PR adds messages to the bare pytest raises in pandas/tests/generic/methods/test_pipe.py and pandas/tests/reshape/merge/test_merge_asof.py. This PR references #30999."
766767053,38466,CLN: astype_nansafe require dtype object,jbrockmendel,closed,2020-12-14T17:51:30Z,2020-12-14T21:17:53Z,
766772177,38467,CLN: unnecessary checks,jbrockmendel,closed,2020-12-14T17:57:27Z,2020-12-14T21:18:13Z,
699572911,36293,BUG: `DataFrame.drop` fails when there is a multiIndex without any level provided,galipremsagar,closed,2020-09-11T17:37:29Z,2020-12-14T23:55:16Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
>>> x = pd.DataFrame({""a"": range(10), ""b"": range(10, 20), ""d"": [""a"", ""v""] * 5}, index=pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],
...                              ['speed', 'weight', 'length']],
...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2, 1],
...                             [0, 1, 2, 0, 1, 2, 0, 1, 2, 1]]))
>>> x
               a   b  d
lama   speed   0  10  a
       weight  1  11  v
       length  2  12  a
cow    speed   3  13  v
       weight  4  14  a
       length  5  15  v
falcon speed   6  16  a
       weight  7  17  v
       length  8  18  a
cow    weight  9  19  v
>>> x.drop(index='cow')
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""/Users/pgali/PycharmProjects/del/venv1/lib/python3.7/site-packages/pandas/core/frame.py"", line 4169, in drop
    errors=errors,
  File ""/Users/pgali/PycharmProjects/del/venv1/lib/python3.7/site-packages/pandas/core/generic.py"", line 3884, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
  File ""/Users/pgali/PycharmProjects/del/venv1/lib/python3.7/site-packages/pandas/core/generic.py"", line 3933, in _drop_axis
    indexer = ~axis.isin(labels)
  File ""/Users/pgali/PycharmProjects/del/venv1/lib/python3.7/site-packages/pandas/core/indexes/multi.py"", line 3600, in isin
    values = MultiIndex.from_tuples(values, names=self.names)._values
  File ""/Users/pgali/PycharmProjects/del/venv1/lib/python3.7/site-packages/pandas/core/indexes/multi.py"", line 501, in from_tuples
    arrays = list(lib.tuples_to_object_array(tuples).T)
  File ""pandas/_libs/lib.pyx"", line 2471, in pandas._libs.lib.tuples_to_object_array
TypeError: Expected tuple, got str


```

#### Problem description

I'd expected `drop` to drop all the rows with `cow` as an index.

#### Expected Output

```python
>>> x.drop(index='cow')
               a   b  d
lama   speed   0  10  a
       weight  1  11  v
       length  2  12  a
falcon speed   6  16  a
       weight  7  17  v
       length  8  18  a
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.7.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_US.UTF-8
pandas           : 1.1.2
numpy            : 1.17.3
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.1.0
Cython           : None
pytest           : None
hypothesis       : 5.29.0
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None


</details>
"
767052931,38484,BUG: Segmentation Fault when read_table is given None,chaburkland,closed,2020-12-15T00:38:23Z,2020-12-15T00:43:31Z,"- [x] I have checked that this issue has not already been reported.

~~- [ ] I have confirmed this bug exists on the latest version of pandas.~~ (Not applicable)

~~- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.~~ (Not applicable)

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pyarrow.parquet as pq

pq.read_table(None)
```

#### Problem description

The above snippet will produce a Segmentation Fault, which is highly undesirable. The reason I discovered this, was I had a function that was supposed to return a filepath, but on my first iteration I forgot to return. Thus, when I ran my module with `pq.read_table(generate_fp())`, it produced a Segmentation Fault.

#### Expected Output

Ideally this will raise an `ValueError`, indicating to the user that `None` is an invalid source/filepath.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.8.3.final.0
python-bits: 64
OS: Linux
OS-release: 5.4.0-56-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 3.8.0
pip: 19.2.3
setuptools: 41.2.0
Cython: 0.29.13
numpy: 1.17.4
scipy: 1.1.0
pyarrow: 0.17.0
xarray: None
IPython: 7.14.0
sphinx: 3.1.2
patsy: None
dateutil: 2.6.1
pytz: 2019.1
blosc: None
bottleneck: None
tables: 3.6.1
numexpr: 2.6.9
feather: None
matplotlib: 3.1.1
openpyxl: 2.5.4
xlrd: 1.0.0
xlwt: None
xlsxwriter: 1.1.2
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: 1.3.16
pymysql: None
psycopg2: 2.8.5 (dt dec pq3 ext lo64)
jinja2: 2.10.3
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
767040493,38483,CI: --strict option in pytest failing builds,jreback,closed,2020-12-15T00:13:07Z,2020-12-15T00:58:27Z,"seems we are passing `--strict` to pytest, we must have picked up an updated version and this is now raising: https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=50126&view=logs&j=eab14f69-13b6-5db7-daeb-7b778629410b&t=ce687173-08c6-5301-838d-71b2dda24510

temporarily can just fix the pytest version or just remove this 

cc @pandas-dev/pandas-core "
766405686,38462,Update 10min.rst,aflah02,closed,2020-12-14T12:59:39Z,2020-12-15T04:31:51Z,"Added Links for Matplotlib Close along with a line explaining what it does.
I had faced difficulty when I read it for the first time and had to google search what close does

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
766880150,38470,Added Link to Conda Website in the README,aflah02,closed,2020-12-14T20:05:51Z,2020-12-15T04:32:58Z,"Added Link to Conda Website in the README for easy access especially for first-timers

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
766487872,38465,API: EAs should default _is_numeric / _is_boolean,jreback,closed,2020-12-14T14:16:13Z,2020-12-15T08:50:46Z,"I think we should define

```
@property
def _is_numeric(self) -> bool:
   return False

@property
def _is_boolean(self) -> bool:
   return False
```

on the EA base class.

Just makes less fragile checks on all EAs as we can assume the attributes exist"
767071633,38485,DOC: Missing ipython block in user_guide/indexing.rst,mzeitlin11,closed,2020-12-15T01:24:00Z,2020-12-15T09:02:13Z,"#### Location of the documentation
https://pandas.pydata.org/docs/user_guide/indexing.html

#### Documentation problem
Looks like a missing ipython block:
<img width=""866"" alt=""Screen Shot 2020-12-14 at 8 20 33 PM"" src=""https://user-images.githubusercontent.com/37011898/102155736-238edf80-3e4a-11eb-8ae3-db335d755b2b.png"">"
326123265,21192,groupby.apply modifies the index of an empty series,jluttine,open,2018-05-24T13:31:04Z,2020-12-15T09:27:27Z,"#### Code Sample, a copy-pastable example if possible

**Correct behaviour for non-empty series** - The index is kept unchanged:
```python
>>> pd.Series(index=pd.DatetimeIndex([""2018-01-01""]), data=[10]).groupby([1]).apply(lambda x: x).index
DatetimeIndex(['2018-01-01'], dtype='datetime64[ns]', freq=None)
```

**Incorrect behaviour for empty series** - The index is changed:
```python
>>> pd.Series(index=pd.DatetimeIndex([]), data=[]).groupby([]).apply(lambda x: x).index
Float64Index([], dtype='float64')
```

#### Problem description

The index should remain unchanged.

Why does this matter at all?

- Can't do operations on the result that work on datetime index but not on float index. For instance `.loc[""2018-01-01"":]`
- I'm using unit tests check that series is what is expected and now it's not because the index is something weird.

#### Expected Output

**Expected behaviour for empty series**:
```
>>> pd.Series(index=pd.DatetimeIndex([]), data=[]).groupby([]).apply(lambda x: x).index
DatetimeIndex([], dtype='datetime64[ns]', freq=None)
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.5.final.0
python-bits: 64
OS: Linux
OS-release: 4.14.42
machine: x86_64
processor: 
byteorder: little
LC_ALL: None>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.5.final.0
python-bits: 64
OS: Linux
OS-release: 4.14.42
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.22.0
pytest: None
pip: None
setuptools: 39.0.1
Cython: 0.28.1
numpy: 1.14.2
scipy: 1.0.1
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2018.3
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: None
openpyxl: 2.5.2
xlrd: 0.9.4
xlwt: 1.3.0
xlsxwriter: None
lxml: 4.2.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.6
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

LANG: en_GB.UTF-8
LOCALE: en_GB.UTF-8

pandas: 0.22.0
pytest: None
pip: None
setuptools: 39.0.1
Cython: 0.28.1
numpy: 1.14.2
scipy: 1.0.1
pyarrow: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2018.3
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: None
openpyxl: 2.5.2
xlrd: 0.9.4
xlwt: 1.3.0
xlsxwriter: None
lxml: 4.2.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.6
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
767008851,38478,CI: Use --strict-markers for pytest 6.0.2,mroeschke,closed,2020-12-14T23:15:52Z,2020-12-15T10:47:21Z,"closes #38483 

Found in the npdev build.

Noted in pytest changlog: https://docs.pytest.org/en/stable/changelog.html#deprecations
"
767349234,38496,Backport PR #38486 on branch 1.2.x (DOC: Fix missing ipython block in user_guide/indexing.rst),meeseeksmachine,closed,2020-12-15T09:03:17Z,2020-12-15T12:16:37Z,Backport PR #38486: DOC: Fix missing ipython block in user_guide/indexing.rst
767453596,38497,Backport PR #38478 on branch 1.2.x (CI: Use --strict-markers for pytest 6.0.2),meeseeksmachine,closed,2020-12-15T10:47:33Z,2020-12-15T13:07:39Z,Backport PR #38478: CI: Use --strict-markers for pytest 6.0.2
709811790,36688,ENH: Change Pandas User-Agent and add possibility to set custom http_headers to pd.read_* functions,astromatt,closed,2020-09-27T20:38:20Z,2020-12-15T13:34:23Z,"Currently Pandas makes HTTP requests using ""Python-urllib/3.8"" as a User Agent.
This prevents from downloading some resources and static files from various places.
What if, Pandas would make requests using ""Pandas/1.1.0"" headers instead?
There should be possibility to add custom headers too (`auth`, `csrf tokens`, `api versions` and so on).


Use Case:

I am writing a book on Pandas:
- https://python.astrotech.io/numerical-analysis/index.html#pandas

I published data in CSV and JSON to use in code listings:
- https://python.astrotech.io/_static/iris.json
- https://python.astrotech.io/_static/iris.csv

You can access those resources via browser, `curl`, or even `requests`, but not using Pandas.
The only change you'd need to do is to set User-Agent.
This is due to the `readthedocs.io` blocking ""Python-urllib/3.8"" User Agent for whatever reason.
The same problem affects many other places where you can get data (not only `readthedocs.io`).

Currently I get those resources with `requests` and then put `response.text` to one of:
- `pd.read_csv`
- `pd.read_json`
- `pd.read_html`

Unfortunately this makes even simplest code listings... quite complex (due to the explanation of `requests` library and why I do this like that).

Pandas uses `urllib.request.urlopen` which does not allow to set `http_headers`
https://github.com/pandas-dev/pandas/blob/master/pandas/io/common.py#L146

Although `urllib.request.urlopen` can take `urllib.request.Request` as an argument.
And `urllib.request.Request` object has possibility to set custom `http_headers`
https://docs.python.org/3/library/urllib.request.html#urllib.request.Request

Possibility to add custom `http_headers` should be in `pd.read_csv`, `pd.read_json` and `pd.read_html` functions.

From what I see, the `read_*` call stack is three to four function deep.
There are only 6 references in 4 files to `urlopen(*args, **kwargs)` function.
So the change shouldn't be quite hard to implement.

`http_headers` parameter can be `Optional[List]` which will be fully backward compatible and would not require any changes to others code."
767294795,38494,TST: don't use global fixture in the base extension tests,jorisvandenbossche,closed,2020-12-15T08:00:28Z,2020-12-15T15:01:31Z,"Follow-up on https://github.com/pandas-dev/pandas/pull/37867. In the discussion about using this fixture, I forgot that we should not use global pandas fixtures in those tests, but only the ones that are defined in the local /pandas/tests/extension/conftest.py, otherwise this doesn't work for downstream projects (eg https://github.com/geopandas/geopandas/issues/1735). For this case it's easier to simply not use a fixture, I think (but still share the common definition of the list)"
767074390,38486,DOC: Fix missing ipython block in user_guide/indexing.rst,mzeitlin11,closed,2020-12-15T01:31:15Z,2020-12-15T16:00:38Z,"- [x] closes #38485

"
754817425,38222,API: make tm.assert_foo_equal kwargs keyword-only for rarely-used keywords,jbrockmendel,closed,2020-12-02T00:00:30Z,2020-12-15T16:19:33Z,I expect this would need a deprecation cycle cc @jorisvandenbossche ?
767679002,38500,Backport PR #38494 on branch 1.2.x (TST: don't use global fixture in the base extension tests),meeseeksmachine,closed,2020-12-15T14:57:39Z,2020-12-15T16:30:28Z,Backport PR #38494: TST: don't use global fixture in the base extension tests
767569551,38498,ENH: Default Excel engine only compatible with XLS files -> replace by new one,dalonsoa,closed,2020-12-15T13:02:35Z,2020-12-15T16:41:59Z,"#### Is your feature request related to a problem?

With the release of [xlrd 2.0.0 the last 11th of December](https://github.com/python-excel/xlrd/blob/master/CHANGELOG.rst#200-11-december-2020), they drop support for any file format except the (very old) XLS. As this is the default Excel engine for Pandas, it is likely there will be a cascade of complains popping up very soon. 

#### Describe the solution you'd like

Replace the default `xlrd` engine by another engine compatible with modern file formats, maybe [openpyxl](https://openpyxl.readthedocs.io/en/stable/).

#### API breaking implications

Shouldn't have any... I think. 

#### Describe alternatives you've considered

There is a bunch of other possible alternative engines: http://www.python-excel.org

#### Additional context

None"
767679695,38501,Backport PR #38480 on branch 1.2.x (CI: Supress moto server logs in tests),meeseeksmachine,closed,2020-12-15T14:58:17Z,2020-12-15T17:18:09Z,Backport PR #38480: CI: Supress moto server logs in tests
741927391,37793,QST: Rolling pd.Grouper over N days,mattc-eostar,closed,2020-11-12T21:04:59Z,2020-12-15T17:52:06Z,"Starting from a desired date in a dataframe, can we group and sum the last 30 days and compare the prior 30 day period? And it would have to be days since this calculation could be done at any point in a month.

The first option below isn't very elegant, but I think gets to the right place. The second option using pd.Grouper, but chops off values during the group and produces different results and groups to the beginning of the period.

Any suggestions? Is there another function to try? Thanks!

```python
data = pd.DataFrame({
    'Timestamp': pd.date_range('8-11-2020', '10-12-20', freq='D'),
    'Qty': [4, 8, 5, 0, 6, 1, 0, 5, 4, 8, 4, 4, 8, 2, 6, 5, 3, 2, 4, 8, 0, 4,
       0, 9, 5, 3, 8, 2, 2, 4, 5, 7, 6, 7, 6, 5, 1, 6, 0, 1, 0, 5, 0, 2,
       3, 6, 5, 2, 5, 8, 3, 7, 8, 6, 4, 5, 8, 7, 7, 5, 9, 6, 6]
})
df = data.groupby(pd.Grouper(key='Timestamp', freq='D')).sum()[['Qty']].sort_index()
df = df.rolling('30D').sum().pct_change(1, freq='30D')
df.loc['10-12-2020']['Qty']

# or...

data \
    .groupby(pd.Grouper(key='Timestamp', freq='30D', label='right', closed='right')) \
    .sum()[['Qty']].sort_index().pct_change()
```
Using grouper begins the grouping of the Timestamp column from the earliest date to the latest. I would want to do the opposite."
748782866,38016,BUG:,Jacques2101,closed,2020-11-23T13:01:03Z,2020-12-15T18:40:37Z,"Hi i have a huge problem and I suppose it could be a bug but not sure. 

```
def volat(prices, n):
    n_years = prices.last_valid_index()-pd.DateOffset(years=n)
    return 100*np.sqrt(52)*prices[n_years:].asfreq('W-FRI', method='ffill').pct_change().std()


################### METHOD 1 ###################
table = (base_corr
         .query(""classification_produit=='Sans classification'"")
         [['date', 'gestionnaire', 'nom_de_produit', 'code_isin_part',
           'classification_produit', 'vl', 'collecte', 'nbre_part', 'actif_net_part']]
         .pivot(columns='date',
                index=['gestionnaire', 'nom_de_produit', 'code_isin_part', 'classification_produit'],
                values=['vl', 'collecte', 'nbre_part', 'actif_net_part'])
         .apply([
             lambda x: volat(x.vl, 3),
         ], axis=1)
         .set_axis([
             'volat 3 ans'
         ],
             axis='columns')
         )

table.loc[(slice(None),slice(None),'FR0011026905')] # part of Result from method 1


################### METHOD 2 ###################
volat(base.query(""code_isin_part=='FR0011026905'"").set_index('date')['vl'], 3)  # Result from method 2


################### METHOD 3 ###################
(base_corr
     .query(""code_isin_part=='FR0011026905'"")
     [['date', 'gestionnaire', 'nom_de_produit', 'code_isin_part',
       'classification_produit', 'vl', 'collecte', 'nbre_part', 'actif_net_part']]
     .pivot(columns='date',
            index=['gestionnaire', 'nom_de_produit', 'code_isin_part', 'classification_produit'],
            values=['vl', 'collecte', 'nbre_part', 'actif_net_part'])
     .apply([
         lambda x: volat(x.vl, 3),
     ], axis=1)
     .set_axis([
         'volat 3 ans'
     ],
         axis='columns')
     )
```

and the output is : 
```
RESULT FROM METHOD 1  (the wrong one) : 

			volat 3 ans
FIDEAS CAPITAL SAS	SICAV DES OLIVIERS	Sans classification	0.05

RESULT FROM METHOD 2 :
7.922765211671436

RESULT FROM METHOD 3 :
				volat 3 ans
FIDEAS CAPITAL SAS	SICAV DES OLIVIERS	FR0011026905	Sans classification	7.92

```

From my point of view I should get exactly the same answer (method 2 & 3 are alright but not method 1). 
I don't know why this is not the case. 
I need maybe to add that base_corr is a huge database (10_892_693 rows × 12 columns) with dailies and weekly data that why I convert each series to weekly data to make it possible to annualize the volatility figures (with np.sqrt(52) multiplication). 


INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.8.6.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 20.1.0
Version          : Darwin Kernel Version 20.1.0: Sat Oct 31 00:07:11 PDT 2020; root:xnu-7195.50.7~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.4
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.1.post20201107
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.6.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: 0.9.0
bs4              : 4.9.3
bottleneck       : None
fsspec           : 0.8.4
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 2.0.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.3
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : None
numba            : 0.51.2


</details>
"
689729869,36019,BUG: Read_Parquet replacing values with None,jaded-1,closed,2020-09-01T01:18:23Z,2020-12-15T18:41:48Z,"Tried reading a parquet file with data below. However, I see that some of the topics are replaced with None in the dataframe and I'm not sure what's causing this. This happens for only some rows.

example.parquet:
   {
        ""id"": ""035f57ed-ef07-4ec1-b8d4-04453b9b8fa1"",
        ""topics"": [
            ""GOVERNMENT"",
            ""ECONOMY"",
            ""POLITICS""
        ]
    },...
```
import pandas as pd

s3_path = 's3://test-bucket/example.parquet'
df = pd.read_parquet(s3_path)
df[['id', 'topics']][df['id'] == '035f57ed-ef07-4ec1-b8d4-04453b9b8fa1'].values

result:
array([['035f57ed-ef07-4ec1-b8d4-04453b9b8fa1',
        array(['GOVERNMENT', 'ECONOMY', None], dtype=object)]],
      dtype=object)
```
	"
706232776,36543,QST: Subclassing DataFrame creates problems with assigning to self when changing size,hydrogeoscience,closed,2020-09-22T09:49:14Z,2020-12-15T18:43:15Z,"I have successfully subclassed DataFrame (say into CustomDataFrame) according to the recommendations. I am now adding new custom methods to enhance my workflow. I have come across an issue which I have not found an answer to anywhere:

My CustomDataFrame has a DatetimeIndex set. When I manipulate the CustomDataFrame in my class, I am able to assign a _new_ DataFrame to _self_ without an error message, but _self_ does not update if _new_ has more, less or a different DatetimeIndex. I can drop rows using `inplace=True`. However, if I expand manually, I can only add one DatetimeIndex at a time in a loop: `self.loc[idx, :] = new.loc[idx, :]` (which is very slow). When _idx_ is an array, then Pandas throws an error: KeyError: ""None of [DatetimeIndex([], dtype='datetime64[ns, pytz.FixedOffset(600)]', name='datetime', length=19998, freq=None)] are in the [index]""

Does anyone know why that is and how I can overcome this issue?
"
754785461,38220,BUG: DataFrame.drop raising TypeError for string label with non-unique MultiIndex and no level provided,phofl,closed,2020-12-01T22:49:48Z,2020-12-15T18:46:47Z,"- [x] closes #36293
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

As far as I know, isin needs a level input when only a string is given for a MultiIndex. If this should not change, we have to set level=0 in this case."
749066194,38022,STYLE use types_or in pre-commit,MarcoGorelli,closed,2020-11-23T19:28:22Z,2020-12-16T00:46:50Z,"[pre-commit 2.9 adds support for types_or](https://github.com/pre-commit/pre-commit/releases/tag/v2.9.0), which would help clear up some of the less readable regexes like `\.(py|pyx|rst)$`, replacing them with `types_or: [python, cython, rst]`"
765707683,38457,STYLE: use types_or in pre-commit,rkc007,closed,2020-12-13T23:51:27Z,2020-12-16T00:46:54Z,"- [x] closes #38022 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
766815091,38468,"TYP : DataFrame.(merge, join)  core.reshape.merge.(merge, ...) (easy)",arw2019,closed,2020-12-14T18:46:35Z,2020-12-16T01:09:31Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
768034453,38507,REF: handle ravel inside astype_nansafe,jbrockmendel,closed,2020-12-15T18:59:45Z,2020-12-16T01:14:31Z,
759853393,38375,DOC: infer_dtypes document is inconsistent with behaviour regarding boolean,tsinggggg,closed,2020-12-08T23:08:13Z,2020-12-16T02:14:18Z,"#### Location of the documentation

https://github.com/pandas-dev/pandas/blob/0711b7adb3150f14461c5d1a111a87a83a45bc90/pandas/_libs/lib.pyx#L1315


#### Documentation problem

In this example the return value should be 'boolean' as mentioned in this test case https://github.com/pandas-dev/pandas/blob/1db3aa2b8ae8bd3b20dea14b92e1e7355bfa266a/pandas/tests/dtypes/test_inference.py#L679

#### Suggested fix for documentation

change 'mix' to 'boolean'
"
767652841,38499,DOC: Fix incorrect doc string for infer_dtype (#38375),tsinggggg,closed,2020-12-15T14:30:52Z,2020-12-16T02:14:22Z,"- [x] closes #38375
- ~~tests added / passed~~
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- ~~whatsnew entry~~

A minor fix to https://github.com/pandas-dev/pandas/issues/38375 correcting the output of an example in `infer_dtype` doc string
"
387040846,24079,DataFrame.fillna() fails with categorical columns present when trying to fill missing values in numeric columns with NaNs,dragoljub,open,2018-12-03T22:49:39Z,2020-12-16T02:16:01Z,"#### Code Sample, a copy-pastable example if possible

```python
import numpy as np
import pandas as pd
df = pd.DataFrame(np.random.randint(0, 3, (3, 3)), columns=list('ABC')).astype(np.float32)
df.B.iloc[1] = None
df.A = df.A.astype('category')

df.fillna(-9999)

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-8-dcdff14fcba0> in <module>
----> 1 df.fillna(-9999)

D:\Python37\lib\site-packages\pandas\core\frame.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs)
   3788                      self).fillna(value=value, method=method, axis=axis,
   3789                                   inplace=inplace, limit=limit,
-> 3790                                   downcast=downcast, **kwargs)
   3791 
   3792     @Appender(_shared_docs['replace'] % _shared_doc_kwargs)

D:\Python37\lib\site-packages\pandas\core\generic.py in fillna(self, value, method, axis, inplace, limit, downcast)
   5425                 new_data = self._data.fillna(value=value, limit=limit,
   5426                                              inplace=inplace,
-> 5427                                              downcast=downcast)
   5428             elif isinstance(value, DataFrame) and self.ndim == 2:
   5429                 new_data = self.where(self.notna(), value)

D:\Python37\lib\site-packages\pandas\core\internals.py in fillna(self, **kwargs)
   3706 
   3707     def fillna(self, **kwargs):
-> 3708         return self.apply('fillna', **kwargs)
   3709 
   3710     def downcast(self, **kwargs):

D:\Python37\lib\site-packages\pandas\core\internals.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)
   3579 
   3580             kwargs['mgr'] = self
-> 3581             applied = getattr(b, f)(**kwargs)
   3582             result_blocks = _extend_blocks(applied, result_blocks)
   3583 

D:\Python37\lib\site-packages\pandas\core\internals.py in fillna(self, value, limit, inplace, downcast, mgr)
   2004                mgr=None):
   2005         values = self.values if inplace else self.values.copy()
-> 2006         values = values.fillna(value=value, limit=limit)
   2007         return [self.make_block_same_class(values=values,
   2008                                            placement=self.mgr_locs,

D:\Python37\lib\site-packages\pandas\util\_decorators.py in wrapper(*args, **kwargs)
    176                 else:
    177                     kwargs[new_arg_name] = new_arg_value
--> 178             return func(*args, **kwargs)
    179         return wrapper
    180     return _deprecate_kwarg

D:\Python37\lib\site-packages\pandas\core\arrays\categorical.py in fillna(self, value, method, limit)
   1754             elif is_hashable(value):
   1755                 if not isna(value) and value not in self.categories:
-> 1756                     raise ValueError(""fill value must be in categories"")
   1757 
   1758                 mask = values == -1

ValueError: fill value must be in categories
```
#### Problem description
fillna() fails on a data frame that has categorical columns without any missing values and numeric columns that have some missing values. It appears that fillna is attempting to fill categorical columns even though they have no missing values. 

#### Expected Output
I expected the missing value in the numeric column to be filled but the categorical columns without any missing values to be untouched.

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit: None
python: 3.7.1.final.0
python-bits: 64
OS: Windows
OS-release: 2008ServerR2
machine: AMD64
processor: Intel64 Family 6 Model 63 Stepping 2, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.23.4
pytest: 4.0.0
pip: 18.1
setuptools: 40.6.2
Cython: 0.29
numpy: 1.15.4
scipy: 1.1.0
pyarrow: 0.11.1
xarray: 0.11.0
IPython: 7.1.1
sphinx: 1.8.2
patsy: 0.5.1
dateutil: 2.7.5
pytz: 2018.7
blosc: 1.6.2
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.8
feather: None
matplotlib: 3.0.2
openpyxl: 2.5.10
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: None
lxml: 4.2.5
bs4: 4.6.3
html5lib: 1.0.1
sqlalchemy: 1.2.14
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: 0.1.6
pandas_gbq: None
pandas_datareader: None

</details>
"
767013268,38479,REF: share unboxing code,jbrockmendel,closed,2020-12-14T23:24:24Z,2020-12-16T02:17:58Z,
767081624,38487,REF: simplify maybe_upcast_putmask,jbrockmendel,closed,2020-12-15T01:49:49Z,2020-12-16T02:19:42Z,
768262518,38512,BUG: .item() incorrectly casts td64/dt64 to int,jbrockmendel,closed,2020-12-15T22:32:47Z,2020-12-16T02:45:48Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
768751011,38520,BUG: method fillna not working when supplied value is a dict,etiennekintzler,closed,2020-12-16T11:28:29Z,2020-12-16T12:24:44Z,"#### Reproducible example

```python
import pandas as pd

s = pd.Series([{""a"": 1}, None])
s.fillna({""a"": 0})
```

returns:
```python
0    {'a': 1}
1         NaN
dtype: object
```

The error is the same when using `np.nan` instead of None.

#### Problem description

The fillna method does not work when the supplied value is a dict.

#### Expected Output

```python
0    {'a': 1}
1    {'a': 0}
dtype: object
```

#### Problem identification

I check the internals of `NDFrame.fillna` the problem seems to be the following lines : 
https://github.com/pandas-dev/pandas/blob/122d50246bcffcf8c3f252146340ac02676a5bf6/pandas/core/generic.py#L6379-L6382
Indeed these lines build the value as :  
```python
value = create_series_with_explicit_dtype(dict(a=0), dtype_if_empty=object)
```
which returns
```
a    0
dtype: int64
```
This index has no overlap with `s.index` and reindex value is :
```python
0   NaN
1   NaN
dtype: float64
```
which seems wrong.

However passing the dict value without the transformation done in the block beginning `if isinstance(value, (dict, ABCSeries))` seems to work fine  : 

```python
new_data = s._mgr.fillna(value=dict(a=0), downcast=None, inplace=None, limit=None)
s._constructor(new_data)

#returns
#0    {'a': 0}
#1    {'a': 1}
#dtype: object
```

(not saying that this approach should be pursue since I am not familiar with pandas internals and thus don't know what the consequences of doing like this could be)

INSTALLED VERSIONS
------------------
commit           : b5958ee1999e9aead1938c0bba2b674378807b3d
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-122-generic
Version          : #124-Ubuntu SMP Thu Oct 15 13:03:05 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.5
numpy            : 1.19.4
pytz             : 2020.4
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 50.3.2
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.4
sqlalchemy       : 1.3.20
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None


</details>

"
419351305,25650,Support of read_excel password protected files,prsh9,closed,2019-03-11T08:42:45Z,2020-12-16T14:25:17Z,"#### Problem description
Most of the times the excel files are password protected and hence unable to be read by the read_excel unless some sort of pre-processing or manual work. Need support for reading the excel files through win32com or different API.

#### Output of ``pd.show_versions()``
<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-1072-aws
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.3
pytest: None
pip: 18.0
setuptools: 40.4.1
Cython: 0.24.1
numpy: 1.11.1
scipy: 0.18.1
pyarrow: 0.8.0
xarray: None
IPython: 2.2.0
sphinx: None
patsy: 0.4.1
dateutil: 2.5.3
pytz: 2016.6.1
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 1.5.3
openpyxl: 2.3.2
xlrd: 1.2.0
xlwt: None
xlsxwriter: None
lxml: 3.6.4
bs4: None
html5lib: 0.999
sqlalchemy: None
pymysql: None
psycopg2: 2.6.2 (dt dec pq3 ext lo64)
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
767174485,38491,Updated README,aflah02,closed,2020-12-15T04:52:09Z,2020-12-16T17:31:23Z," I have updated the README with the following - 
1. Added Links for Conda, Cython and AQR.
2. Added a brief description for what all the Dependencies do so people who are unfamiliar with them can atleast get a basic idea about them
3. Corrected PyPI spelling in one instance"
768063628,38508,"BUG: Series(dt64, dtype=""Sparse[object]"")",jbrockmendel,closed,2020-12-15T19:17:58Z,2020-12-16T17:40:17Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

As a side effect of fixing astype_nansafe, we can rip some code out of Block._astype."
768011021,38506,WIP: CI: Supress urllib3 tests in pytest,mroeschke,closed,2020-12-15T18:44:56Z,2020-12-16T17:50:10Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref https://github.com/pandas-dev/pandas/pull/38480, there are some remaining urllib3 logs coming from somewhere. Testing if this gets rid of those."
766973756,38476,BENCH: Fix CategoricalIndexing benchmark,mroeschke,closed,2020-12-14T22:13:59Z,2020-12-16T21:03:48Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This benchmark behavior changed after https://github.com/pandas-dev/pandas/pull/38372

Additionally, exploring if its feasible to always run the benchmarks to catch these changes earlier."
767034852,38481,REF: share .astype code for astype_nansafe + TDA.astype,jbrockmendel,closed,2020-12-15T00:00:31Z,2020-12-16T21:06:43Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This changes TDI.astype and TDA.astype in the cast with no-NAs to return float64 instead of int64, matching Series behavior."
768431842,38518,REF: use astype_nansafe in Index.astype,jbrockmendel,closed,2020-12-16T04:09:54Z,2020-12-16T23:09:11Z,
769315359,38529,CLN: remove CategoricalIndex._engine,jbrockmendel,closed,2020-12-16T22:25:52Z,2020-12-17T01:27:13Z,
744269418,37902,TYP: Add cast to ABC classes.,rhshadrach,closed,2020-11-16T23:00:05Z,2020-11-24T16:55:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Implements the idea of @simonjayhawkins from https://github.com/pandas-dev/pandas/issues/27353#issuecomment-526391462 to use cast on ABC classes. Currently only implements ABCDataFrame, ABCNDFrame, and ABCSeries. Previously, mypy would resolve e.g. ABCSeries as any, now it gets resolved as Series.

One issue remains: I don't understand why the code below generates this complaint.

> pandas/core/common.py:472: error: Incompatible return value type (got ""Union[Any, List[Any], Series]"", expected ""Union[List[Any], ExtensionArray]"")  [return-value]

```
def convert_to_list_like(
    values: Union[Scalar, Iterable, AnyArrayLike]
) -> Union[List, AnyArrayLike]:
    if isinstance(values, (list, np.ndarray, ABCIndex, ABCSeries, ABCExtensionArray)):
        return values
    ...
```

One can add Series to the return Union to resolve this, but it doesn't seem like that should be necessary.

I'm also getting:

> pandas/core/base.py:923: note: Internal mypy error checking function redefinition

on the lines

```
def map_f(values, f):
    return lib.map_infer_mask(values, f, isna(values).view(np.uint8))
```

where `map_f` is defined in various ways in if-else blocks. I'm not sure if this is a mypy bug."
749662069,38036,speed up apply calculation ,Jacques2101,closed,2020-11-24T12:23:41Z,2020-11-24T17:15:14Z,"Hi, I have a huge database with multi index ((8832, 14384)) from which I calculate some statistics with code like that : 

```python
(df
 .apply([
     lambda x: ...,  
     lambda x: ...,
     lambda x: ...,
     lambda x: ...,
     lambda x: ...,

     lambda x: ...,
     lambda x: ...,
     lambda x: ...,
     lambda x: ...,
     lambda x: ...,
     lambda x: ...,

     lambda x: ...,
     lambda x: ...,
     lambda x: ...,
     lambda x: ...,

     lambda x: ...,
     lambda x: ...,

     lambda x: ...,
     lambda x: ...,
 ], axis=1)
)
```
My problem is that it took **more than 5 min** to do the calculation (!!!!). 
Is there a way to do it better ? Multiprocessing but don't know how to structure it ? 

Thx "
749104785,38024,"Revert ""REF: back IntervalArray by a single ndarray (#37047)""",jorisvandenbossche,closed,2020-11-23T20:32:49Z,2020-11-24T19:22:32Z,"This reverts commit 9cb372376fc78aa66e2559de919592007b74cfaa.

See discussion in https://github.com/pandas-dev/pandas/pull/37047/

@jbrockmendel can you give this a check? (there were already several conflicts)"
406935188,25165,Partial date string indexing doesn't work on MultiIndex,rben01,closed,2019-02-05T19:19:14Z,2020-11-24T21:59:57Z,"#### Code Sample, a copy-pastable example if possible

```python
date_idx = pd.date_range('2019', periods=5, freq='MS')
df_single = pd.DataFrame(list(range(5)), index=date_idx)
df_multi = pd.DataFrame(list(range(15)), 
                        index=pd.MultiIndex.from_product([date_idx, list(range(3))], 
                                                         names=['x', 'y']))

# All of the following get rows starting in February
df_single['2019-2':]
df_single.loc['2019-2':]
df_single.loc(axis=0)['2019-2':]

# But none of the following do -- not even the last one
df_multi['2019-2':]  # TypeError: '<' not supported between instances of 'int' and 'slice'
df_multi.loc['2019-2':]  # Same error
df_multi.loc(axis=0)['2019-2':]  # Same error
df_multi.loc(axis=0)['2019-2':, :]  # AttributeError: 'int' object has no attribute 'stop'
```
#### Problem description
For single (not multi-) datetime indexes, you can use a string coercible to a `pd.Timestamp` to slice the dataframe. For MultiIndexes with one level a DatetimeIndex, this indexing is not possible, no matter how precisely you specify the slice axes and levels (and even when the dataframe is sorted by the DatetimeIndex level, and the DatetimeIndex level is the outermost level).

#### Expected Output
`df_multi.loc(axis=0)['2019-2':, :]` should certainly slice the rows correctly, as 

1. The slicing is totally unambiguous (nothing needs to be inferred by pandas -- the axis and all slice levels are provided)
2. The dataframe is sorted by the DatetimeIndex level
3. The DatetimeIndex level is outermost

Cases more complicated than this might have some caveats, but the example provided above should definitely work.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.1.final.0
python-bits: 64
OS: Windows
OS-release: 2012ServerR2
machine: AMD64
processor: Intel64 Family 6 Model 45 Stepping 2, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.23.4
pytest: 4.0.2
pip: 18.1
setuptools: 40.6.3
Cython: 0.29.2
numpy: 1.15.4
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 7.2.0
sphinx: 1.8.2
patsy: 0.5.1
dateutil: 2.7.5
pytz: 2018.7
blosc: None
bottleneck: 1.2.1
tables: 3.4.4
numexpr: 2.6.8
feather: None
matplotlib: 3.0.2
openpyxl: 2.5.12
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.2
lxml: 4.2.5
bs4: 4.6.3
html5lib: 1.0.1
sqlalchemy: 1.2.15
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
390552858,24263,BUG: MultiIndex.get_loc errors if get_loc on level returns a slice,shoyer,closed,2018-12-13T07:35:27Z,2020-11-24T21:59:58Z,"#### Code Sample, a copy-pastable example if possible

With the dev version of pandas:
```
In [1]: import pandas as pd

In [2]: index = pd.date_range('2001-01-01', periods=100)

In [3]: mindex = pd.MultiIndex.from_arrays([index])

In [4]: mindex.get_loc('2001-01')
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-4-1914bb512715> in <module>
----> 1 mindex.get_loc('2001-01')

~/dev/pandas/pandas/core/indexes/multi.py in get_loc(self, key, method)
   2257
   2258         if not isinstance(key, tuple):
-> 2259             loc = self._get_level_indexer(key, level=0)
   2260             return _maybe_to_slice(loc)
   2261

~/dev/pandas/pandas/core/indexes/multi.py in _get_level_indexer(self, key, level, indexer)
   2525                 return locs
   2526
-> 2527             i = labels.searchsorted(code, side='left')
   2528             j = labels.searchsorted(code, side='right')
   2529             if i == j:

~/dev/pandas/pandas/util/_decorators.py in wrapper(*args, **kwargs)
    175                 else:
    176                     kwargs[new_arg_name] = new_arg_value
--> 177             return func(*args, **kwargs)
    178         return wrapper
    179     return _deprecate_kwarg

~/dev/pandas/pandas/core/indexes/frozen.py in searchsorted(self, value, side, sorter)
    181         # xref: https://github.com/numpy/numpy/issues/5370
    182         try:
--> 183             value = self.dtype.type(value)
    184         except ValueError:
    185             pass

TypeError: int() argument must be a string, a bytes-like object or a number, not 'slice'
```

#### Problem description

This appears to have been introduced by #22230 by @toobaz, which deleted a check for `isinstance(loc, slice)`. I'm not quite sure why, though it looks like this line didn't have test coverage.

I could not figure out how to trigger this directly on a pandas.Series. Indexing like `series.loc['2001-01']` appears to go through a different code path not involving `get_loc`.

#### Expected Output

With pandas 0.23.4, this returns `slice(0, 31, None)` (which is the correct result)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: 2f6d682a06573e0aab64ae90eeaba216f224db12
python: 3.7.1.final.0
python-bits: 64
OS: Darwin
OS-release: 18.2.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.23.0.dev0+2160.g2f6d682a0
pytest: 4.0.1
pip: 18.1
setuptools: 40.6.2
Cython: 0.29.1
numpy: 1.15.4
scipy: None
pyarrow: None
xarray: 0.11.0
IPython: 7.1.1
sphinx: None
patsy: None
dateutil: 2.7.5
pytz: 2018.7
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.0.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
738601642,37707,Bug in loc raised Error when non-integer slice was given for MultiIndex,phofl,closed,2020-11-09T01:05:41Z,2020-11-24T22:01:54Z,"- [x] closes #25165
- [x] closes #24263
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

A slice return caused issues everywhere, when this function was used. So I unpacked it right in there."
525978349,29749,Multiindex with single level no longer allows selecting columns,darindillon,closed,2019-11-20T18:10:14Z,2020-11-25T03:18:36Z,"This works fine in pandas 0.22.0 but fails in 0.25.3 It appears a bug may have been introduced.

I have some old code that accidentally created a MultiIndex with a single level instead of a regular index, but everything worked fine with pandas 0.22.0. However, in new versions of pandas, that MultiIndex breaks pandas ability to select columns and gives an extremely misleading error message. In this case, I don't need the MultiIndex, so I can just remove that; but I think this code ought to work. (And it did work in the old 0.22.0 version).  

```
import numpy as np
import pandas as pd

names = ['FirstColumn', 'SecondColumn']
data = np.array([[5,6],[7,8]])
df = pd.DataFrame(data, columns = [names]) #Bug: the brackets around ""[names]"" creates a
#multi-index but that was unintentional. (It's still legal, though). 
#But ""df.head()"" and ""df.describe()"" both look normal so you can't see anything is wrong. 

#All of the following work as expected in 0.22.0 but give misleading errors in 0.25.3.
#These all work with a regular index, but the 0.25.3 MultIndex doesn't like it. 
df['FirstColumn'] #ERROR! 
df.FirstColumn #ERROR! 
df.loc[:,'FirstColumn'] #ERROR! 

#This works, but it shouldn't be necessary to do this. The above syntax is more standard.
df.xs('FirstColumn', axis=1, level=0)
```
Those failing statements give misleading errors about ```only integer scalar arrays can be converted to a scalar index```  but that code looks like it ought to work, and it did work fine in the previous versions of pandas. Can we make the standard methods of selecting columns work with the MultiIndex? "
749539939,38035,Backport PR #37986 on branch 1.1.x: REGR: fix inplace operations for EAs with non-EA arg,simonjayhawkins,closed,2020-11-24T09:36:48Z,2020-11-25T10:53:11Z,Backport PR #37986
611293178,33948,CI: Move Python 3.9 Build to Conda,alimcmaster1,closed,2020-05-02T23:12:34Z,2020-11-25T10:59:45Z,"Once Python 3.9 is released (Oct 2020) as a follow up from https://github.com/pandas-dev/pandas/pull/33505 we can move the builds to conda and to run on Azure.

Note: Use numpy wheels when available current installs from source.


"
750696624,38059,Backport PR #37039: CI: move py39 build to conda #33948,simonjayhawkins,closed,2020-11-25T10:33:24Z,2020-11-25T11:51:18Z,Backport PR #37039
750756496,38061,Backport PR #38041 on branch 1.1.x: BUG: pytables in py39 (#38041),simonjayhawkins,closed,2020-11-25T11:58:01Z,2020-11-25T12:48:18Z,Backport PR #38041 on branch 1.1.x
743064654,37847,DOC: add unqiue value counts example to groupyby guide,Abo7atm,closed,2020-11-14T19:28:09Z,2020-11-25T13:30:39Z,"Co-authored-by: Abdulelah Almesfer <28743265+abdulelahsm@users.noreply.github.com>
Co-authoerd-by: Abdulellah Alnumay <33042538+Abo7atm@users.noreply.github.com>

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Adding an example to the groupby user guide. The example showcases the usage of ```nunique``` aggregate function. This is from the PyData Global sprint, from [this issue](https://github.com/MarcoGorelli/PyDataGlobal2020-sprint/issues/5). The example is taken from [StackOverflow](https://stackoverflow.com/questions/15411158/pandas-countdistinct-equivalent)."
616286555,34127,CLN: Move _convert_to_list_like to common,dsaxton,closed,2020-05-12T01:13:24Z,2020-11-25T14:54:37Z,
749902703,38041,BUG: pytables in py39,jbrockmendel,closed,2020-11-24T17:26:35Z,2020-11-25T15:28:59Z,"There are failures I'm getting locally since homebrew bumped up to py39.  I'm not seeing them on the CI, but they _did_ appear around the same time as these [different](https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=48423&view=logs&j=2d7fb38a-2053-50f3-a67c-09f6e91d3121&t=449937cc-3d50-56b5-5662-e489f41f1268) unexplained pytables failures started showing up."
749193070,38026,Bug in loc raising KeyError when MultiIndex columns has only one level,phofl,closed,2020-11-23T23:07:42Z,2020-11-25T18:50:52Z,"- [x] closes #29749
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

@jbrockmendel I think we should check for MultiIndex instead of level number here"
631716863,34603,BUG: dangerous inconsistency when setting values on slice of multi-index,sbitzer,closed,2020-06-05T16:06:11Z,2020-11-25T20:51:28Z,"- [ x ] I have checked that this issue has not already been reported.

- [ x ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame(
    [[1, 2], [3, 4], [5, 6], [7, 8]], 
    index=pd.MultiIndex.from_product([['a', 'b'], [0, 1]]))

In [137]: df
Out[137]: 
     0  1
a 0  1  2
  1  3  4
b 0  5  6
  1  7  8

df2 = pd.DataFrame([[9, 10], [11, 12]], index=[1, 0])

In [139]: df2
Out[139]: 
    0   1
1   9  10
0  11  12

df.loc[('a', df2.index), :] = df2.values

In [141]: df
Out[141]: 
      0   1
a 0   9  10
  1  11  12
b 0   5   6
  1   7   8
```

#### Problem description
When assigning to `df` in the last step, I use the index of `df2` to correctly align the rows of the two DataFrames, i.e., to select the order of elements in `df` to assign to. It turns out that this order is ignored in MultiIndex indexing. I believe this is dangerous, because `df2.values` on the right hand side is the only way to pass the values in `df2` to `df` as long as `df2` hasn't got the same levels as `df` (cf. #10440). Unsuspecting users who believe that MultiIndex indexing will work as indexing on a normal Index (I haven't found a warning about this in the docs) will most likely miss that their values have been sorted unexpectedly.

#### Expected Output
```python
In [141]: df
Out[141]: 
      0   1
a 0  11  12
  1   9  10
b 0   5   6
  1   7   8
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 7
machine          : AMD64
processor        : Intel64 Family 6 Model 42 Stepping 7, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : de_DE.UTF-8
LOCALE           : None.None

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.1.3.post20200330
Cython           : None
pytest           : 5.3.4
hypothesis       : None
sphinx           : 3.0.3
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.13.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.3.4
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
555272143,31330,Rows order when using slice(None) on MultiIndex Dataframe.loc,nrebena,closed,2020-01-26T19:03:28Z,2020-11-25T20:51:29Z,"#### Code Sample

```python
df = pd.DataFrame(
      np.arange(12).reshape((4, 3)),
      index=[[""a"", ""a"", ""b"", ""b""], [1, 2, 1, 2]],
      columns=[[""Ohio"", ""Ohio"", ""Colorado""], [""Green"", ""Red"", ""Green""]],
      )

df.loc[(slice(None), [2,1]), :]                                                                                                
# actual output, same as df, order of slice(None) level take absolut precedence
     Ohio     Colorado
    Green Red    Green
a 1     0   1        2
  2     3   4        5
b 1     6   7        8
  2     9  10       11
```
#### Problem description
When working on issue #22797,  I came across this situation about ordering the result of loc. Should the level not explicitely requested be used when ordering the result, and how.
This may be mostly cosmetic, are one should not rely on how the resulted row are ordered, and may be a non issue. If so, feel free to close it.

#### Expected Output
```
# Second level is prioritary on first level
     Ohio     Colorado
    Green Red    Green
a 2     3   4        5
b 2     9  10       11
a 1     0   1        2
b 1     6   7        8

# ---- or ----

# Keep order of first level, then second level
     Ohio     Colorado
    Green Red    Green
a 2     3   4        5
  1     0   1        2
b 2     9  10       11
  1     6   7        8
```"
748135210,37992,Bug in DataFrame.loc returning elements in wrong order when indexer is differently ordered than object,phofl,closed,2020-11-22T01:01:47Z,2020-11-25T20:52:30Z,"- [x] closes #34603
- [x] closes #31330
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
381745388,23743,Implement DataFrame.__array_ufunc__,jorisvandenbossche,closed,2018-11-16T20:17:05Z,2020-11-25T20:52:40Z,"Applying a ufunc on a `DataFrame` with sparse columns does not retain its sparse dtype:

```
In [105]: df = pd.SparseDataFrame(np.array([[0, 1, 0], [1, 0, 1]]),
                                  columns=['a', 'b', 'c'], default_fill_value=0)
In [106]: df2 = pd.DataFrame(df)

In [107]: np.exp(df)['a']
Out[107]: 
0    1.000000
1    2.718282
Name: a, dtype: Sparse[float64, 0]
BlockIndex
Block locations: array([0], dtype=int32)
Block lengths: array([2], dtype=int32)

In [108]: np.exp(df2)['a']
Out[108]: 
0    1.000000
1    2.718282
Name: a, dtype: float64
```

Although `SparseDataFrame` returns the correct thing here, I am not sure it actually *works* as desired, as I am not sure it prevents materializing the full data (which in principle should be possible to not do)

--- 

*edit from Tom*

Implementing `DataFrame.__array_ufunc__` is probably the best way to do this.

The semantics will be similar to
[Series.__array_ufunc__](https://dev.pandas.io/development/extending.html#numpy-universal-functions),
but applied blockwise.

1. Series and DataFrame objs in `inputs` will first be aligned.
2. All arrays will be unboxed from blocks
3. The ufunc will be applied to each array. If the array defines array_ufunc, it'll be called.
4. The results will be re-boxed in a DataFrame with the original labels.

There are some additional complicates with dimensionality, shapes, broadcasting... But the basic idea of using `__array_ufunc__` blockwise so that the underlying array's `__array_ufunc__` is called makes sense."
713636346,36805,DOC: unclear kwarg null_counts,ivanovmg,closed,2020-10-02T13:48:42Z,2020-11-25T20:59:23Z,"#### Location of the documentation

https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.info.html?highlight=info#pandas.DataFrame.info

#### Documentation problem

Keyword ``null_counts`` from its name may be considered as a need to count null items.
However, the documentation explains that is quite the opposite - count **non-null** items.
I suggest to rename it with ``show_counts`` and maintain the old behavior.

#### Suggested fix for documentation

Replace ``null_counts`` with ``show_counts``.
It will make it more clear what this kwarg means without even the need to read docs.

#### API

It the suggested rename is implemented, then need to start deprecation of kwarg ``null_counts``."
750499377,38056,CI: Renable window tests on Windows,mroeschke,closed,2020-11-25T06:05:21Z,2020-11-25T21:10:43Z,"- [x] closes #37535
- [ ] tests added / passed

Checking if the recent cleanup of the `tests/window` directory had any impact.
"
750250082,38050,TST/REF: collect Index formatting tests,jbrockmendel,closed,2020-11-25T01:36:22Z,2020-11-25T21:37:08Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
546428906,30790,PERF: performance regression in 1.0 compared to 0.25,jorisvandenbossche,closed,2020-01-07T18:06:36Z,2020-11-25T21:43:34Z,"I ran a full benchmark on a separate machine locally, comparing current master against 0.25.3.

Some identified cases:

- [x] Indexing slowdown due to `extract_array`, reproducer below at https://github.com/pandas-dev/pandas/issues/30790#issuecomment-572928516
- [ ] `Index.__new__`, reproducer below at https://github.com/pandas-dev/pandas/issues/30790#issuecomment-571959377
- [ ] IntervalIndex (or all ExtensionIndex?) attribute access: https://github.com/pandas-dev/pandas/issues/30742
- [x] `asof` due to additional copy / take: https://github.com/pandas-dev/pandas/pull/30615/#issuecomment-571531394

Full results:

<details>

```
       before           after         ratio
     [62a87bf4]       [526b2f36]
     <v0.25.3^0>       <benchmarks-run>
+      31.4±0.4ms          127±1ms     4.03  eval.Eval.time_chained_cmp('python', 'all')
+      41.7±0.4ms        129±0.9ms     3.08  eval.Eval.time_chained_cmp('python', 1)
+     12.5±0.04μs       37.1±0.2μs     2.98  indexing.NonNumericSeriesIndexing.time_getitem_scalar('string', 'non_monotonic')
+     14.8±0.07μs       38.3±0.6μs     2.59  indexing.NonNumericSeriesIndexing.time_getitem_scalar('string', 'unique_monotonic_inc')
+        87.8±8μs         220±20μs     2.50  indexing.NumericSeriesIndexing.time_getitem_scalar(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
+      58.4±0.2μs        142±0.3μs     2.43  ctors.SeriesDtypesConstructors.time_index_from_array_string
+         208±2μs          502±3μs     2.41  groupby.GroupByMethods.time_dtype_as_group('object', 'all', 'transformation')
+         208±1μs          497±2μs     2.39  groupby.GroupByMethods.time_dtype_as_group('object', 'all', 'direct')
+       210±0.7μs          499±7μs     2.38  groupby.GroupByMethods.time_dtype_as_group('object', 'any', 'transformation')
+         209±2μs          496±3μs     2.37  groupby.GroupByMethods.time_dtype_as_group('object', 'any', 'direct')
+         210±2μs          497±2μs     2.37  groupby.GroupByMethods.time_dtype_as_field('int', 'all', 'transformation')
+         213±2μs          504±6μs     2.37  groupby.GroupByMethods.time_dtype_as_field('float', 'all', 'transformation')
+      20.8±0.3μs       49.2±0.6μs     2.36  indexing.NonNumericSeriesIndexing.time_getitem_scalar('string', 'nonunique_monotonic_inc')
+         210±1μs          497±4μs     2.36  groupby.GroupByMethods.time_dtype_as_field('int', 'all', 'direct')
+         213±2μs          503±2μs     2.36  groupby.GroupByMethods.time_dtype_as_group('int', 'all', 'direct')
+         214±3μs          504±2μs     2.36  groupby.GroupByMethods.time_dtype_as_group('int', 'all', 'transformation')
+         213±2μs          500±3μs     2.35  groupby.GroupByMethods.time_dtype_as_field('float', 'all', 'direct')
+         218±2μs          513±2μs     2.35  groupby.GroupByMethods.time_dtype_as_group('float', 'all', 'direct')
+         215±1μs          505±2μs     2.35  groupby.GroupByMethods.time_dtype_as_group('int', 'any', 'direct')
+       213±0.9μs          499±3μs     2.34  groupby.GroupByMethods.time_dtype_as_field('int', 'any', 'transformation')
+         219±2μs          514±2μs     2.34  groupby.GroupByMethods.time_dtype_as_group('float', 'all', 'transformation')
+       215±0.9μs          504±3μs     2.34  groupby.GroupByMethods.time_dtype_as_group('int', 'any', 'transformation')
+         214±1μs          501±3μs     2.34  groupby.GroupByMethods.time_dtype_as_field('float', 'any', 'transformation')
+         219±1μs          512±2μs     2.33  groupby.GroupByMethods.time_dtype_as_group('float', 'any', 'transformation')
+         219±2μs          511±1μs     2.33  groupby.GroupByMethods.time_dtype_as_group('datetime', 'all', 'transformation')
+         213±2μs          497±3μs     2.33  groupby.GroupByMethods.time_dtype_as_field('int', 'any', 'direct')
+         220±2μs          514±2μs     2.33  groupby.GroupByMethods.time_dtype_as_group('datetime', 'any', 'direct')
+         220±2μs          513±3μs     2.33  groupby.GroupByMethods.time_dtype_as_group('float', 'any', 'direct')
+         221±2μs          512±4μs     2.32  groupby.GroupByMethods.time_dtype_as_group('datetime', 'any', 'transformation')
+         216±2μs          500±2μs     2.32  groupby.GroupByMethods.time_dtype_as_field('float', 'any', 'direct')
+         220±2μs          509±1μs     2.32  groupby.GroupByMethods.time_dtype_as_group('datetime', 'all', 'direct')
+         217±1μs          498±2μs     2.30  groupby.GroupByMethods.time_dtype_as_field('datetime', 'all', 'direct')
+         218±2μs          498±2μs     2.29  groupby.GroupByMethods.time_dtype_as_field('datetime', 'all', 'transformation')
+       219±0.5μs          497±1μs     2.27  groupby.GroupByMethods.time_dtype_as_field('datetime', 'any', 'direct')
+       218±0.6μs        496±0.9μs     2.27  groupby.GroupByMethods.time_dtype_as_field('datetime', 'any', 'transformation')
+         583±3ms          1.32±0s     2.27  groupby.Apply.time_copy_overhead_single_col
+       223±0.7μs        495±0.8μs     2.22  groupby.GroupByMethods.time_dtype_as_field('float', 'shift', 'direct')
+      1.47±0.01s       3.26±0.01s     2.21  groupby.Apply.time_copy_function_multi_col
+         223±1μs        494±0.6μs     2.21  groupby.GroupByMethods.time_dtype_as_field('float', 'shift', 'transformation')
+        28.2±2ms       62.2±0.4ms     2.21  frame_methods.Apply.time_apply_ref_by_name
+      7.63±0.2ms       16.8±0.1ms     2.20  timeseries.AsOf.time_asof('DataFrame')
+       228±0.9μs          494±1μs     2.17  groupby.GroupByMethods.time_dtype_as_group('object', 'shift', 'direct')
+       229±0.4μs          493±1μs     2.15  groupby.GroupByMethods.time_dtype_as_group('object', 'shift', 'transformation')
+         237±1μs        509±0.2μs     2.14  groupby.GroupByMethods.time_dtype_as_group('float', 'shift', 'direct')
+       238±0.4μs        509±0.6μs     2.14  groupby.GroupByMethods.time_dtype_as_group('float', 'shift', 'transformation')
+       231±0.9μs        493±0.8μs     2.14  groupby.GroupByMethods.time_dtype_as_field('datetime', 'shift', 'transformation')
+         234±5μs          500±1μs     2.14  groupby.GroupByMethods.time_dtype_as_group('datetime', 'shift', 'direct')
+       231±0.9μs        491±0.5μs     2.13  groupby.GroupByMethods.time_dtype_as_field('datetime', 'shift', 'direct')
+         236±3μs          499±2μs     2.12  groupby.GroupByMethods.time_dtype_as_group('datetime', 'shift', 'transformation')
+       250±0.4μs          511±1μs     2.05  groupby.GroupByMethods.time_dtype_as_field('int', 'shift', 'direct')
+       253±0.5μs          517±1μs     2.04  groupby.GroupByMethods.time_dtype_as_group('int', 'shift', 'transformation')
+       250±0.6μs        509±0.5μs     2.03  groupby.GroupByMethods.time_dtype_as_field('int', 'shift', 'transformation')
+         255±2μs          517±2μs     2.03  groupby.GroupByMethods.time_dtype_as_group('int', 'shift', 'direct')
+        2.16±0ms       4.32±0.3ms     2.00  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'sum')
+         285±3μs          566±7μs     1.99  groupby.GroupByMethods.time_dtype_as_field('datetime', 'last', 'transformation')
+     2.17±0.02ms       4.30±0.7ms     1.99  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'sum')
+         285±3μs          564±2μs     1.97  groupby.GroupByMethods.time_dtype_as_field('datetime', 'last', 'direct')
+       296±0.9μs          577±3μs     1.94  groupby.GroupByMethods.time_dtype_as_field('datetime', 'first', 'transformation')
+      25.3±0.2μs       49.2±0.4μs     1.94  indexing.NumericSeriesIndexing.time_getitem_scalar(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
+     2.29±0.01ms       4.46±0.3ms     1.94  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'mean')
+         298±2μs          577±1μs     1.93  groupby.GroupByMethods.time_dtype_as_field('datetime', 'first', 'direct')
+     2.30±0.04ms       4.43±0.6ms     1.93  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'mean')
+         285±1μs        545±0.4μs     1.91  groupby.GroupByMethods.time_dtype_as_group('object', 'bfill', 'direct')
+         285±1μs          544±2μs     1.91  groupby.GroupByMethods.time_dtype_as_group('object', 'ffill', 'direct')
+         285±1μs          544±1μs     1.91  groupby.GroupByMethods.time_dtype_as_group('object', 'bfill', 'transformation')
+       286±0.5μs        545±0.8μs     1.91  groupby.GroupByMethods.time_dtype_as_group('object', 'ffill', 'transformation')
+         290±1μs          552±2μs     1.90  groupby.GroupByMethods.time_dtype_as_group('datetime', 'ffill', 'transformation')
+         289±2μs          551±1μs     1.90  groupby.GroupByMethods.time_dtype_as_group('datetime', 'bfill', 'transformation')
+         289±3μs        548±0.8μs     1.90  groupby.GroupByMethods.time_dtype_as_group('datetime', 'bfill', 'direct')
+       290±0.6μs        549±0.6μs     1.90  groupby.GroupByMethods.time_dtype_as_group('datetime', 'ffill', 'direct')
+      2.96±0.2ms       5.50±0.4ms     1.86  rolling.ExpandingMethods.time_expanding('Series', 'int', 'sum')
+         312±3μs          572±1μs     1.83  groupby.GroupByMethods.time_dtype_as_field('datetime', 'max', 'transformation')
+         311±2μs          569±3μs     1.83  groupby.GroupByMethods.time_dtype_as_field('datetime', 'max', 'direct')
+         327±3μs         599±10μs     1.83  groupby.GroupByMethods.time_dtype_as_field('float', 'last', 'direct')
+         330±2μs          596±2μs     1.81  groupby.GroupByMethods.time_dtype_as_field('float', 'last', 'transformation')
+         327±1μs        583±0.9μs     1.78  groupby.GroupByMethods.time_dtype_as_field('datetime', 'min', 'direct')
+         326±1μs          582±2μs     1.78  groupby.GroupByMethods.time_dtype_as_field('datetime', 'min', 'transformation')
+      3.24±0.2ms       5.72±0.4ms     1.76  rolling.ExpandingMethods.time_expanding('Series', 'int', 'mean')
+         345±3μs          607±3μs     1.76  groupby.GroupByMethods.time_dtype_as_field('float', 'first', 'transformation')
+         347±2μs          610±3μs     1.76  groupby.GroupByMethods.time_dtype_as_field('float', 'first', 'direct')
+     3.32±0.04ms       5.77±0.4ms     1.74  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'kurt')
+     2.19±0.01ms      3.80±0.07ms     1.73  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 1, 'nearest')
+     1.98±0.02ms       3.44±0.4ms     1.73  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'sum')
+     2.19±0.01ms      3.79±0.02ms     1.73  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 1, 'lower')
+     2.20±0.01ms      3.79±0.02ms     1.73  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 1, 'linear')
+     2.19±0.02ms       3.79±0.1ms     1.73  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0, 'higher')
+     2.20±0.01ms         3.79±0ms     1.73  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0, 'lower')
+     2.20±0.01ms      3.79±0.02ms     1.72  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 1, 'higher')
+     2.19±0.01ms      3.77±0.01ms     1.72  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0, 'midpoint')
+     3.37±0.07ms       5.79±0.5ms     1.72  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'kurt')
+     2.20±0.02ms      3.78±0.08ms     1.72  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0, 'linear')
+     2.20±0.01ms      3.78±0.01ms     1.72  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 1, 'midpoint')
+     2.20±0.01ms      3.77±0.01ms     1.72  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0, 'nearest')
+        2.18±0ms      3.74±0.07ms     1.71  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 1, 'nearest')
+         355±2μs          608±4μs     1.71  groupby.GroupByMethods.time_dtype_as_field('float', 'sum', 'transformation')
+        2.17±0ms      3.72±0.07ms     1.71  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0, 'midpoint')
+     2.18±0.01ms       3.73±0.1ms     1.71  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 1, 'higher')
+     2.18±0.01ms      3.73±0.09ms     1.71  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 1, 'midpoint')
+         354±2μs          606±1μs     1.71  groupby.GroupByMethods.time_dtype_as_field('float', 'sum', 'direct')
+      3.24±0.2ms       5.54±0.4ms     1.71  rolling.Methods.time_rolling('Series', 1000, 'int', 'sum')
+       390±0.8μs          666±1μs     1.71  groupby.GroupByMethods.time_dtype_as_field('float', 'ffill', 'transformation')
+     2.18±0.01ms       3.72±0.1ms     1.71  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0, 'linear')
+     2.18±0.01ms      3.71±0.08ms     1.71  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 1, 'linear')
+     2.18±0.01ms      3.73±0.08ms     1.71  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 1, 'lower')
+        2.18±0ms      3.72±0.07ms     1.71  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0, 'higher')
+         355±2μs          606±4μs     1.70  groupby.GroupByMethods.time_dtype_as_field('float', 'mean', 'direct')
+     2.18±0.01ms      3.71±0.07ms     1.70  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0, 'nearest')
+       356±0.6μs          606±2μs     1.70  groupby.GroupByMethods.time_dtype_as_field('float', 'mean', 'transformation')
+     2.18±0.01ms      3.71±0.08ms     1.70  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0, 'lower')
+       389±0.8μs          662±1μs     1.70  groupby.GroupByMethods.time_dtype_as_field('float', 'bfill', 'transformation')
+         357±1μs          607±2μs     1.70  groupby.GroupByMethods.time_dtype_as_field('float', 'prod', 'direct')
+       390±0.9μs          662±2μs     1.70  groupby.GroupByMethods.time_dtype_as_field('float', 'ffill', 'direct')
+       390±0.6μs          661±2μs     1.70  groupby.GroupByMethods.time_dtype_as_field('float', 'bfill', 'direct')
+     3.28±0.06ms       5.55±0.5ms     1.69  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'skew')
+      3.36±0.2ms       5.68±0.4ms     1.69  rolling.Methods.time_rolling('Series', 1000, 'int', 'mean')
+       352±0.5μs        595±0.4μs     1.69  groupby.GroupByMethods.time_dtype_as_field('object', 'shift', 'transformation')
+         359±2μs          605±3μs     1.69  groupby.GroupByMethods.time_dtype_as_field('float', 'prod', 'transformation')
+       352±0.9μs          594±2μs     1.69  groupby.GroupByMethods.time_dtype_as_field('object', 'shift', 'direct')
+         368±2μs          619±2μs     1.68  groupby.GroupByMethods.time_dtype_as_field('float', 'var', 'transformation')
+         366±2μs          616±4μs     1.68  groupby.GroupByMethods.time_dtype_as_field('float', 'min', 'transformation')
+         369±2μs        620±0.8μs     1.68  groupby.GroupByMethods.time_dtype_as_field('float', 'var', 'direct')
+         366±1μs          612±1μs     1.67  groupby.GroupByMethods.time_dtype_as_field('float', 'min', 'direct')
+       364±0.7μs          608±2μs     1.67  groupby.GroupByMethods.time_dtype_as_field('float', 'max', 'transformation')
+         364±1μs          607±2μs     1.67  groupby.GroupByMethods.time_dtype_as_field('float', 'max', 'direct')
+       395±0.5μs        659±0.8μs     1.67  groupby.GroupByMethods.time_dtype_as_field('datetime', 'bfill', 'direct')
+       395±0.7μs          659±2μs     1.67  groupby.GroupByMethods.time_dtype_as_field('datetime', 'ffill', 'direct')
+       394±0.6μs          657±2μs     1.67  groupby.GroupByMethods.time_dtype_as_field('datetime', 'bfill', 'transformation')
+       395±0.1μs          655±1μs     1.66  groupby.GroupByMethods.time_dtype_as_field('datetime', 'ffill', 'transformation')
+         392±1μs          649±1μs     1.65  groupby.GroupByMethods.time_dtype_as_group('object', 'last', 'transformation')
+         393±2μs          650±1μs     1.65  groupby.GroupByMethods.time_dtype_as_group('object', 'last', 'direct')
+         392±3μs          646±3μs     1.65  groupby.GroupByMethods.time_dtype_as_field('float', 'median', 'transformation')
+         391±2μs          644±2μs     1.65  groupby.GroupByMethods.time_dtype_as_field('float', 'median', 'direct')
+      3.86±0.1ms      6.34±0.03ms     1.64  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'count')
+       397±0.6μs          652±2μs     1.64  groupby.GroupByMethods.time_dtype_as_group('object', 'first', 'transformation')
+       410±0.5μs          671±2μs     1.64  groupby.GroupByMethods.time_dtype_as_group('int', 'last', 'transformation')
+         410±2μs          670±1μs     1.63  groupby.GroupByMethods.time_dtype_as_group('int', 'last', 'direct')
+         398±1μs          650±1μs     1.63  groupby.GroupByMethods.time_dtype_as_group('object', 'first', 'direct')
+         421±1μs        688±0.8μs     1.63  groupby.GroupByMethods.time_dtype_as_group('float', 'last', 'transformation')
+      2.87±0.2ms       4.68±0.3ms     1.63  rolling.ExpandingMethods.time_expanding('Series', 'float', 'sum')
+         422±2μs          688±1μs     1.63  groupby.GroupByMethods.time_dtype_as_group('float', 'last', 'direct')
+         419±2μs          684±3μs     1.63  groupby.GroupByMethods.time_dtype_as_group('datetime', 'last', 'direct')
+         404±2μs          658±2μs     1.63  groupby.GroupByMethods.time_dtype_as_field('int', 'last', 'direct')
+         418±1μs          682±3μs     1.63  groupby.GroupByMethods.time_dtype_as_group('datetime', 'last', 'transformation')
+         405±2μs          659±2μs     1.63  groupby.GroupByMethods.time_dtype_as_field('int', 'last', 'transformation')
+         421±2μs          684±2μs     1.62  groupby.GroupByMethods.time_dtype_as_group('datetime', 'first', 'direct')
+       422±0.8μs          681±2μs     1.61  groupby.GroupByMethods.time_dtype_as_group('datetime', 'first', 'transformation')
+     7.11±0.09ms       11.4±0.3ms     1.61  timeseries.AsOf.time_asof_nan('DataFrame')
+         429±2μs          686±1μs     1.60  groupby.GroupByMethods.time_dtype_as_group('float', 'first', 'direct')
+         420±1μs        671±0.8μs     1.60  groupby.GroupByMethods.time_dtype_as_field('int', 'first', 'transformation')
+       421±0.9μs          672±1μs     1.60  groupby.GroupByMethods.time_dtype_as_field('int', 'first', 'direct')
+       427±0.7μs          681±1μs     1.60  groupby.GroupByMethods.time_dtype_as_group('int', 'first', 'transformation')
+         427±1μs          681±2μs     1.59  groupby.GroupByMethods.time_dtype_as_group('int', 'first', 'direct')
+         430±3μs        687±0.7μs     1.59  groupby.GroupByMethods.time_dtype_as_group('float', 'first', 'transformation')
+         448±1μs          708±2μs     1.58  groupby.GroupByMethods.time_dtype_as_group('float', 'ffill', 'transformation')
+       449±0.7μs          708±2μs     1.58  groupby.GroupByMethods.time_dtype_as_group('float', 'bfill', 'direct')
+         448±1μs          706±1μs     1.58  groupby.GroupByMethods.time_dtype_as_group('float', 'bfill', 'transformation')
+         449±1μs          706±2μs     1.57  groupby.GroupByMethods.time_dtype_as_group('float', 'ffill', 'direct')
+     2.02±0.02ms       3.17±0.6ms     1.57  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'sum')
+         453±2μs          710±3μs     1.57  groupby.GroupByMethods.time_dtype_as_group('int', 'bfill', 'direct')
+         452±2μs          707±1μs     1.57  groupby.GroupByMethods.time_dtype_as_group('int', 'ffill', 'direct')
+     3.85±0.06ms      6.02±0.02ms     1.56  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'count')
+         450±1μs        703±0.9μs     1.56  groupby.GroupByMethods.time_dtype_as_field('int', 'bfill', 'transformation')
+         449±1μs          700±1μs     1.56  groupby.GroupByMethods.time_dtype_as_field('int', 'ffill', 'transformation')
+     3.91±0.07ms      6.09±0.06ms     1.56  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'count')
+         452±2μs          705±2μs     1.56  groupby.GroupByMethods.time_dtype_as_group('int', 'bfill', 'transformation')
+         453±2μs          706±2μs     1.56  groupby.GroupByMethods.time_dtype_as_group('int', 'ffill', 'transformation')
+         450±1μs          700±1μs     1.56  groupby.GroupByMethods.time_dtype_as_field('int', 'bfill', 'direct')
+      4.12±0.1ms      6.41±0.04ms     1.55  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'count')
+         450±1μs        700±0.9μs     1.55  groupby.GroupByMethods.time_dtype_as_field('int', 'ffill', 'direct')
+     3.16±0.02ms      4.91±0.09ms     1.55  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'min')
+     4.04±0.06ms       6.27±0.1ms     1.55  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'count')
+     3.11±0.01ms      4.81±0.08ms     1.55  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 1, 'higher')
+         447±1μs          691±1μs     1.55  groupby.GroupByMethods.time_dtype_as_group('float', 'max', 'direct')
+     3.11±0.01ms      4.81±0.08ms     1.55  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 1, 'linear')
+       446±0.9μs        689±0.9μs     1.55  groupby.GroupByMethods.time_dtype_as_group('float', 'max', 'transformation')
+         521±1μs          805±2μs     1.54  groupby.GroupByMethods.time_dtype_as_field('datetime', 'quantile', 'transformation')
+         446±2μs          688±2μs     1.54  groupby.GroupByMethods.time_dtype_as_group('datetime', 'min', 'transformation')
+     3.17±0.01ms      4.89±0.08ms     1.54  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'max')
+       448±0.8μs        690±0.4μs     1.54  groupby.GroupByMethods.time_dtype_as_group('float', 'min', 'transformation')
+         448±1μs          691±1μs     1.54  groupby.GroupByMethods.time_dtype_as_group('float', 'min', 'direct')
+     1.83±0.01ms       2.83±0.3ms     1.54  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'sum')
+     3.13±0.02ms      4.82±0.06ms     1.54  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0, 'lower')
+       442±0.5μs        680±0.4μs     1.54  groupby.GroupByMethods.time_dtype_as_group('int', 'max', 'transformation')
+     3.10±0.02ms      4.76±0.08ms     1.54  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'max')
+       444±0.7μs          682±1μs     1.53  groupby.GroupByMethods.time_dtype_as_group('int', 'min', 'transformation')
+      13.7±0.9μs       21.1±0.2μs     1.53  algorithms.MaybeConvertObjects.time_maybe_convert_objects
+       441±0.8μs        676±0.8μs     1.53  groupby.GroupByMethods.time_dtype_as_field('int', 'max', 'direct')
+       442±0.8μs        679±0.7μs     1.53  groupby.GroupByMethods.time_dtype_as_group('int', 'max', 'direct')
+         448±2μs          687±2μs     1.53  groupby.GroupByMethods.time_dtype_as_group('datetime', 'min', 'direct')
+     3.13±0.01ms      4.80±0.07ms     1.53  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0, 'midpoint')
+     4.08±0.06ms       6.26±0.1ms     1.53  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'count')
+         524±2μs          803±2μs     1.53  groupby.GroupByMethods.time_dtype_as_field('datetime', 'quantile', 'direct')
+         479±1μs        735±0.2μs     1.53  groupby.GroupByMethods.time_dtype_as_field('int', 'var', 'transformation')
+     3.13±0.01ms      4.80±0.07ms     1.53  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0, 'linear')
+     3.10±0.01ms      4.76±0.07ms     1.53  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 1, 'lower')
+       447±0.4μs          685±2μs     1.53  groupby.GroupByMethods.time_dtype_as_group('datetime', 'max', 'transformation')
+     3.13±0.01ms      4.79±0.08ms     1.53  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0, 'nearest')
+       443±0.7μs          678±1μs     1.53  groupby.GroupByMethods.time_dtype_as_field('int', 'min', 'transformation')
+       445±0.7μs        681±0.9μs     1.53  groupby.GroupByMethods.time_dtype_as_group('int', 'min', 'direct')
+     3.13±0.01ms      4.79±0.06ms     1.53  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0, 'higher')
+       443±0.5μs          677±1μs     1.53  groupby.GroupByMethods.time_dtype_as_field('int', 'min', 'direct')
+     3.11±0.01ms      4.76±0.09ms     1.53  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 1, 'midpoint')
+         447±2μs          683±1μs     1.53  groupby.GroupByMethods.time_dtype_as_group('datetime', 'max', 'direct')
+     3.14±0.03ms      4.79±0.09ms     1.53  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'min')
+     3.12±0.01ms      4.75±0.08ms     1.52  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 1, 'nearest')
+     3.28±0.02ms      4.99±0.09ms     1.52  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'max')
+     3.29±0.05ms       5.01±0.1ms     1.52  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'min')
+      4.40±0.3ms       6.68±0.4ms     1.52  rolling.ExpandingMethods.time_expanding('Series', 'int', 'kurt')
+      18.8±0.2μs       28.6±0.3μs     1.52  categoricals.CategoricalSlicing.time_getitem_list_like('monotonic_incr')
+         444±1μs          674±2μs     1.52  groupby.GroupByMethods.time_dtype_as_field('int', 'max', 'transformation')
+         481±3μs          731±2μs     1.52  groupby.GroupByMethods.time_dtype_as_field('int', 'var', 'direct')
+         497±3μs          753±2μs     1.52  groupby.GroupByMethods.time_dtype_as_group('float', 'var', 'transformation')
+      19.0±0.4μs       28.8±0.3μs     1.51  categoricals.CategoricalSlicing.time_getitem_list_like('monotonic_decr')
+         497±3μs          753±2μs     1.51  groupby.GroupByMethods.time_dtype_as_group('float', 'var', 'direct')
+     2.17±0.02ms       3.28±0.6ms     1.51  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'mean')
+     3.26±0.01ms      4.90±0.01ms     1.51  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'min')
+     3.24±0.01ms      4.87±0.02ms     1.50  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'max')
+      19.1±0.5μs       28.6±0.2μs     1.50  categoricals.CategoricalSlicing.time_getitem_list_like('non_monotonic')
+     2.02±0.04ms       3.03±0.4ms     1.50  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'sum')
+     3.28±0.01ms      4.91±0.01ms     1.50  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'max')
+       106±0.5μs          158±1μs     1.50  timeseries.SortIndex.time_sort_index(True)
+     3.33±0.05ms      4.96±0.01ms     1.49  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'min')
+     3.16±0.01ms      4.70±0.01ms     1.49  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'max')
+     3.16±0.02ms      4.70±0.02ms     1.49  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 1, 'midpoint')
+     3.17±0.01ms      4.70±0.03ms     1.48  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 1, 'lower')
+     3.17±0.01ms      4.70±0.03ms     1.48  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 1, 'linear')
+     3.16±0.01ms      4.69±0.02ms     1.48  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 1, 'higher')
+      4.55±0.3ms       6.73±0.4ms     1.48  rolling.Methods.time_rolling('Series', 1000, 'int', 'kurt')
+      3.14±0.2ms       4.65±0.3ms     1.48  rolling.Methods.time_rolling('Series', 10, 'float', 'sum')
+     2.18±0.03ms       3.23±0.2ms     1.48  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'mean')
+     3.21±0.01ms      4.74±0.01ms     1.48  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'min')
+     3.17±0.02ms      4.69±0.02ms     1.48  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 1, 'nearest')
+     3.22±0.02ms      4.75±0.02ms     1.47  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 0, 'midpoint')
+      3.14±0.2ms       4.62±0.3ms     1.47  rolling.Methods.time_rolling('Series', 1000, 'float', 'sum')
+     3.22±0.02ms      4.75±0.03ms     1.47  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 0, 'nearest')
+     3.22±0.02ms      4.73±0.02ms     1.47  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 0, 'linear')
+         585±2μs          859±3μs     1.47  multiindex_object.Values.time_datetime_level_values_sliced
+     3.23±0.01ms      4.74±0.02ms     1.47  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 0, 'higher')
+     3.22±0.02ms      4.72±0.01ms     1.46  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 0, 'lower')
+      4.56±0.3ms       6.67±0.4ms     1.46  rolling.ExpandingMethods.time_expanding('Series', 'int', 'skew')
+     2.16±0.04ms       3.16±0.7ms     1.46  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'mean')
+      3.27±0.2ms       4.78±0.3ms     1.46  rolling.Methods.time_rolling('Series', 10, 'float', 'mean')
+         538±3μs          786±3μs     1.46  groupby.GroupByMethods.time_dtype_as_field('float', 'std', 'direct')
+     2.03±0.01ms       2.96±0.4ms     1.46  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'mean')
+         541±2μs          787±4μs     1.46  groupby.GroupByMethods.time_dtype_as_field('float', 'std', 'transformation')
+     3.31±0.05ms       4.82±0.8ms     1.45  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'skew')
+      4.51±0.3ms       6.55±0.4ms     1.45  rolling.Methods.time_rolling('Series', 1000, 'int', 'skew')
+         643±2μs          934±2μs     1.45  groupby.GroupByMethods.time_dtype_as_group('float', 'quantile', 'direct')
+         645±2μs          936±2μs     1.45  groupby.GroupByMethods.time_dtype_as_group('float', 'quantile', 'transformation')
+      3.23±0.2ms       4.69±0.4ms     1.45  rolling.Methods.time_rolling('Series', 10, 'int', 'sum')
+      3.28±0.2ms       4.76±0.3ms     1.45  rolling.Methods.time_rolling('Series', 1000, 'float', 'mean')
+         627±1μs          909±3μs     1.45  groupby.GroupByMethods.time_dtype_as_field('int', 'quantile', 'direct')
+       628±0.7μs          911±2μs     1.45  groupby.GroupByMethods.time_dtype_as_field('int', 'quantile', 'transformation')
+         638±1μs          924±3μs     1.45  groupby.GroupByMethods.time_dtype_as_group('int', 'quantile', 'direct')
+       641±0.9μs          928±2μs     1.45  groupby.GroupByMethods.time_dtype_as_group('datetime', 'quantile', 'direct')
+         641±1μs          926±1μs     1.45  groupby.GroupByMethods.time_dtype_as_group('datetime', 'quantile', 'transformation')
+         638±2μs          921±3μs     1.44  groupby.GroupByMethods.time_dtype_as_group('int', 'quantile', 'transformation')
+         633±1μs          914±5μs     1.44  groupby.GroupByMethods.time_dtype_as_field('float', 'quantile', 'direct')
+         633±2μs          911±1μs     1.44  groupby.GroupByMethods.time_dtype_as_field('float', 'quantile', 'transformation')
+      3.16±0.2ms       4.54±0.4ms     1.44  rolling.ExpandingMethods.time_expanding('Series', 'float', 'mean')
+      3.36±0.2ms       4.83±0.4ms     1.44  rolling.Methods.time_rolling('Series', 10, 'int', 'mean')
+         599±1μs          858±2μs     1.43  groupby.GroupByMethods.time_dtype_as_field('object', 'ffill', 'direct')
+         598±1μs          856±3μs     1.43  groupby.GroupByMethods.time_dtype_as_field('object', 'bfill', 'direct')
+         599±1μs          856±2μs     1.43  groupby.GroupByMethods.time_dtype_as_field('object', 'bfill', 'transformation')
+         601±1μs          859±2μs     1.43  groupby.GroupByMethods.time_dtype_as_field('object', 'ffill', 'transformation')
+      8.43±0.7ms       11.9±0.2ms     1.41  series_methods.NanOps.time_func('std', 1000000, 'float64')
+     3.14±0.02ms       4.40±0.4ms     1.40  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'kurt')
+      7.77±0.2ms      10.8±0.02ms     1.39  timeseries.ResampleSeries.time_resample('period', '5min', 'ohlc')
+         651±3μs          900±2μs     1.38  groupby.GroupByMethods.time_dtype_as_field('int', 'std', 'direct')
+         653±3μs          898±2μs     1.38  groupby.GroupByMethods.time_dtype_as_field('int', 'std', 'transformation')
+     3.22±0.02ms       4.41±0.3ms     1.37  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'kurt')
+     5.65±0.06ms      7.75±0.09ms     1.37  indexing.NonNumericSeriesIndexing.time_getitem_list_like('string', 'nonunique_monotonic_inc')
+         679±2μs          930±3μs     1.37  groupby.GroupByMethods.time_dtype_as_group('float', 'std', 'transformation')
+         681±3μs          930±3μs     1.37  groupby.GroupByMethods.time_dtype_as_group('float', 'std', 'direct')
+     3.19±0.02ms       4.35±0.8ms     1.37  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'kurt')
+        753±10μs      1.03±0.01ms     1.36  groupby.GroupByMethods.time_dtype_as_field('object', 'all', 'direct')
+        752±10μs      1.02±0.01ms     1.36  groupby.GroupByMethods.time_dtype_as_field('object', 'all', 'transformation')
+        756±10μs      1.03±0.01ms     1.36  groupby.GroupByMethods.time_dtype_as_field('object', 'any', 'direct')
+      3.26±0.2ms       4.41±0.1ms     1.35  rolling.Quantile.time_quantile('Series', 10, 'int', 1, 'higher')
+      3.26±0.2ms      4.40±0.09ms     1.35  rolling.Quantile.time_quantile('Series', 10, 'int', 0, 'higher')
+         761±7μs      1.03±0.01ms     1.35  groupby.GroupByMethods.time_dtype_as_field('object', 'any', 'transformation')
+      3.27±0.2ms       4.40±0.1ms     1.35  rolling.Quantile.time_quantile('Series', 10, 'int', 1, 'midpoint')
+      3.26±0.2ms       4.39±0.1ms     1.35  rolling.Quantile.time_quantile('Series', 10, 'int', 1, 'lower')
+      3.26±0.2ms       4.39±0.1ms     1.35  rolling.Quantile.time_quantile('Series', 10, 'int', 1, 'nearest')
+     6.49±0.09ms       8.73±0.2ms     1.35  timeseries.ResampleSeries.time_resample('period', '1D', 'ohlc')
+      3.26±0.2ms       4.38±0.1ms     1.34  rolling.Quantile.time_quantile('Series', 10, 'int', 1, 'linear')
+     3.32±0.04ms       4.46±0.4ms     1.34  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'skew')
+     3.81±0.03ms       5.11±0.1ms     1.34  rolling.ExpandingMethods.time_expanding('Series', 'int', 'std')
+      3.26±0.2ms       4.38±0.1ms     1.34  rolling.Quantile.time_quantile('Series', 10, 'int', 0, 'linear')
+     3.00±0.01ms       4.03±0.2ms     1.34  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'kurt')
+      3.29±0.2ms       4.40±0.1ms     1.34  rolling.Quantile.time_quantile('Series', 10, 'int', 0, 'nearest')
+      3.24±0.2ms      4.33±0.05ms     1.34  rolling.Quantile.time_quantile('Series', 1000, 'int', 1, 'linear')
+     16.5±0.06μs       22.0±0.3μs     1.34  categoricals.CategoricalSlicing.time_getitem_slice('monotonic_decr')
+      3.29±0.2ms       4.40±0.1ms     1.34  rolling.Quantile.time_quantile('Series', 10, 'int', 0, 'midpoint')
+      3.25±0.2ms      4.33±0.05ms     1.34  rolling.Quantile.time_quantile('Series', 1000, 'int', 0, 'lower')
+      4.45±0.3ms       5.94±0.3ms     1.33  rolling.Quantile.time_quantile('Series', 1000, 'float', 1, 'nearest')
+      4.45±0.3ms       5.93±0.3ms     1.33  rolling.Quantile.time_quantile('Series', 1000, 'float', 1, 'linear')
+      4.45±0.3ms       5.91±0.3ms     1.33  rolling.Quantile.time_quantile('Series', 1000, 'float', 1, 'higher')
+     1.04±0.01ms      1.39±0.01ms     1.33  ctors.SeriesConstructors.time_series_constructor(<class 'list'>, False, 'float')
+      3.29±0.2ms      4.37±0.06ms     1.33  rolling.Quantile.time_quantile('Series', 1000, 'int', 0, 'nearest')
+      3.25±0.2ms      4.32±0.05ms     1.33  rolling.Quantile.time_quantile('Series', 1000, 'int', 1, 'midpoint')
+     1.09±0.01ms      1.45±0.01ms     1.33  ctors.SeriesConstructors.time_series_constructor(<class 'list'>, True, 'float')
+      4.50±0.3ms       5.97±0.3ms     1.33  rolling.Quantile.time_quantile('Series', 1000, 'float', 0, 'linear')
+      3.31±0.2ms       4.39±0.1ms     1.33  rolling.Quantile.time_quantile('Series', 10, 'int', 0, 'lower')
+      4.45±0.3ms       5.91±0.3ms     1.33  rolling.Quantile.time_quantile('Series', 1000, 'float', 1, 'lower')
+      3.25±0.3ms      4.32±0.06ms     1.33  rolling.Quantile.time_quantile('Series', 1000, 'int', 0, 'midpoint')
+      3.25±0.2ms      4.31±0.05ms     1.33  rolling.Quantile.time_quantile('Series', 1000, 'int', 0, 'linear')
+      3.25±0.2ms      4.32±0.05ms     1.33  rolling.Quantile.time_quantile('Series', 1000, 'int', 1, 'higher')
+      4.49±0.3ms       5.97±0.3ms     1.33  rolling.Quantile.time_quantile('Series', 1000, 'float', 0, 'higher')
+      3.26±0.2ms      4.32±0.05ms     1.33  rolling.Quantile.time_quantile('Series', 1000, 'int', 1, 'nearest')
+      25.2±0.4ms       33.4±0.4ms     1.33  io.hdf.HDF.time_read_hdf('fixed')
+      4.45±0.3ms       5.90±0.4ms     1.33  rolling.Quantile.time_quantile('Series', 1000, 'float', 1, 'midpoint')
+      4.51±0.3ms       5.98±0.3ms     1.33  rolling.Quantile.time_quantile('Series', 1000, 'float', 0, 'nearest')
+      4.51±0.3ms       5.98±0.3ms     1.33  rolling.Quantile.time_quantile('Series', 1000, 'float', 0, 'midpoint')
+     5.98±0.01ms       7.92±0.1ms     1.32  indexing.NonNumericSeriesIndexing.time_getitem_list_like('string', 'non_monotonic')
+      3.25±0.2ms      4.31±0.05ms     1.32  rolling.Quantile.time_quantile('Series', 1000, 'int', 1, 'lower')
+     3.15±0.03ms       4.17±0.3ms     1.32  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'skew')
+     3.14±0.02ms       4.15±0.8ms     1.32  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'skew')
+      4.52±0.3ms       5.97±0.3ms     1.32  rolling.Quantile.time_quantile('Series', 1000, 'float', 0, 'lower')
+     5.94±0.06ms       7.85±0.1ms     1.32  indexing.NonNumericSeriesIndexing.time_getitem_list_like('string', 'unique_monotonic_inc')
+      4.56±0.3ms       6.01±0.4ms     1.32  rolling.Methods.time_rolling('Series', 10, 'int', 'kurt')
+      3.28±0.2ms      4.32±0.05ms     1.32  rolling.Quantile.time_quantile('Series', 1000, 'int', 0, 'higher')
+      17.0±0.2μs       22.4±0.2μs     1.32  categoricals.CategoricalSlicing.time_getitem_slice('non_monotonic')
+     16.8±0.09μs       22.0±0.4μs     1.31  categoricals.CategoricalSlicing.time_getitem_slice('monotonic_incr')
+         540±5μs          708±9μs     1.31  period.Indexing.time_intersection
+      4.29±0.3ms       5.62±0.4ms     1.31  rolling.ExpandingMethods.time_expanding('Series', 'float', 'kurt')
+      4.39±0.3ms       5.71±0.1ms     1.30  rolling.Quantile.time_quantile('Series', 10, 'float', 0, 'nearest')
+      4.40±0.3ms       5.71±0.2ms     1.30  rolling.Quantile.time_quantile('Series', 10, 'float', 0, 'lower')
+         976±6μs      1.27±0.01ms     1.30  indexing.NonNumericSeriesIndexing.time_getitem_pos_slice('string', 'nonunique_monotonic_inc')
+      4.37±0.3ms       5.68±0.1ms     1.30  rolling.Quantile.time_quantile('Series', 10, 'float', 1, 'nearest')
+        962±10μs      1.25±0.02ms     1.30  indexing.NonNumericSeriesIndexing.time_getitem_pos_slice('string', 'non_monotonic')
+      4.37±0.3ms       5.68±0.3ms     1.30  rolling.Methods.time_rolling('Series', 10, 'float', 'max')
+      4.38±0.3ms       5.68±0.1ms     1.30  rolling.Quantile.time_quantile('Series', 10, 'float', 1, 'higher')
+      4.40±0.3ms       5.71±0.1ms     1.30  rolling.Quantile.time_quantile('Series', 10, 'float', 0, 'linear')
+      4.39±0.3ms       5.70±0.1ms     1.30  rolling.Quantile.time_quantile('Series', 10, 'float', 0, 'midpoint')
+         684±3μs          887±2μs     1.30  groupby.GroupByMethods.time_dtype_as_field('object', 'first', 'direct')
+      4.38±0.3ms      5.68±0.09ms     1.30  rolling.Quantile.time_quantile('Series', 10, 'float', 1, 'linear')
+      4.38±0.3ms       5.67±0.1ms     1.30  rolling.Quantile.time_quantile('Series', 10, 'float', 1, 'midpoint')
+      4.38±0.3ms       5.67±0.1ms     1.29  rolling.Quantile.time_quantile('Series', 10, 'float', 1, 'lower')
+      4.41±0.3ms       5.70±0.1ms     1.29  rolling.Quantile.time_quantile('Series', 10, 'float', 0, 'higher')
+     6.20±0.07μs       7.99±0.1μs     1.29  categoricals.Indexing.time_get_loc
+        1.24±0ms      1.59±0.01ms     1.29  frame_methods.Quantile.time_frame_quantile(1)
+        1.89±0ms      2.43±0.01ms     1.29  groupby.GroupByMethods.time_dtype_as_field('float', 'pct_change', 'direct')
+         685±5μs          882±2μs     1.29  groupby.GroupByMethods.time_dtype_as_field('object', 'first', 'transformation')
+      37.5±0.3ms       48.3±0.6ms     1.29  frame_ctor.FromDicts.time_nested_dict_index
+         679±3μs          873±3μs     1.29  groupby.GroupByMethods.time_dtype_as_field('object', 'last', 'direct')
+        1.89±0ms      2.43±0.01ms     1.29  groupby.GroupByMethods.time_dtype_as_field('float', 'pct_change', 'transformation')
+         405±2μs          521±3μs     1.29  index_object.IntervalIndexMethod.time_intersection(1000)
+         291±3μs          373±2μs     1.28  join_merge.Concat.time_concat_empty_left(1)
+         679±4μs          872±3μs     1.28  groupby.GroupByMethods.time_dtype_as_field('object', 'last', 'transformation')
+      4.39±0.3ms       5.63±0.2ms     1.28  rolling.Methods.time_rolling('Series', 10, 'float', 'min')
+         422±3μs          539±1μs     1.28  index_object.IntervalIndexMethod.time_intersection_one_duplicate(1000)
+         292±3μs          373±2μs     1.28  join_merge.Concat.time_concat_empty_right(1)
+         980±9μs      1.25±0.01ms     1.28  indexing.NonNumericSeriesIndexing.time_getitem_pos_slice('string', 'unique_monotonic_inc')
+         904±6μs      1.15±0.01ms     1.27  stat_ops.SeriesOps.time_op('std', 'float')
+       463±0.2ns          589±9ns     1.27  indexing.MethodLookup.time_lookup_iloc
+     3.17±0.01ms       4.04±0.2ms     1.27  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'skew')
+         689±2μs          874±1μs     1.27  groupby.GroupByMethods.time_dtype_as_field('int', 'prod', 'direct')
+      4.51±0.3ms       5.72±0.4ms     1.27  rolling.Methods.time_rolling('Series', 10, 'int', 'skew')
+         691±4μs          875±3μs     1.27  groupby.GroupByMethods.time_dtype_as_field('int', 'mean', 'direct')
+      37.8±0.1ms       47.8±0.7ms     1.27  frame_ctor.FromDicts.time_nested_dict_index_columns
+         831±6μs         1.05±0ms     1.27  indexing.NonNumericSeriesIndexing.time_getitem_list_like('datetime', 'non_monotonic')
+      4.44±0.3ms      5.61±0.07ms     1.26  rolling.Methods.time_rolling('Series', 1000, 'float', 'max')
+         689±3μs          871±2μs     1.26  groupby.GroupByMethods.time_dtype_as_field('int', 'prod', 'transformation')
+         703±1μs          888±2μs     1.26  groupby.GroupByMethods.time_dtype_as_group('int', 'mean', 'transformation')
+         691±2μs          873±2μs     1.26  groupby.GroupByMethods.time_dtype_as_field('int', 'mean', 'transformation')
+         739±2μs          933±4μs     1.26  groupby.GroupByMethods.time_dtype_as_group('int', 'median', 'direct')
+         829±5μs         1.05±0ms     1.26  indexing.NonNumericSeriesIndexing.time_getitem_list_like('datetime', 'unique_monotonic_inc')
+      4.52±0.2ms      5.71±0.06ms     1.26  rolling.ExpandingMethods.time_expanding('Series', 'int', 'min')
+         703±2μs          887±2μs     1.26  groupby.GroupByMethods.time_dtype_as_group('int', 'mean', 'direct')
+         633±5μs          798±2μs     1.26  groupby.GroupByMethods.time_dtype_as_group('int', 'var', 'direct')
+      4.49±0.3ms      5.66±0.08ms     1.26  rolling.Methods.time_rolling('Series', 1000, 'float', 'min')
+         737±1μs          928±1μs     1.26  groupby.GroupByMethods.time_dtype_as_group('int', 'median', 'transformation')
+         631±3μs          794±1μs     1.26  groupby.GroupByMethods.time_dtype_as_group('int', 'var', 'transformation')
+     1.98±0.01ms         2.49±0ms     1.26  groupby.GroupByMethods.time_dtype_as_field('int', 'pct_change', 'direct')
+     2.81±0.05ms      3.52±0.01ms     1.26  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'std')
+         731±3μs         918±20μs     1.26  groupby.GroupByMethods.time_dtype_as_field('int', 'median', 'transformation')
+         731±3μs          918±2μs     1.26  groupby.GroupByMethods.time_dtype_as_field('int', 'median', 'direct')
+      4.48±0.3ms       5.62±0.2ms     1.25  rolling.Methods.time_rolling('Series', 10, 'int', 'min')
+      4.52±0.2ms      5.67±0.07ms     1.25  rolling.ExpandingMethods.time_expanding('Series', 'int', 'max')
+        1.98±0ms      2.48±0.01ms     1.25  groupby.GroupByMethods.time_dtype_as_field('int', 'pct_change', 'transformation')
+         104±3μs        130±0.6μs     1.25  indexing.NumericSeriesIndexing.time_getitem_scalar(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'unique_monotonic_inc')
+        2.07±0ms      2.58±0.01ms     1.25  groupby.GroupByMethods.time_dtype_as_group('int', 'pct_change', 'direct')
+      4.47±0.3ms       5.59±0.4ms     1.25  rolling.Methods.time_rolling('Series', 10, 'float', 'kurt')
+      4.46±0.2ms       5.58±0.2ms     1.25  rolling.ExpandingMethods.time_expanding('Series', 'float', 'max')
+         725±1μs          906±1μs     1.25  timeseries.ResetIndex.time_reest_datetimeindex('US/Eastern')
+         111±2ms          138±1ms     1.25  reshape.Cut.time_qcut_timedelta(1000)
+     1.08±0.01ms      1.35±0.01ms     1.25  indexing.NonNumericSeriesIndexing.time_getitem_list_like('period', 'non_monotonic')
+        2.06±0ms      2.57±0.01ms     1.25  groupby.GroupByMethods.time_dtype_as_group('int', 'pct_change', 'transformation')
+      37.6±0.4ms       46.8±0.6ms     1.25  frame_ctor.FromDicts.time_list_of_dict
+        1.18±0ms         1.47±0ms     1.24  groupby.GroupByMethods.time_dtype_as_field('float', 'sem', 'direct')
+      53.3±0.3μs       66.3±0.6μs     1.24  indexing.CategoricalIndexIndexing.time_getitem_list_like('monotonic_decr')
+     2.82±0.04ms      3.51±0.01ms     1.24  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'std')
+         468±3ns          582±3ns     1.24  indexing.MethodLookup.time_lookup_loc
+      53.7±0.5μs       66.7±0.4μs     1.24  indexing.CategoricalIndexIndexing.time_getitem_list_like('non_monotonic')
+      4.46±0.3ms       5.54±0.4ms     1.24  rolling.Methods.time_rolling('Series', 1000, 'float', 'kurt')
+      4.47±0.2ms       5.55±0.2ms     1.24  rolling.Methods.time_rolling('Series', 10, 'int', 'max')
+        1.18±0ms      1.47±0.01ms     1.24  groupby.GroupByMethods.time_dtype_as_field('float', 'sem', 'transformation')
+     2.21±0.01ms      2.75±0.01ms     1.24  groupby.GroupByMethods.time_dtype_as_group('float', 'pct_change', 'direct')
+     2.22±0.01ms         2.75±0ms     1.24  groupby.GroupByMethods.time_dtype_as_group('float', 'pct_change', 'transformation')
+      46.5±0.2ms       57.8±0.4ms     1.24  frame_ctor.FromDicts.time_nested_dict
+      53.6±0.6μs       66.5±0.6μs     1.24  indexing.CategoricalIndexIndexing.time_getitem_list_like('monotonic_incr')
+      4.48±0.3ms       5.55±0.2ms     1.24  rolling.ExpandingMethods.time_expanding('Series', 'float', 'min')
+         739±1μs          916±2μs     1.24  groupby.GroupByMethods.time_dtype_as_group('int', 'sum', 'direct')
+      36.5±0.8μs       45.2±0.2μs     1.24  indexing.CategoricalIndexIndexing.time_getitem_slice('monotonic_decr')
+         774±2μs          958±3μs     1.24  groupby.GroupByMethods.time_dtype_as_group('float', 'median', 'direct')
+         765±1μs          946±3μs     1.24  groupby.GroupByMethods.time_dtype_as_group('float', 'mean', 'transformation')
+      12.6±0.3ms      15.5±0.08ms     1.24  io.hdf.HDFStoreDataFrame.time_query_store_table_wide
+      4.47±0.3ms       5.52±0.4ms     1.24  rolling.ExpandingMethods.time_expanding('Series', 'float', 'skew')
+         727±2μs          897±2μs     1.23  groupby.GroupByMethods.time_dtype_as_field('int', 'sum', 'transformation')
+      75.4±0.8ms       93.0±0.2ms     1.23  reshape.Cut.time_qcut_datetime(1000)
+         741±3μs          913±3μs     1.23  groupby.GroupByMethods.time_dtype_as_group('int', 'prod', 'direct')
+         766±3μs          943±2μs     1.23  groupby.GroupByMethods.time_dtype_as_group('float', 'sum', 'transformation')
+         743±2μs          914±1μs     1.23  groupby.GroupByMethods.time_dtype_as_group('int', 'sum', 'transformation')
+         768±2μs          945±2μs     1.23  groupby.GroupByMethods.time_dtype_as_group('float', 'mean', 'direct')
+         777±3μs        957±0.7μs     1.23  groupby.GroupByMethods.time_dtype_as_group('float', 'median', 'transformation')
+         743±2μs          913±1μs     1.23  groupby.GroupByMethods.time_dtype_as_group('int', 'prod', 'transformation')
+     3.31±0.03μs      4.06±0.03μs     1.23  categoricals.Contains.time_categorical_index_contains
+      4.42±0.3ms       5.43±0.4ms     1.23  rolling.Methods.time_rolling('Series', 1000, 'float', 'skew')
+         730±3μs          896±2μs     1.23  groupby.GroupByMethods.time_dtype_as_field('int', 'sum', 'direct')
+         771±5μs          946±2μs     1.23  groupby.GroupByMethods.time_dtype_as_group('float', 'prod', 'direct')
+     2.98±0.02ms      3.65±0.01ms     1.23  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'std')
+         770±3μs        943±0.7μs     1.23  groupby.GroupByMethods.time_dtype_as_group('float', 'prod', 'transformation')
+         156±1ms        191±0.3ms     1.22  inference.ToNumericDowncast.time_downcast('string-float', 'unsigned')
+        1.29±0ms      1.58±0.01ms     1.22  groupby.GroupByMethods.time_dtype_as_field('int', 'sem', 'direct')
+      4.43±0.3ms       5.42±0.4ms     1.22  rolling.Methods.time_rolling('Series', 10, 'float', 'skew')
+        1.29±0ms      1.57±0.01ms     1.22  groupby.GroupByMethods.time_dtype_as_field('int', 'sem', 'transformation')
+         770±9μs          941±2μs     1.22  groupby.GroupByMethods.time_dtype_as_group('float', 'sum', 'direct')
+      4.51±0.3ms       5.51±0.1ms     1.22  rolling.Methods.time_rolling('Series', 1000, 'int', 'max')
+       156±0.9ms        190±0.8ms     1.22  inference.ToNumericDowncast.time_downcast('string-float', 'integer')
+         147±3μs          180±1μs     1.22  indexing.NumericSeriesIndexing.time_getitem_slice(<class 'pandas.core.indexes.numeric.Int64Index'>, 'unique_monotonic_inc')
+       148±0.9μs        181±0.4μs     1.22  indexing.NumericSeriesIndexing.time_getitem_slice(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'unique_monotonic_inc')
+        1.34±0ms      1.63±0.01ms     1.22  groupby.GroupByMethods.time_dtype_as_group('float', 'sem', 'direct')
+      47.3±0.1ms       57.7±0.7ms     1.22  frame_ctor.FromDicts.time_nested_dict_columns
+      4.57±0.3ms       5.57±0.1ms     1.22  rolling.Methods.time_rolling('Series', 1000, 'int', 'min')
+         156±2ms        190±0.4ms     1.22  inference.ToNumericDowncast.time_downcast('string-float', 'signed')
+       674±0.8μs          821±2μs     1.22  groupby.GroupByMethods.time_dtype_as_field('object', 'nunique', 'direct')
+     1.34±0.01ms      1.63±0.01ms     1.22  groupby.GroupByMethods.time_dtype_as_group('float', 'sem', 'transformation')
+         725±9μs          882±3μs     1.22  timeseries.ResetIndex.time_reest_datetimeindex(None)
+     8.02±0.05μs      9.75±0.04μs     1.22  categoricals.Indexing.time_shallow_copy
+       147±0.8μs        179±0.2μs     1.22  indexing.NumericSeriesIndexing.time_getitem_slice(<class 'pandas.core.indexes.numeric.Int64Index'>, 'nonunique_monotonic_inc')
+       148±0.3μs        180±0.3μs     1.22  indexing.NumericSeriesIndexing.time_getitem_slice(<class 'pandas.core.indexes.numeric.UInt64Index'>, 'nonunique_monotonic_inc')
+     1.17±0.02ms      1.41±0.01ms     1.21  indexing.NonNumericSeriesIndexing.time_getitem_label_slice('string', 'nonunique_monotonic_inc')
+     26.3±0.07ms      31.8±0.08ms     1.21  strings.Contains.time_contains(False)
+     3.00±0.01ms      3.62±0.01ms     1.21  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'std')
+     3.25±0.01ms      3.92±0.02ms     1.21  io.csv.ReadCSVFloatPrecision.time_read_csv(';', '.', 'round_trip')
+     2.99±0.03ms      3.61±0.01ms     1.21  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'std')
+       458±0.4μs          553±3μs     1.21  categoricals.Constructor.time_from_codes_all_int8
+         326±4μs          394±7μs     1.21  reindex.Fillna.time_float_32('backfill')
+     3.23±0.01ms       3.90±0.7ms     1.21  rolling.VariableWindowMethods.time_rolling('DataFrame', '50s', 'int', 'sum')
+     3.21±0.02ms       3.88±0.7ms     1.21  rolling.VariableWindowMethods.time_rolling('DataFrame', '1h', 'int', 'sum')
+      66.8±0.6μs         80.5±1μs     1.20  categoricals.IsMonotonic.time_categorical_series_is_monotonic_increasing
+      11.5±0.2μs       13.8±0.7μs     1.20  indexing.CategoricalIndexIndexing.time_get_loc_scalar('monotonic_incr')
+        65.3±1ms         78.4±1ms     1.20  reshape.Cut.time_cut_timedelta(1000)
+     3.21±0.02ms       3.86±0.7ms     1.20  rolling.VariableWindowMethods.time_rolling('DataFrame', '1d', 'int', 'sum')
+     3.26±0.01ms      3.90±0.01ms     1.20  io.csv.ReadCSVFloatPrecision.time_read_csv(',', '.', 'round_trip')
+      67.0±0.6μs         80.0±1μs     1.20  categoricals.IsMonotonic.time_categorical_series_is_monotonic_decreasing
+     1.17±0.04μs      1.40±0.06μs     1.19  index_cached_properties.IndexCache.time_is_monotonic('RangeIndex')
+         942±2μs         1.13±0ms     1.19  groupby.GroupByMethods.time_dtype_as_field('datetime', 'cummin', 'transformation')
+     1.05±0.01ms      1.26±0.02ms     1.19  indexing.NonNumericSeriesIndexing.time_getitem_label_slice('string', 'non_monotonic')
+         943±2μs         1.12±0ms     1.19  groupby.GroupByMethods.time_dtype_as_field('datetime', 'cummin', 'direct')
+        38.3±2μs       45.6±0.4μs     1.19  indexing.CategoricalIndexIndexing.time_getitem_slice('monotonic_incr')
+     3.13±0.01ms      3.72±0.01ms     1.19  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'std')
+      3.84±0.1ms       4.56±0.4ms     1.19  stat_ops.SeriesMultiIndexOps.time_op(0, 'mean')
+       473±0.8μs          562±2μs     1.19  timeseries.ToDatetimeCacheSmallCount.time_unique_date_strings(True, 50)
+     5.89±0.02ms      7.00±0.01ms     1.19  timeseries.ResampleSeries.time_resample('period', '5min', 'mean')
+         810±4μs          962±3μs     1.19  groupby.GroupByMethods.time_dtype_as_group('int', 'std', 'direct')
+       472±0.9μs          561±2μs     1.19  timeseries.ToDatetimeCacheSmallCount.time_unique_date_strings(False, 50)
+         811±2μs          962±2μs     1.19  groupby.GroupByMethods.time_dtype_as_group('int', 'std', 'transformation')
+     8.41±0.03μs       9.98±0.1μs     1.19  indexing.NonNumericSeriesIndexing.time_getitem_scalar('datetime', 'non_monotonic')
+      8.18±0.1ms      9.69±0.04ms     1.18  groupby.Apply.time_scalar_function_single_col
+      40.2±0.1μs         47.6±2μs     1.18  ctors.SeriesDtypesConstructors.time_index_from_array_floats
+       175±0.5μs          207±1μs     1.18  series_methods.NanOps.time_func('std', 1000, 'float64')
+     3.47±0.01ms       4.10±0.7ms     1.18  rolling.VariableWindowMethods.time_rolling('DataFrame', '1d', 'int', 'mean')
+     3.52±0.05ms       4.16±0.8ms     1.18  rolling.VariableWindowMethods.time_rolling('DataFrame', '50s', 'int', 'mean')
+     34.0±0.07ms       40.1±0.2ms     1.18  strings.Methods.time_len
+      43.3±0.3ms      51.0±0.08ms     1.18  reshape.Cut.time_cut_datetime(1000)
+      40.8±0.2ms         48.1±1ms     1.18  stat_ops.FrameMultiIndexOps.time_op(0, 'kurt')
+     3.50±0.01ms       4.13±0.7ms     1.18  rolling.VariableWindowMethods.time_rolling('DataFrame', '1h', 'int', 'mean')
+     3.63±0.02ms      4.28±0.04ms     1.18  rolling.Methods.time_rolling('Series', 1000, 'int', 'std')
+         303±4μs          357±5μs     1.18  join_merge.JoinNonUnique.time_join_non_unique_equal
+        38.5±1μs       45.2±0.4μs     1.18  indexing.CategoricalIndexIndexing.time_getitem_slice('non_monotonic')
+     5.97±0.02ms      7.02±0.04ms     1.17  indexing.MultiIndexing.time_index_slice
+      8.42±0.2ms       9.87±0.7ms     1.17  rolling.Methods.time_rolling('Series', 1000, 'int', 'count')
+     3.64±0.05ms      4.27±0.01ms     1.17  rolling.Methods.time_rolling('Series', 10, 'int', 'std')
+      8.45±0.2ms       9.89±0.7ms     1.17  rolling.Methods.time_rolling('Series', 10, 'int', 'count')
+      8.51±0.2ms       9.96±0.7ms     1.17  rolling.Methods.time_rolling('Series', 10, 'float', 'count')
+     1.50±0.04μs      1.76±0.09μs     1.17  index_cached_properties.IndexCache.time_is_monotonic_decreasing('Int64Index')
+     9.20±0.02ms      10.8±0.07ms     1.17  rolling.Pairwise.time_pairwise(None, 'corr', False)
+      8.51±0.2ms       9.95±0.7ms     1.17  rolling.Methods.time_rolling('Series', 1000, 'float', 'count')
+        1.28±0ms         1.49±0ms     1.16  series_methods.Map.time_map('dict', 'category')
+         561±7μs          651±3μs     1.16  timeseries.ToDatetimeCacheSmallCount.time_unique_date_strings(False, 500)
+         705±8ms          819±5ms     1.16  stat_ops.Correlation.time_corr_wide_nans('spearman')
+     4.68±0.02ms       5.43±0.7ms     1.16  rolling.VariableWindowMethods.time_rolling('DataFrame', '50s', 'int', 'kurt')
+         802±3μs         931±30μs     1.16  indexing.NonNumericSeriesIndexing.time_getitem_list_like('datetime', 'nonunique_monotonic_inc')
+         190±5ms          221±4ms     1.16  io.json.ToJSONISO.time_iso_format('records')
+         478±3μs          552±3μs     1.16  strings.Encode.time_encode_decode
+     4.33±0.02ms       5.00±0.7ms     1.15  rolling.VariableWindowMethods.time_rolling('DataFrame', '1d', 'int', 'skew')
+       225±0.5ms          260±2ms     1.15  io.json.ToJSONISO.time_iso_format('columns')
+         974±5μs         1.12±0ms     1.15  groupby.GroupByMethods.time_dtype_as_field('float', 'cumprod', 'transformation')
+        1.20±0ms         1.39±0ms     1.15  groupby.GroupByMethods.time_dtype_as_field('datetime', 'rank', 'direct')
+     7.71±0.01μs      8.89±0.02μs     1.15  dtypes.Dtypes.time_pandas_dtype('Int8')
+     6.32±0.01ms      7.28±0.02ms     1.15  rolling.Pairwise.time_pairwise(None, 'cov', False)
+         220±1μs        254±0.4μs     1.15  indexing.NonNumericSeriesIndexing.time_getitem_label_slice('datetime', 'non_monotonic')
+      21.2±0.1ms       24.4±0.1ms     1.15  reshape.Cut.time_qcut_datetime(10)
+       143±0.4μs        164±0.4μs     1.15  series_methods.NanOps.time_func('std', 1000, 'int64')
+         982±3μs         1.13±0ms     1.15  groupby.GroupByMethods.time_dtype_as_field('float', 'cumsum', 'direct')
+        1.06±0ms      1.22±0.01ms     1.15  groupby.GroupByMethods.time_dtype_as_group('datetime', 'cummin', 'transformation')
+      35.8±0.3ms       41.1±0.7ms     1.15  io.hdf.HDFStoreDataFrame.time_read_store_mixed
+         987±5μs         1.13±0ms     1.15  groupby.GroupByMethods.time_dtype_as_field('float', 'cummax', 'direct')
+         194±4μs          223±3μs     1.15  timeseries.SortIndex.time_get_slice(False)
+        1.20±0ms         1.38±0ms     1.15  groupby.GroupByMethods.time_dtype_as_field('datetime', 'rank', 'transformation')
+         981±3μs         1.12±0ms     1.15  groupby.GroupByMethods.time_dtype_as_field('float', 'cumprod', 'direct')
+     8.08±0.03ms      9.26±0.08ms     1.15  reshape.Cut.time_cut_datetime(4)
+      19.8±0.2ms      22.7±0.07ms     1.15  reshape.Cut.time_qcut_datetime(4)
+         990±1μs         1.13±0ms     1.15  groupby.GroupByMethods.time_dtype_as_field('float', 'cummin', 'direct')
+     1.24±0.04μs      1.43±0.04μs     1.15  index_cached_properties.IndexCache.time_is_monotonic('Int64Index')
+         989±2μs         1.13±0ms     1.15  groupby.GroupByMethods.time_dtype_as_field('float', 'cumsum', 'transformation')
+         550±2ms          630±3ms     1.15  groupby.Groups.time_series_groups('int64_large')
+        1.07±0ms         1.23±0ms     1.15  groupby.GroupByMethods.time_dtype_as_group('float', 'cumsum', 'transformation')
+         989±3μs         1.13±0ms     1.15  groupby.GroupByMethods.time_dtype_as_field('float', 'cummax', 'transformation')
+        1.05±0ms      1.21±0.01ms     1.15  groupby.GroupByMethods.time_dtype_as_field('int', 'cummax', 'transformation')
+         990±5μs         1.13±0ms     1.15  groupby.GroupByMethods.time_dtype_as_field('float', 'cummin', 'transformation')
+     4.62±0.02ms       5.30±0.7ms     1.15  rolling.VariableWindowMethods.time_rolling('DataFrame', '1d', 'int', 'kurt')
+         128±3ms          146±3ms     1.14  io.json.ToJSON.time_to_json('records', 'df_int_floats')
+        1.07±0ms      1.23±0.01ms     1.14  groupby.GroupByMethods.time_dtype_as_group('float', 'cummax', 'direct')
+        1.07±0ms         1.23±0ms     1.14  groupby.GroupByMethods.time_dtype_as_group('float', 'cummin', 'direct')
+       196±0.5μs          224±1μs     1.14  indexing.NonNumericSeriesIndexing.time_getitem_pos_slice('datetime', 'nonunique_monotonic_inc')
+        1.07±0ms         1.23±0ms     1.14  groupby.GroupByMethods.time_dtype_as_group('float', 'cummin', 'transformation')
+        1.06±0ms      1.22±0.01ms     1.14  groupby.GroupByMethods.time_dtype_as_group('datetime', 'cummin', 'direct')
+     4.67±0.02ms       5.34±0.7ms     1.14  rolling.VariableWindowMethods.time_rolling('DataFrame', '1h', 'int', 'kurt')
+     2.61±0.01ms      2.98±0.03ms     1.14  groupby.RankWithTies.time_rank_ties('int64', 'max')
+     2.80±0.01ms      3.21±0.01ms     1.14  ctors.SeriesConstructors.time_series_constructor(<class 'list'>, True, 'int')
+     4.34±0.05ms       4.96±0.8ms     1.14  rolling.VariableWindowMethods.time_rolling('DataFrame', '50s', 'int', 'skew')
+       143±0.2μs        163±0.2μs     1.14  series_methods.NanOps.time_func('std', 1000, 'int32')
+     2.58±0.01ms      2.94±0.01ms     1.14  groupby.RankWithTies.time_rank_ties('float32', 'average')
+       144±0.7μs        164±0.3μs     1.14  series_methods.NanOps.time_func('std', 1000, 'int8')
+        1.08±0ms      1.23±0.01ms     1.14  groupby.GroupByMethods.time_dtype_as_group('float', 'cumsum', 'direct')
+     2.55±0.01ms      2.92±0.01ms     1.14  groupby.RankWithTies.time_rank_ties('float64', 'average')
+     2.55±0.01ms      2.92±0.01ms     1.14  groupby.RankWithTies.time_rank_ties('float64', 'max')
+       128±0.7ms        146±0.5ms     1.14  io.json.ReadJSON.time_read_json('split', 'int')
+        1.05±0ms         1.20±0ms     1.14  groupby.GroupByMethods.time_dtype_as_field('int', 'cummax', 'direct')
+      37.6±0.1ms       42.9±0.1ms     1.14  strings.Methods.time_endswith
+     2.57±0.01ms      2.94±0.01ms     1.14  groupby.RankWithTies.time_rank_ties('float32', 'first')
+        1.08±0ms         1.23±0ms     1.14  groupby.GroupByMethods.time_dtype_as_group('float', 'cummax', 'transformation')
+     2.59±0.02ms      2.95±0.01ms     1.14  groupby.RankWithTies.time_rank_ties('float32', 'dense')
+        2.57±0ms      2.93±0.01ms     1.14  groupby.RankWithTies.time_rank_ties('float32', 'max')
+       196±0.5μs        224±0.3μs     1.14  indexing.NonNumericSeriesIndexing.time_getitem_pos_slice('datetime', 'non_monotonic')
+     8.07±0.02ms      9.20±0.06ms     1.14  sparse.Arithmetic.time_intersect(0.1, nan)
+        1.04±0ms         1.19±0ms     1.14  groupby.GroupByMethods.time_dtype_as_field('int', 'cumsum', 'transformation')
+        1.05±0ms         1.19±0ms     1.14  groupby.GroupByMethods.time_dtype_as_field('int', 'cumsum', 'direct')
+     2.61±0.02ms      2.97±0.01ms     1.14  groupby.RankWithTies.time_rank_ties('int64', 'min')
+     2.55±0.01ms      2.91±0.01ms     1.14  groupby.RankWithTies.time_rank_ties('float64', 'min')
+         189±1ms          215±1ms     1.14  io.json.ToJSONLines.time_float_int_lines
+        1.05±0ms         1.20±0ms     1.14  groupby.GroupByMethods.time_dtype_as_field('int', 'cummin', 'direct')
+     2.77±0.01ms      3.15±0.01ms     1.14  ctors.SeriesConstructors.time_series_constructor(<class 'list'>, False, 'int')
+     3.78±0.03ms      4.30±0.04ms     1.14  ctors.SeriesConstructors.time_series_constructor(<function arr_dict at 0x7f6536433620>, False, 'float')
+         238±2μs        271±0.4μs     1.14  indexing.NonNumericSeriesIndexing.time_getitem_label_slice('datetime', 'nonunique_monotonic_inc')
+     9.28±0.03ms      10.6±0.02ms     1.14  rolling.Pairwise.time_pairwise(1000, 'corr', False)
+     2.58±0.01ms      2.93±0.02ms     1.14  groupby.RankWithTies.time_rank_ties('float32', 'min')
+        1.06±0ms         1.20±0ms     1.14  groupby.GroupByMethods.time_dtype_as_group('int', 'cumsum', 'transformation')
+     2.61±0.01ms      2.96±0.01ms     1.14  groupby.RankWithTies.time_rank_ties('int64', 'average')
+        1.17±0ms         1.33±0ms     1.14  series_methods.Map.time_map('dict', 'int')
+        1.46±0ms      1.65±0.01ms     1.14  groupby.GroupByMethods.time_dtype_as_group('int', 'sem', 'transformation')
+        1.06±0ms         1.21±0ms     1.14  groupby.GroupByMethods.time_dtype_as_group('int', 'cummin', 'transformation')
+        1.06±0ms         1.20±0ms     1.14  groupby.GroupByMethods.time_dtype_as_group('int', 'cumsum', 'direct')
+     6.37±0.02ms      7.23±0.01ms     1.14  rolling.Pairwise.time_pairwise(1000, 'cov', False)
+     2.59±0.01ms      2.94±0.01ms     1.14  groupby.RankWithTies.time_rank_ties('datetime64', 'min')
+        1.06±0ms         1.21±0ms     1.14  groupby.GroupByMethods.time_dtype_as_group('int', 'cummax', 'transformation')
+        1.06±0ms         1.21±0ms     1.14  groupby.GroupByMethods.time_dtype_as_group('int', 'cummin', 'direct')
+       133±0.8ms        151±0.7ms     1.14  io.json.ReadJSON.time_read_json('split', 'datetime')
+        1.06±0ms         1.21±0ms     1.13  groupby.GroupByMethods.time_dtype_as_group('int', 'cummax', 'direct')
+     4.00±0.03ms      4.54±0.04ms     1.13  ctors.SeriesConstructors.time_series_constructor(<function arr_dict at 0x7f6536433620>, True, 'float')
+     31.8±0.01ms      36.1±0.07ms     1.13  frame_methods.Equals.time_frame_nonunique_unequal
+     2.59±0.01ms         2.93±0ms     1.13  groupby.RankWithTies.time_rank_ties('datetime64', 'max')
+     9.24±0.02ms      10.5±0.02ms     1.13  rolling.Pairwise.time_pairwise(10, 'corr', False)
+     2.59±0.01ms      2.93±0.01ms     1.13  groupby.RankWithTies.time_rank_ties('datetime64', 'dense')
+       227±0.8μs          257±1μs     1.13  indexing.NonNumericSeriesIndexing.time_getitem_pos_slice('datetime', 'unique_monotonic_inc')
+     2.33±0.01ms      2.64±0.01ms     1.13  io.csv.ReadCSVFloatPrecision.time_read_csv(';', '.', 'high')
+     2.58±0.01ms      2.92±0.01ms     1.13  groupby.RankWithTies.time_rank_ties('float64', 'dense')
+        1.36±0ms         1.54±0ms     1.13  groupby.GroupByMethods.time_dtype_as_field('float', 'rank', 'direct')
+     31.8±0.06ms      36.0±0.03ms     1.13  frame_methods.Equals.time_frame_nonunique_equal
+         226±7μs          256±1μs     1.13  timeseries.SortIndex.time_get_slice(True)
+     2.59±0.01ms      2.93±0.02ms     1.13  groupby.RankWithTies.time_rank_ties('datetime64', 'average')
+        1.06±0ms         1.20±0ms     1.13  groupby.GroupByMethods.time_dtype_as_field('int', 'cummin', 'transformation')
+        1.40±0ms      1.58±0.01ms     1.13  groupby.GroupByMethods.time_dtype_as_group('int', 'rank', 'direct')
+     2.62±0.01ms      2.96±0.01ms     1.13  groupby.RankWithTies.time_rank_ties('int64', 'dense')
+     9.41±0.03ms       10.6±0.1ms     1.13  reshape.Cut.time_cut_datetime(10)
+     2.62±0.01ms      2.96±0.01ms     1.13  groupby.RankWithTies.time_rank_ties('int64', 'first')
+        1.40±0ms         1.58±0ms     1.13  groupby.GroupByMethods.time_dtype_as_group('int', 'rank', 'transformation')
+        1.36±0ms         1.54±0ms     1.13  groupby.GroupByMethods.time_dtype_as_field('float', 'rank', 'transformation')
+      6.93±0.1μs       7.82±0.2μs     1.13  index_cached_properties.IndexCache.time_engine('DatetimeIndex')
+     2.33±0.01ms      2.63±0.01ms     1.13  io.csv.ReadCSVFloatPrecision.time_read_csv(',', '.', 'high')
+        1.41±0ms         1.59±0ms     1.13  groupby.GroupByMethods.time_dtype_as_group('float', 'rank', 'transformation')
+        1.40±0ms      1.58±0.01ms     1.13  groupby.GroupByMethods.time_dtype_as_group('datetime', 'rank', 'transformation')
+        1.40±0ms         1.58±0ms     1.13  groupby.GroupByMethods.time_dtype_as_group('datetime', 'rank', 'direct')
+     2.42±0.02ms      2.73±0.01ms     1.13  io.csv.ReadCSVFloatPrecision.time_read_csv(';', '.', None)
+     1.46±0.01ms      1.64±0.01ms     1.13  groupby.GroupByMethods.time_dtype_as_group('int', 'sem', 'direct')
+         159±4ms          179±4ms     1.13  io.json.ToJSON.time_to_json('index', 'df_int_floats')
+       183±0.5ms          207±2ms     1.13  io.json.ToJSONISO.time_iso_format('values')
+         294±2μs          332±3μs     1.13  inference.NumericInferOps.time_multiply(<class 'numpy.int8'>)
+     7.28±0.04ms      8.19±0.01ms     1.13  io.sas.SAS.time_read_sas('xport')
+     2.42±0.01ms      2.73±0.01ms     1.13  io.csv.ReadCSVFloatPrecision.time_read_csv(',', '.', None)
+        1.39±0ms         1.56±0ms     1.13  groupby.GroupByMethods.time_dtype_as_field('int', 'rank', 'transformation')
+        1.42±0ms         1.59±0ms     1.13  groupby.GroupByMethods.time_dtype_as_group('float', 'rank', 'direct')
+         224±5ms          252±4ms     1.13  io.json.ToJSONISO.time_iso_format('split')
+        692±10μs         779±20μs     1.13  inference.NumericInferOps.time_multiply(<class 'numpy.float32'>)
+     2.13±0.01ms         2.39±0ms     1.12  series_methods.Map.time_map('dict', 'object')
+     3.66±0.01ms      4.11±0.01ms     1.12  io.csv.ReadCSVParseDates.time_multiple_date
+        1.39±0ms         1.56±0ms     1.12  groupby.GroupByMethods.time_dtype_as_field('int', 'rank', 'direct')
+        86.7±1ms         97.3±2ms     1.12  frame_ctor.FromRecords.time_frame_from_records_generator(None)
+         646±5ms          726±2ms     1.12  groupby.Groups.time_series_groups('object_large')
+         126±3ms          141±3ms     1.12  io.json.ToJSON.time_to_json('records', 'df_int_float_str')
+     3.01±0.01ms      3.38±0.02ms     1.12  io.csv.ReadCSVParseDates.time_baseline
+         291±1μs          327±2μs     1.12  inference.NumericInferOps.time_add(<class 'numpy.int8'>)
+     8.08±0.01ms      9.05±0.09ms     1.12  sparse.Arithmetic.time_intersect(0.01, nan)
+      38.0±0.3ms      42.6±0.07ms     1.12  strings.Methods.time_startswith
+     21.2±0.03ms      23.7±0.04ms     1.12  io.csv.ReadCSVConcatDatetimeBadDateValue.time_read_csv('nan')
+     19.3±0.09ms       21.6±0.1ms     1.12  io.csv.ReadCSVConcatDatetimeBadDateValue.time_read_csv('')
+         188±1ms          211±2ms     1.12  io.json.ToJSONLines.time_float_int_str_lines
+         539±3μs          603±1μs     1.12  series_methods.Map.time_map('Series', 'category')
+         288±1μs          323±2μs     1.12  inference.NumericInferOps.time_subtract(<class 'numpy.int8'>)
+       291±0.8μs          325±2μs     1.12  inference.NumericInferOps.time_subtract(<class 'numpy.uint8'>)
+     6.43±0.02ms      7.19±0.02ms     1.12  rolling.Pairwise.time_pairwise(10, 'cov', False)
+       269±0.7μs          301±2μs     1.12  indexing.NonNumericSeriesIndexing.time_getitem_label_slice('datetime', 'unique_monotonic_inc')
+         292±1μs          326±1μs     1.12  inference.NumericInferOps.time_add(<class 'numpy.uint8'>)
+       296±0.7μs          330±1μs     1.11  inference.NumericInferOps.time_multiply(<class 'numpy.uint8'>)
+      8.85±0.3ms       9.86±0.5ms     1.11  timeseries.ResampleSeries.time_resample('datetime', '5min', 'ohlc')
+         178±2ms        198±0.1ms     1.11  frame_ctor.FromDicts.time_nested_dict_int64
+     13.5±0.04μs       15.1±0.4μs     1.11  indexing.NonNumericSeriesIndexing.time_getitem_scalar('datetime', 'nonunique_monotonic_inc')
+      29.0±0.2ms       32.3±0.3ms     1.11  frame_ctor.FromLists.time_frame_from_lists
+     3.09±0.05ms      3.43±0.03ms     1.11  rolling.VariableWindowMethods.time_rolling('DataFrame', '1h', 'float', 'sum')
+     8.93±0.06μs       9.92±0.1μs     1.11  dtypes.Dtypes.time_pandas_dtype('Int16')
+         398±3μs          442±3μs     1.11  inference.NumericInferOps.time_subtract(<class 'numpy.int16'>)
+         244±6ms          270±5ms     1.11  io.json.ToJSONISO.time_iso_format('index')
+      28.4±0.3ms       31.4±0.1ms     1.11  groupby.AggFunctions.time_different_python_functions_multicol
+     5.50±0.03ms      6.08±0.04ms     1.11  ctors.SeriesConstructors.time_series_constructor(<function arr_dict at 0x7f6536433620>, False, 'int')
+     11.2±0.03ms      12.4±0.05ms     1.11  io.hdf.HDFStoreDataFrame.time_query_store_table
+      47.8±0.2μs       52.8±0.3μs     1.10  indexing.NonNumericSeriesIndexing.time_getitem_scalar('period', 'non_monotonic')
+       227±0.7ms          251±1ms     1.10  strings.Slice.time_vector_slice
+     2.78±0.02ms      3.08±0.03ms     1.10  io.csv.ReadCSVFloatPrecision.time_read_csv(';', '_', 'round_trip')
+         399±3μs          441±9μs     1.10  inference.NumericInferOps.time_add(<class 'numpy.uint16'>)
+         2.23±0s          2.46±0s     1.10  groupby.GroupByMethods.time_dtype_as_field('float', 'describe', 'transformation')
+         192±2ms          212±2ms     1.10  io.stata.Stata.time_write_stata('tc')
+     3.10±0.03ms      3.42±0.03ms     1.10  rolling.VariableWindowMethods.time_rolling('DataFrame', '50s', 'float', 'sum')
+     5.74±0.03ms      6.32±0.04ms     1.10  ctors.SeriesConstructors.time_series_constructor(<function arr_dict at 0x7f6536433620>, True, 'int')
+         2.23±0s       2.46±0.01s     1.10  groupby.GroupByMethods.time_dtype_as_field('float', 'describe', 'direct')
+            105M             115M     1.10  rolling.PeakMemFixed.peakmem_fixed
+     2.79±0.01ms      3.07±0.02ms     1.10  io.csv.ReadCSVFloatPrecision.time_read_csv(',', '_', None)
+         3.13±0s          3.44±0s     1.10  groupby.GroupByMethods.time_dtype_as_group('int', 'describe', 'direct')
+     6.05±0.02ms      6.66±0.02ms     1.10  reshape.SimpleReshape.time_stack
+         405±2μs          446±7μs     1.10  inference.NumericInferOps.time_multiply(<class 'numpy.int16'>)
+         2.12±0s          2.33±0s     1.10  groupby.GroupByMethods.time_dtype_as_field('int', 'describe', 'direct')
+         251±3ms          276±5ms     1.10  io.stata.StataMissing.time_write_stata('tc')
+      18.4±0.2ms      20.2±0.06ms     1.10  reshape.Cut.time_qcut_timedelta(4)
+     2.77±0.01ms      3.05±0.01ms     1.10  io.csv.ReadCSVFloatPrecision.time_read_csv(';', '_', None)
+         3.13±0s          3.44±0s     1.10  groupby.GroupByMethods.time_dtype_as_group('int', 'describe', 'transformation')
-      1.10±0.01s          996±6ms     0.91  reshape.Unstack.time_without_last_row('category')
-      16.9±0.1ms      15.4±0.06ms     0.91  frame_methods.Apply.time_apply_lambda_mean
-         262±1μs        238±0.3μs     0.91  indexing.CategoricalIndexIndexing.time_getitem_bool_array('monotonic_decr')
-     3.97±0.01ms         3.59±0ms     0.91  offset.OffsetSeriesArithmetic.time_add_offset(<SemiMonthBegin: day_of_month=15>)
-      3.71±0.2μs       3.36±0.1μs     0.90  index_cached_properties.IndexCache.time_inferred_type('IntervalIndex')
-      16.7±0.9ms      15.0±0.07ms     0.90  stat_ops.Rank.time_average_old('DataFrame', False)
-         117±1ms        106±0.6ms     0.90  io.json.ToJSON.time_to_json('split', 'df_date_idx')
-      7.54±0.6μs       6.78±0.2μs     0.90  index_cached_properties.IndexCache.time_shape('TimedeltaIndex')
-     5.96±0.03ms       5.35±0.1ms     0.90  frame_methods.Interpolate.time_interpolate_some_good('infer')
-     4.43±0.05μs      3.98±0.01μs     0.90  series_methods.SeriesGetattr.time_series_datetimeindex_repr
-         186±6ms          167±1ms     0.90  categoricals.Rank.time_rank_string
-      78.7±0.3ms       70.3±0.3ms     0.89  rolling.Apply.time_rolling('Series', 3, 'int', <built-in function sum>, False)
-      84.0±0.6ms       74.9±0.5ms     0.89  binary_ops.Ops.time_frame_comparison(False, 1)
-      84.6±0.9ms         75.4±1ms     0.89  binary_ops.Ops.time_frame_comparison(False, 'default')
-        74.2±2ms       66.1±0.3ms     0.89  rolling.Apply.time_rolling('Series', 300, 'int', <built-in function sum>, False)
-         169±1μs        151±0.9μs     0.89  frame_methods.Dtypes.time_frame_dtypes
-      98.6±0.7μs       87.6±0.8μs     0.89  series_methods.NanOps.time_func('argmax', 1000, 'float64')
-      79.7±0.4ms       70.7±0.2ms     0.89  rolling.Apply.time_rolling('DataFrame', 3, 'int', <built-in function sum>, False)
-      79.5±0.4ms       70.5±0.3ms     0.89  rolling.Apply.time_rolling('DataFrame', 3, 'float', <built-in function sum>, False)
-        75.1±2ms       66.6±0.2ms     0.89  rolling.Apply.time_rolling('DataFrame', 300, 'float', <built-in function sum>, False)
-      12.5±0.5ms       11.1±0.3ms     0.88  categoricals.Rank.time_rank_string_cat
-      79.1±0.8ms       69.9±0.1ms     0.88  rolling.Apply.time_rolling('Series', 3, 'float', <built-in function sum>, False)
-     3.71±0.02μs      3.27±0.03μs     0.88  dtypes.DtypesInvalid.time_pandas_dtype_invalid('scalar-int')
-        74.5±2ms       65.6±0.2ms     0.88  rolling.Apply.time_rolling('Series', 300, 'float', <built-in function sum>, False)
-      52.6±0.2μs       46.3±0.1μs     0.88  timedelta.TimedeltaIndexing.time_series_loc
-        75.6±2ms      66.4±0.08ms     0.88  rolling.Apply.time_rolling('DataFrame', 300, 'int', <built-in function sum>, False)
-     3.50±0.02ms      3.06±0.01ms     0.88  offset.OffsetSeriesArithmetic.time_add_offset(<BusinessDay>)
-     5.17±0.02ms      4.50±0.03ms     0.87  timeseries.ResampleSeries.time_resample('datetime', '1D', 'mean')
-     1.77±0.01ms         1.54±0ms     0.87  timeseries.ToDatetimeCacheSmallCount.time_unique_date_strings(True, 5000)
-      3.93±0.1μs      3.40±0.09μs     0.87  index_cached_properties.IndexCache.time_shape('PeriodIndex')
-      6.49±0.2μs       5.61±0.1μs     0.87  index_object.Indexing.time_get_loc('Int')
-      26.4±0.7μs       22.8±0.2μs     0.86  series_methods.SearchSorted.time_searchsorted('int32')
-     2.91±0.01ms      2.51±0.01ms     0.86  sparse.FromCoo.time_sparse_series_from_coo
-     6.52±0.03μs      5.62±0.04μs     0.86  index_object.Indexing.time_get_loc_sorted('Int')
-         920±2μs          793±1μs     0.86  frame_methods.Iteration.time_itertuples_raw_start
-         621±2μs          535±2μs     0.86  groupby.GroupByMethods.time_dtype_as_field('datetime', 'tail', 'direct')
-      26.5±0.7μs       22.9±0.4μs     0.86  series_methods.SearchSorted.time_searchsorted('int64')
-         274±2ms          235±2ms     0.86  io.json.ToJSON.time_to_json_wide('index', 'df_td_int_ts')
-         927±1μs          797±2μs     0.86  frame_methods.Iteration.time_itertuples_raw_read_first
-      26.1±0.3μs       22.4±0.2μs     0.86  series_methods.SearchSorted.time_searchsorted('uint8')
-         772±2μs          662±1μs     0.86  timeseries.ToDatetimeCacheSmallCount.time_unique_date_strings(True, 500)
-         597±2μs          512±2μs     0.86  groupby.GroupByMethods.time_dtype_as_field('datetime', 'head', 'direct')
-         622±2μs          532±1μs     0.86  groupby.GroupByMethods.time_dtype_as_field('datetime', 'tail', 'transformation')
-         600±2μs          513±2μs     0.85  groupby.GroupByMethods.time_dtype_as_field('datetime', 'head', 'transformation')
-      26.4±0.6μs       22.5±0.3μs     0.85  series_methods.SearchSorted.time_searchsorted('int8')
-      9.52±0.5μs       8.12±0.1μs     0.85  algorithms.DuplicatedUniqueIndex.time_duplicated_unique('float')
-         270±1ms          230±1ms     0.85  io.csv.ToCSV.time_frame('long')
-         273±2ms          233±2ms     0.85  io.json.ToJSON.time_to_json_wide('records', 'df_td_int_ts')
-         180±3ms          153±2ms     0.85  io.json.ToJSON.time_to_json('index', 'df_td_int_ts')
-      26.1±0.3μs       22.1±0.4μs     0.85  series_methods.SearchSorted.time_searchsorted('uint16')
-      26.8±0.3μs       22.7±0.1μs     0.85  series_methods.SearchSorted.time_searchsorted('int16')
-         152±1ms          128±2ms     0.85  io.json.ToJSON.time_to_json('records', 'df_td_int_ts')
-     9.50±0.03μs      8.04±0.06μs     0.85  algorithms.DuplicatedUniqueIndex.time_duplicated_unique('int')
-      26.5±0.3μs      22.3±0.09μs     0.84  series_methods.SearchSorted.time_searchsorted('uint32')
-      29.5±0.6μs       24.8±0.3μs     0.84  series_methods.SearchSorted.time_searchsorted('uint64')
-     4.61±0.04ms      3.84±0.02ms     0.83  timeseries.ResampleDatetetime64.time_resample
-      9.74±0.9μs       8.11±0.1μs     0.83  algorithms.DuplicatedUniqueIndex.time_duplicated_unique('string')
-     9.90±0.03ms      8.15±0.01ms     0.82  inference.DateInferOps.time_add_timedeltas
-       286±0.7ms        235±0.9ms     0.82  frame_methods.Apply.time_apply_user_func
-        70.4±3ms         57.9±1ms     0.82  plotting.SeriesPlotting.time_series_plot('line')
-         526±2ms        432±0.7ms     0.82  frame_methods.Nunique.time_frame_nunique
-       294±0.6ms          242±3ms     0.82  frame_methods.Duplicated.time_frame_duplicated_wide
-         256±2ms        210±0.4ms     0.82  io.json.ToJSON.time_to_json_wide('split', 'df_td_int_ts')
-     3.46±0.02ms      2.84±0.03ms     0.82  frame_methods.Interpolate.time_interpolate_some_good(None)
-     4.10±0.04ms      3.35±0.01ms     0.82  groupby.Datelike.time_sum('date_range')
-         256±1ms          209±2ms     0.82  io.json.ToJSON.time_to_json_wide('values', 'df_td_int_ts')
-        199±20ms        161±0.4ms     0.81  algorithms.Factorize.time_factorize(True, 'string')
-      10.3±0.2ms      8.36±0.02ms     0.81  inference.DateInferOps.time_subtract_datetimes
-       216±0.8μs        175±0.7μs     0.81  period.Indexing.time_unique
-      47.4±0.1μs       38.2±0.3μs     0.81  series_methods.NanOps.time_func('argmax', 1000, 'int64')
-         246±1ms        198±0.8ms     0.80  frame_methods.Interpolate.time_interpolate('infer')
-     7.62±0.01ms      6.10±0.05ms     0.80  io.hdf.HDFStoreDataFrame.time_store_info
-     34.0±0.08ms      27.2±0.05ms     0.80  io.csv.ToCSV.time_frame('mixed')
-      47.2±0.2μs       37.5±0.1μs     0.80  series_methods.NanOps.time_func('argmax', 1000, 'int8')
-      47.4±0.2μs       37.6±0.2μs     0.79  series_methods.NanOps.time_func('argmax', 1000, 'int32')
-     10.5±0.05ms      8.28±0.01ms     0.79  reindex.DropDuplicates.time_frame_drop_dups(True)
-         160±4ms          126±4ms     0.79  io.json.ToJSON.time_to_json('columns', 'df_td_int_ts')
-         188±2ms          148±2ms     0.79  frame_methods.Interpolate.time_interpolate(None)
-      21.1±0.4ms         16.6±1ms     0.78  algorithms.FactorizeUnique.time_factorize(False, 'string')
-     12.0±0.04ms      9.33±0.02ms     0.78  reindex.DropDuplicates.time_frame_drop_dups_na(True)
-         316±1μs          242±2μs     0.76  index_object.SetOperations.time_operation('datetime', 'union')
-     2.34±0.02ms      1.78±0.01ms     0.76  categoricals.CategoricalSlicing.time_getitem_bool_array('monotonic_incr')
-     2.36±0.01ms      1.79±0.01ms     0.76  categoricals.CategoricalSlicing.time_getitem_bool_array('monotonic_decr')
-      14.2±0.2μs      10.7±0.08μs     0.76  timeseries.AsOf.time_asof_single_early('Series')
-      15.1±0.1ms       11.4±0.1ms     0.76  io.csv.ToCSVDatetime.time_frame_date_formatting
-      15.3±0.2μs       11.5±0.2μs     0.75  timeseries.DatetimeIndex.time_get('tz_aware')
-         621±2ms        466±0.8ms     0.75  package.TimeImport.time_import
-       243±0.9μs        181±0.7μs     0.74  timeseries.DatetimeIndex.time_unique('dst')
-     2.68±0.01ms      1.98±0.01ms     0.74  timeseries.ResampleDataFrame.time_method('mean')
-        60.7±2ms       44.5±0.1ms     0.73  io.hdf.HDFStoreDataFrame.time_write_store_table_wide
-         919±5ns         667±20ns     0.73  timedelta.TimedeltaIndexing.time_shape
-     1.29±0.01ms          931±1μs     0.72  offset.OffsetSeriesArithmetic.time_add_offset(<DateOffset: days=2, months=2>)
-      9.40±0.1μs       6.70±0.3μs     0.71  timedelta.TimedeltaIndexing.time_get_loc
-        935±10ns          658±6ns     0.70  period.Indexing.time_shape
-         497±3ms          349±2ms     0.70  groupby.GroupByMethods.time_dtype_as_group('float', 'unique', 'direct')
-         502±5ms          351±3ms     0.70  groupby.GroupByMethods.time_dtype_as_group('float', 'unique', 'transformation')
-         224±3ms        156±0.9ms     0.70  groupby.GroupByMethods.time_dtype_as_field('float', 'unique', 'transformation')
-         218±2ms          151±1ms     0.69  groupby.GroupByMethods.time_dtype_as_field('int', 'unique', 'direct')
-         505±4ms          350±2ms     0.69  groupby.GroupByMethods.time_dtype_as_group('datetime', 'unique', 'direct')
-     11.2±0.08μs      7.76±0.05μs     0.69  timedelta.TimedeltaIndexing.time_shallow_copy
-         219±2ms        151±0.6ms     0.69  groupby.GroupByMethods.time_dtype_as_field('int', 'unique', 'transformation')
-         506±2ms          350±2ms     0.69  groupby.GroupByMethods.time_dtype_as_group('datetime', 'unique', 'transformation')
-         227±9ms        157±0.7ms     0.69  groupby.GroupByMethods.time_dtype_as_field('float', 'unique', 'direct')
-         325±3ms        223±0.6ms     0.69  groupby.GroupByMethods.time_dtype_as_group('int', 'unique', 'direct')
-       139±0.8ms       95.2±0.6ms     0.68  io.json.ToJSON.time_to_json('values', 'df_td_int_ts')
-        30.9±1ms       21.1±0.6ms     0.68  algorithms.Duplicated.time_duplicated(False, 'string')
-      31.5±0.2ms      21.5±0.06ms     0.68  stat_ops.Correlation.time_corrwith_cols('pearson')
-         331±6ms          225±1ms     0.68  groupby.GroupByMethods.time_dtype_as_group('int', 'unique', 'transformation')
-        12.0±4μs      8.08±0.05μs     0.68  algorithms.DuplicatedUniqueIndex.time_duplicated_unique('uint')
-     1.06±0.01ms          709±3μs     0.67  offset.OffsetSeriesArithmetic.time_add_offset(<BusinessMonthEnd>)
-        1.07±0ms          709±5μs     0.67  offset.OffsetSeriesArithmetic.time_add_offset(<BusinessYearEnd: month=12>)
-        1.06±0ms          702±3μs     0.66  offset.OffsetSeriesArithmetic.time_add_offset(<BusinessQuarterEnd: startingMonth=3>)
-        45.9±4ms       30.4±0.1ms     0.66  algorithms.Factorize.time_factorize(False, 'string')
-     1.03±0.01ms          681±2μs     0.66  offset.OffsetSeriesArithmetic.time_add_offset(<BusinessQuarterBegin: startingMonth=3>)
-        1.04±0ms          687±2μs     0.66  offset.OffsetSeriesArithmetic.time_add_offset(<YearEnd: month=12>)
-         163±1ms          107±1ms     0.66  io.json.ToJSON.time_to_json('split', 'df_td_int_ts')
-        1.03±0ms          678±1μs     0.66  offset.OffsetSeriesArithmetic.time_add_offset(<BusinessYearBegin: month=1>)
-        1.03±0ms          676±3μs     0.65  offset.OffsetSeriesArithmetic.time_add_offset(<QuarterEnd: startingMonth=3>)
-        1.03±0ms          670±1μs     0.65  offset.OffsetSeriesArithmetic.time_add_offset(<BusinessMonthBegin>)
-        1.03±0ms          671±4μs     0.65  offset.OffsetSeriesArithmetic.time_add_offset(<MonthEnd>)
-         255±1ms        167±0.6ms     0.65  groupby.GroupByMethods.time_dtype_as_field('object', 'unique', 'direct')
-        1.02±0ms        662±0.7μs     0.65  offset.OffsetSeriesArithmetic.time_add_offset(<QuarterBegin: startingMonth=3>)
-        1.01±0ms          659±5μs     0.65  offset.OffsetSeriesArithmetic.time_add_offset(<YearBegin: month=1>)
-     1.01±0.01ms          652±1μs     0.65  offset.OffsetSeriesArithmetic.time_add_offset(<MonthBegin>)
-         258±2ms          166±1ms     0.64  groupby.GroupByMethods.time_dtype_as_field('object', 'unique', 'transformation')
-         252±2ms          163±1ms     0.64  eval.Eval.time_and('python', 'all')
-            206M             133M     0.64  reshape.Cut.peakmem_cut_interval(10)
-            206M             133M     0.64  reshape.Cut.peakmem_cut_interval(4)
-            207M             133M     0.64  reshape.Cut.peakmem_cut_interval(1000)
-     4.76±0.02ms      3.03±0.01ms     0.64  timeseries.ResampleDataFrame.time_method('min')
-         983±2ms          618±1ms     0.63  io.json.ReadJSON.time_read_json('index', 'datetime')
-        1.02±0ms          640±2μs     0.63  offset.OffsetSeriesArithmetic.time_add_offset(<Day>)
-        3.87±0ms      2.41±0.01ms     0.62  index_object.SetOperations.time_operation('datetime', 'intersection')
-         150±4ms         92.7±4ms     0.62  binary_ops.Ops.time_frame_multi_and(False, 'default')
-         151±4ms         92.9±3ms     0.62  binary_ops.Ops.time_frame_multi_and(False, 1)
-         263±2ms          162±1ms     0.62  eval.Eval.time_and('python', 1)
-     1.92±0.05ms      1.18±0.06ms     0.62  reindex.LevelAlign.time_align_level
-     8.72±0.08μs      5.35±0.03μs     0.61  timeseries.DatetimeIndex.time_get('repeated')
-       191±0.6μs        117±0.5μs     0.61  timedelta.TimedeltaIndexing.time_unique
-      22.7±0.2ms      13.8±0.06ms     0.61  algorithms.Duplicated.time_duplicated('last', 'string')
-      1.12±0.01s          679±1ms     0.60  io.json.ReadJSON.time_read_json('index', 'int')
-     8.52±0.07μs      5.14±0.02μs     0.60  timeseries.DatetimeIndex.time_get('tz_naive')
-     4.75±0.01ms         2.86±0ms     0.60  timeseries.ResampleDataFrame.time_method('max')
-     8.49±0.05μs      5.12±0.02μs     0.60  timeseries.DatetimeIndex.time_get('dst')
-      22.8±0.3ms      13.7±0.02ms     0.60  algorithms.Duplicated.time_duplicated('first', 'string')
-         149±4ms         87.2±3ms     0.59  binary_ops.Ops.time_frame_multi_and(True, 'default')
-         160±4ms         93.6±3ms     0.59  binary_ops.Ops.time_frame_multi_and(True, 1)
-        1.95±0ms         1.12±0ms     0.57  groupby.GroupByMethods.time_dtype_as_group('object', 'unique', 'direct')
-     1.95±0.01ms         1.11±0ms     0.57  groupby.GroupByMethods.time_dtype_as_group('object', 'unique', 'transformation')
-     2.10±0.06ms      1.20±0.06ms     0.57  reindex.LevelAlign.time_reindex_level
-         652±1ms          348±1ms     0.53  stat_ops.Correlation.time_corrwith_rows('pearson')
-     1.83±0.03ms          949±8μs     0.52  replace.FillNa.time_replace(True)
-      10.0±0.5ms       5.06±0.8ms     0.50  binary_ops.Timeseries.time_timestamp_ops_diff('US/Eastern')
-        184±10ms         89.7±1ms     0.49  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function pow>)
-         128±1ms       61.4±0.5ms     0.48  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function floordiv>)
-        197±20ms         93.4±4ms     0.47  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function pow>)
-         124±2ms       58.6±0.5ms     0.47  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function floordiv>)
-      9.59±0.4ms       4.51±0.1ms     0.47  rolling.EWMMethods.time_ewm('Series', 10, 'int', 'mean')
-         125±1ms       58.7±0.5ms     0.47  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function floordiv>)
-         126±2ms       59.0±0.5ms     0.47  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function floordiv>)
-         133±1ms       61.2±0.7ms     0.46  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function floordiv>)
-        202±10ms         92.7±4ms     0.46  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function pow>)
-         132±1ms       60.3±0.5ms     0.46  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function floordiv>)
-      9.37±0.4ms       4.26±0.1ms     0.45  rolling.EWMMethods.time_ewm('Series', 10, 'float', 'mean')
-      9.59±0.4ms       4.35±0.1ms     0.45  rolling.EWMMethods.time_ewm('Series', 1000, 'int', 'mean')
-      9.43±0.3ms       4.25±0.1ms     0.45  rolling.EWMMethods.time_ewm('Series', 1000, 'float', 'mean')
-         127±1ms       55.9±0.4ms     0.44  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function floordiv>)
-         128±1ms       56.1±0.5ms     0.44  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function floordiv>)
-       126±0.9μs      53.9±0.09μs     0.43  categoricals.CategoricalOps.time_categorical_op('__eq__')
-       127±0.6μs       54.3±0.2μs     0.43  categoricals.CategoricalOps.time_categorical_op('__gt__')
-       127±0.9μs       54.3±0.3μs     0.43  categoricals.CategoricalOps.time_categorical_op('__ge__')
-      75.5±0.5ms       27.1±0.6ms     0.36  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function mod>)
-     7.66±0.01ms      2.74±0.06ms     0.36  rolling.EWMMethods.time_ewm('DataFrame', 10, 'int', 'mean')
-       114±0.4ms      40.6±0.02ms     0.36  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function mod>)
-     5.33±0.07ms         1.87±0ms     0.35  series_methods.Dir.time_dir_strings
-      77.0±0.6ms       27.0±0.7ms     0.35  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function mod>)
-     7.67±0.03ms      2.69±0.02ms     0.35  rolling.EWMMethods.time_ewm('DataFrame', 1000, 'int', 'mean')
-         114±1ms      39.7±0.03ms     0.35  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function mod>)
-      93.1±0.9ms      32.0±0.06ms     0.34  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function mod>)
-      86.7±0.7ms      29.5±0.03ms     0.34  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function mod>)
-      86.9±0.9ms      29.5±0.04ms     0.34  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function mod>)
-     7.52±0.02ms      2.55±0.01ms     0.34  rolling.EWMMethods.time_ewm('DataFrame', 10, 'float', 'mean')
-     7.53±0.02ms      2.54±0.01ms     0.34  rolling.EWMMethods.time_ewm('DataFrame', 1000, 'float', 'mean')
-      88.8±0.7ms      29.7±0.01ms     0.33  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function mod>)
-      3.40±0.02s       1.08±0.01s     0.32  reshape.Cut.time_cut_interval(1000)
-         105±4ms       32.9±0.5ms     0.31  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function pow>)
-         333±4ms        102±0.2ms     0.31  binary_ops.Ops2.time_frame_float_div_by_zero
-       336±0.9ms        103±0.4ms     0.31  binary_ops.Ops2.time_frame_int_div_by_zero
-        97.8±1ms       29.5±0.4ms     0.30  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function pow>)
-        97.5±1ms       29.4±0.3ms     0.30  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function pow>)
-      2.91±0.01s         792±10ms     0.27  reshape.Cut.time_cut_interval(4)
-      2.98±0.03s          803±7ms     0.27  reshape.Cut.time_cut_interval(10)
-      96.2±0.8ms       25.3±0.7ms     0.26  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function pow>)
-      19.4±0.7ms       4.87±0.3ms     0.25  rolling.EWMMethods.time_ewm('Series', 10, 'int', 'std')
-      19.3±0.7ms       4.71±0.3ms     0.24  rolling.EWMMethods.time_ewm('Series', 1000, 'int', 'std')
-     2.34±0.02μs         557±10ns     0.24  multiindex_object.Integer.time_is_monotonic
-      19.5±0.3ms       4.61±0.2ms     0.24  rolling.EWMMethods.time_ewm('Series', 10, 'float', 'std')
-      19.5±0.3ms       4.61±0.2ms     0.24  rolling.EWMMethods.time_ewm('Series', 1000, 'float', 'std')
-     19.5±0.07ms      3.66±0.02ms     0.19  rolling.EWMMethods.time_ewm('DataFrame', 1000, 'int', 'std')
-     19.6±0.07ms      3.66±0.02ms     0.19  rolling.EWMMethods.time_ewm('DataFrame', 10, 'int', 'std')
-     19.4±0.08ms      3.50±0.01ms     0.18  rolling.EWMMethods.time_ewm('DataFrame', 1000, 'float', 'std')
-     19.4±0.05ms      3.49±0.01ms     0.18  rolling.EWMMethods.time_ewm('DataFrame', 10, 'float', 'std')
-      18.7±0.2ms      2.98±0.01ms     0.16  stat_ops.Correlation.time_corr('spearman')
-         476±9ms       45.6±0.8ms     0.10  binary_ops.Ops2.time_frame_float_floor_by_zero
-      8.78±0.5ms          746±3μs     0.09  period.Algorithms.time_drop_duplicates('series')
-        757±10ms       60.1±0.2ms     0.08  stat_ops.Correlation.time_corr_wide('spearman')
-     18.8±0.03ms         1.32±0ms     0.07  frame_methods.SelectDtypes.time_select_dtypes(100)
-        66.8±1ms       4.41±0.2ms     0.07  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function truediv>)
-        64.0±1ms       3.90±0.6ms     0.06  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function add>)
-      17.7±0.2ms         1.07±0ms     0.06  index_object.IntervalIndexMethod.time_intersection_both_duplicate(1000)
-        65.8±2ms       3.73±0.1ms     0.06  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function truediv>)
-        64.1±1ms       3.57±0.3ms     0.06  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function sub>)
-      64.6±0.9ms      3.58±0.09ms     0.06  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function mul>)
-      64.5±0.3ms       3.57±0.3ms     0.06  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function sub>)
-      72.3±0.9ms       4.00±0.3ms     0.06  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function truediv>)
-      64.2±0.7ms       3.49±0.2ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function mul>)
-        71.2±1ms       3.85±0.2ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function truediv>)
-      64.7±0.7ms       3.44±0.1ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function add>)
-      65.2±0.7ms       3.40±0.3ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function truediv>)
-      66.4±0.9ms       3.36±0.1ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function truediv>)
-      70.3±0.7ms       3.51±0.2ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function truediv>)
-      58.1±0.9ms      2.74±0.02ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function eq>)
-      58.3±0.8ms      2.73±0.01ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function ge>)
-      58.0±0.8ms      2.70±0.01ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function lt>)
-      58.4±0.8ms      2.71±0.01ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function gt>)
-      58.3±0.6ms         2.70±0ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function le>)
-      58.3±0.7ms      2.69±0.01ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function ne>)
-      59.0±0.9ms      2.71±0.01ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function ge>)
-        74.2±3ms       3.41±0.3ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function truediv>)
-      59.3±0.9ms      2.72±0.02ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function lt>)
-      59.0±0.8ms         2.71±0ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function gt>)
-      58.8±0.5ms      2.69±0.02ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function eq>)
-      63.4±0.6ms       2.88±0.2ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function sub>)
-      64.1±0.9ms       2.91±0.2ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function add>)
-        69.1±1ms       3.13±0.2ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function add>)
-      59.3±0.7ms      2.68±0.01ms     0.05  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function ne>)
-        59.8±1ms      2.68±0.01ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function le>)
-        63.7±1ms      2.85±0.09ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function mul>)
-      64.2±0.8ms      2.86±0.08ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function mul>)
-      64.1±0.8ms       2.83±0.1ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function sub>)
-      63.2±0.4ms       2.78±0.1ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function add>)
-      67.3±0.5ms       2.94±0.2ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function mul>)
-        69.3±1ms       2.99±0.2ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function add>)
-     22.4±0.04ms         948±10μs     0.04  index_cached_properties.IndexCache.time_is_monotonic_decreasing('MultiIndex')
-        69.9±1ms       2.94±0.2ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 5.0, <built-in function sub>)
-        70.8±2ms       2.93±0.2ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function sub>)
-        72.2±2ms       2.95±0.2ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function mul>)
-        71.5±1ms       2.91±0.2ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function sub>)
-        70.4±2ms       2.84±0.2ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 3.0, <built-in function mul>)
-        72.2±1ms       2.87±0.2ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function add>)
-      71.6±0.7ms       2.73±0.1ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function sub>)
-      71.4±0.6ms       2.69±0.1ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function add>)
-        72.8±1ms       2.67±0.2ms     0.04  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function mul>)
-     22.1±0.06ms         717±10μs     0.03  index_cached_properties.IndexCache.time_is_monotonic_increasing('MultiIndex')
-     22.1±0.05ms         715±10μs     0.03  index_cached_properties.IndexCache.time_is_monotonic('MultiIndex')
-      56.4±0.5ms      1.48±0.01ms     0.03  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function lt>)
-      57.2±0.7ms      1.48±0.01ms     0.03  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function le>)
-      56.5±0.4ms         1.46±0ms     0.03  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function ge>)
-      57.1±0.6ms         1.47±0ms     0.03  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function le>)
-      57.0±0.6ms         1.47±0ms     0.03  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function lt>)
-      56.8±0.8ms      1.46±0.01ms     0.03  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function gt>)
-      57.0±0.5ms         1.47±0ms     0.03  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function ge>)
-      56.7±0.6ms         1.45±0ms     0.03  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function gt>)
-      56.4±0.3ms      1.42±0.01ms     0.03  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function lt>)
-      56.4±0.4ms      1.42±0.01ms     0.03  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function le>)
-      56.3±0.3ms      1.41±0.01ms     0.03  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function ge>)
-      56.8±0.3ms      1.42±0.01ms     0.03  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function lt>)
-      56.7±0.3ms      1.42±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function ne>)
-      57.0±0.2ms      1.42±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function ge>)
-      56.4±0.7ms         1.40±0ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function gt>)
-      57.0±0.2ms      1.42±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function le>)
-      57.1±0.4ms      1.42±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function le>)
-      57.1±0.4ms      1.42±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function le>)
-      57.3±0.4ms         1.42±0ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function gt>)
-      57.2±0.4ms         1.42±0ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function lt>)
-      57.0±0.8ms         1.41±0ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function ge>)
-      57.7±0.3ms         1.42±0ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function ne>)
-      57.4±0.4ms      1.41±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function lt>)
-      57.3±0.4ms      1.41±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function ge>)
-        57.3±4ms      1.41±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function gt>)
-      56.9±0.5ms      1.40±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 5.0, <built-in function eq>)
-      57.8±0.5ms         1.42±0ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function ne>)
-      57.4±0.5ms      1.40±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function gt>)
-        57.9±5ms      1.42±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function ne>)
-      57.4±0.4ms      1.40±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 4, <built-in function eq>)
-      56.7±0.7ms      1.39±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function ne>)
-      56.9±0.7ms         1.38±0ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function ne>)
-      57.4±0.3ms      1.40±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 3.0, <built-in function eq>)
-     57.9±0.05ms         1.40±0ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.float64'>, 2, <built-in function eq>)
-      56.6±0.5ms      1.37±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 4, <built-in function eq>)
-      57.0±0.5ms      1.37±0.01ms     0.02  binary_ops.IntFrameWithScalar.time_frame_op_with_scalar(<class 'numpy.int64'>, 2, <built-in function eq>)
-      7.38±0.01s          115±1ms     0.02  index_object.IntervalIndexMethod.time_intersection_both_duplicate(100000)
-         276±2ms       2.27±0.1ms     0.01  frame_ctor.FromRange.time_frame_from_range
-       201±0.5ms         1.40±0ms     0.01  frame_methods.SelectDtypes.time_select_dtypes(1000)
-      27.1±0.04s        151±0.4ms     0.01  replace.ReplaceList.time_replace_list_one_match(False)
-      24.8±0.04s       93.4±0.4ms     0.00  replace.ReplaceList.time_replace_list(False)
-      25.3±0.02s      59.0±0.08ms     0.00  replace.ReplaceList.time_replace_list_one_match(True)
-      13.7±0.5ms       5.18±0.4μs     0.00  dtypes.InferDtypes.time_infer_skipna('np-int')
-      14.6±0.4ms       4.97±0.2μs     0.00  dtypes.InferDtypes.time_infer_skipna('np-null')
-      14.9±0.4ms       5.01±0.1μs     0.00  dtypes.InferDtypes.time_infer_skipna('np-floating')
-         331±1ms      6.55±0.03μs     0.00  index_object.IndexEquals.time_non_object_equals_multiindex
-         331±3ms      2.74±0.03μs     0.00  multiindex_object.Equals.time_equals_non_object_index
-      22.9±0.06s          115±1μs     0.00  replace.ReplaceList.time_replace_list(True)

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
```

</details>

I already commented on a few PRs, for the rest would need to take a further look. Help is certainly welcome to check certain cases.

One recurrent theme seems to be a rather consistent slowdown of a bunch of groupby methods. This can also be seen on the benchmark machine (eg https://pandas.pydata.org/speed/pandas/index.html#groupby.GroupByMethods.time_dtype_as_group?p-dtype='int'&p-method='all'&p-method='any'&odfpy=)

"
748946742,38020,BUG: algos.isin numeric vs datetimelike,jbrockmendel,closed,2020-11-23T16:28:22Z,2020-11-25T22:11:40Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
467560211,27365,ERR: missing key not displayed in exception message for IntervalIndex,simonjayhawkins,closed,2019-07-12T19:09:52Z,2020-11-26T00:40:36Z,"#### Code Sample, a copy-pastable example if possible

adapted from `test_non_matching` in `pandas\tests\indexing\interval\test_interval.py`

```python
>>> import numpy as np
>>> import pandas as pd
>>> pd.__version__
'0.25.0rc0+59.g0437f6899'
>>>
>>> s=pd.Series(np.arange(5), pd.IntervalIndex.from_breaks(np.arange(6)))
>>> s
(0, 1]    0
(1, 2]    1
(2, 3]    2
(3, 4]    3
(4, 5]    4
dtype: int32
>>>
>>> s.loc[[4,5]]
(3, 4]    3
(4, 5]    4
dtype: int32
>>>
>>> s.loc[[4,5,6]]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexing.py"", line 1409, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexing.py"", line 1816, in _getitem_axis
    return self._getitem_iterable(key, axis=axis)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexing.py"", line 1118, in _getitem_iterable
    keyarr, indexer = self._get_listlike_indexer(key, axis, raise_missing=False)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexing.py"", line 1060, in _get_listlike_indexer
    indexer, keyarr = ax._convert_listlike_indexer(key, kind=self.name)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexes\base.py"", line 3239, in _convert_listlike_indexer
    indexer = self._convert_list_indexer(keyarr, kind=kind)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexes\interval.py"", line 626, in _convert_list_indexer
    raise KeyError
KeyError
>>>
>>> s.to_frame().loc[[4,5,6]]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexing.py"", line 1409, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexing.py"", line 1816, in _getitem_axis
    return self._getitem_iterable(key, axis=axis)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexing.py"", line 1118, in _getitem_iterable
    keyarr, indexer = self._get_listlike_indexer(key, axis, raise_missing=False)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexing.py"", line 1060, in _get_listlike_indexer
    indexer, keyarr = ax._convert_listlike_indexer(key, kind=self.name)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexes\base.py"", line 3239, in _convert_listlike_indexer
    indexer = self._convert_list_indexer(keyarr, kind=kind)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexes\interval.py"", line 626, in _convert_list_indexer
    raise KeyError
KeyError
>>>


```
#### Problem description

With a list indexer, the missing key should be in the exception message
 
#### Expected Output
```python
KeyError: 6
```

another example - from `test_loc_with_overlap` in `pandas\tests\indexing\interval\test_interval_new.py`

```python
>>> idx = pd.IntervalIndex.from_tuples([(1, 5), (3, 7)])
>>> s = pd.Series(range(len(idx)), index=idx)
>>> s
(1, 5]    0
(3, 7]    1
dtype: int64
>>>
>>> s.loc[pd.Interval(3, 5)]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexing.py"", line 1409, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexing.py"", line 1827, in _getitem_axis
    return self._get_label(key, axis=axis)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexing.py"", line 152, in _get_label
    return self.obj._xs(label, axis=axis)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\generic.py"", line 3738, in xs
    loc = self.index.get_loc(key)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexes\interval.py"", line 875, in get_loc
    raise KeyError(key)
KeyError: Interval(3, 5, closed='right')
>>>
>>> s.loc[[pd.Interval(3, 5)]]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexing.py"", line 1409, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexing.py"", line 1816, in _getitem_axis
    return self._getitem_iterable(key, axis=axis)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexing.py"", line 1118, in _getitem_iterable
    keyarr, indexer = self._get_listlike_indexer(key, axis, raise_missing=False)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexing.py"", line 1060, in _get_listlike_indexer
    indexer, keyarr = ax._convert_listlike_indexer(key, kind=self.name)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexes\base.py"", line 3239, in _convert_listlike_indexer
    indexer = self._convert_list_indexer(keyarr, kind=kind)
  File ""C:\Users\simon\OneDrive\code\pandas-simonjayhawkins\pandas\core\indexes\interval.py"", line 626, in _convert_list_indexer
    raise KeyError
KeyError
>>>

```
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
608169133,33843,Roll quantile support multiple quantiles as per #12093: rough sketch for discussion,cottrell,closed,2020-04-28T09:34:20Z,2020-11-26T01:25:19Z,"- [ ] closes #12093

Putting this in a PR for discussion. It definitely needs more work on kicking the arguments through properly in the python interface but someone who has strong opinions about that can hopefully step in and then will refactor and test etc.

The ""content"" is really just the cython change. 
"
694233814,36157,DOC: Fix capitalization among headings in doc/source/user_guide/ part…,ShyamDesai,closed,2020-09-06T03:08:46Z,2020-11-26T01:34:49Z,"… 1 (#32550)

- [ ] xref #32550 
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
739610557,37735,TST: parametrize in tests/plotting/test_frame.py,ivanovmg,closed,2020-11-10T06:05:22Z,2020-11-26T03:57:10Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Parametrize some tests in ``pandas/tests/plotting/test_frame.py``."
746834708,37958,BUG: DataFrame.to_html ignores formatters,ma3da,closed,2020-11-19T18:46:30Z,2020-11-26T08:06:02Z,"- [x] closes #36525
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
713009937,36779,CI: troubleshoot UnsatisfiableError in travis-36-cov on 1.1.x,simonjayhawkins,closed,2020-10-01T16:44:58Z,2020-11-26T10:31:31Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
742556876,37813,DOC: corrupted function signatures on stable,ivanovmg,closed,2020-11-13T15:53:33Z,2020-11-26T12:55:34Z,"#### Location of the documentation

Stable (v1.1.4) docs.
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_parquet.html

#### Documentation problem

On stable (v1.1.4) there are corrupted function signatures in docs for these functions at least.
 - pandas.DataFrame.to_stata
 - pandas.DataFrame.to_feather
 - pandas.DataFrame.to_parquet

![image](https://user-images.githubusercontent.com/41443370/99090999-ded11800-2601-11eb-903c-f7e600a82716.png)

Instead of named arguments followed by kwargs there are only kwargs.

There is no such problem on dev docs.
For example, here https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.to_parquet.html?highlight=to_parquet#pandas.DataFrame.to_parquet

#### Suggested fix for documentation

I though that probably decorator ``deprecate_kwargs`` was causing problems (the functions concerned are decorated),
but there is no such problem on dev docs, so the reason is not clear to me."
750822735,38064,DOC: tidy 1.1.5 release notes,simonjayhawkins,closed,2020-11-25T13:34:43Z,2020-11-26T13:07:33Z,
751575831,38081,Backport PR #38064 on branch 1.1.x (DOC: tidy 1.1.5 release notes),meeseeksmachine,closed,2020-11-26T13:04:16Z,2020-11-26T13:56:29Z,Backport PR #38064: DOC: tidy 1.1.5 release notes
726497920,37311,CI: troubleshoot travis-36-cov on 1.1.x,simonjayhawkins,closed,2020-10-21T13:37:23Z,2020-11-26T13:59:20Z,closes #37309
726434816,37309,CI: travis-36-cov builds regularly fails to build with The build has been terminated on 1.1.x,simonjayhawkins,closed,2020-10-21T12:29:53Z,2020-11-26T14:00:37Z,"The last good build can be seen at https://travis-ci.org/github/pandas-dev/pandas/builds/736681105

There are some failures that have started to appear since this started timing out that also need to be resolved.

```
=========================== short test summary info ============================
FAILED pandas/tests/test_downstream.py::test_pandas_gbq - AttributeError: mod...
FAILED pandas/tests/io/test_sql.py::TestMySQLAlchemy::test_to_sql_with_negative_npinf[load_iris_data0-input0]
FAILED pandas/tests/io/test_sql.py::TestMySQLAlchemy::test_to_sql_with_negative_npinf[load_iris_data0-input1]
FAILED pandas/tests/io/test_sql.py::TestMySQLAlchemy::test_to_sql_with_negative_npinf[load_iris_data0-input2]
FAILED pandas/tests/io/test_sql.py::TestMySQLAlchemyConn::test_to_sql_with_negative_npinf[load_iris_data0-input0]
FAILED pandas/tests/io/test_sql.py::TestMySQLAlchemyConn::test_to_sql_with_negative_npinf[load_iris_data0-input1]
FAILED pandas/tests/io/test_sql.py::TestMySQLAlchemyConn::test_to_sql_with_negative_npinf[load_iris_data0-input2]
FAILED pandas/tests/util/test_show_versions.py::test_show_versions - Attribut...
ERROR pandas/tests/io/test_gbq.py - AttributeError: module 'google.protobuf.d...
= 8 failed, 73783 passed, 1033 skipped, 1053 xfailed, 6 xpassed, 77 warnings, 1 error in 1533.46s (0:25:33) =
```

xref #36779, https://github.com/pandas-dev/pandas/pull/37303#issuecomment-713513751, https://github.com/pandas-dev/pandas/pull/37304#issuecomment-713515608"
750345801,38055,CLN: remove ABCIndex,jbrockmendel,closed,2020-11-25T03:20:37Z,2020-11-26T15:34:43Z,"It's a footgun.

BTW I expect the CI to fail until #38020 goes in, which will [demonstrate](https://github.com/pandas-dev/pandas/pull/38020#discussion_r529159757) that there is a test case that uses MultiIndex."
681826774,35803,Performance regression in tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc,TomAugspurger,closed,2020-08-19T13:09:01Z,2020-11-26T15:35:30Z,"https://pandas.pydata.org/speed/pandas/index.html#tslibs.tz_convert.TimeTZConvert.time_tz_convert_from_utc?p-size=1000000&p-tz=datetime.timezone.utc&commits=3701a9b1bfc3ad3890c2fb1fe1974a4768f6d5f8-522f855a38d52763c4e2b07480786163b91dad38

Somewhere in https://github.com/pandas-dev/pandas/compare/3701a9b1bfc3ad3890c2fb1fe1974a4768f6d5f8...522f855a38d52763c4e2b07480786163b91dad38"
751168770,38074,PERF: fix regression in tz_convert_from_utc,lidavidm,closed,2020-11-25T23:10:32Z,2020-11-26T15:35:34Z,"- [x] closes #35803 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry - N/A I believe

Get rid of an unnecessary copy."
742760389,37821,BUG: Groupby.last accepts min_count but implementation raises,phofl,closed,2020-11-13T20:18:45Z,2020-11-26T15:36:26Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
df =  DataFrame({""a"": [1, 2, 3, 4, 5, 6]})
df.groupby(level=0).last(min_count=1)

```

#### Problem description

This raises 
```
Traceback (most recent call last):
  File ""/home/developer/.config/JetBrains/PyCharm2020.2/scratches/scratch_4.py"", line 97, in <module>
    print(df.groupby(level=0).last(min_count=1))
  File ""/home/developer/PycharmProjects/pandas/pandas/core/groupby/groupby.py"", line 1710, in last
    return self._agg_general(
  File ""/home/developer/PycharmProjects/pandas/pandas/core/groupby/groupby.py"", line 1032, in _agg_general
    result = self._cython_agg_general(
  File ""/home/developer/PycharmProjects/pandas/pandas/core/groupby/generic.py"", line 1018, in _cython_agg_general
    agg_mgr = self._cython_agg_blocks(
  File ""/home/developer/PycharmProjects/pandas/pandas/core/groupby/generic.py"", line 1116, in _cython_agg_blocks
    new_mgr = data.apply(blk_func, ignore_failures=True)
  File ""/home/developer/PycharmProjects/pandas/pandas/core/internals/managers.py"", line 425, in apply
    applied = b.apply(f, **kwargs)
  File ""/home/developer/PycharmProjects/pandas/pandas/core/internals/blocks.py"", line 368, in apply
    result = func(self.values, **kwargs)
  File ""/home/developer/PycharmProjects/pandas/pandas/core/groupby/generic.py"", line 1067, in blk_func
    result, _ = self.grouper.aggregate(
  File ""/home/developer/PycharmProjects/pandas/pandas/core/groupby/ops.py"", line 594, in aggregate
    return self._cython_operation(
  File ""/home/developer/PycharmProjects/pandas/pandas/core/groupby/ops.py"", line 547, in _cython_operation
    result = self._aggregate(result, counts, values, codes, func, min_count)
  File ""/home/developer/PycharmProjects/pandas/pandas/core/groupby/ops.py"", line 608, in _aggregate
    agg_func(result, counts, values, comp_ids, min_count)
  File ""pandas/_libs/groupby.pyx"", line 906, in pandas._libs.groupby.group_last
AssertionError: 'min_count' only used in add and prod

Process finished with exit code 1
```

#### Expected Output

I would expect, that this function does not accept ``min_count`` as input (similar to ``Resample.last``) and raises the same error as described in #37768 
Additonally the docs need adjustments, because ``min_count`` is documented. This probably holds true for most of the functions mentioned in https://github.com/pandas-dev/pandas/issues/37768#issuecomment-727009192 too.

#### Output of ``pd.show_versions()``

<details>

master

</details>
"
741072963,37768,"BUG:  downsampling with last doesn't accept ""min_count"" keyword",DiSchi123,closed,2020-11-11T21:11:02Z,2020-11-26T15:36:26Z,"I verified this on v 1.1.3. I am on miniconda, 1.1.4 is not available yet. There are ways to install probably but if I read documentation right, this feature should be around since 0.22


```
import numpy as np
import pandas as pd

index = pd.date_range(start='2020', freq='M', periods=6)
data = np.ones(6)
data[4:6] = np.nan
datetime = pd.Series(data, index)
period = datetime.to_period()
datetime
2020-01-31    1.0
2020-02-29    1.0
2020-03-31    1.0
2020-04-30    1.0
2020-05-31    NaN
2020-06-30    NaN
Freq: M, dtype: float64
```

```
datetime.resample('Q').sum(min_count=2)
2020-03-31    3.0
2020-06-30    NaN
Freq: Q-DEC, dtype: float64
```

```
datetime.resample('Q').last(min_count=3)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-5-bd5dfd934676> in <module>
----> 1 datetime.resample('Q').last(min_count=3)

~\miniconda3\lib\site-packages\pandas\core\resample.py in g(self, _method, *args, **kwargs)
    933 
    934     def g(self, _method=method, *args, **kwargs):
--> 935         nv.validate_resampler_func(_method, args, kwargs)
    936         return self._downsample(_method)
    937 

~\miniconda3\lib\site-packages\pandas\compat\numpy\function.py in validate_resampler_func(method, args, kwargs)
    395             )
    396         else:
--> 397             raise TypeError(""too many arguments passed in"")
    398 
    399 

TypeError: too many arguments passed in
```



#### Problem description
According to the documentation, .last() does accept the keyword ""min_count"", just like for example .sum()
where it works fine, see above

So I should not see the error above. The ""min_count"" is useful also for .last() if you have nans in your data and want to avoid the record that is not truly the last record in the segment.

Doc:
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.resample.Resampler.last.html#pandas.core.resample.Resampler.last)



#### Expected Output
```
2020-03-31    1.0
2020-06-30    NaN
```

#### Output of ``pd.show_versions()``
See above - verified with pd 1.1.3. I started the issue on my laptop with older pandas but finished with 1.1.3. Error message and pd.show_versions() is up to date

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 8.1
Version          : 6.3.9600
machine          : AMD64
processor        : Intel64 Family 6 Model 62 Stepping 4, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.1.post20201107
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.3
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.49.1


</details>
"
751624377,38088,DOC: clarify docs related to deprecation of indexing DataFrame with single partial datetime-string,jorisvandenbossche,closed,2020-11-26T14:15:02Z,2020-11-26T15:44:49Z,"A follow-up on https://github.com/pandas-dev/pandas/pull/36179#discussion_r494830867 (cc @jbrockmendel). Attempt to clarify what is actually deprecated, remove the `:okwarning:` for several cases in the docs that were not actually deprecated, and updated to use the preferred `.loc` for some others."
693051421,36113,"BUG: pandas.DataFrame().stack() raise an error, while expected is empty",evyasonov,closed,2020-09-04T11:06:46Z,2020-11-26T15:54:44Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
pandas.DataFrame().stack()
pandas.DataFrame().stack(dropna = True)
```

#### Problem description

I expect that stack() of empty dataframe will be empty dataframe and not an exception"
743330747,37870,ENH: Add support for min_count keyword for Resample and Groupby functions,phofl,closed,2020-11-15T19:50:09Z,2020-11-26T16:03:45Z,"- [x] closes #37768
- [x] closes #37821
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This would add support for the documented keyword ```min_count``. If we decide to move forward with this, we have to decide what to do in the following case:
```
df = DataFrame({""a"": [1] * 3, ""b"": [np.nan] * 3})
result = df.groupby(""a"").func(min_count=0)
```

``first`` and ``last`` return ``0.0`` which is fine I think and consistent with ``sum``, but ``min`` returns ``inf`` while ``max`` returns ``-inf``.

Default parameter is set to ``1`` to avoid backwards incompatible changes. We should keep this at least until 2.0

If we decide to not implement ``min_count`` here, we should adjust the docs and the function signatures for the groupby functions.

cc @rhshadrach "
751242739,38076,CLN: inconsistent namespace usage in tests.indexes,jbrockmendel,closed,2020-11-26T03:00:39Z,2020-11-26T16:15:46Z,"Finding these annoying while doing other work in this directory.
"
749215605,38027,REF: Share code between NumericIndex subclasses,jbrockmendel,closed,2020-11-24T00:03:27Z,2020-11-26T16:26:45Z,
747913285,37987,CLN/TST: delegate StringArray.fillna() to parent class + add tests,arw2019,closed,2020-11-21T03:28:28Z,2020-11-26T16:41:42Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
751508581,38080,DOC: Clean .ix references in indexing.rst,topper-123,closed,2020-11-26T11:17:31Z,2020-11-26T16:43:45Z,"`DataFrame.ix` is removed from the code base, so rephrase a section in the docs that was referencing it."
742140135,37807,PERF: avoid object array cast in pd.isna,arw2019,closed,2020-11-13T04:48:45Z,2020-11-26T16:50:53Z,"- [x] closes #18856
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

#18856 is a fairly old issue about avoiding an object cast in the `_isna` method in `core/dtypes/missing.py`. AFAICT we never hit that clause in the method in our current testing suite. Modulo CI turning something up we should just delete it

Update: this *does* get hit. Looking into the ideas in #18856"
705061468,36495,REGR: fillna not filling NaNs after pivot without explicitly listing pivot values,miles82,closed,2020-09-20T07:26:51Z,2020-11-26T17:00:20Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

Hi,

I have this (simplified) piece of code which seems to have stopped working correctly in Pandas 1.1:

```python
import pandas as pd

data = [
    [1, 1, 1, 1.],
    [2, 2, 2, 2.],
    [3, 3, 3, 3.],
]

df = pd.DataFrame(data, columns=['i1', 'i2', 'i3', 'f1'])
df = df.pivot('i1', 'i2')
df = df.fillna(0)
print(df)

```

#### Problem description

This is the output in Pandas 1.1.2:
```
     i3             f1
i2    1    2    3    1    2    3
i1
1   1.0  NaN  NaN  1.0  0.0  0.0
2   NaN  2.0  NaN  0.0  2.0  0.0
3   NaN  NaN  3.0  0.0  0.0  3.0
```

It contains NaNs after using fillna. It can be fixed by explicitly providing remaining columns as values to pivot (which should be the default):
`df = df.pivot('i1', 'i2', values=['i3', 'f1'])`

It also works if all columns are of the same type, like when I change f1 to contain ints instead of floats. In that case I don't need to specify the values.

#### Expected Output

This is the output on Pandas 1.0.5 and 0.25.3:
```
     i3             f1
i2    1    2    3    1    2    3
i1
1   1.0  0.0  0.0  1.0  0.0  0.0
2   0.0  2.0  0.0  0.0  2.0  0.0
3   0.0  0.0  3.0  0.0  0.0  3.0
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.7.8.final.0
python-bits      : 32
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 9, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : None.None

pandas           : 1.1.2
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.18
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
None

</details>
"
700445921,36319,"API/BUG: NumericIndex.insert(1, bool) should raise instead of cast",jbrockmendel,closed,2020-09-13T01:28:45Z,2020-11-26T17:26:00Z,"There are a bunch of setitem-like methods and we should try to be consistent in what we allow in them.  In particular, `Float64Index.putmask` will reject a bool `value` while `insert` will cast it to float."
741022211,37763,BUG: DataFrame.at setter of categorical DF overwrites entire row,treszkai,closed,2020-11-11T19:42:40Z,2020-11-26T18:08:12Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

On a DataFrame with categorical dtype, `df.at[x,y] = v` sets all non-initialized values in row `x`.

#### Code Sample, a copy-pastable example

```
$ python
Python 3.8.0 (default, Oct 28 2019, 16:14:01) 
[GCC 8.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
```

```python
>>> import pandas as pd
>>> pd.__version__
'1.2.0.dev0+1137.g50b34a4a8'
>>> df = pd.DataFrame(index=range(3), columns=range(3), dtype=pd.CategoricalDtype(['foo', 'bar']))
>>> df.at[1,2] = 'foo'
>>> df
     0    1    2
0  NaN  NaN  NaN
1  foo  foo  foo
2  NaN  NaN  NaN
```

It doesn't overwrite values that have been set with `df.loc`:

```python
>>> df = pd.DataFrame(index=range(3), columns=range(3), dtype=pd.CategoricalDtype(['foo', 'bar']))
>>> df.loc[1,1] = 'bar'  # not necessary, just for demo
>>> df.at[1,2] = 'foo'
>>> df
     0    1    2
0  NaN  NaN  NaN
1  foo  bar  foo
2  NaN  NaN  NaN
```

#### Problem description

`df.at[x, y] = v` on a categorical dtype should behave as with other dtypes, and the same as `df.loc[x, y] = v`.

#### Expected Output

The same as what happens with a DF initialized with `None`s:

```python
>>> df = pd.DataFrame([[None] * 3] * 3, index=range(3), columns=range(3), dtype=pd.CategoricalDtype(['foo', 'bar']))
>>> df.loc[1,1] = 'bar'
>>> df.at[1,2] = 'foo'
>>> df
     0    1    2
0  NaN  NaN  NaN
1  NaN  bar  foo
2  NaN  NaN  NaN
```

Or as with `dtype=float`:

```python
>>> df = pd.DataFrame(index=range(3), columns=range(3), dtype=float)
>>> df.loc[1,1] = 1
>>> df.at[1,2] = 27
>>> df
    0    1     2
0 NaN  NaN   NaN
1 NaN  1.0  27.0
2 NaN  NaN   NaN
```


#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
<ul>
<li>------------------</li>
<li>commit           : 50b34a4a8008b6da24c3dac6ae6d66536e463239</li>
<li>python           : 3.8.0.final.0</li>
<li>python-bits      : 64</li>
<li>OS               : Linux</li>
<li>OS-release       : 5.4.0-52-generic</li>
<li>Version          : #57~18.04.1-Ubuntu SMP Thu Oct 15 14:04:49 UTC 2020</li>
<li>machine          : x86_64</li>
<li>processor        : x86_64</li>
<li>byteorder        : little</li>
<li>LC_ALL           : None</li>
<li>LANG             : en_US.UTF-8</li>
<li>LOCALE           : en_US.UTF-8</li>
<li></li>
<li>pandas           : 1.2.0.dev0+1137.g50b34a4a8</li>
<li>numpy            : 1.19.4</li>
<li>pytz             : 2020.4</li>
<li>dateutil         : 2.8.1</li>
<li>pip              : 20.2.4</li>
<li>setuptools       : 50.3.2</li>
<li>Cython           : None</li>
<li>pytest           : None</li>
<li>hypothesis       : None</li>
<li>sphinx           : None</li>
<li>blosc            : None</li>
<li>feather          : None</li>
<li>xlsxwriter       : None</li>
<li>lxml.etree       : None</li>
<li>html5lib         : None</li>
<li>pymysql          : None</li>
<li>psycopg2         : None</li>
<li>jinja2           : None</li>
<li>IPython          : None</li>
<li>pandas_datareader: None</li>
<li>bs4              : None</li>
<li>bottleneck       : None</li>
<li>fsspec           : None</li>
<li>fastparquet      : None</li>
<li>gcsfs            : None</li>
<li>matplotlib       : None</li>
<li>numexpr          : None</li>
<li>odfpy            : None</li>
<li>openpyxl         : None</li>
<li>pandas_gbq       : None</li>
<li>pyarrow          : None</li>
<li>pyxlsb           : None</li>
<li>s3fs             : None</li>
<li>scipy            : None</li>
<li>sqlalchemy       : None</li>
<li>tables           : None</li>
<li>tabulate         : None</li>
<li>xarray           : None</li>
<li>xlrd             : None</li>
<li>xlwt             : None</li>
<li>numba            : None</li>
</ul>
</details>
"
750296304,38052,BUG: get_indexer_non_unique not excluding boolean like get_indexer,jbrockmendel,closed,2020-11-25T02:26:54Z,2020-11-26T18:48:00Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

There's a check in get_indexer that is missing from get_indexer_non_unique, which this adds.  This is going to make some de-duplication easier.

Hard to find a user-facing example bc indexing with all-bool indexer we treat it as a mask and not a list of labels.  The closest I got to a user-facing example was in Series.drop, but that ran in to #38051."
751148256,38072,TST/REF: take things out of Base tests,jbrockmendel,closed,2020-11-25T22:15:45Z,2020-11-26T18:48:31Z,"Trying to clear up the confusing:

common.py defines Base
test_base.py imports Base from common
test_common does not

and try to get away from the create_index usage to just use the index fixture

Small steps.
"
749929141,38045,CLN: dont mix Int64Index into PeriodIndex,jbrockmendel,closed,2020-11-24T18:07:05Z,2020-11-26T18:50:07Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
750171827,38049,REF: share argmax axis validation test across all indexes,jbrockmendel,closed,2020-11-25T00:10:51Z,2020-11-26T18:52:17Z,"
"
749978953,38047,DOC: Remove repeated words and wrong /it's/ usage,eumiro,closed,2020-11-24T19:30:11Z,2020-11-26T18:55:15Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Remove (manually reviewed) repeated words from documentation/comments as well as wrong usage of `it's` (replaced by `its` where appropriated)."
655502373,35257,"Add keep parameter for duplicates values in idxmax, idxmin, argmax, and argmin",MatthewMoye,closed,2020-07-13T00:01:14Z,2020-11-26T18:59:27Z,"- [x] closes #34205
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
749321780,38030,"BUG: NumericIndex.insert(0, False) casting to int",jbrockmendel,closed,2020-11-24T04:02:27Z,2020-11-26T19:00:04Z,"- [x] closes #36319
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

xref #16277
"
538058730,30282,"solve ""Int64 with null value mangles large-ish integers"" problem",rushabh-v,closed,2019-12-15T14:29:59Z,2020-11-26T19:04:42Z,"- [x] closes #30268 
- [ ] tests added/passed
- [x] passes black pandas
- [x] passes git diff upstream/master -u -- ""*.py"" | flake8 --diff
- [x] whatsnew entry
The `pd.Series` was giving less precise output when inputting a list containing large integers and np.nan values and passing `dtype=""Int64""`. Which was due to converting it to `np.array`. This PR aims to solve that bug."
751589514,38084,BLD: Use conda(-forge) compilers,xhochy,closed,2020-11-26T13:24:34Z,2020-11-26T19:13:32Z,"Use the compiler setup from conda-forge inside a conda environment. The compilers in conda-forge can be different and slightly incompatible with the system ones causing some headaches. Also this hopefully helps with initial setup of new contributors.

Is this worth a whatsnew entry?

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
743293228,37864,ENH: Implement cross method for Merge Operations,phofl,closed,2020-11-15T16:35:06Z,2020-11-26T21:12:43Z,"- [x] closes #5401
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This is a first draft of a ``cross`` method for merge operations. As suggested by @jreback I added a new column and dispatched to ``inner`` as a first naive implementation. I am currently wondering, if there would be a better place to adjust the arguments than the places I selected. I wanted to avoid to modify ``self`` in some method called within the constructor, so i modified the inputs before adding the parameters to ``self``. This currently does not support the ``cross`` method for join. If we get a consensus where to put the modifications, I would add support for ``join``. Also have to add more tests probably and something to the userguide


Have to look into the performance afterwards. Copies of the Left and Right frames may be a big performance hit for bigger frames?"
743341193,37873,BUG: __getitem__ raise blank KeyError for IntervalIndex and missing keys,phofl,closed,2020-11-15T20:46:31Z,2020-11-26T21:45:48Z,"- [x] closes #27365
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Without ``tolist`` error would look like ``KeyError: array([6])``"
745308828,37931,BUG: loc.setitem corner case,jbrockmendel,closed,2020-11-18T04:03:03Z,2020-11-26T22:26:54Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @phofl if you've got any thoughts on how to handle this case"
749350264,38034,ENH: preserve RangeIndex in factorize,jbrockmendel,closed,2020-11-24T05:17:15Z,2020-11-26T22:30:49Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
749298079,38028,BUG: RangeIndex.difference with mismatched step,jbrockmendel,closed,2020-11-24T02:59:02Z,2020-11-26T22:33:20Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
742750386,37820,BUG: MultiIndex.drop does not raise if labels are partially found,phofl,closed,2020-11-13T19:59:51Z,2020-11-26T22:38:05Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
mi = MultiIndex.from_tuples([(1, 2), (2, 2), (3, 2)])
mi.drop([1, 4], level=0) # does not raise
mi.drop(4, level=0) # raises


```

#### Problem description
The docs say
```
Raises

    KeyError

        If not all of the labels are found in the selected axis

```
I think both statements should raise ``KeyError``
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.drop.html

#### Expected Output

Both are raising ``KeyError: 'labels [4] not found in level'``

For regular Indexes this holds True

```
index = Index([1, 2])
index.drop([1, 3]) # Raises
```

#### Output of ``pd.show_versions()``

<details>

master

</details>
"
701719497,36380,ENH: Make non-aggregating methods on groupby groups optionally return groups,kaiogu,open,2020-09-15T08:20:58Z,2020-11-26T23:07:13Z,"#### Is your feature request related to a problem?

I want to apply chained operations on the same groupby groups without having to actually make identical costly groupby calls before each operation. The Related problem is available on the following SO question:

https://codereview.stackexchange.com/questions/249222/get-exactly-n-unique-randomly-sampled-rows-per-category-in-a-dataframe


#### Describe the solution you'd like

Add and optional argument that changes the behavior of non-aggregating methods on groupby groups to return groups instead of DataFrames.

This is useful if you want to make multiple chained operations on the Groups without having to do a GroupBy each time (split once, apply multiple times, combine once). This would also have the benefit of actually decoupling the apply and combine parts of the split-apply-combine paradigm when possible. This does not make sense for aggregating methods, but for something like `groupby.DataFrameGroupBy.filter` or `groupby.DataFrameGroupBy.sample` (maybe anything that does not reduce the groups to a single value) returning groups was actually the expected behavior for me and for at least the person who answered the SO question above.

#### API breaking implications

This would not break the API afaik, as the current behavior can be set as the default i.e. `return_groups=False`.

#### Describe alternatives you've considered

The alternatives that I could come up with all involved multiple groupby calls (I think there is an implicit call in `value_counts`). For my specific use case (see SO question), an option to drop groups with insufficient rows would actually suffice, but I think the proposed solution is more general and decoupled.

#### Additional context
Here is the code to randomly sample exactly `n` rows per group, dropping groups with insufficient rows:

```python
n = 4
df = pd.DataFrame({'category': [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1],
    'value' : range(12)})

# sample exactly x rows per category
df = df.groupby('category').filter(lambda x: len(x) >= n).groupby('category').sample(n)

# proposed alternative solution:
df = df.groupby('category').filter(lambda x: len(x) >= n, return_groups=True).sample(n)
```

I also expected `filter` to actually pass groups instead of DataFrames to the lambda."
750321100,38054,TST: tighten assert_index_equal calls,jbrockmendel,closed,2020-11-25T02:53:57Z,2020-11-26T23:07:43Z,
317513438,20816,pd.concat fails if given repeated keys,ilyagr,closed,2018-04-25T07:42:39Z,2020-11-26T23:29:27Z,"#### Code Sample, a copy-pastable example if possible

```python
series_list = [pd.Series({""a"": 1}), pd.Series({""b"": 2}), pd.Series({""c"": 3})]
print(pd.concat(series_list, verify_integrity=True))  # This works fine

# Raises TypeError 
print(pd.concat(series_list, keys=[""red"", ""red"", ""red""]))

# Does NOT raise an exception without verify_integirty.
# However, the object `pd.concat` returns is broken (see below for details).
print(pd.concat(series_list, keys=[""red"", ""blue"", ""red""]))

# Raises ValueError
print(pd.concat(
    series_list, keys=[""red"", ""blue"", ""red""],
    verify_integrity=True)) 
```

#### Expected Output
Since the indices of the Series don't conflict, it should be fine to use the same key for several rows (after all, it works fine without any keys at all). None of these calls should raise an exception and the objects they return should not be broken (see below for what I mean). Expected output:

```
a    1
b    2
c    3
dtype: int64
red  a    1
red   b    2
red  c    3
dtype: int64
red  a    1
blue   b    2
red  c    3
dtype: int64
```

#### Problem description

This is similar to issue #20565 (but even worse, IMO). I think the problem is that `pd.concat` creates a Series with duplicate entries in its index. This is illustrated by the broken object one of the above calls creates:

```python
> broken = pd.concat(series_list, keys=[""red"", ""blue"", ""red""])
> broken.index  # Below, `levels[0]` should contain one u'red', not two.
MultiIndex(levels=[[u'red', u'blue', u'red'], [u'a', u'b', u'c']],
           labels=[[1, 0, 1, 1, 1, 0, 1], [0, 1, 2]])
```

Lots of operations fail on this object. For instance,  `broken.sort_index()`   raises ""ValueError: operands could not be broadcast together with shapes (7,) (3,) (7,)"".


#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit: None
python: 2.7.6.final.0
python-bits: 64
OS: Linux
OS-release: 4.3.5-smp-811.22.0.0
machine: x86_64
processor: 
byteorder: little
LC_ALL: en_US.UTF-8
LANG: None
LOCALE: None.None

pandas: 0.22.0
pytest: None
pip: None
setuptools: None
Cython: None
numpy: 1.13.3
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 2.0.0
sphinx: None
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2018.3
blosc: None
bottleneck: None
tables: 3.1.1
numexpr: 2.5
feather: None
matplotlib: 1.5.2
openpyxl: None
xlrd: 0.9.3
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.8
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None

</details>
"
751593706,38086,PERF: performance regression in replace() corner cases,jorisvandenbossche,closed,2020-11-26T13:30:44Z,2020-11-27T01:01:26Z,"ASV shows a *gigantic* regression (14629.25x) in a certain `replace` benchmark: https://pandas.pydata.org/speed/pandas/#replace.ReplaceList.time_replace_list?python=3.8&Cython=0.29.21&p-inplace=True&commits=07559156-dbee8fae

The simplified case is:

```
In [5]: df = pd.DataFrame({""A"": 0, ""B"": 0}, index=range(4 * 10 ** 7))

In [6]: %timeit df.replace([np.inf, -np.inf], np.nan)
100 ms ± 6.76 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)   # 1.1
1.2 s ± 31.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)  # master
```

Compared to 1.1, I don't see such a huge difference, but still a decent slowdown (x10).

Now, in this case, we have integer columns, but trying to replace infinity, which of course can never be present. So maybe before we had some shortcut for that. 
This also seems quite a cornercase, though. So not sure how critical the regression is."
742969993,37830,BUG: MultiIndex.drop does not raise if labels are partially found,GYHHAHA,closed,2020-11-14T09:42:49Z,2020-11-27T01:02:10Z,"- [x] closes #37820
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
751831109,38097,PERF: replace_list,jbrockmendel,closed,2020-11-26T21:19:47Z,2020-11-27T01:20:07Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

closes #38086, this gets us back to (very slightly ahead of) 1.1.4 performance for the cases there, will leave to @jorisvandenbossche whether that is enough to close the issue."
631897606,34608,Coding Style Guidline.rst,Stockfoot,closed,2020-06-05T19:49:06Z,2020-11-27T03:54:43Z,"Created a unified version of the Coding Style Guidelines
Extracted rules from various pandas website information as well as submission scripts that verify style requirements
Work in progress

- [x] closes #33851
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
550316439,31048,FIX: fix interpolate with kwarg limit area and limit direction using pad or bfill,cchwala,closed,2020-01-15T17:06:53Z,2020-11-27T05:06:53Z,"- [x] closes #26796
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This separates a large part of the code changes from #25141 to keep things comprehendible.
"
748090561,37989,TST: Series construction from ExtensionDtype scalar,arw2019,closed,2020-11-21T19:48:15Z,2020-11-27T05:08:52Z,"- [x] closes #28401
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
751585184,38083,PERF: regression in Series.asof with single date,jorisvandenbossche,closed,2020-11-26T13:18:21Z,2020-11-27T07:24:04Z,"From https://pandas.pydata.org/speed/pandas/#timeseries.AsOf.time_asof_single?p-constructor='Series'&commits=52a17259-24e881d4&x-axis=commit&Cython=0.29.21&python=3.8

Snippet extracted from ASV:

```python
N = 10000
rng = pd.date_range(start=""1/1/1990"", periods=N, freq=""53s"")
s = pd.Series(np.random.randn(N), index=rng)

dates = pd.date_range(start=""1/1/1990"", periods=N * 10, freq=""5s"")
date = dates[0]

%timeit s.asof(date)
```

On pandas 1.1 this takes around 20µs, on master I get around 110µs

Commit range indicated by ASV is https://github.com/pandas-dev/pandas/compare/52a17259...24e881d4
"
694940719,36185,"BUG: pandas.DataFrame().stack() raise an error, while expected is empty",steveya,closed,2020-09-07T10:07:24Z,2020-11-27T07:29:39Z,"- [ ] closes #36113 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
750122797,38048,ENH: adding support for Py3.6+ memory tracing for khash-maps,realead,closed,2020-11-24T22:48:40Z,2020-11-27T08:15:07Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Tracemalloc is  a standard tool for tracing of memory consumption in Python. Until now khash-maps were invisible for this tool.

This PR follows more or less numpy's approach (see https://github.com/numpy/numpy/commit/03534ec90dba2bcdcd649be64be57939dde4c6f5) for adding  tracemalloc-support.

This PR fixes also silly mistakes from #37920 (see https://github.com/pandas-dev/pandas/pull/38048/commits/3291ed198432a80eb559382fbf9a35cb6a8ad749) and the somewhat imprecise implementation of `sizeof` for `XXXHashTable`-classes.

"
751995935,38107,BUG: `df.copy` removes attributes from series in the original dataframe,ivirshup,closed,2020-11-27T05:41:19Z,2020-11-27T08:19:56Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
# Your code here
import pandas as pd

df = pd.DataFrame({""a"": [1, 2, 3]})
df[""a""].attrs[""key""] = ""value""
display(df[""a""].attrs)
# {'key': 'value'}
_ = df.copy()
display(df[""a""].attrs)
# {}
```

#### Problem description

Calling copy on a dataframe probably shouldn't modify it or it's contents (especially those exposed through public attributes) in any way.

I would also ague the copy should keep these attributes, but that's more related to #35425

#### Expected Output

The second `display(df[""a""].attrs)` should print: `{'key': 'value'}`

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : 67a3d4241ab84419856b84fc3ebc9abcbe66c6b3
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Oct 29 22:56:45 PDT 2020; root:xnu-6153.141.2.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.4
numpy            : 1.19.4
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.2
Cython           : 0.29.21
pytest           : 6.1.2
hypothesis       : None
sphinx           : 3.3.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.19.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.7.4
fastparquet      : 0.4.1
gcsfs            : None
matplotlib       : 3.3.3
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 2.0.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.4
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : None
numba            : 0.51.2
```

</details>
"
752145624,38108,"BUG: calling at[] on a Series with a single-level MultiIndex returns a Series, not a scalar",steveya,closed,2020-11-27T10:25:00Z,2020-11-27T10:26:57Z,"- [ ] closes #GH38053
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
706657835,36556,[TST]: Add test for duplicate keys in concat,phofl,closed,2020-09-22T20:05:24Z,2020-11-27T11:55:16Z,"- [x] closes #20816
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Added a test"
623138606,34307,ENH: nullable Float32/64 ExtensionArray,jorisvandenbossche,closed,2020-05-22T11:18:22Z,2020-11-27T14:18:27Z,"As proposed at https://github.com/pandas-dev/pandas/issues/32265#issuecomment-627004139, this is adding a float extension dtype and array (for now avoiding/ignoring the NaN-discussion), using the masked array approach following our other nullable dtypes.

Brief overview:

- This PR adds a `FloatingArray` and `Float32Dtype` / `Float64Dtype` (similar as for the nullable integer dtypes)
- I am using the ""Float64"" pattern with capital ""F"" as name for the dtype to distinguish the string name of the ""float64"" dtype from numpy (consistent with Int64 etc)
- For now I only added Float32 and Float64 (we can see later if we want to add others like float16)
- Upon conversion of input (both in construction as in operations such as setitem), any `np.nan` is coerced to NA (we can see later to add an option / config for this)
- The added tests are for now mostly copied from the existing integer/boolean tests, and adapted for Float
- It directly uses some of the masked ops I implemented earlier for IntegerArray, which means that eg `sum` already shows a similar perf benefit:
  ```python
  a = np.random.randn(1_000_000) 
  a[np.random.randint(0, 1_000_000, 100)] = np.nan 
  s1 = pd.Series(a)
  s2 = pd.Series(a, dtype=""Float64"")

  In [6]: np.allclose(s1.sum(), s2.sum()) 
  Out[6]: True

  # numpy float series
  In [7]: %timeit s1.sum()   
  2.73 ms ± 104 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

  # nullable float series
  In [8]: %timeit s2.sum()   
  875 µs ± 18.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
  ```

- Still to do:
    - [x] Add whatsnew + docs
    - [ ] Division with Int64 dtype can now result in nullable Float Dtype instead of numpy float array

- Other todo's but that can probably be left for follow-up PRs:
    - Update the `convert_dtypes` method to also return nullable floats (one question here: what with floats that are all ""integer""-like? I think we want to keep returning nullable int for that, but we might want to add a parameter controlling that behaviour?)
    - Some more arithmetical operations with IntegerArray/BooleanArray can now return a FloatingArray instead of numpy float array
    - Ufuncs on IntegerArray/BooleanArray can return FloatingArray, ufuncs on mixture of IntegerArray/FloatArray
    - Consolidate some tests that are common for integer/boolean/float
    - Deduplicate more things between the Array classes and move to BaseMaskedArray (eg the arithmetic / comparison ops can probably be moved with a bit of refactoring)
        - `__from_arrow__`
        - ufuncs
        - `coerce_to_array`, but this will probably require some refactoring
  - formatting options


"
751942418,38103,PERF: Index.searchsorted,jbrockmendel,closed,2020-11-27T02:44:09Z,2020-11-27T15:25:02Z,"- [x] closes #38083
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
751251775,38077,BUG: raise consistent exception on slicing failure,jbrockmendel,closed,2020-11-26T03:25:46Z,2020-11-27T17:41:13Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
751790722,38095,CLN: testing window namespace,mroeschke,closed,2020-11-26T19:18:41Z,2020-11-27T18:42:48Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

It appears we are heading this general direction in pandas imports in tests."
712311078,36754,ENH: Add headers paramater to read_json and read_csv,Antetokounpo,closed,2020-09-30T22:03:49Z,2020-11-27T18:43:36Z,"- [x] closes #36688
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This adds the option to specify headers when reading a csv or a json file from an URL in Python3.
Let me know if new tests are needed."
714095943,36834,Fix/empty string datetimelike conversion/issue 36550,nrebena,closed,2020-10-03T13:57:31Z,2020-11-27T18:47:26Z,"- [x] xref #36550
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

# Empty string conversion

The actual comportment of Timedelta, Timestamp and to_datetime is to return NaT.
```python
import pandas as pd
pd.to_datetime("""", errors=""raise"")
# NaT
pd.to_datetime("""", errors=""coerce"")
# NaT
pd.to_datetime("""", errors=""ignore"")
# NaT
pd.Timedelta("""")
# NaT
pd.Timestamp("""")
# NaT
```

I would propose to raise a ValueError instead, same as for string that are not datetimelike, as follow:

 ```python
import pandas as pd
import pytest

with pytest.raises(ValueError):
    pd.to_datetime("""", errors=""raise"")
pd.to_datetime("""", errors=""coerce"")
# NaT
pd.to_datetime("""", errors=""ignore"")
# ''

with pytest.raises(ValueError):
    pd.Timedelta("""")
with pytest.raises(ValueError):
    pd.Timestamp("""")
```

# Tests

I added the relevant test for the expected comportment.
I still will have to fix some test that rely on the current behaviors.

# Benchmark

I ran the following command
`asv continuous -E virtualenv -f 1.1 upstream/master HEAD -b ""^tslibs.(timestamp|timedelta|tslib)""`
<details>

```
       before           after         ratio
     [8f6ec1e8]       [4393cc9e]
     <master>         <fix/empty-string-datetimelike-conversion/issue-36550>
+         173±1ns         202±10ns     1.17  tslibs.timestamp.TimestampProperties.time_is_leap_year(tzlocal(), 'B')
+         162±2ns          188±4ns     1.16  tslibs.timestamp.TimestampProperties.time_is_quarter_start(tzlocal(), None)
+         178±1ns         205±20ns     1.15  tslibs.timestamp.TimestampProperties.time_dayofweek(tzfile('/usr/share/zoneinfo/US/Central'), 'B')
+         163±2ns         186±10ns     1.15  tslibs.timestamp.TimestampProperties.time_is_year_end(tzlocal(), None)
+         162±1ns         185±10ns     1.15  tslibs.timestamp.TimestampProperties.time_is_year_end(tzfile('/usr/share/zoneinfo/US/Central'), None)
+         162±2ns         185±10ns     1.14  tslibs.timestamp.TimestampProperties.time_is_year_end(<UTC>, None)
+         169±2ns         193±10ns     1.14  tslibs.timestamp.TimestampProperties.time_is_quarter_end(tzlocal(), None)
+         164±2ns          186±4ns     1.14  tslibs.timestamp.TimestampProperties.time_is_quarter_start(tzutc(), None)
+     4.60±0.03μs       5.16±0.3μs     1.12  tslibs.timestamp.TimestampProperties.time_is_year_end(datetime.timezone(datetime.timedelta(seconds=3600)), 'B')
+         147±1ns          163±4ns     1.11  tslibs.timestamp.TimestampProperties.time_microsecond(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, 'B')
+         170±2ns          188±7ns     1.11  tslibs.timestamp.TimestampProperties.time_is_quarter_end(tzfile('/usr/share/zoneinfo/US/Central'), None)
+       179±0.7ns         197±10ns     1.10  tslibs.timestamp.TimestampProperties.time_dayofweek(datetime.timezone(datetime.timedelta(seconds=3600)), None)
+       174±0.3ns         191±10ns     1.10  tslibs.timestamp.TimestampProperties.time_is_leap_year(<DstTzInfo 'Europe/Amsterdam' LMT+0:20:00 STD>, 'B')

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE DECREASED.
```
</details>
"
752372150,38115,"Backport PR #34407 on branch 1.1.x: REGR: revert ""CLN: _consolidate_inplace less"" / fix regression in fillna()",simonjayhawkins,closed,2020-11-27T16:44:12Z,2020-11-27T19:48:01Z,Backport PR #34407
745313196,37932,BUG: loc.setitem with expansion expanding rows,jbrockmendel,closed,2020-11-18T04:14:47Z,2020-11-27T19:52:26Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

discovered on branch that has dataframe always go through split_path"
594985720,33315,Remove `-Werror` from setup.py,xhochy,closed,2020-04-06T10:15:10Z,2020-11-27T20:03:58Z,"It seems nice to have `-Werror` in CI but as a conda-forge maintainer I quite often have to write patches to remove `-Werror` from releases as the system distributions build packages on are not identical to the ones CI of upstream runs on. There is a large variety of compiler warnings and some may only show up when other C-defines are set. Also many new warning appear when you build with a newer compiler or different versions of the dependencies used.

My suggestion would be:
 * Remove `-Werror` from `setup.py`.
 * Add `-Werror` to `CFLAGS` on CI to have the same result for pandas-dev as it is currently.

Original PR: https://github.com/pandas-dev/pandas/pull/32163

Related issues: https://github.com/pandas-dev/pandas/issues/33314, https://github.com/pandas-dev/pandas/issues/33224"
594983166,33314,"BUG: clang-9: error: -Wl,-export_dynamic: 'linker' input unused (OSX)",xhochy,closed,2020-04-06T10:11:20Z,2020-11-27T20:03:58Z,"- [x] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

### conda environment

```
conda create -n pandas-compile-test compilers numpy cython
```

<details>

```

  binutils-meta      conda-forge/osx-64::binutils-meta-1.0.4-0
  c-compiler         conda-forge/osx-64::c-compiler-1.0.4-h1239861_0
  ca-certificates    conda-forge/osx-64::ca-certificates-2020.4.5.1-hecc5488_0
  cctools            conda-forge/osx-64::cctools-927.0.2-h5ba7a2e_4
  certifi            conda-forge/osx-64::certifi-2020.4.5.1-py38h32f6830_0
  clang              conda-forge/osx-64::clang-9.0.1-default_hf57f61e_0
  clang_osx-64       conda-forge/osx-64::clang_osx-64-9.0.1-h05bbb7f_0
  clangxx            conda-forge/osx-64::clangxx-9.0.1-default_hf57f61e_0
  clangxx_osx-64     conda-forge/osx-64::clangxx_osx-64-9.0.1-h05bbb7f_0
  compiler-rt        conda-forge/osx-64::compiler-rt-9.0.1-h6a512c6_3
  compiler-rt_osx-64 conda-forge/noarch::compiler-rt_osx-64-9.0.1-h99342c6_3
  compilers          conda-forge/osx-64::compilers-1.0.4-0
  cxx-compiler       conda-forge/osx-64::cxx-compiler-1.0.4-h707564b_0
  cython             conda-forge/osx-64::cython-0.29.16-py38hc84c608_0
  fortran-compiler   conda-forge/osx-64::fortran-compiler-1.0.4-he991be0_0
  gfortran_impl_osx~ conda-forge/osx-64::gfortran_impl_osx-64-7.3.0-hf4212f2_2
  gfortran_osx-64    conda-forge/osx-64::gfortran_osx-64-7.3.0-h22b1bf0_9
  gmp                conda-forge/osx-64::gmp-6.2.0-h4a8c4bd_2
  isl                conda-forge/osx-64::isl-0.19-0
  ld64               conda-forge/osx-64::ld64-450.3-h3c32e8a_4
  libblas            conda-forge/osx-64::libblas-3.8.0-16_openblas
  libcblas           conda-forge/osx-64::libcblas-3.8.0-16_openblas
  libcxx             conda-forge/osx-64::libcxx-9.0.1-1
  libffi             conda-forge/osx-64::libffi-3.2.1-h4a8c4bd_1007
  libgfortran        conda-forge/osx-64::libgfortran-4.0.0-2
  libiconv           conda-forge/osx-64::libiconv-1.15-h0b31af3_1006
  liblapack          conda-forge/osx-64::liblapack-3.8.0-16_openblas
  libllvm9           conda-forge/osx-64::libllvm9-9.0.1-ha1b3eb9_0
  libopenblas        conda-forge/osx-64::libopenblas-0.3.9-h3d69b6c_0
  llvm-openmp        conda-forge/osx-64::llvm-openmp-9.0.1-h28b9765_2
  mpc                conda-forge/osx-64::mpc-1.1.0-h4160ff4_1006
  mpfr               conda-forge/osx-64::mpfr-4.0.2-h44b798e_0
  ncurses            conda-forge/osx-64::ncurses-6.1-h0a44026_1002
  numpy              conda-forge/osx-64::numpy-1.18.1-py38h1f821a2_1
  openssl            conda-forge/osx-64::openssl-1.1.1f-h0b31af3_0
  pip                conda-forge/noarch::pip-20.0.2-py_2
  python             conda-forge/osx-64::python-3.8.2-hd5f0129_5_cpython
  python_abi         conda-forge/osx-64::python_abi-3.8-1_cp38
  readline           conda-forge/osx-64::readline-8.0-hcfe32e1_0
  setuptools         conda-forge/osx-64::setuptools-46.1.3-py38h32f6830_0
  sqlite             conda-forge/osx-64::sqlite-3.30.1-h93121df_0
  tapi               conda-forge/osx-64::tapi-1000.10.8-ha1b3eb9_4
  tk                 conda-forge/osx-64::tk-8.6.10-hbbe82c9_0
  wheel              conda-forge/noarch::wheel-0.34.2-py_1
  xz                 conda-forge/osx-64::xz-5.2.4-h0b31af3_1002
  zlib               conda-forge/osx-64::zlib-1.2.11-h0b31af3_1006
```
</details>

Error message:

```
clang-9: error: -Wl,-export_dynamic: 'linker' input unused [-Werror,-Wunused-command-line-argument]
```

Output of `pip install -e ~/Development/pandas --no-deps --no-use-pep517 -vvv`:

<details>

```
Non-user install because site-packages writeableCreated temporary directory: /private/var/folders/3r/k0d_q3hs2mb2_jxfh_9hjyqr0000gn/T/pip-ephem-wheel-cache-ehxkyhsg
Created temporary directory: /private/var/folders/3r/k0d_q3hs2mb2_jxfh_9hjyqr0000gn/T/pip-req-tracker-waeimcjv
Initialized build tracking at /private/var/folders/3r/k0d_q3hs2mb2_jxfh_9hjyqr0000gn/T/pip-req-tracker-waeimcjv
Created build tracker: /private/var/folders/3r/k0d_q3hs2mb2_jxfh_9hjyqr0000gn/T/pip-req-tracker-waeimcjv
Entered build tracker: /private/var/folders/3r/k0d_q3hs2mb2_jxfh_9hjyqr0000gn/T/pip-req-tracker-waeimcjv
Created temporary directory: /private/var/folders/3r/k0d_q3hs2mb2_jxfh_9hjyqr0000gn/T/pip-install-5lvoa7_h
Obtaining file:///Users/uwe/Development/pandas
  Added file:///Users/uwe/Development/pandas to build tracker '/private/var/folders/3r/k0d_q3hs2mb2_jxfh_9hjyqr0000gn/T/pip-req-tracker-waeimcjv'
    Running setup.py (path:/Users/uwe/Development/pandas/setup.py) egg_info for package from file:///Users/uwe/Development/pandas
    Running command python setup.py egg_info
    Compiling pandas/_libs/hashing.pyx because it depends on /Users/uwe/miniconda3/envs/pandas-compile-test/lib/python3.8/site-packages/Cython/Includes/numpy/__init__.pxd.
    Compiling pandas/_libs/reshape.pyx because it depends on /Users/uwe/miniconda3/envs/pandas-compile-test/lib/python3.8/site-packages/Cython/Includes/numpy/__init__.pxd.
    Compiling pandas/_libs/window/indexers.pyx because it depends on /Users/uwe/miniconda3/envs/pandas-compile-test/lib/python3.8/site-packages/Cython/Includes/numpy/__init__.pxd.
    [1/3] Cythonizing pandas/_libs/hashing.pyx
    [2/3] Cythonizing pandas/_libs/reshape.pyx
    [3/3] Cythonizing pandas/_libs/window/indexers.pyx
    running egg_info
    writing pandas.egg-info/PKG-INFO
    writing dependency_links to pandas.egg-info/dependency_links.txt
    writing entry points to pandas.egg-info/entry_points.txt
    writing requirements to pandas.egg-info/requires.txt
    writing top-level names to pandas.egg-info/top_level.txt
    reading manifest file 'pandas.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    no previously-included directories found matching 'doc/build'
    warning: no previously-included files matching '*.pyd' found anywhere in distribution
    warning: no previously-included files matching '*~' found anywhere in distribution
    warning: no previously-included files matching '.DS_Store' found anywhere in distribution
    warning: no previously-included files matching '#*' found anywhere in distribution
    writing manifest file 'pandas.egg-info/SOURCES.txt'
  Source in /Users/uwe/Development/pandas has version 1.1.0.dev0+1122.g01f73100d.dirty, which satisfies requirement pandas==1.1.0.dev0+1122.g01f73100d.dirty from file:///Users/uwe/Development/pandas
  Removed pandas==1.1.0.dev0+1122.g01f73100d.dirty from file:///Users/uwe/Development/pandas from build tracker '/private/var/folders/3r/k0d_q3hs2mb2_jxfh_9hjyqr0000gn/T/pip-req-tracker-waeimcjv'
Installing collected packages: pandas
  Running setup.py develop for pandas
    Running command /Users/uwe/miniconda3/envs/pandas-compile-test/bin/python3.8 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/Users/uwe/Development/pandas/setup.py'""'""'; __file__='""'""'/Users/uwe/Development/pandas/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' develop --no-deps
    running develop
    running egg_info
    writing pandas.egg-info/PKG-INFO
    writing dependency_links to pandas.egg-info/dependency_links.txt
    writing entry points to pandas.egg-info/entry_points.txt
    writing requirements to pandas.egg-info/requires.txt
    writing top-level names to pandas.egg-info/top_level.txt
    reading manifest file 'pandas.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    no previously-included directories found matching 'doc/build'
    warning: no previously-included files matching '*.pyd' found anywhere in distribution
    warning: no previously-included files matching '*~' found anywhere in distribution
    warning: no previously-included files matching '.DS_Store' found anywhere in distribution
    warning: no previously-included files matching '#*' found anywhere in distribution
    writing manifest file 'pandas.egg-info/SOURCES.txt'
    running build_ext
    building 'pandas._libs.algos' extension
    x86_64-apple-darwin13.4.0-clang -fno-strict-aliasing -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O3 -Wall -Wstrict-prototypes -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O3 -pipe -fdebug-prefix-map=${SRC_DIR}=/usr/local/src/conda/${PKG_NAME}-${PKG_VERSION} -fdebug-prefix-map=/Users/uwe/miniconda3/envs/pandas-compile-test=/usr/local/src/conda-prefix -flto -Wl,-export_dynamic -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O3 -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -isystem /Users/uwe/miniconda3/envs/pandas-compile-test/include -D_FORTIFY_SOURCE=2 -mmacosx-version-min=10.9 -isystem /Users/uwe/miniconda3/envs/pandas-compile-test/include -DNPY_NO_DEPRECATED_API=0 -I./pandas/_libs -Ipandas/_libs/src/klib -I/Users/uwe/miniconda3/envs/pandas-compile-test/lib/python3.8/site-packages/numpy/core/include -I/Users/uwe/miniconda3/envs/pandas-compile-test/include/python3.8 -c pandas/_libs/algos.c -o build/temp.macosx-10.9-x86_64-3.8/pandas/_libs/algos.o -Werror
    clang-9: error: -Wl,-export_dynamic: 'linker' input unused [-Werror,-Wunused-command-line-argument]
    error: command 'x86_64-apple-darwin13.4.0-clang' failed with exit status 1
Cleaning up...
Removed build tracker: '/private/var/folders/3r/k0d_q3hs2mb2_jxfh_9hjyqr0000gn/T/pip-req-tracker-waeimcjv'
ERROR: Command errored out with exit status 1: /Users/uwe/miniconda3/envs/pandas-compile-test/bin/python3.8 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/Users/uwe/Development/pandas/setup.py'""'""'; __file__='""'""'/Users/uwe/Development/pandas/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' develop --no-deps Check the logs for full command output.
```

</details>


`export | grep FLAGS`:
 
```
CFLAGS='-march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -isystem /Users/uwe/miniconda3/envs/pandas-compile-test/include'
CONDA_BACKUP_LDFLAGS=' -Wl,-rpath,/Users/uwe/miniconda3/envs/pandas-compile-test/lib -L/Users/uwe/miniconda3/envs/pandas-compile-test/lib'
CPPFLAGS='-D_FORTIFY_SOURCE=2 -mmacosx-version-min=10.9 -isystem /Users/uwe/miniconda3/envs/pandas-compile-test/include'
CXXFLAGS='-march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -stdlib=libc++ -fvisibility-inlines-hidden -std=c++14 -fmessage-length=0 -isystem /Users/uwe/miniconda3/envs/pandas-compile-test/include'
DEBUG_CFLAGS='-march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -Og -g -Wall -Wextra -isystem /Users/uwe/miniconda3/envs/pandas-compile-test/include'
DEBUG_CXXFLAGS='-march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -stdlib=libc++ -fvisibility-inlines-hidden -std=c++14 -fmessage-length=0 -Og -g -Wall -Wextra -isystem /Users/uwe/miniconda3/envs/pandas-compile-test/include'
DEBUG_FFLAGS='-march=nocona -mtune=core2 -ftree-vectorize -fPIC -fstack-protector -O2 -pipe -isystem /Users/uwe/miniconda3/envs/pandas-compile-test/include -march=nocona -mtune=core2 -ftree-vectorize -fPIC -fstack-protector -O2 -pipe -Og -g -Wall -Wextra -fcheck=all -fbacktrace -fimplicit-none -fvar-tracking-assignments'
DEBUG_FORTRANFLAGS='-march=nocona -mtune=core2 -ftree-vectorize -fPIC -fstack-protector -O2 -pipe -isystem /Users/uwe/miniconda3/envs/pandas-compile-test/include -march=nocona -mtune=core2 -ftree-vectorize -fPIC -fstack-protector -O2 -pipe -Og -g -Wall -Wextra -fcheck=all -fbacktrace -fimplicit-none -fvar-tracking-assignments'
FFLAGS='-march=nocona -mtune=core2 -ftree-vectorize -fPIC -fstack-protector -O2 -pipe -isystem /Users/uwe/miniconda3/envs/pandas-compile-test/include'
FORTRANFLAGS='-march=nocona -mtune=core2 -ftree-vectorize -fPIC -fstack-protector -O2 -pipe -isystem /Users/uwe/miniconda3/envs/pandas-compile-test/include'
LDFLAGS=' -Wl,-rpath,/Users/uwe/miniconda3/envs/pandas-compile-test/lib -L/Users/uwe/miniconda3/envs/pandas-compile-test/lib'
LDFLAGS_LD='-pie -headerpad_max_install_names -dead_strip_dylibs -rpath /Users/uwe/miniconda3/envs/pandas-compile-test/lib -L/Users/uwe/miniconda3/envs/pandas-compile-test/lib'
XPC_FLAGS=0x0
```

Seems like this is not the first issue in response to https://github.com/pandas-dev/pandas/pull/32163, there is also e.g. https://github.com/pandas-dev/pandas/issues/33224"
688967461,36003,REGR: Column with datetime values too big to be converted to pd.Timestamp leads to assertion error in groupby,Khris777,closed,2020-08-31T07:03:11Z,2020-11-27T20:12:02Z,"- [X ] I have checked that this issue has not already been reported.

- [ X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
#### Code Sample, a copy-pastable example

Two different dates, one within the range of what `pd.Timestamp` can handle, the other outside of that range:
```
import pandas as pd
import datetime
df = pd.DataFrame({'A': ['X', 'Y'], 'B': [datetime.datetime(2005, 1, 1, 10, 30, 23, 540000),
                                          datetime.datetime(3005, 1, 1, 10, 30, 23, 540000)]})
print(df.groupby('A').B.max())
```

#### Problem description

`pd.Timestamp` can't deal with a too big date like the year 3005, so to represent such a date I need to use the `datetime.datetime` type. Before 1.1.1 (1.1.0?) this hasn't been an issue, but now this code throws an assertion error:

```
Traceback (most recent call last):

  File ""<ipython-input-38-8b8ec5e4e179>"", line 5, in <module>
    print(df.groupby('A').B.max())

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\groupby\groupby.py"", line 1558, in max
    numeric_only=numeric_only, min_count=min_count, alias=""max"", npfunc=np.max

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\groupby\groupby.py"", line 1015, in _agg_general
    result = self.aggregate(lambda x: npfunc(x, axis=self.axis))

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\groupby\generic.py"", line 261, in aggregate
    func, *args, engine=engine, engine_kwargs=engine_kwargs, **kwargs

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\groupby\groupby.py"", line 1083, in _python_agg_general
    result, counts = self.grouper.agg_series(obj, f)

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\groupby\ops.py"", line 644, in agg_series
    return self._aggregate_series_fast(obj, func)

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\groupby\ops.py"", line 669, in _aggregate_series_fast
    result, counts = grouper.get_result()

  File ""pandas\_libs\reduction.pyx"", line 256, in pandas._libs.reduction.SeriesGrouper.get_result

  File ""pandas\_libs\reduction.pyx"", line 74, in pandas._libs.reduction._BaseGrouper._apply_to_group

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\groupby\groupby.py"", line 1060, in <lambda>
    f = lambda x: func(x, *args, **kwargs)

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\groupby\groupby.py"", line 1015, in <lambda>
    result = self.aggregate(lambda x: npfunc(x, axis=self.axis))

  File ""<__array_function__ internals>"", line 6, in amax

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\numpy\core\fromnumeric.py"", line 2706, in amax
    keepdims=keepdims, initial=initial, where=where)

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\numpy\core\fromnumeric.py"", line 85, in _wrapreduction
    return reduction(axis=axis, out=out, **passkwargs)

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\generic.py"", line 11460, in stat_func
    func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\series.py"", line 4220, in _reduce
    delegate = self._values

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\series.py"", line 572, in _values
    return self._mgr.internal_values()

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\internals\managers.py"", line 1615, in internal_values
    return self._block.internal_values()

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\internals\blocks.py"", line 2019, in internal_values
    return self.array_values()

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\internals\blocks.py"", line 2022, in array_values
    return self._holder._simple_new(self.values)

  File ""C:\Users\My.Name\AppData\Local\Continuum\miniconda3\envs\main\lib\site-packages\pandas\core\arrays\datetimes.py"", line 290, in _simple_new
    assert values.dtype == ""i8""

AssertionError
```

From testing with mixing `pd.Timestamp` and `datetime.datetime` types I presume pandas is converting applicable dates (first line in the example) to `pd.Timestamp` while leaving the others as `datetime.datetime` leading to a mixed-type result column and the assertion error.

#### Expected Output

Since I'm explicitely operating with datatype `datetime.datetime` there should be no implicit conversion to `pd.Timestamp` if it's not assured that all values are within the range that `pd.Timestamp` allows.

#### Output of ``pd.show_versions()``

<details>

commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.7.8.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.19041
machine          : AMD64
processor        : Intel64 Family 6 Model 79 Stepping 1, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en
LOCALE           : None.None

pandas           : 1.1.1
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 50.0.0.post20200830
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.3
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : 0.8.0
fastparquet      : 0.4.1
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.19
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.51.1

</details>
"
751724966,38094,REGR: fix regression in groupby aggregation with out-of-bounds datetimes,jorisvandenbossche,closed,2020-11-26T16:46:02Z,2020-11-27T20:47:55Z,Closes #36003
752466769,38123,Backport PR #38094 on branch 1.1.x (REGR: fix regression in groupby aggregation with out-of-bounds datetimes),meeseeksmachine,closed,2020-11-27T20:48:34Z,2020-11-27T22:31:03Z,Backport PR #38094: REGR: fix regression in groupby aggregation with out-of-bounds datetimes
752467223,38124,Backport PR #38087 on branch 1.1.x (BLD: Only enable -Werror in the CI jobs),meeseeksmachine,closed,2020-11-27T20:50:06Z,2020-11-27T23:35:32Z,Backport PR #38087: BLD: Only enable -Werror in the CI jobs
752352064,38113,TST: rewrite convert_dtypes test to make it easier extendable,jorisvandenbossche,closed,2020-11-27T16:13:11Z,2020-11-28T14:31:45Z,"I have a branch adding `convert_floating` support for ``convert_dtypes``, and the current way they are written is not really maintainable when adding more keywords (eg the tuples of 4 would become tuples of 5, making it even harder to know what is what, and making the formatting less easy to follow).

In this PR, I just refactored how the test cases are defined and didn't change any actual content."
659222658,35322,DOC: add contributors,simonjayhawkins,closed,2020-07-17T12:39:05Z,2020-11-28T14:56:39Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
752704368,38133,test backportability of #36927,simonjayhawkins,closed,2020-11-28T16:02:09Z,2020-11-28T16:54:01Z,
354754141,22529,Crosstab Not Working with Duplicate Column Labels,fran6w,closed,2018-08-28T14:20:57Z,2020-11-28T17:29:04Z,"#### Code Sample, a copy-pastable example if possible

# 1. Example with 2 lambdas applied on same column
df = pd.DataFrame([[0, 1], [0, 1], [1, 0]], columns=['a', 'b'])
df
   a b
0 0 1
1 0 1
2 1 0

pd.crosstab(df['a'].apply(lambda x: x), df['a'].apply(lambda x: x+1))
a 1 2
a
1 2 0
2 0 1

# 2. Example by manually renaming a column
df = pd.DataFrame([[0, 1], [0, 1], [1, 0]], columns=['a', 'b'])
df
   a b
0 0 1
1 0 1
2 1 0

pd.crosstab(df['a'], df['b'])
b 0 1
a
0 0 2
1 1 0

s = df['b']
s.name = 'a'
pd.crosstab(df['a'], s)
a 0 1
a
0 1 0
1 0 2

In both cases, the output is not the one expected. The crosstab applies one Series to itself.

#### Problem description

I encountered the problem when using a crosstab on the same column with 2 different lambdas. The crosstab output a crosstab applying the second column to itself. It seems that crosstab is confused by the same name of the 2 series.

The problem is the same when the 2nd column is manually renamed with the name of the 1rst one, before applying the crosstab. See examples.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.4.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.22.0
pytest: 3.3.2
pip: 10.0.0
setuptools: 38.4.0
Cython: 0.27.3
numpy: 1.14.0
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: 1.6.6
patsy: 0.5.0
dateutil: 2.6.1
pytz: 2017.3
blosc: None
bottleneck: 1.2.1
tables: 3.4.2
numexpr: 2.6.4
feather: None
matplotlib: 2.1.2
openpyxl: 2.4.10
xlrd: 1.1.0
xlwt: 1.3.0
xlsxwriter: 1.0.2
lxml: 4.1.1
bs4: 4.6.0
html5lib: 1.0.1
sqlalchemy: 1.2.1
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
"
748171686,37997,BUG: crosstab with duplicate column or index labels,arw2019,closed,2020-11-22T05:12:56Z,2020-11-28T17:29:11Z,"- [x] closes #22529
- [x] tests added / passed
- [x] passes black pandas
- [x] passes git diff upstream/master -u -- ""*.py"" | flake8 --diff
- [x] whatsnew entry

Picking up from #28474 

cc @jreback in case this can go in in time for 1.2"

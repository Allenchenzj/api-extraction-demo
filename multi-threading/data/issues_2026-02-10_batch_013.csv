id,number,title,user,state,created_at,updated_at,body
718376470,37011,BUG: Incorrect data when modifying a dataframe column with += ,alexifm,closed,2020-10-09T19:32:16Z,2020-10-09T21:33:01Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

Get the column as a series and modify inplace with `+=`:
```python
In [1]: import pandas as pd
    ...: df = pd.DataFrame({""col"": [1 ,2 ,3, 4]})
    ...: s = df[""col""]
    ...: s += 1
    ...: print(s)
    ...: print(df[""col""])
    ...: print(df[[""col""]])
0    2
1    3
2    4
3    5
Name: col, dtype: int64
0    2
1    3
2    4
3    5
Name: col, dtype: int64
   col
0    1
1    2
2    3
3    4
```

Modify the column directly with `+=`:
```python
In [2]: import pandas as pd
    ...: df = pd.DataFrame({""col"": [1 ,2 ,3, 4]})
    ...: df[""col""] += 1
    ...: print(df[""col""])
    ...: print(df[[""col""]])
0    2
1    3
2    4
3    5
Name: col, dtype: int64
   col
0    2
1    3
2    4
3    5
```

#### Problem description

The data in the column should be consistent whether accessed as `df[""col""]` or `df[[""col""]]`.

#### Expected Output

Either `+=` modifies the values in the dataframe column or it doesn't but `df[""col""]` and `df[[""col""]]` should show the same values.

#### Output of ``pd.show_versions()``

```python
In [13]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.1.0
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : 0.10.1
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.14.0
pandas_datareader: None
bs4              : None
bottleneck       : 1.3.2
fsspec           : 0.8.3
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.4.1
sqlalchemy       : 1.3.19
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
```

For master branch:
```python
In [1]: import pandas as pd; pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : 9787744272c13a1dcbbcdfc7daaae8cc73ac78a3
python           : 3.8.5.final.0
...

pandas           : 1.2.0.dev0+684.g978774427
...
```

</details>
"
707088028,36567,REF: Remove rolling window fixed algorithms,mroeschke,closed,2020-09-23T06:33:13Z,2020-10-09T23:34:10Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Pros:
* Less code to maintain + makes internals simplier
* Fixed a bug: https://github.com/pandas-dev/pandas/pull/36567/files#diff-7656adc204bcddba0534588cabdab62eR770

Cons:
* A performance hit

```
       before           after         ratio
     [d9722efe]       [198ab9e8]
     <clean/rolling_aggregations^2>       <clean/rolling_aggregations>
+     1.27±0.02ms         4.05±1ms     3.19  rolling.Quantile.time_quantile('Series', 10, 'float', 1, 'linear')
+     1.27±0.01ms       3.67±0.7ms     2.88  rolling.Quantile.time_quantile('Series', 10, 'float', 1, 'nearest')
+     1.27±0.01ms       3.22±0.3ms     2.54  rolling.Quantile.time_quantile('Series', 10, 'float', 1, 'higher')
+     1.27±0.01ms      2.99±0.08ms     2.36  rolling.Quantile.time_quantile('Series', 10, 'float', 1, 'midpoint')
+        1.29±0ms      2.92±0.01ms     2.26  rolling.Quantile.time_quantile('Series', 10, 'float', 0, 'nearest')
+        1.29±0ms      2.91±0.01ms     2.26  rolling.Quantile.time_quantile('Series', 10, 'float', 0, 'midpoint')
+     1.29±0.08ms      2.91±0.01ms     2.25  rolling.Methods.time_rolling('Series', 10, 'float', 'max')
+     1.29±0.01ms         2.91±0ms     2.25  rolling.Quantile.time_quantile('Series', 10, 'float', 0, 'higher')
+     1.29±0.01ms      2.90±0.02ms     2.24  rolling.Quantile.time_quantile('Series', 10, 'float', 0, 'linear')
+     1.30±0.02ms      2.91±0.04ms     2.24  rolling.Quantile.time_quantile('Series', 10, 'float', 0, 'lower')
+     1.30±0.03ms      2.90±0.01ms     2.23  rolling.Quantile.time_quantile('Series', 10, 'float', 1, 'lower')
+     1.32±0.08ms      2.93±0.02ms     2.22  rolling.Methods.time_rolling('Series', 10, 'float', 'min')
+     1.33±0.03ms         2.95±0ms     2.21  rolling.Methods.time_rolling('Series', 10, 'int', 'max')
+     1.44±0.01ms      3.11±0.03ms     2.16  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 1, 'lower')
+     1.37±0.05ms      2.96±0.01ms     2.16  rolling.Methods.time_rolling('Series', 10, 'int', 'min')
+     1.44±0.01ms      3.10±0.02ms     2.15  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 1, 'midpoint')
+     1.44±0.01ms      3.07±0.01ms     2.14  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 1, 'higher')
+        1.44±0ms      3.08±0.01ms     2.14  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 1, 'nearest')
+     1.45±0.01ms      3.07±0.01ms     2.13  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'max')
+     1.46±0.01ms      3.07±0.01ms     2.10  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 1, 'linear')
+        1.47±0ms      3.09±0.02ms     2.10  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0, 'linear')
+     1.48±0.02ms      3.10±0.02ms     2.10  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0, 'higher')
+        1.47±0ms         3.08±0ms     2.09  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'min')
+     1.47±0.01ms      3.07±0.01ms     2.09  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0, 'midpoint')
+      1.48±0.2ms      3.10±0.02ms     2.09  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0, 'nearest')
+     1.47±0.04ms      3.05±0.02ms     2.08  rolling.Quantile.time_quantile('Series', 1000, 'float', 1, 'linear')
+     1.52±0.01ms      3.15±0.02ms     2.07  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'max')
+     1.54±0.01ms      3.17±0.03ms     2.06  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'min')
+        1.49±0ms      3.05±0.06ms     2.04  rolling.Quantile.time_quantile('Series', 1000, 'float', 1, 'nearest')
+        1.49±0ms      3.03±0.03ms     2.03  rolling.Quantile.time_quantile('Series', 1000, 'float', 1, 'lower')
+        1.49±0ms      2.98±0.03ms     1.99  rolling.Quantile.time_quantile('Series', 1000, 'float', 1, 'midpoint')
+     1.40±0.01ms      2.75±0.01ms     1.97  rolling.ExpandingMethods.time_expanding('Series', 'float', 'max')
+        1.49±0ms      2.93±0.02ms     1.96  rolling.Quantile.time_quantile('Series', 1000, 'float', 1, 'higher')
+     1.44±0.01ms      2.81±0.02ms     1.96  rolling.ExpandingMethods.time_expanding('Series', 'int', 'max')
+      1.59±0.2ms      3.11±0.03ms     1.95  rolling.Quantile.time_quantile('DataFrame', 10, 'float', 0, 'lower')
+         930±3μs      1.80±0.01ms     1.94  rolling.Quantile.time_quantile('Series', 10, 'int', 0, 'higher')
+        1.52±0ms      2.94±0.02ms     1.93  rolling.Quantile.time_quantile('Series', 1000, 'float', 0, 'linear')
+      1.51±0.2ms      2.92±0.01ms     1.93  rolling.Methods.time_rolling('Series', 1000, 'float', 'max')
+         933±4μs      1.80±0.01ms     1.93  rolling.Quantile.time_quantile('Series', 10, 'int', 0, 'nearest')
+         931±4μs      1.79±0.01ms     1.93  rolling.Quantile.time_quantile('Series', 10, 'int', 1, 'nearest')
+     1.69±0.01ms      3.26±0.01ms     1.93  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 1, 'linear')
+         933±4μs      1.80±0.01ms     1.93  rolling.Quantile.time_quantile('Series', 10, 'int', 0, 'midpoint')
+      1.52±0.2ms      2.93±0.01ms     1.93  rolling.Methods.time_rolling('Series', 1000, 'float', 'min')
+        937±10μs      1.80±0.01ms     1.92  rolling.Quantile.time_quantile('Series', 10, 'int', 0, 'lower')
+        1.56±0ms      2.99±0.05ms     1.92  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'max')
+        940±30μs      1.80±0.01ms     1.91  rolling.Quantile.time_quantile('Series', 10, 'int', 1, 'midpoint')
+     1.56±0.02ms      2.98±0.01ms     1.91  rolling.Methods.time_rolling('Series', 1000, 'int', 'max')
+         935±3μs      1.79±0.01ms     1.91  rolling.Quantile.time_quantile('Series', 10, 'int', 1, 'higher')
+         934±7μs      1.78±0.01ms     1.91  rolling.Quantile.time_quantile('Series', 10, 'int', 0, 'linear')
+         931±3μs      1.78±0.01ms     1.91  rolling.Quantile.time_quantile('Series', 10, 'int', 1, 'lower')
+     1.45±0.01ms         2.76±0ms     1.91  rolling.ExpandingMethods.time_expanding('Series', 'float', 'min')
+        1.63±0ms      3.11±0.01ms     1.90  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 0, 'nearest')
+         937±4μs      1.78±0.01ms     1.90  rolling.Quantile.time_quantile('Series', 10, 'int', 1, 'linear')
+         961±2μs      1.82±0.04ms     1.90  rolling.Quantile.time_quantile('Series', 1000, 'int', 0, 'midpoint')
+     1.49±0.02ms      2.82±0.01ms     1.89  rolling.ExpandingMethods.time_expanding('Series', 'int', 'min')
+         964±9μs      1.82±0.02ms     1.88  rolling.Quantile.time_quantile('Series', 1000, 'int', 0, 'nearest')
+         963±7μs      1.81±0.05ms     1.88  rolling.Quantile.time_quantile('Series', 1000, 'int', 0, 'linear')
+         962±8μs      1.81±0.01ms     1.88  rolling.Quantile.time_quantile('Series', 1000, 'int', 0, 'higher')
+         967±3μs      1.81±0.01ms     1.88  rolling.Quantile.time_quantile('Series', 1000, 'int', 0, 'lower')
+         966±3μs      1.81±0.02ms     1.87  rolling.Quantile.time_quantile('Series', 1000, 'int', 1, 'linear')
+         964±1μs      1.80±0.02ms     1.87  rolling.Quantile.time_quantile('Series', 1000, 'int', 1, 'nearest')
+     1.66±0.03ms      3.10±0.01ms     1.87  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 1, 'nearest')
+         964±4μs         1.80±0ms     1.87  rolling.Quantile.time_quantile('Series', 1000, 'int', 1, 'higher')
+        1.12±0ms      2.08±0.01ms     1.87  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 1, 'nearest')
+     1.67±0.01ms      3.10±0.01ms     1.86  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 1, 'midpoint')
+     1.63±0.02ms      3.02±0.03ms     1.86  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'max')
+     1.68±0.01ms      3.11±0.01ms     1.86  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 1, 'lower')
+         969±3μs      1.80±0.02ms     1.85  rolling.Quantile.time_quantile('Series', 1000, 'int', 1, 'midpoint')
+     1.68±0.01ms      3.11±0.02ms     1.85  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 1, 'higher')
+     1.61±0.09ms      2.97±0.01ms     1.85  rolling.Methods.time_rolling('Series', 1000, 'int', 'min')
+         970±7μs      1.79±0.01ms     1.84  rolling.Quantile.time_quantile('Series', 1000, 'int', 1, 'lower')
+     1.72±0.01ms       3.16±0.1ms     1.84  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 0, 'linear')
+     1.71±0.02ms      3.16±0.07ms     1.84  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'max')
+     1.70±0.08ms      3.13±0.02ms     1.84  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'min')
+     1.70±0.07ms      3.11±0.01ms     1.83  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 0, 'lower')
+     1.71±0.01ms      3.13±0.02ms     1.83  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 0, 'higher')
+     1.65±0.01ms      3.00±0.02ms     1.82  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'min')
+     1.15±0.01ms      2.09±0.01ms     1.82  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 1, 'linear')
+     1.71±0.02ms      3.11±0.01ms     1.82  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'max')
+        1.71±0ms      3.11±0.01ms     1.82  rolling.Quantile.time_quantile('DataFrame', 1000, 'float', 0, 'midpoint')
+     1.62±0.01ms      2.94±0.03ms     1.82  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'min')
+        1.12±0ms      2.02±0.05ms     1.80  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 1, 'higher')
+     1.76±0.08ms      3.15±0.01ms     1.80  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'min')
+     1.12±0.01ms      1.99±0.03ms     1.78  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0, 'nearest')
+     1.12±0.01ms      1.99±0.02ms     1.78  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 1, 'lower')
+        1.14±0ms      2.03±0.03ms     1.78  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0, 'higher')
+        1.12±0ms      1.99±0.01ms     1.78  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0, 'higher')
+        1.15±0ms      2.03±0.05ms     1.77  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0, 'linear')
+     1.13±0.01ms      2.00±0.05ms     1.77  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0, 'nearest')
+        1.12±0ms      1.98±0.04ms     1.77  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0, 'lower')
+        1.12±0ms      1.97±0.01ms     1.76  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 1, 'midpoint')
+     1.13±0.01ms      1.99±0.01ms     1.76  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0, 'midpoint')
+        1.12±0ms      1.97±0.01ms     1.76  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 1, 'linear')
+     1.14±0.01ms      1.98±0.01ms     1.74  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 1, 'midpoint')
+     1.14±0.02ms         1.97±0ms     1.74  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0, 'lower')
+     1.14±0.01ms         1.97±0ms     1.73  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 1, 'higher')
+     1.14±0.01ms      1.97±0.01ms     1.73  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 0, 'midpoint')
+        1.14±0ms      1.97±0.01ms     1.72  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 1, 'nearest')
+     1.14±0.01ms      1.97±0.01ms     1.72  rolling.Quantile.time_quantile('DataFrame', 1000, 'int', 1, 'lower')
+     1.46±0.07ms      2.48±0.01ms     1.70  rolling.Methods.time_rolling('Series', 10, 'float', 'std')
+     1.21±0.09ms      2.00±0.02ms     1.66  rolling.Quantile.time_quantile('DataFrame', 10, 'int', 0, 'linear')
+     1.54±0.03ms      2.54±0.01ms     1.65  rolling.Methods.time_rolling('Series', 1000, 'int', 'std')
+     1.62±0.04ms      2.66±0.01ms     1.65  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'std')
+     1.68±0.01ms      2.75±0.02ms     1.64  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'std')
+      1.56±0.1ms      2.53±0.01ms     1.63  rolling.Methods.time_rolling('Series', 10, 'int', 'std')
+     1.66±0.03ms      2.67±0.01ms     1.61  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'std')
+     1.71±0.03ms      2.73±0.01ms     1.59  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'std')
+        913±40μs         1.42±0ms     1.56  rolling.Methods.time_rolling('Series', 10, 'float', 'mean')
+      1.62±0.2ms      2.49±0.01ms     1.54  rolling.Methods.time_rolling('Series', 1000, 'float', 'std')
+       933±100μs      1.43±0.01ms     1.53  rolling.Methods.time_rolling('Series', 1000, 'float', 'mean')
+        980±30μs      1.48±0.01ms     1.51  rolling.Methods.time_rolling('Series', 10, 'int', 'mean')
+     1.07±0.01ms      1.60±0.01ms     1.50  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'mean')
+        985±60μs      1.47±0.01ms     1.50  rolling.Methods.time_rolling('Series', 1000, 'int', 'mean')
+        1.21±0ms      1.81±0.01ms     1.49  rolling.ExpandingMethods.time_expanding('Series', 'float', 'kurt')
+        1.43±0ms      2.11±0.01ms     1.47  rolling.Methods.time_rolling('Series', 1000, 'int', 'kurt')
+        1.14±0ms      1.68±0.01ms     1.47  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'mean')
+        1.26±0ms         1.86±0ms     1.47  rolling.ExpandingMethods.time_expanding('Series', 'int', 'kurt')
+     1.40±0.08ms      2.06±0.01ms     1.47  rolling.Methods.time_rolling('Series', 10, 'float', 'kurt')
+     1.14±0.01ms         1.67±0ms     1.46  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'mean')
+     1.12±0.01ms      1.63±0.01ms     1.45  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'mean')
+     1.55±0.01ms      2.24±0.01ms     1.44  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'kurt')
+         846±4μs         1.22±0ms     1.44  rolling.ExpandingMethods.time_expanding('Series', 'float', 'mean')
+     1.38±0.01ms      1.98±0.01ms     1.43  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'kurt')
+        1.62±0ms      2.30±0.01ms     1.42  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'kurt')
+     1.27±0.02ms      1.80±0.02ms     1.41  rolling.Methods.time_rolling('Series', 10, 'float', 'skew')
+     1.57±0.02ms      2.22±0.06ms     1.41  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'kurt')
+     1.32±0.08ms      1.86±0.01ms     1.41  rolling.Methods.time_rolling('Series', 10, 'int', 'skew')
+     1.44±0.01ms      2.03±0.01ms     1.41  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'kurt')
+         895±3μs         1.26±0ms     1.40  rolling.ExpandingMethods.time_expanding('Series', 'int', 'mean')
+     1.32±0.06ms         1.84±0ms     1.40  rolling.Methods.time_rolling('Series', 1000, 'int', 'skew')
+     1.64±0.04ms      2.29±0.01ms     1.40  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'kurt')
+     1.42±0.01ms         1.97±0ms     1.39  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'skew')
+        900±50μs      1.25±0.02ms     1.38  rolling.Methods.time_rolling('Series', 10, 'float', 'sum')
+     2.12±0.07ms      2.92±0.01ms     1.38  rolling.Quantile.time_quantile('Series', 1000, 'float', 0, 'lower')
+     1.49±0.01ms         2.04±0ms     1.37  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'skew')
+     1.45±0.04ms      1.98±0.03ms     1.37  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'skew')
+     1.51±0.02ms      2.05±0.01ms     1.36  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'skew')
+      1.51±0.2ms      2.05±0.01ms     1.36  rolling.Methods.time_rolling('Series', 1000, 'float', 'kurt')
+     1.02±0.01ms      1.38±0.01ms     1.36  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'mean')
+     1.08±0.01ms      1.46±0.02ms     1.36  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'mean')
+        962±60μs         1.28±0ms     1.34  rolling.Methods.time_rolling('Series', 10, 'int', 'sum')
+         841±5μs      1.12±0.01ms     1.33  rolling.ExpandingMethods.time_expanding('Series', 'float', 'sum')
+     1.26±0.01ms      1.66±0.02ms     1.33  rolling.ExpandingMethods.time_expanding('Series', 'int', 'skew')
+        969±80μs      1.28±0.01ms     1.32  rolling.Methods.time_rolling('Series', 1000, 'int', 'sum')
+     1.20±0.02ms      1.58±0.01ms     1.31  rolling.ExpandingMethods.time_expanding('Series', 'float', 'skew')
+     1.14±0.01ms      1.48±0.01ms     1.30  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'sum')
+         899±5μs         1.17±0ms     1.30  rolling.ExpandingMethods.time_expanding('Series', 'int', 'sum')
+     1.08±0.04ms      1.40±0.01ms     1.30  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'sum')
+     1.09±0.04ms      1.41±0.01ms     1.29  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'sum')
+        1.36±0ms      1.75±0.01ms     1.29  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'skew')
+     1.15±0.06ms      1.47±0.01ms     1.28  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'sum')
+        1.01±0ms         1.28±0ms     1.27  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'sum')
+        1.43±0ms      1.81±0.01ms     1.27  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'skew')
+     1.22±0.03ms      1.54±0.01ms     1.26  rolling.Methods.time_rolling('Series', 10, 'float', 'count')
+        1.10±0ms      1.38±0.01ms     1.26  rolling.ExpandingMethods.time_expanding('Series', 'int', 'count')
+     1.07±0.01ms         1.35±0ms     1.26  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'sum')
+        1.36±0ms      1.71±0.02ms     1.26  rolling.Methods.time_rolling('DataFrame', 10, 'int', 'count')
+     1.40±0.01ms      1.74±0.01ms     1.24  rolling.Methods.time_rolling('DataFrame', 10, 'float', 'count')
+     1.14±0.01ms         1.40±0ms     1.23  rolling.ExpandingMethods.time_expanding('Series', 'float', 'count')
+      1.22±0.1ms         1.49±0ms     1.22  rolling.Methods.time_rolling('Series', 10, 'int', 'count')
+     1.40±0.04ms      1.70±0.01ms     1.21  rolling.Methods.time_rolling('DataFrame', 1000, 'int', 'count')
+     1.43±0.09ms      1.73±0.01ms     1.21  rolling.Methods.time_rolling('DataFrame', 1000, 'float', 'count')
+        1.29±0ms      1.56±0.01ms     1.21  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'count')
+     1.33±0.01ms      1.60±0.01ms     1.20  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'count')
+        1.38±0ms      1.66±0.01ms     1.20  rolling.ExpandingMethods.time_expanding('Series', 'float', 'std')
+     1.44±0.01ms      1.72±0.01ms     1.19  rolling.ExpandingMethods.time_expanding('Series', 'int', 'std')
+     1.56±0.01ms         1.84±0ms     1.18  rolling.ExpandingMethods.time_expanding('DataFrame', 'float', 'std')
+     1.62±0.01ms      1.89±0.01ms     1.17  rolling.ExpandingMethods.time_expanding('DataFrame', 'int', 'std')

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE DECREASED.
```"
718463532,37016,ERR: Better error message for MultiIndex.astype,dsaxton,closed,2020-10-09T22:48:18Z,2020-10-10T02:36:19Z,"Error message could be nicer I think
```python
TypeError: Setting <class 'pandas.core.indexes.multi.MultiIndex'> dtype to anything other than object is not supported
```"
718385548,37012,DOC: pandas.Index.astype says it raises ValueError instead of TypeError,coelhudo,closed,2020-10-09T19:51:01Z,2020-10-10T07:40:52Z,"#### Location of the documentation

The documentation is located at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.astype.html 

#### Documentation problem

The documentation says "" When conversion is impossible, a ValueError exception is raised."" However, checking the code it can be seen that a TypeError is raised instead of ValueError.

#### Suggested fix for documentation

Just change ValueError to TypeError in the docstring.
"
718408214,37013,DOC: pandas.Index.astype says it raises ValueError instead of TypeError,coelhudo,closed,2020-10-09T20:36:34Z,2020-10-10T07:41:00Z,"- [X] closes #37012 
- [X] passes pytest
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
241287610,16854,Is it possible to have sub-headers in Pandas?,sashml,closed,2017-07-07T14:49:45Z,2020-10-10T09:20:01Z,"Can someone please review and give answer if that is feature or not?
https://stackoverflow.com/questions/44973087/is-it-possible-to-create-dataframe-with-sub-headers-in-pandas

"
715374467,36911,BUG: RollingGroupby not respecting sort=False,mroeschke,closed,2020-10-06T06:14:54Z,2020-10-10T09:54:00Z,"- [x] closes #36889
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
718598796,37028,BUG: DateOffset replaces month with given value instead of adding it to the current month.,erfannariman,closed,2020-10-10T11:26:55Z,2020-10-10T11:48:38Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
from pandas.tseries.offsets import DateOffset

print(pd.__version__)

df = pd.DataFrame(pd.date_range('2020-10-01', periods=3, freq='D'))
print(df[0] + DateOffset(month=1))

1.2.0.dev0+690.g3a043f2d2
0   2020-01-01
1   2020-01-02
2   2020-01-03
Name: 0, dtype: datetime64[ns]
```

#### Problem description

DateOffset replaces the month instead of adding to the current month
#### Expected Output
```python
           0
0 2020-11-01
1 2020-11-02
2 2020-11-03

"
704509194,36453,CLN Upgrade pandas/core syntax,MarcoGorelli,closed,2020-09-18T16:32:11Z,2020-10-10T14:14:44Z,xref  #36450
687373624,35929,CLN remove unnecessary trailing commas,MarcoGorelli,closed,2020-08-27T16:20:07Z,2020-10-10T14:14:44Z,xref #35927 
561898576,31796,ENH: add Series.info,MarcoGorelli,closed,2020-02-07T22:12:38Z,2020-10-10T14:14:44Z,"- [x] initial draft to close #5167
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

screenshot of the example
![image](https://user-images.githubusercontent.com/33491632/74069703-f1754b80-49f6-11ea-8c9e-8ec4e16166f1.png)

------

Note to self: this change from `ba72b59b44f15076877e784e16cbc66aa03e0adc` breaks the tests

```diff
-    # groupby dtype.name to collect e.g. Categorical columns
-    counts = data.dtypes.value_counts().groupby(lambda x: x.name).sum()
+    counts = data._data.get_dtype_counts()
```

----

### Updated

Currently, looks like this:

![image](https://user-images.githubusercontent.com/33491632/86509914-7a948100-bde3-11ea-84fb-907df7a2a40c.png)
"
638136703,34743,CLN: make Info and DataFrameInfo subclasses,MarcoGorelli,closed,2020-06-13T10:19:10Z,2020-10-10T14:14:45Z,"precursor to #31796

Makes Info class and DataFrameInfo subclass so there's no need for all the
```python
if isinstance(data, ABCDataFrame)
```"
553725253,31216,(wip) BUG: replacing one column's values was changing other columns' dtypes,MarcoGorelli,closed,2020-01-22T18:46:30Z,2020-10-10T14:14:46Z,"…s' dtypes

- [x] closes #30512 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

"
553122516,31185,DEPR: _BaseOffset's parameter `n` should be prevented from being 0,MarcoGorelli,closed,2020-01-21T20:52:21Z,2020-10-10T14:14:49Z,"- [ ] closes #31184 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
561893508,31795,ENH: add from_dummies,MarcoGorelli,closed,2020-02-07T21:59:08Z,2020-10-10T14:14:50Z,"Just a first draft

Will close #8745 

Screenshot of examples

![image](https://user-images.githubusercontent.com/33491632/74546261-3989f600-4f42-11ea-98e0-14c8ae4658ff.png)
"
572770278,32332,BUG: Rolling groupby should not maintain the by column in the resulting DataFrame,MarcoGorelli,closed,2020-02-28T13:55:34Z,2020-10-10T14:14:53Z,"- [x] closes #32262
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Does this seem like the correct idea? If so, I'll expand on tests / write whatsnew / think about a cleaner patch

EDIT
----
lots of unwanted changes here, just seeing what tests fail

EDIT
----
need to check the types make sense and see what's the proper way to write
```
kwargs[""exclusions""] = self.exclusions
```"
578695218,32583,CLN: remove unreachable code in pandas/core/groupby/generic.py::DataFrameGroupBy::_transform_general,MarcoGorelli,closed,2020-03-10T16:09:32Z,2020-10-10T14:14:54Z,"Is this part of the code
```
        else:
            # Handle cases like BinGrouper
            return self._concat_objects(keys, values, not_indexed_same=not_indexed_same)
```
necessary? The condition before it is
```
elif self.grouper.groupings is not None
```
. However, even in the case of `BinGrouper`, I don't believe `BinGrouper.groupings` can be `None`:
```
>>> from pandas.core.groupby.ops import BinGrouper
>>> BinGrouper([], []).groupings                                                                                                                                                                   
[Grouping(None)]
```

 and so this code is unreachable. It's not covered by the tests, at least - https://codecov.io/gh/pandas-dev/pandas/src/master/pandas/core/groupby/generic.py#L1354

If it can be removed, I've taken it out and then shifted the code above it back by one tab (because we no longer need the guard)"
577203100,32506,(wip) BUG: groupby with sort=False create buggy multiindex,MarcoGorelli,closed,2020-03-06T22:11:55Z,2020-10-10T14:14:56Z,"- [ ] closes #32259 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Still messy, just running this through the test suite as my laptop's not great (but please let me know if such behaviour is unwelcome and I won't do it again)

pandas/tests/groupby/test_timegrouper.py
pandas/tests/groupby/test_grouping.py
pandas/tests/groupby/test_categorical.py
pandas/tests/groupby/test_groupby.py
pandas/tests/test_multilevel.py"
572409039,32315,REGR: SeriesGroupby transform unique,MarcoGorelli,closed,2020-02-27T22:46:27Z,2020-10-10T14:14:56Z,"- [x] closes #31849 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
570863211,32252,(wip) BUG: itertuples was retuning empty list if frame had no columns,MarcoGorelli,closed,2020-02-25T22:01:03Z,2020-10-10T14:14:57Z,"(currently not right)

- [ ] closes #25408 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
567069090,32079,BUG: fix in categorical merges,MarcoGorelli,closed,2020-02-18T18:12:14Z,2020-10-10T14:14:58Z,"test adapted (at this point kind of loosely) from #28296

- [x] closes #28189
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
565732283,32012,(wip)BUG: make sure combine doesn't alter dtype for nullable dtypes,MarcoGorelli,closed,2020-02-15T10:59:40Z,2020-10-10T14:15:00Z,"- [ ] closes #31899 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Haven't added new tests, but have changed the expected return dtype in existing ones

EDIT
----
This is the wrong fix - for, say
```
s1.combine(s2, lambda x, y: x<y)
```
the dtype _would_ be expected to change (in this case to bool)"
715226338,36903,"TYP: io.json._json, util._decorators",jbrockmendel,closed,2020-10-05T23:22:21Z,2020-10-10T14:42:36Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
688359413,35966,BUG: instantiation using a dict with a period scalar,justinessert,closed,2020-08-28T21:15:57Z,2020-10-10T15:07:02Z,"- [x] closes #35965
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Fixing bug discussed in [issue 35965](https://github.com/pandas-dev/pandas/issues/35965) where `pd.DataFrame({'a': pd.Period('2020-01')})` created `a` as an object column instead of a `period[m]` column.

Changing the functionality of `infer_dtype_from_scalar` isn't necessarily required here, but the fact that `infer_dtype_from_scalar` would return the `period.ordinal` value seems inconsistent with the behavior for other dtypes in this function. Additionally, that functionality was only used in a single place within the code (`interval.py`), which I fixed accordingly."
718517881,37020,ENH: allow NaN value in str.place,GYHHAHA,closed,2020-10-10T03:14:27Z,2020-10-10T15:08:00Z,"```python
>>>pd.Series(['A',pd.NA],dtype='string').str.replace(pd.NA,0)
TypeError: repl must be a string or callable
>>>pd.Series(['A',pd.NA],dtype='string').str.replace(0,pd.NA)
TypeError: repl must be a string or callable
```

Now both `str.replace(pd.NA,0)` and `str.replace(0,pd.NA)` raise.

I think it will be better to allow these replacements since `replace` method can handle this."
693816164,36132,API: reimplement FixedWindowIndexer.get_window_bounds to fix groupby bug,justinessert,closed,2020-09-04T23:37:20Z,2020-10-10T15:55:41Z,"- [x] closes #36040
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Creating PR to solve [Issue 36040](https://github.com/pandas-dev/pandas/issues/36040).

The old `FixedWindowIndexer.get_window_bounds` function provided unintuitive bounds that went beyond the length of the original array. Additionally, to do a centered rolling operation, it required NaN values to be appended to the end of original array to enable some roundabout way of achieving the centering.

I replaced it with one that seems much simpler and actually creates ""fixed"" size windows (at least prior to clipping the ends), which the previous function did not.

That being said, I know this PR fails some tests, I'm would appreciate some advice on how best to proceed!"
718573223,37026,TST: insert 'match' to bare pytest raises in pandas/tests/test_flags.py,krajatcl,closed,2020-10-10T09:02:17Z,2020-10-10T16:03:01Z,"- [ ] ref https://github.com/pandas-dev/pandas/issues/30999
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
715949188,36924,TYP: check_untyped_defs core.indexes.base,simonjayhawkins,closed,2020-10-06T19:05:51Z,2020-10-10T16:05:47Z,"pandas\core\indexes\base.py:975: error: Argument 1 to ""format_object_attrs"" has incompatible type ""Index""; expected ""Sequence[Any]""  [arg-type]
pandas\core\indexes\base.py:1612: error: ""Index"" has no attribute ""levels""; maybe ""nlevels""?  [attr-defined]
pandas\core\indexes\base.py:1613: error: ""Index"" has no attribute ""codes""  [attr-defined]
pandas\core\indexes\base.py:3763: error: ""Index"" has no attribute ""levels""; maybe ""nlevels""?  [attr-defined]
pandas\core\indexes\base.py:3779: error: ""Index"" has no attribute ""codes""  [attr-defined]
pandas\core\indexes\base.py:3787: error: ""Index"" has no attribute ""codes""  [attr-defined]
pandas\core\indexes\base.py:3790: error: ""Index"" has no attribute ""codes""  [attr-defined]
pandas\core\indexes\base.py:3793: error: ""Index"" has no attribute ""levels""; maybe ""nlevels""?  [attr-defined]
pandas\core\indexes\base.py:3837: error: ""Index"" has no attribute ""codes""  [attr-defined]
pandas\core\indexes\base.py:3840: error: ""Index"" has no attribute ""codes""  [attr-defined]
pandas\core\indexes\base.py:4807: error: Too many arguments for ""get_indexer_non_unique"" of ""Index""  [call-arg]
pandas\core\indexes\base.py:5403: error: Cannot assign to a method  [assignment]
pandas\core\indexes\base.py:5404: error: Cannot assign to a method  [assignment]
pandas\core\indexes\base.py:5405: error: Unsupported left operand type for < (""Type[Index]"")  [operator]
pandas\core\indexes\base.py:5406: error: Unsupported left operand type for > (""Type[Index]"")  [operator]
pandas\core\indexes\base.py:5407: error: Unsupported left operand type for <= (""Type[Index]"")  [operator]
pandas\core\indexes\base.py:5408: error: Unsupported left operand type for >= (""Type[Index]"")  [operator]
pandas\core\indexes\base.py:5415: error: Unsupported left operand type for + (""Type[Index]"")  [operator]
pandas\core\indexes\base.py:5416: error: ""Type[Index]"" has no attribute ""__radd__""  [attr-defined]
pandas\core\indexes\base.py:5417: error: Unsupported left operand type for - (""Type[Index]"")  [operator]
pandas\core\indexes\base.py:5418: error: ""Type[Index]"" has no attribute ""__rsub__""  [attr-defined]
pandas\core\indexes\base.py:5419: error: ""Type[Index]"" has no attribute ""__rpow__""  [attr-defined]
pandas\core\indexes\base.py:5420: error: Unsupported left operand type for ** (""Type[Index]"")  [operator]
pandas\core\indexes\base.py:5422: error: Unsupported left operand type for / (""Type[Index]"")  [operator]
pandas\core\indexes\base.py:5423: error: ""Type[Index]"" has no attribute ""__rtruediv__""  [attr-defined]
pandas\core\indexes\base.py:5425: error: Unsupported left operand type for % (""Type[Index]"")  [operator]
pandas\core\indexes\base.py:5426: error: ""Type[Index]"" has no attribute ""__rmod__""  [attr-defined]
pandas\core\indexes\base.py:5427: error: Unsupported left operand type for // (""Type[Index]"")  [operator]
pandas\core\indexes\base.py:5428: error: ""Type[Index]"" has no attribute ""__rfloordiv__""  [attr-defined]
pandas\core\indexes\base.py:5429: error: Unsupported left operand type for divmod (""Type[Index]"")  [operator]
pandas\core\indexes\base.py:5430: error: ""Type[Index]"" has no attribute ""__rdivmod__""  [attr-defined]
pandas\core\indexes\base.py:5431: error: Unsupported left operand type for * (""Type[Index]"")  [operator]
pandas\core\indexes\base.py:5432: error: ""Type[Index]"" has no attribute ""__rmul__""  [attr-defined]
pandas\core\indexes\base.py:5449: error: Unsupported operand type for unary - (""Type[Index]"")  [operator]
pandas\core\indexes\base.py:5450: error: Unsupported operand type for unary + (""Type[Index]"")  [operator]
pandas\core\indexes\base.py:5451: error: ""Type[Index]"" has no attribute ""__abs__""  [attr-defined]
pandas\core\indexes\base.py:5452: error: ""Type[Index]"" has no attribute ""__inv__""  [attr-defined]
pandas\core\indexes\base.py:5564: error: ""Type[Index]"" has no attribute ""all""  [attr-defined]
pandas\core\indexes\base.py:5567: error: ""Type[Index]"" has no attribute ""any""  [attr-defined]
pandas\core\indexes\base.py:5576: error: ""Type[Index]"" has no attribute ""all""  [attr-defined]
pandas\core\indexes\base.py:5577: error: ""Type[Index]"" has no attribute ""any""  [attr-defined]"
717597629,36989,"TYP: generic, series, frame",jbrockmendel,closed,2020-10-08T19:16:38Z,2020-10-10T16:24:50Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @simonjayhawkins 
"
718512459,37019,CLN/REF: de-duplicate DatetimeTZBlock.setitem,jbrockmendel,closed,2020-10-10T02:41:27Z,2020-10-10T16:29:24Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Move towards avoiding special treatment for pandas-internal EAs.

xref #24020"
718502356,37017,REF/TYP: define NDFrame numeric methods non-dynamically,jbrockmendel,closed,2020-10-10T01:42:41Z,2020-10-10T16:31:15Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @simonjayhawkins along with #36989 this gets most of the generic.py mypy complaints (notwithstanding complaints about `cls.foo = foo` that can be addressed by changing to `setattr(cls, ""foo"", foo)`.

it'd be nice to have a less-verbose way of pinning the docstrings.  i haven't had any luck so far at making that work."
718511769,37018,CLN: require td64 in TimeDeltaBlock,jbrockmendel,closed,2020-10-10T02:36:55Z,2020-10-10T16:35:10Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

only one place where the check currently fails, fixed in core.missing"
679646959,35744,BUG: pd.Series() constructor with 2d NumPy array raises bare Exception,micahjsmith,closed,2020-08-15T21:51:29Z,2020-10-10T16:46:24Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np
arr = np.random.rand(10, 4)
pd.Series(arr)
```

I get this output

```
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-4-974eca309da1> in <module>
      2 import numpy as np
      3 arr = np.random.rand(10, 4)
----> 4 pd.Series(arr)

~/.pyenv/versions/3.8.3/envs/census/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath)
    325                     data = data.copy()
    326             else:
--> 327                 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True)
    328 
    329                 data = SingleBlockManager.from_array(data, index)

~/.pyenv/versions/3.8.3/envs/census/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure)
    490     elif subarr.ndim > 1:
    491         if isinstance(data, np.ndarray):
--> 492             raise Exception(""Data must be 1-dimensional"")
    493         else:
    494             subarr = com.asarray_tuplesafe(data, dtype=dtype)

Exception: Data must be 1-dimensional
```

#### Problem description

Yes, creating a series from an array is invalid. But the problem is that a bare `Exception` is raised for this. A different exception type like `ValueError` is expected. User code should [mostly] never be catching bare Exceptions, but may reasonable catch `ValueError`s or `TypeError`s from pandas routines.

I'm not entirely sure of the rationale for raising a bare Exception in the `sanitize_array` method, I started looking through the git blame but didn't go back that far.

I think a PR that just raises a `ValueError` in `sanitize_array` would be an appropriate fix and I can create that if given the thumbs up.

#### Expected Output

A `ValueError` is raised, that can be caught be user code.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Sun Jul  5 00:43:10 PDT 2020; root:xnu-6153.141.1~9/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.0
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
718524404,37021,"CLN: collected cleanups, warning suppression in tests",jbrockmendel,closed,2020-10-10T03:57:18Z,2020-10-10T17:01:09Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
437861127,26223,DataFrame.apply returns Series of type object instead of correct dtype,jrbrodie77,closed,2019-04-26T22:01:11Z,2020-10-10T17:17:48Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
df = pd.DataFrame([[1.0, 2.0, 3.0, 'A'],
                   [1.0, 2.0, 2.0, 'A'],
                   [2.0, 4.3, 2.3, 'B'],
                   [6.5, 3.4, 2.1, 'B']],
                   columns=['n0', 'n1', 'n2', 'letter'])

def print_series_dtype(series):
    print(series.name, series.dtype)
    
def print_dataframe_dtypes(dataframe):
    print(""DF dtypes:"", list(dataframe.dtypes))
    print(""DF colnames"", list(dataframe.columns))
    dataframe.apply(print_series_dtype)

df.groupby('letter').apply(print_dataframe_dtypes)

# Output
DF dtypes: [dtype('float64'), dtype('float64'), dtype('float64'), dtype('O')]
DF colnames ['n0', 'n1', 'n2', 'letter']
n0 object
n1 object
n2 object
letter object


```
#### Problem description
DataFrame.gropby(..).apply(<func>) provides func with a series that has a dtype of object, regardless of the original dtype.

The code snippet above shows that the series dtype is mismatched. 

#### Expected Output
I would expect the dtypes to propagate unchanged. (in this case, it caused some downstream code to seriously misbehave).

#### Output of ``pd.show_versions()``
In [24]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.0.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.23.4
pytest: 3.8.2
pip: 19.0.3
setuptools: 40.2.0
Cython: 0.29.7
numpy: 1.15.2
scipy: 1.1.0
pyarrow: 0.11.1
xarray: None
IPython: 7.3.0
sphinx: None
patsy: None
dateutil: 2.7.3
pytz: 2018.5
blosc: None
bottleneck: None
tables: 3.4.4
numexpr: 2.6.8
feather: None
matplotlib: 3.0.0
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None"
407233257,25187,DOC: DataFrame.mask() uses df.where in the examples section,miguelfg,closed,2019-02-06T13:23:28Z,2020-10-10T17:26:25Z,"In the stables doc page of `pandas.DataFrame.mask()` in the bottom section of 'Examples', is showing executions of using `pandas.DataFrame.where()` method instead of mask. I refer to this [page](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mask.html)
"
575600360,32441,`DataFrame.diff` on `datetime64` returns result inconsistent with `Series.diff` ,batterseapower,closed,2020-03-04T17:27:15Z,2020-10-10T17:40:57Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> pd.Series(['NaT', '2019-01-01', '2019-01-02'], dtype='datetime64[ns]').diff()
0      NaT
1      NaT
2   1 days
dtype: timedelta64[ns]

>>> pd.Series(['NaT', '2019-01-01', '2019-01-02'], dtype='datetime64[ns]').to_frame().diff()
                             0
0                          NaT
1 -88855 days +00:12:43.145224
2              1 days 00:00:00
```

#### Problem description

Doing `.diff()` on a `DataFrame` of `datetime64` should yield exactly time deltas in exactly the same way as on the `Series` i.e. index `(1,0)` in the `DataFrame` above should be `NaT`, not an overflown timedelta64.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.1.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 63 Stepping 2, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : English_United Kingdom.936

pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 45.2.0.post20200210
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None
</details>
"
716049627,36928,BUG: `dict_keys` cannot be used as `pd.read_csv`'s `names` parameter,abmyii,closed,2020-10-06T21:59:28Z,2020-10-10T17:45:01Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example
`test.csv`:
```csv
a,b
1,2
```

```python
import pandas as pd
data = {'a': 10, 'b': 20}
pd.read_csv('test.csv', names=data.keys())
```

Error:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py"", line 691, in read_csv
    return _read(filepath_or_buffer, kwds)
  File ""/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py"", line 451, in _read
    _validate_names(kwds.get(""names"", None))
  File ""/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py"", line 424, in _validate_names
    raise ValueError(""Names should be an ordered collection."")
ValueError: Names should be an ordered collection.
```

#### Problem description

When a `dict_keys` object is passed, the keys aren't used as names. This is because `dict_keys` isn't list_like (https://github.com/pandas-dev/pandas/blob/4e553464f97a83dd2d827d559b74e17603695ca7/pandas/io/parsers.py#L423) or indexable, so the issue is understandable, but it would be nice to be able to pass `dict_keys` without issue.

#### Expected Output

Passing the dict keys to names shouldn't fail.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : 4e553464f97a83dd2d827d559b74e17603695ca7
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-118-generic
Version          : #119-Ubuntu SMP Tue Sep 8 12:30:01 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+632.g4e553464f
numpy            : 1.18.2
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 46.1.3
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 1.6.7
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 0.999999999
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : None
pandas_datareader: None
bs4              : 4.6.0
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
718278869,37008,Add pandas-genomics to the ecosystem list,jrm5100,closed,2020-10-09T16:31:31Z,2020-10-10T17:45:46Z,"- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Added [as suggested here](https://github.com/pandas-dev/pandas/pull/36987#pullrequestreview-505157583)."
717721967,36995,TYP: IntervalIndex.SetopCheck,jbrockmendel,closed,2020-10-08T22:47:56Z,2020-10-10T18:07:29Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Moves us closer to #33455"
717641249,36990,"TYP: sas, stata, style",jbrockmendel,closed,2020-10-08T20:28:13Z,2020-10-10T18:08:02Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
718366659,37010,REF/TYP: consistent return type for Block.replace,jbrockmendel,closed,2020-10-09T19:13:07Z,2020-10-10T18:08:15Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

move some convert logic out of _replace_coerce to avoid an otherwise-duplicated method"
717799930,36998,BUG: DataFrame.diff with dt64 and NaTs,jbrockmendel,closed,2020-10-09T02:20:07Z,2020-10-10T18:09:04Z,"- [x] closes #32441
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

There's a related bug in algos.diff, but the elegant solution goes through the cython code, so im doing that in a separate branch."
716949947,36965,CLN: share to_native_types,jbrockmendel,closed,2020-10-08T01:16:28Z,2020-10-10T18:09:42Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
716674199,36943,REF/TYP: define methods non-dynamically for SparseArray,jbrockmendel,closed,2020-10-07T16:21:39Z,2020-10-10T18:10:33Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
716901580,36961,TYP: use OpsMixin for DecimalArray,jbrockmendel,closed,2020-10-07T22:51:43Z,2020-10-10T18:11:11Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
717418085,36982,FIX: fix cleanup warnings for errorbar timeseries,ivanovmg,closed,2020-10-08T14:52:03Z,2020-10-10T18:11:39Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Currently in ``pandas/tests/plotting/tests_frame.py`` there are several warnings emitted.

```
pandas/tests/plotting/test_frame.py::TestDataFramePlots::test_mpl2_color_cycle_str
  /workspaces/pandas/pandas/plotting/_matplotlib/style.py:64: MatplotlibDeprecationWarning: Support for uppercase single-letter colors is deprecated since Matplotlib 3.1 and will be removed in 3.3; please use lowercase instead.
    [conv.to_rgba(c) for c in colors]

pandas/tests/plotting/test_frame.py::TestDataFramePlots::test_errorbar_plot
pandas/tests/plotting/test_frame.py::TestDataFramePlots::test_errorbar_plot
pandas/tests/plotting/test_frame.py::TestDataFramePlots::test_errorbar_plot
pandas/tests/plotting/test_frame.py::TestDataFramePlots::test_errorbar_timeseries
pandas/tests/plotting/test_frame.py::TestDataFramePlots::test_errorbar_timeseries
pandas/tests/plotting/test_frame.py::TestDataFramePlots::test_errorbar_timeseries
  /workspaces/pandas/pandas/plotting/_matplotlib/__init__.py:61: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared
    plot_obj.generate()

-- Docs: https://docs.pytest.org/en/stable/warnings.html
```

This PR handles warnings in ``test_errorbar_timeseries``.
If this approach is considered reasonable by the reviewers, then I will do the same for ``test_errorbar_plot`` as well as other similar warnings among the plotting-related tests."
715253740,36904,REGR: change in Series.astype(str) behavior for None,itholic,closed,2020-10-06T00:39:53Z,2020-10-10T18:24:59Z,"#### Question about pandas

Let's say we have a `Series` with `None` like the below.

```python
pser = pd.Series([""hi"", ""hi "", "" "", "" \t"", """", None], name=""x"")

print(pser)
0      hi
1     hi
2
3      \t
4
5    None
Name: x, dtype: object
```

the last value is `None`, but it is casted to `nan` after using `astype` with `str` parameter.

```python
print(pser.astype(str))
0     hi
1    hi
2
3     \t
4
5    nan
Name: x, dtype: object
```

Is it intended behavior in pandas 1.1.1. ??

Thanks :)"
665559232,35409,"DOC: Inconsistent return type documentation for functions with ""inplace"" option",SycamoreLeaf,closed,2020-07-25T10:17:59Z,2020-10-10T19:41:57Z,"#### Location of the documentation

This proposal concerns all methods with the `inplace` keyword argument. Three examples:

1 (Return type described as ""blah-blah-type or `None` if `inplace=True`) https://pandas.pydata.org/docs/dev/reference/api/pandas.CategoricalIndex.add_categories.html?highlight=inplace

2 (Return type specified, but doesn't mention `None`) https://pandas.pydata.org/docs/dev/reference/api/pandas.Series.interpolate.html?highlight=inplace

#### Documentation problem

Many methods in pandas have keyword argument `inplace`. When `inplace=True` is supplied, the method updates `self` and returns `None`. When `inplace=False` is supplied, the method does not affect `self` and returns a changed copy of `self`.

Some of these functions' return types are documented as ""Returns blah-type if inplace=True"". Others just say ""Returns blah-type"". This is confusing because it suggests inconsistencies in *behavior* that are not really there.

#### Suggested fix for documentation

Find all methods with the `inplace` keyword argument. Change the ""returns"" section of their docstrings to this form:
```
        Returns
        -------
        Series or DataFrame
            Returns the same object type as the caller, interpolated at
            some or all ``NaN`` values. (Or return `None` if
            `inplace=True`.)
```
Also change all the formal return type signatures to the form ""--> Optional[Foo]"" if they aren't all already like that."
717262076,36979,CLN: re-wrap docstrings in pandas\compat\numpy\function.py,simonjayhawkins,closed,2020-10-08T11:31:06Z,2020-10-10T19:43:44Z,
717721032,36994,TYP/REF: use OpsMixin for arithmetic methods,jbrockmendel,closed,2020-10-08T22:45:38Z,2020-10-10T20:04:36Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

We'll be able to use this in DataFrame too following #36843"
717447783,36983,BUG: Index of an empty Series cannot be compared to a Timestamp anymore,gabicca,closed,2020-10-08T15:27:56Z,2020-10-10T21:26:12Z,"- [Y] I have checked that this issue has not already been reported.

- [Y] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
def test_series_timestamp_comparison():
    my_series = pd.Series([],  dtype=np.float64)
    date = pd.Timestamp('2015-07-15 00:00:00')

    res = my_series.index < date
    assert res.dtype == bool
    assert len(res) == 0
```

#### Problem description

In pandas 1.1.2 the above test passes, while in 1.1.3 it throws an error on line  5:

Error in 1.1.3:
TypeError: '<' not supported between instances of 'numpy.ndarray' and 'Timestamp'

As far as I can tell, this started to occur because of a change in pandas.core.indexes.base._make_comparison_op,
which change was added by this PR: https://github.com/pandas-dev/pandas/pull/36440

My question is if the new behaviour is legit for the above comparison, is it expected?

In 1.1.2 this comparison didn't fail, because it would end up in
```
        else:
            with np.errstate(all=""ignore""):
                result = op(self._values, np.asarray(other))
```
which was changed to:
```
        elif is_interval_dtype(self.dtype):
            with np.errstate(all=""ignore""):
                result = op(self._values, np.asarray(other))
```
and the else part started to use ops.comparison_op which now fails in the above example (since is_interval_dtype(self.dtype) is false for the above case).

A bit more background: the above comparison is used when subselecting data in the series without knowing whether the input series has data in it or not. So e.g.:
```
my_series[my_series.index < date] = 0.4
```

Apologies if this question has been raised, I couldn't find anything similar in Open Issues.

#### Output of ``pd.show_versions()``

<details>

python           : 3.7.4.final.0
pandas           : 1.1.3
numpy            : 1.19.2

</details>
"
714219387,36847,CI: pin pymysql #36465,fangchenli,closed,2020-10-04T02:51:52Z,2020-10-10T21:32:25Z,"Part of #36465
"
708625533,36626,CI: add py38 slow build #35160,fangchenli,closed,2020-09-25T04:11:33Z,2020-10-10T21:32:34Z,"- [x] closes #35160

This issue was created before we dropped py36 support. So I think we should add equivalent CI build for py38.

"
713245182,36796,"API: disallow frame.__and__(other, axis=""foo"", fill_value=""bar"") etc",jbrockmendel,closed,2020-10-01T23:20:46Z,2020-10-10T22:27:19Z,"For code-sharing purposes we re-use ops.arith_method_FRAME for both flex and non-flex methods.  As a result, the non-flex methods have the flex signature, and accept arguments not generally accepted by dunder methods.  We even have a few tests e.g.

```
result = d.__and__(s, axis=""columns"")
result = d.__and__(s, axis=0)
```

in tests.frame.test_operators.

Should we disallow passing these arguments?  Is it relevant that we do _not_ have flex versions of and/or/xor?
"
690215816,36040,BUG: Rolling min_periods not working on groupby object,justinessert,closed,2020-09-01T14:57:53Z,2020-10-10T22:30:51Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame({
    'segment': 'A',
    'data': range(10)
})

df.rolling(5, center=True, min_periods=1).max()

df.groupby('segment').rolling(5, center=True, min_periods=1).max().reset_index(drop=True)
```

#### Problem description

For the DataFrame above, with a single segment 'A', the result of `df.rolling(5, center=True, min_periods=1).max()` should be identical to that of `df.groupby('segment').rolling(5, center=True, min_periods=1).max().reset_index(drop=True)`. Instead, the latter operation has NaNs in the last two positions of the data column.

#### Expected Output

Both operations should return the sequence `[2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 9.0, 9.0]`. Instead, `df.groupby('segment').rolling(5, center=True, min_periods=1).max().reset_index(drop=True)` returns  `[2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, NaN, NaN]`

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : f2ca0a2665b2d169c97de87b8e778dbed86aea07
python           : 3.7.7.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.1
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.0
pip              : 20.1.1
setuptools       : 47.3.0.post20200616
Cython           : None
pytest           : 6.0.0
hypothesis       : None
sphinx           : 3.1.1
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.0
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.0
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
718647929,37035,API: reimplement FixedWindowIndexer.get_window_bounds,justinessert,closed,2020-10-10T15:54:29Z,2020-10-10T22:30:54Z,"- [x] closes #36040
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This PR replaces [36132](https://github.com/pandas-dev/pandas/pull/36132) following @mroeschke's [36567](https://github.com/pandas-dev/pandas/pull/36567).

Originally, my PR was to fix Issue [36040](https://github.com/pandas-dev/pandas/issues/36040). But it appears that Matthew's PR already fixed it! Nonetheless, I still included two things from my previous PR:

1. I added tests to cover the bug outlined in Issue [36040](https://github.com/pandas-dev/pandas/issues/36040).
2. I reimplemented FixedWindowIndexer.get_window_bounds in a way that I believe if a lot more intuitive and simple. Please lmk if you disagree, in which case I could remove this from the PR.

I don't believe any functionality was altered by this PR so I did not include a whatsnew entry"
637417899,34725,"BUG: fillna(method=""ffill"") and ffill() on DataFrameGroupBy gives different results",anjanibhat,closed,2020-06-12T01:17:02Z,2020-10-10T22:32:29Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Your code here

import numpy as np
import pandas as pd
df = pd.DataFrame({'a':[1, 1, np.NaN, np.NaN], 'b':[10, np.NaN, 40, np.NaN]})
```
```
df
     a     b
0  1.0  10.0
1  1.0   NaN
2  NaN  40.0
3  NaN   NaN
```

```
# Case 1 - Forward fill using fillna
df.groupby([""a""]).fillna(method=""ffill"")

df
      b
0  10.0
1  10.0
2   NaN
3   NaN
```

```
# Case 2 - Forward fill using ffill
df.groupby([""a""]).ffill()

df
      b
0  10.0
1  10.0
2  40.0
3  40.0
```

#### Problem description
`df.groupby([""a""]).ffill()` is not ignoring the NaN group while performing forward fill. 

#### Expected Output
`df.groupby([""a""]).ffill()` should give the same output as `df.groupby([""a""]).fillna(method=""ffill"")` since NaN groups are ignored as per https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html#na-values-in-groupby.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 16.7.0
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_US.UTF-8
pandas           : 1.0.4
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 44.0.0.post20200106
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.2
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : 0.17.0
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : 0.4.2
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : 0.8.6
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
713205459,36790,BUG: GroupBy.ffill()/bfill() do not return NaN values for NaN groups,smithto1,closed,2020-10-01T21:45:55Z,2020-10-10T22:32:35Z,"- [x] closes #34725
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

See #34725 for a good description of the problem. 

This fixes the issue in the tidiest way I could find. 

One test is added that covers all relevant cases I could think of. The comments in the test can be removed, they are just there to make it easier for the reviewer to follow as the test is a bit complicated. 

This also fixes the copy-pastable example from the #34725."
717097038,36972,BUG: warning when using colors 'CN',ivanovmg,closed,2020-10-08T07:28:24Z,2020-10-10T22:33:15Z,"- [x] I have checked that this issue has not already been reported.

- [ ] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


df = pd.DataFrame(np.random.randn(10, 3), columns=[""a"", ""b"", ""c""])
df.plot(color='C0')
plt.show()

```

#### Problem description

On master branch when executing the code above the following warning is raised.
```
workspaces/pandas/pandas/plotting/_matplotlib/style.py:64: MatplotlibDeprecationWarning: Support for uppercase single-letter colors is deprecated since Matplotlib 3.1 and will be removed in 3.3; please use lowercase instead.
  [conv.to_rgba(c) for c in colors]
```

#### Expected Output

Expected no warnings.

Related issue: https://github.com/pandas-dev/pandas/issues/15516. The feature with ""CN""-like colors was introduced in https://github.com/pandas-dev/pandas/pull/15873.

I figured out that the problem lies in the line
```
        maybe_color_cycle = _maybe_valid_colors(list(colors))
```
So, if colors is ""C0"", then it is split into [""C"", ""0""].

I will fix it.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 32
OS               : Windows
OS-release       : 10
machine          : AMD64
processor        : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : English_United States.1251

pandas           : 1.0.5
numpy            : 1.19.0
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.1
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
716177102,36932,REF/TYP: arrays.datetimelike,jbrockmendel,closed,2020-10-07T04:04:38Z,2020-10-10T22:45:23Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

@simonjayhawkins this is still giving me two mypy complaints locally:

```
pandas/core/arrays/datetimelike.py:549: error: Too many arguments for ""object""  [call-arg]
pandas/core/arrays/datetimelike.py:556: error: Too many arguments for ""object""  [call-arg]
```

These lines are both calls to `self._scalar_type(foo)`

_scalar_type is annotated as `Type[DatetimeLikeScalar]`, which is a TypeVar for Period/Timestamp/Timedelta.  Is there a non-ignoring solution here?"
714197272,36843,REF: separate flex from non-flex DataFrame ops,jbrockmendel,closed,2020-10-03T23:42:40Z,2020-10-10T22:45:58Z,"- [x] closes #36796
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Unless supporting this is intentional, in which case we should deprecate and explicitly implement flex-bool operations.

If/when this is done, we can stop passing `special` and even `cls`."
718412951,37014,DOC: Clean merging.rst,dsaxton,closed,2020-10-09T20:45:44Z,2020-10-10T22:46:54Z,Closes #36777
712921422,36777,DOC: Fix code style in documentation,dsaxton,closed,2020-10-01T14:54:50Z,2020-10-10T22:55:19Z,"The docs use certain coding style conventions that are no longer used in pandas (e.g., we adopt black's opinions in terms of line wrapping and quoting), for instance https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html. We should update the docs to match current style preferences.

[blacken-docs](https://github.com/asottile/blacken-docs) is a useful tool that can help in doing _some_ of this formatting, but it has to be used carefully since it might render lines that are too long and will fail flake8-rst checks, or it may not be able to parse certain files because of nuances in how the docs are written (raw output or magic commands in code blocks). One approach is to comment out the erring lines, let the tool handle all the others, then finish the commented lines manually.

Example cleanup: https://github.com/pandas-dev/pandas/pull/36700/files

List of files that need updating (if you submit a PR be sure to link back to this issue):

**doc/source/development:**

- [x] code_style.rst
- [x] contributing.rst
- [x] contributing_docstring.rst
- [x] developer.rst
- [x] extending.rst
- [x] index.rst
- [x] internals.rst
- [x] maintaining.rst
- [x] meeting.rst
- [x] policies.rst
- [x] roadmap.rst

**doc/source/getting_started/comparison:**

- [x] comparison_with_r.rst
- [x] comparison_with_sas.rst
- [x] comparison_with_sql.rst
- [x] comparison_with_stata.rst
- [x] index.rst

**doc/source/getting_started/intro_tutorials:**

- [x] 01_table_oriented.rst
- [x] 02_read_write.rst
- [x] 03_subset_data.rst
- [x] 04_plotting.rst
- [x] 05_add_columns.rst
- [x] 06_calculate_statistics.rst
- [x] 07_reshape_table_layout.rst
- [x] 08_combine_dataframe.rst
- [x] 09_timeseries.rst
- [x] 10_text_data.rst

**doc/source/getting_started:**

- [x] index.rst
- [x] install.rst
- [x] overview.rst
- [x] tutorials.rst

**doc/source/user_guide**:

- [x] 10min.rst
- [x] advanced.rst
- [x] basics.rst
- [x] boolean.rst
- [x] categorical.rst
- [x] computation.rst
- [x] cookbook.rst
- [x] dsintro.rst
- [x] duplicates.rst
- [x] enhancingperf.rst
- [x] gotchas.rst
- [x] groupby.rst
- [x] index.rst
- [x] integer_na.rst
- [x] io.rst
- [x] merging.rst
- [x] missing_data.rst
- [x] options.rst
- [x] reshaping.rst
- [x] scale.rst
- [x] sparse.rst
- [x] text.rst
- [x] timedeltas.rst
- [x] timeseries.rst
- [x] visualization.rst"
714834398,36888,BUG: to_json(lines=True) does not add a trailing newline,lleeoo,closed,2020-10-05T13:19:25Z,2020-10-10T22:55:57Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
df = pd.DataFrame({'AAA': [4, 5, 6, 7],
                   'BBB': [10, 20, 30, 40],
                   'CCC': [100, 50, -30, -50]})
df.to_json('foo.jsonl', orient='records', lines=True)                                       
open('foo.jsonl').read()                                                                    
```
output (Notice - no newline after the last line):
```
'{""AAA"":4,""BBB"":10,""CCC"":100}\n{""AAA"":5,""BBB"":20,""CCC"":50}\n{""AAA"":6,""BBB"":30,""CCC"":-30}\n{""AAA"":7,""BBB"":40,""CCC"":-50}'
```

#### Problem description

When outputting JSON lines, i.e. `lines=True`, every record should end with a newline, including the last record. Otherwise, files cannot be concatenated, the number of records will be counted incorrectly by `wc -l`, etc.

#### Expected Output

There should be a newline at the end, i.e.:
```
'{""AAA"":4,""BBB"":10,""CCC"":100}\n{""AAA"":5,""BBB"":20,""CCC"":50}\n{""AAA"":6,""BBB"":30,""CCC"":-30}\n{""AAA"":7,""BBB"":40,""CCC"":-50}\n'
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1127.19.1.el7.x86_64
Version          : #1 SMP Tue Aug 11 19:12:04 EDT 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.6.0.post20200814
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.0
fastparquet      : 0.4.1
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.50.1

</details>
"
715115054,36898,BUG: Add trailing trailing newline in to_json,Rohith295,closed,2020-10-05T19:48:07Z,2020-10-10T22:56:01Z,"- [x] closes #36888
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
716843217,36958,DOC: Add GCS as supported filesystem for read_parquet,joshtemple,closed,2020-10-07T20:51:41Z,2020-10-10T22:56:41Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
709857965,36695,API: Deprecate regex=True default in Series.str.replace,dsaxton,closed,2020-09-28T00:58:54Z,2020-10-10T23:01:27Z,"From some old discussion it looks like there was interest in changing the default value of regex from True to False within Series.str.replace. I think this makes sense since it would align this method with others within pandas along with the standard library, and would also make fixing https://github.com/pandas-dev/pandas/issues/24804 slightly less disruptive. I'm assuming this would warrant a deprecation note in the next whatsnew?

I think this part of the docs will need to be updated: https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#splitting-and-replacing-strings

https://github.com/pandas-dev/pandas/pull/24809"
578771759,32590,Histogram or kde from datetime column,JulianWgs,closed,2020-03-10T18:15:03Z,2020-10-10T23:07:42Z,"It always annoyed me that it was not possible to generate a histogram or kernel density estimation plot from a datetime column.

Is there interest in a solution following this approach?
[Link to SO](https://stackoverflow.com/a/60617297)

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = pd.DataFrame({""datetime"": pd.to_datetime(np.random.randint(1582800000000000000, 1583500000000000000, 100, dtype=np.int64))})
fig, ax = plt.subplots()
df[""datetime""].astype(np.int64).plot.hist(ax=ax)
labels = ax.get_xticks().tolist()
labels = pd.to_datetime(labels)
ax.set_xticklabels(labels, rotation=90)
plt.show()
```

![2UUMI](https://user-images.githubusercontent.com/31596773/76345209-3563b500-6303-11ea-8f81-d6a2198e8c4f.jpg)



I didn't look into the the pandas source code yet, but if there is interest in something like this I will look into it.

I think the main issue will be finding good x tick labels, if thats a requirement.

Greetings!
"
699118608,36287,ENH: support of pandas.DataFrame.hist for datetime data,onshek,closed,2020-09-11T10:04:07Z,2020-10-10T23:07:49Z,"- [x] closes #32590
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

# Update

```
import numpy as np
from pandas import DataFrame, to_datetime
from datetime import timedelta

df = DataFrame(
    {
        ""a"": np.random.rand(10),
        ""b"": [timedelta(np.random.randn()) for _ in range(10)],
        ""c"": to_datetime(
            np.random.randint(
                1582800000000000000, 1583500000000000000, 10, dtype=np.int64
            )
        ),
        ""d"": to_datetime(
            np.random.randint(
                1582800000000000000, 1583500000000000000, 10, dtype=np.int64
            ),
            utc=True
        ),
    }
)

df.dtypes
a                float64
b        timedelta64[ns]
c         datetime64[ns]
d    datetime64[ns, UTC]
dtype: object

df.hist(xrot=90, figsize=(9, 6))
array([[<AxesSubplot:title={'center':'a'}>,
        <AxesSubplot:title={'center':'c'}>],
       [<AxesSubplot:title={'center':'d'}>, <AxesSubplot:>]], dtype=object)
```
![download](https://user-images.githubusercontent.com/21543236/93713409-3f0d8580-fb8e-11ea-934a-a6e302a7b14d.png)

# ~~WIP~~

Since there's no response for several days in https://github.com/pandas-dev/pandas/issues/32590, I decide to open this PR for further discussion.
There's two feasible options:

Plan A:
As is shown in the files changed, `data._get_numeric_data()` is modified to `data._get_numeric_or_datetime_data()`, and I will work on related tests as the next step.

Plan B, this may be a fotfix according to https://github.com/pandas-dev/pandas/issues/32590#issuecomment-687477764:
```
column_dt = data.select_dtypes(include='datetime64[ns]')
if len(column_dt) > 0:
    data[column_dt] = data[column_dt].astype(np.int64)
data = data._get_numeric_data()
if len(column_dt) > 0:
    data[column_dt] = data[column_dt].astype('datetime64[ns]')
```
Also, related tests will be modified.

Any comment / thought is welcomed."
715951465,36925,CLN: Clean groupby tests,phofl,closed,2020-10-06T19:09:07Z,2020-10-10T23:31:40Z,Found a few unused parts while moving tests for another PR
712295904,36753,BUG: Segfault with string Index when using Rolling after Groupby,phofl,closed,2020-09-30T21:33:33Z,2020-10-10T23:35:12Z,"- [x] closes #36727
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The segfault was caused by ``self.obj.index.asi8=None`` when Index is a string Index. ``self._on.asi8`` solves that issue.

Additionally I noticed, that ``obj.index`` was already sorted, so the insert of ``extra_col`` mixed up the order. We should use ``self.obj.index``.

I will add a whats new after #36689 is merged.

cc @mroeschke "
718638196,37032,DOC: Group relevant df.mask/df.where examples together,hongshaoyang,closed,2020-10-10T15:02:19Z,2020-10-11T02:35:32Z,"df.mask() and df.where() are complementary and so they share docs, see [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mask.html). I grouped related examples together.

- [x] Close #25187
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
718597935,37027,TST: insert 'match' to bare pytest raises in pandas/tests/tools/test_…,krajatcl,closed,2020-10-10T11:22:23Z,2020-10-11T03:12:26Z,"- [ ] ref https://github.com/pandas-dev/pandas/issues/30999
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
717580478,36985,CLN: move maybe_casted_values from pandas/core/frame.py to pandas/core/dtype/cast.py,arw2019,closed,2020-10-08T18:49:26Z,2020-10-11T04:01:00Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref #36876, #27370 (stale PR)"
711024336,36722,"CI unpin flake8, only run flake8-rst in pre-commit",MarcoGorelli,closed,2020-09-29T11:33:21Z,2020-10-11T08:36:20Z,"- [ ] closes #34150 (maybe?)
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
718722077,37045,MYPY: Delete unnecessary unused type hint,phofl,closed,2020-10-10T23:29:24Z,2020-10-11T09:55:51Z,cc @jreback 
718643393,37033,CLN: Simplify aggregation.aggregate,rhshadrach,closed,2020-10-10T15:29:54Z,2020-10-11T13:15:21Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
717809769,36999,CLN: Move _aggregate and _aggregate_multiple_funcs to core.aggregation,rhshadrach,closed,2020-10-09T02:52:36Z,2020-10-11T13:22:02Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Renamed functions by removing underscores and renamed `self` argument to `obj`."
709610242,36677,CLN/TYP: aggregation methods in core.base,rhshadrach,closed,2020-09-26T21:00:28Z,2020-10-11T13:22:02Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This is in order to move _aggregate and _aggregate_multiple_funcs into core.aggregation - I think that's desirable. Type-hinting is slightly more strong there (checks untyped defs), and would involve renaming `self` to `obj`. Thus the renaming of `obj` to `selected_obj` here."
709202095,36643,TYP: selection and groups type-hinting in groupby,rhshadrach,closed,2020-09-25T18:42:21Z,2020-10-11T13:22:02Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
704933426,36478,CLN: aggregation.transform,rhshadrach,closed,2020-09-19T17:21:59Z,2020-10-11T13:22:02Z,"- [x] closes #36330
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Followup to #35964 

- Moved whatsnew to reshaping
- Use is_list_like/is_dict_like in aggregate.transform
- Broke out dict-like and str/callable computations to own functions to split up the transform function
- Added/refined typing"
705965272,36536,CLN: Break up wrap applied output,rhshadrach,closed,2020-09-21T22:50:51Z,2020-10-11T13:22:08Z,Splitting off the Series case seemed to be a natural way to break up this method. Always open to other thoughts.
708581332,36618,CLN: Break up aggregate.transform,rhshadrach,closed,2020-09-25T01:57:24Z,2020-10-11T13:22:10Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Precursor to #36478. Just breaks up transform without any code changes."
638225475,34756,BUG: DataFrameGroupBy.quantile raises for non-numeric dtypes rather than dropping columns,rhshadrach,closed,2020-06-13T20:17:17Z,2020-10-11T13:22:14Z,"- [x] closes #27892
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Unlike what is mentioned in #27892, this will raise if there are no columns to aggregate. Both mean and median raise with ""No numeric types to aggregate"" in such a case, so I was thinking perhaps we should be consistent with them. Any thoughts @WillAyd and @TomAugspurger?

"
611298610,33949,WIP: BUG: Setting DataFrame values via iloc aligns when arguments are lists,rhshadrach,closed,2020-05-02T23:57:30Z,2020-10-11T13:22:15Z,"- [x] closes #22046
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This PR currently fails two tests on Windows due to int32 vs int64, but I cant' duplicate on my machine.

````
pandas\tests\indexing\test_indexing.py:588
test_astype_assignment

df = df_orig.copy()
df.iloc[:, 0:2] = df.iloc[:, 0:2].astype(np.int64)
expected = DataFrame(
    [[1, 2, ""3"", "".4"", 5, 6.0, ""foo""]], columns=list(""ABCDEFG"")
)
>       tm.assert_frame_equal(df, expected)
E       AssertionError: Attributes of DataFrame.iloc[:, 0] (column name=""A"") are different
E       
E       Attribute ""dtype"" are different
E       [left]:  int32
E       [right]: int64
````

Within these test, the change made causes the setting in iloc to be done here:

https://github.com/pandas-dev/pandas/blob/c6ad13d57c8157cd175a294f06c427f8a239dcea/pandas/core/indexing.py#L1722-L1724

as opposed to the usual path here:

https://github.com/pandas-dev/pandas/blob/c6ad13d57c8157cd175a294f06c427f8a239dcea/pandas/core/indexing.py#L1695-L1705

My best guess is that this is a bug in the former path that is being uncovered by this change, but I am not sure how to fix. Any suggestions or help is much appreciated."
718747110,37048,TST: Fix failing test from #37027,hongshaoyang,closed,2020-10-11T03:11:57Z,2020-10-11T13:33:48Z,"Part of #30999
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
718844702,37052,"CI clean pre-commit: upgrade isort, use correct file types in flake8",MarcoGorelli,closed,2020-10-11T14:13:19Z,2020-10-11T16:08:26Z,"A few things here:

- updating isort version
- updating to use `types` instead of `files` (see https://github.com/pre-commit/pre-commit/issues/1436#issuecomment-624219819 and https://github.com/PyCQA/isort/pull/1549#issuecomment-706621474)
- removing `exclude: ^pandas/__init__\.py$|^pandas/core/api\.py$` as it's no longer necessary
- replacing `files: \.(pyx|pxd)$` with `types: [cython]`:

  ```bash
  $ identify-cli pandas/_libs/groupby.pyx
  [""cython"", ""file"", ""non-executable"", ""text""]
  $ identify-cli pandas/_libs/lib.pxd
  [""cython"", ""file"", ""non-executable"", ""text""]
  ```

- replacing the name `flake8-pxd` with `flake8 (cython template)`, as that hook wasn't actually checking `.pxd` files (the `flake8-pyx` one was)"
718713107,37040,CLN: remove unnecessary DatetimeTZBlock.fillna,jbrockmendel,closed,2020-10-10T22:14:54Z,2020-10-11T17:36:16Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

and no need to have TimeDeltaBlock inherit from IntBlock"
718854144,37054,troubleshoot CI,jbrockmendel,closed,2020-10-11T14:58:51Z,2020-10-11T17:41:39Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
549329286,30994,API: DatetimeIndex.get_loc(datetime) should require tzawareness compat?,jbrockmendel,closed,2020-01-14T04:46:06Z,2020-10-11T19:16:25Z,"```
dti = pd.date_range(""2016-01-01"", periods=3)
dti2 = dti.tz_localize(""UTC"").tz_convert(""US/Pacific"")

>>> dti.get_loc(dti2[0])
0
>>> dti2.get_loc(dti[0])
KeyError: Timestamp('2016-01-01 00:00:00-0800', tz='US/Pacific', freq='D')

ser = pd.Series(range(3), index=dti)
ser2 = pd.Series(range(3), index=dti2)

>>> ser.loc[dti2[1]]
1
>>> ser2.loc[dti[1]]
KeyError: Timestamp('2016-01-02 00:00:00-0800', tz='US/Pacific', freq='D')
```

get_loc is effectively an equality check, so I think it should behave like all our other comparisons and require tzawareness-compat.  So all of these would raise KeyError.

xref #17920, https://github.com/pandas-dev/pandas/pull/30819#issuecomment-572281006 "
714373149,36865,TST: insert 'match' to bare pytest raises in pandas/tests/tseries/off…,krajatcl,closed,2020-10-04T18:27:10Z,2020-10-11T19:21:32Z,"…sets/test_offsets.py

- [ ] ref https://github.com/pandas-dev/pandas/issues/30999
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
717047285,36970,DOC: improve description of the examplewhich dataframe has two rows,yonashub,closed,2020-10-08T05:59:57Z,2020-10-11T19:38:46Z,"#### Location of the documentation

[this should provide the location of the documentation, e.g. ""pandas.read_csv"" or the URL of the documentation, e.g. ""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mode.html""]

**Note**: You can check the latest versions of the docs on `master` [here](https://pandas.pydata.org/docs/dev/).

#### Documentation problem

[this should provide a description of what documentation you believe needs to be fixed/improved]
... but the DataFrame has two rows. it is not clear which dataframe it refers to.
#### Suggested fix for documentation

[this should explain the suggested fix and **why** it's better than the existing documentation]
... but the resulting DataFrame has two rows."
717056383,36971,DOC: improve description of the example which dataframe has two rows,yonashub,closed,2020-10-08T06:17:46Z,2020-10-11T19:39:17Z,"improve description of the example which dataframe has two rows

- [ ] closes #36970
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
389404557,24206,BUG: reset_index of MultiIndex with CategoricalIndex levels with missing values fails,jorisvandenbossche,closed,2018-12-10T17:21:49Z,2020-10-11T20:01:58Z,"MultiIndex with categorical levels without missing values, this works:

```
In [23]: idx = pd.MultiIndex([pd.CategoricalIndex(['A', 'B']), pd.CategoricalIndex(['a', 'b'])], [[0, 0, 1, 1], [0, 1, 0, 1]])


In [25]: df = pd.DataFrame({'col': range(len(idx))}, index=idx)

In [26]: df
Out[26]:
     col
A a    0
  b    1
B a    2
  b    3

In [28]: df.reset_index()
Out[28]:
  level_0 level_1  col
0       A       a    0
1       A       b    1
2       B       a    2
3       B       b    3
```

Now with a missing value (note the last `-1` in the labels, that's the only difference):
```
In [29]: idx = pd.MultiIndex([pd.CategoricalIndex(['A', 'B']), pd.CategoricalIndex(['a', 'b'])], [[0, 0, 1, 1], [0, 1, 0, -1]])

In [30]: df = pd.DataFrame({'col': range(len(idx))}, index=idx)

In [31]: df
Out[31]:
       col
A a      0
  b      1
B a      2
  NaN    3

In [32]: df.reset_index()
/home/joris/miniconda3/lib/python3.5/site-packages/pandas/core/frame.py:4091: FutureWarning: Interpreting negative values in 'indexer' as missing values.
In the future, this will change to meaning positional indicies
from the right.

Use 'allow_fill=True' to retain the previous behavior and silence this
warning.

Use 'allow_fill=False' to accept the new behavior.
  values = values.take(labels)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/miniconda3/lib/python3.5/site-packages/pandas/core/dtypes/cast.py in maybe_upcast_putmask(result, mask, other)
    249         try:
--> 250             np.place(result, mask, other)
    251         except Exception:

~/miniconda3/lib/python3.5/site-packages/numpy/lib/function_base.py in place(arr, mask, vals)
   2371         raise TypeError(""argument 1 must be numpy.ndarray, ""
-> 2372                         ""not {name}"".format(name=type(arr).__name__))
   2373

TypeError: argument 1 must be numpy.ndarray, not Categorical

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-32-6983677cc901> in <module>()
----> 1 df.reset_index()

~/miniconda3/lib/python3.5/site-packages/pandas/core/frame.py in reset_index(self, level, drop, inplace, col_level, col_fill)
   4136                     name = tuple(name_lst)
   4137                 # to ndarray and maybe infer different dtype
-> 4138                 level_values = _maybe_casted_values(lev, lab)
   4139                 new_obj.insert(0, name, level_values)
   4140

~/miniconda3/lib/python3.5/site-packages/pandas/core/frame.py in _maybe_casted_values(index, labels)
   4092                     if mask.any():
   4093                         values, changed = maybe_upcast_putmask(
-> 4094                             values, mask, np.nan)
   4095             return values
   4096

~/miniconda3/lib/python3.5/site-packages/pandas/core/dtypes/cast.py in maybe_upcast_putmask(result, mask, other)
    250             np.place(result, mask, other)
    251         except Exception:
--> 252             return changeit()
    253
    254     return result, False

~/miniconda3/lib/python3.5/site-packages/pandas/core/dtypes/cast.py in changeit()
    222             # isn't compatible
    223             r, _ = maybe_upcast(result, fill_value=other, copy=True)
--> 224             np.place(r, mask, other)
    225
    226             return r, True

~/miniconda3/lib/python3.5/site-packages/numpy/lib/function_base.py in place(arr, mask, vals)
   2370     if not isinstance(arr, np.ndarray):
   2371         raise TypeError(""argument 1 must be numpy.ndarray, ""
-> 2372                         ""not {name}"".format(name=type(arr).__name__))
   2373
   2374     return _insert(arr, mask, vals)

TypeError: argument 1 must be numpy.ndarray, not Categorical

```"
714493564,36876,BUG: in DataFrame.reset_index() only call maybe_upcast_putmask with ndarrays,arw2019,closed,2020-10-05T04:10:06Z,2020-10-11T20:05:50Z,"- [x] closes #24206
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
718417877,37015,Call finalize in Series.str,TomAugspurger,closed,2020-10-09T20:56:04Z,2020-10-11T20:09:39Z,"xref #28283
"
718747501,37049,REF: use OpsMixin in EAs,jbrockmendel,closed,2020-10-11T03:15:15Z,2020-10-11T20:19:31Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
718717938,37044,REF/TYP: use OpsMixin for DataFrame,jbrockmendel,closed,2020-10-10T22:53:56Z,2020-10-11T21:57:38Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

made possible by #36483"
695507502,36210,API/DEPR: casting bool-indexer to slice in dt64/td64/period,jbrockmendel,closed,2020-09-08T03:03:56Z,2020-10-12T01:23:58Z,"We use special logic for dt64/td64/period dtypes that makes view/copy behavior different from other dtypes:

```
dti = pd.date_range(""2016-01-01"", periods=4, tz=""US/Pacific"")
key = np.array([True, True, False, False])

ser1 = pd.Series(dti._data)
ser2 = pd.Series(range(4))

res1 = ser1[key]
res2 = ser2[key]

>>> res1._values._data.base is None
False
>>> res2._values.base is None
True
```

cc @jorisvandenbossche IIRC you advocated not doing this special casing."
725894271,37285,"Backport PR #37256 on branch 1.1.x (BUG: with integer column labels, .info() throws KeyError )",meeseeksmachine,closed,2020-10-20T19:48:53Z,2020-10-21T10:51:54Z,"Backport PR #37256: BUG: with integer column labels, .info() throws KeyError "
725117892,37270,REGR: Make comparisons consistent for PeriodDtype,dsaxton,closed,2020-10-20T01:59:04Z,2020-10-21T11:38:18Z,"- [x] closes #37265
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
726278744,37303,CI: temporary skip parquet tz test for pyarrow>=2.0.0,jorisvandenbossche,closed,2020-10-21T08:55:05Z,2020-10-21T12:07:28Z,"See https://github.com/pandas-dev/pandas/issues/37286, this PR is not the actual fix, but at least ensures we don't have failing CI in other PRs (I prefer to skip the test instead of pinning pyarrow to <= 1.0, since all other tests are passing on 2.0, which is good to still run)"
726336498,37304,TST: correct parquet test expected partition column dtype for pyarrow 2.0,jorisvandenbossche,closed,2020-10-21T10:10:07Z,2020-10-21T12:25:05Z,"Follow-up on https://github.com/pandas-dev/pandas/pull/35814, but now incorporating the changes in pyarrow 2.0

cc @alimcmaster1 

Also related to https://github.com/pandas-dev/pandas/issues/37286 and https://github.com/pandas-dev/pandas/pull/37296/ (timezone related failures), but doing this fix separate as it's not something that needs to be reverted.
"
719464856,37082,CI: remove xfail for numpy-dev on branch 1.1.x only,simonjayhawkins,closed,2020-10-12T15:22:22Z,2020-10-21T12:36:19Z,"partially reverts pandas-dev/pandas#35537, xref https://github.com/pandas-dev/pandas/issues/35481#issuecomment-705144719"
726397066,37307,Backport PR #37270 on branch 1.1.x: REGR: Make comparisons consistent for PeriodDtype,simonjayhawkins,closed,2020-10-21T11:35:52Z,2020-10-21T12:47:08Z,Backport PR #37270 on branch 1.1.x
712294762,36752,REF: simplify info.py,ivanovmg,closed,2020-09-30T21:31:23Z,2020-10-21T12:55:00Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Enable polymorphism and builder pattern in ``info.py``.
The main benefits:
- Separated data representation from data itself
- More clear construction of each element via builder pattern
- Different builders for various cases (verbose with counts, verbose without counts, non-verbose) enabled one eliminate numerous if statements

May be useful to implement the present PR first and then build ``Series.info`` (https://github.com/pandas-dev/pandas/pull/31796) on top of this.
I do realize that I deleted ``BaseInfo`` class.
Now I see that it is required by the referenced PR, so I can move it back.

**Note:** needed to change test behavior. Here is the reason why.
I noticed inconsistency:
```
>>> df = pd.DataFrame({'long long column': np.random.rand(1000000)})
>>> df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1000000 entries, 0 to 999999
Data columns (total 1 columns):
 #   Column            Non-Null Count    Dtype
---  ------            --------------    -----
 0   long long column  1000000 non-null  float64
dtypes: float64(1)
memory usage: 7.6 MB
```
Here we have two spaces between columns.

However, if we create a dataframe with 10000 columns, then the distance between # col and ""Column"" is only one space.
```
>>> import pandas as pd
>>> import numpy as np
>>> df = pd.DataFrame(np.random.rand(3, 10001))
>>> with open('out.txt', 'w') as buf: df.info(verbose=True, buf=buf)
```

```
$ tail out.txt
 9993  9993    float64
 9994  9994    float64
 9995  9995    float64
 9996  9996    float64
 9997  9997    float64
 9998  9998    float64
 9999  9999    float64
 10000 10000   float64
dtypes: float64(10001)
memory usage: 234.5 KB
```
See, only one space between the first and the second columns.
I find it inconsistent.
So, I changed it, so that there are two spaces everywhere.
What do you think?

I am going to finalize it by documenting and introducing typing where required.
"
726423038,37308,Backport PR #37304 on branch 1.1.x (TST: correct parquet test expected partition column dtype for pyarrow 2.0),meeseeksmachine,closed,2020-10-21T12:13:53Z,2020-10-21T13:20:26Z,Backport PR #37304: TST: correct parquet test expected partition column dtype for pyarrow 2.0
726105276,37300,TST: collect tests by method,jbrockmendel,closed,2020-10-21T03:42:01Z,2020-10-21T14:49:20Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
726043444,37296,CI: tz dtype mismatch in test_parquet following pyarrow upgrade,jbrockmendel,closed,2020-10-21T00:51:44Z,2020-10-21T14:52:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
726053914,37297,CLN: _almost_ always rebox_native following _unbox,jbrockmendel,closed,2020-10-21T01:21:34Z,2020-10-21T15:07:35Z,"A couple more special cases to handle, after which we will always _rebox_native and it can be rolled into _unbox so we can get rid of the method altogether"
716735346,36951,REGR: resample apply fails with KeyError/AttributeError for function that works on DataFrame but fails on Series,jorisvandenbossche,closed,2020-10-07T17:54:35Z,2020-10-21T15:12:56Z,"A resample `apply` accessing a column of the subsetted dataframe fails (bubbling up the error from first trying the function on a single column, i.e. a Series):

```
In [25]: df = pd.DataFrame({""col"": range(10)}, index=pd.date_range(""2012-01-01"", periods=10, freq=""20min""))  

In [26]: df.resample(""H"").apply(lambda group: len(group['col'].unique()))   
...
KeyError: 'col'

In [27]: df.resample(""H"").apply(lambda group: len(group.col.unique())) 
...
AttributeError: 'Series' object has no attribute 'col'
```

This seems a regression, or was there a deliberate change in `apply` to only apply it on the columns and no longer on the full DataFrame ? 
cc @jbrockmendel 
"
725967069,37289,CLN: make PeriodArray signature match DTA/TDA,jbrockmendel,closed,2020-10-20T21:46:25Z,2020-10-21T15:19:09Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
725834476,37281,REF/TST: collect fillna tests,jbrockmendel,closed,2020-10-20T18:22:17Z,2020-10-21T15:20:53Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
725991198,37292,BUG: Problem using to_csv with BytesIO,amotl,closed,2020-10-20T22:35:26Z,2020-10-21T15:52:12Z,"Dear people of Pandas,

first things first: Thanks for all of your excellent work conceiving and maintaining Pandas. You know who you are.

With kind regards,
Andreas.

---

- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
from io import BytesIO

df = pd.DataFrame()
buffer = BytesIO()

df.to_csv(buffer)
```

#### Problem description

The snippet above croaks with
```
TypeError: a bytes-like object is required, not 'str'
```

We expected this to work. However, we will be happy to learn otherwise. We also had a look at #22555 and #35129 which seem to be related but not exactly on the spot.

The background on this is that we are currently in the process of upgrading [Kotori](https://github.com/daq-tools/kotori) to Python 3 (yeah, we are late to the game). However, coming from this, we can confirm it worked when using Pandas 0.18.1 on Python 2 the other day.

The relevant code is
- https://github.com/daq-tools/kotori/blob/0.24.5/kotori/io/protocol/http.py#L8
- https://github.com/daq-tools/kotori/blob/0.24.5/kotori/io/protocol/http.py#L588-L602

Thanks already for looking into this!

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.6.9.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 17.7.0
Version          : Darwin Kernel Version 17.7.0: Thu Jun 18 21:21:34 PDT 2020; root:xnu-4570.71.82.5~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2018.9
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.0
Cython           : None
pytest           : 4.6.9
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.3.6
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.8
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
617186979,34150,CI: Linting errors from flake >= 3.8.1,mgmarino,closed,2020-05-13T06:43:50Z,2020-10-21T15:54:26Z,"The new version of flake bumps the version of pycodestyle, which results in several linting errors on CI.  These include:

- (many instances) E741:ambiguous variable name 'l'
- (many instances) F541:f-string is missing placeholders
- (one or few instances) E721:do not compare types, use 'isinstance()'

Here the output:

```
2020-05-13T05:37:20.8820311Z ##[error]./pandas/_testing.py:630:22:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8824860Z ##[error]./pandas/_testing.py:1006:17:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8825837Z ##[error]./pandas/core/indexing.py:1569:42:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8826437Z ##[error]./pandas/core/series.py:2588:17:F541:f-string is missing placeholders
2020-05-13T05:37:20.8826992Z ##[error]./pandas/core/common.py:40:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8827546Z ##[error]./pandas/core/common.py:280:20:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8828505Z ##[error]./pandas/core/common.py:288:24:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8829122Z ##[error]./pandas/core/frame.py:685:37:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8830045Z ##[error]./pandas/core/indexes/multi.py:992:49:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8830652Z ##[error]./pandas/core/indexes/multi.py:1143:15:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8831243Z ##[error]./pandas/core/indexes/multi.py:1146:29:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8831817Z ##[error]./pandas/core/reshape/reshape.py:143:37:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8832381Z ##[error]./pandas/core/dtypes/concat.py:27:21:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8832963Z ##[error]./pandas/core/internals/concat.py:446:28:E721:do not compare types, use 'isinstance()'
2020-05-13T05:37:20.8833301Z ##[error]./pandas/core/arrays/interval.py:1054:5:E301:expected 1 blank line, found 0
2020-05-13T05:37:20.8833868Z ##[error]./pandas/util/_doctools.py:37:52:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8834428Z ##[error]./pandas/util/_doctools.py:38:48:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8834986Z ##[error]./pandas/util/_doctools.py:40:49:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8835554Z ##[error]./pandas/util/_doctools.py:41:49:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8836113Z ##[error]./pandas/util/_doctools.py:61:35:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8837945Z ##[error]./pandas/util/_doctools.py:76:55:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8838579Z ##[error]./pandas/util/_doctools.py:77:55:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8839206Z ##[error]./pandas/util/_doctools.py:91:17:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8839782Z ##[error]./pandas/plotting/_matplotlib/tools.py:374:9:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8840359Z ##[error]./pandas/plotting/_matplotlib/boxplot.py:147:29:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8840933Z ##[error]./pandas/plotting/_matplotlib/boxplot.py:148:43:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8842322Z ##[error]./pandas/plotting/_matplotlib/core.py:1508:56:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8843347Z ##[error]./pandas/io/pytables.py:4587:59:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8844029Z ##[error]./pandas/io/parsers.py:2971:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8845476Z ##[error]./pandas/io/parsers.py:3000:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8846243Z ##[error]./pandas/io/parsers.py:3020:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8847558Z ##[error]./pandas/io/formats/format.py:979:57:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8848615Z ##[error]./pandas/tests/groupby/test_categorical.py:505:13:F541:f-string is missing placeholders
2020-05-13T05:37:20.8849732Z ##[error]./pandas/tests/groupby/transform/test_numba.py:20:46:F541:f-string is missing placeholders
2020-05-13T05:37:20.8850374Z ##[error]./pandas/tests/groupby/transform/test_numba.py:23:46:F541:f-string is missing placeholders
2020-05-13T05:37:20.8851580Z ##[error]./pandas/tests/groupby/aggregate/test_numba.py:21:46:F541:f-string is missing placeholders
2020-05-13T05:37:20.8852714Z ##[error]./pandas/tests/groupby/aggregate/test_numba.py:24:46:F541:f-string is missing placeholders
2020-05-13T05:37:20.8853337Z ##[error]./pandas/tests/indexes/interval/test_interval.py:56:43:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8853951Z ##[error]./pandas/tests/indexes/interval/test_interval.py:77:17:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8854548Z ##[error]./pandas/tests/indexes/interval/test_constructors.py:333:17:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8855131Z ##[error]./pandas/tests/indexes/multi/test_indexing.py:797:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8855712Z ##[error]./pandas/tests/plotting/test_datetimelike.py:760:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8856293Z ##[error]./pandas/tests/plotting/test_datetimelike.py:787:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8856877Z ##[error]./pandas/tests/plotting/test_datetimelike.py:800:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8857459Z ##[error]./pandas/tests/plotting/test_datetimelike.py:866:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8858037Z ##[error]./pandas/tests/plotting/test_datetimelike.py:884:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8858611Z ##[error]./pandas/tests/plotting/test_datetimelike.py:997:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8859187Z ##[error]./pandas/tests/plotting/test_datetimelike.py:1006:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8859765Z ##[error]./pandas/tests/plotting/test_datetimelike.py:1131:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8860363Z ##[error]./pandas/tests/plotting/test_datetimelike.py:1135:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8860937Z ##[error]./pandas/tests/plotting/test_datetimelike.py:1236:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8861515Z ##[error]./pandas/tests/frame/methods/test_to_dict.py:121:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8862087Z ##[error]./pandas/tests/reshape/test_pivot.py:2069:17:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8862677Z ##[error]./pandas/tests/reshape/merge/test_merge.py:2183:38:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8863272Z ##[error]./pandas/tests/reshape/merge/test_merge.py:2183:69:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8863847Z ##[error]./pandas/tests/io/test_stata.py:1864:35:F541:f-string is missing placeholders
2020-05-13T05:37:20.8864409Z ##[error]./pandas/tests/io/test_sql.py:2402:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8864972Z ##[error]./pandas/tests/io/test_sql.py:2683:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8865545Z ##[error]./pandas/tests/io/formats/test_format.py:1801:31:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8866121Z ##[error]./pandas/tests/extension/test_interval.py:30:32:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8866713Z ##[error]./pandas/tests/indexing/test_floats.py:287:50:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8867287Z ##[error]./pandas/tests/indexing/test_floats.py:346:17:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8867867Z ##[error]./pandas/tests/indexing/test_floats.py:360:17:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8868581Z ##[error]./pandas/tests/indexing/test_floats.py:383:17:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8869229Z ##[error]./pandas/tests/indexing/test_floats.py:407:48:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8869822Z ##[error]./pandas/tests/indexing/test_floats.py:439:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8870388Z ##[error]./pandas/tests/indexing/test_floats.py:455:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8870956Z ##[error]./pandas/tests/indexing/test_floats.py:470:13:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8871524Z ##[error]./pandas/tests/indexing/test_floats.py:492:66:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8872091Z ##[error]./pandas/tests/indexing/test_floats.py:519:32:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8872862Z ##[error]./pandas/tests/indexing/test_loc.py:802:22:E741:ambiguous variable name 'l'
2020-05-13T05:37:20.8873460Z ##[error]./asv_bench/benchmarks/reshape.py:106:36:E741:ambiguous variable name 'l'
```

There is an additional issue with flake8-rst, e.g. here:

- https://gitlab.com/pycqa/flake8/-/issues/641
- https://github.com/kataev/flake8-rst/issues/22

Happy to have a go, but would like suggestions about how to proceed?  Suggestion:

Staged approach:
- [x] Pin flake < 3.8.0 to deal with flake8-rst issue
- [x] Ignore errors to get build green
- [x] Address F541
- [x] Address E721
- [ ] Address E741 (or not?)"
725835112,37282,REF/TST: collect astype tests,jbrockmendel,closed,2020-10-20T18:22:43Z,2020-10-21T16:02:09Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
724241898,37237,DOC: update PeriodArray docstring,arw2019,closed,2020-10-19T04:08:42Z,2020-10-21T18:28:34Z,"AFAICT we currently don't have dedicated pages for these methods in the API reference. 

When we add them we also want add links on the `pd.PeriodArray` [page](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.arrays.PeriodArray.html) (and possibly in other places)"
724680485,37251,DOC: update PeriodArray docstring,arw2019,closed,2020-10-19T14:27:37Z,2020-10-21T18:30:32Z,"- [x] closes #37237 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
720656936,37105,ENH: df.to_parquet() should return bytes,impredicative,closed,2020-10-13T18:33:08Z,2020-10-21T19:14:04Z,"#### Is your feature request related to a problem?

I find it useful to write a parquet to a `bytes` object for some unit tests. The code that I currently use to do this is quite verbose.

To provide some background, `df.to_csv()` (w/o args) just works. It returns a `str` object as is expected. In the same vein, **`df.to_parquet()` (w/o args) should return a `bytes` object.**

More precisely, the current behavior is:
```python
>>> df = pd.DataFrame()

>>> type(df.to_csv())  # This works
<class 'str'>

>>> df.to_parquet() # This should be made to work
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
TypeError: to_parquet() missing 1 required positional argument: 'path'
```

#### Describe the solution you'd like

The requested behavior is:
```python
>>> df = pd.DataFrame()

>>> type(df.to_parquet())
<class 'bytes'>
```

Other uses of `df.to_parquet` should obviously remain unaffected.

#### API breaking implications

It won't break the documented API.

#### Describe alternatives you've considered

I currently use this verbose code to get what I want:
```python
import io

import pandas as pd

df = pd.DataFrame()
pq_file = io.BytesIO()
df.to_parquet(pq_file)
pq_bytes = pq_file.getvalue()
```
This workaround is too effortful."
722000655,37129,ENH: DataFrame.to_parquet() returns bytes if path_or_buf not provided,arw2019,closed,2020-10-15T05:35:54Z,2020-10-21T19:14:15Z,"- [x] closes #37105 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Not sure if this is an API breaking change. Prior to this patch `path` was a required positional argument but it becomes optional here. It is now consistent with, for example, the csv writer."
725027134,37267,BUG: offsets are now unhashable,dhirschfeld,closed,2020-10-19T22:39:36Z,2020-10-22T00:08:33Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---
As discussed on gitter, this was apparently an unindended consequence of https://github.com/pandas-dev/pandas/pull/34227

![image](https://user-images.githubusercontent.com/881019/96518983-56d04a80-12af-11eb-951d-89b02badf9df.png)
https://gitter.im/pydata/pandas?at=5f8d8a106c8d484be294872f

#### Code Sample, a copy-pastable example

```python
>>> import pandas as pd
>>> hash(pd.offsets.Day())
Traceback (most recent call last):
  File ""C:\Users\dhirschf\envs\dev\lib\site-packages\IPython\core\interactiveshell.py"", line 3417, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-46-d78c0d3ee4ad>"", line 1, in <module>
    hash(pd.offsets.Day())
TypeError: unhashable type: 'pandas._libs.tslibs.offsets.Day'
```

#### Problem description

My code relied on the hashability of offsets and so this broke my code and has so far prevented me upgrading to the latest `pandas`

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

```
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.7.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.17763
machine          : AMD64
processor        : Intel64 Family 6 Model 58 Stepping 0, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None
pandas           : 1.1.3
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 50.3.0.post20201006
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : 5.37.3
sphinx           : 3.2.1
blosc            : 1.9.2
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.0
html5lib         : 1.1
pymysql          : 0.10.1
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.4
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.20
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.51.2

```
</details>
"
726700357,37316,DOC: `to_pickle` function takes pickle v5 but doc says only 1-4 supported,ianozsvald,closed,2020-10-21T17:16:06Z,2020-10-22T00:11:03Z,"#### Location of the documentation

* https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_pickle.html
* https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.DataFrame.to_pickle.html (same as stable)

#### Documentation problem

The docstring states ` DataFrame.to_pickle(path, compression='infer', protocol=5, storage_options=None)` noting _protocol 5_ by default, the Parameters following text notes ""The possible values are 0, 1, 2, 3, 4"" which does not include protocol 5. 

#### Suggested fix for documentation

Add ""5"" to the docs: ""The possible values are 0, 1, 2, 3, 4, 5"" which was included from Python 3.8 https://docs.python.org/3/library/pickle.html"
623251023,34315,EHN: Implement closed=left|right|neither in DataFrame.rolling for fixed windows,victorbrjorge,closed,2020-05-22T14:32:06Z,2020-10-22T00:21:11Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

data = [0, 1, 1, 0, 0, 1, 0, 1]
df = pd.DataFrame({'binary_col': data}, index=pd.date_range(start='2020-01-01', freq=""min"", periods=len(data)))

rolling = df.rolling(window=len(df), closed='left', min_periods=1)
rolling.mean()
```

#### Problem description

The rolling window ignores the closed='left' parameter. Since it was supposed to be closed only on the left side, the current right side of the window should not be used on the calculations. But it turns out it is.

#### Original dataframe

| index               | binary_col |
|---------------------|------------|
| 2020-01-01 00:00:00 |          0 |
| 2020-01-01 00:01:00 |          1 |
| 2020-01-01 00:02:00 |          1 |
| 2020-01-01 00:03:00 |          0 |
| 2020-01-01 00:04:00 |          0 |
| 2020-01-01 00:05:00 |          1 |
| 2020-01-01 00:06:00 |          0 |
| 2020-01-01 00:07:00 |          1 |

#### Actual output

| index               | binary_col |
|---------------------|------------|
| 2020-01-01 00:00:00 |   0.000000 |
| 2020-01-01 00:01:00 |   0.500000 |
| 2020-01-01 00:02:00 |   0.666667 |
| 2020-01-01 00:03:00 |   0.500000 |
| 2020-01-01 00:04:00 |   0.400000 |
| 2020-01-01 00:05:00 |   0.500000 |
| 2020-01-01 00:06:00 |   0.428571 |
| 2020-01-01 00:07:00 |   0.500000 |

#### Expected Output

| index               | binary_col |
|---------------------|------------|
| 2020-01-01 00:00:00 |   NaN |
| 2020-01-01 00:01:00 |   0.000000 |
| 2020-01-01 00:02:00 |   0.500000 |
| 2020-01-01 00:03:00 |  0.666667 |
| 2020-01-01 00:04:00 |   0.500000 |
| 2020-01-01 00:05:00 |   0.400000 |
| 2020-01-01 00:06:00 |  0.500000  |
| 2020-01-01 00:07:00 |   0.428571 |

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-70-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : pt_BR.UTF-8

pandas           : 1.0.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 46.2.0.post20200511
Cython           : None
pytest           : 5.4.2
hypothesis       : None
sphinx           : 3.0.3

</details>
"
726922176,37325,"TST/REF: method-specific files for lookup, get_value, set_value",jbrockmendel,closed,2020-10-21T23:07:54Z,2020-10-22T01:12:04Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
726620526,37314,TST/REF: collect stack/unstack tests,jbrockmendel,closed,2020-10-21T15:47:36Z,2020-10-22T01:13:12Z,
726718504,37317,TST/REF: rename test files,jbrockmendel,closed,2020-10-21T17:41:45Z,2020-10-22T01:18:51Z,
726010643,37293,REF: de-duplicate DTA/TDA validators by standardizing exception messages,jbrockmendel,closed,2020-10-20T23:21:40Z,2020-10-22T01:19:21Z,"After this, _validate_setitem_value is the same as _validate_where_value, and we are close to having _validate_insert_value and _validate_searchsorted_value math too."
723844542,37207,ENH: Support closed for fixed windows in rolling,mroeschke,closed,2020-10-17T20:45:53Z,2020-10-22T04:41:30Z,"- [x] closes #34315
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
714253267,36852,REF/CLN: pandas/io/parsers.py,ivanovmg,closed,2020-10-04T07:17:43Z,2020-10-22T07:31:10Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Refactor/cleanup ``pandas/io/parsers.py``
- Extract method _refresh_kwargs_based_on_dialect
- Extract method _validate_skipfooter
- Drop local variable engine_specified
- Clean-up FutureWarning issue
"
726944065,37326,Backport PR #37288 on branch 1.1.x (Regression in offsets caused offsets to be no longer hashable),meeseeksmachine,closed,2020-10-22T00:08:47Z,2020-10-22T07:37:38Z,Backport PR #37288: Regression in offsets caused offsets to be no longer hashable
726054700,37298,CI: Add unwanted pattern check,dsaxton,closed,2020-10-21T01:23:52Z,2020-10-22T08:19:42Z,Follow up to https://github.com/pandas-dev/pandas/pull/37188
727175265,37334,CLN make namespace usage in tests consistent,MarcoGorelli,closed,2020-10-22T08:38:06Z,2020-10-22T09:16:39Z,"Currently, there are many test files where we use the pandas namespace inconsistently: for example, we do
```
import pandas as pd
from pandas import DataFrame
```
and then one test function uses `pd.DataFrame`, the other `DataFrame`.

The goal here is to be consistent: each file should either just contain pd.DataFrame, or DataFrame. Same goes for, e.g. `to_timedelta`, `Series`, ...

Here are the files which have such inconsistencies. If you'd like to work on this, please choose 10-15 files at a time.

**There is no need to ask for permission to work on this.** Just open a pull request, and link to this issue: write `xref #` in the pull request description. I'll do my best to keep the checklist up-to-date.

List of files which may need changing (along with an example of an offending line):

- [ ] pandas/tests/api/test_api.py:238:        assert datetime(2015, 1, 2, 0, 0) == pd.datetime(2015, 1, 2, 0, 0)
- [ ] pandas/tests/arithmetic/conftest.py:85:    Float64Index([nan, inf, inf, inf, inf], dtype='float64')
- [ ] pandas/tests/arithmetic/test_datetime64.py:54:        dti = date_range(""20130101"", periods=3, tz=tz)
- [ ] pandas/tests/arithmetic/test_interval.py:53:def array(left_right_dtypes):
- [ ] pandas/tests/arithmetic/test_numeric.py:138:        expected = pd.TimedeltaIndex([""10s"", ""40s"", ""90s""])
- [ ] pandas/tests/arithmetic/test_object.py:132:            [Timestamp(""2011-01-01""), Timestamp(""2011-01-02""), pd.NaT],
- [ ] pandas/tests/arithmetic/test_period.py:35:        pi = pd.period_range(""2000"", periods=4)
- [ ] pandas/tests/arithmetic/test_timedelta64.py:57:        tdi = pd.timedelta_range(""2H"", periods=4)
- [ ] pandas/tests/arrays/boolean/test_arithmetic.py:12:    return pd.array(
- [ ] pandas/tests/arrays/boolean/test_astype.py:10:    arr = pd.array([True, False, None], dtype=""boolean"")
- [ ] pandas/tests/arrays/boolean/test_comparison.py:12:    return pd.array(
- [ ] pandas/tests/arrays/boolean/test_construction.py:11:    values = np.array([True, False, True, False], dtype=""bool"")
- [ ] pandas/tests/arrays/boolean/test_function.py:13:    a = pd.array([True, False, None], dtype=""boolean"")
- [ ] pandas/tests/arrays/boolean/test_logical.py:14:        a = pd.array([True, False, None], dtype=""boolean"")
- [ ] pandas/tests/arrays/categorical/test_constructors.py:35:        ordered = np.array([0, 1, 2])
- [ ] pandas/tests/arrays/categorical/test_indexing.py:37:        expected = Categorical([""c"", ""b"", ""b"", ""a"", ""a"", ""c"", ""c"", ""c""], ordered=True)
- [ ] pandas/tests/arrays/categorical/test_missing.py:24:        tm.assert_numpy_array_equal(isna(cat), labels == -1)
- [ ] pandas/tests/arrays/categorical/test_operators.py:15:        factor = Categorical([""a"", ""b"", ""b"", ""a"", ""a"", ""c"", ""c"", ""c""], ordered=True)
- [ ] pandas/tests/arrays/floating/test_arithmetic.py:27:    a = pd.array([1.0, 2.0, None, 4.0, 5.0], dtype=dtype)
- [ ] pandas/tests/arrays/floating/test_astype.py:10:    arr = pd.array([0.1, 0.2, None], dtype=""Float64"")
- [ ] pandas/tests/arrays/floating/test_construction.py:11:    a = pd.array([1, None], dtype=pd.Float64Dtype())
- [ ] pandas/tests/arrays/floating/test_function.py:12:    a = pd.array([1, 2, -3, np.nan], dtype=""Float64"")
- [ ] pandas/tests/arrays/floating/test_to_numpy.py:16:    expected = np.array([0.1, 0.2, 0.3], dtype=""object"")
- [ ] pandas/tests/arrays/integer/test_arithmetic.py:21:    a = pd.array([0, 1, None, 3, 4], dtype=dtype)
- [ ] pandas/tests/arrays/integer/test_construction.py:12:    a = pd.array([1, None], dtype=pd.Int64Dtype())
- [ ] pandas/tests/arrays/integer/test_dtypes.py:46:        {""B"": np.array([1.0, 3.0]), ""C"": integer_array([1, 3], dtype=""Int64"")},
- [ ] pandas/tests/arrays/integer/test_function.py:48:    arr = np.array([1, 2, 3, 4])
- [ ] pandas/tests/arrays/interval/test_interval.py:25:        (date_range(""20170101"", periods=3), date_range(""20170102"", periods=3)),
- [ ] pandas/tests/arrays/masked/test_arithmetic.py:11:arrays = [pd.array([1, 2, 3, None], dtype=dtype) for dtype in tm.ALL_EA_INT_DTYPES]
- [ ] pandas/tests/arrays/masked/test_arrow_compat.py:8:arrays = [pd.array([1, 2, 3, None], dtype=dtype) for dtype in tm.ALL_EA_INT_DTYPES]
- [ ] pandas/tests/arrays/sparse/test_array.py:19:        self.arr_data = np.array([np.nan, np.nan, 1, 2, 3, np.nan, 4, 5, np.nan, 6])
- [ ] pandas/tests/arrays/sparse/test_dtype.py:22:    sparse_dtype = SparseDtype(dtype)
- [ ] pandas/tests/arrays/string_/test_string.py:13:    df = pd.DataFrame({""A"": pd.array([""a"", pd.NA, ""b""], dtype=""string"")})
- [ ] pandas/tests/arrays/test_array.py:33:        ([1, 2], object, PandasArray(np.array([1, 2], dtype=object))),
- [ ] pandas/tests/arrays/test_datetimelike.py:677:            arr.take([-1, 1], allow_fill=True, fill_value=pd.Period(""2014Q1""))
- [ ] pandas/tests/arrays/test_datetimes.py:24:        arr = np.array([0, 1, 2, 3], dtype=""M8[h]"").astype(""M8[ns]"")
- [ ] pandas/tests/base/test_conversion.py:126:        i = CategoricalIndex([Timestamp(""1999-12-31""), Timestamp(""2000-12-31"")])
- [ ] pandas/tests/base/test_misc.py:112:    assert Index([1]).item() == 1
- [ ] pandas/tests/base/test_value_counts.py:36:        expected.index = pd.Index(expected.index)
- [ ] pandas/tests/computation/test_eval.py:191:            result = pd.eval(ex, engine=self.engine, parser=self.parser)
- [ ] pandas/tests/dtypes/test_common.py:170:    assert com.is_object_dtype(np.array([], dtype=object))
- [ ] pandas/tests/dtypes/test_dtypes.py:313:        dr = date_range(""20130101"", periods=3, tz=""US/Eastern"")
- [ ] pandas/tests/dtypes/test_inference.py:83:    (Index([1]), True, ""Index""),
- [ ] pandas/tests/dtypes/test_missing.py:67:        assert isna(np.array(np.nan))
- [ ] pandas/tests/extension/arrow/arrays.py:119:            pa.chunked_array([pa.array(result, mask=pd.isna(self._data.to_pandas()))])
- [ ] pandas/tests/extension/base/getitem.py:134:        expected = data[np.array([], dtype=""int64"")]
- [ ] pandas/tests/extension/base/methods.py:33:        data = data[:10].unique()
- [ ] pandas/tests/extension/base/missing.py:13:        result = pd.isna(data_missing)
- [ ] pandas/tests/extension/base/setitem.py:47:        data[np.array([], dtype=int)] = []
- [ ] pandas/tests/extension/decimal/test_decimal.py:72:            # need to convert array([Decimal(NaN)], dtype='object') to np.NaN
- [ ] pandas/tests/extension/list/array.py:44:            if not isinstance(val, self.dtype.type) and not pd.isna(val):
- [ ] pandas/tests/extension/test_boolean.py:36:    return pd.array(make_data(), dtype=dtype)
- [ ] pandas/tests/extension/test_categorical.py:51:    return Categorical(make_data())
- [ ] pandas/tests/extension/test_floating.py:44:    return pd.array(make_data(), dtype=dtype)
- [ ] pandas/tests/extension/test_numpy.py:48:        return PandasArray(np.array([np.nan, (1,)], dtype=object))
- [ ] pandas/tests/extension/test_sparse.py:29:    return SparseDtype()
- [ ] pandas/tests/frame/apply/test_frame_apply.py:67:                ""A"": date_range(""20130101"", periods=3),
- [ ] pandas/tests/frame/indexing/test_categorical.py:17:        labels = Categorical([f""{i} - {i + 499}"" for i in range(0, 10000, 500)])
- [ ] pandas/tests/frame/indexing/test_indexing.py:65:        msg = ""\""None of [Index(['baf'], dtype='object')] are in the [columns]\""""
- [ ] pandas/tests/frame/indexing/test_sparse.py:50:        expected = np.full(cols, SparseDtype(dtype, fill_value=0))
- [ ] pandas/tests/frame/indexing/test_xs.py:59:        # no columns but Index(dtype=object)
- [ ] pandas/tests/frame/methods/test_align.py:89:        tm.assert_index_equal(bf.index, Index([]))
- [ ] pandas/tests/frame/methods/test_append.py:132:        df1 = DataFrame({""bar"": Timestamp(""20130101"")}, index=range(5))
- [ ] pandas/tests/frame/methods/test_cov_corr.py:73:        ""other_column"", [pd.array([1, 2, 3]), np.array([1.0, 2.0, 3.0])]
- [ ] pandas/tests/frame/methods/test_describe.py:249:        start = Timestamp(2018, 1, 1)
- [ ] pandas/tests/frame/methods/test_diff.py:78:        dti = pd.date_range(""2016-01-01"", periods=4, tz=tz)
- [ ] pandas/tests/frame/methods/test_drop.py:90:        expected = Index([""a"", ""b"", ""c""], name=""first"")
- [ ] pandas/tests/frame/methods/test_quantile.py:247:            [Timestamp(""2010-07-02 12:00:00""), 2.5], index=[""a"", ""b""], name=0.5
- [ ] pandas/tests/frame/methods/test_replace.py:964:                Timestamp(""20130102"", tz=""US/Eastern""),
- [ ] pandas/tests/frame/methods/test_reset_index.py:42:        expected[""idx""] = expected[""idx""].apply(lambda d: Timestamp(d, tz=tz))
- [ ] pandas/tests/frame/methods/test_sort_values.py:243:        df = DataFrame({""x"": pd.Categorical(np.repeat([1, 2, 3, 4], 5), ordered=True)})
- [ ] pandas/tests/frame/test_analytics.py:68:        df = DataFrame({""b"": date_range(""1/1/2001"", periods=2)})
- [ ] pandas/tests/frame/test_constructors.py:94:            ([[]], RangeIndex(1), RangeIndex(0)),
- [ ] pandas/tests/frame/test_dtypes.py:62:                ""A"": date_range(""20130101"", periods=3),
- [ ] pandas/tests/frame/test_query_eval.py:46:        result = df.eval(""A+1"")
- [ ] pandas/tests/frame/test_reshape.py:64:        tm.assert_index_equal(result.columns, Index([""A"", ""B""], name=0))
- [ ] pandas/tests/frame/test_to_csv.py:40:    def read_csv(self, path, **kwargs):
- [ ] pandas/tests/generic/test_finalize.py:35:        (np.array([0], dtype=""float64"")),
- [ ] pandas/tests/generic/test_generic.py:792:        df1[""start""] = date_range(""2000-1-1"", periods=10, freq=""T"")
- [ ] pandas/tests/generic/test_series.py:65:        o = Series(date_range(""20130101"", periods=3))
- [ ] pandas/tests/groupby/aggregate/test_aggregate.py:135:    exp = Series([], dtype=np.float64, index=pd.Index([], dtype=np.float64))
- [ ] pandas/tests/groupby/aggregate/test_other.py:109:            ""time"": date_range(""1/1/2011"", periods=8, freq=""H""),
- [ ] pandas/tests/groupby/test_apply.py:43:    exp_idx = pd.Index(
- [ ] pandas/tests/groupby/test_categorical.py:88:    cats = Categorical(
- [ ] pandas/tests/groupby/test_counting.py:240:            index=Index([0.0, 3.0, 4.0, 5.0], name=""B""), data={""A"": [1, 1, 1, 1]}
- [ ] pandas/tests/groupby/test_function.py:66:    exp_df = DataFrame([exp] * 2, columns=[""val""], index=Index([""a"", ""b""], name=""key""))
- [ ] pandas/tests/groupby/test_groupby.py:19:    result = repr(pd.Grouper(key=""A"", level=""B""))
- [ ] pandas/tests/groupby/test_grouping.py:159:        dates = date_range(d0, date.today())
- [ ] pandas/tests/groupby/test_missing.py:14:        columns=pd.Index([""type"", ""a"", ""b""], name=""idx""),
- [ ] pandas/tests/groupby/test_nth.py:14:    expected.index = Index([""bar"", ""foo""], name=""A"")
- [ ] pandas/tests/groupby/test_pipe.py:35:    index = Index([""bar"", ""foo""], dtype=""object"", name=""A"")
- [ ] pandas/tests/groupby/test_quantile.py:48:        [a_expected, b_expected], columns=[""val""], index=Index([""a"", ""b""], name=""key"")
- [ ] pandas/tests/groupby/test_timegrouper.py:54:                index=date_range(
- [ ] pandas/tests/groupby/transform/test_transform.py:103:            ""d"": pd.date_range(""2014-1-1"", ""2014-1-4""),
- [ ] pandas/tests/indexes/base_class/test_reshape.py:14:        index = pd.Index([1, 2, 3])
- [ ] pandas/tests/indexes/base_class/test_setops.py:15:        idx1 = pd.Index([""a"", ""b""])
- [ ] pandas/tests/indexes/categorical/test_category.py:24:        return CategoricalIndex(list(""aabbca""), categories=categories, ordered=ordered)
- [ ] pandas/tests/indexes/categorical/test_formats.py:12:        idx = pd.CategoricalIndex([""a"", ""bb"", ""ccc""])
- [ ] pandas/tests/indexes/categorical/test_indexing.py:14:        idx = pd.CategoricalIndex([1, 2, 3], name=""xxx"")
- [ ] pandas/tests/indexes/categorical/test_map.py:20:        index = CategoricalIndex(data, categories=categories, ordered=ordered)
- [ ] pandas/tests/indexes/common.py:96:            result = pd.Index(expected)
- [ ] pandas/tests/indexes/datetimes/test_astype.py:24:        idx = DatetimeIndex([""2016-05-16"", ""NaT"", NaT, np.NaN], name=""idx"")
- [ ] pandas/tests/indexes/datetimes/test_constructors.py:30:            dt_cls([pd.NaT, pd.Timestamp(""2011-01-01"")], freq=""D"")
- [ ] pandas/tests/indexes/datetimes/test_date_range.py:32:        rng = date_range(""20090415"", ""20090519"", tz=""US/Eastern"")
- [ ] pandas/tests/indexes/datetimes/test_datetime.py:17:        index = date_range(""20130101"", periods=3, tz=""US/Eastern"")
- [ ] pandas/tests/indexes/datetimes/test_indexing.py:20:        idx = pd.date_range(
- [ ] pandas/tests/indexes/datetimes/test_misc.py:17:        idx = pd.date_range(
- [ ] pandas/tests/indexes/datetimes/test_ops.py:44:        rng = date_range(""1/1/2000"", ""1/1/2001"")
- [ ] pandas/tests/indexes/datetimes/test_setops.py:57:        rng1 = pd.date_range(""1/1/2000"", freq=""D"", periods=5, tz=tz)
- [ ] pandas/tests/indexes/datetimes/test_shift.py:23:        idx = pd.DatetimeIndex([], name=""xxx"", tz=tz)
- [ ] pandas/tests/indexes/datetimes/test_timezones.py:55:        idx = DatetimeIndex(dates)
- [ ] pandas/tests/indexes/interval/test_interval.py:57:        ivs = [Interval(l, r, closed) for l, r in zip(range(10), range(1, 11))]
- [ ] pandas/tests/indexes/multi/test_analytics.py:35:    major_axis = Index(list(range(4)))
- [ ] pandas/tests/indexes/multi/test_constructors.py:21:    expected = Index([""foo"", ""bar"", ""baz"", ""qux""], name=""first"")
- [ ] pandas/tests/indexes/multi/test_get_level_values.py:22:    expected = Index([""foo"", ""foo"", ""bar"", ""baz"", ""qux"", ""qux""], name=""first"")
- [ ] pandas/tests/indexes/multi/test_get_set.py:289:    expected = pd.MultiIndex(levels=[[0, 1]], codes=[[0, 1]], names=[""first""])
- [ ] pandas/tests/indexes/multi/test_indexing.py:67:        index = MultiIndex(
- [ ] pandas/tests/indexes/multi/test_integrity.py:161:    # isna(MI)
- [ ] pandas/tests/indexes/multi/test_join.py:10:    ""other"", [Index([""three"", ""one"", ""two""]), Index([""one""]), Index([""one"", ""three""])]
- [ ] pandas/tests/indexes/multi/test_missing.py:42:    idx = MultiIndex(
- [ ] pandas/tests/indexes/multi/test_monotonic.py:37:    i = MultiIndex(
- [ ] pandas/tests/indexes/multi/test_sorting.py:178:    mi = MultiIndex(
- [ ] pandas/tests/indexes/period/test_formats.py:117:        exp1 = """"""Series([], dtype: period[D])""""""
- [ ] pandas/tests/indexes/period/test_indexing.py:28:        idx = period_range(""2011-01-01"", ""2011-01-31"", freq=""D"", name=""idx"")
- [ ] pandas/tests/indexes/period/test_ops.py:32:        idx = PeriodIndex(np.repeat(idx._values, range(1, len(idx) + 1)), freq=""H"")
- [ ] pandas/tests/indexes/ranges/test_indexing.py:11:        index = RangeIndex(start=0, stop=20, step=2)
- [ ] pandas/tests/indexes/ranges/test_range.py:108:        expected = pd.Index([0, pd.NaT, 1, 2, 3, 4], dtype=object)
- [ ] pandas/tests/indexes/test_base.py:50:        return Index(list(""abcde""))
- [ ] pandas/tests/indexes/test_numeric.py:42:        expected = Float64Index(arr)
- [ ] pandas/tests/indexes/timedeltas/test_astype.py:21:        idx = timedelta_range(start=""1 days"", periods=4, freq=""D"", name=""idx"")
- [ ] pandas/tests/indexes/timedeltas/test_constructors.py:17:            TimedeltaIndex([1, 3, 7], unit)
- [ ] pandas/tests/indexes/timedeltas/test_formats.py:46:        exp1 = """"""Series([], dtype: timedelta64[ns])""""""
- [ ] pandas/tests/indexes/timedeltas/test_indexing.py:15:        idx = timedelta_range(""1 day"", ""31 day"", freq=""D"", name=""idx"")
- [ ] pandas/tests/indexes/timedeltas/test_ops.py:16:        idx = timedelta_range(""1 days 09:00:00"", freq=""H"", periods=10)
- [ ] pandas/tests/indexes/timedeltas/test_scalar_compat.py:19:        rng = timedelta_range(""1 days, 10:11:12.100123456"", periods=2, freq=""s"")
- [ ] pandas/tests/indexes/timedeltas/test_setops.py:14:        i1 = timedelta_range(""1day"", periods=5)
- [ ] pandas/tests/indexes/timedeltas/test_shift.py:17:        idx = pd.TimedeltaIndex([], name=""xxx"")
- [ ] pandas/tests/indexing/multiindex/test_multiindex.py:63:                Index([""R1"", ""R2"", np.nan, ""R4""], name=""a""),
- [ ] pandas/tests/indexing/test_categorical.py:101:        exp = np.array([4, 3, 2, 1], dtype=np.int64)
- [ ] pandas/tests/indexing/test_check_indexer.py:13:        ([1, 2], np.array([1, 2], dtype=np.intp)),
- [ ] pandas/tests/indexing/test_datetime.py:18:        idx = date_range(""20010101"", periods=4, tz=""UTC"")
- [ ] pandas/tests/indexing/test_iloc.py:238:        df = concat([df1, df2], axis=1)
- [ ] pandas/tests/indexing/test_indexing.py:39:            df.loc[df.index[2:5], ""bar""] = np.array([2.33j, 1.23 + 0.1j, 2.2, 1.0])
- [ ] pandas/tests/indexing/test_loc.py:332:        expected = df.loc[np.array(mask)]
- [ ] pandas/tests/internals/test_internals.py:108:        values = Categorical([1, 1, 2, 2, 3, 3, 3, 3, 4, 4])
- [ ] pandas/tests/io/excel/test_readers.py:842:        si = Index(
- [ ] pandas/tests/io/excel/test_xlrd.py:30:        with ExcelFile(book, engine=engine) as xl:
- [ ] pandas/tests/io/formats/test_format.py:185:        with option_context(
- [ ] pandas/tests/io/formats/test_printing.py:112:        with cf.option_context(""display.unicode.ambiguous_as_wide"", True):
- [ ] pandas/tests/io/formats/test_to_html.py:197:    with option_context(""display.multi_sparse"", multi_sparse):
- [ ] pandas/tests/io/json/test_pandas.py:72:        result = read_json(df.to_json(orient=orient), orient=orient)
- [ ] pandas/tests/io/json/test_readlines.py:21:    result = read_json('{""a"": 1, ""b"": 2}\n{""b"":2, ""a"" :1}\n', lines=True)
- [ ] pandas/tests/io/parser/test_c_parser_only.py:35:        parser.read_csv(StringIO(malformed))
- [ ] pandas/tests/io/parser/test_parse_dates.py:448:                Timestamp(""05/31/2012, 15:30:00.029""),
- [ ] pandas/tests/io/parser/test_read_fwf.py:30:    result = read_fwf(StringIO(data))
- [ ] pandas/tests/io/pytables/test_store.py:69:            with HDFStore(path) as store:
- [ ] pandas/tests/io/pytables/test_timezones.py:47:                    Timestamp(""20130102 2:00:00"", tz=gettz(""US/Eastern""))
- [ ] pandas/tests/io/test_clipboard.py:152:        result = read_clipboard(sep=sep or ""\t"", index_col=0, encoding=encoding)
- [ ] pandas/tests/io/test_feather.py:40:            result = read_feather(path, **read_kwargs)
- [ ] pandas/tests/io/test_parquet.py:191:                actual = read_parquet(path, **read_kwargs)
- [ ] pandas/tests/io/test_sql.py:607:        iris_frame = sql.read_sql_query(""SELECT * FROM iris"", self.conn)
- [ ] pandas/tests/io/test_stata.py:52:    parsed_114 = read_stata(dta14_114, convert_dates=True)
- [ ] pandas/tests/plotting/test_frame.py:508:        idx = date_range(start=""2014-07-01"", freq=""M"", periods=10)
- [ ] pandas/tests/reductions/test_reductions.py:58:            expected = pd.Period(ordinal=getattr(obj.asi8, opname)(), freq=obj.freq)
- [ ] pandas/tests/resample/test_datetime_index.py:44:    b = Grouper(freq=Minute(5))
- [ ] pandas/tests/resample/test_deprecated.py:43:    idx = pd.date_range(""2001-01-01"", periods=4, freq=""T"")
- [ ] pandas/tests/resample/test_period_index.py:45:            new_index = date_range(start=start, end=end, freq=freq, closed=""left"")
- [ ] pandas/tests/resample/test_resample_api.py:11:dti = date_range(start=datetime(2005, 1, 1), end=datetime(2005, 1, 10), freq=""Min"")
- [ ] pandas/tests/resample/test_resampler_grouper.py:16:    index=date_range(""1/1/2000"", freq=""s"", periods=40),
- [ ] pandas/tests/resample/test_time_grouper.py:13:test_series = Series(np.random.randn(1000), index=date_range(""1/1/2000"", periods=1000))
- [ ] pandas/tests/resample/test_timedelta.py:17:        index=timedelta_range(""0 day"", periods=4, freq=""1T""),
- [ ] pandas/tests/reshape/merge/test_join.py:412:        ex_index = Index(index1.values).union(Index(index2.values))
- [ ] pandas/tests/reshape/merge/test_merge.py:127:        result = pd.merge(df_empty, df_a, left_index=True, right_index=True)
- [ ] pandas/tests/reshape/merge/test_merge_asof.py:21:        x.time = to_datetime(x.time)
- [ ] pandas/tests/reshape/merge/test_multi.py:110:        expected = pd.merge(left, right.reset_index(), on=on_cols, how=join_type)
- [ ] pandas/tests/reshape/test_concat.py:50:            pd.Timestamp(""2011-01-01""),
- [ ] pandas/tests/reshape/test_cut.py:29:    result = cut(data, 4, labels=False)
- [ ] pandas/tests/reshape/test_melt.py:29:        result = melt(self.df)
- [ ] pandas/tests/reshape/test_pivot.py:31:    return Categorical(pd.IntervalIndex.from_arrays(left, right, closed))
- [ ] pandas/tests/reshape/test_qcut.py:127:    exp_levels = np.array(
- [ ] pandas/tests/reshape/test_union_categoricals.py:42:                result = union_categoricals([box(Categorical(a)), box(Categorical(b))])
- [ ] pandas/tests/scalar/period/test_period.py:22:        i1 = Period(""1/1/2005"", freq=""M"")
- [ ] pandas/tests/scalar/test_na_scalar.py:102:        value = np.array([value])
- [ ] pandas/tests/scalar/timedelta/test_arithmetic.py:29:            Timedelta(10, unit=""s""),
- [ ] pandas/tests/scalar/timestamp/test_unary_ops.py:327:        # datetime.timestamp() converts in the local timezone
- [ ] pandas/tests/series/apply/test_series_apply.py:28:        s = Series(dtype=object, name=""foo"", index=pd.Index([], name=""bar""))
- [ ] pandas/tests/series/indexing/test_datetime.py:29:    assert s[Timestamp(datetime(2009, 1, 2))] == 48
- [ ] pandas/tests/series/indexing/test_indexing.py:73:        pd.date_range(""2011-01-01"", periods=3, tz=""US/Eastern""), index=[""a"", ""b"", ""c""]
- [ ] pandas/tests/series/indexing/test_where.py:182:        [Timestamp(""2017-01-01""), pd.NaT, Timestamp(""2017-01-02"")],
- [ ] pandas/tests/series/methods/test_interpolate.py:552:            [1, np.nan, 3], index=date_range(""1/1/2000"", periods=3, tz=tz_naive_fixture)
- [ ] pandas/tests/series/methods/test_quantile.py:28:        assert q == Timestamp(""2000-01-10 19:12:00"")
- [ ] pandas/tests/series/methods/test_shift.py:76:        index = date_range(""2000-01-01"", periods=5)
- [ ] pandas/tests/series/methods/test_truncate.py:76:        rng = pd.date_range(""2011-01-01"", ""2012-01-01"", freq=""W"")
- [ ] pandas/tests/series/methods/test_value_counts.py:88:        values = pd.Categorical([1, 2, 3, 1, 1, 3], ordered=True)
- [ ] pandas/tests/series/test_api.py:158:            [x[1] for x in _d], index=pd.Index([x[0] for x in _d], tupleize_cols=False)
- [ ] pandas/tests/series/test_arithmetic.py:244:        dt = Series(date_range(""2012-1-1"", periods=3, freq=""D""))
- [ ] pandas/tests/series/test_constructors.py:137:        # these are Index() and RangeIndex() which don't compare type equal
- [ ] pandas/tests/series/test_datetime_values.py:85:            Series(date_range(""20130101"", periods=5), name=""xxx""),
- [ ] pandas/tests/series/test_io.py:15:    def read_csv(self, path, **kwargs):
- [ ] pandas/tests/series/test_logical_ops.py:164:        result = left & pd.Index(right)
- [ ] pandas/tests/series/test_missing.py:215:        tm.assert_series_equal(result, ts[pd.notna(ts)])
- [ ] pandas/tests/series/test_period.py:19:            [pd.Period(""2011-01-01"", freq=""D""), pd.Period(""2011-02-01"", freq=""D"")]
- [ ] pandas/tests/series/test_repr.py:169:        with pd.option_context(""max_rows"", None):
- [ ] pandas/tests/series/test_timeseries.py:19:        rng = date_range(""1/1/2000"", ""3/1/2000"", freq=""B"")
- [ ] pandas/tests/series/test_ufunc.py:246:    arr = np.array([Dummy(0), Dummy(1)])
- [ ] pandas/tests/test_algos.py:43:        codes, uniques = algos.factorize([""a"", ""b"", ""b"", ""a"", ""a"", ""c"", ""c"", ""c""])
- [ ] pandas/tests/test_multilevel.py:307:            index=pd.Index([2], name=""ix1""),
- [ ] pandas/tests/test_strings.py:189:    values = np.array(values, dtype=object)  # object dtype to avoid casting
- [ ] pandas/tests/tools/test_to_datetime.py:43:        results1 = [Timestamp(""20000101""), Timestamp(""20000201""), Timestamp(""20000301"")]
- [ ] pandas/tests/tools/test_to_numeric.py:58:    result = to_numeric(ser, **input_kwargs)
- [ ] pandas/tests/tools/test_to_timedelta.py:14:        result = to_timedelta(["""", """"])
- [ ] pandas/tests/window/test_expanding.py:53:    e = Expanding(Series([2, 4, 6]), window=2)
- [ ] pandas/tests/window/test_rolling.py:81:        {""value"": np.arange(n)}, index=pd.date_range(""2015-12-24"", periods=n, freq=""D"")

----

See #37335 for an example pull request"
727199476,37335,CLN make namespace usage in tests consistent,MarcoGorelli,closed,2020-10-22T09:09:17Z,2020-10-22T09:18:56Z,Example pull request for #37334
723533728,37171,CLN: ensure we pass correct type to DTI/TDI shallow_copy,jbrockmendel,closed,2020-10-16T21:10:21Z,2020-10-22T12:02:57Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
726902225,37322,DOC: Add protocol value '5' to pickle #37316,botplex,closed,2020-10-21T22:18:57Z,2020-10-22T16:04:19Z,"- [X] closes #37316
"
727529588,37341,1.1.x,botplex,closed,2020-10-22T16:11:39Z,2020-10-22T16:15:02Z,"- [X] closes #37316

"
727237950,37337,[WIP] Arrow string array: Common base class,simonjayhawkins,closed,2020-10-22T09:57:56Z,2020-10-22T19:14:35Z,CI testing alternative to #37336
727200741,37336,[WIP] Arrow string array: Split array and dtype,simonjayhawkins,closed,2020-10-22T09:10:43Z,2020-10-22T19:14:47Z,CI testing before considering including in #35259
727313875,37338,CI: test_binary_arith_ops failing in travis-38-slow,simonjayhawkins,closed,2020-10-22T11:46:37Z,2020-10-22T19:36:25Z,"```
=========================== short test summary info ============================
FAILED pandas/tests/computation/test_eval.py::TestEvalNumexprPandas::test_binary_arith_ops
= 1 failed, 14730 passed, 11 skipped, 1 xfailed, 37 warnings in 672.01s (0:11:12) =
```

e.g. https://travis-ci.org/github/pandas-dev/pandas/jobs/737965996"
724083306,37226,DOC: Fix typos and broken formatting,sahidvelji,closed,2020-10-18T19:45:07Z,2020-10-22T21:02:51Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This PR fixes minor typos in the getting started tutorial [How to manipulate textual data?](https://pandas.pydata.org/pandas-docs/stable/getting_started/intro_tutorials/10_text_data.html). I built the documentation to confirm that I fixed the formatting of the Wikipedia link. I also fixed the inconsistent use of ""element-wise"" versus ""element wise""."
725955407,37288,Regression in offsets caused offsets to be no longer hashable,phofl,closed,2020-10-20T21:25:29Z,2020-10-22T21:42:02Z,"- [x] closes #37267
- [x] tests added / passed Not quite sure, if these test is sufficient, if yes, I would tests for other functions.
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
726920490,37324,TST/REF: collect _libs tests,jbrockmendel,closed,2020-10-21T23:03:25Z,2020-10-22T22:16:39Z,
727545731,37342,TST/REF: collect tests by method,jbrockmendel,closed,2020-10-22T16:32:45Z,2020-10-22T22:18:51Z,
723878093,37213,QST: How to handle Index._id in an Index subclass?,spencerkclark,closed,2020-10-18T00:53:38Z,2020-10-22T23:31:02Z,"- [ ] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

In xarray we make use of an `Index` subclass for datetime-like indexing with non-standard calendars, which we call `CFTimeIndex`.  A recent change, #37087, led to some test failures on our end.  We currently do not define an `_id` attribute on our subclass, so we are vulnerable to the kinds of attribute errors illustrated below (note this example requires the [xarray](https://github.com/pydata/xarray) and [cftime](https://github.com/Unidata/cftime) libraries be installed, with the development version of pandas):

```
In [1]: import xarray as xr

In [2]: xr.cftime_range(""2000"", periods=2).view()
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-2-5226cd5e3bc1> in <module>
----> 1 xr.cftime_range(""2000"", periods=2).view()

~/Software/pandas/pandas/core/indexes/base.py in view(self, cls)
    630             result = self._shallow_copy()
    631         if isinstance(result, Index):
--> 632             result._id = self._id
    633         return result
    634

AttributeError: 'CFTimeIndex' object has no attribute '_id'
```

Where possible, we try to avoid using private methods/attributes on our subclass.  In this case would you recommend we go against that by setting the `_id` attribute, e.g. by using `_reset_identity` in our constructor, or would it be possible to continue to have `_id` be somewhat of an optional attribute of `Index` objects in pandas?
"
727488962,37339,TST: parametrize slow test,jbrockmendel,closed,2020-10-22T15:21:28Z,2020-10-22T23:48:29Z,
726738626,37319,TST/REF: misplaced frame.indexing tests,jbrockmendel,closed,2020-10-21T17:59:06Z,2020-10-22T23:49:11Z,I've been saving the indexing tests for last-ish because they are some of the hardest to organize consistently.  This starts in on that.
726648319,37315,TST/REF: collect tests by method,jbrockmendel,closed,2020-10-21T16:06:01Z,2020-10-22T23:49:49Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
727728397,37349,TST: fixturize/parametrize test_eval to the bone,jbrockmendel,closed,2020-10-22T21:19:39Z,2020-10-22T23:50:18Z,Might help us track down the failing test there.
727668921,37347,CI: xfail test_binary_arith_ops,jbrockmendel,closed,2020-10-22T19:37:39Z,2020-10-22T23:50:48Z,"xref #37328, #37338"
724122166,37227,CLN: Simplify gathering of results in aggregate,rhshadrach,closed,2020-10-18T21:14:48Z,2020-10-22T23:54:28Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Reduces the number of paths when collecting results in `aggregation.aggregate`; one where we are gathering NDFrames using `concat` and the other where we are gathering scalars using `Series`.

The reason for the one test change is as follows. In one case, we previously gathered results using `DataFrame`. When this occurs and the indexes are not all equal, `DataFrame` will sort the index whereas `concat` will have the index in order of appearance. For example with

```
df = DataFrame(
    {
        'A': pd.Series([1, 2], index=['b', 'a']),
        'B': pd.Series([3, 4], index=['c', 'a'])
    }
)
```

gives 

```
     A    B
a  2.0  4.0
b  1.0  NaN
c  NaN  3.0
```

whereas using `concat` instead of `DataFrame` on the first line with `axis=1` gives:

```
     A    B
b  1.0  NaN
a  2.0  4.0
c  NaN  3.0
```

If in this example you replace the 2nd index with ['b', 'a'] (so that they are equal), then both `Dataframe` and `concat` will produce the same result with index `['b', 'a']`. If on the other hand you replace the 2nd index with `['a', 'b']`, then `DataFrame` will result in index `['a', 'b']` whereas concat will result in index `['b', 'a']`."
715355212,36910,BUG: Joining data frames with MultiIndex results in non-deterministic level order.,theemathas,closed,2020-10-06T05:37:57Z,2020-10-23T00:20:33Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

foo = pd.DataFrame(data={'e': 5}, index=pd.MultiIndex.from_tuples([(1, 2, 4)], names=('a', 'b', 'd')))
bar = pd.DataFrame(data={'f': 6}, index=pd.MultiIndex.from_tuples([(2, 3)], names=('b', 'c')))
joined = foo.join(bar, how='inner')
print(foo.join(bar))
```

#### Expected Output

Something deterministic

#### Produced Output

When running the code multiple times (restart the python interpreter), the output switches (apparently randomly) between these two outputs:

```
         e  f
b d a c
2 4 1 3  5  6
```

```
         e  f
b a d c
2 1 4 3  5  6
```

#### Problem description

When joining with a data frame with a MultiIndex, the order of the levels in the resulting data frame is non-deterministic. This is very surprising, and resulted in a hard-to-debug issue in my code. This issue happens with both inner joins and left joins.

Possibly related issues: #36909, #25760

Past discussion on non-determinism in pandas: #32514, #32449, #12679

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : English_United States.1252

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 50.3.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
```

</details>
"
723778457,37194,BUG: .loc with MultiIndex with names[1] = 0,jbrockmendel,closed,2020-10-17T15:05:46Z,2020-10-23T00:54:28Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
727822996,37352,CI: Fix test file lint,dsaxton,closed,2020-10-23T01:25:57Z,2020-10-23T02:12:31Z,Should help fix red CI on master
727830145,37353,TST: collect indexing tests by method,jbrockmendel,closed,2020-10-23T01:50:05Z,2020-10-23T04:04:28Z,
724434890,37240,CI move non-standard-import checks over to pre-commit,MarcoGorelli,closed,2020-10-19T09:20:36Z,2020-10-23T07:17:22Z,"Like this they're cross-platform, provide faster feedback to devs, and can use pre-commit `pygrep` instead of the custom `invgrep`"
725819917,37280,"BUG: regression: pandas.read_stata(filename, iterator=True) raises ValueError",remram44,closed,2020-10-20T18:00:15Z,2020-10-23T12:15:12Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
>>> import pandas
>>> # for example https://gitlab.com/ViDA-NYU/datamart/datamart/-/blob/master/tests/data/stata118.dta
>>> iterator = pandas.read_stata(stata_file_name, iterator=True)
>>> list(iterator)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""site-packages/pandas/io/stata.py"", line 1523, in __next__
    raise ValueError(
ValueError: chunksize must be set to a positive integer to use as an iterator.
```

#### Problem description

`read_stata(filename, iterator=True)` no longer works in pandas 1.1.3. **It worked in pandas 1.0.5.**

#### Expected Output

DataFrame is loaded correctly

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-51-generic
Version          : #56-Ubuntu SMP Mon Oct 5 14:28:49 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 44.0.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.14.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.4
fastparquet      : None
gcsfs            : 0.7.1
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.3
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None
```

</details>
"
726249342,37302,BUG: Allow empty chunksize in stata reader when using iterator,bashtage,closed,2020-10-21T08:15:17Z,2020-10-23T12:35:48Z,"- [X] closes #37280 
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
357865287,22624,Refactor test_sql.py,alimcmaster1,closed,2018-09-06T23:27:53Z,2020-10-23T15:21:14Z,"Off the back of discussion in this [PR](https://github.com/pandas-dev/pandas/pull/22515) we should consider refactoring [test_sql.py](https://github.com/pandas-dev/pandas/blob/master/pandas/tests/io/test_sql.py) to use just [PyTest Fixtures](https://docs.pytest.org/en/latest/fixture.html) opposed to fixures and the [old xunit](https://docs.pytest.org/en/latest/xunit_setup.html) style setup.  

Thanks,

Alistair"
728088231,37360,TST: moved file test_concat.py to folder ./concat/ (#37243),kamilt5,closed,2020-10-23T10:05:18Z,2020-10-23T15:27:44Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
727498082,37340,TYP: is_dtype_compat,jbrockmendel,closed,2020-10-22T15:32:20Z,2020-10-23T15:37:31Z,Broken off from #37224
727814959,37351,CI/CLN: Add more test file linting,dsaxton,closed,2020-10-23T01:00:41Z,2020-10-23T17:05:23Z,Fixing use of pd.Index when Index is imported directly
727842186,37354,"TST/REF: collect tests by method, some misplaced",jbrockmendel,closed,2020-10-23T02:26:45Z,2020-10-23T18:06:47Z,Looking to get rid of some more of the not-obviously-scoped test files
727897780,37356,API: require timezone match in DatetimeArray.take,jbrockmendel,closed,2020-10-23T04:23:07Z,2020-10-23T18:07:47Z,Consistent with #37299
728471986,37368,TST: split up test_concat.py #37243 - follows up,kamilt5,closed,2020-10-23T19:31:48Z,2020-10-23T20:42:21Z,"* created test_datetime.py and
  split datetime/timezone/period related tests

- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
728617214,37375,CI: troubleshoot windows parallel tests,jbrockmendel,closed,2020-10-24T00:46:31Z,2020-10-24T01:07:38Z,
168101167,13828,to_html ignores na_rep when float_format set,paulperry,closed,2016-07-28T13:53:34Z,2020-10-24T02:49:08Z,"#### Code Sample, a copy-pastable example if possible

http://stackoverflow.com/questions/23700835/pandas-to-html-no-value-representation

df = pd.DataFrame([['A',1.2225],['A',]], columns=['Group','Data'])
df.to_html(na_rep=""Ted"",float_format='{0:.2f}'.format)
#### Expected Output

<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>Group</th>
      <th>Data</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td> A</td>
      <td>1.22</td>
    </tr>
    <tr>
      <th>1</th>
      <td> A</td>
      <td> Ted</td>
    </tr>
  </tbody>
</table>

#### output of `pd.show_versions()`
## INSTALLED VERSIONS

commit: None
python: 3.5.1.final.0
python-bits: 64
OS: Darwin
OS-release: 15.6.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8

pandas: 0.18.0
nose: 1.3.7
pip: 8.1.2
setuptools: 20.3
Cython: 0.23.4
numpy: 1.10.4
scipy: 0.17.0
statsmodels: 0.6.1
xarray: None
IPython: 4.1.2
sphinx: 1.3.5
patsy: 0.4.0
dateutil: 2.5.1
pytz: 2016.2
blosc: None
bottleneck: 1.0.0
tables: 3.2.2
numexpr: 2.5
matplotlib: 1.5.1
openpyxl: 2.3.2
xlrd: 0.9.4
xlwt: 1.0.0
xlsxwriter: 0.8.4
lxml: 3.6.0
bs4: 4.4.1
html5lib: None
httplib2: None
apiclient: None
sqlalchemy: 1.0.12
pymysql: None
psycopg2: None
jinja2: 2.8
boto: 2.39.0
"
51316614,9046,DataFrame.to_latex() should honor na_rep after formatter.,moorepants,closed,2014-12-08T15:51:33Z,2020-10-24T02:49:09Z,"In the example below I would expect that 'np.nan' would be replaced by 'aaah'.

```
In [3]: df = pd.DataFrame({'a': np.arange(3), 'b': [0.1, np.nan, 0.2]})

In [4]: df
Out[4]: 
   a    b
0  0  0.1
1  1  NaN
2  2  0.2

In [5]: print(df.to_latex(formatters={'b': lambda x: '{:0.4f}'.format(x)}, na_rep='aaah'))
\begin{tabular}{lrr}
\toprule
{} &  a &      b \\
\midrule
0 &  0 & 0.1000 \\
1 &  1 &    nan \\
2 &  2 & 0.2000 \\
\bottomrule
\end{tabular}
```
"
709823953,36690,BUG: Don't ignore na_rep in DataFrame.to_html,dsaxton,closed,2020-09-27T21:58:25Z,2020-10-24T02:51:30Z,"- [x] closes #13828
- [x] closes #9046
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
638204037,34751,BUG: Hypothesis test failure - test_on_offset_implementations ,simonjayhawkins,closed,2020-06-13T17:54:45Z,2020-10-24T02:55:20Z,"https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=37199&view=logs&j=76104ccd-8dcc-5006-a17c-28bcdd709542&t=cd420693-444f-5c55-14e0-1be238e189a1

```
2020-06-13T14:27:46.0206662Z ================================== FAILURES ===================================
2020-06-13T14:27:46.0207049Z _______________________ test_on_offset_implementations ________________________
2020-06-13T14:27:46.0207455Z [gw2] win32 -- Python 3.7.7 C:\Miniconda\envs\pandas-dev\python.exe
2020-06-13T14:27:46.0207694Z 
2020-06-13T14:27:46.0207905Z     @given(gen_random_datetime, gen_yqm_offset)
2020-06-13T14:27:46.0208239Z >   def test_on_offset_implementations(dt, offset):
2020-06-13T14:27:46.0208612Z 
2020-06-13T14:27:46.0208902Z pandas\tests\tseries\offsets\test_offsets_properties.py:89: 
2020-06-13T14:27:46.0209414Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2020-06-13T14:27:46.0209969Z pandas\tests\tseries\offsets\test_offsets_properties.py:94: in test_on_offset_implementations
2020-06-13T14:27:46.0210468Z     compare = (dt + offset) - offset
2020-06-13T14:27:46.0210950Z pandas\_libs\tslibs\offsets.pyx:438: in pandas._libs.tslibs.offsets.BaseOffset.__add__
2020-06-13T14:27:46.0211305Z     return other.__add__(self)
2020-06-13T14:27:46.0211774Z pandas\_libs\tslibs\offsets.pyx:440: in pandas._libs.tslibs.offsets.BaseOffset.__add__
2020-06-13T14:27:46.0212253Z     return self.apply(other)
2020-06-13T14:27:46.0212687Z pandas\_libs\tslibs\offsets.pyx:135: in pandas._libs.tslibs.offsets.apply_wraps.wrapper
2020-06-13T14:27:46.0213172Z     result = result.tz_localize(tz)
2020-06-13T14:27:46.0213673Z pandas\_libs\tslibs\timestamps.pyx:1258: in pandas._libs.tslibs.timestamps.Timestamp.tz_localize
2020-06-13T14:27:46.0215173Z     value = tz_localize_to_utc(np.array([self.value], dtype='i8'), tz,
2020-06-13T14:27:46.0215769Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2020-06-13T14:27:46.0216156Z 
2020-06-13T14:27:46.0216385Z >   raise pytz.NonExistentTimeError(stamp)
2020-06-13T14:27:46.0216849Z E   pytz.exceptions.NonExistentTimeError: 1906-01-01 00:00:00
2020-06-13T14:27:46.0217233Z 
2020-06-13T14:27:46.0217489Z pandas\_libs\tslibs\tzconversion.pyx:276: NonExistentTimeError
2020-06-13T14:27:46.0217864Z --------------------------------- Hypothesis ----------------------------------
2020-06-13T14:27:46.0218223Z Falsifying example: test_on_offset_implementations(
2020-06-13T14:27:46.0218570Z     dt=datetime.datetime(1900, 1, 1, 0, 0, tzinfo=tzfile('Asia/Calcutta')),
2020-06-13T14:27:46.0219018Z     offset=<72 * MonthBegins>,
2020-06-13T14:27:46.0219213Z )
```
"
728622453,37376,TST: on_offset_implementations closes #34751,jbrockmendel,closed,2020-10-24T01:11:58Z,2020-10-24T02:59:55Z,"- [x] closes #34751
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
728587233,37373,CLN: de-duplicate _isnan,jbrockmendel,closed,2020-10-23T23:03:30Z,2020-10-24T03:00:20Z,
722668750,37148,BUG: DataFrame[td64].sum(skipna=False),jbrockmendel,closed,2020-10-15T20:53:27Z,2020-10-24T03:08:28Z,"- [x] closes #17235
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The same underlying problem affects mean, so this fixes that too.
"
728512773,37369,BUG: Call finalize in DataFrame.unstack,mzeitlin11,closed,2020-10-23T20:44:05Z,2020-10-24T03:54:08Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This commit adds a step to call `__finalize__` in `DataFrame.unstack`, as motivated by #28283.  Adding this step also prevents needing a step to call `__finalize__` in `DataFrame.pivot` since `DataFrame.unstack` is called before returning, so the xfail for that test was removed as well."
249799328,17235,BUG:  Summation of NaT in a DataFrame with axis=1 does not return NaT,s-celles,closed,2017-08-12T09:01:10Z,2020-10-24T06:37:03Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
import io

dat = """"""s1;s2
1000;2000
3000;1500
500""""""

df = pd.read_csv(io.StringIO(dat), sep="";"")
df.index = df.index + 1
df = df.apply(lambda x: pd.to_timedelta(x, unit='ms'))

df['laptime'] = df.sum(axis=1, skipna=False)

print(df)
```
#### Problem description

This code displays

```
               s1              s2                       laptime
1        00:00:01        00:00:02               0 days 00:00:03
2        00:00:03 00:00:01.500000        0 days 00:00:04.500000
3 00:00:00.500000             NaT -106752 days +00:12:43.645223
```

```
In [81]: df['laptime'][3]
Out[81]: Timedelta('-106752 days +00:12:43.645223')
```

#### Expected Output

```
               s1              s2                       laptime
1        00:00:01        00:00:02               0 days 00:00:03
2        00:00:03 00:00:01.500000        0 days 00:00:04.500000
3 00:00:00.500000             NaT                           NaT
```

```
In [81]: df['laptime'][3]
Out[81]: NaT
```


That's very strange because `pd.to_timedelta(500, unit='ms') + pd.NaT` returns `NaT` (which is expected) but it's doesn't work as expected when summing `Series`)

#### Output of ``pd.show_versions()``

<details>

: pd.show_versions()
/Users/scls/anaconda/lib/python3.6/site-packages/xarray/core/formatting.py:16: FutureWarning: The pandas.tslib module is deprecated and will be removed in a future version.
  from pandas.tslib import OutOfBoundsDatetime

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.1.final.0
python-bits: 64
OS: Darwin
OS-release: 16.7.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: fr_FR.UTF-8
LOCALE: fr_FR.UTF-8

pandas: 0.20.1
pytest: 3.1.2
pip: 9.0.1
setuptools: 27.2.0
Cython: 0.25.2
numpy: 1.12.1
scipy: 0.19.0
xarray: 0.9.5
IPython: 5.3.0
sphinx: 1.5.6
patsy: 0.4.1
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: 1.2.1
tables: 3.3.0
numexpr: 2.6.2
feather: None
matplotlib: 2.0.2
openpyxl: 2.4.7
xlrd: 1.0.0
xlwt: 1.2.0
xlsxwriter: 0.9.6
lxml: 3.7.3
bs4: 4.6.0
html5lib: 0.999
sqlalchemy: 1.1.9
pymysql: None
psycopg2: None
jinja2: 2.9.6
s3fs: None
pandas_gbq: None
pandas_datareader: 0.5.0

</details>
"
724661041,37248,DOC: update development documentation for link to built docs,jreback,closed,2020-10-19T14:13:03Z,2020-10-24T07:27:52Z,"this has the built docs: https://pandas.pydata.org/pandas-docs/dev/ rather than https://dev.pandas.io/, so need to update the contributing docs."
727974491,37357,DOC: update development documentation for link to built docs #37248,aniaan,closed,2020-10-23T07:10:36Z,2020-10-24T07:28:07Z,"- [x] closes #37248

cc: @jreback 
"
727095706,37331,DOC: Remove confusing description from `core.DataFrame.iterrows`,takegue,closed,2020-10-22T06:36:01Z,2020-10-24T07:31:13Z,"Current [`core.DataFrame.iterrows`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html) is as follows
![image](https://user-images.githubusercontent.com/5734732/96833205-00176c00-147b-11eb-8dc3-6371b2718e76.png)

However, `core.DataFrame.iterrows` actually returns an iterator which yields `(index, data)`, not `(index, data, it)`.
This PRs remove such confusing description like [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iteritems.html?highlight=yields]

- [ ] closes #xxxx (no issues found)
- [ ] tests passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
726833019,37321,BUG: Index._id,topper-123,closed,2020-10-21T20:16:12Z,2020-10-24T08:05:43Z,"Fixes #37213.
"
506217092,28942,TST: add test_series_any_timedelta for GH17667,rohitsanj,closed,2019-10-12T17:51:22Z,2020-10-24T14:10:17Z,"Test case example is the same as the one given in the issue #17667 

- [x] closes #17667 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
728183420,37364,Backport PR #37302 on branch 1.1.x (BUG: Allow empty chunksize in stata reader when using iterator),meeseeksmachine,closed,2020-10-23T12:36:24Z,2020-10-24T16:11:09Z,Backport PR #37302: BUG: Allow empty chunksize in stata reader when using iterator
728834277,37385,BUG: pandas 1.1.3 fails to install using pip and clang due to unreachable code,Chaz6,closed,2020-10-24T17:17:16Z,2020-10-24T17:18:34Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
pip3 install -U --user pandas

```

#### Problem description

The installation fails with the following output:-

```
  clang -pthread -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -DNPY_NO_DEPRECATED_API=0 -I./pandas/_libs -Ipandas/_libs/src/klib -I/tmp/
pip-build-env-ikhpgen8/overlay/lib/python3.9/site-packages/numpy/core/include -I/home/chaz/.local/local/python-3.9.0/include/python3.9 -c pandas/_libs/algos.c -o build/temp.linux-x
86_64-3.9/pandas/_libs/algos.o -Werror
  pandas/_libs/algos.c:81785:3: error: code will never be executed [-Werror,-Wunreachable-code]
    __Pyx_SafeReleaseBuffer(&__pyx_pybuffernd_arr.rcbuffer->pybuffer);
    ^~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/algos.c:83402:3: error: code will never be executed [-Werror,-Wunreachable-code]
    __Pyx_SafeReleaseBuffer(&__pyx_pybuffernd_arr.rcbuffer->pybuffer);
    ^~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/algos.c:84302:3: error: code will never be executed [-Werror,-Wunreachable-code]
    __Pyx_SafeReleaseBuffer(&__pyx_pybuffernd_arr.rcbuffer->pybuffer);
    ^~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/algos.c:85202:3: error: code will never be executed [-Werror,-Wunreachable-code]
    __Pyx_SafeReleaseBuffer(&__pyx_pybuffernd_arr.rcbuffer->pybuffer);
    ^~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/algos.c:85385:3: error: code will never be executed [-Werror,-Wunreachable-code]
    __Pyx_SafeReleaseBuffer(&__pyx_pybuffernd_arr.rcbuffer->pybuffer);
    ^~~~~~~~~~~~~~~~~~~~~~~
  pandas/_libs/algos.c:86285:3: error: code will never be executed [-Werror,-Wunreachable-code]
    __Pyx_SafeReleaseBuffer(&__pyx_pybuffernd_arr.rcbuffer->pybuffer);
    ^~~~~~~~~~~~~~~~~~~~~~~
  6 errors generated.
  error: command '/home/chaz/.local/local/clang+llvm/bin/clang' failed with exit code 1
  ----------------------------------------
  ERROR: Failed building wheel for pandas
```

#### Expected Output

The module should be installed successfully.

#### Output of ``pd.show_versions()``

N/A

[paste the output of ``pd.show_versions()`` here leaving a blank line after the details tag]

</details>
"
728819857,37383,CI fix failing codecheck,MarcoGorelli,closed,2020-10-24T16:05:58Z,2020-10-24T17:23:12Z,
728559718,37372,TST/REF: collect tests by method,jbrockmendel,closed,2020-10-23T22:34:40Z,2020-10-24T18:00:19Z,
728847946,37388,PERF: release gil for ewmc_time,fangchenli,closed,2020-10-24T18:30:14Z,2020-10-24T18:32:01Z,no perf change for `rolling.EWMMethods`
722360244,37137,TYP: use overload to refine return type of reset_index,simonjayhawkins,closed,2020-10-15T13:55:31Z,2020-10-24T21:02:24Z,"maybe alternative to #33519

with `from __future__ import annotations` on py3.7 the types in the function signature are not evaluated at run-time, so this shouldn't fail 🤞 "
728902268,37391,BUG: nanstd with td64 and skipna=False,jbrockmendel,closed,2020-10-24T23:53:22Z,2020-10-24T23:55:40Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The actual bugfix is the one line changed in nanops; the test is rolled in to the existing TDA.std test.  Everything else is cleanup that was involved in tracking down the bug."
725371127,37272,DOC: list all options for encoding and quoting in read_csv,YuriyTigiev,closed,2020-10-20T09:06:23Z,2020-10-25T17:52:58Z,"#### Location of the documentation

[pandas.to_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html)

**Note**: You can check the latest versions of the docs on `master` [here](https://pandas.pydata.org/docs/dev/).

#### Documentation problem
""encoding"" and ""quoting"" described briefly. 

#### Suggested fix for documentation
Should provide all parameters which support these options. "
356804263,22585,Series constructor with missing values and int dtype fails or passes depending on data type,TomAugspurger,closed,2018-09-04T13:13:23Z,2020-10-25T18:52:39Z,"I'm not sure what the expected behavior is, but these should match

```python
In [16]: pd.Series([1, 2, np.nan], dtype=int)
```
```pytb
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-16-f0075752a7f2> in <module>()
----> 1 pd.Series([1, 2, np.nan], dtype=int)

~/sandbox/pandas/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath)
    280             else:
    281                 data = _sanitize_array(data, index, dtype, copy,
--> 282                                        raise_cast_failure=True)
    283
    284                 data = SingleBlockManager(data, index, fastpath=True)

~/sandbox/pandas/pandas/core/series.py in _sanitize_array(data, index, dtype, copy, raise_cast_failure)
   4154         if dtype is not None:
   4155             try:
-> 4156                 subarr = _try_cast(data, False)
   4157             except Exception:
   4158                 if raise_cast_failure:  # pragma: no cover

~/sandbox/pandas/pandas/core/series.py in _try_cast(arr, take_fast_path)
   4087             # that we can convert the data to the requested dtype.
   4088             if is_float_dtype(dtype) or is_integer_dtype(dtype):
-> 4089                 subarr = maybe_cast_to_integer_array(arr, dtype)
   4090
   4091             subarr = maybe_cast_to_datetime(arr, dtype)

~/sandbox/pandas/pandas/core/dtypes/cast.py in maybe_cast_to_integer_array(arr, dtype, copy)
   1341     try:
   1342         if not hasattr(arr, ""astype""):
-> 1343             casted = np.array(arr, dtype=dtype, copy=copy)
   1344         else:
   1345             casted = arr.astype(dtype, copy=copy)

ValueError: cannot convert float NaN to integer
```

and when `data` is an array, we end up with float dtype.

```
In [17]: pd.Series(np.array([1, 2, np.nan]), dtype=np.int64)
Out[17]:
0    1.0
1    2.0
2    NaN
dtype: float64
```"
723795283,37199,BUG: Non deterministic level order in MultiIndex with join,phofl,closed,2020-10-17T16:24:09Z,2020-10-25T19:19:14Z,"- [x] closes #36910 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This fixes the issue, if the number should be deterministic. If not, we should add a note in the docs"
729096739,37404,CLN: Fix unwanted pattern in unittest,phofl,closed,2020-10-25T19:38:48Z,2020-10-25T19:54:02Z,"- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Introduced with #37090
"
709518047,36662,test_pivot_empty on windows py_3.8_32 and linux py_3.8_32 on MacPython.pandas-wheels nightly failing,simonjayhawkins,closed,2020-09-26T11:09:26Z,2020-10-25T20:07:35Z,"https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=43395&view=logs&j=bb1c2637-64c6-57bd-9ea6-93823b2df951

windows py_3.8_32

```
=========================== short test summary info ===========================
FAILED test_venv/lib/site-packages/pandas/tests/frame/test_reshape.py::TestDataFrameReshape::test_pivot_empty
= 1 failed, 66757 passed, 7472 skipped, 1130 xfailed, 14 warnings in 829.09s (0:13:49) =
```

linux py_3.8_32
```
=========================== short test summary info ============================
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/frame/test_reshape.py::TestDataFrameReshape::test_pivot_empty
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/io/parser/test_c_parser_only.py::test_float_precision_options[c_high]
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/io/parser/test_c_parser_only.py::test_float_precision_options[c_low]
= 3 failed, 66801 passed, 7441 skipped, 1129 xfailed, 14 warnings in 428.19s (0:07:08) =
```

test_c_parser_only is a seperate issue xref #36429"
728983239,37399,CI: enable 32bit build on CI,simonjayhawkins,closed,2020-10-25T09:50:24Z,2020-10-25T20:08:49Z,"the number of failures on the 32bit build on https://github.com/MacPython/pandas-wheels is starting to accumulate

we should consider having a 32bit build on CI to catch these earlier (and can be addressed by the people more familiar with the changes)

a snapshot of the failures on linux py_3.8_32

```
=========================== short test summary info ============================
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/arrays/floating/test_function.py::test_stat_method[var-kwargs1]
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/arrays/floating/test_function.py::test_stat_method[var-kwargs0]
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/arrays/floating/test_function.py::test_stat_method[kurtosis-kwargs2]
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/arrays/floating/test_function.py::test_stat_method[sem-kwargs4]
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/base/test_misc.py::test_memory_usage[series-with-empty-index]
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/groupby/test_groupby.py::test_groupby_nat_exclude
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/io/formats/test_info.py::test_info_int_columns
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/io/parser/test_c_parser_only.py::test_float_precision_options[c_high]
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/io/parser/test_c_parser_only.py::test_float_precision_options[c_low]
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/reshape/test_pivot.py::TestPivot::test_pivot_empty
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/window/test_rolling.py::test_rolling_var_numerical_issues[var-1-values0]
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/window/test_rolling.py::test_rolling_var_numerical_issues[std-1-values1]
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/window/test_rolling.py::test_rolling_var_numerical_issues[var-2-values2]
FAILED ../../venv/lib/python3.8/site-packages/pandas/tests/window/test_rolling.py::test_rolling_var_numerical_issues[std-2-values3]
= 14 failed, 80215 passed, 16918 skipped, 1380 xfailed, 13 warnings in 528.07s (0:08:48) =
```

#37398, #36662 and #36429 address some of these."
716193076,36933,TST/CLN: roll_sum/mean/var/skew/kurt: simplification for non-monotonic indices,twoertwein,closed,2020-10-07T04:49:54Z,2020-10-25T22:09:55Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

The removed for-loop doesn't seem to be necessary (I hope this code is tested by an existing test).

I feel like I'm missing an obvious reason why these for-loops are needed: looking at the code I don't think we need them and the tests also pass."
728923966,37395,TST/REF: collect tests by method,jbrockmendel,closed,2020-10-25T02:37:34Z,2020-10-26T03:33:00Z,
728908593,37392,BUG: DataFrame.std(skipna=False) with td64 dtype,jbrockmendel,closed,2020-10-25T00:36:44Z,2020-10-26T03:33:43Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
339476519,21828, Initializing from file failed,JafferWilson,closed,2018-07-09T14:36:18Z,2020-10-26T05:03:43Z,"Kindly, share your insights regarding the following error
```
  File ""source\matrix_gen.py"", line 68, in <module>
    main()
  File ""source\matrix_gen.py"", line 22, in main
    data = pd.read_csv(sys.argv[1], parse_dates=True, dayfirst=True)# argv[1]: stock_symbol.txt
  File ""C:\Users\aims\Anaconda3\envs\py35\lib\site-packages\pandas\io\parsers.py"", line 678, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""C:\Users\aims\Anaconda3\envs\py35\lib\site-packages\pandas\io\parsers.py"", line 440, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File ""C:\Users\aims\Anaconda3\envs\py35\lib\site-packages\pandas\io\parsers.py"", line 787, in __init__
    self._make_engine(self.engine)
  File ""C:\Users\aims\Anaconda3\envs\py35\lib\site-packages\pandas\io\parsers.py"", line 1014, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File ""C:\Users\aims\Anaconda3\envs\py35\lib\site-packages\pandas\io\parsers.py"", line 1708, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File ""pandas\_libs\parsers.pyx"", line 384, in pandas._libs.parsers.TextReader.__cinit__
  File ""pandas\_libs\parsers.pyx"", line 697, in pandas._libs.parsers.TextReader._setup_parser_source
OSError: Initializing from file failed
```"
728999905,37400,REGR/PERF: Index.is_ performance regression,topper-123,closed,2020-10-25T11:30:35Z,2020-10-26T08:03:30Z,"Fixes https://github.com/pandas-dev/pandas/pull/37321#issuecomment-715878012

The offender is actually #37321. Apparantly is's a bit slow to do the check using a generator like `com.any_none`.

```python
In [1]: from pandas import *
In [2]: idx_large_fast = RangeIndex(100000)
   ...: idx_small_slow = date_range(start=""1/1/2012"", periods=1)
   ...: mi_large_slow = MultiIndex.from_product([idx_large_fast, idx_small_slow])
   ...:
   ...: idx_non_object = RangeIndex(1)
In[3]: %timeit mi_large_slow.equals(idx_non_object)
3.13 µs ± 79.4 ns per loop  # master
2.09 µs ± 31.1 ns per loop  # This PR
```"
728818487,37382,TST: Different tests were collected between gw0 and gw1,simonjayhawkins,closed,2020-10-24T15:58:59Z,2020-10-26T09:40:48Z,"not sure if anyone else if having issue running tests locally. have created a clean pandas-dev env on a different machine so maybe a transient thing.

https://github.com/pytest-dev/pytest-xdist/issues/432

<details>

```
INSTALLED VERSIONS
------------------
commit           : dce547ea2fe7f8c5c51c04d0a87704a2b4abf476
python           : 3.8.6.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : English_United Kingdom.1252

pandas           : 1.2.0.dev0+908.gdce547ea2f
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 49.6.0.post20201009
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : 5.37.4
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.4
fastparquet      : 0.4.1
gcsfs            : 0.7.1
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 2.0.0
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.0
sqlalchemy       : 1.3.20
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.51.2
```
</details>"
722760168,37151,API/BUG: empty sum with td64 NaT or timedelta(0)?,jbrockmendel,closed,2020-10-15T23:53:04Z,2020-10-26T12:06:46Z,"```
ser = pd.Series([], dtype=""m8[ns]"")

>>> ser.sum()
NaT
>>> pd.Index(ser).sum()
NaT
>>> ser.array.sum()
NaT

>>> ser.to_frame().sum()
0   0 days
dtype: timedelta64[ns]
```

I think zero makes more sense.  Easy fix if we have consensus.
"
166085272,13691,s.loc[[]] raises error (only) if index is not unique,toobaz,closed,2016-07-18T12:46:10Z,2020-10-16T19:17:11Z,"#### Code Sample, a copy-pastable example if possible

```

In [2]: s = pd.Series(0, index=pd.MultiIndex.from_product([[0], [1,2]]))

In [3]: s.loc[[]]
Out[3]: Series([], dtype: int64)

In [4]: s = pd.Series(0, index=pd.MultiIndex.from_product([[0], [1,1]]))

In [5]: s.loc[[]]
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-5-f9b6211189ca> in <module>()
----> 1 s.loc[[]]

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in __getitem__(self, key)
   1304             return self._getitem_tuple(key)
   1305         else:
-> 1306             return self._getitem_axis(key, axis=0)
   1307 
   1308     def _getitem_axis(self, key, axis=0):

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_axis(self, key, axis)
   1464                     raise ValueError('Cannot index with multidimensional key')
   1465 
-> 1466                 return self._getitem_iterable(key, axis=axis)
   1467 
   1468             # nested tuple slicing

/home/pietro/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_iterable(self, key, axis)
   1095 
   1096                 new_target, indexer, new_indexer = labels._reindex_non_unique(
-> 1097                     keyarr)
   1098 
   1099                 if new_indexer is not None:

/home/pietro/nobackup/repo/pandas/pandas/indexes/base.py in _reindex_non_unique(self, target)
   2497                 new_indexer[~check] = -1
   2498 
-> 2499         new_index = self._shallow_copy_with_infer(new_labels, freq=None)
   2500         return new_index, indexer, new_indexer
   2501 

/home/pietro/nobackup/repo/pandas/pandas/indexes/multi.py in _shallow_copy_with_infer(self, values, **kwargs)
    393 
    394     def _shallow_copy_with_infer(self, values=None, **kwargs):
--> 395         return self._shallow_copy(values, **kwargs)
    396 
    397     @Appender(_index_shared_docs['_shallow_copy'])

/home/pietro/nobackup/repo/pandas/pandas/indexes/multi.py in _shallow_copy(self, values, **kwargs)
    402             # discards freq
    403             kwargs.pop('freq', None)
--> 404             return MultiIndex.from_tuples(values, **kwargs)
    405         return self.view()
    406 

/home/pietro/nobackup/repo/pandas/pandas/indexes/multi.py in from_tuples(cls, tuples, sortorder, names)
    889         if len(tuples) == 0:
    890             # I think this is right? Not quite sure...
--> 891             raise TypeError('Cannot infer number of levels from empty list')
    892 
    893         if isinstance(tuples, (np.ndarray, Index)):

TypeError: Cannot infer number of levels from empty list

```
#### Expected Output

`Out[5]: Series([], dtype: int64)
`

By the way, on top of the inconsistency above,

`In [6]: s.loc[s.iloc[0:0]]`

results in the same error, which is even more unexpected because `s.iloc[0:0].index`  is a perfectly valid `MultiIndex`, and so it is certainly possible to infer the number of levels from it.

Might be related to #13490 (the error is the same, but I ignore the level at which the open PR applies).
#### output of `pd.show_versions()`

```

In [7]: pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.1.final.0
python-bits: 64
OS: Linux
OS-release: 4.5.0-2-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: it_IT.utf8
LOCALE: it_IT.UTF-8

pandas: 0.18.1+206.gc42455b
nose: 1.3.7
pip: 1.5.6
setuptools: 18.4
Cython: 0.23.4
numpy: 1.10.4
scipy: 0.16.0
statsmodels: 0.8.0.dev0+111ddc0
xarray: None
IPython: 5.0.0.dev
sphinx: 1.3.1
patsy: 0.3.0-dev
dateutil: 2.2
pytz: 2012c
blosc: None
bottleneck: 1.1.0dev
tables: 3.2.2
numexpr: 2.5
matplotlib: 1.5.1
openpyxl: None
xlrd: 0.9.4
xlwt: 1.1.2
xlsxwriter: 0.7.3
lxml: None
bs4: 4.4.0
html5lib: 0.999
httplib2: 0.9.1
apiclient: 1.5.0
sqlalchemy: 1.0.11
pymysql: None
psycopg2: None
jinja2: 2.8
boto: 2.38.0
pandas_datareader: 0.2.1

```
"
722982110,37156,CLN/REF: Refactor min_periods calculation in rolling,mroeschke,closed,2020-10-16T07:30:53Z,2020-10-16T19:20:30Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
710321365,36702,Undocumented behavior change (1.0.5->1.1.0) when using arithmetic operations on dataframes,mhaselsteiner,closed,2020-09-28T14:32:45Z,2020-10-16T19:24:40Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---



#### Code Sample, a copy-pastable example
pandas 1.0.5:
```python
(pd.DataFrame({'x':[1,2,],'y':[1,2]})+[pd.Series([1,1]),pd.Series([1,1]) ]).iloc[0,0]
Out[37]: 2
```
pandas 1.1.0:
```python 
>>> (pd.DataFrame({'x':[1,2,],'y':[1,2]})+[pd.Series([1,1]),pd.Series([1,1]) ]).iloc[0,0]
0    2
1    2
dtype: int64
```

#### Problem description

When applying arithmetic operations between dataframe and list of Series prior to 1.0.5 and we get different results then starting with 1.1.0. Starting with 1.1.0 we seem to get a another level of indexing, altough the dataframes columns and index return the same values, iterating over it returns different objects.

Since now the return type of the chained iloc has changed from float/int to series, this breaks functions using the arithmetic operator like this.

#### Expected Output

#### Output of ``pd.show_versions()``
using the latest pandas version:
<details>
INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-48-generic
Version          : #52-Ubuntu SMP Thu Sep 10 10:58:49 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.2
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.6.0.post20200917
Cython           : None
pytest           : 6.0.2
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
</details>


using 1.0.3
<details>
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.8.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-48-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8
pandas           : 1.0.3
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 47.3.1.post20200616
Cython           : None
pytest           : 5.4.3
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.2.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 5.4.3
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.18
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : NoneI


</details>
"
723323522,37161,BUG: Series.loc[[]] with non-unique MultiIndex #13691,jbrockmendel,closed,2020-10-16T15:31:10Z,2020-10-16T20:35:36Z,"- [x] closes #13691
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Broken off from #37150"
560497639,31710,ValueError: buffer source array is read-only,kernc,closed,2020-02-05T16:47:31Z,2020-10-16T21:35:56Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> index = pd.date_range('2020', 'now', freq='1h') 
>>> arr = np.zeros_like(index) 
>>> arr.setflags(write=False)
>>> pd.Series(arr, index=index).resample('1d').agg('last')

-------------------------------------------------------------------------
~/pandas/pandas/_libs/groupby.pyx in pandas._libs.groupby.group_last()
~/pandas/pandas/_libs/groupby.cpython-37m-x86_64-linux-gnu.so in View.MemoryView.memoryview_cwrapper()
~/pandas/pandas/_libs/groupby.cpython-37m-x86_64-linux-gnu.so in View.MemoryView.memoryview.__cinit__()
ValueError: buffer source array is read-only
```
#### Problem description

Groupby fails on some read-only buffers (I couldn't quickly reproduce it with `.groupby()` itself, sorry).

The prime solution would be to add `const` specifier to the input `values` here (and related entries):
https://github.com/pandas-dev/pandas/blob/0b6debf825db39f8f63d300cb8007a7996d002fc/pandas/_libs/groupby.pyx#L853

if it were not for Cython's non-support of const fused types (https://github.com/cython/cython/issues/1772), resolved in (https://github.com/cython/cython/pull/3118), but despite [miniscule change](https://github.com/cython/cython/pull/3118/files?file-filters%5B%5D=.py) only scheduled for release in Cython 3.0. I guess wait until then.

#### Expected Output

Resampling/groupby works with read-only arrays.

#### Output of ``pd.show_versions()``

pandas 1.1.0.dev0+361.gf0b00f887
cython 0.29.14
"
390170525,24248,DatetimeIndex.take() fails if index has more than max int rows,bchu,open,2018-12-12T10:55:05Z,2020-10-16T23:14:06Z,"```
>>> index = pd.DatetimeIndex(np.empty(2**31))
>>> index.take([1, 2, 5, 6])
  File ""/pandas/core/indexes/datetimelike.py"", line 529, in take
    maybe_slice = lib.maybe_indices_to_slice(indices, len(self))
  File ""pandas/_libs/lib.pyx"", line 351, in pandas._libs.lib.maybe_indices_to_slice
OverflowError: value too large to convert to int
```

With `index = pd.DatetimeIndex(np.empty(2**31-1))` the `take` works fine."
723498865,37168,NIT: Format text.rst doc code,dsaxton,closed,2020-10-16T20:01:58Z,2020-10-17T00:03:23Z,Minor formatting mistake I made in another PR
695410606,36204,BUG: Incorrect parsing of ISO 8601 durations,lmeyerov,closed,2020-09-07T22:06:02Z,2020-10-17T00:07:07Z,"- [x ] I have checked that this issue has not already been reported.

- [ x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---


#### Code Sample, a copy-pastable example

```python
pd.Timedelta('P1Y')
pd.Timedelta('P1M')
pd.Timedelta('P1W')
pd.Timedelta('P1D')
pd.Timedelta('P1DT1H')
pd.Timedelta('PT1H')
pd.Timedelta('P1H')
```

#### Problem description

Trying [most iso8601 duration formats](https://en.wikipedia.org/wiki/ISO_8601#Durations) fails.

For above examples:

* Expected pd.Timedelta instances of 1yr, 1mo, 1wk, 1d, 1d 1hr
* ... but got `ValueError: Invalid ISO 8601 Duration format - ...` on Y/M/W, PT1H
* ... 1D was '0'
* ... 1DT1H was  '1d' (no hr)

Encountered while trying to figure out neo4j.time.Duration().isoformat() -> pandas ->arrow > rapids cudf timedelta[*] . Neo4j returns values like `P1Y3M15DT2H3M` and `P1Y3M`."
526648412,29773,Timedelta not parsing ISO-8601 strings properly,datajanko,closed,2019-11-21T14:59:08Z,2020-10-17T00:07:07Z,"#### Code Sample, a copy-pastable example if possible

```python
pd.Timedelta(""P0DT1H"")
Timedelta('0 days 00:00:00')
```

and

```python
pd.Timedelta(""PT1H"")
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
pandas/_libs/tslibs/timedeltas.pyx in pandas._libs.tslibs.timedeltas.parse_timedelta_unit()

KeyError: ''

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-5-35a9c7000b3e> in <module>()
----> 1 pd.Timedelta(""PT1H"")

pandas/_libs/tslibs/timedeltas.pyx in pandas._libs.tslibs.timedeltas.Timedelta.__new__()

pandas/_libs/tslibs/timedeltas.pyx in pandas._libs.tslibs.timedeltas.parse_iso_format_string()

pandas/_libs/tslibs/timedeltas.pyx in pandas._libs.tslibs.timedeltas.timedelta_from_spec()

pandas/_libs/tslibs/timedeltas.pyx in pandas._libs.tslibs.timedeltas.parse_timedelta_unit()

ValueError: invalid unit abbreviation: 
```
#### Problem description

The ISO strings should be parsed correctly and provide valid output
as described [here](https://github.com/pandas-dev/pandas/pull/15136) and shown [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/timedeltas.html)

#### Expected Output

First example and second example should have output
```
Timedelta('0 days 01:00:00')
```

#### Output of ``pd.show_versions()``
I had the same outputs locally and on google cola
<details><summary>Versions</summary>

```
INSTALLED VERSIONS
------------------
commit           : None
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.137+
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.17.4
pytz             : 2018.9
dateutil         : 2.6.1
pip              : 19.3.1
setuptools       : 41.6.0
Cython           : 0.29.14
pytest           : 3.6.4
hypothesis       : None
sphinx           : 1.8.5
blosc            : None
feather          : 0.4.0
xlsxwriter       : None
lxml.etree       : 4.2.6
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.6.1 (dt dec pq3 ext lo64)
jinja2           : 2.10.3
IPython          : 5.5.0
pandas_datareader: 0.7.4
bs4              : 4.6.3
bottleneck       : 1.3.0
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.2.6
matplotlib       : 3.1.1
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 2.5.9
pandas_gbq       : 0.11.0
pyarrow          : 0.14.1
pytables         : None
s3fs             : 0.4.0
scipy            : 1.3.2
sqlalchemy       : 1.3.11
tables           : 3.4.4
xarray           : 0.11.3
xlrd             : 1.1.0
xlwt             : 1.3.0
xlsxwriter       : None
```

</details>
"
723561519,37177,REF: Simplify creation of rolling window indexers internally,mroeschke,closed,2020-10-16T22:15:57Z,2020-10-17T01:00:24Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
723579003,37180,BUG: use cls instead of MultiIndex in MultiIndex classmethods,jbrockmendel,closed,2020-10-16T23:08:53Z,2020-10-17T01:51:49Z,"- [x] closes #11267
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This doesn't go through all the places where MultiIndex is hard-coded, just the classmethods where in `cls` should be used as a matter of principle."
447161040,26490,RecursionError when selecting single column from IntervalIndex,sbitzer,closed,2019-05-22T14:11:24Z,2020-10-17T01:53:49Z,"#### Code Sample, a copy-pastable example if possible

```python
df = pd.DataFrame(
        np.ones((3, 4)), 
        columns=pd.IntervalIndex.from_breaks(np.arange(5)))
df[0.5]
df.loc[:, 0.5]
```
#### Problem description

Instead of returning the selected column, either calling `df[0.5]` or `df.loc[:, 0.5]` raises `RecursionError`.

The issue is in `frame.__getitem__` where `key in self.columns == False`. The code then correctly identifies the desired column by integer index using `self.columns.get_loc(key)`, but then goes on to call 
```python
data = self._take(indexer, axis=1)
data = data[key]
```
Because `self._take` returns a DataFrame, `data[key]` enters `frame.__getitem__` again and we are caught in an infinite loop.

#### Expected Output

The selected column as Series.

#### Output of ``pd.show_versions()``

<details>

commit: None
python: 3.6.8.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en
LOCALE: None.None

pandas: 0.24.2
pytest: 4.3.1
pip: 19.0.3
setuptools: 40.8.0
Cython: 0.29.7
numpy: 1.16.2
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 7.4.0
sphinx: 1.8.5
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: None
tables: 3.5.1
numexpr: 2.6.9
feather: None
matplotlib: 3.0.3
openpyxl: 2.6.1
xlrd: 1.2.0
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: 1.3.1
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
722794517,37152,BUG: RecursionError when selecting single column from IntervalIndex #26490,jbrockmendel,closed,2020-10-16T01:36:41Z,2020-10-17T01:54:05Z,"- [x] closes #26490
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

#37150 is failing, this is the first of two orthogonal bugfixes in that PR"
718633362,37030,DOC: Clarify display.precision,hongshaoyang,closed,2020-10-10T14:36:37Z,2020-10-17T02:51:06Z,"- [x] closes #21004
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
723138104,37159,BUG: Fix parsing of ISO8601 durations,mgmarino,closed,2020-10-16T11:29:05Z,2020-10-17T06:34:22Z,"This PR fixes the following behavior:

- Adds ""W"" as a valid weeks designator
- Fixes parsing of durations with empty periods (e.g. ""PT10M"")
- Fixes partial parsing of durations (e.g. ""P1DT1H"")

Fixes #29773.
Fixes #36204.

- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
716329651,36938,DOC: Update dependency for to_markdown documentation,eyaltrabelsi,closed,2020-10-07T08:49:02Z,2020-10-17T10:21:31Z,"Adding to the documentation that tabulate is required for to_markdown to be executed

- [x] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
714379943,36868,conflict with PyPDF2?,fdq09eca,closed,2020-10-04T19:02:50Z,2020-10-17T13:13:31Z,"I am not sure if it is a bug of pandas or mine.
but when I test my code I have the following peculiar error: **an error that disappears if I run again the exact same line of code of pandas**
After I successfully ran my code, I try the following block of code
```
import pandas as pd
df = pd.DataFrame([['a', 'b', 'c'], ['d', '6', '8']])
df[df.apply(lambda x: x.str.contains('e')).any()]
```
and it hit an error, strange is that the code block behaves as expected if I try it again after it hit the error, see below
```
~/Documents/cs_project/toc(master*) » python3 -i corp_gov_report.py                                                                                                     macone@Macs-MacBook-Pro
Bounding box (Decimal('124.724'), Decimal('46.220'), Decimal('752.754'), Decimal('753.770')) is not fully within parent page bounding box (Decimal('0'), Decimal('0'), Decimal('595.276'), Decimal('807.874'))
Bounding box (Decimal('124.724'), Decimal('46.220'), Decimal('752.754'), Decimal('753.770')) is not fully within parent page bounding box (Decimal('0'), Decimal('0'), Decimal('595.276'), Decimal('807.874'))
currency                                   HK$’000
5         Audit and audit related services   1,800
8                     financial statements     600
Independent Auditor’s Remuneration (iv)  獨立核數師之酬金
During the year, the remuneration paid or payable to the  於本年度，已付或應付予獨立核數師安
independent auditor, Ernst & Young, is set out as follows: 永會計師事務所之酬金載列如下：
Services rendered HK$’000
所提供服務 千港元
     
Audit and audit related services 審核及審核相關服務 1,800
Non-audit services 非審核服務
Review of interim condensed consolidated  審閱中期簡明綜合財務報告
financial statements 600
Tax advisory services 稅務諮詢服務 113
>>> df = pd.DataFrame([['a', 'b', 'c'], ['d', '6', '8']])
>>> df[df.apply(lambda x: x.str.contains('e')).any()]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py"", line 2890, in __getitem__
    return self._getitem_bool_array(key)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py"", line 2933, in _getitem_bool_array
    stacklevel=3,
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/warnings.py"", line 110, in _showwarnmsg
    msg.file, msg.line)
  File ""/Users/macone/Library/Python/3.7/lib/python/site-packages/PyPDF2/pdf.py"", line 1069, in _showwarning
    file.write(formatWarning(message, category, filename, lineno, line))
  File ""/Users/macone/Library/Python/3.7/lib/python/site-packages/PyPDF2/utils.py"", line 69, in formatWarning
    file = filename.replace(""/"", ""\\"").rsplit(""\\"", 1)[1] # find the file name
IndexError: list index out of range
>>> df[df.apply(lambda x: x.str.contains('e')).any()]
Empty DataFrame
Columns: [0, 1, 2]
Index: []
```
but if i ran `df[df.apply(lambda x: x.str.contains('e')).any()]` again, the error appear again, but if i trid again, it disappear
```
>>> df[df.apply(lambda x: x.str.contains('e')).any()]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py"", line 2890, in __getitem__
    return self._getitem_bool_array(key)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py"", line 2933, in _getitem_bool_array
    stacklevel=3,
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/warnings.py"", line 110, in _showwarnmsg
    msg.file, msg.line)
  File ""/Users/macone/Library/Python/3.7/lib/python/site-packages/PyPDF2/pdf.py"", line 1069, in _showwarning
    file.write(formatWarning(message, category, filename, lineno, line))
  File ""/Users/macone/Library/Python/3.7/lib/python/site-packages/PyPDF2/utils.py"", line 69, in formatWarning
    file = filename.replace(""/"", ""\\"").rsplit(""\\"", 1)[1] # find the file name
IndexError: list index out of range
>>> df[df.apply(lambda x: x.str.contains('e')).any()]
Empty DataFrame
Columns: [0, 1, 2]
Index: []
>>> df[df.apply(lambda x: x.str.contains('e')).any()]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py"", line 2890, in __getitem__
    return self._getitem_bool_array(key)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py"", line 2933, in _getitem_bool_array
    stacklevel=3,
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/warnings.py"", line 110, in _showwarnmsg
    msg.file, msg.line)
  File ""/Users/macone/Library/Python/3.7/lib/python/site-packages/PyPDF2/pdf.py"", line 1069, in _showwarning
    file.write(formatWarning(message, category, filename, lineno, line))
  File ""/Users/macone/Library/Python/3.7/lib/python/site-packages/PyPDF2/utils.py"", line 69, in formatWarning
    file = filename.replace(""/"", ""\\"").rsplit(""\\"", 1)[1] # find the file name
IndexError: list index out of range
>>> df[df.apply(lambda x: x.str.contains('e')).any()]
Empty DataFrame
Columns: [0, 1, 2]
Index: []
>>> df[df.apply(lambda x: x.str.contains('e')).any()]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
```
see here for the image:
![image](https://user-images.githubusercontent.com/51902242/95024823-25a12900-06b8-11eb-9762-8a50212d73e8.png)

In addition, I encounter this error when I was trying to do this during the project development.
```
    @property
    def df_clean_table(self):
        df_filtered_table = self.df_filtered_table
        currency_cols = self.get_cond_cols(df_filtered_table, self.currency_element)
        financial_num_cols = self.get_cond_cols(df_filtered_table, self.financial_num_element)
        currency_rows= self.get_cond_rows(df_filtered_table, self.currency_element)
        year_rows = self.get_cond_rows(df_filtered_table, self.year_element)


        currency_row_idx = df_filtered_table.loc[currency_rows, financial_num_cols].index
        currency_row = df_filtered_table.loc[currency_row_idx]
        currency_header = currency_row[currency_row.apply(self.currency_element, axis=1)].fillna('').values.flatten()
        
        year_row_idx = df_filtered_table.loc[year_rows, currency_cols].index
        year_row = df_filtered_table.loc[year_row_idx]
        year_header = year_row[year_row.apply(self.year_element, axis=1)].fillna('').values.flatten()

        if year_header:
            df_filtered_table.columns = pd.MultiIndex.from_arrays([year_header, currency_header], names = ['year', 'currency'])
        else:
            df_filtered_table.columns = pd.MultiIndex.from_arrays([currency_header], names=['currency'])
        return df_filtered_table.drop(year_row_idx.append(currency_row_idx))
```
see below
![image](https://user-images.githubusercontent.com/51902242/95023884-246cfd80-06b2-11eb-883e-f2ae57ebf512.png)

but where I was debugging, I tried:
```
df_filtered_table = table.df_filtered_table
year_rows = table.get_cond_rows(df_filtered_table, table.year_element)
currency_cols = table.get_cond_cols(df_filtered_table, table.currency_element)
year_row_idx = df_filtered_table.loc[year_rows, currency_cols].index
year_row = df_filtered_table.loc[year_row_idx]
year_header = year_row[year_row.apply(table.year_element, axis=1)].fillna('').values.flatten()
if not year_header:
    print('here is the error!')

if not year_header:
    print('here no the error!')

```
see below:
![image](https://user-images.githubusercontent.com/51902242/95024258-79117800-06b4-11eb-89cf-99ba4deb4bc1.png)

here is the full error and the debugging..
```
~/Documents/cs_project/toc(master*) » python3 -i corp_gov_report.py                                                                                                     macone@Macs-MacBook-Pro
Bounding box (Decimal('124.724'), Decimal('46.220'), Decimal('752.754'), Decimal('753.770')) is not fully within parent page bounding box (Decimal('0'), Decimal('0'), Decimal('595.276'), Decimal('807.874'))
Bounding box (Decimal('124.724'), Decimal('46.220'), Decimal('752.754'), Decimal('753.770')) is not fully within parent page bounding box (Decimal('0'), Decimal('0'), Decimal('595.276'), Decimal('807.874'))
Traceback (most recent call last):
  File ""corp_gov_report.py"", line 161, in <module>
    print(table.df_clean_table)
  File ""corp_gov_report.py"", line 128, in df_clean_table
    if year_header:
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/warnings.py"", line 110, in _showwarnmsg
    msg.file, msg.line)
  File ""/Users/macone/Library/Python/3.7/lib/python/site-packages/PyPDF2/pdf.py"", line 1069, in _showwarning
    file.write(formatWarning(message, category, filename, lineno, line))
  File ""/Users/macone/Library/Python/3.7/lib/python/site-packages/PyPDF2/utils.py"", line 69, in formatWarning
    file = filename.replace(""/"", ""\\"").rsplit(""\\"", 1)[1] # find the file name
IndexError: list index out of range
>>> df_filtered_table = table.df_filtered_table
year_rows = table.get_cond_rows(df_filtered_table, table.year_element)
currency_cols = table.get_cond_cols(df_filtered_table, table.currency_element)
year_row_idx = df_filtered_table.loc[year_rows, currency>>> year_rows = table.get_cond_rows(df_filtered_table, table.year_element)
currency_cols = table.get_cond_cols(df_filtered_table, table.currency_element)
year_row_idx = df_filtered_table.loc[year_rows, currency_cols].index
year_row = df_filtered_table.loc[year_row_idx]
year_header = year_row[year_row.apply(ta>>> currency_cols = table.get_cond_cols(df_filtered_table, table.currency_element)
year_row_idx = df_filtered_table.loc[year_rows, currency_cols].index
year_row = df_filtered_table.loc[year_row_idx]
year_header = year_row[year_row.apply(table.year_element, axis=1)].fillna('').values.flatt>>> year_row_idx = df_filtered_table.loc[year_rows, currency_cols].index
>>> year_row = df_filtered_table.loc[year_row_idx]
>>> year_header = year_row[year_row.apply(table.year_element, axis=1)].fillna('').values.flatten()
if not year_header:
    print('here is the error!')

if not year_header:
    print('here no the>>> if not year_header:
...     print('here is the error!')
... 
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/warnings.py"", line 110, in _showwarnmsg
    msg.file, msg.line)
  File ""/Users/macone/Library/Python/3.7/lib/python/site-packages/PyPDF2/pdf.py"", line 1069, in _showwarning
    file.write(formatWarning(message, category, filename, lineno, line))
  File ""/Users/macone/Library/Python/3.7/lib/python/site-packages/PyPDF2/utils.py"", line 69, in formatWarning
    file = filename.replace(""/"", ""\\"").rsplit(""\\"", 1)[1] # find the file name
IndexError: list index out of range
>>> if not year_header:
...     print('here no the error!')
... 
here no the error!
```
you may see the error disappears when I try again to check the if-clause:`if not year_header`. 

Here is the error message

```
Traceback (most recent call last):
  File ""corp_gov_report.py"", line 161, in <module>
    print(table.df_clean_table)
  File ""corp_gov_report.py"", line 128, in df_clean_table
    if year_header:
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/warnings.py"", line 110, in _showwarnmsg
    msg.file, msg.line)
  File ""/Users/macone/Library/Python/3.7/lib/python/site-packages/PyPDF2/pdf.py"", line 1069, in _showwarning
    file.write(formatWarning(message, category, filename, lineno, line))
  File ""/Users/macone/Library/Python/3.7/lib/python/site-packages/PyPDF2/utils.py"", line 69, in formatWarning
    file = filename.replace(""/"", ""\\"").rsplit(""\\"", 1)[1] # find the file name
IndexError: list index out of range
```

The error message is very similar to the replicated example I showed at the beginning, except the following lines.

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py"", line 2890, in __getitem__
    return self._getitem_bool_array(key)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py"", line 2933, in _getitem_bool_array
    stacklevel=3,
```

If any worth, the following is my developing project, you may see the full project in this [repo](https://github.com/fdq09eca/cs-project/tree/master/toc) (the file hitting the error is `corp_gov_report.py` and its content is shown as below)

```
from pdf import PDF, Outline, ReportOutline, PageWithSection
from helper import flatten
import datetime
import re
import pandas as pd


class CorporateGovReport(ReportOutline):

    title_regex = r'^(?=.*report).*corporate governance.*$'

    def __init__(self, outline):
        super().__init__(outline)

    @property
    def audit_fee(self):
        return AuditFee.retrieve(self.pages)


class AuditFee(PageWithSection):

    section_regex = r""^(?!.*Nomination|.*Report)(?=.*REMUNERATION|.*independent|.*external|.*Accountability).*auditor.*$""

    def __init__(self, pages):
        super().__init__(pages)

    @property
    def sections(self):
        return flatten([page.get_section(AuditFee.section_regex) for page in self.pages])

    @property
    def tables(self):
        return [AuditFeeTable.retrieve(section) for section in self.sections]


class AuditFeeTable:
    setting = {
        ""vertical_strategy"": ""text"",
        ""horizontal_strategy"": ""text""
    }

    currency_regex = 'HK'
    financial_num_regex = '\d,?\d*'
    year_regex = '|'.join(map(str, range(int(datetime.datetime.now().year) - 2, int(datetime.datetime.now().year) + 1)))

    def __init__(self, section):
        self.section = section

    @classmethod
    def retrieve(cls, section):
        return cls(section.page)

    @classmethod
    def set_year_regex(cls, current_year: int):
        cls.year_regex = '|'.join(
            map(str, range(current_year - 2, current_year + 1)))
        print(f'Set {cls.__name__}.year_regex to {cls.year_regex}')

    @property
    def raw_table(self):
        return self.section.extract_table(AuditFeeTable.setting)

    @property
    def df_raw_table(self):
        df = pd.DataFrame(self.raw_table)
        return df

    @staticmethod
    def currency_element(x):
        return x.str.contains(AuditFeeTable.currency_regex)

    @staticmethod
    def financial_num_element(x):
        return x.str.contains(AuditFeeTable.financial_num_regex)

    @staticmethod
    def year_element(x):
        return x.str.contains(AuditFeeTable.year_regex)
    
    @staticmethod
    def eng_element(x):
        return x.str.contains(r'[A-Za-z0-9]+')
    
    @staticmethod
    def get_cond_cols(df, element_cond_func):
        return df.apply(element_cond_func).any()
    
    @staticmethod
    def get_cond_rows(df, element_cond_func):
        return df.apply(element_cond_func).any(axis=1)
    
    @property
    def df_fee(self):
        df = self.df_raw_table
        currency_cols = self.get_cond_cols(df, self.currency_element)
        currency_rows = self.get_cond_rows(df, self.currency_element)
        financial_num_rows = self.get_cond_rows(df, self.financial_num_element)
        year_rows = self.get_cond_rows(df, self.year_element)
        df_fee = df.loc[(financial_num_rows|year_rows) , currency_cols]
        return df_fee

   
    @property
    def df_filtered_table(self):
        df_fee = self.df_fee
        df_filtered_table = self.df_raw_table.iloc[df_fee.index]
        df_filtered_table = df_filtered_table.loc[:, self.get_cond_cols(df_filtered_table, self.eng_element)]
        return df_filtered_table
    
    
    @property
    def df_clean_table(self):
        df_filtered_table = self.df_filtered_table
        currency_cols = self.get_cond_cols(df_filtered_table, self.currency_element)
        financial_num_cols = self.get_cond_cols(df_filtered_table, self.financial_num_element)
        currency_rows= self.get_cond_rows(df_filtered_table, self.currency_element)
        year_rows = self.get_cond_rows(df_filtered_table, self.year_element)


        currency_row_idx = df_filtered_table.loc[currency_rows, financial_num_cols].index
        currency_row = df_filtered_table.loc[currency_row_idx]
        currency_header = currency_row[currency_row.apply(self.currency_element, axis=1)].fillna('')
        
        year_row_idx = df_filtered_table.loc[year_rows, currency_cols].index
        year_row = df_filtered_table.loc[year_row_idx]
        year_header = year_row[year_row.apply(self.year_element, axis=1)].fillna('')

        if not year_header.empty and not currency_header.enpty:
            df_filtered_table.columns = pd.MultiIndex.from_arrays([year_header.iloc[0], currency_header.iloc[0]], names = ['year', 'currency'])
        else:
            df_filtered_table.columns = pd.MultiIndex.from_arrays([currency_header.iloc[0]], names=['currency'])
        return df_filtered_table.drop(year_row_idx.append(currency_row_idx))


    @property
    def currency(self):
        pass

    @property
    def currency_unit(self):
        pass

    @property
    def total(self):
        pass

    def __repr__(self):
        return f'{self.__class__.__name__} - {self.section}'


if __name__ == '__main__':
    url, p = 'https://www1.hkexnews.hk/listedco/listconews/sehk/2020/0721/2020072100713.pdf', 61
    # url, p = 'https://www1.hkexnews.hk/listedco/listconews/sehk/2020/0721/2020072100653.pdf', 94
    pdf = PDF.create(url)

    corp_gov_report = pdf.get_outline(CorporateGovReport.title_regex)
    corp_gov_report = CorporateGovReport.create(corp_gov_report[0])
    page = corp_gov_report.audit_fee.pages[0]
    sec = corp_gov_report.audit_fee.sections[0]
    table = corp_gov_report.audit_fee.tables[0]
    print(table.df_clean_table)
    print(sec.text)
    # print(corp_gov_report.pages)

    # page = pdf.
```
here are the dependencies 
```
~/Documents/cs_project/toc(master*) » pip3 freeze 
chardet==3.0.4
numpy==1.19.1
pandas==1.1.0
pdfminer.six==20200517
pdfplumber==0.5.23
Pillow==7.2.0
pycryptodome==3.9.8
PyPDF2==1.26.0
python-dateutil==2.8.1
pytz==2020.1
six==1.15.0
sortedcontainers==2.2.2
Wand==0.6.2
```"
723736268,37189,Backport PR #37144 on branch 1.1.x: BLD: update build requirement for py39 #37135,simonjayhawkins,closed,2020-10-17T11:27:04Z,2020-10-17T13:27:57Z,Backport PR #37144 on branch 1.1.x
723616685,37183,CLN: core/dtypes/cast.py::maybe_casted_values,arw2019,closed,2020-10-17T01:54:14Z,2020-10-17T13:36:00Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref https://github.com/pandas-dev/pandas/pull/36985#issuecomment-705804862"
395556542,24589,Examples for Series ops should be specific to each method.,timgeb,closed,2019-01-03T12:30:01Z,2020-10-17T13:38:54Z,"The code example for [Series.divmod](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.divmod.html) uses `Series.add` and I'm not sure what this is supposed to convey.

In addition, ""Equivalent to `series divmod other`"" and ""See also: `Series.None`"" look strange to me.

 


"
723625446,37185,DOC: Add example to Series.divmod,hongshaoyang,closed,2020-10-17T02:42:14Z,2020-10-17T13:39:56Z,"
- [x] closes #24589
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

-----
[Current doc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.divmod.html); Rendered doc:
![Screenshot 2020-10-17 at 10 44 26 AM](https://user-images.githubusercontent.com/19281800/96326790-3d888d80-1066-11eb-94e7-0a4e0db5d4ba.png)
"
721941158,37126,[REDO] CLN: core/dtypes/cast.py::maybe_downcast_to_dtype ,arw2019,closed,2020-10-15T03:26:12Z,2020-10-17T13:47:14Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref https://github.com/pandas-dev/pandas/pull/37050#issuecomment-708487786

The combination of #37050 and #37024 generated a `mypy` complaint. This PR is resubmission of #37050 with the correct type hint.

cc @simonjayhawkins"
721681386,37118,PERF: regression in DataFrame reduction ops performance #37081,ukarroum,closed,2020-10-14T18:22:06Z,2020-10-17T13:50:01Z,"- [x] closes #37081 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Made the change proposed by @jorisvandenbossche in https://github.com/pandas-dev/pandas/pull/35881#discussion_r504035006 

Did a very quick comparison : 

With `self.dtypes` (old version) : 

```
In [8]: values = np.random.randn(100000, 4)   
   ...: df = pd.DataFrame(values).astype(""int"") 
   ...: %timeit df.sum() 
714 µs ± 21 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

With `self._iter_column_arrays()` (new version) : 

```
In [4]: values = np.random.randn(100000, 4)   
   ...: df = pd.DataFrame(values).astype(""int"") 
   ...: %timeit df.sum() 
477 µs ± 8.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```"
723623163,37184,BUG: Series.loc[timestamp] with MultiIndex with DTI first-level,jbrockmendel,closed,2020-10-17T02:29:15Z,2020-10-17T15:05:17Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The underlying problem here is that `MultiIndex._get_level_number` is not idempotent."
723452689,37165,"ENH: DataFrame __divmod__, __rdivmod__",jbrockmendel,closed,2020-10-16T18:50:53Z,2020-10-17T15:09:04Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
722459342,37140,BUG: algos.diff with datetimelike and NaT,jbrockmendel,closed,2020-10-15T15:51:55Z,2020-10-17T15:10:29Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
723550876,37175,DOC: Same description of `Returns` in `isna` and `notna`,galipremsagar,closed,2020-10-16T21:48:10Z,2020-10-17T16:00:53Z,"#### Location of the documentation


https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isna.html
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.notna.html

#### Documentation problem

There seems to be a typo in `Returns` section where both `isna` and `notna` are having this same description:


_Mask of bool values for each element in DataFrame that indicates whether an element is **not an NA value.**_


#### Suggested fix for documentation

I think `isna` doc should be corrected with:

_Mask of bool values for each element in DataFrame that indicates whether an element is **an NA value.**_
"
723754244,37192,DOC: fix doc isna same as notna,erfannariman,closed,2020-10-17T13:04:26Z,2020-10-17T16:01:50Z,"- [x] closes #37175 
"
723752265,37191,BUG: Regression in Resample.apply raised error when apply affected only a Series,phofl,closed,2020-10-17T12:54:13Z,2020-10-17T16:12:45Z,"- [x] closes #36951
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Error was no longer caught as before."
698065702,36267,ENH: short caption for LaTeX tables,ivanovmg,closed,2020-09-10T14:43:09Z,2020-10-17T16:23:32Z,"#### Is your feature request related to a problem?

``DataFrame.to_latex()`` does not allow one to specify short caption.
Short caption is useful for the creation of neat-looking list of tables, where the caption is short, fitting into one line.

#### Describe the solution you'd like

Either add kwarg ``short_caption`` to ``to_latex`` method.
Alternatively expand the meaning of kwarg ``caption``, so that it can optionally be unpacked into ``full_caption, short_caption``.

#### API breaking implications

No.

#### Describe alternatives you've considered

No.

#### Additional context

Example of LaTeX table with short caption.

```
\begin{table}
  \centering
  \caption[Short caption]{Long-long full caption}
  \label{tab:label}
  \begin{tabular}{lr}
    \toprule
    Col1 & Col2 \\
    \midrule
    1 & 3 \\
    2 & 4 \\
    \bottomrule
  \end{tabular}
\end{table}
```

Possible code snippet:

```
df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})
df.to_latex(caption=('long caption', 'short caption'))
```"
723548548,37174,BUG: ValueError: buffer source array is read-only,dmitra79,closed,2020-10-16T21:42:35Z,2020-10-17T17:04:02Z,"- [ ] I have checked that this issue has not already been reported.

- [ X ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

This seems similar to: https://github.com/pandas-dev/pandas/issues/31710 
I am getting it in a dash application, so it's difficult to get an example from there.
However,  isin() example from the above post also causes the error: 

#### Code Sample, a copy-pastable example

```python
>>> import numpy as np  # v1.16.4, then v1.18.1
>>> import pandas as pd  # v1.0.1
>>>
>>> arr = np.array([1,2,3], dtype=np.int64) # works fine if I don't set the dtype!
>>>
>>> arr.setflags(write=False) # make it read-only
>>>
>>> df = pd.DataFrame({""col"": [2,4,8]})
>>>
>>> test = df.col.isin(arr)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/z0022z7b/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/series.py"", line 4685, in isin
    result = algorithms.isin(self, values)
  File ""/home/z0022z7b/anaconda3/envs/mindsynchro/lib/python3.8/site-packages/pandas/core/algorithms.py"", line 465, in isin
    return f(comps, values)
  File ""pandas/_libs/hashtable_func_helper.pxi"", line 566, in pandas._libs.hashtable.ismember_int64
  File ""stringsource"", line 658, in View.MemoryView.memoryview_cwrapper
  File ""stringsource"", line 349, in View.MemoryView.memoryview.__cinit__
ValueError: buffer source array is read-only
```

#### Problem description

The problem is this error. My application code worked fine  before, but I had to set it up on a new machine with new pandas and cython a couple days ago, and ran into this. 
(Previously I had
  - cython=0.29.14=py37he1b5a44_0
  - pandas=0.24.2=py37he6710b0_0
Don't know if the isin() example would have thrown an error with that) 

#### Output of ``pd.show_versions()``

'''
INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.15.0-118-generic
Version          : #119-Ubuntu SMP Tue Sep 8 12:30:01 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.6.0.post20201009
Cython           : 0.29.21
lxml.etree       : 4.5.2
jinja2           : 2.11.2
IPython          : 7.18.1
fsspec           : 0.8.4
fastparquet      : 0.4.1
matplotlib       : 3.3.2
pyarrow          : 0.17.1
scipy            : 1.5.2
xarray           : 0.16.1
numba            : 0.51.2
'''

</details>
"
723795342,37200,DOC: excel format tag (binary) seems to be missing in the table output,jreback,closed,2020-10-17T16:24:30Z,2020-10-17T18:36:33Z,https://pandas.pydata.org/docs/user_guide/io.html
723806533,37202,DOC: update io-excel tag,knabben,closed,2020-10-17T17:14:41Z,2020-10-17T18:36:37Z,"- [x] closes #37200
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
723804392,37201,Backport PR #37181 on branch 1.1.x (BUG: Fix isin with read-only target),meeseeksmachine,closed,2020-10-17T17:04:54Z,2020-10-17T18:56:41Z,Backport PR #37181: BUG: Fix isin with read-only target
500343867,28688,Possible overflow errors with pd.rolling(...).std(),fjanoos,closed,2019-09-30T15:13:45Z,2020-10-18T00:18:02Z,"I'm experiencing a very weird bug with one very specific dataset - when I try to use pandas' rolling.std function on it.

Basically - the setup is - 
* I have a dataframe with 1 column stored in float32 format
```
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 2186 entries, 2010-11-30 16:00:00 to 2019-08-16 16:00:00
Data columns (total 1 columns):
high    2186 non-null float32
dtypes: float32(1)
memory usage: 25.6 KB
```

```python
ddf.head()
```
![image](https://user-images.githubusercontent.com/923438/65890281-9ceb0100-e370-11e9-85e4-fa55d4a5c44b.png)

Here is a plot of the full timeseries (left column) along with a tail of 200 rows (right column)
![image](https://user-images.githubusercontent.com/923438/65890340-b7bd7580-e370-11e9-82ef-14d7c931d7f3.png)
Note the scale of the numbers here - it goes from 1e8 to 1e1.

Next I compute the rolling standard deviations using rolling means, as per:
```python
rs = np.sqrt((ddf.high ** 2).rolling(10).mean()  - (ddf.high.rolling(10).mean() ) ** 2)
```
This is what the rolling std computed this way it looks like (and matches what I would expect):
![image](https://user-images.githubusercontent.com/923438/65891047-ea1ba280-e371-11e9-8534-72de70ee83f3.png)

But if I use
```python
rs = ddf.rolling(10).high.std()
```
this is what I get:
![image](https://user-images.githubusercontent.com/923438/65891792-24d20a80-e373-11e9-9042-26edba473b05.png)
Something has gotten corrupted - as we can see in the tail of 200 rows in the right.

Now - however, if I rescale the data to make the numbers sit in a smaller range, compute the rolling std and scale it back up 
```python
ddf = ddf.assign( high_rescaled=ddf.high / 1e8 )
rs = ddf.rolling(10).high_rescaled.std() * 1e8
```
This is what I get
![image](https://user-images.githubusercontent.com/923438/65892342-1afcd700-e374-11e9-8054-019b039ba40c.png)
which matches the output computed using rolling means !

Note - the original data was in np.float32 format. So I thought that this bug might be happening because of some overflow issues (which it really should not - the window is only 10 long !!).
So I converted the data to float64 to test this:

```python
ddf.high = ddf.high.astype(np.float64)
display( ddf.info() )
```
```
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 2186 entries, 2010-11-30 16:00:00 to 2019-08-16 16:00:00
Data columns (total 1 columns):
high    2186 non-null float64
dtypes: float64(1)
memory usage: 34.2 KB
```
In this case - even applying rolling.std() to rescaled version of the data (which worked for float32) - is broken !!

![image](https://user-images.githubusercontent.com/923438/65891431-8180f580-e372-11e9-9611-69ad9f509b58.png)

#### Minimum Reproducible Example
Regarding generating an MRE - here is the rub.
The bug seems to be a function of the numerics specific to this dataset - and I cannot reproduce it using random data (since I don't know what feature of the numerics is causing this).

Now, if I save the data as a pickle file (using df.to_pickle) and load it back in - I can reproduce these results exactly. 

However, if I save it as a csv file (for sharing here) - and load it back in - I get a whole new level of badness. The results look really bad for all cases after this round-tripping. This seems to indicate that there is some thing about the *exact numbers* of the dataset that is triggering some numerical problems with rolling.std.

[rs.zip](https://github.com/pandas-dev/pandas/files/3672047/rs.zip)

```python
from pylab import *


import pandas as pd

ddf = pd.read_csv( '/path/to/rs.csv')

ddf.high = ddf.high
display( ddf.info() )


figure()
subplot(121)
plot( ddf.high, '-r' )
subplot(122)
plot( ddf.high.tail(200), '-r' )
gcf().suptitle( 'original data' )

figure()
subplot(121)
rs = np.sqrt( (ddf.high ** 2).rolling(10).mean()  - (ddf.high.rolling(10).mean() ) ** 2 )
plot( rs, '-b' )
subplot(122)
plot( rs.tail(200), '-b' )
gcf().suptitle( 'rolling std computed using rolling means on original data' )

figure()
subplot(121)
rs = ddf.rolling(10).high.std()
plot( rs, '-b' )
subplot(122)
plot( rs.tail(200), '-b' )
gcf().suptitle( 'rolling std computed on original data' )

figure()
subplot(121)
ddf = ddf.assign( high_rescaled=ddf.high / 1e8 )
rs = ddf.rolling(10).high_rescaled.std() * 1e8
plot( rs, '-b' )
subplot(122)
plot( rs.tail(200), '-b' )
gcf().suptitle( 'rolling std computed on rescaled data' )
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.0-9-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: None
pip: 19.2.3
setuptools: 41.2.0
Cython: None
numpy: 1.16.4
scipy: 1.3.1
pyarrow: 0.13.0
xarray: 0.13.0+6.g4617e68b
IPython: 7.8.0
sphinx: None
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2019.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 3.1.0
openpyxl: None
xlrd: 1.2.0
xlwt: None
xlsxwriter: None
lxml.etree: 4.4.1
bs4: 4.8.0
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: 0.3.0
</details>
@jreback "
707700316,36588,CLN: clean up new detected trailing whitespace,plammens,closed,2020-09-23T21:47:47Z,2020-10-18T10:40:18Z,"- [N/A] closes #xxxx (xref [#36386 (comment)](https://github.com/pandas-dev/pandas/pull/36386#discussion_r493865987))
- [N/A] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [N/A] whatsnew entry

Fixed the remaining trailing whitespace problems that weren't being found by `ci/code_checks.sh` because of the file type filters.
"
723593055,37181,BUG: Fix isin with read-only target,dsaxton,closed,2020-10-17T00:02:58Z,2020-10-18T14:04:50Z,"- [x] closes #37174
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
712400798,36757,BUG: GroupBy().fillna() performance regression,alippai,closed,2020-10-01T01:27:16Z,2020-10-18T14:56:32Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

```python
import pandas as pd
import numpy as np

N = 2000
df = pd.DataFrame({""A"": [1] * N, ""B"": [np.nan, 1.0] * (N // 2)})
df = df.sort_values(""A"").set_index(""A"")

df[""B""] = df.groupby(""A"")[""B""].fillna(method=""ffill"")
```

#### Problem description
The groupby + fillna gets extremely slow increasing the N.
This is a regression from 1.0.5->1.1.0.

Note: if I remove the `.set_index(""A"")` it's fast again.

#### Expected Output
Same output, just faster.

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.4.110-1.el7.elrepo.x86_64
Version          : #1 SMP Fri Jan 5 11:35:48 EST 2018
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.6.0.post20200917
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
618508999,34180,BUG: Index formed with list of tuples return wrong output in eq ops,charlesdong1991,closed,2020-05-14T20:17:30Z,2020-10-18T15:01:55Z,"xref #34137

Currently

```python
>>> pd.Index([('a', 'b'), ('b', 'c'), ('c', 'a')]) == ('c', 'a')
>>> array([False, False, False])
```
The issue is such case is treated as a MultiIndex other than plain Index.

Expected:
```python
>>> pd.Index([('a', 'b'), ('b', 'c'), ('c', 'a')]) == ('c', 'a')
>>> array([False, False, True])
```
"
333169647,21517,Comparison of (Multi)Index to tuple interprets tuple as collection,toobaz,closed,2018-06-18T08:06:12Z,2020-10-18T15:01:55Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: mi = pd.MultiIndex.from_product([[1, 2]]*2)

In [3]: mi == mi[0]
Out[3]: False

In [4]: fl = pd.Index(range(4))

In [5]: fl == fl[0]
Out[5]: array([ True, False, False, False])

In [6]: fl == mi[0]
Out[6]: False
```

#### Problem description

The tuple should be interpreted as a key (value), and a boolean array should be returned (and this should probably be the correct behavior also for a flat index, i.e., ``In [6]:``).

#### Expected Output

``Out[5]:``

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.0-6-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: it_IT.UTF-8
LOCALE: it_IT.UTF-8

pandas: 0.24.0.dev0+121.g6185722bb.dirty
pytest: 3.5.0
pip: 9.0.1
setuptools: 39.2.0
Cython: 0.25.2
numpy: 1.14.3
scipy: 0.19.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: 1.5.6
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: 1.2.0dev
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.2.2.post1153+gff6786446
openpyxl: 2.3.0
xlrd: 1.0.0
xlwt: 1.3.0
xlsxwriter: 0.9.6
lxml: 4.1.1
bs4: 4.5.3
html5lib: 0.999999999
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.2.1


</details>
"
723525478,37170,BUG: MultiIndex comparison with tuple #21517,jbrockmendel,closed,2020-10-16T20:54:17Z,2020-10-18T17:35:26Z,"- [x] closes #21517
- [x] closes #34180
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
723555375,37176,BUG: infer_dtype with decimal/complex,jbrockmendel,closed,2020-10-16T21:59:49Z,2020-10-18T17:39:28Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Does this need a whatsnew?"
723877781,37212,TYP: core.window,jbrockmendel,closed,2020-10-18T00:50:59Z,2020-10-18T17:49:26Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
715096359,36896,Single quotes,skorani,closed,2020-10-05T19:18:30Z,2020-10-18T21:38:52Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
723956445,37216,BUG:cannot perform __truediv__ with this index type: DatetimeArray,syejing,closed,2020-10-18T08:27:53Z,2020-10-19T02:23:55Z,"#### Code Sample, a copy-pastable example

```python
df_price_new.pct_change()
```

#### Problem description

Index is time, column is 300 stock names, calculating the growth rate, pandas reported an error

df_price_new.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 81 entries, 2019-09-02 to 2019-12-31
Columns: 300 entries, 000001.SZ to 603993.SH
dtypes: datetime64[ns](1), float64(299)
memory usage: 190.5 KB

#### Expected Output

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-27-de16c3a3e118> in <module>
----> 1 df_price_new.pct_change()

d:\Anaconda3\lib\site-packages\pandas\core\generic.py in pct_change(self, periods, fill_method, limit, freq, **kwargs)
  10239             data = _data
  10240 
> 10241         rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1
  10242         if freq is not None:
  10243             # Shift method is implemented differently when freq is not None

d:\Anaconda3\lib\site-packages\pandas\core\ops\__init__.py in f(self, other, axis, level, fill_value)
    649         if isinstance(other, ABCDataFrame):
    650             # Another DataFrame
--> 651             new_data = self._combine_frame(other, na_op, fill_value)
    652 
    653         elif isinstance(other, ABCSeries):

d:\Anaconda3\lib\site-packages\pandas\core\frame.py in _combine_frame(self, other, func, fill_value)
   5864                 return func(left, right)
   5865 
-> 5866         new_data = ops.dispatch_to_series(self, other, _arith_op)
   5867         return new_data
   5868 

d:\Anaconda3\lib\site-packages\pandas\core\ops\__init__.py in dispatch_to_series(left, right, func, axis)
    273         #  _frame_arith_method_with_reindex
    274 
--> 275         bm = left._mgr.operate_blockwise(right._mgr, array_op)
    276         return type(left)(bm)
    277 

d:\Anaconda3\lib\site-packages\pandas\core\internals\managers.py in operate_blockwise(self, other, array_op)
    362         Apply array_op blockwise with another (aligned) BlockManager.
    363         """"""
--> 364         return operate_blockwise(self, other, array_op)
    365 
    366     def apply(self: T, f, align_keys=None, **kwargs) -> T:

d:\Anaconda3\lib\site-packages\pandas\core\internals\ops.py in operate_blockwise(left, right, array_op)
     36             lvals, rvals = _get_same_shape_values(blk, rblk, left_ea, right_ea)
     37 
---> 38             res_values = array_op(lvals, rvals)
     39             if left_ea and not right_ea and hasattr(res_values, ""reshape""):
     40                 res_values = res_values.reshape(1, -1)

d:\Anaconda3\lib\site-packages\pandas\core\ops\array_ops.py in arithmetic_op(left, right, op)
    184     if should_extension_dispatch(lvalues, rvalues) or isinstance(rvalues, Timedelta):
    185         # Timedelta is included because numexpr will fail on it, see GH#31457
--> 186         res_values = op(lvalues, rvalues)
    187 
    188     else:

d:\Anaconda3\lib\site-packages\pandas\core\ops\invalid.py in invalid_op(self, other)
     51     def invalid_op(self, other=None):
     52         typ = type(self).__name__
---> 53         raise TypeError(f""cannot perform {name} with this index type: {typ}"")
     54 
     55     invalid_op.__name__ = name

TypeError: cannot perform __truediv__ with this index type: DatetimeArray

#### Output of ``pd.show_versions()``

<details>


INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.3.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.15063
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : Chinese (Simplified)_China.936

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.1.1
setuptools       : 49.2.0.post20200714
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : None
sphinx           : 3.1.2
blosc            : None
feather          : None
xlsxwriter       : 1.2.9
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : 0.9.2
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: 0.9.0
bs4              : 4.9.1
bottleneck       : 1.3.2
fsspec           : 0.7.4
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.0
sqlalchemy       : 1.3.18
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.50.1

</details>
"
576283182,32463,Difference between count and nunique formatting,MathieuDutSik,closed,2020-03-05T14:13:53Z,2020-10-19T06:44:53Z,"The count with groupby gives the number of entries while nunique gives the number of unique entries. The problem is that we get an additional column
```python
>>> df = pd.DataFrame({""A"": [0,0,1,1,0], ""B"": [1,2,3,4,5]})
>>> df.groupby(""A"").count()
   B
A   
0  3
1  2
>>> df.groupby(""A"").nunique()
   A  B
A      
0  1  3
1  1  2
```
Why is the A column being added in nunique output?"
723786435,37197,"QST: read_csv succeeded with one file, but failed with another with the same format",playgithub,closed,2020-10-17T15:43:55Z,2020-10-19T07:24:23Z,"version
* python 3.8 64bit for windows
* pandas 1.1.3

code
```python
import pandas as pd

path = 'tmp.txt'

df = pd.read_csv(path, sep='\s+', index_col=0)

print(df)
```

Below are two `tmp.txt` files with different contents but the same format.
The 1st one works with `read_csv`.
The 2nd one doesn't, error message:
```
pandas.errors.ParserError: Error tokenizing data. C error: Expected 4 fields in line 3, saw 5
```
Why? How to fix?


tmp.txt (in utf-8)
```
                         0      1               2               3
0                       项目     附注     2019年12月31日     2018年12月31日
1                    流动资产：
2                     货币资金   七（1）   8,356,153,735   6,365,973,126
3                    结算备付金
4                     拆出资金
5                  交易性金融资产   七（2）     860,894,383     387,261,777
6   以公允价值计量且其变动计入当期损益的金融资产
7                   衍生金融资产   七（3）          85,110      47,542,362
8                     应收票据   七（4）      20,011,631     710,399,926
9                     应收账款   七（5）   3,457,428,686   3,593,707,590
10                  应收款项融资   七（6）     784,417,775
11                    预付款项   七（7）     222,501,827     220,126,772
12                    应收保费
13                  应收分保账款
14               应收分保合同准备金
15                   其他应收款   七（8）     472,000,751     510,753,825
16                 其中：应收利息
17                    应收股利
18                买入返售金融资产
19                      存货   七（9）   3,280,465,303   3,241,739,977
20                    合同资产
21                  持有待售资产
22             一年内到期的非流动资产  七（12）                     190,000,000
23                  其他流动资产  七（13）     320,404,434     313,634,314
24                  流动资产合计         17,774,363,635  15,581,139,669
25                  非流动资产：
26                 发放贷款和垫款
27                    债权投资
28                可供出售金融资产
29                  其他债权投资
30                 持有至到期投资
31                   长期应收款  七（16）     180,000,000
32                  长期股权投资  七（17）     199,805,151     205,738,050
33                其他权益工具投资
34               其他非流动金融资产
35                  投资性房地产
36                    固定资产  七（21）  14,520,366,436  13,629,887,296
37                    在建工程  七（22）   2,901,032,823   2,936,812,592
38                 生产性生物资产
39                    油气资产
40                   使用权资产  七（25）     701,329,178
41                    无形资产  七（26）   1,337,282,523   1,219,578,721
```

tmp.txt (in utf-8)
```
                         0      1               2               3
0                     开发支出
1                       商誉  七（28）     154,940,513     153,707,174
2                   长期待摊费用  七（29）     538,654,362     510,271,130
3                  递延所得税资产  七（30）     518,504,986     252,461,078
4                  其他非流动资产  七（31）                         842,960
5                  非流动资产合计         21,051,915,972  18,909,299,001
6                     资产总计         38,826,279,607  34,490,438,670
7                    流动负债：
8                     短期借款  七（32）   8,491,599,785   5,567,436,870
9                  向中央银行借款
10                    拆入资金
11                 交易性金融负债
12  以公允价值计量且其变动计入当期损益的金融负债
13                  衍生金融负债  七（34）       3,795,000       3,077,741
14                    应付票据  七（35）     860,739,543   1,164,568,692
15                    应付账款  七（36）   1,236,580,062   1,300,781,289
16                    预收款项
17                    合同负债  七（38）     695,400,166     594,503,112
18               卖出回购金融资产款
19               吸收存款及同业存放
20                 代理买卖证券款
21                 代理承销证券款
22                  应付职工薪酬  七（39）     473,972,041     483,015,711
23                    应交税费  七（40）     344,357,096     449,716,675
24                   其他应付款  七（41）   1,555,660,964   1,177,199,622
25                 其中：应付利息
26                    应付股利
27                应付手续费及佣金
28                  应付分保账款
29                  持有待售负债
30             一年内到期的非流动负债  七（43）   1,123,793,163   1,315,823,251
31                  其他流动负债                            300,984,971
32                  流动负债合计         14,785,897,820  12,357,107,934
33                  非流动负债：
34                 保险合同准备金
35                    长期借款  七（45）   1,193,000,000   1,246,875,075
36                    应付债券
37                  其中：优先股
38                     永续债
39                    租赁负债  七（47）     571,281,590
40                   长期应付款  七（48）      72,490,512
41                长期应付职工薪酬
42                    预计负债
43                    递延收益  七（51）     673,448,373     536,834,206
44                 递延所得税负债  七（30）     161,079,790     159,748,849
45                 其他非流动负债
46                 非流动负债合计          2,671,300,265   1,943,458,130
```"
722726665,37149,BUG: GroupBy().fillna() performance regression,smithto1,closed,2020-10-15T22:52:55Z,2020-10-19T09:50:03Z,"- [x] closes #36757 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

A performance regression was introduced with #30679 in handling grouping on a duplicate axis. The regression can be avoided by skipping the call to `get_indexer_non_unique` if the axis is unchanged (i.e. like it is in `fillna`). 

I don't know how to write a test for this fix since it is just an issue of speed. If there is a test pattern or other check that should be included please highlight and I'm happy to add it. 

In lieu of a test, running the minimal example from the issue report on the fixed branch shows the performance fix:

```
In [5]: import pandas as pd
   ...: import numpy as np
   ...: 
   ...: N = 2000
   ...: df = pd.DataFrame({""A"": [1] * N, ""B"": [np.nan, 1.0] * (N // 2)})
   ...: df = df.sort_values(""A"").set_index(""A"")
   ...: 
   ...: %time df.groupby(""A"")[""B""].fillna(method=""ffill"")
   ...:
Wall time: 1.03 ms
Out[5]:
A
1    NaN
1    1.0
1    1.0
1    1.0
1    1.0
    ...
1    1.0
1    1.0
1    1.0
1    1.0
1    1.0
Name: B, Length: 2000, dtype: float64            
```"
724051311,37223,Backport PR #37149 on branch 1.1.x (BUG: GroupBy().fillna() performance regression),meeseeksmachine,closed,2020-10-18T17:01:58Z,2020-10-19T11:03:09Z,Backport PR #37149: BUG: GroupBy().fillna() performance regression
583300342,32788,Request: Changing default na_action of Series.map to 'ignore',tsoernes,closed,2020-03-17T21:09:05Z,2020-10-19T11:41:11Z,"Looking at a pretty big Pandas codebase, through a couple of hundred of uses of `Series.map`, I'm unable to find a single usage where the `na_action` argument is left to its default value of `None`.

How often do people really call the mapping function or dictionary with a bunch of NaNs, instead of just using `fillna`?

I propose changing the default value of `na_action` to `'ignore'`."
585758755,32907,Memory issues with HashTable when resizing,Mibu287,open,2020-03-22T16:34:50Z,2020-10-19T11:54:16Z,"As I read file `pandas/_libs/src/klib/khash.h`, I notice lines 226, 227 in macro `kh_resize_##name` ([source](https://github.com/pandas-dev/pandas/blob/master/pandas/_libs/src/klib/khash.h#L226))
```
h->keys = (khkey_t*)realloc(h->keys, new_n_buckets * sizeof(khkey_t)); \
if (kh_is_map) h->vals = (khval_t*)realloc(h->vals, new_n_buckets * sizeof(khval_t)); \
```
If realloc fail in either statement, it set h->keys or h->vals to NULL and leak original pointer value. Any attempt to deref new value of h->keys or h->vals will lead to undefined behaviors.

Should we assign result of memory allocation to a temporary variable and perform check before reassign to HashTable's internal data?"
199562277,15086,OSError when reading file with accents in file path,JGoutin,closed,2017-01-09T14:16:22Z,2020-10-19T13:39:30Z,"#### Code Sample, a copy-pastable example if possible
`test.txt` and `test_é.txt` are the same file, only the name change:
```python
pd.read_csv('test.txt')
Out[3]: 
   1 1 1
0  1 1 1
1  1 1 1

pd.read_csv('test_é.txt')
Traceback (most recent call last):

  File ""<ipython-input-4-fd67679d1d17>"", line 1, in <module>
    pd.read_csv('test_é.txt')

  File ""d:\app\python36\lib\site-packages\pandas\io\parsers.py"", line 646, in parser_f
    return _read(filepath_or_buffer, kwds)

  File ""d:\app\python36\lib\site-packages\pandas\io\parsers.py"", line 389, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)

  File ""d:\app\python36\lib\site-packages\pandas\io\parsers.py"", line 730, in __init__
    self._make_engine(self.engine)

  File ""d:\app\python36\lib\site-packages\pandas\io\parsers.py"", line 923, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)

  File ""d:\app\python36\lib\site-packages\pandas\io\parsers.py"", line 1390, in __init__
    self._reader = _parser.TextReader(src, **kwds)

  File ""pandas\parser.pyx"", line 373, in pandas.parser.TextReader.__cinit__ (pandas\parser.c:4184)

  File ""pandas\parser.pyx"", line 669, in pandas.parser.TextReader._setup_parser_source (pandas\parser.c:8471)

OSError: Initializing from file failed

```
#### Problem description

Pandas return OSError when trying to read a file with accents in file path.

The problem is new (Since I upgraded to Python 3.6 and Pandas 0.19.2)

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: None
python: 3.6.0.final.0
python-bits: 64
OS: Windows
OS-release: 7
machine: AMD64
processor: Intel64 Family 6 Model 94 Stepping 3, GenuineIntel
byteorder: little
LC_ALL: None
LANG: fr
LOCALE: None.None

pandas: 0.19.2
nose: None
pip: 9.0.1
setuptools: 32.3.1
Cython: 0.25.2
numpy: 1.11.3
scipy: 0.18.1
statsmodels: None
xarray: None
IPython: 5.1.0
sphinx: 1.5.1
patsy: None
dateutil: 2.6.0
pytz: 2016.10
blosc: None
bottleneck: 1.2.0
tables: None
numexpr: 2.6.1
matplotlib: 1.5.3
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 0.999999999
httplib2: None
apiclient: None
sqlalchemy: 1.1.4
pymysql: None
psycopg2: None
jinja2: 2.9.3
boto: None
pandas_datareader: None
</details>
"
724671971,37250,BUG: Rolling by day ,paris0120,closed,2020-10-19T14:23:38Z,2020-10-19T15:09:33Z,"- [ X] I have checked that this issue has not already been reported.

- [ X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
dsf['sigma'] = dsf.groupby('PERMNO').rolling('365D', on='DATE')['RET'].std()


```

#### Problem description

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
C:\Users\paris\.conda\envs\ml\lib\site-packages\pandas\core\frame.py in reindexer(value)
   3726                 try:
-> 3727                     value = value.reindex(self.index)._values
   3728                 except ValueError as err:

C:\Users\paris\.conda\envs\ml\lib\site-packages\pandas\core\series.py in reindex(self, index, **kwargs)
   4398     def reindex(self, index=None, **kwargs):
-> 4399         return super().reindex(index=index, **kwargs)
   4400 

C:\Users\paris\.conda\envs\ml\lib\site-packages\pandas\core\generic.py in reindex(self, *args, **kwargs)
   4461         return self._reindex_axes(
-> 4462             axes, level, limit, tolerance, method, fill_value, copy
   4463         ).__finalize__(self, method=""reindex"")

C:\Users\paris\.conda\envs\ml\lib\site-packages\pandas\core\generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy)
   4476             new_index, indexer = ax.reindex(
-> 4477                 labels, level=level, limit=limit, tolerance=tolerance, method=method
   4478             )

C:\Users\paris\.conda\envs\ml\lib\site-packages\pandas\core\indexes\multi.py in reindex(self, target, method, level, limit, tolerance)
   2324                 # hopefully?
-> 2325                 target = MultiIndex.from_tuples(target)
   2326 

C:\Users\paris\.conda\envs\ml\lib\site-packages\pandas\core\indexes\multi.py in from_tuples(cls, tuples, sortorder, names)
    500 
--> 501             arrays = list(lib.tuples_to_object_array(tuples).T)
    502         elif isinstance(tuples, list):

pandas\_libs\lib.pyx in pandas._libs.lib.tuples_to_object_array()

ValueError: Buffer dtype mismatch, expected 'Python object' but got 'long long'

The above exception was the direct cause of the following exception:

TypeError                                 Traceback (most recent call last)
<ipython-input-12-60461c76cedf> in <module>()
----> 1 dsf['sigma'] = dsf.groupby('PERMNO').rolling('365D', on='DATE')['RET'].std()

C:\Users\paris\.conda\envs\ml\lib\site-packages\pandas\core\frame.py in __setitem__(self, key, value)
   3038         else:
   3039             # set column
-> 3040             self._set_item(key, value)
   3041 
   3042     def _setitem_slice(self, key: slice, value):

C:\Users\paris\.conda\envs\ml\lib\site-packages\pandas\core\frame.py in _set_item(self, key, value)
   3114         """"""
   3115         self._ensure_valid_index(value)
-> 3116         value = self._sanitize_column(key, value)
   3117         NDFrame._set_item(self, key, value)
   3118 

C:\Users\paris\.conda\envs\ml\lib\site-packages\pandas\core\frame.py in _sanitize_column(self, key, value, broadcast)
   3739 
   3740         if isinstance(value, Series):
-> 3741             value = reindexer(value)
   3742 
   3743         elif isinstance(value, DataFrame):

C:\Users\paris\.conda\envs\ml\lib\site-packages\pandas\core\frame.py in reindexer(value)
   3735                     raise TypeError(
   3736                         ""incompatible index of inserted column with frame index""
-> 3737                     ) from err
   3738             return value
   3739 

TypeError: incompatible index of inserted column with frame index

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.9.final.0
python-bits      : 64
OS               : Windows
OS-release       : 10
Version          : 10.0.18362
machine          : AMD64
processor        : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.None

pandas           : 1.1.3
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.0.post20201006
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 5.8.0
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.0
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.19
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None

</details>
"
724172449,37233,REF: collect tests by method,jbrockmendel,closed,2020-10-19T00:02:37Z,2020-10-19T16:31:22Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
724135591,37230,TST: collect unary tests,jbrockmendel,closed,2020-10-18T22:29:49Z,2020-10-19T16:31:48Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
724157467,37232,REF: move misplaced pd.concat tests,jbrockmendel,closed,2020-10-18T23:14:25Z,2020-10-19T16:32:24Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
723421973,37163,DEPR: Series/Index .values special cases,jbrockmendel,closed,2020-10-16T17:56:38Z,2020-10-19T16:50:19Z,"- [x] closes #36957
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
724763495,37254,[Doc] fix spelling errors,lacrosse91,closed,2020-10-19T16:06:01Z,2020-10-19T18:06:50Z,I fixed the spelling errors. #37252
724754442,37253,"fixed spelling errors in whatsnew, userguide, and ecosystem",asharma13524,closed,2020-10-19T15:54:27Z,2020-10-19T18:07:35Z,"Hi there, 

I fixed the spelling errors mentioned here. https://github.com/pandas-dev/pandas/issues/37252

 The 2 ""wit"" spelling errors were confusing, and I could not understand the sentences. 

This is my first pull request, so please excuse my ignorance if I did something incorrectly.

Thanks. 




"
724735555,37252,DOC Fix some typos in the docs,MarcoGorelli,closed,2020-10-19T15:31:03Z,2020-10-19T18:36:39Z,"Here's a few typos I found via `codespell`, first-timers are welcome to open a pull request to fix them
```
doc/source/ecosystem.rst:233: copyed ==> copied
doc/source/whatsnew/v0.24.1.rst:3: Whats ==> What's
doc/source/whatsnew/v0.16.2.rst:150: arithmetics ==> arithmetic
doc/source/whatsnew/v0.24.2.rst:3: Whats ==> What's
doc/source/whatsnew/v1.2.0.rst:183: Expecially ==> Especially
```"
694062666,36136,API: Make ExtensionDtype.construct_array_type a method,TomAugspurger,closed,2020-09-05T11:10:53Z,2020-10-19T19:04:26Z,"This allows a single dtype to support multiple array classes.
For arrow-backed strings, we'll likely want a separate array class
for ease of implementation, clarity. But we'll have a parametrized
dtype.

```python
class StringDtype:
    def __init__(self, storage=""python""):
        self.storage = storage

    def construct_array_type(self):  # regular method
        if self.storage == ""python"":
            return StringArray
        else:
            return ArrowStringArray
```

Closes #36126"
717592791,36988,DOC: Added indexing views to roadmap,TomAugspurger,closed,2020-10-08T19:08:16Z,2020-10-19T21:25:20Z,"This adds the indexing copy vs. view proposal to the roadmap (#36195).

Given the length of the proposal, I've included a summary in the main `roadmap.rst`, with a link to the full proposal in a second document. Here's the summary:

The major change from the Google doc shared earlier is the recommendation for ""Error on Write"" over ""Copy on Write"". I think the proposal should recommend one over the other, and I think ""error on write"" is probably the way to go, but I'm fairly uncertain on this point.

cc @pandas-dev/pandas-core "
724071449,37225,CLN: remove unused arguments in _get_string_slice,jbrockmendel,closed,2020-10-18T18:45:20Z,2020-10-19T22:45:41Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
724140264,37231,REF: move get_op_result_name out of ops.__init__,jbrockmendel,closed,2020-10-18T22:53:01Z,2020-10-19T23:43:56Z,Slowly chipping away at the long-time goal of not having a bunch of code in `ops.__init__`
724656882,37247,DOC: Replace pandas on Ray in ecosystem.rst with Modin,devin-petersohn,closed,2020-10-19T14:09:52Z,2020-10-20T00:35:16Z,"#### Location of the documentation

https://pandas.pydata.org/docs/ecosystem.html#ray

#### Documentation problem

Pandas on Ray does not exist anymore.

#### Suggested fix for documentation

Update references to Modin. 

I will do this.
"
725007935,37266,REF: nargsort incorrectly calling _values_for_argsort,jbrockmendel,closed,2020-10-19T22:01:31Z,2020-10-20T00:38:13Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @jorisvandenbossche "
725080602,37269,DOC: troubleshoot docbuild,jbrockmendel,closed,2020-10-20T00:29:23Z,2020-10-20T00:38:41Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
724610582,37245,"BUG: with integer column labels, .info() throws KeyError after column subsetting with .loc[]",stefan-jansen,closed,2020-10-19T13:28:55Z,2020-10-20T00:51:57Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.random(size=(2, 3)),
                  index=['A', 'B'],
                  columns=[0, 1, 2])

print(df.info())
<class 'pandas.core.frame.DataFrame'>
Index: 2 entries, A to B
Data columns (total 3 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   0       2 non-null      float64
 1   1       2 non-null      float64
 2   2       2 non-null      float64
dtypes: float64(3)
memory usage: 64.0+ bytes
None

print(df.loc[:, [1, 2]])
          1         2
A  0.950714  0.731994
B  0.156019  0.155995

print(df.loc[:, [1, 2]].info())
Traceback (most recent call last):
  File "".../.pyenv/versions/dm/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 2895, in get_loc
    return self._engine.get_loc(casted_key)
  File ""pandas/_libs/index.pyx"", line 70, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/index.pyx"", line 101, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1032, in pandas._libs.hashtable.Int64HashTable.get_item
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1039, in pandas._libs.hashtable.Int64HashTable.get_item
KeyError: 0

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""some_file.py"", line 32, in <module>
    print(df.loc[:, [1, 2]].info())
  File "".../.pyenv/versions/dm/lib/python3.8/site-packages/pandas/core/frame.py"", line 2589, in info
    return DataFrameInfo(
  File "".../.pyenv/versions/dm/lib/python3.8/site-packages/pandas/io/formats/info.py"", line 250, in info
    self._verbose_repr(lines, ids, dtypes, show_counts)
  File "".../.pyenv/versions/dm/lib/python3.8/site-packages/pandas/io/formats/info.py"", line 335, in _verbose_repr
    dtype = dtypes[i]
  File "".../.pyenv/versions/dm/lib/python3.8/site-packages/pandas/core/series.py"", line 882, in __getitem__
    return self._get_value(key)
  File "".../.pyenv/versions/dm/lib/python3.8/site-packages/pandas/core/series.py"", line 989, in _get_value
    loc = self.index.get_loc(label)
  File "".../.pyenv/versions/dm/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 2897, in get_loc
    raise KeyError(key) from err
KeyError: 0

Process finished with exit code 1
```

#### Problem description
When column names are integers, calling `.info()` after selecting a subset of the columns using `.loc[:, [...]]` causes the above error.

Converting to `str` avoids the error:
```
df.columns = df.columns.astype(str)
print(df.loc[:, ['1', '2']].info())

<class 'pandas.core.frame.DataFrame'>
Index: 2 entries, A to B
Data columns (total 2 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   1       2 non-null      float64
 1   2       2 non-null      float64
dtypes: float64(2)
memory usage: 48.0+ bytes
None
```

#### Expected Output
Standard `.info()` DataFrame summary.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-51-generic
Version          : #56-Ubuntu SMP Mon Oct 5 14:28:49 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.3
fastparquet      : 0.4.1
gcsfs            : 0.7.1
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.51.2
None]

</details>
"
318143798,20833,REF: Simplify JoinUnit.is_na for categorical,TomAugspurger,closed,2018-04-26T18:20:33Z,2020-10-20T01:28:50Z,"In JoinUnit.is_na we get the categories

https://github.com/pandas-dev/pandas/blob/6cacdde5630c593999059833b516e1fec60aaf72/pandas/core/internals.py#L5782-L5783

and then check if they're all not na. Categories can't have NA in them, so we can just return False (I think)."
283776480,18888,Category type is discarded with `where` series method,andrewdalecramer,closed,2017-12-21T05:23:33Z,2020-10-20T01:37:29Z,"#### Code Sample, a copy-pastable example if possible

```python
>>> s = pd.Series([""A"",""A"",""B"",""B"",""C""],dtype='category')
>>> s.dtype
CategoricalDtype(categories=['A', 'B', 'C'], ordered=False)
>>> s.where(s!=""C"", s)
0    A
1    A
2    B
3    B
4    C
dtype: object
>>> s2 = pd.Series(range(5))
>>> s2.dtype
dtype('int64')
>>> s2.where(s2<3,s2+3)
0    0
1    1
2    2
3    6
4    7
dtype: int64
```
#### Problem description

Categories are dropped when using the `pd.Series.where` method. This increases memory usage for categorical data by making large temporaries and increases the noisiness of code as the type must be reinforced after the statement.

#### Expected Output

```
>>> s.where(s!=""C"", s).dtype
CategoricalDtype(categories=['A', 'B', 'C'], ordered=False)
```

#### Output of ``pd.show_versions()``

Is the one in the ubuntu repos:

```
>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit: None
python: 3.5.2.final.0
python-bits: 64
OS: Linux
OS-release: 4.4.0-43-Microsoft
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.21.0
pytest: None
pip: 9.0.1
setuptools: 20.7.0
Cython: None
numpy: 1.13.3
scipy: 1.0.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: None
patsy: None
dateutil: 2.6.1
pytz: 2017.3
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: 1.1.0
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0b10
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.8
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
```
"
723608087,37182,Tests for where() with categorical,gabriellm1,closed,2020-10-17T01:09:43Z,2020-10-20T01:37:40Z,"- [X] closes #18888
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`


Hey everyone, 

I wrote some tests for issue #18888. I don't know exactly if the format is correct but if any change is needed I'm happy to make it right. 

Which 'whatsnew entry' should I report for both new tests?

Also, if you think this is a decent contribution and if there's no problem with doing that, it would be nice to tag this PR with hacktoberfest-accepted label :)"
723785334,37196,BUG: JoinUnit.is_na wrong for CategoricalDtype,jbrockmendel,closed,2020-10-17T15:38:41Z,2020-10-20T01:48:35Z,"- [x] closes #20833
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

JoinUnit.is_na is basically checking `isna(self.block.values).all()`.  The check for is_categorical is an attempted optimization bc values.categories is often much smaller than values.  But Categorical represents its NAs in its codes, not in its categories.  So this will incorrectly always return False in the status quo. 

Having trouble coming up with a useful test.  I can adapt a test from test_concat that returns an incorrect answer from is_na, but that does not appear to affect the result of the higher-level pd.concat call."
468783339,27420,DataFrame with MultiIndex raises `TypeError` instead of `KeyError` for inexistent keys ,epassaro,closed,2019-07-16T17:46:28Z,2020-10-20T02:08:29Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd

a = {'A': [1, 2, 3, 4], 'B': [5, 6, 7, 8], 'C': [9, 10, 11, 12]}

df = pd.DataFrame.from_records(a, index=['A', 'B'])

df.loc[(1,6)]
```
#### Problem description

When a non existent key `.loc` raises `TypeError`. Is this the intended behavior?

#### Expected Output

Maybe should raise `KeyError`

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.7.final.0
python-bits: 64
OS: Linux
OS-release: 4.18.0-25-generic
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 4.5.0
pip: 19.0.3
setuptools: 41.0.0
Cython: 0.29.12
numpy: 1.16.4
scipy: 1.1.0
pyarrow: None
xarray: None
IPython: 7.6.1
sphinx: 1.5.6
patsy: None
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: None
tables: 3.5.2
numexpr: 2.6.9
feather: None
matplotlib: 3.1.1
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.3.5
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None

</details>
"
325166864,21168," Wrong ""Too many indexers"" error asking for missing key in Series MultiIndex",toobaz,closed,2018-05-22T07:12:16Z,2020-10-20T02:08:29Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: s = pd.Series(-1, index=pd.MultiIndex.from_product([[0, 1]]*2))

In [3]: s.loc[0, 3]
---------------------------------------------------------------------------
IndexingError                             Traceback (most recent call last)
<ipython-input-3-16ed32312931> in <module>()
----> 1 s.loc[0, 3]

/home/nobackup/repo/pandas/pandas/core/indexing.py in __getitem__(self, key)
   1470             except (KeyError, IndexError):
   1471                 pass
-> 1472             return self._getitem_tuple(key)
   1473         else:
   1474             # we by definition only have the 0th axis

/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_tuple(self, tup)
    873 
    874         # no multi-index, so validate all of the indexers
--> 875         self._has_valid_tuple(tup)
    876 
    877         # ugly hack for GH #836

/home/nobackup/repo/pandas/pandas/core/indexing.py in _has_valid_tuple(self, key)
    218         for i, k in enumerate(key):
    219             if i >= self.obj.ndim:
--> 220                 raise IndexingError('Too many indexers')
    221             try:
    222                 self._validate_key(k, i)

IndexingError: Too many indexers

In [4]: s.loc[3, 1]
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/home/nobackup/repo/pandas/pandas/core/indexing.py in _validate_key(self, key, axis)
   1789                 if not ax.contains(key):
-> 1790                     error()
   1791             except TypeError as e:

/home/nobackup/repo/pandas/pandas/core/indexing.py in error()
   1784                                .format(key=key,
-> 1785                                        axis=self.obj._get_axis_name(axis)))
   1786 

KeyError: 'the label [3] is not in the [index]'

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
<ipython-input-4-0ff3baa1fa82> in <module>()
----> 1 s.loc[3, 1]

/home/nobackup/repo/pandas/pandas/core/indexing.py in __getitem__(self, key)
   1470             except (KeyError, IndexError):
   1471                 pass
-> 1472             return self._getitem_tuple(key)
   1473         else:
   1474             # we by definition only have the 0th axis

/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_tuple(self, tup)
    873 
    874         # no multi-index, so validate all of the indexers
--> 875         self._has_valid_tuple(tup)
    876 
    877         # ugly hack for GH #836

/home/nobackup/repo/pandas/pandas/core/indexing.py in _has_valid_tuple(self, key)
    220                 raise IndexingError('Too many indexers')
    221             try:
--> 222                 self._validate_key(k, i)
    223             except ValueError:
    224                 raise ValueError(""Location based indexing can only have ""

/home/nobackup/repo/pandas/pandas/core/indexing.py in _validate_key(self, key, axis)
   1796                 raise
   1797             except:
-> 1798                 error()
   1799 
   1800     def _is_scalar_access(self, key):

/home/nobackup/repo/pandas/pandas/core/indexing.py in error()
   1783                 raise KeyError(u""the label [{key}] is not in the [{axis}]""
   1784                                .format(key=key,
-> 1785                                        axis=self.obj._get_axis_name(axis)))
   1786 
   1787             try:

KeyError: 'the label [3] is not in the [index]'

```

#### Problem description

The second error is correct, the first is wrong. Might be related to #20951, which however is easier to explain given that it happens on a DataFrame, where there is the axis/level ambiguity.

#### Expected Output

``KeyError: 'the label [3] is not in the [index]'``

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: 1288e5269e576bb299be4d1cdd8062ec87a13470
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.9.0-6-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: it_IT.UTF-8
LOCALE: it_IT.UTF-8

pandas: 0.24.0.dev0+23.g1288e5269
pytest: 3.5.0
pip: 9.0.1
setuptools: 39.0.1
Cython: 0.25.2
numpy: 1.14.3
scipy: 0.19.0
pyarrow: None
xarray: None
IPython: 6.2.1
sphinx: 1.5.6
patsy: 0.5.0
dateutil: 2.7.3
pytz: 2018.4
blosc: None
bottleneck: 1.2.0dev
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.0.0
openpyxl: 2.3.0
xlrd: 1.0.0
xlwt: 1.3.0
xlsxwriter: 0.9.6
lxml: 4.1.1
bs4: 4.5.3
html5lib: 0.999999999
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: 0.2.1

</details>
"
532787813,30053,`MultiIndex.get_loc` throws `KeyError` under specific circumstances,p-himik,closed,2019-12-04T16:15:08Z,2020-10-20T02:08:30Z,"#### Code Sample, a copy-pastable example if possible

```python
from pandas import MultiIndex

key = ('a', 7)
idx = MultiIndex(levels=[['a'], [0, 7], [1]],
                 codes=[[0, 0], [1, 0], [0, 0]],
                 names=['x', 'y', 'z'],
                 sortorder=0)

idx.get_loc(key)
```
#### Problem description

Executing the above code results in
```python
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-19-26c5d4d0888a> in <module>
----> 1 idx.get_loc(key)

~/soft/miniconda3/envs/junk/lib/python3.7/site-packages/pandas/core/indexes/multi.py in get_loc(self, key, method)
   2696 
   2697         if start == stop:
-> 2698             raise KeyError(key)
   2699 
   2700         if not follow_key:

KeyError: ('a', 1, 2)
```
Note that the exception disappears if you do any of the following:
- change `sortorder` to `None`
- change the codes of level 1 to `[0, 1]`
- change the values of level 1 to `[0, 7]`
- completely remove level 2

#### Expected Output

I don't think there should be any exception here since the way I created the index appears to be valid, and the `key` is definitely in there.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.1.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.3.0-23-generic
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 0.25.3
numpy            : 1.15.4
pytz             : 2018.9
dateutil         : 2.7.5
pip              : 18.1
setuptools       : 40.6.3
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.10
IPython          : 7.2.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.0.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None

</details>
"
220033564,15928,AttributeError when slicing a datetime MultiIndex level,toobaz,closed,2017-04-06T21:07:33Z,2020-10-20T02:08:30Z,"#### Code Sample, a copy-pastable example if possible

```python
In [2]: df = pd.read_csv(pd.__path__[0]+'/tests/tools/data/quotes2.csv', parse_dates=['time']).set_index(['ticker', 'time']).sort_index()

In [3]: df.loc['AAPL'].loc[slice('2016-05-25 13:30:00'), :].head()
Out[3]: 
                           bid    ask
time                                 
2016-05-25 13:30:00.075  98.55  98.56
2016-05-25 13:30:00.076  98.55  98.56
2016-05-25 13:30:00.076  98.55  98.56
2016-05-25 13:30:00.076  98.55  98.56
2016-05-25 13:30:00.080  98.55  98.56

In [4]: df.loc[('AAPL', slice('2016-05-25 13:30:00')), :].head()
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-4-552cfee2e501> in <module>()
----> 1 df.loc[('AAPL', slice('2016-05-25 13:30:00')), :].head()

/home/nobackup/repo/pandas/pandas/core/indexing.py in __getitem__(self, key)
   1322             except (KeyError, IndexError):
   1323                 pass
-> 1324             return self._getitem_tuple(key)
   1325         else:
   1326             key = com._apply_if_callable(key, self.obj)

/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_tuple(self, tup)
    833     def _getitem_tuple(self, tup):
    834         try:
--> 835             return self._getitem_lowerdim(tup)
    836         except IndexingError:
    837             pass

/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_lowerdim(self, tup)
    945         # we may have a nested tuples indexer here
    946         if self._is_nested_tuple_indexer(tup):
--> 947             return self._getitem_nested_tuple(tup)
    948 
    949         # we maybe be using a tuple to represent multiple dimensions here

/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_nested_tuple(self, tup)
   1020 
   1021             current_ndim = obj.ndim
-> 1022             obj = getattr(obj, self.name)._getitem_axis(key, axis=axis)
   1023             axis += 1
   1024 

/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_axis(self, key, axis)
   1542             # nested tuple slicing
   1543             if is_nested_tuple(key, labels):
-> 1544                 locs = labels.get_locs(key)
   1545                 indexer = [slice(None)] * self.ndim
   1546                 indexer[axis] = locs

/home/nobackup/repo/pandas/pandas/indexes/multi.py in get_locs(self, tup)
   2173                 # a slice, include BOTH of the labels
   2174                 indexer = _update_indexer(_convert_to_indexer(
-> 2175                     self._get_level_indexer(k, level=i, indexer=indexer)),
   2176                     indexer=indexer)
   2177             else:

/home/nobackup/repo/pandas/pandas/indexes/multi.py in _get_level_indexer(self, key, level, indexer)
   2054                 # note that the stop ALREADY includes the stopped point (if
   2055                 # it was a string sliced)
-> 2056                 return convert_indexer(start.start, stop.stop, step)
   2057 
   2058             elif level > 0 or self.lexsort_depth == 0 or step is not None:

AttributeError: 'int' object has no attribute 'start'
```
#### Problem description

The two calls should be perfectly equivalent.

By the way, if I swap the levels, I get a different error:

``` python
In [16]: df = pd.read_csv(pd.__path__[0]+'/tests/tools/data/quotes2.csv', parse_dates=['time']).set_index(['time', 'ticker']).sort_index()

In [26]: df.loc[:'2016-05-25 13:30:00'].head()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-26-a0298286991a> in <module>()
----> 1 df.loc[:'2016-05-25 13:30:00'].head()

/home/nobackup/repo/pandas/pandas/core/indexing.py in __getitem__(self, key)
   1325         else:
   1326             key = com._apply_if_callable(key, self.obj)
-> 1327             return self._getitem_axis(key, axis=0)
   1328 
   1329     def _is_scalar_access(self, key):

/home/nobackup/repo/pandas/pandas/core/indexing.py in _getitem_axis(self, key, axis)
   1503         if isinstance(key, slice):
   1504             self._has_valid_type(key, axis)
-> 1505             return self._get_slice_axis(key, axis=axis)
   1506         elif is_bool_indexer(key):
   1507             return self._getbool_axis(key, axis=axis)

/home/nobackup/repo/pandas/pandas/core/indexing.py in _get_slice_axis(self, slice_obj, axis)
   1353         labels = obj._get_axis(axis)
   1354         indexer = labels.slice_indexer(slice_obj.start, slice_obj.stop,
-> 1355                                        slice_obj.step, kind=self.name)
   1356 
   1357         if isinstance(indexer, slice):

/home/nobackup/repo/pandas/pandas/indexes/base.py in slice_indexer(self, start, end, step, kind)
   3226         """"""
   3227         start_slice, end_slice = self.slice_locs(start, end, step=step,
-> 3228                                                  kind=kind)
   3229 
   3230         # return a slice

/home/nobackup/repo/pandas/pandas/indexes/multi.py in slice_locs(self, start, end, step, kind)
   1741         # This function adds nothing to its parent implementation (the magic
   1742         # happens in get_slice_bound method), but it adds meaningful doc.
-> 1743         return super(MultiIndex, self).slice_locs(start, end, step, kind=kind)
   1744 
   1745     def _partial_tup_index(self, tup, side='left'):

/home/nobackup/repo/pandas/pandas/indexes/base.py in slice_locs(self, start, end, step, kind)
   3413         end_slice = None
   3414         if end is not None:
-> 3415             end_slice = self.get_slice_bound(end, 'right', kind)
   3416         if end_slice is None:
   3417             end_slice = len(self)

/home/nobackup/repo/pandas/pandas/indexes/multi.py in get_slice_bound(self, label, side, kind)
   1712         if not isinstance(label, tuple):
   1713             label = label,
-> 1714         return self._partial_tup_index(label, side=side)
   1715 
   1716     def slice_locs(self, start=None, end=None, step=None, kind=None):

/home/nobackup/repo/pandas/pandas/indexes/multi.py in _partial_tup_index(self, tup, side)
   1770                 start = start + section.searchsorted(idx, side='left')
   1771             else:
-> 1772                 return start + section.searchsorted(idx, side=side)
   1773 
   1774     def get_loc(self, key, method=None):

TypeError: unorderable types: int() > slice()
```

... while everything works fine if there is one level only.

#### Expected Output

The same as ``Out[3]:``.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit: 4502e82083f4e253630588665a4fc6002c4f32ed
python: 3.5.3.final.0
python-bits: 64
OS: Linux
OS-release: 4.7.0-1-amd64
machine: x86_64
processor: 
byteorder: little
LC_ALL: None
LANG: it_IT.utf8
LOCALE: it_IT.UTF-8

pandas: 0.19.0+743.g4502e8208
pytest: 3.0.6
pip: 9.0.1
setuptools: 33.1.1
Cython: 0.25.2
numpy: 1.12.0
scipy: 0.18.1
xarray: 0.9.1
IPython: 5.1.0.dev
sphinx: 1.4.9
patsy: 0.3.0-dev
dateutil: 2.5.3
pytz: 2016.7
blosc: None
bottleneck: 1.2.0
tables: 3.3.0
numexpr: 2.6.1
feather: 0.3.1
matplotlib: 2.0.0
openpyxl: 2.3.0
xlrd: 1.0.0
xlwt: 1.1.2
xlsxwriter: 0.9.6
lxml: 3.7.1
bs4: 4.5.3
html5lib: 0.999999999
sqlalchemy: 1.0.15
pymysql: None
psycopg2: None
jinja2: 2.8
s3fs: None
pandas_gbq: None
pandas_datareader: 0.2.1


</details>
"
724132866,37228,"TST: indexing tests for #21168, #27420, #15928, #30053",jbrockmendel,closed,2020-10-18T22:14:44Z,2020-10-20T02:08:43Z,"- [x] closes #21168
- [x] closes #27420
- [x] closes #15928
- [x] closes #30053
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
724794066,37255,TST: use equivalent fixtures in test_multilevel,jbrockmendel,closed,2020-10-19T16:49:34Z,2020-10-20T02:09:23Z,
723907319,37215,CLN: de-duplicate in arithmetic/test_numeric,ivanovmg,closed,2020-10-18T05:13:57Z,2020-10-20T05:47:18Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

- Move one test to another one parametrized.
- Address one part of TODO item"
724382252,37238,CLN: clean-up test on addition of series/frames,ivanovmg,closed,2020-10-19T08:09:30Z,2020-10-20T05:47:31Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Cleanup in ``pandas/tests/arithmetic/test_numeric.py``.
- Split ``test_arith_ops_df_compat`` into ``test_add_series`` and ``test_add_frames``.
- Parametrize tests."
724475394,37241,"CI move code directives to pre-commit, remove some outdated checks",MarcoGorelli,closed,2020-10-19T10:15:32Z,2020-10-20T07:39:53Z,"Moving checks for incorrect code block or IPython directives to pre-commit so they're cross-platform and give faster feedback to devs.

Also removing some no-longer-necessary code checks:

- ""Check that no file in the repo contains trailing whitespaces"" is taken care of by the `trailing-whitespace` hook
- ""Check for extra blank lines after the class definition"" is enforced by `black`"
717309849,36981,BUG: fix matplotlib warning on CN color,ivanovmg,closed,2020-10-08T12:42:03Z,2020-10-20T11:37:15Z,"- [ ] closes #36972
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
724965774,37265,BUG: PeriodIndex.dtype comparison with string exhibits contradiction,hickmanw,closed,2020-10-19T20:51:13Z,2020-10-20T16:50:01Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample

```python
index = pd.period_range(start='01Jan2019', end='01Dec2019', freq='M')
string = ""period[M]""
index.dtype == string  # True
index.dtype != string  # also True
```

#### Problem description

`PeriodIndex.dtype` is both `__eq__` and `__ne__` to the string representation of its frequency. I suspect that comparing the dtype to a string is not recommended, but the contradiction seemed worth reporting. I don't know when this was introduced, but it did not occur in 0.25.3.

#### Expected Output
One of the two comparisons to evaluate to `False`.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : a34a408e854b6300dbbdd0b29026d19bc5477d73
python           : 3.8.6.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.14.198-152.320.amzn2.x86_64
Version          : #1 SMP Wed Sep 23 23:57:28 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+830.ga34a408e8
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 49.6.0.post20201009
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : 5.37.3
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.7
lxml.etree       : 4.6.1
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.4
fastparquet      : 0.4.1
gcsfs            : 0.7.1
matplotlib       : 3.3.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 1.0.1
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.2
sqlalchemy       : 1.3.20
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.16.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.51.2
</details>
"
724867196,37261,ENH: match warning messages,ivanovmg,closed,2020-10-19T18:14:58Z,2020-10-20T16:51:01Z,"#### Is your feature request related to a problem?

I wish we could match warning messages using ``pandas._testing.assert_produces_warning``.
Currently it catches the warning category, but does not match the message.
Recently (https://github.com/pandas-dev/pandas/pull/36982) I stumbled upon the need for this feature.

#### Describe the solution you'd like

``pandas._testing.assert_produces_warning`` should take optional keyword argument ``match``, to make it look similar to ``pytest.warns``.

#### API breaking implications

Should not break existing API, as ``match`` would be an optional argument.

#### Describe alternatives you've considered

I used custom way of catching warnings (via ``warnings.catch_warnings``) and matching their messages, but was suggested to give preference to ``assert_produces_warning``.
"
725163468,37271,REF: collect tests by method,jbrockmendel,closed,2020-10-20T03:39:12Z,2020-10-20T17:31:13Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
724133775,37229,CLN: consolidate exception messages in datetimelike validate_listlike,jbrockmendel,closed,2020-10-18T22:19:27Z,2020-10-20T17:31:51Z,"The next step is to consolidate the messages in _validate_scalar, then we should be able to get rid of some duplicate _validate_foo methods"
723627283,37186,call __finalize__ in more methods,arw2019,closed,2020-10-17T02:52:42Z,2020-10-20T18:01:30Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The goal here is to make progress on some of the more uncontroversial/easier content of #28283. I'll split off into another PR if this gets too big"
724797111,37256,"BUG: with integer column labels, .info() throws KeyError ",MarcoGorelli,closed,2020-10-19T16:54:17Z,2020-10-20T19:47:54Z,"- [ ] closes #37245
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
616528531,34128,BUG/ENH: Improve categorical construction when using the iterator in StataReader,bashtage,closed,2020-05-12T10:04:26Z,2020-10-20T19:52:21Z,"- [X] closes #31544
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [X] whatsnew entry
"
724203652,37234,TST/CLN: Split out some to_string tests,dsaxton,closed,2020-10-19T02:06:46Z,2020-10-20T23:05:39Z,"test_format.py is over 3000 lines long so breaking some (but not all) to_string-specific tests into a separate test_to_string.py file (we already have test_to_html.py, test_to_latex.py, and test_to_markdown.py)"
709193850,36642,CI: Deduplicate linting checks,dsaxton,closed,2020-09-25T18:27:27Z,2020-10-20T23:08:47Z,"https://github.com/pandas-dev/pandas/pull/36471 added some linting checks (e.g., black, flake8) to a pre-commit workflow which are duplicated in https://github.com/pandas-dev/pandas/blob/master/ci/code_checks.sh. Once we feel good that the pre-commit checks are right we should go ahead and delete them from `code_checks.sh`.

cc @MarcoGorelli "
725079079,37268,BUG: Don't raise TypeError when converting NA from string to numeric,dsaxton,closed,2020-10-20T00:26:02Z,2020-10-20T23:12:42Z,"- [x] closes #37262
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
705186552,36510,REF: dataframe formatters/outputs,ivanovmg,closed,2020-09-20T21:09:09Z,2020-10-20T23:19:04Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Partially addresses #36407

This is a continuation of https://github.com/pandas-dev/pandas/pull/36434.

- Separated ``StringFormatter`` (or better ``ConsoleFormatter``?) from ``DataFrameFormatter``. Placed it into the new module (subject to discussion).
- Used composition of ``DataFrameFormatter`` in ``HTMLFormatter``, ``LatexFormatter``, ``CSVFormatter`` and ``StringFormatter``. It turned out that the inheritance of each of the formatters from the base ``DataFrameFormatter`` was too complicated to comprehend. The composition seems to suit here better.
- Created new class ``DataFrameRenderer`` to keep methods for outputs in each of the formats.

This is not the ultimate refactoring, but just one more step.
"
710543466,36716,CI: remove duplicated code check #36642,fangchenli,closed,2020-09-28T20:07:54Z,2020-10-21T00:28:58Z,"- [x] closes #36642
"
722666350,37147,CLN: sync min versions,fangchenli,closed,2020-10-15T20:49:08Z,2020-10-21T00:29:38Z,The minimum version of pytables and pytest was not synchronized.
725867002,37283,REF/TST: collect reindex tests,jbrockmendel,closed,2020-10-20T19:04:59Z,2020-10-21T00:45:32Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
723652955,37188,CI: Check for inconsistent pandas namespace usage,dsaxton,closed,2020-10-17T03:51:42Z,2020-10-21T00:54:43Z,Adding a CI check that we aren't (for instance) using Series(...) and pd.Series(...) in the same file. ~This is kept intentionally small in scope (checking only DataFrame and Series for one file name right now) since this is very common in the code base and I'm not sure if this is something we'd actually want to enforce.~
575512496,32434,Rounding issue when using pd.read_excel() followed by pd.to_sql(),mralecthomas,closed,2020-03-04T15:39:10Z,2020-10-21T01:43:03Z,"#### Code Sample, a copy-pastable example if possible
Single column Excel(.xls) file with the following data:

Document Summary
All
Total Sales
1479.9
39002.33
11295.76

```python
import pandas as pd
import sqlalchemy

db_user = ""user""
db_pass = ""pass""
server = ""server_name""
db = ""database""
sql_engine = sqlalchemy.create_engine(
    f""mssql+pyodbc://{db_user}:{db_pass}@{server}/{db}""
    ""?driver=ODBC+Driver+17+for+SQL+Server""
)

# single column Excel file containing mixed string and float data types
file_path = (
    r""doc_summary.xls""
)
df = pd.read_excel(io=file_path, sheet_name=0, header=None)

# check our values once loaded into pandas dataframe
print(df[0].values)
# prints ['Document Summary' 'All' 'Total Sales' 1479.9 39002.32999999997 11295.76000000001]

table_name = ""stopRounding""

# load dataframe into sql table
df.to_sql(table_name, sql_engine, if_exists=""replace"")
# !! this is where floating point values above are being rounded

# retrieve data from table created from dataframe
sql_val = pd.read_sql(f""SELECT * FROM {table_name}"", sql_engine)

print(sql_val.values)
# prints [[0 'Document Summary']
# [1 'All']
# [2 'Total Sales']
# [3 '1479.9']
# [4 '39002.3']
# [5 '11295.8']]

```
#### Problem description

When using pandas.read_excel() to load the data into a dataframe followed by pandas.to_sql(), the float precision is being rounded up to the nearest tenth decimal place.

I read over the pandas documentation and do not see any arguments that can be passed to maintain the float precision.

I've also tried the same thing with pandas.read_csv() followed by pandas.to_sql() using the same data, and the float precision is maintained!  So the issue seems to be with read_excel() only.

#### Expected Output
Document Summary
All
Total Sales
1479.9
39002.33
11295.76
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
pandas           : 1.0.1
numpy            : 1.18.1
pytz             : 2019.3
dateutil         : 2.8.0
pip              : 20.0.2
setuptools       : 41.2.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.2.8
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : 4.4.1
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : 3.0.3
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : 1.3.13
tables           : None
tabulate         : None
xarray           : None
xlrd             : 1.2.0
xlwt             : None
xlsxwriter       : 1.2.8
numba            : None
</details>
"
724870533,37262,BUG: to_numeric() raises TypeError for StringDtype series,RauliRuohonen,closed,2020-10-19T18:20:34Z,2020-10-21T07:11:18Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
pd.to_numeric(pd.Series(['1', None, '2'], dtype='string'))
```

#### Problem description

Running this code results in `TypeError: Invalid object type at position 1`

The same code works for old-style str dtype. Running

```python
import pandas as pd
pd.to_numeric(pd.Series(['1', None, '2'], dtype='str'))
```

outputs

```
0    1.0
1    NaN
2    2.0
dtype: float64
```

This is a common way to convert strings to floats, and switching from the old-style 'str' to the new-style 'string' dtype will break code using it.

#### Expected Output

```
0    1.0
1    NaN
2    2.0
dtype: float64
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.2.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : 0.4.1
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.19
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.50.0

</details>

#### Output of ``pd.show_versions()`` (master)

<details>

INSTALLED VERSIONS
------------------
commit           : c15ded3ed653447415d34b72b62127fa612fd741
python           : 3.8.2.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+829.gc15ded3ed
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.2.0
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : 0.4.1
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.17.1
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.19
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.50.0

</details>
"
723793238,37198,BUG: Regression in Resample.apply raised error when apply affected only a Series,phofl,closed,2020-10-17T16:14:41Z,2020-10-21T10:46:18Z,"- [x] closes #36951
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Error was no longer caught as before.

Sorry made a mistake when pushing..."
369813065,23130,BUG: Series.ne() returns False for both NaN value even if something is passed to fill_value,kaybhutani,closed,2018-10-13T15:35:01Z,2020-10-12T04:04:07Z,"
```


# importing pandas module  
import pandas as pd  
  
# importing numpy module 
import numpy as np 
  
# creating series 1 
series1 = pd.Series([70, 5, 0, 225, 1, 16, np.nan, 10, np.nan]) 
  
# creating series 2 
series2 = pd.Series([27, np.nan, 2, 23, 1, 95, np.nan , 3, 19]) 

#NaN replacement
replace_nan=5

#calling and returning to result variable
result=series1.ne(series2,fill_value=replace_nan)

#display 
result 


```
Series.ne() returns False if value at the index in both series is NaN even if some value is passed to fill_value parameter. 
As in above example, after replacement, condition should be 5 != 5 which should return False. But output was True.

Expected Output:

0     True
1    False
2     True
3     True
4    False
5     True
6     False
7     True
8     True
dtype: bool


Returned Output: 
0     True
1    False
2     True
3     True
4    False
5     True
6     True
7     True
8     True
dtype: bool"
226058724,16218,Incorrect parsing of UTF-16 TSV if file includes Unicode 'FULLWIDTH COLON' (U+FF1A),ecwootten,closed,2017-05-03T17:28:17Z,2020-10-12T06:38:30Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
print ""This is what the data should look like (file with regular colon).\n""
df = pd.read_csv('testcase-utf16.txt', encoding='utf-16', delimiter='\t', engine='python')
print df
print ""\n\nIncorrect dataframe  (file with Unicode full-width colon).\n""
df = pd.read_csv('testcase2-utf16.txt', encoding='utf-16', delimiter='\t', engine='python')
print df

```
Test files:
[testcase2-utf16.txt](https://github.com/pandas-dev/pandas/files/974155/testcase2-utf16.txt)
[testcase-utf16.txt](https://github.com/pandas-dev/pandas/files/974154/testcase-utf16.txt)

#### Problem description

If a UTF-16-encoded TSV file contains a full-width colon, read_csv causes the data to be parsed wrongly. I'm running Python from a Windows command prompt (Windows 10); this bug does **not** happen on a Mac, or on a Windows machine running Python under Cygwin. 

It seems to cause ""some"" amount of data to be skipped - I initially ran into this in the middle of a large file. It raised an exception saying that the line contained too many columns, because the row with the full-width colon had become mashed into the end of the row several lines later.

In the minimal test case above, the two files differ only in that testcase2.txt contains a full-width colon in the first row, and testcase.txt contains a regular colon. In testcase2.txt, everything from the full-width colon disappears from the dataframe:

This is what the data should look like (file with regular colon).

   Block                                               Link
0      9  Survey: 57% of Tokyo high schools demand hair-...
1     10  17 Best ideas about Hair Colors on Pinterest |...
2     11  What It's Like to Change Your Hair Color - I T...
3     12                 Hair Dye: A History - The Atlantic
4     13  A molecular basis for classic blond hair color...


Incorrect dataframe (file with Unicode full-width colon).

   Block                                               Link
0      9  Survey: 57% of Tokyo high schools demand hair-...

#### Expected Output

Dataframe should contain all data, correctly formatted, including fancy Unicode colon - the two dataframes should look the same when printed.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 2.7.6.final.0
python-bits: 64
OS: Windows
OS-release: 8
machine: AMD64
processor: Intel64 Family 6 Model 58 Stepping 9, GenuineIntel
byteorder: little
LC_ALL: None
LANG: None
LOCALE: None.None

pandas: 0.19.2
nose: None
pip: 9.0.1
setuptools: 34.3.2
Cython: None
numpy: 1.12.1
scipy: None
statsmodels: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
matplotlib: 2.0.0
openpyxl: None
xlrd: 0.9.3
xlwt: 0.7.5
xlsxwriter: 0.5.7
lxml: None
bs4: 4.5.1
html5lib: 0.9999999
httplib2: 0.10.3
apiclient: 1.6.2
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
boto: None
pandas_datareader: None
</details>
"
714187324,36841,TST:  Verify parsing of data with encoded special characters (16218),avinashpancham,closed,2020-10-03T22:23:18Z,2020-10-12T06:38:38Z,"

- [x] closes #16218 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
711499828,36731,EA: tighten TimedeltaArray._from_sequence signature,jbrockmendel,closed,2020-09-29T22:16:24Z,2020-10-12T08:50:21Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

xref #36718"
719283641,37074,"BUG: read_json fails to read data fields with "":\\""",dhanesh123in,closed,2020-10-12T10:57:32Z,2020-10-12T11:53:53Z,"- [x ] I have checked that this issue has not already been reported.

- [x ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd
d = '{""columns"":[""col 1"",""col 2""], ""index"":[""row 1"",""row 2""], ""data"":[[""random s3://xyz/abc"",""wherever https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html""],[""c"",""d""]]}'
pd.read_json(d, orient='split')

```

#### Problem description

This leads to an error in pandas 1.1.x and works fine in <=1.0.5. 

#### Expected Output
```
                     col 1                                              col 2
row 1  random s3://xyz/abc  wherever https://pandas.pydata.org/pandas-docs...
row 2                    c                                                  d
```
#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.7.4.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Thu Jun 18 20:49:00 PDT 2020; root:xnu-6153.141.1~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : en_US.UTF-8
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.19.1
pytz             : 2019.3
dateutil         : 2.8.1
pip              : 19.2.3
setuptools       : 41.4.0
Cython           : 0.29.13
pytest           : 5.2.1
hypothesis       : 5.10.4
sphinx           : 2.2.0
blosc            : None
feather          : None
xlsxwriter       : 1.2.1
lxml.etree       : 4.4.1
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.3
IPython          : 7.8.0
pandas_datareader: None
bs4              : 4.8.0
bottleneck       : 1.2.1
fsspec           : 0.8.3
fastparquet      : None
gcsfs            : None
matplotlib       : 3.0.3
numexpr          : 2.7.0
odfpy            : None
openpyxl         : 3.0.0
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.13
tables           : 3.5.2
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.45.1
</details>
"
718646191,37034,REGR: Fix casting of None to str during astype,dsaxton,closed,2020-10-10T15:45:00Z,2020-10-12T14:00:04Z,"- [x] closes #36904
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
716305073,36937,BUG: GH36928 Allow dict_keys to be used as column names by read_csv,abmyii,closed,2020-10-07T08:14:21Z,2020-10-12T14:08:39Z,"- [x] closes #36928 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry"
718935308,37061,REF/TYP: use OpsMixin for StringArray,jbrockmendel,closed,2020-10-11T21:58:55Z,2020-10-12T14:26:17Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

its already mixed in, this just removes the leftover _create_arithmetic_method usage.

This is the last one outside of tests, for which we still have DecimalArray to do."
719413649,37079,Backport PR #37046 on branch 1.1.x: Add whatsnew for #36727,simonjayhawkins,closed,2020-10-12T14:12:53Z,2020-10-12T15:06:28Z,Backport PR #37046 on branch 1.1.x
322040876,21004,(Further) clarify documentation for display.precision,dmolesUC,closed,2018-05-10T18:49:16Z,2020-10-12T15:09:05Z,"The generated ""stable"" [docs for `pandas.set_option()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html) (as well as the [Travis docs](https://pandas-docs.github.io/pandas-docs-travis/generated/pandas.set_option.html) as of this writing) describe `display.precision` as:

> Floating point output precision (number of significant digits)

However, the “[Options and Settings](http://pandas.pydata.org/pandas-docs/version/0.22/options.html)“  docs for describe `display.precision` as

> Floating point output precision in terms of number of places after the decimal

The second description appears to be more accurate (as well as newer; compare for instance the still-Google-popular [0.15 docs](http://pandas.pydata.org/pandas-docs/version/0.15/options.html)).

#### Code Sample

```python
import pandas as pd
df = pd.DataFrame([[1234.0, 123.4, 12.34, 1.234, .1234, .01234, .001234]])
df
#         0      1      2      3       4        5         6
# 0  1234.0  123.4  12.34  1.234  0.1234  0.01234  0.001234
pd.options.display.precision = 1
df
#         0      1     2    3    4        5        6
# 0  1234.0  123.4  12.3  1.2  0.1  1.2e-02  1.2e-03
```

#### Expected Output

The output above clearly isn't significant digits, which (using numpy's [`format_float_positional()`](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.format_float_positional.html)) would look something more like:

```python
import numpy as np
df.applymap(lambda x: np.format_float_positional(x, 1, fractional=False).rstrip('.'))
#       0    1   2  3    4     5      6
# 0  1000  100  10  1  0.1  0.01  0.001
```

#### Problem description

The generated documentation for `set_option()` is inconsistent with the (hand-written?) “Options and Settings” doc. It’s not obvious which is correct until either you try it yourself, or dig through enough old docs to realize the “Options and Settings” doc has been more recently updated.

That said, ""Number of places after the decimal"" is also somewhat unclear, as it's not obvious why `1234.0` is `1234.0` rather than `1.2e+03`, or why `123.4` isn't `1.2e+02`. From experimentation, it looks like pandas decides this based on the entire series, rather than the individual value, so that if I transpose the frame, I do get a consistent one digit after the decimal point:

```python
df.transpose()
#          0
# 0  1.2e+03
# 1  1.2e+02
# 2  1.2e+01
# 3  1.2e+00
# 4  1.2e-01
# 5  1.2e-02
# 6  1.2e-03
```

I'm sure this is documented somewhere, but it's not all that intuitive, and it would be nice if that document was linked from both the `set_option()` and “Options and Settings” docs for `display.precision`.

(Also, it would be nice if pandas had a display option for significant digits. :) ) 

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.5.final.0
python-bits: 64
OS: Darwin
OS-release: 17.5.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.22.0
pytest: None
pip: 10.0.1
setuptools: 39.1.0
Cython: None
numpy: 1.14.3
scipy: 1.0.1
pyarrow: None
xarray: None
IPython: 6.3.1
sphinx: None
patsy: None
dateutil: 2.7.2
pytz: 2018.4
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: 2.2.2
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: 1.0.1
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
</details>
"
719409864,37078,Backport PR #36937 on branch 1.1.x: BUG: GH36928 Allow dict_keys to be used as column names by read_csv,simonjayhawkins,closed,2020-10-12T14:07:40Z,2020-10-12T15:09:56Z,Backport PR #36937 on branch 1.1.x
624868083,34384,TST: use ensure_clean rather than explicit os.remove,jreback,closed,2020-05-26T13:04:56Z,2020-10-12T15:11:12Z,"```
~/pandas-dev$ grep -r os.remove pandas/tests/
pandas/tests/io/excel/test_openpyxl.py:    os.remove(filename)
pandas/tests/io/excel/test_writers.py:    #     os.remove(filename)
pandas/tests/io/excel/test_writers.py:    #     os.remove(filename)
pandas/tests/io/pytables/common.py:            os.remove(path)
```

we should also create a rule in code_checks.sh to avoid doing this in the future"
715255846,36906,CLN: remove inplace kwarg from NDFrame._consolidate,jbrockmendel,closed,2020-10-06T00:45:57Z,2020-10-12T15:23:22Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
446300873,26476,Standard Error of the Mean Unavailable in RollingGroupby,blalterman,closed,2019-05-20T20:41:04Z,2020-10-12T15:30:03Z,"#### Code Sample, a copy-pastable example if possible
Standard Error of the Mean works on `Groupby` objects, but not `RollingGroupby` objects.

`Groupby` object
```python
data = pd.DataFrame({""a"": np.linspace(0, 100, 101), 
                                       ""b"": np.random.random(101).round(1) # values we can group on
                                      })

# print values to 2 decimal places. Just showing it works.
data.groupby(""b"").sem().T.round(2)
```
#### Problem description

`RollingGroupby` object raises `AttributeError`
```python
data = pd.DataFrame({""a"": np.linspace(0, 100, 101), 
                                       ""b"": np.random.random(101).round(1) # values we can group on
                                      })

# print values to 2 decimal places. Just showing it works.
data.rolling(5).sem()
```
I believe API of `Groupby` and `RollingGroupby` are supposed to be as similar as possible. Than an optomized aggregation is available in one, but not the other is a problem.

#### Expected Output

The output should be the equivalent of 
```python
>>> data.rolling(5).std() / (data.rolling(5).count() - ddof).pow(0.5)
```
where ``ddof`` is degrees of freedom.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Linux
OS-release: 3.10.0-693.11.6.el7.x86_64
machine: x86_64
processor: x86_64
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.24.2
pytest: 4.4.1
pip: 19.0.3
setuptools: 41.0.0
Cython: None
numpy: 1.16.3
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 7.4.0
sphinx: 2.0.1
patsy: None
dateutil: 2.8.0
pytz: 2019.1
blosc: None
bottleneck: 1.2.1
tables: 3.5.1
numexpr: 2.6.9
feather: None
matplotlib: 3.0.3
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml.etree: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: 2.10.1
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
718989377,37065,REF: define NDFrame inplace ops non-dynamically,jbrockmendel,closed,2020-10-12T02:17:39Z,2020-10-12T15:30:39Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
511923432,29203,Improve docs on what the axis= kwarg does in individual functions/methods,dlukes,closed,2019-10-24T12:50:52Z,2020-10-12T15:35:01Z,"### `axis=0` or `axis=1`, which is it?

I've always found it hard to remember which axis (`0/""index""` vs. `1/""columns""`) does what for various operations. I suppose some people find it intuitive, while others (like me) find it confusing and inconsistent.

Case in point, `DataFrame.sum` vs. `DataFrame.drop`: if I want **column sums**, I need `axis=0`...

```python
>>> import pandas as pd
>>> df = pd.DataFrame(dict(a=[1, 2, 3], b=[4, 5, 6]))
>>> df
   a  b
0  1  4
1  2  5
2  3  6
>>> df.sum(axis=0)
a     6
b    15
dtype: int64
```

... but if I want to **drop a column**, I need `axis=1`:

```python
>>> df.drop(""a"", axis=1)
   b
0  4
1  5
2  6
```

There's an analogous discrepancy in numpy (which is probably where pandas inherited it from?):

```python
>>> import numpy as np
>>> a = df.to_numpy()
>>> a
array([[1, 4],
       [2, 5],
       [3, 6]])
>>> np.sum(a, axis=0)  # sum columns
array([ 6, 15])
>>> np.delete(a, 0, axis=1)  # delete first column
array([[4],
       [5],
       [6]])
```

I just intuitively conceptualize these operations as working along the same axis, so it's hard for me to internalize that the value of the axis parameter is different in each case. Apparently, [I'm not the only person to find this confusing](https://www.sharpsightlabs.com/blog/numpy-axes-explained/) (quoting from the article: ""For example, in the np.sum() function, the axis parameter behaves in a way that many people think is counter intuitive"").

At the same time, I can imagine that some people find this behavior completely natural (at the very least those who designed the API). And I understand that changing this in pandas while keeping the status quo in numpy would introduce a (probably) worse inconsistency, so I'm not suggesting that.

### Suggestion for improvement

What I am suggesting is **reviewing the documentation of functions/methods using the `axis=` keyword argument** and (where applicable) **improving the description of what it controls in each case**. Pandas is typically used interactively, so documentation is easily accessible. If it contains useful hints on what each `axis` value does (and possibly why), it's not such a big problem if this behavior goes against some people's expectations.

### Examples

For example, based on the [current master docs](https://pandas-docs.github.io/pandas-docs-travis/), the [description of the `axis` parameter for `drop`](https://pandas-docs.github.io/pandas-docs-travis/reference/api/pandas.DataFrame.drop.html#pandas.DataFrame.drop) does a good job at this:

> axis : {0 or ‘index’, 1 or ‘columns’}, default 0
> Whether to drop labels from the index (0 or ‘index’) or columns (1 or ‘columns’).

This makes it reasonably clear to me that if I specify `0`, I'll be removing rows, whereas `1` will result in removing columns.

By contrast, the [description of the `axis` parameter for `sum`](https://pandas-docs.github.io/pandas-docs-travis/reference/api/pandas.DataFrame.sum.html#pandas.DataFrame.sum) is somewhat too generic:

> axis : {index (0), columns (1)}
> Axis for the function to be applied on.

Based on this, I could conclude (and have repeatedly done so) that if I want column sums, I need to ""apply the function on columns"", hence `axis=1` (which is wrong, cf. above).

A revised description could look something like the following:

> axis : {index (0), columns (1)}
> Whether to collapse the index (0 or ‘index’), resulting in column sums, or the columns (1 or ‘columns’), resulting in row sums."
718700502,37038,REF/TYP: define Index methods non-dynamically,jbrockmendel,closed,2020-10-10T20:51:01Z,2020-10-12T15:35:13Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
718616194,37029,DOC: Improve what the `axis=` kwarg does for generic methods,hongshaoyang,closed,2020-10-10T13:04:41Z,2020-10-12T15:35:49Z,"### [`axis=0` or `axis=1`, which is it?](https://github.com/pandas-dev/pandas/issues/29203#issue-511923432)
Opening a PR to discuss #29203 on what it means to do `df.sum(axis=0)` versus `df.sum(axis=1)`. The docs does not make it intuitively clear to the user. I propose a simple change where **""summing across`axis=0`""** makes it clear that you're summing across indexes (i.e. **column sums**).


- [x] Closes #29203
- [x] Passes pytest
- [x] Passes `black pandas`
- [x] Passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
639825126,34827,BUG fix _Unstacker int32 limit in dataframe sizes (pandas-dev#26314),KaonToPion,closed,2020-06-16T16:52:04Z,2020-10-12T16:07:34Z,"- [x] closes #26314 
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry



I have tested it with :
`df = pd.DataFrame(np.random.randint(low=0, high=1500000, size=(90000, 2)), columns=['a', 'b'])`
`df.set_index(['a', 'b']).unstack()`

I am not sure if I should add it as a test, it requires quite some memory.  I am also hesitant about where the test should be located in case it's added."
694202993,36151,ENH/API: errors argument for ExtensionArray.astype,dsaxton,closed,2020-09-05T23:05:18Z,2020-10-12T16:33:18Z,"`Series.astype` and `DataFrame.astype` both allow control over whether the original object should be returned on errors in conversion by setting the `errors` argument to either `""ignore""` or `""raise""`. Should `ExtensionArray.astype` also allow this?

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.astype.html
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html

xref https://github.com/pandas-dev/pandas/pull/35979"
719405082,37076,Backport PR #37034 on branch 1.1.x (REGR: Fix casting of None to str during astype),meeseeksmachine,closed,2020-10-12T14:00:58Z,2020-10-12T16:47:42Z,Backport PR #37034: REGR: Fix casting of None to str during astype
718892089,37058,TYP: core.internals,jbrockmendel,closed,2020-10-11T18:04:07Z,2020-10-12T17:01:31Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Annotating Block as `Union[np.ndarray, ExtensionArray]`, mypy gives tons of complaints about EA not having `.T`, so this adds that, then updates tests as appropriate"
718967348,37064,PERF: ExpandingGroupby,mroeschke,closed,2020-10-12T00:51:38Z,2020-10-12T17:08:12Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

```
       before           after         ratio
     [89cc5bf9]       [6af880e7]
     <master>         <feature/groupby_expanding_indexer>
-      45.5±0.6ms      3.78±0.04ms     0.08  rolling.ExpandingMethods.time_expanding_groupby('Series', 'int', 'max')
-      46.5±0.5ms      3.86±0.06ms     0.08  rolling.ExpandingMethods.time_expanding_groupby('Series', 'int', 'mean')
-      45.4±0.4ms      3.74±0.05ms     0.08  rolling.ExpandingMethods.time_expanding_groupby('DataFrame', 'int', 'min')
-      45.5±0.4ms      3.73±0.06ms     0.08  rolling.ExpandingMethods.time_expanding_groupby('Series', 'int', 'min')
-      46.6±0.3ms      3.80±0.01ms     0.08  rolling.ExpandingMethods.time_expanding_groupby('Series', 'int', 'median')
-        46.7±1ms      3.77±0.05ms     0.08  rolling.ExpandingMethods.time_expanding_groupby('DataFrame', 'int', 'max')
-      46.3±0.6ms      3.73±0.02ms     0.08  rolling.ExpandingMethods.time_expanding_groupby('DataFrame', 'int', 'sum')
-      46.8±0.3ms       3.76±0.1ms     0.08  rolling.ExpandingMethods.time_expanding_groupby('Series', 'int', 'sum')
-      46.7±0.5ms      3.75±0.02ms     0.08  rolling.ExpandingMethods.time_expanding_groupby('DataFrame', 'int', 'mean')
-      47.2±0.3ms      3.78±0.04ms     0.08  rolling.ExpandingMethods.time_expanding_groupby('DataFrame', 'int', 'median')
-      51.2±0.7ms       3.89±0.1ms     0.08  rolling.ExpandingMethods.time_expanding_groupby('Series', 'int', 'std')
-      51.3±0.3ms      3.78±0.02ms     0.07  rolling.ExpandingMethods.time_expanding_groupby('DataFrame', 'int', 'std')
-      62.6±0.6ms      3.98±0.02ms     0.06  rolling.ExpandingMethods.time_expanding_groupby('Series', 'int', 'count')
-      62.2±0.7ms      3.91±0.03ms     0.06  rolling.ExpandingMethods.time_expanding_groupby('DataFrame', 'int', 'count')
-      64.3±0.7ms      3.98±0.05ms     0.06  rolling.ExpandingMethods.time_expanding_groupby('Series', 'float', 'sum')
-      63.5±0.7ms      3.92±0.01ms     0.06  rolling.ExpandingMethods.time_expanding_groupby('DataFrame', 'float', 'max')
-      63.6±0.6ms      3.91±0.03ms     0.06  rolling.ExpandingMethods.time_expanding_groupby('Series', 'float', 'min')
-      63.2±0.7ms      3.86±0.02ms     0.06  rolling.ExpandingMethods.time_expanding_groupby('DataFrame', 'float', 'min')
-        64.2±1ms      3.89±0.04ms     0.06  rolling.ExpandingMethods.time_expanding_groupby('Series', 'float', 'max')
-      64.8±0.8ms      3.91±0.06ms     0.06  rolling.ExpandingMethods.time_expanding_groupby('DataFrame', 'float', 'sum')
-      65.2±0.6ms      3.92±0.05ms     0.06  rolling.ExpandingMethods.time_expanding_groupby('DataFrame', 'float', 'mean')
-      65.2±0.9ms      3.90±0.05ms     0.06  rolling.ExpandingMethods.time_expanding_groupby('Series', 'float', 'median')
-      65.5±0.5ms      3.91±0.02ms     0.06  rolling.ExpandingMethods.time_expanding_groupby('DataFrame', 'float', 'median')
-      65.3±0.2ms      3.86±0.01ms     0.06  rolling.ExpandingMethods.time_expanding_groupby('Series', 'float', 'mean')
-      69.4±0.8ms      4.01±0.07ms     0.06  rolling.ExpandingMethods.time_expanding_groupby('Series', 'float', 'std')
-      69.8±0.6ms      3.93±0.01ms     0.06  rolling.ExpandingMethods.time_expanding_groupby('DataFrame', 'float', 'std')
-      80.1±0.2ms       4.24±0.1ms     0.05  rolling.ExpandingMethods.time_expanding_groupby('DataFrame', 'float', 'count')
-      80.5±0.9ms      4.14±0.02ms     0.05  rolling.ExpandingMethods.time_expanding_groupby('Series', 'float', 'count')

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE INCREASED.
```
"
490746014,28339,[WIP] fix --check-untyped-defs for MyPy,simonjayhawkins,closed,2019-09-08T12:04:36Z,2020-10-12T17:32:43Z,"**tl;dr No need for review!**

no intention of merging this so using patterns that probably won't be accepted.

PR is for visibility and reference only.

using http://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/ as a suitable template for a project plan. resolution of #27568 would come before too much work on adding type hints in general.

There is quite a bit of work involved to enable check-untyped-defs globally so need to keep the phases described in ""Fully annotating a large codebase"" section as out-of-scope for now.

I do not expect to complete this in the near term, so am using Python 3.6 type annotations for variables (better formatting with black and no need to initialize variables to None) and mypy 0.720 for checking (less false positives)

One of the advantages of enabling check-untyped-defs globally, is the additional checking of PRs.

However, for as long as I can keep this branch in sync with master, this additional checking is effectively being performed.

The goal is to cover as much ground in the shortest possible time. The piecemeal approach of submitting separate PRs was, IMO, not allowing us to see the wood from the trees. The extent of the work involved (and changes required) won't be known till the majority of modules have been covered.

So far, nothing major has surfaced. If latent bugs are discovered could always break bits off and submit for review along the way.


"
718723327,37046,DOC: Add whatsnew for #36727,phofl,closed,2020-10-10T23:40:39Z,2020-10-12T19:01:29Z,"xref #36727 

Was not quick enough to add a whatsnew with the PR when 1.1.4 whatsnew was merged."
718716178,37043,ENH: Implement sem for Rolling and Expanding,phofl,closed,2020-10-10T22:39:15Z,2020-10-12T19:01:47Z,"- [x] closes #26476
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

I added sem to the Rolling and Expanding API as mentioned in the issue linked above."
709406572,36654,CLN: dont special-case DatetimeArray indexing,jbrockmendel,closed,2020-09-26T02:46:43Z,2020-10-12T19:06:20Z,"- [x] closes #36210
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

cc @jorisvandenbossche "
494192595,28466,API: DataFrame.__and__ vs Series.__and__ fillna behavior mismatch,jbrockmendel,closed,2019-09-16T17:56:07Z,2020-10-12T21:56:48Z,"Based on tests.series.test_operators `test_logical_ops_df_compat`

```
# GH#1134
s1 = pd.Series([True, False, True], index=list(""ABC""))
s2 = pd.Series([True, True, False], index=list(""ABD""))

exp_ser = pd.Series([True, False, False, False], index=list(""ABCD""))
res_ser = s1 & s2
tm.assert_series_equal(res_ser, exp_ser)

# DataFrame doesn't fill nan with False
exp_frame = pd.DataFrame({""x"": [True, False, np.nan, np.nan]}, index=list(""ABCD""))
res_frame = s1.to_frame() & s2.to_frame()
assert_frame_equal(res_frame, exp_frame)
```

Is there still a compelling reason to keep this mismatched behavior?"
360563340,22724,Mismatch in Series/DataFrame boolean ops behavior,jbrockmendel,closed,2018-09-15T18:19:23Z,2020-10-12T22:10:06Z,"Based on `test_bool_ops_df_compat` from `tests.series.test_operators`:

```
s1 = pd.Series([True, False, np.nan, True], dtype=object)
s2 = pd.Series([True, True, False, np.nan], dtype=object)

>>> s1 & s2
0     True
1    False
2    False
3    False
dtype: bool

>>> s1.to_frame() & s2.to_frame()
       0
0   True
1  False
2    NaN
3    NaN
```

The DataFrame behavior strikes me as More Correct.  Is the mismatch intentional?"
691203711,36074,Pandas MultiIndex causes out of memory error,mahsa-ebrahimian,closed,2020-09-02T17:01:40Z,2020-10-12T23:28:34Z,"I have used multi indexing in my code which is causing out of memory error.

`import pandas as pd
import numpy as np
import io
import requests
url=""https://raw.githubusercontent.com/mahsa-ebrahimian/netflix_project/master/netflix_sample_complete.csv""
movie_db=pd.read_csv(url, error_bad_lines=False)
del movie_db['Unnamed: 0']
iix_n = pd.MultiIndex.from_product([np.unique(movie_db.user_id), np.unique(movie_db.date)]) 
arr = (movie_db.pivot_table('rating', ['user_id', 'date'], 'item_id', aggfunc='sum').reindex(iix_n,copy=False).to_numpy().reshape(movie_db.user_id.nunique(),movie_db.date.nunique(),-1))`

any performance tip or alternative solution to change my data into desired 3D way would be appreciated."
719017209,37068,CLN: remove unnecessary Categorical._validate_setitem_key,jbrockmendel,closed,2020-10-12T03:45:43Z,2020-10-13T00:43:48Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

and some unrelated cleanup in internals"
718923416,37060,TST/CLN: relocate tests for PyTables roundtrip of timezone-aware Index,arw2019,closed,2020-10-11T20:45:07Z,2020-10-13T02:44:43Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
719052073,37070,"BUG: Resolve passing datetime strings of the form ""1-01-01 00:00:00""",znicholls,closed,2020-10-12T05:18:25Z,2020-10-13T04:52:04Z,"- [x] closes #37071 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
719057031,37071,"BUG: Passing datetime strings of the form ""1-01-01 00:00:00""",znicholls,closed,2020-10-12T05:27:32Z,2020-10-13T06:36:34Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
>>> import pandas as pd
>>> pd.Timestamp(""1-01-01 00:00:00"")
Timestamp('2001-01-01 00:00:00')
```

#### Problem description

Passing ""1-01-01 00:00:00"" to ""2001-01-01 00:00:00"" is clearly wrong, this should raise an out of bounds datetime error.

#### Expected Output

```python
>>> import pandas as pd
>>> pd.Timestamp(""1-01-01 00:00:00"")
OutOfBoundsDatetime
```

However, the problem here appears to be deeper than pandas, specifically 

```python
>>> from dateutil import parser
>>> parser.parse(""1-01-01 00:00:00"")
datetime.datetime(2001, 1, 1, 0, 0)
```

If it's helpful, I'm happy to raise an issue in https://github.com/dateutil/dateutil instead. It's not super clear to me where the error really is (maybe dateutil have a very good reason for parsing ""1-01-01 00:00:00"" as ""2001-01-01 00:00:00"").

#### Output of ``pd.show_versions()``

<details>

>>> pd.show_versions()

INSTALLED VERSIONS
------------------
commit           : cd88ee2dddbb25f15742cf8c2d6141da5d6fa342
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 18.7.0
Version          : Darwin Kernel Version 18.7.0: Mon Aug 31 20:53:32 PDT 2020; root:xnu-4903.278.44~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_AU.UTF-8
LOCALE           : en_AU.UTF-8

pandas           : 1.2.0.dev0+746.gcd88ee2dd
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.1.0.post20200704
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : 5.37.1
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : 1.3.6
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.8.3
fastparquet      : 0.4.0
gcsfs            : 0.7.1
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 0.17.1
pytables         : None
pyxlsb           : None
s3fs             : 0.4.2
scipy            : 1.5.1
sqlalchemy       : 1.3.19
tables           : 3.6.1
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : 1.3.0
numba            : 0.48.0

</details>
"
624901482,34385,use ensure_clean rather than explicit os.remove #34384,jnecus,closed,2020-05-26T13:53:08Z,2020-10-13T07:32:40Z,"closes #34384 
I have made a start at this issue #34384. Please let me know if I am along the right lines (beginner contributor).

I've left a space in code_checks.sh where I expect to implement the check for instances of 'os.remove' throughout the code."
233253388,16585,qcut doesn't always properly distribute across bins,AndrewRook,open,2017-06-02T17:47:19Z,2020-10-13T15:41:10Z,"#### Code Sample, a copy-pastable example if possible

```python
import pandas as pd
df = pd.DataFrame({""col"": list(range(100))})
quantiles = pd.qcut(df[""col""], 11)
print(quantiles.groupby(quantiles).size())
```
prints
```
col
(-0.001, 9.0]    10
(9.0, 18.0]       9
(18.0, 27.0]      8
(27.0, 36.0]     10
(36.0, 45.0]      9
(45.0, 54.0]      8
(54.0, 63.0]     10
(63.0, 72.0]      9
(72.0, 81.0]      9
(81.0, 90.0]      9
(90.0, 99.0]      9
Name: col, dtype: int64
```
#### Problem description

`qcut` isn't distributing values across bins quite right for this case – for `range(100)` there are 10 values in the interval `(-0.001, 9.0]` and 9 in all the others. 

#### Expected Output
```
col
(-0.001, 9.0]    10
(9.0, 18.0]       9
(18.0, 27.0]      9
(27.0, 36.0]      9
(36.0, 45.0]      9
(45.0, 54.0]      9
(54.0, 63.0]      9
(63.0, 72.0]      9
(72.0, 81.0]      9
(81.0, 90.0]      9
(90.0, 99.0]      9
Name: col, dtype: int64
```
#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit: None
python: 3.6.1.final.0
python-bits: 64
OS: Darwin
OS-release: 16.5.0
machine: x86_64
processor: i386
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: en_US.UTF-8

pandas: 0.20.1
pytest: None
pip: 9.0.1
setuptools: 27.2.0
Cython: None
numpy: 1.12.1
scipy: None
xarray: None
IPython: None
sphinx: None
patsy: None
dateutil: 2.6.0
pytz: 2017.2
blosc: None
bottleneck: None
tables: None
numexpr: None
feather: None
matplotlib: None
openpyxl: None
xlrd: None
xlwt: None
xlsxwriter: None
lxml: None
bs4: None
html5lib: None
sqlalchemy: None
pymysql: None
psycopg2: None
jinja2: None
s3fs: None
pandas_gbq: None
pandas_datareader: None
```

</details>

Apologies if this has been posted already, but I didn't see anything from searching around."
715289261,36908,BUG: IntervalArray.__eq__ not deferring to Series,jbrockmendel,closed,2020-10-06T02:29:29Z,2020-10-13T16:28:09Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
719635386,37085,API: Series[dt64].astype(dt64tz) vs Datetime(Array|Index)[naive].astype(dt64tz),jbrockmendel,closed,2020-10-12T20:14:22Z,2020-10-13T16:33:47Z,"```
dti = pd.date_range(""2016-01-01"", periods=2)
dtype = dti.tz_localize(""US/Pacific"").dtype

dta = dti._data
ser = pd.Series(dta)

>>> dta.astype(dtype)   # matches dti.astype(dtype)
<DatetimeArray>
['2016-01-01 00:00:00-08:00', '2016-01-02 00:00:00-08:00']
Length: 2, dtype: datetime64[ns, US/Pacific]

>>> ser.astype(dtype)._values
<DatetimeArray>
['2015-12-31 16:00:00-08:00', '2016-01-01 16:00:00-08:00']
Length: 2, dtype: datetime64[ns, US/Pacific]
```

`Series.astype` behavior is defined in `DatetimeBlock.astype`, is equivalent to `dta.tz_localize(""UTC"").tz_convert(dtype.tz)`"
646920381,35039,Fix issue #29837: added test case for aggregation with isnan,biddwan09,closed,2020-06-28T13:26:37Z,2020-10-13T17:32:19Z,"- [x] closes #29837
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
539519101,30317,.to_parquet() to a StringIO() causes python to crash,rootcss,closed,2019-12-18T08:01:40Z,2020-10-13T18:38:00Z,"```python
import pandas as pd
from io import StringIO
df = pd.DataFrame({'x': [1, 2, 2, 4]})
df.to_parquet('some_file')  # works fine

s = StringIO()
df.to_parquet(s)  # this causes the crash with following error
```

Error:
```
libc++abi.dylib: terminating with uncaught exception of type parquet::ParquetException: Arrow error: IOError: string argument expected, got 'bytes'
[1]    42832 abort      python
```


Edit:
The cause for the error is that instead of a String buffer, we should use Bytes buffer (io.BytesIO). But anyhow, the exception should be handled and shouldn't cause the interpreter itself to die."
719722536,37089,CLN: in maybe_cast_to_integer_array assert that dtype arg is an integer dtype,arw2019,closed,2020-10-12T22:59:43Z,2020-10-14T12:25:55Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry"
620681081,34244,ENH: Support out-of-band pickling (protocol 5),jakirkham,closed,2020-05-19T05:06:47Z,2020-10-14T12:27:38Z,"#### Is your feature request related to a problem?

It would be nice if Pandas objects supported [pickle's protocol 5 for out-of-band serialization]( https://www.python.org/dev/peps/pep-0574/ ). This would allow the underlying data to be captured in `PickleBuffer`s (specialized `memoryview`). For libraries using pickle's protocol 5 to transmit data over the wire, this would allow for zero-copy data transmission.

#### Describe the solution you'd like

Pandas objects implement `__reduce_ex__` and if the `protocol` argument is `5` or greater, they construct `PickleBuffer`s out of any data arguments.

#### API breaking implications

NA as it should be possible to fallback to existing behavior for older pickle protocols. Users have to actively opt-in at a higher level API (through pickle) to see any effect.

#### Describe alternatives you've considered

NA

#### Additional context

This would be useful in libraries that support distributed dataframes ;)
"
718870144,37056,Write pickle to file-like without intermediate in-memory buffer,ig248,closed,2020-10-11T16:15:57Z,2020-10-14T12:27:44Z,"Before this change, calling `pickle.dumps()` created an in-memory byte buffer, negating the advantage
of zero-copy pickle protocol 5. After this change, `pickle.dump` writes directly to open file(-like),
cutting peak memory in half in most cases.

Profiling was done with pandas@master and python 3.8.5

Related issues:
- https://github.com/pandas-dev/pandas/issues/34244

![image](https://user-images.githubusercontent.com/25141514/95683850-9929c580-0be5-11eb-8ab1-b9e43407c5bb.png)

## Update: ASV results
`$ asv continuous -f 1.1 origin/master HEAD -b pickle` yields:
```
       before           after         ratio
     [58f74686]       [3c1ee270]
                      <zero-copy-pickle>
-           91.9M            80.1M     0.87  io.pickle.Pickle.peakmem_write_pickle

SOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.
PERFORMANCE INCREASED.
```


- [ ] closes #34244
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
451628388,26637,Improve docs for pandas_version in Dataframe to_json(orient='table'),shsnyder,closed,2019-06-03T18:36:16Z,2020-10-14T12:33:58Z,"#### Code Sample, a copy-pastable example if possible

```python
# Your code here
import pandas as pd
df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6], 'C': [7,8,9] })
df.to_json(orient='table', index='False')
```
#### Problem description
I was attempting to serialize a dataframe and omit the indexes.
In the generated JSON the indexes remained and the pandas_version value in the JSON was ""0.20.0""

The results I obtained on Windows 10 and MacOS 10.14.5 was:
{""schema"": {""fields"":[{""name"":""values"",""type"":""integer""},{""name"":""A"",""type"":""integer""},{""name"":""B"",""type"":""integer""},{""name"":""C"",""type"":""integer""}],""primaryKey"":[null]**,""pandas_version"":""0.20.0""}**, ""data"": [{""index"":0,""A"":1,""B"":4,""C"":7},{""index"":1,""A"":2,""B"":5,""C"":8},{""index"":2,""A"":3,""B"":6,""C"":9}]}'

[this should explain **why** the current behaviour is a problem and why the expected output is a better solution.]

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output
{""schema"": {""fields"":[{""name"":""values"",""type"":""integer""},{""name"":""A"",""type"":""integer""},{""name"":""B"",""type"":""integer""},{""name"":""C"",""type"":""integer""}],""primaryKey"":[null],""pandas_version"":""0.24.2""}, ""data"": [{""A"":1,""B"":4,""C"":7},{""A"":2,""B"":5,""C"":8},{""A"":3,""B"":6,""C"":9}]}'
#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]
INSTALLED VERSIONS
------------------
commit: None
python: 3.7.3.final.0
python-bits: 64
OS: Windows
OS-release: 10
machine: AMD64
processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel
byteorder: little
LC_ALL: None
LANG: en_US.UTF-8
LOCALE: None.None

pandas: 0.24.2
pytest: 4.3.1
pip: 19.0.3
setuptools: 40.8.0
Cython: 0.29.6
numpy: 1.16.2
scipy: 1.2.1
pyarrow: None
xarray: None
IPython: 7.4.0
sphinx: 1.8.5
patsy: 0.5.1
dateutil: 2.8.0
pytz: 2018.9
blosc: None
bottleneck: 1.2.1
tables: 3.5.1
numexpr: 2.6.9
feather: None
matplotlib: 3.0.3
openpyxl: 2.6.1
xlrd: 1.2.0
xlwt: 1.3.0
xlsxwriter: 1.1.5
lxml.etree: 4.3.2
bs4: 4.7.1
html5lib: 1.0.1
sqlalchemy: 1.3.1
pymysql: None
psycopg2: None
jinja2: 2.10
s3fs: None
fastparquet: None
pandas_gbq: None
pandas_datareader: None
gcsfs: None
</details>
"
718546859,37024,TYP: core/dtypes/cast.py,arw2019,closed,2020-10-10T06:32:00Z,2020-10-14T12:35:20Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
718568589,37025,DOC: Clarified pandas_version in to_json,tnwei,closed,2020-10-10T08:35:54Z,2020-10-14T12:46:19Z,"- [X] closes #26637
- [ ] tests added / passed
- [X] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
685077276,35881,REF: ignore_failures in BlockManager.reduce,jbrockmendel,closed,2020-08-25T00:54:54Z,2020-10-14T13:04:24Z,"Moving towards collecting all of the ignore_failures code in one place.

The case where we have object dtypes is kept separate in this PR, will be handled in the next pass."
719703574,37087,CLN: clean Index._id,topper-123,closed,2020-10-12T22:15:38Z,2020-10-14T13:30:01Z,Minor cleanup.
721055501,37112,CLN: runtime imports,jbrockmendel,closed,2020-10-14T01:05:36Z,2020-10-14T14:41:30Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
718899415,37059,"TYP: mostly io, plotting",jbrockmendel,closed,2020-10-11T18:37:56Z,2020-10-14T14:41:51Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

cc @simonjayhawkins can i get your thoughts on two remaining mypy complaints

```
pandas/io/excel/_base.py:795: error: ""Callable[[ExcelWriter], Tuple[str, ...]]"" has no attribute ""__iter__"" (not iterable)  [attr-defined]
pandas/io/stata.py:892: error: unused 'type: ignore' comment
```

The excel one is because it doesn't recognize 

```
@property
@abstractmethod
def supported_extensions(self) -> Tuple[str, ...]:
```

as returning `Tuple[str, ...]`.  I expect if that were resolved it would still complain bc subclasses just pin the tuple as a class attribute instead of a property (easy to make a property, but much more verbose which im not wild about)

The stata unused type:ignore becomes used once we remove stata from the setup.cfg exclusion.  The only working way I've found to make mypy accept `TYPE_MAP: List[Union[int, str]] = list(range(251)) + list(""abcd"")` is with

```
TYPE_MAP: List[Union[int, str]] = cast(List[Union[int, str]], list(range(251))) + list(""abcd"")
```

which i find much less clear than the `# type:ignore`d version."
720661359,37106,CLN: remove unnecessary _validate_foo methods,jbrockmendel,closed,2020-10-13T18:37:02Z,2020-10-14T14:42:29Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

since DatetimeLike and Categorical no longer special-case these, we can inline them."
721054443,37111,"CLN: ops, unnecessary mixin",jbrockmendel,closed,2020-10-14T01:02:28Z,2020-10-14T14:43:15Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
721413635,37113,BUG: Rolling mean with ~4k records returns infinite values on ~95% of the rows,barberogaston,closed,2020-10-14T12:26:04Z,2020-10-14T14:47:29Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
from requests import get

import numpy as np
import pandas as pd

# This is just to get the exact data I used
url = ('https://bitcoincharts.com/charts/chart.json?'
       'm=bitstampUSD&SubmitButton=Draw&r=180&i=Hourly&c=0&s=&e=&Prev=&Next=&'
       't=S&b=&a1=&m1=10&a2=&m2=25&x=0&i1=&i2=&i3=&i4=&v=1&cv=0&ps=0&l=0&p=0&')
res = get(url)
data = res.json()

df = pd.DataFrame(data)

# Using .rolling(20).mean() on all the records (4309 rows) will produce infs on almost all columns
print((df.rolling(20).mean() == np.inf).any())

# However, using only the last 100 records this doesn't happen
print((df.tail(100).rolling(20).mean() == np.inf).any())

# Exactly the same when using only a series. Here I include the count of values.
# We can see that ~95% of the rows turn infinite.
print((df[3].rolling(20).mean() == np.inf).value_counts())
```

#### Problem description

When applying the `.rolling(20).mean()` functions on either a `DataFrame` or a `Series` with ~4k rows approximately 95% of the results are `inf`. However, trimming down the data to only 100 rows, this works fine.

#### Expected Output
This should be the expected output of the last 10 rows:

<img width=""750"" alt=""Captura de Pantalla 2020-10-14 a la(s) 09 24 51"" src=""https://user-images.githubusercontent.com/12915138/95988378-33188a80-0dff-11eb-9ba6-476c09264de1.png"">

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Mon Aug 31 22:12:52 PDT 2020; root:xnu-6153.141.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.18.5
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.0.post20201006
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
713284473,36799,BUG/CLN: Clean float / complex string formatting,dsaxton,closed,2020-10-02T01:24:54Z,2020-10-14T15:06:37Z,"Noticed while working on another bug. The _is_number helper here is wrong and can cause incorrect results given that this code path is hit by arbitrary strings (e.g., it thinks ""foo"" is a number). Also the _trim_zeros_complex helper apparently does nothing:
```python
[ins] In [3]: _trim_zeros_float([""0.00000""])
Out[3]: ['0.0']

[ins] In [4]: _trim_zeros_complex([""1.000+1.000000j""])
Out[4]: ['1.000+1.000000j']
```"
715148147,36900,DEPR: Index.ravel returning an ndarray,jbrockmendel,closed,2020-10-05T20:42:51Z,2020-10-14T15:34:11Z,"- [x] closes #19956
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Make Index.ravel() behavior match every other ravel() method in existence."
718754465,37050,CLN: core/dtypes/cast.py::maybe_downcast_to_dtype,arw2019,closed,2020-10-11T04:17:50Z,2020-10-14T16:15:11Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
721573182,37116,"Revert ""CLN: core/dtypes/cast.py::maybe_downcast_to_dtype""",simonjayhawkins,closed,2020-10-14T15:44:15Z,2020-10-14T16:41:43Z,Reverts pandas-dev/pandas#37050
718062767,37001,VIS: Allow xlabel and ylabel in DataFrame.plot.scatter plot and hexbin plot,treuherz,closed,2020-10-09T11:14:05Z,2020-10-14T17:54:11Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl

ax = (pd.DataFrame(np.random.randn(100, 2), columns=['A','B'])
    .plot.scatter(
        x='A',
        y='B',
        xlabel='my x label',
        ylabel='my y label',
    )
)
```

#### Problem description

In #34223 (released in 1.1.0), the ability to set `xlabel` and `ylabel` in `DataFrame.plot()` was added. This works fine when calling `plot()` directly or `df.plot.line()`. When calling `df.plot.scatter()` with these args, you still get the index labels as the axis labels, instead of the strings you pass in.

These args are not documented to be incompatible or unavailable on this function, and no error is printed.

![Actual Output](https://user-images.githubusercontent.com/1574403/95576518-7dc58b80-0a28-11eb-8e74-58864c47a802.png)

#### Expected Output

The plot's axis labels should be the strings provided, ""my x label"" and ""my y label"".

#### Output of ``pd.show_versions()``

<details>

```
INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Sun Jul  5 00:43:10 PDT 2020; root:xnu-6153.141.1~9/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
```
</details>
"
721417509,37114,TST: add message matches to pytest.raises in test_duplicate_labels.py GH30999,theMogget,closed,2020-10-14T12:31:40Z,2020-10-14T18:07:12Z,"Reference https://github.com/pandas-dev/pandas/issues/30999

- [x] tests added / passed
- [x] passes `black pandas`
- [ ]  passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Hello! This is my first PR here. I followed the steps in the contributing guidelines [here](https://github.com/pandas-dev/pandas/blob/master/doc/source/development/contributing.rst#contributing-your-changes-to-pandas), but I didn't see anything about `git diff upstream/master -u -- ""*.py"" | flake8 --diff`. 

When I run the command locally, with `origin/master`, it returns no output. I get fatal with `upstream/master`. Is that expected, and if it isn't, could anyone advise on how to resolve the issue please? 

Thank you!"
453663317,26713,Performance for TimedeltaArray.__iter__ and PeriodArray.__iter__,TomAugspurger,closed,2019-06-07T19:34:28Z,2020-10-14T18:19:25Z,Right now `DatetimeArray.__iter__` does it's boxing in chunks. We could do something similar for TimedeltaArray and possibly PeriodArray.
706527684,36551,PERF: TimedeltaArray.__iter__,jbrockmendel,closed,2020-09-22T16:27:26Z,2020-10-14T18:30:06Z,"- [x] closes #26713
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Also 2d compat for TDA/DTA `__iter__`"
518728227,29442,DataFrame.groupby doesn't preserve _metadata,pandichef,closed,2019-11-06T20:36:45Z,2020-10-14T18:37:51Z,"I'm following the 0.25.3 [documentation](https://pandas.pydata.org/pandas-docs/stable/development/extending.html#subclassing-pandas-data-structures) on using _metadata.

```python
sdf = SubclassedDataFrame2(my_data_as_dictionary)
sdf.added_property = 'hello pandas'
df = sdf.groupby('mycategorical')[['myfloat1', 'myfloat2']].sum()
print(df.added_property)
```
The above output produces the result:
AttributeError: 'DataFrame' object has no attribute 'added_property'

**added_property** is _not_ being ""passed to manipulation results"" as described in the documentation."
719409093,37077,October 2020 Developer Meeting,TomAugspurger,closed,2020-10-12T14:06:35Z,2020-10-14T19:42:10Z,"The monthly dev meeting is Wednesday October 14th, at 18:00 UTC. Our calendar is at https://pandas.pydata.org/docs/development/meeting.html#calendar to check your local time.

Video Call:  https://zoom.us/j/942410248?pwd=T2l2Qi9vaC82Z294ZEtFczYxMVM2dz09
Minutes: https://docs.google.com/document/u/1/d/1tGbTiYORHiSPgVMXawiweGJlBw5dOkVJLY-licoBmBU/edit?ouid=102771015311436394588&usp=docs_home&ths=true

**Note: the call URL is different than previous meetings. You may need to download the meeting calendar again**. Zoom changed some things w.r.t. authentication and waiting rooms.

Please add items you'd like to see discussed to the agenda. All are welcome to attend."
677708170,35688,Fix GH-29442 DataFrame.groupby doesn't preserve _metadata,Japanuspus,closed,2020-08-12T13:47:15Z,2020-10-14T19:50:59Z,"This bug is a regression in v1.1.0 and was introduced by the fix for GH-34214 in commit [6f065b].

Underlying cause is that the `*Splitter` classes do not use the `._constructor` property and do not call `__finalize__`.

Please note that the method name used for `__finalize__` calls was my best guess since documentation for the value has been hard to find.

[6f065b]: https://github.com/pandas-dev/pandas/commit/6f065b6d423ea211d803e8be93c27f547541c372

- [x] closes #29442
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
718866056,37055,ENH: Use Kahan summation and Welfords method to calculate rolling var and std,phofl,closed,2020-10-11T15:56:35Z,2020-10-14T20:04:13Z,"- [x] xref #37051
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

As suggested by @mroeschke Kahan summation fixes the numerical problems. Additionally I used Welfords Method to calculate ``ssqdm``, because previously the tests I have added would return

```
0             NaN
1    3.500000e+34
2    3.000000e+34
3    3.000000e+34
4    3.000000e+34
5    3.000000e+34
6    3.000000e+34
7    3.000000e+34
8    3.000000e+34
9    3.000000e+34
dtype: float64
```

for ``var()``. I am running the asv and will post the results when available"
523071758,29621,BLD:can't install dev requirements after upgrading to python 3.8.0,ShaharNaveh,closed,2019-11-14T19:56:07Z,2020-10-15T00:11:03Z,"After upgrading to python 3.8.0. I purged my entire venv (pip).
So I decided to reinstall all of the packages.
And while running the command ```python -m pip install -r requirements-dev.txt``` I got this output:
```
Building wheels for collected packages: bottleneck, pyarrow
  Building wheel for bottleneck (PEP 517) ... done
  Created wheel for bottleneck: filename=Bottleneck-1.3.0-cp38-cp38-linux_x86_64.whl size=402563 sha256=113ba04f2528195389b7c40b22edd36aef331a2b28728754c3f14c3f4db54146
  Stored in directory: /home/bummy/.cache/pip/wheels/10/90/86/d59b7357e08bbc4c1018dffede068dc2ecb170b16bf20024bc
  Building wheel for pyarrow (PEP 517) ... error
  ERROR: Command errored out with exit status 1:
   command: /home/bummy/Documents/Github/Community/Python/Venvs/venv-pandas/bin/python /home/bummy/Documents/Github/Community/Python/Venvs/venv-pandas/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py build_wheel /tmp/tmpunb7hoky                                                                                                                                                                                                                 
       cwd: /tmp/pip-install-4yj1nxdu/pyarrow                                                                                                                                                                                    
  Complete output (427 lines):                                                                                                                                                                                                   
  running bdist_wheel                                                                                                                                                                                                            
  running build                                                                                                                                                                                                                  
  running build_py                                                                                                                                                                                                               
  creating build                                                                                                                                                                                                                 
  creating build/lib.linux-x86_64-3.8                                                                                                                                                                                            
  creating build/lib.linux-x86_64-3.8/pyarrow                                                                                                                                                                                    
  copying pyarrow/util.py -> build/lib.linux-x86_64-3.8/pyarrow                                                                                                                                                                  
  copying pyarrow/types.py -> build/lib.linux-x86_64-3.8/pyarrow 

.....................................................

copying pyarrow/tests/data/parquet/v0.7.1.column-metadata-handling.parquet -> build/lib.linux-x86_64-3.8/pyarrow/tests/data/parquet
  copying pyarrow/tests/data/parquet/v0.7.1.parquet -> build/lib.linux-x86_64-3.8/pyarrow/tests/data/parquet
  copying pyarrow/tests/data/parquet/v0.7.1.some-named-index.parquet -> build/lib.linux-x86_64-3.8/pyarrow/tests/data/parquet
  running build_ext
  creating build/temp.linux-x86_64-3.8
  -- Running cmake for pyarrow
  cmake -DPYTHON_EXECUTABLE=/home/bummy/Documents/Github/Community/Python/Venvs/venv-pandas/bin/python  -DPYARROW_BOOST_USE_SHARED=on -DCMAKE_BUILD_TYPE=release /tmp/pip-install-4yj1nxdu/pyarrow
  error: command 'cmake' failed with exit status 1
  ----------------------------------------
  ERROR: Failed building wheel for pyarrow
  Running setup.py clean for pyarrow
Successfully built bottleneck
Failed to build pyarrow
ERROR: Could not build wheels for pyarrow which use PEP 517 and cannot be installed directly                                                                                                     
```

"
632396026,34617,BUG: rolling aggregations not checking monotonic index when grouping by,frndrs,closed,2020-06-06T11:30:05Z,2020-10-15T00:28:32Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python

import pandas as pd
import numpy as np

df = pd.DataFrame({'a':['2020-06-01 12:00',
                       '2020-06-01 12:30',
                       '2020-06-01 13:00',
                       '2020-06-01 14:00',
                       np.nan],
                   'b':[1,2,3,4,5],
                   'c':[1,1,1,1,1]})
df.a = pd.to_datetime(df.a)
df.set_index('a',inplace=True)
df.groupby('c').b.rolling('60min').sum() 

''' 

Note that if there's no group by, ie:
df.b.rolling('60min').sum() 
 index monotonicity is checked and an error is reported due to the NaT.
''''

```

#### Problem description

When using a rolling window on a timestamp index + a group by, there is no validation about the monotonicity of the index. This can lead to unexpected behaviours, as the value of the calculation can change if the index is not monotonic.

#### Expected Output

Output:
c a
1  2020-06-01 12:00:00     1.0
   2020-06-01 12:30:00     3.0
   2020-06-01 13:00:00     6.0
   2020-06-01 14:00:00    10.0
   NaT                    15.0

Expected (correct) output:

c  a                  
1  2020-06-01 12:00:00    1.0
   2020-06-01 12:30:00    3.0
   2020-06-01 13:00:00    5.0
   2020-06-01 14:00:00    4.0

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : None
python           : 3.7.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.9.184-linuxkit
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : C.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.0.4
numpy            : 1.18.0
pytz             : 2019.1
dateutil         : 2.7.5
pip              : 19.3.1
setuptools       : 41.4.0
Cython           : None
pytest           : 4.1.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : 0.9.3
psycopg2         : 2.8.2 (dt dec pq3 ext lo64)
jinja2           : 2.10.1
IPython          : 7.11.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fastparquet      : None
gcsfs            : None
lxml.etree       : None
matplotlib       : 3.1.1
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pytest           : 4.1.1
pyxlsb           : None
s3fs             : None
scipy            : 1.2.0
sqlalchemy       : 1.2.16
tables           : None
tabulate         : 0.8.3
xarray           : None
xlrd             : None
xlwt             : None
xlsxwriter       : None
numba            : None

</details>
"
721702848,37120,ERR: Removing quotation marks in error message,gfyoung,closed,2020-10-14T18:57:43Z,2020-10-15T00:48:34Z,"Follow-up to https://github.com/pandas-dev/pandas/pull/36852

When reviewing, I got confused and thought `iteration` was somehow an argument to `read_csv` (like `nrows`).  Since it is now, the quotation marks don't make as much sense to have.

Doing this as a follow-up since the original PR was a pure refactoring."
718715756,37042,DOC: Add summary to interpolate,erictleung,closed,2020-10-10T22:35:53Z,2020-10-15T01:45:22Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

The interpolate method was missing a short summary. I hope this addition is reasonable. Happy to change it with feedback."
714354205,36862,DOC: make return type documentation of series methods consistent #35409,junjunjunk,closed,2020-10-04T16:49:03Z,2020-10-15T06:09:08Z,"- [x] closes #35409 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`"
714401677,36869,DOC: Distinguish between different types of boolean indexing #10492,junjunjunk,closed,2020-10-04T21:00:55Z,2020-10-15T06:09:22Z,"- [x] closes #10492
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
720446785,37102,VIS: Accept xlabel and ylabel for scatter and hexbin plots,treuherz,closed,2020-10-13T15:44:23Z,2020-10-15T08:02:43Z,"- [x] closes #37001
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
721731354,37122,Backport PR #35688 on branch 1.1.x: Fix GH-29442 DataFrame.groupby doesn't preserve _metadata,simonjayhawkins,closed,2020-10-14T19:44:15Z,2020-10-15T12:02:59Z,"Backport PR #35688
"
719044234,37069,BUG: preserve timezone info when writing empty tz-aware frame to HDF5,arw2019,closed,2020-10-12T05:03:20Z,2020-10-15T13:27:07Z,"- [ ] partially closes #20594
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Will handle series case in follow-on"
701094516,36357,REF: Dispatch string methods to ExtensionArray,TomAugspurger,closed,2020-09-14T13:17:27Z,2020-10-15T13:44:06Z,"This is a large refactor of our string methods. The goal is to have the `Series.str` accessor dispatch the actual compute down to the ExtensionArray. The motivation is to allow a Series backed by Arrow memory to use the Arrow string kernel (e.g. `Series.str.lower()` on an Arrow-backed StringArray should eventually call `pyarrow.compute.utf8_lower()`.)

To facilitate this, I made the following changes

1. Split `core/strings.py` into a sub package:
  - `core/strings/accessor.py`: Implements the `Series.str` accessor, which (mostly) just delegates to the EA and wraps
  - `core/strings/object_array.py`: Implements the string methods for object-type ndarray. 
  - `core/strings/categorical.py`, `core/strings/string_array.py`, implements categorical & StringArray-specific methods
2. Defines a new `ExtensionArray._str` extension point. This is where EAs get to take over and use their compute

Note that there are a few methods like `cat`, `extract`, and `extractall` that don't yet dispatch. I need to look into them a bit more, there implementation is a bit confusing.

Closes https://github.com/pandas-dev/pandas/issues/36216"
721759247,37123,"STY: avoid 1-letter variable names, especially pdb keywords",jbrockmendel,closed,2020-10-14T20:29:42Z,2020-10-15T16:22:28Z,"Every so often I use pdb and want to access a variable that happens to be a pdb keyword and its a hassle.  Let's avoid these: [""h"", ""w"", ""u"", ""d"", ""b"", ""s"", ""n"", ""r"", ""c"", ""j"", ""l"", ""a"", ""p""].  Or simpler, just avoid 1-letter variable names altogether."
713956429,36822,BUG: Raise ValueError with nan in timeaware windows,phofl,closed,2020-10-02T23:47:22Z,2020-10-15T18:54:02Z,"- [x] closes #34617
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Raising a ValueError when NaN is in timeaware window.

cc @mroeschke "
719746437,37091,REF/TYP: pandas/core/window/*.py,mroeschke,closed,2020-10-12T23:55:29Z,2020-10-15T23:22:48Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

* Create clearer separation for methods belonging to `Window` vs `Rolling`
* More typing"
722201405,37133,CI remove redundant checks covered by pyupgrade,MarcoGorelli,closed,2020-10-15T10:20:32Z,2020-10-16T01:08:05Z,"Some of these checks, e.g.
```bash
    MSG='Check for python2-style file encodings' ; echo $MSG
    invgrep -R --include=""*.py"" --include=""*.pyx"" -E ""# -\*- coding: utf-8 -\*-"" pandas scripts
    RET=$(($RET + $?)) ; echo $MSG ""DONE""

    MSG='Check for python2-style super usage' ; echo $MSG
    invgrep -R --include=""*.py"" -E ""super\(\w*, (self|cls)\)"" pandas
    RET=$(($RET + $?)) ; echo $MSG ""DONE""

```
are already checked by pyupgrade.

Opening this as a placeholder to go through the checks and see what no longer needs to be there

EDIT
----

also, as a note, in `pandas/core/generic.py` there is an instance of `.. warning:`, but the `Check for incorrect sphinx directives` isn't picking it up. `pygrep` finds it though. I'm inclined to move all of these over to pre-commit so that they're cross-platform and can selectively be run only on staged files"
722542936,37142,CLN: indexing,jbrockmendel,closed,2020-10-15T17:50:27Z,2020-10-16T01:09:22Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

Working on refactoring the indexing setitem code so that we go down the same path regardless of consolidation status.  This splits off a small piece of that as a prelim."
424568514,25857,"expanding(min_periods=N).count(), for any N, returns same result as expanding(min_periods=1).count()",magicmathmandarin,closed,2019-03-24T02:50:56Z,2020-10-16T01:16:23Z,"```python
idx=pd.date_range('1/1/2019', periods=5)
s = pd.Series([2,3, np.nan, 10,20], index=idx)
s.expanding(min_periods=1).count()
s.expanding(min_periods=100).count()
s.expanding(min_periods=10000).count()

```
#### Problem description
They should not be returning the same results because the Series has only 5 records. 

**Note**: We receive a lot of issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates!

**Note**: Many problems can be resolved by simply upgrading `pandas` to the latest version. Before submitting, please check if that solution works for you. If possible, you may want to check if `master` addresses this issue, but that is not necessary.

For documentation-related issues, you can check the latest versions of the docs on `master` here:

https://pandas-docs.github.io/pandas-docs-travis/

If the issue has not been resolved there, go ahead and file it in the issue tracker.

#### Expected Output

#### Output of ``pd.show_versions()``

<details>

[paste the output of ``pd.show_versions()`` here below this line]

</details>
"
722549654,37143,DOC: Remove redundant table in Series docs,erictleung,closed,2020-10-15T18:00:42Z,2020-10-16T01:21:23Z,"The attribute `Series.index` was separated in another table, which makes
the rendering of the documentation slightly inconsistent. This commit
consolidates the tables together to make the documentation consistent
and aligned.

**Live site** (https://pandas.pydata.org/docs/reference/series.html):

![image](https://user-images.githubusercontent.com/2754821/96168089-20708500-0ed5-11eb-8bab-cc34ebb7f7bc.png)

**Locally rendered change**:

![image](https://user-images.githubusercontent.com/2754821/96168171-4007ad80-0ed5-11eb-99a2-48c200602213.png)

I'm not sure why the color changed. My guess is because I just rendered that page with `python make.py --single reference/series.rst`, so the links to those pages are nonexistent.

If this separate table was intentional, feel free to close this PR. Thanks for considering this!

- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
722665859,37146,DOC: capitalize Python as proper noun,erictleung,closed,2020-10-15T20:48:13Z,2020-10-16T01:21:57Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
719956438,37095,Add CI check to check that `ensure_clean` is used instead of `os.remove`,MarcoGorelli,closed,2020-10-13T07:30:58Z,2020-10-16T01:30:55Z,"xref https://github.com/pandas-dev/pandas/pull/34385#pullrequestreview-506705288

This could also be a pre-commit hook

Opening this issue as a placeholder so we don't forget"
714487679,36874,DOC: GL01 errors in shared_docs.py,Iqrar99,closed,2020-10-05T03:49:00Z,2020-10-16T01:32:19Z,"#### Location of the documentation

[`shared_docs.py`](https://github.com/pandas-dev/pandas/blob/master/pandas/core/shared_docs.py)

https://github.com/pandas-dev/pandas/blob/32d79ef46621de51e13732690c2e52566a5c67b6/pandas/core/shared_docs.py#L6-L8

https://github.com/pandas-dev/pandas/blob/32d79ef46621de51e13732690c2e52566a5c67b6/pandas/core/shared_docs.py#L48-L50

https://github.com/pandas-dev/pandas/blob/32d79ef46621de51e13732690c2e52566a5c67b6/pandas/core/shared_docs.py#L77-L79

https://github.com/pandas-dev/pandas/blob/32d79ef46621de51e13732690c2e52566a5c67b6/pandas/core/shared_docs.py#L146-L148

https://github.com/pandas-dev/pandas/blob/32d79ef46621de51e13732690c2e52566a5c67b6/pandas/core/shared_docs.py#L260-L262

#### Documentation problem

GL01 pandas docstring errors. `Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between.`

#### Suggested fix for documentation


It's really unnecessary using `\` to break the line when using `""""""` as the docstring. It also treats the next line as the same line. Remove the `\` instead."
714487794,36875,DOC: Fix GL01 docstring errors in some functions,Iqrar99,closed,2020-10-05T03:49:23Z,2020-10-16T01:32:24Z,"- [x] fixes #36874
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

There are some GL01 pandas docstring errors in `shared_docs.py`.
```
GL01
Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between.
```"
712892072,36775,BUG: Nullable floats can look like ints in repr,dsaxton,closed,2020-10-01T14:21:54Z,2020-10-16T01:46:54Z,"Nullable floats when output to the screen can look like ints, but in my opinion should look the same as regular non-NaN floats so as not to cause confusion.
```python
import pandas as pd

s = pd.Series([0.0, 1.0, None], dtype=""Float64"")
print(s)
# 0       0
# 1       1
# 2    <NA>
# dtype: Float64
print(s.astype(float))
# 0    0.0
# 1    1.0
# 2    NaN
# dtype: float64
pd.__version__
# '1.2.0.dev0+560.g86f543143'

```
cc @jorisvandenbossche "
713288194,36800,BUG: Fix FloatingArray output formatting,dsaxton,closed,2020-10-02T01:38:28Z,2020-10-16T06:17:06Z,"- [x] closes #36775
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
"
722328071,37136,CI move check incorrect sphinx directives to pre-commit,MarcoGorelli,closed,2020-10-15T13:17:33Z,2020-10-16T07:26:27Z,"Currently, the check (which uses `grep` with the return codes inverted) doesn't catch

```python
    .. warning:
```
, while using `pygrep` from `pre-commit` (which is a Python implementation of `grep` with inverted return codes) does. I think this is because grep doesn't count the newline as a character

Advantages of moving this (and, later, other) `invgrep` checks to pre-commit are:

- they'll be cross-platform
- they'll be simpler to write/maintain because `pre-commit` comes with `pygrep` built-in, so there's no need to manually define something like `invgrep`
- they'll give devs faster feedback on when they introduce unwanted patterns"
722418098,37138,Remove no-longer-necessary CI checks,MarcoGorelli,closed,2020-10-15T15:01:11Z,2020-10-16T07:27:05Z,"xrange is no longer valid in Python3, so this check is unnecessary.

`# -\*- coding: utf-8 -\*-""`, `class\s\S*\((object)?\):`, and `""super\(\w*, (self|cls)\)` are fixed by pyupgrade


closes #37133 
"
713264136,36798,Reading xml files not directly available in Pandas?,RahulReady,closed,2020-10-02T00:17:24Z,2020-10-16T08:21:11Z,"- [ X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com). (There are workarounds to this, but this is a question to the devs)

---

#### Question about pandas
I have already figured out a workaround to reading xml files by using '**xml.etree.ElementTree**', but I was not able to find a read_xml() function in the pandas library. 

There is another package '**https://pypi.org/project/pandas-read-xml/**' that implements this function, but it seems like extra steps for an individual to download another package if they wanted to read the xml file (should they decide not to implement it themselves).

May I know the thought process for not having such function, or if I am blatantly missing something, kindly excuse the ignorance. 
"
722085796,37131,TST: test for expanding window with min_periods GH25857,theMogget,closed,2020-10-15T07:43:15Z,2020-10-16T09:08:24Z,"- [x] closes #25857
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Hi @mroeschke, here's the new test - please let me know if it needs any changes."
720107944,37097,Update code_checks.sh to check for instances of os.remove,jnecus,closed,2020-10-13T10:42:01Z,2020-10-16T09:13:58Z,"I have included a check for instances of os.remove in the tests folder. This is in line with issue #37095 whereby ensure_clean should be used rather than os.remove.

 A few outstanding questions remain:

- Should this be expanded to search for cases of os.remove elsewhere?

- ~~Cases of `os.remove ` still exist in common.py, test_writers.py, and test_store.py. For some reason, the case in test_store.py is not picked up by this check? Any idea why?~~ _(solved by passing arguments separately rather than as a list)_

- ~~Advice on how to implement this as a pre-commit hook also welcome.~~ _(to be implemented in separate PR)_



- [x] closes #37095 
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry


"
722518726,37141,BUG: groupby.rolling.mean seems to roll over different groups when center=True,behrenhoff,closed,2020-10-15T17:14:05Z,2020-10-16T14:13:22Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

To reproduce:

```python
import pandas as pd

# Create a DF with a group column and more than 1 group (10 rows toy data)
dfBug = pd.DataFrame(data={
    'Date': pd.date_range('2020-01-01', '2020-01-10'),
    'gb': ['group_1'] * 6 + ['group_2'] * 4,
    'value': range(10),
})

center = True  # Problem only occurs when center=True
# the apply produces the expected result
via_apply = dfBug.groupby('gb').apply(lambda df: \
    df.set_index('Date').rolling(6, center=center, min_periods=1).value.mean())
# the groupby.rolling... produces a different result
via_groupbyrolling = dfBug.groupby('gb') \
    .rolling(6, on='Date', center=center, min_periods=1).value.mean()

print(pd.concat([via_apply.rename('apply'), 
                 via_groupbyrolling.rename('groupbyrolling'),
                 (via_apply == via_groupbyrolling).rename('is-equal'),
                ], axis=1))

assert (abs(via_apply - via_groupbyrolling) < 1e-4).all()
```

#### Problem description

The code via apply and via groupbyrolling should produce identical results, yet the groupbyrolling version seems to ignore the grouping for the mean. Strange enough, when setting center to False, both variants produce identical results.

Output is the following (I expect the is-equal column to be True in all rows):
```
                    apply  groupbyrolling  is-equal
gb      Date                                       
group_1 2020-01-01    1.0             1.0      True
        2020-01-02    1.5             1.5      True
        2020-01-03    2.0             2.0      True
        2020-01-04    2.5             2.5      True
        2020-01-05    3.0             6.0     False
        2020-01-06    3.5             6.5     False
group_2 2020-01-07    7.0             7.0      True
        2020-01-08    7.5             7.5      True
        2020-01-09    7.5             NaN     False
        2020-01-10    7.5             NaN     False
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-51-generic
Version          : #56-Ubuntu SMP Mon Oct 5 14:28:49 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : de_DE.UTF-8
LOCALE           : de_DE.UTF-8

pandas           : 1.1.3
numpy            : 1.17.4
pytz             : 2020.1
dateutil         : 2.7.3
pip              : 20.0.2
setuptools       : 45.2.0
Cython           : 0.29.21
pytest           : 4.6.9
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.3.3
lxml.etree       : 4.5.0
html5lib         : 1.0.1
pymysql          : None
psycopg2         : None
jinja2           : 2.10.1
IPython          : 7.13.0
pandas_datareader: None
bs4              : 4.8.2
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.1
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 3.0.5
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : 3.6.1
tabulate         : None
xarray           : None
xlrd             : 1.1.0
xlwt             : None
numba            : None

</details>
"
721719724,37121,ENH: No need to call _choose_path in Dataframe groupby transform function ,jmishra01,closed,2020-10-14T19:25:19Z,2020-10-16T15:21:22Z,"When every time the slow_path method run in self._choose_path, then why not we remove the fast_path code.

self._choose_path call
https://github.com/pandas-dev/pandas/blob/d7a5b838d8d6234f6bec5a30bfa33b24bd4afbd9/pandas/core/groupby/generic.py#L1313

Line to change
path, res = show_path, slow_path(group)


definition of self._choose_path method
https://github.com/pandas-dev/pandas/blob/d7a5b838d8d6234f6bec5a30bfa33b24bd4afbd9/pandas/core/groupby/generic.py#L1425

Execution time difference
----------------------------------
With fast_path method - 21 ms
Without fast_path method - 9ms

Used dataframe
df = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar'], 'B': ['one', 'one', 'two', 'three', 'two', 'two'],  'C': [10, 5, 50, 2, 50, 5],  'D': [2.0, 5., 8., 1., 2., 9.]})
"
722758875,37150,"BUG: indexing bugs #26490, #13691",jbrockmendel,closed,2020-10-15T23:48:49Z,2020-10-16T15:31:47Z,"- [x] closes #26490
- [x] closes #13691
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
722874906,37154,"TST: RollingGroupby(..., center=True, on=...)",mroeschke,closed,2020-10-16T04:10:02Z,2020-10-16T16:16:14Z,"- [x] closes #37141
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Didn't look like we had a test case that exactly covered this."
722969170,37155,REF: avoid access to _mgr.blocks -> _mgr._is_single_block,jorisvandenbossche,closed,2020-10-16T07:09:09Z,2020-10-16T17:27:38Z,"@jbrockmendel a small follow-up on https://github.com/pandas-dev/pandas/pull/36873. It keeps the same spirit of that PR (I think), but just replacing the access of `len(self._mgr.blocks)` with an existing attribute on the manager (could also remove the leading underscore on the attribute). 
(this makes it easier to override this attribute in the ArrayManager, and limits the access to the blocks outside of the internals)"
723353143,37162,TST: share logical tests Series/DataFrame,jbrockmendel,closed,2020-10-16T16:13:11Z,2020-10-16T17:58:08Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
716610508,36941,BUG: series.eq(other) does not equal series == other when the series contain pd.NA,connesy,open,2020-10-07T15:00:35Z,2020-10-16T18:43:13Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
# Using pd.NA:
>>> import pandas as pd
>>> df = pd.DataFrame(data=[[1, 1], [2, 2], [3, 3], [pd.NA, pd.NA]], columns=['a','b'])
>>> df.a == df.b
0     True
1     True
2     True
3    False
dtype: bool
>>> df.a.eq(df.b)
0    False
1    False
2    False
3    False
dtype: bool

# Using np.nan instead of pd.NA:
>>> import numpy as np
>>> df_np = pd.DataFrame(data=[[1, 1], [2, 2], [3, 3], [np.nan, np.nan]], columns=['a','b'])
>>> df_np.a.eq(df_np.b)
0     True
1     True
2     True
3    False
dtype: bool
```

#### Problem description

From the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.eq.html#pandas.Series.eq) of `Series.eq`: ""Equivalent to `series == other`"", I would expect `df.a == df.b` to yield the same result as `df.a.eq(df.b)`, but it doesn't when the series contain `pd.NA`.

#### Expected Output
`df.a.eq(df.b)` gives the same result as `df.a == df.b` even when the series contain `pd.NA`.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.5.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-48-generic
Version          : #52-Ubuntu SMP Thu Sep 10 10:58:49 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.1
setuptools       : 49.2.1.post20200802
Cython           : None
pytest           : 6.0.1
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : 1.3.1
lxml.etree       : 4.5.1
html5lib         : None
pymysql          : 0.10.0
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.15.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.7.4
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 1.0.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.18
tables           : None
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.2.0
xlwt             : None
numba            : 0.48.0

</details>
"
728770012,37379,CI move validate unwanted patterns over to pre-commit,MarcoGorelli,closed,2020-10-24T11:29:40Z,2020-10-26T12:51:10Z,"This becomes much simpler with pre-commit :smile: As with other PRs of this kind, the motivation for moving checks like these over to pre-commit is that they become cross-platform and that they provide faster feedback to devs

----

example: if I apply
```diff
diff --git a/pandas/io/formats/info.py b/pandas/io/formats/info.py
index 891b3ea7a..6c1075910 100644
--- a/pandas/io/formats/info.py
+++ b/pandas/io/formats/info.py
@@ -479,7 +479,7 @@ class DataFrameTableBuilder(TableBuilderAbstract):
         collected_dtypes = [
             f""{key}({val:d})"" for key, val in sorted(self.dtype_counts.items())
         ]
-        self._lines.append(f""dtypes: {', '.join(collected_dtypes)}"")
+        self._lines.append("" "" f""dtypes: {', '.join(collected_dtypes)}"")
 
     def add_memory_usage_line(self) -> None:
         """"""Add line containing memory usage.""""""
```
then I get
```console
$ pre-commit run unwanted-patterns-strings-to-concatenate --all
Check for use of not concatenated strings................................Failed
- hook id: unwanted-patterns-strings-to-concatenate
- exit code: 1

pandas/io/formats/info.py:482:String unnecessarily split in two by black. Please merge them manually.
```"
728838295,37386,DOC: Improve clarity and fix grammar,sahidvelji,closed,2020-10-24T17:38:39Z,2020-10-26T14:38:14Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

This PR improves the ""Performance considerations"" section of the IO tools user guide:
https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#performance-considerations"
716741508,36953,REGR: slicing an irregular DatetimeIndex with NaT fails with AssertionError,jorisvandenbossche,closed,2020-10-07T18:04:29Z,2020-10-26T14:51:58Z,"This seems to be triggered in a very specific case, i.e. an irregular time series and a missing value as last element in the index:

```
increment = pd.Series(pd.to_timedelta([30.0, 30.1, 29.9, 30.0], unit='s')).cumsum()
increment[-1] = None
index = pd.Timestamp(""2012-01-01 09:00"") + increment
df = pd.Series(range(len(index)), index=index).to_frame()

In [11]: df[""2012-01-01"":""2012-01-05""]
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-11-ded270432cfc> in <module>
----> 1 df[""2012-01-01"":""2012-01-05""]

~/miniconda3/envs/pandas11/lib/python3.8/site-packages/pandas/core/frame.py in __getitem__(self, key)
   2883             # either we have a slice or we have a string that can be converted
   2884             #  to a slice for partial-string date indexing
-> 2885             return self._slice(indexer, axis=0)
   2886 
   2887         # Do we have a (boolean) DataFrame?

~/miniconda3/envs/pandas11/lib/python3.8/site-packages/pandas/core/generic.py in _slice(self, slobj, axis)
   3552         Slicing with this method is *always* positional.
   3553         """"""
-> 3554         assert isinstance(slobj, slice), type(slobj)
   3555         axis = self._get_block_manager_axis(axis)
   3556         result = self._constructor(self._mgr.get_slice(slobj, axis=axis))

AssertionError: <class 'numpy.ndarray'>
```

"
671136053,35509,BUG: Possible Regression in DatetimeIndex Slicing,stefmolin,closed,2020-08-01T19:35:41Z,2020-10-26T14:51:58Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
>>> import pandas as pd
>>> df = pd.DataFrame(
...     {'col1': ['a', 'b', 'c'], 'col2': [1, 2, 3]}, 
...     index=pd.to_datetime(['2020-08-01', '2020-07-02', '2020-08-05'])
... )
>>> df['2020-08']
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-82-c8d0440364af> in <module>
----> 1 df['2020-08']

~/book_env/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key)
   2880             # either we have a slice or we have a string that can be converted
   2881             #  to a slice for partial-string date indexing
-> 2882             return self._slice(indexer, axis=0)
   2883 
   2884         # Do we have a (boolean) DataFrame?

~/book_env/lib/python3.7/site-packages/pandas/core/generic.py in _slice(self, slobj, axis)
   3546         Slicing with this method is *always* positional.
   3547         """"""
-> 3548         assert isinstance(slobj, slice), type(slobj)
   3549         axis = self._get_block_manager_axis(axis)
   3550         result = self._constructor(self._mgr.get_slice(slobj, axis=axis))

AssertionError: <class 'numpy.ndarray'>
```

You now have to run `sort_index()` before slicing:
```python
>>> df.sort_index()['2020-08']
           col1  col2
2020-08-01    a     1
2020-08-05    c     3
```

#### Problem description

Before 1.1.0, you didn't have to sort the index before slicing. Now, you have to run `sort_index()` and the error message is not helpful at all.

#### Expected Output
```python
>>> df['2020-08']
           col1  col2
2020-08-01    a     1
2020-08-05    c     3
```

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.7.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.40-04224-g891a6cce2d44
Version          : #1 SMP PREEMPT Tue Jun 23 20:21:29 PDT 2020
machine          : x86_64
processor        : 
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 18.1
setuptools       : 40.8.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.16.1
pandas_datareader: 0.9.0
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.18
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
729182653,37411,TST/CLN: misplaced factorize tests,jbrockmendel,closed,2020-10-26T02:15:20Z,2020-10-26T15:03:30Z,
729078784,37403,TST/REF: collect tests by method,jbrockmendel,closed,2020-10-25T18:11:28Z,2020-10-26T15:08:26Z,
728929300,37396,CLN: nanops,jbrockmendel,closed,2020-10-25T03:19:21Z,2020-10-26T15:11:01Z,"This sits on top of #37394.  The cleanup in _maybe_get_mask means that we end up getting a correctly-dtyped result instead of object-dtype, which becomes important in the upcoming nanmean PR."
718546609,37023,REGR: fix bug in DatetimeIndex slicing with irregular or unsorted indices,CloseChoice,closed,2020-10-10T06:30:19Z,2020-10-26T15:15:06Z,"closes #36953 
closes #35509
- [x] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
729177450,37410,TST/REF: misplaced tests in tests.base,jbrockmendel,closed,2020-10-26T01:56:41Z,2020-10-26T15:17:33Z,
728923299,37394,"BUG: Series[td64].sum() wrong on empty series, closes #37151",jbrockmendel,closed,2020-10-25T02:32:00Z,2020-10-26T15:19:15Z,"- [x] closes #37151
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Will handle min_count on the next pass."
729561691,37417,DOC: move release note for 35792,simonjayhawkins,closed,2020-10-26T13:26:39Z,2020-10-26T15:24:31Z,follow-on from #37416
729502968,37416,Backport PR #35792 on branch 1.1.x: CLN/BUG: Clean/Simplify _wrap_applied_output,simonjayhawkins,closed,2020-10-26T12:00:57Z,2020-10-26T15:41:32Z,"Backport PR #35792

https://github.com/pandas-dev/pandas/pull/37305#issuecomment-716498455 cc @jreback "
681365263,35792,CLN/BUG: Clean/Simplify _wrap_applied_output,rhshadrach,closed,2020-08-18T21:54:42Z,2020-10-26T15:42:17Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

First part of #35412. I could not find a relevant issue for the test change/whatsnew update."
729634283,37418,Backport PR #37023 on branch 1.1.x (REGR: fix bug in DatetimeIndex slicing with irregular or unsorted indices),meeseeksmachine,closed,2020-10-26T14:52:40Z,2020-10-26T15:46:48Z,Backport PR #37023: REGR: fix bug in DatetimeIndex slicing with irregular or unsorted indices
704513564,36454,[TST]: Wrong Corr with Timedelta index,phofl,closed,2020-09-18T16:40:00Z,2020-10-26T16:50:56Z,"- [x] xref #31286 (closes the issue, if there is no way around the numerical issues)
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

"
729652188,37419,fix windows issue pre-commit,MarcoGorelli,closed,2020-10-26T15:13:43Z,2020-10-26T17:17:23Z,"By not explicitly using `python` in the entry, we would get
```
Executable `python3` not found
```
on Windows

xref https://github.com/pandas-dev/pandas/pull/37023#issuecomment-716607730"
726362617,37305,Backport PR #37198 on branch 1.1.x (BUG: Regression in Resample.apply raised error when apply affected only a Series),meeseeksmachine,closed,2020-10-21T10:46:47Z,2020-10-26T18:21:27Z,Backport PR #37198: BUG: Regression in Resample.apply raised error when apply affected only a Series
729662691,37420,TST/REF: collect tests by method from tests.internals,jbrockmendel,closed,2020-10-26T15:25:27Z,2020-10-26T19:09:10Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
729675617,37422,"REF: avoid special-casing inside DTA/TDA.mean, flesh out tests",jbrockmendel,closed,2020-10-26T15:40:02Z,2020-10-26T20:54:54Z,"- [ ] closes #xxxx
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
729776161,37424,TST/REF: misplaced tests in frame.test_dtypes,jbrockmendel,closed,2020-10-26T17:46:38Z,2020-10-26T22:07:47Z,"After moving them, test_dtypes is specific to DataFrame.dtypes, so move that file to tests/frame/methods/test_dtypes.py"
728526401,37371,PERF: ensure_string_array with non-numpy input array,topper-123,closed,2020-10-23T21:11:34Z,2020-10-26T22:45:48Z,"Currently, if the input array to `ensure_string_array` is a python object (e.g. an `ExtensionArray`), the function will constantly switch between python and cython level code, which is slow.

This PR fixes that by ensuring we always have a numpy array, avoiding the trips to python level code.

``` python
>>> n = 50_000
>>> cat = pd.Categorical([*['a', pd.NA] * n])
>>> %timeit cat.astype(""string"")
447 ms ± 11.9 ms per loop  # master
5.43 ms ± 80.5 µs per loop  # this PR
```

xref #35519, #36464."
729950603,37434,TST/REF: collect tests by method,jbrockmendel,closed,2020-10-26T22:25:19Z,2020-10-27T02:44:59Z,
729980946,37435,CI: xfail windows numexpr test,jbrockmendel,closed,2020-10-26T23:34:39Z,2020-10-27T02:57:54Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
730212114,37438,xlrd is required for handle excel in pandas but it is EOL and yield warnings on py37,YievCkim,closed,2020-10-27T08:00:06Z,2020-10-27T08:51:50Z,"Hello,

To use `pandas.read_excel` and `pandas.DataFrame.to_excel` we need xlrd.

xlrd is EOL, and can be replaced with OpenPyXL Do you plan this migration ?

Moreover, now xlrd yield warnings with python 3.7

```
xlrd/xlsx.py:266: PendingDeprecationWarning: This method will be removed in future versions.  Use 'tree.iter()' or 'list(tree.iter())' instead.
    for elem in self.tree.iter() if Element_has_iter else self.tree.getiterator():

```

```
xlrd/xlsx.py:312: PendingDeprecationWarning: This method will be removed in future versions.  Use 'tree.iter()' or 'list(tree.iter())' instead.
    for elem in self.tree.iter() if Element_has_iter else self.tree.getiterator()
```


"
718714523,37041,BUG:  set value with iloc and list like indexer,erfannariman,closed,2020-10-10T22:25:35Z,2020-10-27T11:18:54Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
729471896,37415,BUG: groupby with std aggregation of pandas integer dtype throws exception: 'IntegerArray' object has no attribute 'reshape',hannesdm,closed,2020-10-26T11:11:43Z,2020-10-27T12:38:14Z,"- [x ] I have checked that this issue has not already been reported.

- [x ] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
df = pd.DataFrame({'a': [1,1,1,2,2],'b': [1,2,3,4,5]})
df['b'] = df['b'].astype(pd.Int32Dtype())
df.groupby('a').agg('std') # Error 'IntegerArray' object has no attribute 'reshape'. The same error occurs with all pandas integer types

```

#### Problem description

For all integer pandas dtypes the code expects an IntegerArray to have the reshape function as is the case with numpy arrays.
It is possible that the error will also occur with other operations performed after a groupby, this has only been checked for 'std'.
The error occurs at
pandas\core\groupby\groupby.py"", line 2491, in _get_cythonized_result
    vals = vals.reshape((-1, 1))

#### Expected Output

The same output as with a numpy data type, for example above this would be:

 
a 	 b
1 	1.000000
2 	0.707107



#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.6.9.final.0
python-bits      : 64
OS               : Linux
OS-release       : 4.19.112+
Version          : #1 SMP Thu Jul 23 08:00:38 PDT 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.18.5
pytz             : 2018.9
dateutil         : 2.8.1
pip              : 19.3.1
setuptools       : 50.3.0
Cython           : 0.29.21
pytest           : 3.6.4
hypothesis       : None
sphinx           : 1.8.5
blosc            : None
feather          : 0.4.1
xlsxwriter       : None
lxml.etree       : 4.2.6
html5lib         : 1.0.1
pymysql          : None
psycopg2         : 2.7.6.1 (dt dec pq3 ext lo64)
jinja2           : 2.11.2
IPython          : 5.5.0
pandas_datareader: 0.9.0
bs4              : 4.6.3
bottleneck       : 1.3.2
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.2.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : 2.5.9
pandas_gbq       : 0.13.2
pyarrow          : 0.14.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.4.1
sqlalchemy       : 1.3.19
tables           : 3.4.4
tabulate         : 0.8.7
xarray           : 0.15.1
xlrd             : 1.1.0
xlwt             : 1.3.0
numba            : 0.48.0

</details>
"
612588294,33999,Failing test_missing_required_dependency in pandas-wheels,TomAugspurger,closed,2020-05-05T13:07:38Z,2020-10-27T12:49:45Z,"The test `test_missing_required_dependency` is failing in MacPython/pandas-wheels. https://dev.azure.com/pandas-dev/pandas-wheels/_build/results?buildId=34824&view=logs&j=c0130b29-789d-5a3c-6978-10796a508a7f&t=e120bc6c-1f5e-5a41-8f0a-1d992cd2fbfb.

This has caused issues in the past. It's low priority so for now I'm just going to skip it over there."
724011850,37220,BUG: MultiIndex.is_montonic erronoes when nan in at least one level,phofl,closed,2020-10-18T13:52:48Z,2020-10-27T12:51:13Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
print(pd.MultiIndex.from_tuples([(np.nan, ), (1,)], names=[""test""]).is_monotonic_increasing)

```

#### Problem description

This returns ``True``, which is obviously not correct. This was introduced by #27495. The error persists with more than one level in the ``MultiIndex``. The example is just for simplicity.

#### Expected Output

``False``

#### Output of ``pd.show_versions()``

<details>

master

</details>
"
470733345,27498,Investigate whether re-encoding MultiIndex levels is beneficial,qwhelan,closed,2019-07-21T02:09:24Z,2020-10-27T12:51:13Z,"Follow up for #27495, where we handle the common case of `MultiIndex` being created from helper functions that ensure the levels are sorted. In order to handle the case of `MultiIndex(codes=..., labels=...)` and labels being individually unsorted, we likely need to re-encode that level. If that is performant, it would allow deleting a non-performant base case in `MultiIndex.is_monotonic`"
729861556,37428,Failing test_missing_required_dependency in pandas-wheels,simonjayhawkins,closed,2020-10-26T19:57:06Z,2020-10-27T12:57:40Z,"- [ ] closes #33999
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry

tested at https://github.com/simonjayhawkins/pandas-release/actions/runs/329803554 ( patched from Gist https://gist.github.com/simonjayhawkins/f26d3e26fd55159299b38204f7f3295c and skips for test_missing_required_dependencies removed for conda and pip tests)

following merge can also remove from https://github.com/MacPython/pandas-wheels/blob/5543626b261f5cb55cebfb300c156718c1796ce3/config.sh#L34"
724014216,37221,Fix regression for is_monotonic_increasing with nan in MultiIndex,phofl,closed,2020-10-18T14:03:54Z,2020-10-27T13:12:34Z,"- [x] closes #37220
- [x] closes #27498 (closes that one too probably, because we need this case)
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Found this while debugging a different issue. "
730242522,37440,"Upgrade pygrep, use in-built rst-directive-colons",MarcoGorelli,closed,2020-10-27T08:47:40Z,2020-10-27T13:28:49Z,"I got a PR in to pre-commit/pygrep-hooks to make the ""incorrect-sphinx-directives"" hook a built-in one (as it's not specific to pandas) :tada: So, let's use it?

They also have another useful hook (`rst-inline-touching-normal`), which caught a couple of errors which I've fixed below.

The reason why I've kept `rst-backticks` to just `rst` files (as opposed to `types: [text]` like the other two) is that in docstrings, the numpy guide instructs to use single backticks for variable (though if it was ever desirable to change that, then this hook would make it pretty easy to check for consistency)"
730427107,37445,Backport PR #37428 on branch 1.1.x (Failing test_missing_required_dependency in pandas-wheels),meeseeksmachine,closed,2020-10-27T12:57:46Z,2020-10-27T14:30:04Z,Backport PR #37428: Failing test_missing_required_dependency in pandas-wheels
730430281,37446,Backport PR #37221 on branch 1.1.x (Fix regression for is_monotonic_increasing with nan in MultiIndex),meeseeksmachine,closed,2020-10-27T13:01:56Z,2020-10-27T14:30:47Z,Backport PR #37221: Fix regression for is_monotonic_increasing with nan in MultiIndex
730422462,37444,Backport PR #37433 on branch 1.1.x (REGR: fix groupby std() with nullable dtypes),meeseeksmachine,closed,2020-10-27T12:51:22Z,2020-10-27T14:49:09Z,Backport PR #37433: REGR: fix groupby std() with nullable dtypes
729924255,37430,TST/REF: collect tests by method,jbrockmendel,closed,2020-10-26T21:33:45Z,2020-10-27T15:06:46Z,
729133676,37407,TST: Set timeoffset as the window parameter,gabriellm1,closed,2020-10-25T22:51:15Z,2020-10-27T15:45:14Z,"- [X] closes #28266
- [X] tests added / passed
- [X] passes `black pandas`
- [X] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`


Hi guys, just added tests for issue #28266. I'm participating in the Hacktoberfest event and would much appreciate it if someone put the label ‘hacktoberfest-accepted’  on this PR. Any modification needed just hit me up :)
"
716628641,36942,TST: astyp via loc to int64,hardikpnsp,closed,2020-10-07T15:22:10Z,2020-10-27T16:11:15Z,"- [x] closes #31861
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

Adds a test for a special case of `astype` via `loc` to extension type Int64 when data has np.nan or pd.NA. "
725969683,37290,BUG: rank raises error with read-only data,zeromh,closed,2020-10-20T21:51:36Z,2020-10-27T22:09:06Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np
arr = np.arange(10)
arr.setflags(write=False)
pd.Series(arr).rank()
```
Output:
```
ValueError                                Traceback (most recent call last)
<ipython-input-5-afa6b4ecf509> in <module>
      3 arr = np.arange(10)
      4 arr.setflags(write=False)
----> 5 pd.Series(arr).rank()

~/anaconda/envs/xfactor/lib/python3.8/site-packages/pandas/core/generic.py in rank(self, axis, method, numeric_only, na_option, ascending, pct)
   8334         if numeric_only is None:
   8335             try:
-> 8336                 return ranker(self)
   8337             except TypeError:
   8338                 numeric_only = True

~/anaconda/envs/xfactor/lib/python3.8/site-packages/pandas/core/generic.py in ranker(data)
   8319 
   8320         def ranker(data):
-> 8321             ranks = algos.rank(
   8322                 data.values,
   8323                 axis=axis,

~/anaconda/envs/xfactor/lib/python3.8/site-packages/pandas/core/algorithms.py in rank(values, axis, method, na_option, ascending, pct)
    934     if values.ndim == 1:
    935         values = _get_values_for_rank(values)
--> 936         ranks = algos.rank_1d(
    937             values,
    938             ties_method=method,

pandas/_libs/algos.pyx in pandas._libs.algos.rank_1d()

~/anaconda/envs/xfactor/lib/python3.8/site-packages/pandas/_libs/algos.cpython-38-darwin.so in View.MemoryView.memoryview_cwrapper()

~/anaconda/envs/xfactor/lib/python3.8/site-packages/pandas/_libs/algos.cpython-38-darwin.so in View.MemoryView.memoryview.__cinit__()

ValueError: buffer source array is read-only
```
#### Problem description
`rank` should work with read-only data.

I noticed the problem when using `check_estimator` from `sklearn.utils.estimator_checks` on an estimator that uses pandas `rank`. I haven't explored fully but I assume `check_estimator` uses read-only data for running its tests, which causes this error.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Mon Aug 31 22:12:52 PDT 2020; root:xnu-6153.141.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.6.0.post20200814
Cython           : None
pytest           : 6.0.2
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.18.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
</details>
"
711992062,36741,BUG: Setting values with ILOC and list like indexers raises ,phofl,closed,2020-09-30T14:21:40Z,2020-10-28T00:22:30Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [x] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd


df = pd.DataFrame({""flag"": [""x"", ""y""], ""value"": [1, 2]})
df.iloc[[True, False], 1] = df.iloc[[True, False], 1] * 2
print(df)

```

#### Problem description

This worked on 1.0.5 and returned 
```
  flag  value
0    x      2
1    y      2
```

On master this raises:

```
Traceback (most recent call last):
  File ""/home/developer/.config/JetBrains/PyCharm2020.2/scratches/scratch_5.py"", line 221, in <module>
    df.iloc[[True, False], 1] = x
  File ""/home/developer/PycharmProjects/pandas/pandas/core/indexing.py"", line 681, in __setitem__
    iloc._setitem_with_indexer(indexer, value)
  File ""/home/developer/PycharmProjects/pandas/pandas/core/indexing.py"", line 1756, in _setitem_with_indexer
    self._setitem_single_column(ilocs[0], value, pi)
  File ""/home/developer/PycharmProjects/pandas/pandas/core/indexing.py"", line 1800, in _setitem_single_column
    ser._mgr = ser._mgr.setitem(indexer=pi, value=value)
  File ""/home/developer/PycharmProjects/pandas/pandas/core/internals/managers.py"", line 532, in setitem
    return self.apply(""setitem"", indexer=indexer, value=value)
  File ""/home/developer/PycharmProjects/pandas/pandas/core/internals/managers.py"", line 397, in apply
    applied = getattr(b, f)(**kwargs)
  File ""/home/developer/PycharmProjects/pandas/pandas/core/internals/blocks.py"", line 923, in setitem
    check_setitem_lengths(indexer, value, values)
  File ""/home/developer/PycharmProjects/pandas/pandas/core/indexers.py"", line 158, in check_setitem_lengths
    raise ValueError(
ValueError: cannot set using a list-like indexer with a different length than the value

Process finished with exit code 1

```

Was this change of behavior intended?

#### Expected Output

I would expect, that this would work and returns the results from 1.0.5

#### Output of ``pd.show_versions()``

<details>

master

</details>
"
729928532,37431,TST/REF: collect multilevel tests by method,jbrockmendel,closed,2020-10-26T21:41:28Z,2020-10-28T00:36:58Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
581342356,32709,Restore 32 Bit Linux CI ,WillAyd,closed,2020-03-14T19:37:35Z,2020-10-28T03:15:11Z,Noticed in #32706 and temporarily disabled in a follow up #32708
35074844,7355,BUG: to_sql with sqlite fallback rewrites table scheme even with if_exists='append',jorisvandenbossche,closed,2014-06-05T16:40:06Z,2020-10-28T03:35:16Z,"From SO: http://stackoverflow.com/questions/24064509/to-sql-pandas-method-changes-the-scheme-of-sqlite-tables

The reason is that `get_table` is not implemented for fallback mode (https://github.com/pydata/pandas/blob/master/pandas/io/sql.py#L1072), so a `create_table_statement` is always called (https://github.com/pydata/pandas/blob/master/pandas/io/sql.py#L525).

Should look at it further to see if this is solvable, or a restriction of sqlite fallback.
"
730215998,37439,REGR: fix rank algo for read-only data,jorisvandenbossche,closed,2020-10-27T08:05:41Z,2020-10-28T07:23:30Z,Closes #37290
731025275,37459,Backport PR #37439 on branch 1.1.x (REGR: fix rank algo for read-only data),meeseeksmachine,closed,2020-10-28T02:23:41Z,2020-10-28T07:24:13Z,Backport PR #37439: REGR: fix rank algo for read-only data
729932318,37432,Fix regression in iloc with boolean list,phofl,closed,2020-10-26T21:48:22Z,2020-10-28T09:26:50Z,"- [x] closes #36741
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Run tests in ``indexes`` and ``indexing`` locally without failure."
731220185,37463,Backport PR #37432 on branch 1.1.x,phofl,closed,2020-10-28T08:27:23Z,2020-10-28T09:32:25Z,"(cherry picked from commit 52925335fe6d5f1ea6c79bb03da70654d9be9668)

Tried to merge it. Hopefully nothing was broken in the process. Run the indexing tests locally, but have to wait for ci

cc @simonjayhawkins "
723839534,37205,BUG: Don't copy DataFrame columns as metadata,TomAugspurger,closed,2020-10-17T20:14:42Z,2020-10-28T11:33:02Z,"See https://github.com/pandas-dev/pandas/pull/37186/files#r506978889.
Basically, we don't want to accidentally do `series.name =
dataframe.name` and try to set a column.

cc @jorisvandenbossche. This could conceivably affect geopandas, since geodataframe has different metadata than geoseries: https://github.com/geopandas/geopandas/blob/924cdf65c7c15b01749d1cdd036c5c291e87b0f4/geopandas/geodataframe.py#L83

But I don't think it'll actually break anything for you."
724587559,37243,TST: split up test_concat.py,jreback,closed,2020-10-19T13:00:21Z,2020-10-28T12:04:16Z,"we should split this to a sub-module (concat) and then split logically (similar to merge)

https://github.com/pandas-dev/pandas/blob/master/pandas/tests/reshape/test_concat.py"
726979467,37328,TST: eval test unreliable,rhshadrach,closed,2020-10-22T01:54:16Z,2020-10-28T12:04:57Z,"Ran into an unrelated failure I haven't seen before in #37227. Occurs in pandas/tests/computation/test_eval.py:211

https://dev.azure.com/pandas-dev/pandas/_build/results?buildId=45754&view=logs&j=89027c74-d1cd-5e5d-942f-e6a83ebe07fe&t=768a7acf-33bc-5b3d-16e1-da874611990b&l=92

```

E           numpy array values are different (2.0 %)
E           [left]:  [[10.885282508088554, 24.01653292315993, 2.5334764666276754, -26.20850153209755, -9.292195139788854], [4.631505078070531, 0.43305142455418066, 9.796254287558813, -0.817344630073512, -0.22979400652647], [16.964677878827604, 3.69159983714683, 0.16736723603895715, -13.866779975183137, -0.6547978574323233], [0.9332007475405953, 7.326122317479498, 2.2912333245159338, -0.9957313583066364, -0.21968895568980523], [1.2901449066742292, 1.5178122458579082, 0.3101899430985658, -0.3600640963287845, -3.8622009358387115], [1.7324531378555383, 1.6717678316305915, 0.39248288379342006, -2.259323953594114, -13.215192982180358], [60.71781357885021, 13.08256705151808, 3.306197592158367, -2.603163236030203, -0.5783506355776388], [47.71284995565457, 39.508260846448245, 0.16695497225705538, -7.39400467084908, -0.9437156402277913], [0.40861584564047, 611.3601636482858, 0.8350779591410551, -3.034557944041943, -0.33953697203938915], [46.62807235120426, 6.308023182021756, 5.01671850424403, -0.23877079736974396, -0.5460348132251863]]
E           [right]: [[10.885282508088554, 24.01653292315993, 2.5334764666276754, -26.20850153209755, -9.292195139788854], [4.631505078070531, 0.43305142455418066, 9.796254287558813, -0.817344630073512, -0.22979400652647], [16.964677878827608, 3.69159983714683, 0.16736723603895715, -13.866779975183137, -0.6547978574323233], [0.9332007475405953, 7.326122317479498, 2.2912333245159338, -0.9957313583066364, -0.21968895568980523], [1.2901449066742292, 1.5178122458579082, 0.3101899430985658, -0.3600640963287845, -3.8622009358387115], [1.7324531378555383, 1.6717678316305915, 0.39248288379342006, -2.259323953594114, -13.215192982180358], [60.71781357885021, 13.08256705151808, 3.306197592158367, -2.603163236030203, -0.5783506355776388], [47.71284995565457, 39.508260846448245, 0.16695497225705538, -7.39400467084908, -0.9437156402277913], [0.40861584564047, 611.3601636482858, 0.8350779591410551, -3.034557944041943, -0.33953697203938915], [46.62807235120426, 6.308023182021756, 5.01671850424403, -0.23877079736974396, -0.5460348132251863]]
```

where one entry is `16.964677878827604` and the other is `16.964677878827608`. Haven't looked into it, but guessing this is a numerical precision issue. "
728797518,37380,TST: Update unreliable num precision component of test_binary_arith_ops (GH37328),avinashpancham,closed,2020-10-24T14:11:12Z,2020-10-28T12:05:00Z,"- [x] closes #37328 
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
731326697,37466,Backport PR #35898 on branch 1.1.x: CI: docker 32-bit linux build,simonjayhawkins,closed,2020-10-28T10:46:05Z,2020-10-28T12:22:41Z,Backport PR #35898
728847335,37387,TST: split up test_concat.py #37243 - more follows up,kamilt5,closed,2020-10-24T18:27:03Z,2020-10-28T12:31:53Z," * created test_categorical.py

- [x] closes #37243 
- [ ] tests added / passed
- [x] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
730435543,37447,DataFrame.to_csv() behaves differently when passing compression parameter,tonhui,closed,2020-10-27T13:08:40Z,2020-10-28T15:43:56Z,"If i use:

    df.to_csv('dir1/dir2/test.csv.zip')

or

    df.to_csv('dir1/dir2/test.csv', compression='zip')

to save a csv file, i found that
1. the path name no matter reletive or absolute will be included in the zip archive
2. the extension name of the actual file in the zip archive will be kept the same with the one you passed in which means if you pass in 'zip' you get a 'zip' in the archive.

None of the above are true if extension name is one of [‘.gz’, ‘.bz2’, or ‘.xz’] or compression mode is one of [‘gzip’, ‘bz2’], are those desirable behavior rather than keep them all the same?

Additionly if you set compression mode with one of ['gzip', 'bz2'] and passin the file name with extension 'csv' you get the compressed archive end with '.csv' which is confusing because you will get all hex code if you open it with a text editor.

pd.show_version() output

INSTALLED VERSIONS
------------------
commit           : d9fff2792bf16178d4e450fe7384244e50635733
python           : 3.8.5.final.0
python-bits      : 64
OS               : Darwin
OS-release       : 19.6.0
Version          : Darwin Kernel Version 19.6.0: Mon Aug 31 22:12:52 PDT 2020; root:xnu-6153.141.2~1/RELEASE_X86_64
machine          : x86_64
processor        : i386
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : None.UTF-8

pandas           : 1.1.0
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.2
setuptools       : 49.3.1.post20200810
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : 3.2.1
blosc            : None
feather          : 0.4.1
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : 1.1
pymysql          : 0.10.0
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.17.0
pandas_datareader: None
bs4              : 4.9.1
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.0
numexpr          : None
odfpy            : None
openpyxl         : 3.0.4
pandas_gbq       : None
pyarrow          : 1.0.0
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : 1.3.18
tables           : None
tabulate         : 0.8.7
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : None"
684856691,35875,COMPAT: Ensure rolling indexers return intp during take operations,mroeschke,closed,2020-08-24T17:55:45Z,2020-10-28T15:55:27Z,"- [x] closes #35294
- [x] closes #35148
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry


Tested locally with @TomAugspurger docker container and the tests were working: https://github.com/pandas-dev/pandas/pull/35228#issuecomment-658833239


"
731297991,37464,"BUG: `pd.concat(..., axis=""columns"")` inconsistently keeps/drops index name",DanielFEvans,closed,2020-10-28T10:06:04Z,2020-10-28T16:50:46Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df1 = pd.DataFrame({""IndexColumn"": [1, 2, 3], ""DataColumn1"": [""x"", ""y"", ""z""]})
df2 = pd.DataFrame({""IndexColumn"": [4, 1, 3], ""DataColumn2"": [""j"", ""k"", ""l""]})

df3 = pd.DataFrame({""IndexColumn"": [""a"", ""b"", ""c""], ""DataColumn1"": [""x"", ""y"", ""z""]})
df4 = pd.DataFrame({""IndexColumn"": [""d"", ""a"", ""c""], ""DataColumn2"": [""j"", ""k"", ""l""]})

df1_2_merged = pd.concat([df1.set_index(""IndexColumn""), df2.set_index(""IndexColumn"")], axis=""columns"")
print(""Integer index retained name:"", df1_2_merged.index.name == ""IndexColumn"")
# True

df3_4_merged = pd.concat([df3.set_index(""IndexColumn""), df4.set_index(""IndexColumn"")], axis=""columns"")
print(""String index retains name:"", df3_4_merged.index.name == ""IndexColumn"")
# False
```

#### Problem description

`pd.concat([df, other_df], axis=""columns"")` with two dataframes with identical index names sometimes retains the index name, but at other times drops it. The index values themselves are kept, and the behaviour is otherwise as expected.

It seems logical to always keep the index name if it is the same between both, rather than having inconsistent behaviour.

This _may_ be the same as #35847. However, that discusses `Index.union()`, and taking the union of the `df3` and `df4` indices directly results in the name being propagated to the output, unlike the example above:

```
>>> df3_index = df3.set_index(""IndexColumn"").index
>>> df4_index = df4.set_index(""IndexColumn"").index
>>> df3_index.union(df4_index)
Index(['a', 'b', 'c', 'd'], dtype='object', name='IndexColumn')
```

(I've not been able to test the master branch as it appears to now require Python 3.7, and I haven't got a Py3.7 environment handy currently)

#### Expected Output

In the code example above, I'd expect both to output ""True"", i.e. that the index name is copied through to the output.

#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.6.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1127.19.1.el7.x86_64
Version          : #1 SMP Thu Aug 20 14:39:03 CDT 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_GB.UTF-8
LOCALE           : en_GB.UTF-8

pandas           : 1.1.3
numpy            : 1.18.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.4
setuptools       : 45.2.0
Cython           : 0.29.21
pytest           : 5.4.3
hypothesis       : 5.16.0
sphinx           : 2.4.0
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : 4.5.2
html5lib         : None
pymysql          : None
psycopg2         : 2.8.4 (dt dec pq3 ext)
jinja2           : 2.11.2
IPython          : None
pandas_datareader: None
bs4              : 4.9.3
bottleneck       : 1.3.2
fsspec           : 0.6.2
fastparquet      : None
gcsfs            : None
matplotlib       : 3.1.2
numexpr          : 2.7.1
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 0.15.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.2.1
sqlalchemy       : None
tables           : 3.5.2
tabulate         : 0.8.6
xarray           : None
xlrd             : 1.2.0
xlwt             : None
numba            : 0.51.0

</details>
"
731574266,37471,Backport PR #35875 on branch 1.1.x (COMPAT: Ensure rolling indexers return intp during take operations),meeseeksmachine,closed,2020-10-28T15:57:14Z,2020-10-28T17:58:00Z,Backport PR #35875: COMPAT: Ensure rolling indexers return intp during take operations
724857940,37259,TST: split up test_concat.py #37243,kamilt5,closed,2020-10-19T18:00:26Z,2020-10-28T19:36:48Z,"- [ ] closes #37243 
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
731685295,37474,CLN: Add init files in test folders,phofl,closed,2020-10-28T18:22:07Z,2020-10-28T21:56:29Z,"was not sure why they were missing there, so I added them
"
384506701,23930,check_comprehensiveness breaks running a subset of tests,TomAugspurger,closed,2018-11-26T20:55:57Z,2020-10-29T00:58:48Z,"https://github.com/pandas-dev/pandas/blob/b7294dd3ec47dffa50f3c8bdf89aa8a01f4494f2/pandas/tests/indexing/test_coercion.py#L19

so I can't run, e.g. `pytest pandas/tests/indexing/test_coercion.py::TestFillnaSeriesCoercion::test_fillna_datetime64tz`, which is a tad annoying. Can we rewrite the check to be aware of what was requested?"
60151799,9606,API: timeseries accessors naming convention,jreback,closed,2015-03-06T20:08:11Z,2020-10-29T01:18:57Z,"we have a slightly odd naming convention for several index (and .dt) accessors

`dayofweek`
`dayofyear`

(and adding `daysinmonth` in #9605)

so these should prob be `day_of_week` 

we should be able to maintain compat on these and just add the new ones.
"
728892259,37390,CLN: Deprecate dayofweek/hello day_of_week (#9606),abhishekmangla,closed,2020-10-24T22:39:20Z,2020-10-29T01:19:03Z,"Rename dayofweek properties in Period, Datetime, and PeriodArray classes.
Add day_of_week property to respective tests for each class.

- [x] closes #9606
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
731564911,37470,TST/REF: finish collecting sample tests,jbrockmendel,closed,2020-10-28T15:46:37Z,2020-10-29T01:58:35Z,"Parametrizes a few tests from the frame.methods.test_sample file, splits up giant tests from the generic file."
729779959,37425,BUG: DataFrame.min/max dt64 with skipna=False,jbrockmendel,closed,2020-10-26T17:52:06Z,2020-10-29T01:59:05Z,"- [x] closes #36907
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry
"
731634500,37472,TST: collect tests by method from test_api,jbrockmendel,closed,2020-10-28T17:11:17Z,2020-10-29T01:59:23Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
730979041,37456,TST/REF: misplaced .xs tests,jbrockmendel,closed,2020-10-28T00:39:50Z,2020-10-29T01:59:36Z,
730704904,37449,TST/REF: collect tests by method,jbrockmendel,closed,2020-10-27T18:25:18Z,2020-10-29T01:59:54Z,
728342215,37365,"TYP: DatetimeIndex, TimedeltaIndex",jbrockmendel,closed,2020-10-23T16:08:47Z,2020-10-29T02:00:11Z,Better alternative to parts of #37224
731694819,37475,REF: helper to ensure column indexer is iterable,jbrockmendel,closed,2020-10-28T18:36:05Z,2020-10-29T02:01:27Z,"Helper method separated out from upcoming PR to split _setitem_with_indexer into more manageable pieces, which in turn is part of fixing a goes-through-two-separate-paths problem."
729052532,37402,TST: check_comprehensiveness compat for --lf and -k,jbrockmendel,closed,2020-10-25T16:02:28Z,2020-10-29T02:01:57Z,"- [x] closes #23930
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
730718129,37451,TST/REF: collect tests by method from test_io,jbrockmendel,closed,2020-10-27T18:44:32Z,2020-10-29T02:02:20Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
731866447,37476,CI: troubleshoot ResourceWarnings,jbrockmendel,closed,2020-10-28T23:28:30Z,2020-10-29T02:53:28Z,
688668452,35984,ENH: Add axis argument to Dataframe.corr,kc611,closed,2020-08-30T07:12:04Z,2020-10-29T04:13:02Z,"- [x] closes #35002
- [x] tests added / passed
"
38383495,7817,period_range() bug?,ifmihai,closed,2014-07-22T09:35:00Z,2020-10-29T05:49:15Z,"I did:

``` python
63 In : df = P.DataFrame(index=P.period_range('2000-1-1 10:20', '2005-1-1 12:00'))

64 In : df.index[0]
64 Out: Period('2000-01-01', 'D')

65 In : df.index[0].hour
65 Out: 0
```

it should have been 10, right?

why does period_range() loses time information?

I was recommended to use periods and period_range() because of pandas Timestamp limitation (nanoseconds time units)

now I'm stuck, because I need datetimes outside Timestamp range

ps.
would it be too drastic to change time units from nanoseconds to microseconds in Timestamp? so that we would be happier? :P
see also #7307
"
715272559,36907,BUG: `NaT` does not propagate in row-wise `max` for `datetime64` rows,isVoid,closed,2020-10-06T01:38:12Z,2020-10-29T10:52:25Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
pdf = pd.DataFrame()
pdf['a'] = pd.Series(['2020-01-01 08:00:00', '1920-02-01 09:00:00'], dtype='datetime64[ns]')
pdf['b'] = pd.Series(['2020-02-01 08:00:00', pd.NaT], dtype='datetime64[ns]')
pdf.max(axis=1, skipna=False)
```

#### Problem description
Current output is 
```
0   2020-02-01 08:00:00
1   1920-02-01 09:00:00
dtype: datetime64[ns]
```
In general, missing values involved in reduction should propagate.

#### Expected Output
```
0   2020-02-01 08:00:00
1   NaT
dtype: datetime64[ns]
```

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : 2a7d3326dee660824a8433ffd01065f8ac37f7d6
python           : 3.7.8.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-48-generic
Version          : #52~18.04.1-Ubuntu SMP Thu Sep 10 12:50:22 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : None
LOCALE           : en_US.UTF-8

pandas           : 1.1.2
numpy            : 1.19.1
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 49.6.0.post20200917
Cython           : 0.29.21
pytest           : 6.1.1
hypothesis       : 5.28.0
sphinx           : 3.2.1
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : 2.11.2
IPython          : 7.3.0
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : 0.8.3
fastparquet      : None
gcsfs            : None
matplotlib       : 3.3.2
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : 1.0.1
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : 1.5.2
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : 0.51.0rc1
</details>
"
694961388,36186,RLS: 1.1.3,simonjayhawkins,closed,2020-09-07T10:32:12Z,2020-10-29T13:27:08Z,"[![Tag Release](https://github.com/simonjayhawkins/pandas-release/workflows/Tag%20Release/badge.svg)](https://github.com/simonjayhawkins/pandas-release/actions?query=workflow%3A%22Tag+Release%22)

Tracking issue for the 1.1.3 release.

https://github.com/pandas-dev/pandas/milestone/77

Please do not remove/change milestones from these issues without a note explaining the reasoning (changing milestones doesn't trigger notification)

During issue triage, regressions from 1.0.5 onwards should be milestoned 1.1.3 in the first instance. cc @pandas-dev/pandas-triage 
"
730706437,37450,PERF: faster dir() calls,topper-123,closed,2020-10-27T18:27:26Z,2020-10-29T14:01:01Z,"I experience slow tab-completion in iPython, so I've optimized `dir` calls in pandas, e.g.

```python
>>> n = 100_000
>>> ser = pd.Series(['a'] * n]
>>> %timeit dir(ser)
3.73 ms ± 34.7 µs per loop  # master
253 µs ± 4.3 µs per loop  # this PR
```

It does this by caching the output for the info axis, when the info axis may have string, and returning an empty set for info axes that cannot have strings (numeric indexes etc.)

The above didn't actually improve the subjective speed of tab completion for me, so the problem there probably is in Ipython, but the change in this PR can't hurt either."
724065608,37224,TYP: indexes,jbrockmendel,closed,2020-10-18T18:15:48Z,2020-10-29T15:55:27Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
711763274,36739,Is there any doucument about telling how to   cross-compile pandas  to arm ? so difficult ,fanucwj,closed,2020-09-30T08:56:07Z,2020-10-29T16:07:19Z,"- [ ] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions.

- [ ] I have asked my usage related question on [StackOverflow](https://stackoverflow.com).

---

#### Question about pandas

**Note**: If you'd still like to submit a question, please read [this guide](
https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your question.

```python
# Your code here, if applicable

```
"
724003754,37219,DOC: Updated resample.py and groupby.py to fix SA04 Errors,nagesh-chowdaiah,closed,2020-10-18T13:09:21Z,2020-10-29T16:07:25Z,"- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry: Fixed Partial SA04 Errors 

"
679906718,35758,REGR: re-add encoding for read_excel,twoertwein,closed,2020-08-17T03:29:47Z,2020-10-29T18:09:49Z,"- [ ] closes #35753
- [ ] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

Needs tests!"
720552156,37103,CLN: unify numpy.random-related imports in pandas/tests/plotting,onshek,closed,2020-10-13T17:05:26Z,2020-10-29T18:49:45Z,"# latest update
all files are moved to #37492

***
- [x] closes #37053 ~~(keep this issue open until both #37103 and #37117 are completed)~~
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

A separate PR(CI) will be opened to add related code_check(#37117).
~~PS: Should a whatsnew note be added?~~

# update

The whole project is reformatted except `pandas/_testing.py` for there are quite a few `tm.randn` used in testing files.

below is the script to do the whole clean-up
```
import os
import re


class NumpyRandomRelatedScript:
    def __init__(self, base_dir: str = ""/dir/of/pandas/pandas"") -> None:
        self.base_dir = base_dir
        self.py_files = []
        self.backup_files = []
        self.p1 = re.compile(""import numpy as np"")
        self.p2 = re.compile(""from numpy.random import[ ,a-zA-Z]*[\s]"")
        self.p3 = re.compile(""from numpy import random[\s]"")

    def search_py_files(self, pandas_file_dir: str) -> None:
        if os.path.isfile(pandas_file_dir):
            if pandas_file_dir[-3:] == "".py"":
                if pandas_file_dir != os.path.join(self.base_dir, ""_testing.py""):
                    self.py_files.append(pandas_file_dir)
        elif os.path.isdir(pandas_file_dir):
            for d in os.listdir(pandas_file_dir):
                self.search_py_files(os.path.join(pandas_file_dir, d))

    def do_the_clean_up(self, file_dir: str) -> None:
        with open(file_dir, ""r"") as file:
            data = file.read()
            m1, m2, m3 = (
                re.search(self.p1, data),
                re.search(self.p2, data),
                re.search(self.p3, data),
            )
            if not (m2 or m3):
                # print(""There's no need to change, please recheck!"")
                return

        backup_dir = file_dir + "".issue37053_backup""
        self.backup_files.append(backup_dir)
        print(""Backup: "" + backup_dir)
        with open(backup_dir, ""w+"") as file:
            file.write(data)

        with open(file_dir, ""w+"") as file:
            if m2:
                if not m1:
                    data = re.sub(self.p2, ""import numpy as np\n"", data)
                    m1 = True
                else:
                    data = re.sub(self.p2, """", data)
                methods = (
                    m2.group(0)
                    .replace(""from numpy.random import"", """")
                    .replace("" "", """")
                    .replace(""\n"", """")
                    .split("","")
                )
                if isinstance(methods, str):
                    methods = [methods]
                for method in methods:
                    data = re.sub(
                        r""[\s]{meth}[^a-z]"".format(meth=method),
                        "" np.random.{meth}("".format(meth=method),
                        data,
                    )
                    data = re.sub(
                        r""[\s]-{meth}[^a-z]"".format(meth=method),
                        "" -np.random.{meth}("".format(meth=method),
                        data,
                    )
                    data = re.sub(
                        r""[\(]{meth}[^a-z]"".format(meth=method),
                        ""(np.random.{meth}("".format(meth=method),
                        data,
                    )
            if m3:
                if not m1:
                    data = re.sub(self.p3, ""import numpy as np\n"", data)
                else:
                    data = re.sub(self.p3, """", data)
                data = re.sub(r""[\s]random.{1}"", "" np.random."", data)
                data = re.sub(r""[\(]random.{1}"", ""(np.random."", data)
            file.write(data)
            print(""Clean: "" + file_dir)

    def remove_backup(self) -> None:
        for file in self.backup_files:
            print(""Remove: "" + file)
            os.remove(file)


if __name__ == ""__main__"":
    script = NumpyRandomRelatedScript()
    script.search_py_files(script.base_dir)
    for f in script.py_files:
        script.do_the_clean_up(f)
    script.remove_backup()
```"
729936441,37433,REGR: fix groupby std() with nullable dtypes,jorisvandenbossche,closed,2020-10-26T21:56:27Z,2020-10-29T19:31:00Z,"Fixes #37415
"
399643140,24794,read_sas column limit,binaryc0de,closed,2019-01-16T04:29:56Z,2020-10-29T19:42:16Z,"I get an index error for reading any sas file (sas7bdat) over 2047 columns wide.  This issue can be reproduced simply by generating a 2048 column width file in excel, importing into SAS and writing it back out as sas7bdat file then trying to read with read_sas.

[testdata.zip](https://github.com/pandas-dev/pandas/files/2788506/testdata.zip)

IndexError: index 1 is out of bounds for axis 0 with size 1
"
396364109,24657,BUG: DataFrame/Series.tz_convert does not modifies original data with copy=False,mroeschke,closed,2019-01-07T06:29:49Z,2020-10-29T21:23:11Z,"- [x] closes #6326
- [x] tests added / passed
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [x] whatsnew entry

This also impacted `tz_localize` as well. Non blocking for 0.24
"
732651674,37496,DOC: release date for 1.1.4,simonjayhawkins,closed,2020-10-29T20:52:07Z,2020-10-29T22:00:51Z,
731641937,37473,CI: 32 bit maybe_indices_to_slice,jbrockmendel,closed,2020-10-28T17:21:27Z,2020-10-29T22:03:06Z,"- [ ] closes #xxxx
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
732693007,37502,REGR: revert behaviour of getitem with assigning with a Series,jorisvandenbossche,closed,2020-10-29T22:00:13Z,2020-10-30T08:42:59Z,"Closes #37427

@jbrockmendel this is reverting part of the clean-up you did in https://github.com/pandas-dev/pandas/pull/33643. I think the behavioural change was unintentional in that PR. We might still want to do it for 1.2, but then with a deprecation first."
701522113,36373,REGR: inplace arithmetic operation on Series no longer updating parent DataFrame,hjweide,closed,2020-09-15T00:45:16Z,2020-10-30T08:59:45Z,"- [X] I have checked that this issue has not already been reported.

- [X] I have confirmed this bug exists on the latest version of pandas.

- [X] (optional) I have confirmed this bug exists on the master branch of pandas.

---

**Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug.

#### Code Sample, a copy-pastable example

```python
import pandas as pd

df = pd.DataFrame({""A"": [1, 2, 3]})

a = df[""A""]
a += 999

print(df)
print(df[""A""])

assert a is df[""A""]
```

#### Problem description

I expect `print(df)` and `print(df[""A""])` to show identical values for column `A`.  This is not the case.  The output of the code above:
```
   A
0  1
1  2
2  3
0    1000
1    1001
2    1002
Name: A, dtype: int64
```

#### Expected Output
```
   A
0  1000
1  1001
2  1002
0    1000
1    1001
2    1002
Name: A, dtype: int64
```
#### Output of ``pd.show_versions()``

<details>

INSTALLED VERSIONS
------------------
commit           : fd20f7d34b8601a99b450a615241c233f034fc88
python           : 3.8.2.final.0
python-bits      : 64
OS               : Linux
OS-release       : 5.4.0-47-generic
Version          : #51-Ubuntu SMP Fri Sep 4 19:50:52 UTC 2020
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.0.dev0+379.gfd20f7d34
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.0.2
setuptools       : 44.0.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
None


</details>

Possibly related to https://github.com/pandas-dev/pandas/pull/36051?"
732988913,37514,Backport PR #37502: REGR: revert behaviour of getitem with assigning with a Series,jorisvandenbossche,closed,2020-10-30T08:42:39Z,2020-10-30T09:35:48Z,Backport of https://github.com/pandas-dev/pandas/pull/37502
732670672,37497,REGR: fix inplace arithmetic operation on Series no longer updating parent DataFrame,jorisvandenbossche,closed,2020-10-29T21:19:41Z,2020-10-30T09:36:58Z,"Closes #36373

See discussion at https://github.com/pandas-dev/pandas/pull/36498#issuecomment-708578774 for reverting the original fix (https://github.com/pandas-dev/pandas/pull/30501 for #30484)"
732694489,37503,Backport PR #37473 on branch 1.1.x (CI: 32 bit maybe_indices_to_slice),meeseeksmachine,closed,2020-10-29T22:03:15Z,2020-10-30T10:12:38Z,Backport PR #37473: CI: 32 bit maybe_indices_to_slice
705104401,36498,Maintain inplace operation on series,samilAyoub,closed,2020-09-20T12:47:33Z,2020-10-30T10:25:45Z,"- [ ] closes #36373
- [ ] tests added / passed
- [ ] passes `black pandas`
- [ ] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`
- [ ] whatsnew entry
"
732686851,37500,Backport PR #37496 on branch 1.1.x (DOC: release date for 1.1.4),meeseeksmachine,closed,2020-10-29T21:48:21Z,2020-10-30T10:32:39Z,Backport PR #37496: DOC: release date for 1.1.4
733005359,37515,Backport PR #37508: REGR: inplace Series op not actually operating inplace,jorisvandenbossche,closed,2020-10-30T09:06:46Z,2020-10-30T10:50:13Z,Backport of https://github.com/pandas-dev/pandas/pull/37508
719949541,37094,"BUG: Pandas 1.1.3 read_csv raises a TypeError when dtype, and index_col are provided, and file has >1M rows ",mgeplf,closed,2020-10-13T07:22:20Z,2020-10-30T10:51:28Z,"- [x] I have checked that this issue has not already been reported.

- [x] I have confirmed this bug exists on the latest version of pandas.

- [ ] (optional) I have confirmed this bug exists on the master branch of pandas.

---

#### Code Sample, a copy-pastable example

```python
import pandas as pd
import numpy as np

ROWS = 1000001  #  <--------- with 1000000, it works

with open('out.dat', 'w') as fd:
    for i in range(ROWS):
        fd.write('%d\n' % i)

df = pd.read_csv('out.dat', names=['a'], dtype={'a': np.float64}, index_col=['a'])

```

#### Problem description

When `ROWS = 1000001`, I get the following traceback:

```
Traceback (most recent call last):
  File ""try.py"", line 10, in <module>
    df = pd.read_csv('out.dat', names=['a'], dtype={'a': np.float64}, index_col=['a'])
  File ""/tmp/new_pandas/lib64/python3.6/site-packages/pandas/io/parsers.py"", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File ""/tmp/new_pandas/lib64/python3.6/site-packages/pandas/io/parsers.py"", line 458, in _read
    data = parser.read(nrows)
  File ""/tmp/new_pandas/lib64/python3.6/site-packages/pandas/io/parsers.py"", line 1196, in read
    ret = self._engine.read(nrows)
  File ""/tmp/new_pandas/lib64/python3.6/site-packages/pandas/io/parsers.py"", line 2231, in read
    index, names = self._make_index(data, alldata, names)
  File ""/tmp/new_pandas/lib64/python3.6/site-packages/pandas/io/parsers.py"", line 1677, in _make_index
    index = self._agg_index(index)
  File ""/tmp/new_pandas/lib64/python3.6/site-packages/pandas/io/parsers.py"", line 1770, in _agg_index
    arr, _ = self._infer_types(arr, col_na_values | col_na_fvalues)
  File ""/tmp/new_pandas/lib64/python3.6/site-packages/pandas/io/parsers.py"", line 1871, in _infer_types
    mask = algorithms.isin(values, list(na_values))
  File ""/tmp/new_pandas/lib64/python3.6/site-packages/pandas/core/algorithms.py"", line 443, in isin
    if np.isnan(values).any():
TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

```

#### Expected Output

With pandas 1.1.2, or ROWS = 1000000, it works fine.

#### Output of ``pd.show_versions()``

<details>
INSTALLED VERSIONS
------------------
commit           : db08276bc116c438d3fdee492026f8223584c477
python           : 3.6.3.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-957.38.3.el7.x86_64
Version          : #1 SMP Mon Nov 11 12:01:33 EST 2019
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.1.3
numpy            : 1.19.2
pytz             : 2020.1
dateutil         : 2.8.1
pip              : 20.2.3
setuptools       : 50.3.0
Cython           : None
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : None
jinja2           : None
IPython          : 7.16.1
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pytables         : None
pyxlsb           : None
s3fs             : None
scipy            : None
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None

</details>
"
732678757,37499,REGR: fix isin for large series with nan and mixed object dtype (causing regression in read_csv),jorisvandenbossche,closed,2020-10-29T21:33:13Z,2020-10-30T11:02:59Z,"Closes #37094
"
733082801,37517,Backport PR #37499 on branch 1.1.x (REGR: fix isin for large series with nan and mixed object dtype (causing regression in read_csv)),meeseeksmachine,closed,2020-10-30T11:03:40Z,2020-10-30T11:53:34Z,Backport PR #37499: REGR: fix isin for large series with nan and mixed object dtype (causing regression in read_csv)
718852456,37053,CLN: unify numpy.random-related imports,onshek,closed,2020-10-11T14:50:31Z,2020-10-30T13:22:51Z,"#### Location of the imports
https://github.com/pandas-dev/pandas/blob/8a25b23dd69d1e24840337797e1d20184920b5f3/pandas/tests/plotting/test_hist_method.py#L3-L4

#### Imports problem
~~All numpy-related imports are only about `numpy.random` including `randint`, `choice`, `normal`, etc~~
Part of numpy.random-related imports use aliases

#### Suggested fix for imports
eg: ~~`import numpy as np ` && `np.ranom.randn` --> `from numpy.random import randn`~~
`from numpy.random import randn` --> `import numpy as np ` && `np.ranom.randn`"
732563267,37492,CLN: unify numpy.random-related imports,onshek,closed,2020-10-29T18:41:55Z,2020-10-30T14:08:07Z,"- [x] closes #37053 ~~(keep this issue open until both #37492 and #37117 are completed)~~
- [x] tests added / passed
- [x] passes `black pandas`
- [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff`

# update 2020-10-30
This is the replacement for #37103, since I messed up the git timeline in the previous PR(the conflicts are too complex to fix). The separate CI is in #37117.

The whole project is reformatted except `pandas/_testing.py` for there are quite a few `tm.randn` used in testing files.

below is the script to do the whole clean-up
```
import os
import re


class NumpyRandomRelatedScript:
    def __init__(self, base_dir: str = ""/dir/of/pandas/pandas"") -> None:
        self.base_dir = base_dir
        self.py_files = []
        self.backup_files = []
        self.p1 = re.compile(""import numpy as np"")
        self.p2 = re.compile(""from numpy.random import[ ,a-zA-Z]*[\s]"")
        self.p3 = re.compile(""from numpy import random[\s]"")

    def search_py_files(self, pandas_file_dir: str) -> None:
        if os.path.isfile(pandas_file_dir):
            if pandas_file_dir[-3:] == "".py"":
                if pandas_file_dir != os.path.join(self.base_dir, ""_testing.py""):
                    self.py_files.append(pandas_file_dir)
        elif os.path.isdir(pandas_file_dir):
            for d in os.listdir(pandas_file_dir):
                self.search_py_files(os.path.join(pandas_file_dir, d))

    def do_the_clean_up(self, file_dir: str) -> None:
        with open(file_dir, ""r"") as file:
            data = file.read()
            m1, m2, m3 = (
                re.search(self.p1, data),
                re.search(self.p2, data),
                re.search(self.p3, data),
            )
            if not (m2 or m3):
                # print(""There's no need to change, please recheck!"")
                return

        backup_dir = file_dir + "".issue37053_backup""
        self.backup_files.append(backup_dir)
        print(""Backup: "" + backup_dir)
        with open(backup_dir, ""w+"") as file:
            file.write(data)

        with open(file_dir, ""w+"") as file:
            if m2:
                if not m1:
                    data = re.sub(self.p2, ""import numpy as np\n"", data)
                    m1 = True
                else:
                    data = re.sub(self.p2, """", data)
                methods = (
                    m2.group(0)
                    .replace(""from numpy.random import"", """")
                    .replace("" "", """")
                    .replace(""\n"", """")
                    .split("","")
                )
                if isinstance(methods, str):
                    methods = [methods]
                for method in methods:
                    data = re.sub(
                        r""[\s]{meth}[^a-z]"".format(meth=method),
                        "" np.random.{meth}("".format(meth=method),
                        data,
                    )
                    data = re.sub(
                        r""[\s]-{meth}[^a-z]"".format(meth=method),
                        "" -np.random.{meth}("".format(meth=method),
                        data,
                    )
                    data = re.sub(
                        r""[\(]{meth}[^a-z]"".format(meth=method),
                        ""(np.random.{meth}("".format(meth=method),
                        data,
                    )
            if m3:
                if not m1:
                    data = re.sub(self.p3, ""import numpy as np\n"", data)
                else:
                    data = re.sub(self.p3, """", data)
                data = re.sub(r""[\s]random.{1}"", "" np.random."", data)
                data = re.sub(r""[\(]random.{1}"", ""(np.random."", data)
            file.write(data)
            print(""Clean: "" + file_dir)

    def remove_backup(self) -> None:
        for file in self.backup_files:
            print(""Remove: "" + file)
            os.remove(file)


if __name__ == ""__main__"":
    script = NumpyRandomRelatedScript()
    script.search_py_files(script.base_dir)
    for f in script.py_files:
        script.do_the_clean_up(f)
    script.remove_backup()
```"
